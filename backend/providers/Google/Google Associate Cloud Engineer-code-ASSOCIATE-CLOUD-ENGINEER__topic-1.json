[
  {
    "topic": 1,
    "index": 1,
    "url": "https://www.examtopics.com/discussions/google/view/21681-exam-associate-cloud-engineer-topic-1-question-1-discussion/",
    "body": "Every employee of your company has a Google account. Your operational team needs to manage a large number of instances on Compute Engine. Each member of this team needs only administrative access to the servers. Your security team wants to ensure that the deployment of credentials is operationally efficient and must be able to determine who accessed a given instance. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGenerate a new SSH key pair. Give the private key to each member of your team. Configure the public key in the metadata of each instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAsk each member of the team to generate a new SSH key pair and to send you their public key. Use a configuration management tool to deploy those keys on each instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAsk each member of the team to generate a new SSH key pair and to add the public key to their Google account. Grant the \u05d2\u20accompute.osAdminLogin\u05d2\u20ac role to the Google group corresponding to this team.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGenerate a new SSH key pair. Give the private key to each member of your team. Configure the public key as a project-wide public SSH key in your Cloud Platform project and allow project-wide public SSH keys on each instance."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 66,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 4,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-09-20T01:33:00.000Z",
        "voteCount": 81,
        "content": "C is correct - https://cloud.google.com/compute/docs/instances/managing-instance-access"
      },
      {
        "date": "2024-09-20T01:40:00.000Z",
        "voteCount": 15,
        "content": "We recommend collecting users with the same responsibilities into groups and assigning IAM roles to the groups rather than to individual users. For example, you can create a \"data scientist\" group and assign appropriate roles to enable interaction with BigQuery and Cloud Storage. When a new data scientist joins your team, you can simply add them to the group and they will inherit the defined permissions. You can create and manage groups through the Admin Console."
      },
      {
        "date": "2020-05-31T13:40:00.000Z",
        "voteCount": 22,
        "content": "Send private key to users is not safe, i think it's C"
      },
      {
        "date": "2024-09-24T00:12:00.000Z",
        "voteCount": 1,
        "content": ". Which Kubernetes component would you use to ensure traffic is correctly routed to pods running your application?\nA. Pod router\nB. Deployment\nC. Service\nD. PersistentIPClaim\n\nCloud anyone please tell me the answer?"
      },
      {
        "date": "2021-09-08T02:20:00.000Z",
        "voteCount": 2,
        "content": "C. Service"
      },
      {
        "date": "2024-09-24T00:11:00.000Z",
        "voteCount": 5,
        "content": "Option C is correct because asking each member of the team to generate a new SSH key pair and to add the public key to their Google account allows the security team to manage the deployment of credentials efficiently. \n\nIt also allows the security team to determine who accessed a given instance because the public key is associated with the Google account of the user. \n\nGranting the \"compute.osAdminLogin\" role to the Google group corresponding to this team ensures that all members of the team have administrative access to the servers."
      },
      {
        "date": "2024-09-24T00:11:00.000Z",
        "voteCount": 5,
        "content": "C is correct\nIn this scenario, granting the \"compute.osAdminLogin\" role to a Google group corresponding to the operational team would be the best option. This role would provide each team member with administrative access to instances, and by adding their public key to their Google account, they can use it to authenticate their access to instances without the need for a separate key management system. Additionally, the security team's requirement for auditing access can be met by using Cloud Audit Logging to log all access to the instances."
      },
      {
        "date": "2024-09-24T00:11:00.000Z",
        "voteCount": 1,
        "content": "Avoiding the other options:\n\nDistributing a single private key to multiple members is not a best practice as it doesn't provide individual accountability.\nManually deploying public keys on each instance using a configuration management tool can be cumbersome and doesn't provide the flexibility and integration that Google's IAM provides.\nBy following the recommended approach, the organization can maintain a secure, traceable, and efficient method for managing access to Compute Engine instances."
      },
      {
        "date": "2024-09-24T00:10:00.000Z",
        "voteCount": 3,
        "content": "C is the most appropriate. In this option, each team member generates their own SSH key pair and adds the public key to their Google account. By granting the `compute.osAdminLogin` role to the corresponding Google group for this team, security is enhanced, and operational efficiency is improved. This setup also allows precise tracking of which member accessed which instance."
      },
      {
        "date": "2024-09-24T00:10:00.000Z",
        "voteCount": 2,
        "content": "Best Choice: C\n\nOption C is the best approach because it uses Google Cloud IAM roles to control access, integrates with Google accounts for individual credential management, and supports efficient auditing and access tracking. Each user will have their own SSH key, and their actions can be traced through IAM logs, providing both security and accountability."
      },
      {
        "date": "2024-08-16T02:27:00.000Z",
        "voteCount": 2,
        "content": "There is something which you must know. NEVER SHARE PRIVATE KEY."
      },
      {
        "date": "2024-07-21T17:55:00.000Z",
        "voteCount": 1,
        "content": "Why in the world would I give the private key to each member of my team. The answer is C"
      },
      {
        "date": "2024-05-23T09:57:00.000Z",
        "voteCount": 1,
        "content": "D https://cloud.google.com/compute/docs/instances/adding-removing-ssh-keys"
      },
      {
        "date": "2024-04-10T12:44:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2024-04-02T17:48:00.000Z",
        "voteCount": 1,
        "content": "C is Correct"
      },
      {
        "date": "2024-03-03T07:19:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2024-02-26T13:10:00.000Z",
        "voteCount": 1,
        "content": "Because they need administrative access only on the machines"
      },
      {
        "date": "2024-02-19T09:39:00.000Z",
        "voteCount": 1,
        "content": "C is Correct"
      },
      {
        "date": "2024-02-13T02:30:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 2,
    "url": "https://www.examtopics.com/discussions/google/view/16611-exam-associate-cloud-engineer-topic-1-question-2-discussion/",
    "body": "You need to create a custom VPC with a single subnet. The subnet's range must be as large as possible. Which range should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t0.0.0.0/0",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t10.0.0.0/8\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t172.16.0.0/12",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t192.168.0.0/16"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 65,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-09-20T01:41:00.000Z",
        "voteCount": 14,
        "content": "B is correct\nto calculate the range size for a network:\n10.0.0.0/x for example\nrange= 2^(32-x)\nthen the smaller x, the larger the range\n0.0.0.0/0 is not a valid network ip but is the broadcast ip"
      },
      {
        "date": "2024-09-24T00:13:00.000Z",
        "voteCount": 7,
        "content": "In Google Cloud Platform (GCP), when creating a VPC network, you should use the IP ranges that are reserved for private networks as defined by the RFC 1918. Here are the private IP address ranges defined by RFC 1918:\n\n10.0.0.0 to 10.255.255.255 (10.0.0.0/8)\n172.16.0.0 to 172.31.255.255 (172.16.0.0/12)\n192.168.0.0 to 192.168.255.255 (192.168.0.0/16)\nFrom the provided options:\n\nA. 0.0.0.0/0: This is not a private IP address range. It represents all possible IP addresses.\n\nB. 10.0.0.0/8: This is a private IP range that covers all IP addresses from 10.0.0.0 to 10.255.255.255. It's the largest range among the options.\n\nC. 172.16.0.0/12: This is a private IP range, but it's smaller than 10.0.0.0/8.\n\nD. 192.168.0.0/16: This is also a private IP range, but it's smaller than both B and C.\n\nSo, if you want the subnet's range to be as large as possible:\n\nThe correct answer is B. 10.0.0.0/8."
      },
      {
        "date": "2024-10-03T04:45:00.000Z",
        "voteCount": 1,
        "content": "I will become an Associate Cloud Engineer"
      },
      {
        "date": "2024-09-26T13:18:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is B. 10.0.0.0/8.\n\nHere's why:\n\nClass A: 10.0.0.0/8 provides the largest subnet range with 16,777,214 possible IP addresses. This is because it uses only the first 8 bits for the network address, leaving the remaining 24 bits for host addresses.\nClass B: 172.16.0.0/12 provides a smaller range with 1,048,574 possible IP addresses.\nClass C: 192.168.0.0/16 provides an even smaller range with 65,534 possible IP addresses.\n0.0.0.0/0: This is not a valid subnet range for a VPC. It represents the entire internet.\nTherefore, using the 10.0.0.0/8 range for your single subnet VPC will provide you with the maximum number of available IP addresses."
      },
      {
        "date": "2024-09-24T00:14:00.000Z",
        "voteCount": 1,
        "content": "Option B is the correct answer.\n\nTo create a custom VPC with a single subnet with the largest possible range, you should use the range 10.0.0.0/8. This range consists of 16,777,216 addresses, which is more than enough for most use cases."
      },
      {
        "date": "2024-09-24T00:13:00.000Z",
        "voteCount": 1,
        "content": "B is the correct answer: 10.0.0.0/8.\n\nThis is the largest subnet range that you can use in a custom VPC. It has 16,777,216 addresses, which is more than enough for most organizations.\n\nThe other options are smaller subnet ranges:\n\n0.0.0.0/0 is the entire IPv4 address space. It is not recommended to use this range for a subnet, because it would give you too many IP addresses to manage.\n\n172.16.0.0/12 has 1,048,576 addresses.\n\n192.168.0.0/16 has 65,536 addresses."
      },
      {
        "date": "2024-09-24T00:13:00.000Z",
        "voteCount": 2,
        "content": "The best option is **B. 10.0.0.0/8**.\n\nThis is because the `/8` in `10.0.0.0/8` denotes that only the first 8 bits (out of 32 bits in an IPv4 address) are reserved for identifying the network part of the address. The remaining 24 bits are available for host addresses within the network, which makes the subnet range as large as possible.\n\nOption A (`0.0.0.0/0`) represents all possible IPv4 addresses, which is not a valid or safe range for a single subnet. Options C (`172.16.0.0/12`) and D (`192.168.0.0/16`) have more bits reserved for the network part of the address, which leaves fewer bits for host addresses within the network, thus making the subnet range smaller than `10.0.0.0/8`. Therefore, option B is the best choice for a subnet with the largest possible range."
      },
      {
        "date": "2024-05-23T09:59:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer is A"
      },
      {
        "date": "2024-05-06T11:51:00.000Z",
        "voteCount": 1,
        "content": "A (0.0.0.0/0) is invalid. I tried creating a subnet with such a CIRD range and although the cloud console UI allowed it, I got an error at a later stage saying that I specified an invalid CIDR range for my subnet."
      },
      {
        "date": "2024-04-02T17:55:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2024-03-07T09:34:00.000Z",
        "voteCount": 1,
        "content": "even though option B (10.0.0.0/8) offers a larger total number of addresses, within a single subnet, the /12 prefix (172.16.0.0/12) provides more usable IP addresses due to a smaller network portion dedicated to the subnet itself. This allows you to assign more IP addresses to devices within that single subnet."
      },
      {
        "date": "2023-12-13T12:13:00.000Z",
        "voteCount": 1,
        "content": "B is Correct!"
      },
      {
        "date": "2023-12-10T12:25:00.000Z",
        "voteCount": 1,
        "content": "Custom VPC"
      },
      {
        "date": "2023-11-20T22:35:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2023-11-17T05:25:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2023-11-05T13:47:00.000Z",
        "voteCount": 1,
        "content": "The answer is B"
      },
      {
        "date": "2023-10-13T14:37:00.000Z",
        "voteCount": 1,
        "content": "A makes no sense, B is correct"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 3,
    "url": "https://www.examtopics.com/discussions/google/view/17330-exam-associate-cloud-engineer-topic-1-question-3-discussion/",
    "body": "You want to select and configure a cost-effective solution for relational data on Google Cloud Platform. You are working with a small set of operational data in one geographic location. You need to support point-in-time recovery. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSelect Cloud SQL (MySQL). Verify that the enable binary logging option is selected.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSelect Cloud SQL (MySQL). Select the create failover replicas option.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSelect Cloud Spanner. Set up your instance with 2 nodes.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSelect Cloud Spanner. Set up your instance as multi-regional."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 22,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-09-20T01:43:00.000Z",
        "voteCount": 51,
        "content": "A is Correct. You must enable binary logging to use point-in-time recovery. Enabling binary logging causes a slight reduction in write performance. https://cloud.google.com/sql/docs/mysql/backup-recovery/backups"
      },
      {
        "date": "2024-09-20T01:44:00.000Z",
        "voteCount": 4,
        "content": "In this link below, the docs explains clearly that point-in-time recovery requires binary logging.\n\nhttps://cloud.google.com/sql/docs/mysql/backup-recovery/pitr#disk-usage"
      },
      {
        "date": "2020-03-23T21:07:00.000Z",
        "voteCount": 11,
        "content": "A is correct, as Binary Logging enables Point in Recovery in Cloud SQL"
      },
      {
        "date": "2024-09-25T05:26:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/sql/docs/mysql/backup-recovery/restore#tips-pitr"
      },
      {
        "date": "2024-10-14T01:40:00.000Z",
        "voteCount": 1,
        "content": "When you create a Cloud SQL instance in the Google Cloud console, PITR is enabled by default. \nPITR uses binary logging to archive logs."
      },
      {
        "date": "2024-09-20T01:43:00.000Z",
        "voteCount": 1,
        "content": "**Best Choice: A**\n\nCloud SQL (MySQL) with binary logging enabled will provide point-in-time recovery capabilities, which meet your requirement for relational data in a single geographic location. It is also cost-effective for smaller datasets compared to Cloud Spanner."
      },
      {
        "date": "2024-06-12T17:32:00.000Z",
        "voteCount": 1,
        "content": "I agree with A"
      },
      {
        "date": "2024-02-19T09:52:00.000Z",
        "voteCount": 1,
        "content": "A is Correct"
      },
      {
        "date": "2023-12-13T13:37:00.000Z",
        "voteCount": 1,
        "content": "A is right answer!"
      },
      {
        "date": "2023-11-20T22:38:00.000Z",
        "voteCount": 1,
        "content": "No need of Cloud Spanner in this scenario. A is correct."
      },
      {
        "date": "2023-11-05T13:50:00.000Z",
        "voteCount": 1,
        "content": "The answer is A"
      },
      {
        "date": "2023-10-13T14:41:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2023-10-11T02:48:00.000Z",
        "voteCount": 1,
        "content": "A is Correct"
      },
      {
        "date": "2023-09-22T07:09:00.000Z",
        "voteCount": 4,
        "content": "Correct answer is A. Cloud SQL is a fully-managed relational database service that supports MySQL, PostgreSQL, and SQL Server. It offers high availability, automatic backups, and point-in-time recovery. By enabling binary logging, you can restore your database to a specific point in time. Cloud SQL is a cost-effective solution for small sets of operational data in one geographic location. It is also a good choice for developers who are familiar with MySQL."
      },
      {
        "date": "2023-09-01T09:11:00.000Z",
        "voteCount": 1,
        "content": "for point in recovery - binary logging a is the correct answer"
      },
      {
        "date": "2023-07-23T21:53:00.000Z",
        "voteCount": 1,
        "content": "Answe A"
      },
      {
        "date": "2023-06-07T14:43:00.000Z",
        "voteCount": 2,
        "content": "A is the correct answer Answer A: small set of operational data in one geographic location - CloudSQL (Cloud Spanner generally to hold large data and global)"
      },
      {
        "date": "2023-04-18T06:02:00.000Z",
        "voteCount": 1,
        "content": "Option A:\n\nhttps://cloud.google.com/sql/docs/mysql/backup-recovery/pitr\n\nPerform the point-in-time recovery using binary log positions\nWhile we recommend you perform point-in-time recovery using timestamps as described in the previous procedure, you can also perform point-in-time recovery by providing a specific binary log position in a binary log file."
      },
      {
        "date": "2023-03-22T02:36:00.000Z",
        "voteCount": 1,
        "content": "Binary logging and Cloud SQL is correct fit"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 4,
    "url": "https://www.examtopics.com/discussions/google/view/16782-exam-associate-cloud-engineer-topic-1-question-4-discussion/",
    "body": "You want to configure autohealing for network load balancing for a group of Compute Engine instances that run in multiple zones, using the fewest possible steps.<br>You need to configure re-creation of VMs if they are unresponsive after 3 attempts of 10 seconds each. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an HTTP load balancer with a backend configuration that references an existing instance group. Set the health check to healthy (HTTP)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an HTTP load balancer with a backend configuration that references an existing instance group. Define a balancing mode and set the maximum RPS to 10.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a managed instance group. Set the Autohealing health check to healthy (HTTP)\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a managed instance group. Verify that the autoscaling setting is on."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 27,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-09-22T06:32:00.000Z",
        "voteCount": 96,
        "content": "C, Agreed\nreference :  https://cloud.google.com/compute/docs/tutorials/high-availability-autohealing\n\nPro Tip: Use separate health checks for load balancing and for autohealing. Health checks for load balancing detect unresponsive instances and direct traffic away from them. Health checks for autohealing detect and recreate failed instances, so they should be less aggressive than load balancing health checks. Using the same health check for these services would remove the distinction between unresponsive instances and failed instances, causing unnecessary latency and unavailability for your users."
      },
      {
        "date": "2021-08-16T23:27:00.000Z",
        "voteCount": 28,
        "content": "I also vote for C\ngo to gcp console create a httpa load balancer and in the health check settings take your mouse to question mark it says \n\"\"\"Ensures that requests are sent only to instances that are up and running\"\"\" \nso its not recreating, if the vm not working it redirect to one which work.\n\ngo to gpc console create MIG and check the questions mark of Autohealing health check settings it says \n\"\"\"Autohealing allows recreating VM instances when needed. You can use a health check to recreate a VM instance if the health check finds it unresponsive. If you don't select a health check, Compute Engine will recreate VM instances only when they're not running.\"\"\"\n \nhope this help :)"
      },
      {
        "date": "2020-04-04T20:52:00.000Z",
        "voteCount": 22,
        "content": "A. Create an HTTP load balancer with a backend configuration that references an existing instance group. Set the health check to healthy(HTTP)\n\nThis is a possible answer. This answer assumes that the existing backend is configured correctly.\n\nB. Create an HTTP load balancer with a backend configuration that references an existing instance group. Define a balancing mode and set the maximum RPS to 10.\n\nThis is a possible answer. This answer assumes that the existing backend is configured correctly. This answer adds an additional step over answer A.\n\nC. Create a managed instance group. Set the Autohealing health check to healthy(HTTP)\n\nThis is only a partial solution. The default configuration is auto scaling enabled. You still need to create the HTTP Load Balancer.\n\nD. Create a managed instance group. Verify that the auto scaling setting is on.\n\nThis is only a partial solution. Creating a Managed Instance Group with Auto Scaling is required, but you still need to create the HTTP Load Balancer.\n\nTherefore the best answer is A in my opinion."
      },
      {
        "date": "2024-09-21T06:07:00.000Z",
        "voteCount": 1,
        "content": "It can't be A or B.  Question clearly states, \"using the fewest possible steps\" and setting up an HTTP load balancer is a long, drawn out process that requires quite a few steps and is never mentioned as part of the requirement."
      },
      {
        "date": "2021-07-27T22:34:00.000Z",
        "voteCount": 3,
        "content": "I hope you saw that in the questions is stated: \"network load balancing\""
      },
      {
        "date": "2020-11-04T23:05:00.000Z",
        "voteCount": 1,
        "content": "It's A.\nManaged group already exists so create a LB with health checks. \n\nIf you go for C, you will have to create a LB anyway so it's more steps to achieve the goal"
      },
      {
        "date": "2021-04-06T04:06:00.000Z",
        "voteCount": 8,
        "content": "https://www.youtube.com/watch?v=dT7xDEtALPQ&amp;list=PLIivdWyY5sqIij_cgINUHZDMnGjVx3rxi&amp;index=36\nstep-1: go to the instance group\nstep-2: click edit\nstep-3: scroll down you will see auto-healing off by default change to ON\nstep-4: create a health check saying 10 seconds as  CHECK INTERVAL  and UNHEALTHY THRESHOLD=3"
      },
      {
        "date": "2021-03-05T07:10:00.000Z",
        "voteCount": 6,
        "content": "It can't be A as a load balancer does not re-create unhealthy instances, as per the requirement.\n\nHas to be C\n\nhttps://cloud.google.com/compute/docs/instance-groups"
      },
      {
        "date": "2024-09-24T00:22:00.000Z",
        "voteCount": 2,
        "content": "C is the only one with the AUTOHEALING option, but it is not really correct. Remember, the GIVEN information are  \"a NETWORK load balancer and a group of Compute Engine Instances that run in multiple zones\" which gives us an idea that the existing configuration is a target pool-based network lb.\n\nIf we are to use the existing group of VMs, we need to choose UNMANAGED Instance Group, UNMANAGED Instance Group does not have Autohealing, only a health check. Health check only checks if VMs are responsive or not but does not re-create instances as what Autohealing and Autoscaling do. \n\nYou can also try re-creating the scenario or check this\nhttps://cloud.google.com/load-balancing/docs/network/transition-to-backend-services#console\n\nSo, if a MANAGED INSTANCE group is to be used, then you need to create an instance template and use it for your MIG. Ofc, you cannot use the existing VMs, then you create a new load balancer. Ofc, the existing group of VMs mentioned in the question will no longer be used but rather a new set of VMs based on the instance template will be created.  The choices should be updated."
      },
      {
        "date": "2024-09-24T00:22:00.000Z",
        "voteCount": 2,
        "content": "Option C is correct because creating a managed instance group allows you to use autohealing to automatically recreate VMs that are unresponsive after 3 attempts of 10 seconds each. You can set the Autohealing health check to healthy (HTTP) to specify the health check that determines whether the instances are considered healthy or not. If an instance becomes unresponsive, Autohealing will recreate the instance and attach it to the managed instance group.\n\nhttps://cloud.google.com/compute/docs/instance-groups/autohealing-instance-groups"
      },
      {
        "date": "2023-06-18T09:59:00.000Z",
        "voteCount": 1,
        "content": "The link url is invalid"
      },
      {
        "date": "2024-09-24T00:22:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is C.\nManaged instance groups are groups of homogeneous Compute Engine instances that are managed as a single entity. They can be used to distribute traffic across multiple instances and to provide high availability.\n\nAutohealing is a feature of managed instance groups that automatically replaces instances that fail health checks. You can configure autohealing to recreate instances if they are unresponsive after a certain number of attempts.\n\nTo configure autohealing for network load balancing, you need to create a managed instance group and set the Autohealing health check to healthy (HTTP). The health check will periodically probe the instances in the group to see if they are responding. If an instance fails the health check, autohealing will recreate it."
      },
      {
        "date": "2024-09-24T00:22:00.000Z",
        "voteCount": 1,
        "content": "Option C correct C. Create a managed instance group. Set the Autohealing health check to healthy (HTTP)\nExplanation:\n * Managed Instance Groups (MIGs) are specifically designed for managing and scaling groups of instances. They offer features like autohealing, load balancing, and autoscaling.\n * Autohealing is a key feature of MIGs that allows you to automatically recreate unhealthy instances based on health checks.\n\nWhy other options are incorrect:\n * A and B involve creating an HTTP load balancer, which is not directly related to autohealing. Load balancers distribute traffic but do not handle instance health checks and recreation.\n * D only creates a managed instance group and verifies autoscaling, which is not sufficient for autohealing. Autohealing requires a health check configuration.\nBy choosing option C, you directly address the requirements of configuring autohealing for a group of Compute Engine instances with the fewest possible steps."
      },
      {
        "date": "2024-09-20T03:28:00.000Z",
        "voteCount": 1,
        "content": "here we clearly need auto healing capability so C is correct."
      },
      {
        "date": "2024-07-25T13:29:00.000Z",
        "voteCount": 1,
        "content": "C, Agreed\n\n\nPro Tip: Use separate health checks for load balancing and for autohealing. Health checks for load balancing detect unresponsive instances and direct traffic away from them. Health checks for autohealing detect and recreate failed instances, so they should be less aggressive than load balancing health checks. Using the same health check for these services would remove the distinction between unresponsive instances and failed instances, causing unnecessary latency and unavailability for your users."
      },
      {
        "date": "2024-06-21T20:16:00.000Z",
        "voteCount": 1,
        "content": "A,B,C are regarding http whereas question is about network Lb. So Option D is correct"
      },
      {
        "date": "2024-05-23T10:00:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer is D"
      },
      {
        "date": "2024-02-19T09:58:00.000Z",
        "voteCount": 1,
        "content": "C is Correct"
      },
      {
        "date": "2024-01-27T08:21:00.000Z",
        "voteCount": 1,
        "content": "C is correct."
      },
      {
        "date": "2023-12-13T13:09:00.000Z",
        "voteCount": 1,
        "content": "C is Correct!"
      },
      {
        "date": "2023-11-21T07:00:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2023-11-05T14:03:00.000Z",
        "voteCount": 1,
        "content": "The answer is C"
      },
      {
        "date": "2023-10-11T02:50:00.000Z",
        "voteCount": 1,
        "content": "C is Correct Answer"
      },
      {
        "date": "2023-09-01T09:15:00.000Z",
        "voteCount": 1,
        "content": "C is the right answer"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 5,
    "url": "https://www.examtopics.com/discussions/google/view/20299-exam-associate-cloud-engineer-topic-1-question-5-discussion/",
    "body": "You are using multiple configurations for gcloud. You want to review the configured Kubernetes Engine cluster of an inactive configuration using the fewest possible steps. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse gcloud config configurations describe to review the output.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse gcloud config configurations activate and gcloud config list to review the output.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse kubectl config get-contexts to review the output.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse kubectl config use-context and kubectl config view to review the output.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 23,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 8,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-05-11T08:26:00.000Z",
        "voteCount": 46,
        "content": "D is correct"
      },
      {
        "date": "2021-05-10T14:06:00.000Z",
        "voteCount": 11,
        "content": "https://medium.com/google-cloud/kubernetes-engine-kubectl-config-b6270d2b656c\n\nexplains it well"
      },
      {
        "date": "2020-06-10T02:53:00.000Z",
        "voteCount": 25,
        "content": "C is correct , Use kubectl config get-contexts to review the output : shows the clusters and the configurations and based on the output we can identify the inactive configurations"
      },
      {
        "date": "2020-06-21T18:01:00.000Z",
        "voteCount": 10,
        "content": "This is wrong get-contexts does not show clusters it only shows contexts."
      },
      {
        "date": "2020-06-25T04:04:00.000Z",
        "voteCount": 1,
        "content": "True . \nWill give only below results    \nkubectl config get-contexts\nCURRENT   NAME          CLUSTER AUTHINFO           NAMESPACE\n*         white         white   dazwilkin\n          black         black   dazwilkin"
      },
      {
        "date": "2022-11-18T07:26:00.000Z",
        "voteCount": 2,
        "content": "kubectl config get-contexts displays a list of contexts as well as the clusters that use them. Here's a sample output."
      },
      {
        "date": "2024-10-14T01:45:00.000Z",
        "voteCount": 1,
        "content": "D is correct."
      },
      {
        "date": "2024-09-24T00:27:00.000Z",
        "voteCount": 2,
        "content": "Answer A: Using `gcloud config configurations described` will only show you the details of the current configuration, not the Kubernetes Engine cluster of an inactive configuration.\n\nAnswer B: Using `gcloud config configurations activate` and `gcloud config list` to review the output will only show you the list of configurations and activate one of them, but it won't provide you with the details of the Kubernetes Engine cluster of an inactive configuration.\n\nAnswer C: Using `kubectl config get-contexts` will only list the available contexts, including their clusters, but it won't provide you with the details of the Kubernetes Engine cluster of an inactive configuration."
      },
      {
        "date": "2024-09-24T00:13:00.000Z",
        "voteCount": 1,
        "content": "This command allows you to see the contexts available in your kubeconfig file, including those associated with inactive configurations, without needing to activate any configuration. It provides a straightforward way to review the clusters and contexts without additional steps."
      },
      {
        "date": "2024-09-18T13:32:00.000Z",
        "voteCount": 1,
        "content": "To review inactive gcloud configuration, the most direct and efficient command is to use gcloud config configurations describe, as it allows you to view the details of a specific configuration without needing to activate it first. This is ideal for reviewing the Kubernetes Engine cluster configured in that configuration without changing the active context.\n\nB involves activating the setting first, which requires additional steps.\nC and D refer to kubectl commands, which are not directly related to gcloud configuration but rather to managing Kubernetes contexts."
      },
      {
        "date": "2024-09-18T13:31:00.000Z",
        "voteCount": 1,
        "content": "To review inactive gcloud configuration, the most direct and efficient command is to use gcloud config configurations describe, as it allows you to view the details of a specific configuration without needing to activate it first. This is ideal for reviewing the Kubernetes Engine cluster configured in that configuration without changing the active context.\n\nB involves activating the setting first, which requires additional steps.\nC and D refer to kubectl commands, which are not directly related to gcloud configuration but rather to managing Kubernetes contexts."
      },
      {
        "date": "2024-08-10T23:00:00.000Z",
        "voteCount": 1,
        "content": "Best Choice: A\n\nOption A is the best choice. It allows you to review the details of the configurations using the `gcloud config configurations describe`, which provides a straightforward way to examine the configurations themselves. This approach does not require activating the configuration and provides the necessary configuration details directly.\n\nIf you need to see details about the Kubernetes Engine cluster in an inactive configuration, you might typically need to activate the configuration first and then use gcloud commands to list clusters, but since the goal is to use the fewest steps and you are only reviewing configurations, gcloud config configurations describe is the best fit for directly reviewing the configuration details."
      },
      {
        "date": "2024-08-03T18:17:00.000Z",
        "voteCount": 1,
        "content": "Option D assumes you know the configuration name and want to make it active for reviewing purposes. But to review a configuration you need not make it active. \nIf you don\u2019t want to activate the inactive configuration, then:\nUse gcloud config configurations list or Use kubectl config get-contexts to view the name of the configuration and it\u2019s status (active/inactive). This won't give cluster-related information.\n\nThere is No correct option. But, to review the configured Kubernetes Engine cluster, kubectl config view is a must. \nNote: the gcloud commands will only provide gcp-related details, not cluster-specific."
      },
      {
        "date": "2024-06-14T18:06:00.000Z",
        "voteCount": 2,
        "content": "The fewest steps to review the configured Kubernetes Engine cluster of an inactive configuration involve using the gcloud command-line tool and its configuration functionality:\n\n1. Use gcloud config configurations describe:\n\nThis command displays details about a specific configuration named after your inactive cluster setup. It provides information about the project, compute zone, and other cluster settings without needing to activate it."
      },
      {
        "date": "2024-05-23T10:01:00.000Z",
        "voteCount": 2,
        "content": "Correct Answer: D\nReference:\nhttps://medium.com/google-cloud/kubernetes-engine-kubectl-config-b6270d2b656c"
      },
      {
        "date": "2024-02-12T23:24:00.000Z",
        "voteCount": 1,
        "content": "C. Use kubectl config get-contexts to review the output.\nOption C, using kubectl config get-contexts, allows you to directly see the available contexts, including those from inactive configurations, and review the Kubernetes Engine clusters associated with them. This approach provides the necessary information efficiently.\nOption D (Use kubectl config use-context and kubectl config view to review the output) involves changing the active context and viewing the Kubernetes configuration but may involve unnecessary steps."
      },
      {
        "date": "2024-02-01T04:02:00.000Z",
        "voteCount": 2,
        "content": "D is right answer!\n\nGet Up-to-date: https://www.pinterest.com/pin/937522847419093171"
      },
      {
        "date": "2024-01-14T05:49:00.000Z",
        "voteCount": 3,
        "content": "A.\n\nIf we look at B:\nhttps://cloud.google.com/sdk/gcloud/reference/config/list - gcloud config list will show name of Kubernetes cluster used by default when working with a profile. \nhttps://cloud.google.com/sdk/gcloud/reference/config/configurations/activate - to switch to another profile you need to run gcloud config configurations activate. \nHowever, knowing the name of the gcloud profile, we can directly query it using \"gcloud config configurations describe &lt;PROFILE NAME&gt; --all\", while \"gcloud config list\" has no option for profile name as argument. \n\nIf we look at C:\n\"kubectl config get-contexts\" will show contexts configured for Kubectl tool, without considering the multiple profiles on gcloud. \n\nIf we look at D:\n\"kubectl config view\" output of this command the same in every selected context, except for the line of \"current-context\". One command more than in C and it is still not considering the Gcloud profile configurations."
      },
      {
        "date": "2024-01-07T23:13:00.000Z",
        "voteCount": 2,
        "content": "C is the right answer"
      },
      {
        "date": "2023-12-21T07:14:00.000Z",
        "voteCount": 1,
        "content": "The answer cannot be C or D because they are related to kubectl commands, not gcloud commands.\n\nOption C suggests using the kubectl config get-contexts command to review the Kubernetes context, which is not related to gcloud configurations.\nOption D suggests using the kubectl config use-context and kubectl config view commands to review the Kubernetes context, which again is not related to gcloud configurations.\nTherefore, the correct answer is B, which suggests using gcloud commands to activate the desired configuration and then list the details of the activated configuration, including the configured Kubernetes Engine cluster."
      },
      {
        "date": "2023-12-14T03:10:00.000Z",
        "voteCount": 2,
        "content": "Option A - gcloud config configurations describe &lt;INACTIVE_CONFIGURATION&gt;\nSeems to be correct as this command directly retrieves the details of the specified inactive configuration, including its Kubernetes Engine cluster configuration, in a single step. It avoids activating the inactive configuration or switching contexts, making it the most efficient and accurate approach.\n\nOption D -  kubectl config use-context and kubectl config view\nAlso correct but compared to the single command in option A, option D involves two separate commands and potentially an additional step to identify the correct context name for the inactive configuration.\n\nWhich one to chose? I think A right?"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 6,
    "url": "https://www.examtopics.com/discussions/google/view/18595-exam-associate-cloud-engineer-topic-1-question-6-discussion/",
    "body": "Your company uses Cloud Storage to store application backup files for disaster recovery purposes. You want to follow Google's recommended practices. Which storage option should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMulti-Regional Storage",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRegional Storage",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNearline Storage",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tColdline Storage\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 27,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 18,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 4,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-06-10T02:57:00.000Z",
        "voteCount": 71,
        "content": "Best Answer is \" Archive Storage \" \nhttps://cloud.google.com/storage/docs/storage-classes\n\nBut as per the given option next best solution is \" Coldline Storage\""
      },
      {
        "date": "2024-09-22T06:45:00.000Z",
        "voteCount": 1,
        "content": "No, archive storage might not be the correct choice as we need to consider the access time. Coldline Storage provides relatively faster access times compared to Archive Storage, which is important if you need to recover data quickly in a disaster scenario. \nColdline Storage: Fits well with disaster recovery use cases where data is infrequently accessed but needs to be available relatively quickly if a disaster occurs."
      },
      {
        "date": "2021-02-24T07:37:00.000Z",
        "voteCount": 6,
        "content": "Perfectly stated"
      },
      {
        "date": "2020-05-11T08:29:00.000Z",
        "voteCount": 12,
        "content": "D is correct, \nColdline Storage\tCOLDLINE\t90 days\t\n99.95% in multi-regions and dual-regions\n99.9% in regions"
      },
      {
        "date": "2024-10-14T01:48:00.000Z",
        "voteCount": 1,
        "content": "Answer is D."
      },
      {
        "date": "2024-09-24T01:25:00.000Z",
        "voteCount": 5,
        "content": "Ans is D. Coldline Storage: If the backups are truly for disaster scenarios and you expect very infrequent access (less than once a year).\nHere's how other option fits into this context:\n\nA. Multi-Regional Storage: This is best for data that is frequently accessed and needs to be highly available. It's ideal for serving content to users globally and for data that is accessed more often.\n\nB. Regional Storage: This offers high availability and is suited for storing data that is accessed frequently, but within a specific region. It's good for data used in compute operations in the same region.\n\nC. Nearline Storage: This is a low-cost option for storing infrequently accessed data. It's ideal for data that is accessed less than once a month. There are costs associated with accessing the data, so it's better for data that you don't expect to access frequently."
      },
      {
        "date": "2024-09-05T04:17:00.000Z",
        "voteCount": 2,
        "content": "honestly, they should've added archive storage because it is better suited for disaster recovery backup. It is also stated in the doc how archive is better choice than 'coldline storage'. Also in terms of latency, archive will take care of that in events of disaster;  as stated in the doc, \"In the event of a disaster recovery event, recovery time is key. Cloud Storage provides low latency access to data stored as Archive storage.\"\nhttps://cloud.google.com/storage/docs/storage-classes#archive"
      },
      {
        "date": "2024-05-23T10:02:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer: D\nReference:\nhttps://cloud.google.com/storage/docs/storage-classes#nearline"
      },
      {
        "date": "2024-05-09T06:11:00.000Z",
        "voteCount": 2,
        "content": "Multi-regional storage = standard storage used only for objects stored in multi-regiones or dual-regions. \nRegional storage = standard storage used only for objects stored in regions \nRefer to https://cloud.google.com/storage/docs/storage-classes#legacy  \nEach of the standard, nearline, coldline, and archive storage classes can have any of the location types, i.e., region, dual region, multi-region. \nCold-line storage is the ideal option here according to Google's best practices."
      },
      {
        "date": "2024-02-22T12:50:00.000Z",
        "voteCount": 1,
        "content": "Google recommends Archive Storage for disaster recovery, since Archive is not available as an answer, coldline is the nearest possible."
      },
      {
        "date": "2024-01-16T03:38:00.000Z",
        "voteCount": 4,
        "content": "disaster recovery purposes = Multi-Regional Storage\nhttps://cloud.google.com/storage/docs/locations#location_recommendations"
      },
      {
        "date": "2024-01-07T00:26:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/docs/geography-and-regions"
      },
      {
        "date": "2023-12-08T07:45:00.000Z",
        "voteCount": 1,
        "content": "This talks about disaster recovery and you need to be able to retrieve data according to the RTO, except for Nearline all others only guarantee 99.95 availability and you want to avoid retrieval costs. All the other storage classes have a cost associated with it. With that in mind nearline is the best storage class from a DR perspective."
      },
      {
        "date": "2023-11-23T19:25:00.000Z",
        "voteCount": 2,
        "content": "Best answer is D. Coldline Storage.\n\nThe choices specifically said Multi-Regional and Regional \"Storage\".\nMulti-Regional Storage is equivalent to Standard Storage and is different to Multi-regional location. \nCheck additional classes: https://cloud.google.com/storage/docs/storage-classes#legacy"
      },
      {
        "date": "2023-11-23T08:26:00.000Z",
        "voteCount": 4,
        "content": "for disaster recovery multi region is recommended."
      },
      {
        "date": "2023-11-07T07:04:00.000Z",
        "voteCount": 1,
        "content": "Answer should be ARCHIVE STORAGE\nArchive storage\nArchive storage is the lowest-cost, highly durable storage service for data archiving, online backup, and disaster recovery. Unlike the \"coldest\" storage services offered by other Cloud providers, your data is available within milliseconds, not hours or days.\nhttps://cloud.google.com/storage/docs/storage-classes#:~:text=Archive%20storage%20is%20the%20lowest,milliseconds%2C%20not%20hours%20or%20days."
      },
      {
        "date": "2023-11-05T14:18:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is D"
      },
      {
        "date": "2023-11-01T10:04:00.000Z",
        "voteCount": 1,
        "content": "B is correct following Google best practices"
      },
      {
        "date": "2023-10-11T02:53:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 7,
    "url": "https://www.examtopics.com/discussions/google/view/18825-exam-associate-cloud-engineer-topic-1-question-7-discussion/",
    "body": "Several employees at your company have been creating projects with Cloud Platform and paying for it with their personal credit cards, which the company reimburses. The company wants to centralize all these projects under a single, new billing account. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tContact cloud-billing@google.com with your bank account details and request a corporate billing account for your company.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a ticket with Google Support and wait for their call to share your credit card details over the phone.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the Google Platform Console, go to the Resource Manage and move all projects to the root Organizarion.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the Google Cloud Platform Console, create a new billing account and set up a payment method.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 22,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 8,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-05-26T04:19:00.000Z",
        "voteCount": 57,
        "content": "C is incomplete. Moving projects under an organisation doesn't change their linked billing project.\nhttps://cloud.google.com/resource-manager/docs/migrating-projects-billing\n----\nNote: The link between projects and billing accounts is preserved, irrespective of the hierarchy. When you move your existing projects into the organization they will continue to work and be billed as they used to before the migration, even if the corresponding billing account has not been migrated yet. \n----\nD is incomplete as well, after setting the billing account in the organisation you need to link the projects to the new billing account."
      },
      {
        "date": "2020-12-15T01:54:00.000Z",
        "voteCount": 3,
        "content": "I agree that neither C or D is correct. I did the cert a month ago and this question was not on it. Although a similar question about how to change the payment method from your own card in your project to to the company's \"card\". So they might have removed this one."
      },
      {
        "date": "2021-01-29T06:47:00.000Z",
        "voteCount": 7,
        "content": "What's was the answer your chose for your particular exam question?"
      },
      {
        "date": "2022-11-05T22:56:00.000Z",
        "voteCount": 2,
        "content": "We need to add a new payment method and need to set that as Primary, post that we need to remove the previous one\n\"If you want to remove a payment method, you should add a new payment method first.\"\nRefer : https://cloud.google.com/billing/docs/how-to/payment-methods"
      },
      {
        "date": "2022-01-24T13:30:00.000Z",
        "voteCount": 3,
        "content": "The given answers make D the only possible solution. C can not be right, you all need to look it up here: https://cloud.google.com/resource-manager/docs/project-migration#change_billing_account"
      },
      {
        "date": "2022-07-29T18:15:00.000Z",
        "voteCount": 3,
        "content": "This link explains clearly that move a project won't affect billing.\nhttps://cloud.google.com/resource-manager/docs/project-migration#permissions-billing"
      },
      {
        "date": "2020-06-10T03:00:00.000Z",
        "voteCount": 17,
        "content": "C is correct Answer. there will be 1 billing account for the organization and all projects under that organization are linked to single billing account."
      },
      {
        "date": "2022-03-18T00:12:00.000Z",
        "voteCount": 7,
        "content": "https://cloud.google.com/resource-manager/docs/project-migration#change_billing_account\n\"Moving a project from one organization to another won't impact billing, and charges will continue against the old billing account. \""
      },
      {
        "date": "2023-06-28T00:12:00.000Z",
        "voteCount": 1,
        "content": "The question is under the organization different projects are maintained the different cloud platforms.all the different project should single corporate bill account instead of the employee billing account. So try to update the corporate bill account details and mark it as primary for the all projects, post that employee account details need  to removed. So suitable recomanded option is D"
      },
      {
        "date": "2024-09-24T01:26:00.000Z",
        "voteCount": 4,
        "content": "Option A is incorrect because you cannot request a corporate billing account by emailing cloud-billing@google.com. This email address is for general billing inquiries and support.\n\nOption B is incorrect because you cannot create a ticket with Google Support to share your credit card details over the phone. To set up a payment method for a billing account, you must do it through the Google Cloud Platform Console.\n\nOption C is incorrect because moving projects to the root organization will not create a new billing account. You must first create a new billing account and then move the projects to the root organization to ensure that they are all billed to the same billing account.\n\nTherefore, the correct answer is Option D.\n\nhttps://cloud.google.com/billing/docs/how-to/manage-billing-account#create_a_new_billing_account"
      },
      {
        "date": "2024-09-24T01:26:00.000Z",
        "voteCount": 4,
        "content": "The correct answer is D. You have to follow the complete steps to successfully:\n1. Create a new Billing Account\n2. Move the existing projects into the new billing account\n3. Cancel the earlier billing accounts of individual projects \n\nThis would meet all the requirements in the question - to centrally have all the projects under a single billing account."
      },
      {
        "date": "2024-09-08T14:14:00.000Z",
        "voteCount": 1,
        "content": "D is correct,\n\"get-contexts\" shows us our Kubernetes cluster contexts, that's right. But the question says that you want to review the cluster itself, so you need to use-context to get into the cluster. Answer A: Using `gcloud config configurations described` will only show you the details of the current configuration, not the Kubernetes Engine cluster of an inactive configuration. Answer B: Using `gcloud config configurations activate` and `gcloud config list` to review the output will only show you the list of configurations and activate one of them, but it won't provide you with the details of the Kubernetes Engine cluster of an inactive configuration. Answer C: Using `kubectl config get-contexts` will only list the available contexts, including their clusters, but it won't provide you with the details of the Kubernetes Engine cluster of an inactive configuration."
      },
      {
        "date": "2024-09-08T14:13:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer is D"
      },
      {
        "date": "2024-04-02T13:11:00.000Z",
        "voteCount": 1,
        "content": "C is correct. Because it is the best practice recommended by google. And D, you cannot have multiple billing account in one project."
      },
      {
        "date": "2023-11-05T14:20:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is D."
      },
      {
        "date": "2023-11-01T01:18:00.000Z",
        "voteCount": 1,
        "content": "C is the Google best practice"
      },
      {
        "date": "2023-10-11T02:54:00.000Z",
        "voteCount": 2,
        "content": "D is right Ans"
      },
      {
        "date": "2023-09-01T09:22:00.000Z",
        "voteCount": 1,
        "content": "D : as per question new account , d is the right answer"
      },
      {
        "date": "2023-04-26T11:25:00.000Z",
        "voteCount": 2,
        "content": "I think this is multiple choice. C and D together make sense but otherwise they are both incomplete."
      },
      {
        "date": "2023-01-05T14:00:00.000Z",
        "voteCount": 2,
        "content": "D is correct, C is incorrect, because all accounts already is in root"
      },
      {
        "date": "2022-10-23T01:37:00.000Z",
        "voteCount": 2,
        "content": "D, as other options are not correct"
      },
      {
        "date": "2022-10-20T01:56:00.000Z",
        "voteCount": 2,
        "content": "D is more complete than C"
      },
      {
        "date": "2022-10-10T03:26:00.000Z",
        "voteCount": 2,
        "content": "It\u2019s pretty straight-forward: you should establish a new billing account with a company-based payment method. Then set all the projects to use that new billing account."
      },
      {
        "date": "2022-09-27T21:31:00.000Z",
        "voteCount": 3,
        "content": "I think \"D\" is the right answer (https://cloud.google.com/resource-manager/docs/project-migration#change_billing_account)"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 8,
    "url": "https://www.examtopics.com/discussions/google/view/20300-exam-associate-cloud-engineer-topic-1-question-8-discussion/",
    "body": "You have an application that looks for its licensing server on the IP 10.0.3.21. You need to deploy the licensing server on Compute Engine. You do not want to change the configuration of the application and want the application to be able to reach the licensing server. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReserve the IP 10.0.3.21 as a static internal IP address using gcloud and assign it to the licensing server.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReserve the IP 10.0.3.21 as a static public IP address using gcloud and assign it to the licensing server.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the IP 10.0.3.21 as a custom ephemeral IP address and assign it to the licensing server.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStart the licensing server with an automatic ephemeral IP address, and then promote it to a static internal IP address."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 19,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-05-13T08:04:00.000Z",
        "voteCount": 30,
        "content": "A\nIP 10.0.3.21 is internal by default, and to ensure that it will be static non-changing it should be selected as static internal ip address."
      },
      {
        "date": "2022-11-18T02:22:00.000Z",
        "voteCount": 1,
        "content": "How do you know it is internal by default?"
      },
      {
        "date": "2022-12-03T12:10:00.000Z",
        "voteCount": 4,
        "content": "https://cloud.google.com/vpc/docs/subnets#valid-ranges"
      },
      {
        "date": "2020-05-11T08:32:00.000Z",
        "voteCount": 24,
        "content": "it's obvious, A"
      },
      {
        "date": "2024-09-24T01:30:00.000Z",
        "voteCount": 5,
        "content": "The correct answer is Option A.\n\nTo deploy the licensing server on Compute Engine and ensure that the application can reach it, you should reserve the IP 10.0.3.21 as a static internal IP address and assign it to the licensing server.\n\nBy reserving IP 10.0.3.21 as a static internal IP address, you can ensure that the application can reach the licensing server at that IP address without changing the application's configuration.\n\nTo reserve the IP 10.0.3.21 as a static internal IP address and assign it to a Compute Engine instance using gcloud, you can use the following command:\n\ngcloud compute instances create [INSTANCE_NAME] --address [IP_ADDRESS] --no-address\n\nReplace [INSTANCE_NAME] with the name of the Compute Engine instance that you want to create, and [IP_ADDRESS] with the desired static internal IP address (in this case, 10.0.3.21). \n\nThe --no-address flag specifies that the instance should not be assigned a public IP address."
      },
      {
        "date": "2024-09-24T01:30:00.000Z",
        "voteCount": 4,
        "content": "The correct answer is A. Reserve the IP 10.0.3.21 as a static internal IP address using gcloud and assign it to the licensing server.\n\nTo reserve a static internal IP address, you can use the gcloud command-line tool. For example, to reserve the IP address 10.0.3.21, you would run the following command:\n\ngcloud compute addresses reserve 10.0.3.21\n\nOnce you have reserved the static internal IP address, you can assign it to the licensing server by running the following command:\n\ngcloud compute instances set-address licensing-server --address 10.0.3.21\n\nOnce you have assigned the static internal IP address to the licensing server, the application will be able to reach it using that IP address."
      },
      {
        "date": "2024-08-11T00:25:00.000Z",
        "voteCount": 1,
        "content": "Correct A: Due to an Application licensing server on the Static IP 10.0.3.21. You need to deploy the licensing server on Compute Engine. You do not want to change the configuration of the application and want the application to be able to reach the licensing server."
      },
      {
        "date": "2024-05-30T04:31:00.000Z",
        "voteCount": 1,
        "content": "D\nApplication Compatibility: It maintains the application's existing configuration looking for the IP 10.0.3.21 (assumed to be an internal address).\nStatic IP: Promoting the ephemeral IP to static ensures the licensing server retains the 10.0.3.21 address even after restarts."
      },
      {
        "date": "2024-05-23T10:03:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer is A"
      },
      {
        "date": "2023-12-24T21:46:00.000Z",
        "voteCount": 3,
        "content": "I agree that we should not expose the licensing server to the internet. But at the same time the in the question it is not mentioned that the application is deployed in the gcp environment."
      },
      {
        "date": "2023-11-05T14:26:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is A"
      },
      {
        "date": "2023-10-11T02:55:00.000Z",
        "voteCount": 1,
        "content": "A is Right Ans"
      },
      {
        "date": "2023-09-01T09:23:00.000Z",
        "voteCount": 1,
        "content": "Internal IP address"
      },
      {
        "date": "2023-08-23T21:38:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is A"
      },
      {
        "date": "2023-03-22T02:59:00.000Z",
        "voteCount": 3,
        "content": "Considering app is hosted in gcp internal ip address can be set as static"
      },
      {
        "date": "2022-12-12T09:48:00.000Z",
        "voteCount": 1,
        "content": "Option A is right among choices"
      },
      {
        "date": "2022-11-18T02:23:00.000Z",
        "voteCount": 3,
        "content": "It's not stated whether the app is deployed on GCP. A or B are both correct in my opinion."
      },
      {
        "date": "2023-01-29T03:16:00.000Z",
        "voteCount": 3,
        "content": "ip that starts with 10. is not a public IP. B = wrong."
      },
      {
        "date": "2022-06-22T14:57:00.000Z",
        "voteCount": 1,
        "content": "Static IP point to internal .. Option A is right"
      },
      {
        "date": "2022-05-28T09:10:00.000Z",
        "voteCount": 1,
        "content": "A\nobvious answer"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 9,
    "url": "https://www.examtopics.com/discussions/google/view/20302-exam-associate-cloud-engineer-topic-1-question-9-discussion/",
    "body": "You are deploying an application to App Engine. You want the number of instances to scale based on request rate. You need at least 3 unoccupied instances at all times. Which scaling type should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tManual Scaling with 3 instances.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBasic Scaling with min_instances set to 3.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBasic Scaling with max_instances set to 3.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAutomatic Scaling with min_idle_instances set to 3.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 12,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-05-11T08:48:00.000Z",
        "voteCount": 54,
        "content": "D is correct.\nApp Engine supports the following scaling types, which controls how and when instances are created:\n\nAutomatic\nBasic\nManual\nYou specify the scaling type in your app's app.yaml.\n\nAutomatic scaling\nAutomatic scaling creates instances based on request rate, response latencies, and other application metrics. You can specify thresholds for each of these metrics, as well as a minimum number instances to keep running at all times."
      },
      {
        "date": "2023-09-18T00:10:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/appengine/docs/legacy/standard/python/how-instances-are-managed"
      },
      {
        "date": "2021-05-12T21:47:00.000Z",
        "voteCount": 16,
        "content": "D is correct : https://cloud.google.com/appengine/docs/standard/go/config/appref\n \"App Engine calculates the number of instances necessary to serve your current application traffic based on scaling settings such as target_cpu_utilization and target_throughput_utilization. Setting min_idle_instances specifies the number of instances to run in addition to this calculated number. For example, if App Engine calculates that 5 instances are necessary to serve traffic, and min_idle_instances is set to 2, App Engine will run 7 instances (5, calculated based on traffic, plus 2 additional per min_idle_instances).\""
      },
      {
        "date": "2024-09-24T01:32:00.000Z",
        "voteCount": 2,
        "content": "The correct answer is Option D.\n\nTo scale the number of instances based on request rate and ensure that there are always at least 3 unoccupied instances, you should use Automatic Scaling with min_idle_instances set to 3.\n\nAutomatic Scaling automatically scales the number of instances based on request rate and other metrics, such as CPU and memory utilization. By setting min_idle_instances to 3, you can ensure that the instance group maintains at least 3 idle instances at all times, ready to handle incoming requests."
      },
      {
        "date": "2024-09-24T01:32:00.000Z",
        "voteCount": 4,
        "content": "The correct answer is D. Automatic Scaling with min_idle_instances set to 3. By setting min_idle_instances to 3, you can ensure that there are always at least 3 instances available to handle new requests. The other options are not as good:\n\nA. Manual Scaling requires you to manually adjust the number of instances running your application. \nB. Basic Scaling is a simpler version of Automatic Scaling. It automatically scales the number of instances based on request rate, but it does not allow you to specify the minimum number of idle instances. This means that there is no guarantee that there will always be at least 3 instances available to handle new requests.\nC. The max_instances setting specifies the maximum number of instances to keep running. By setting max_instances to 3, you are limiting the number of instances that your application can scale to. This is not ideal, especially if your application experiences sudden spikes in traffic."
      },
      {
        "date": "2024-09-24T01:32:00.000Z",
        "voteCount": 4,
        "content": "D. Automatic Scaling with min_idle_instances set to 3.\n\nAutomatic scaling adjusts the number of instances based on the request rate, while maintaining a minimum number of instances available. By setting min_idle_instances to 3, you ensure that at least 3 instances are running and available to handle requests, even when the request rate is low.\nManual scaling allows you to set a fixed number of instances, but does not automatically adjust based on the request rate. Basic scaling adjusts the number of instances based on the request rate, but does not allow you to set a minimum number of idle instances.\nIn order to keep at least 3 instances running and ready to handle requests, Automatic scaling with min_idle_instances set to 3 is the correct option."
      },
      {
        "date": "2024-05-23T10:03:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer: D\nReference:\nhttps://cloud.google.com/appengine/docs/standard/python/how-instances-are-managed"
      },
      {
        "date": "2023-11-22T01:27:00.000Z",
        "voteCount": 1,
        "content": "D.\nAs per, https://cloud.google.com/appengine/docs/legacy/standard/python/how-instances-are-managed#scaling_types"
      },
      {
        "date": "2023-11-05T14:28:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is D"
      },
      {
        "date": "2023-10-11T02:57:00.000Z",
        "voteCount": 1,
        "content": "D is Answer"
      },
      {
        "date": "2023-09-01T09:25:00.000Z",
        "voteCount": 1,
        "content": "D, as per request it should be automatic right , so d is the right answer"
      },
      {
        "date": "2022-12-12T05:01:00.000Z",
        "voteCount": 1,
        "content": "automatic scaling scales based on request rates, and you giveset min instances that you can run at all times"
      },
      {
        "date": "2022-12-11T01:48:00.000Z",
        "voteCount": 2,
        "content": "B is correct"
      },
      {
        "date": "2023-01-25T20:58:00.000Z",
        "voteCount": 2,
        "content": "Basic Scaling can\u2019t take min_instances as parameter."
      },
      {
        "date": "2023-01-28T00:37:00.000Z",
        "voteCount": 2,
        "content": "True. Reference: https://cloud.google.com/appengine/docs/legacy/standard/java/config/appref#scaling_elements"
      },
      {
        "date": "2022-10-23T01:43:00.000Z",
        "voteCount": 1,
        "content": "D, basic and manual scaling are not wise actions"
      },
      {
        "date": "2022-06-22T14:56:00.000Z",
        "voteCount": 1,
        "content": "D is correct."
      },
      {
        "date": "2022-05-22T03:17:00.000Z",
        "voteCount": 1,
        "content": "Go for D"
      },
      {
        "date": "2021-11-19T00:11:00.000Z",
        "voteCount": 1,
        "content": "D. Automatic Scaling with min_idle_instances set to 3."
      },
      {
        "date": "2021-11-18T22:31:00.000Z",
        "voteCount": 1,
        "content": "D. Automatic Scaling with min_idle_instances set to 3."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 10,
    "url": "https://www.examtopics.com/discussions/google/view/16682-exam-associate-cloud-engineer-topic-1-question-10-discussion/",
    "body": "You have a development project with appropriate IAM roles defined. You are creating a production project and want to have the same IAM roles on the new project, using the fewest possible steps. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse gcloud iam roles copy and specify the production project as the destination project.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse gcloud iam roles copy and specify your organization as the destination organization.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the Google Cloud Platform Console, use the 'create role from role' functionality.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the Google Cloud Platform Console, use the 'create role' functionality and select all applicable permissions."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 70,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-03-15T10:37:00.000Z",
        "voteCount": 43,
        "content": "Correct Answer is A not B"
      },
      {
        "date": "2020-05-01T17:12:00.000Z",
        "voteCount": 24,
        "content": "Correct answer is A"
      },
      {
        "date": "2024-09-24T01:35:00.000Z",
        "voteCount": 4,
        "content": "A could be possible if we were talking about organization in the question, But here, it's clearly specified \"*project*\"\n\"You have a development project with appropriate IAM roles defined. You are creating a production project and want to have the same IAM roles on the new project, using the fewest possible steps.\"\nfrom google doc: https://cloud.google.com/sdk/gcloud/reference/iam/roles/copy\n\nEXAMPLES\nTo create a copy of an existing role spanner.databaseAdmin into an organization with 1234567, run:\n\ngcloud iam roles copy --source=\"roles/spanner.databaseAdmin\" --destination=CustomViewer --dest-organization=1234567\nTo create a copy of an existing role spanner.databaseAdmin into a project with PROJECT_ID, run:\n\n\ngcloud iam roles copy --source=\"roles/spanner.databaseAdmin\" --destination=CustomSpannerDbAdmin --dest-project=PROJECT_ID"
      },
      {
        "date": "2024-09-24T01:35:00.000Z",
        "voteCount": 9,
        "content": "The correct answer is Option A.\n\nTo create the same IAM roles in a production project as in a development project, using the fewest possible steps, you can use the gcloud iam roles copy command and specify the production project as the destination project.\n\nThe `gcloud iam roles copy` command allows you to copy IAM roles between projects or organizations. By specifying the production project as the destination project, you can copy the IAM roles from the development project to the production project.\n\nOption B is incorrect because specifying your organization as the destination organization will copy the IAM roles to all projects within the organization, which is not what you want."
      },
      {
        "date": "2024-09-24T01:34:00.000Z",
        "voteCount": 7,
        "content": "The correct answer is A. Use gcloud iam roles copy and specify the production project as the destination project.\n\nThe gcloud iam roles copy command copies a role from one project to another. To use this command, you will need to know the name of the role that you want to copy and the name of the destination project.\n\nFor example, to copy the role roles/compute.instanceAdmin from the project my-dev-project to the project my-prod-project, you would run the following command:\n\ngcloud iam roles copy roles/compute.instanceAdmin my-dev-project my-prod-project\n\nThis command will copy the role roles/compute.instanceAdmin to the project my-prod-project. The role will have the same permissions in the production project as it does in the development project."
      },
      {
        "date": "2024-06-11T23:17:00.000Z",
        "voteCount": 3,
        "content": "Correct answer is A"
      },
      {
        "date": "2024-06-09T12:38:00.000Z",
        "voteCount": 2,
        "content": "A is right. B will propagate for all projects, not desired as per case description"
      },
      {
        "date": "2024-05-23T10:04:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer: B\nReference:\nhttps://cloud.google.com/sdk/gcloud/reference/iam/roles/copy"
      },
      {
        "date": "2024-06-09T12:37:00.000Z",
        "voteCount": 1,
        "content": "the exact same link shows A as right one. If you set dest org for it, it will inherint for all other projects."
      },
      {
        "date": "2024-01-22T23:56:00.000Z",
        "voteCount": 1,
        "content": "A is correct answer"
      },
      {
        "date": "2024-01-10T03:37:00.000Z",
        "voteCount": 1,
        "content": "it's A"
      },
      {
        "date": "2023-12-29T03:09:00.000Z",
        "voteCount": 1,
        "content": "A true"
      },
      {
        "date": "2023-11-22T02:05:00.000Z",
        "voteCount": 2,
        "content": "A\nAs per, https://cloud.google.com/sdk/gcloud/reference/iam/roles/copy"
      },
      {
        "date": "2023-11-05T14:31:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is A"
      },
      {
        "date": "2023-11-01T01:20:00.000Z",
        "voteCount": 1,
        "content": "Because the roles to be copied to a new project in the same organization"
      },
      {
        "date": "2023-10-24T12:37:00.000Z",
        "voteCount": 2,
        "content": "A is correct"
      },
      {
        "date": "2023-09-28T03:39:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is A"
      },
      {
        "date": "2023-09-01T09:26:00.000Z",
        "voteCount": 1,
        "content": "Yes, A is the right answer"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 11,
    "url": "https://www.examtopics.com/discussions/google/view/74372-exam-associate-cloud-engineer-topic-1-question-11-discussion/",
    "body": "You need a dynamic way of provisioning VMs on Compute Engine. The exact specifications will be in a dedicated configuration file. You want to follow Google's recommended practices. Which method should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeployment Manager\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Composer",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tManaged Instance Group",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUnmanaged Instance Group"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 42,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-29T13:55:00.000Z",
        "voteCount": 20,
        "content": "The correct answer is Option A - Deployment Manager. Deployment Manager is a configuration management tool that allows you to define and deploy a set of resources, including Compute Engine VMs, in a declarative manner. You can use it to specify the exact specifications of your VMs in a configuration file, and Deployment Manager will create and manage those VMs for you. Deployment Manager is recommended by Google as a way to automate and manage the deployment of resources on the Google Cloud Platform.\n\nhttps://cloud.google.com/deployment-manager/docs/"
      },
      {
        "date": "2023-10-24T23:20:00.000Z",
        "voteCount": 7,
        "content": "The question says - \"Dynamic way of provision VMs on Compute Engine\" which both the Managed Instance group and the Deployment Manager does, but for different purposes. So here the answer can be more of Deployment Manager as it covers the scope of question, while the Managed Instance group can also dynamically provision VMs based on configuration file but only for auto-healing or horizontal scaling purposes, the answer could have been C if the question was asked as \"Dynamic way of provision VMs on Compute Engine for horizontal scaling/auto healing\""
      },
      {
        "date": "2024-08-11T02:02:00.000Z",
        "voteCount": 1,
        "content": "A - due to provisioning VMs on Compute Engine - exact specifications will be in a dedicated configuration file."
      },
      {
        "date": "2024-05-23T11:17:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer: C\nReference:\nhttps://cloud.google.com/compute/docs/instances/"
      },
      {
        "date": "2023-11-05T14:32:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is A"
      },
      {
        "date": "2023-10-24T12:38:00.000Z",
        "voteCount": 1,
        "content": "C makes no sense, it's A"
      },
      {
        "date": "2024-05-10T05:19:00.000Z",
        "voteCount": 1,
        "content": "Why does C make no sense?"
      },
      {
        "date": "2023-10-11T03:01:00.000Z",
        "voteCount": 1,
        "content": "A  deployment manager"
      },
      {
        "date": "2023-09-01T09:28:00.000Z",
        "voteCount": 1,
        "content": "Yes, deployment manager is  the right answer,it helps you to configure resourres as per your file"
      },
      {
        "date": "2023-08-01T02:51:00.000Z",
        "voteCount": 4,
        "content": "I would go with C. A has a broader scope, but C is the correct answer for VM's."
      },
      {
        "date": "2023-07-26T09:22:00.000Z",
        "voteCount": 1,
        "content": "correct is A"
      },
      {
        "date": "2023-07-20T04:47:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is : A"
      },
      {
        "date": "2023-07-19T07:46:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is:\n\nA. Deployment Manager"
      },
      {
        "date": "2023-04-13T04:15:00.000Z",
        "voteCount": 2,
        "content": "A for sure"
      },
      {
        "date": "2023-03-29T21:10:00.000Z",
        "voteCount": 4,
        "content": "any one can share why is C"
      },
      {
        "date": "2023-04-07T21:49:00.000Z",
        "voteCount": 5,
        "content": "Managed Instance Group (option C) and Unmanaged Instance Group (option D) are Compute Engine features that allow you to group related VM instances and manage them as a single entity. However, they do not provide a dynamic way of provisioning VMs based on a configuration file like Deployment Manager does."
      },
      {
        "date": "2023-01-08T22:56:00.000Z",
        "voteCount": 2,
        "content": "Hi which answer we need to select? from discussion or website answer? could you please tell me?"
      },
      {
        "date": "2023-02-06T03:29:00.000Z",
        "voteCount": 2,
        "content": "Discussion is normally the correct one"
      },
      {
        "date": "2022-12-05T22:30:00.000Z",
        "voteCount": 1,
        "content": "A.\nExplained here: https://cloud.google.com/deployment-manager/docs/configuration/create-basic-configuration"
      },
      {
        "date": "2022-11-30T04:10:00.000Z",
        "voteCount": 2,
        "content": "A is the answer. We are talking about a dedicated config file. https://cloud.google.com/deployment-manager/docs"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 12,
    "url": "https://www.examtopics.com/discussions/google/view/18826-exam-associate-cloud-engineer-topic-1-question-12-discussion/",
    "body": "You have a Dockerfile that you need to deploy on Kubernetes Engine. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse kubectl app deploy &lt;dockerfilename&gt;.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse gcloud app deploy &lt;dockerfilename&gt;.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a docker image from the Dockerfile and upload it to Container Registry. Create a Deployment YAML file to point to that image. Use kubectl to create the deployment with that file.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a docker image from the Dockerfile and upload it to Cloud Storage. Create a Deployment YAML file to point to that image. Use kubectl to create the deployment with that file."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 44,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-04-20T20:17:00.000Z",
        "voteCount": 41,
        "content": "C is correct"
      },
      {
        "date": "2022-12-29T14:00:00.000Z",
        "voteCount": 23,
        "content": "The correct answer is Option C. To deploy a Docker container on Kubernetes Engine, you should first create a Docker image from the Dockerfile and push it to Container Registry, which is a fully-managed Docker container registry that makes it easy for you to store, manage, and deploy Docker container images. Then, you can create a Deployment YAML file that specifies the image to use and other desired deployment options, and use the kubectl command-line tool to create the deployment based on the YAML file. \n\nOption A is incorrect because kubectl app deploy is not a valid command. \n\nOption B is incorrect because gcloud app deploy is used to deploy applications to App Engine, not Kubernetes Engine. \n\nOption D is incorrect because it involves storing the image in Cloud Storage rather than Container Registry.\n\nhttps://cloud.google.com/kubernetes-engine/docs/how-to/deploying-a-container"
      },
      {
        "date": "2023-02-23T03:40:00.000Z",
        "voteCount": 3,
        "content": "The link you provided has stopped working :_("
      },
      {
        "date": "2024-10-01T07:36:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2024-05-23T11:18:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer: C\nReference -\nhttps://cloud.google.com/kubernetes-engine/docs/tutorials/hello-app"
      },
      {
        "date": "2023-11-05T14:33:00.000Z",
        "voteCount": 1,
        "content": "The answer is C."
      },
      {
        "date": "2023-10-11T03:03:00.000Z",
        "voteCount": 1,
        "content": "C is Correct Answer"
      },
      {
        "date": "2023-09-06T19:50:00.000Z",
        "voteCount": 1,
        "content": "C is correct. \nWe need to build docker image then push to Container Registry and setup yaml deployment in GKE to pull our registry image."
      },
      {
        "date": "2023-09-01T09:29:00.000Z",
        "voteCount": 1,
        "content": "c is correct ,, you have to first upload it in the continer registry"
      },
      {
        "date": "2023-03-14T13:35:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2022-11-30T04:11:00.000Z",
        "voteCount": 1,
        "content": "C is correct."
      },
      {
        "date": "2022-10-23T02:02:00.000Z",
        "voteCount": 1,
        "content": "c, u need to upload to Container Registry before deploying"
      },
      {
        "date": "2022-10-20T02:04:00.000Z",
        "voteCount": 1,
        "content": "C is correct, cuz you need to upload to Container Registry at first"
      },
      {
        "date": "2022-07-01T23:27:00.000Z",
        "voteCount": 1,
        "content": "Answer is C"
      },
      {
        "date": "2022-06-22T14:53:00.000Z",
        "voteCount": 1,
        "content": "C is right"
      },
      {
        "date": "2022-06-19T01:14:00.000Z",
        "voteCount": 1,
        "content": "C is correct answer"
      },
      {
        "date": "2022-05-30T03:51:00.000Z",
        "voteCount": 6,
        "content": "C is correct.\nA can be eliminated because kubectl app * is not a valid command\nB can be eliminated because gcloud app deploy deploys on app engine, not on kubernetes (also it still requires a config file pointing to the image).\nD is not correct, since you cannot deploy a container image directly from GCS"
      },
      {
        "date": "2022-05-22T03:28:00.000Z",
        "voteCount": 1,
        "content": "Go for C"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 13,
    "url": "https://www.examtopics.com/discussions/google/view/18827-exam-associate-cloud-engineer-topic-1-question-13-discussion/",
    "body": "Your development team needs a new Jenkins server for their project. You need to deploy the server using the fewest steps possible. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDownload and deploy the Jenkins Java WAR to App Engine Standard.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new Compute Engine instance and install Jenkins through the command line interface.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Kubernetes cluster on Compute Engine and create a deployment with the Jenkins Docker image.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse GCP Marketplace to launch the Jenkins solution.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 28,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-04-20T20:17:00.000Z",
        "voteCount": 34,
        "content": "D is correct"
      },
      {
        "date": "2022-12-29T14:07:00.000Z",
        "voteCount": 17,
        "content": "The correct answer is Option D. By using GCP Marketplace to launch the Jenkins solution, you can quickly deploy a Jenkins server with minimal steps. \n\nOption A involves deploying the Jenkins Java WAR to App Engine Standard, which requires more steps and may not be suitable for your requirements. \n\nOption B involves creating a new Compute Engine instance and manually installing Jenkins, which also requires more steps. \n\nOption C involves creating a Kubernetes cluster and creating a deployment with the Jenkins Docker image, which again involves more steps and may not be the most efficient solution."
      },
      {
        "date": "2024-10-01T07:42:00.000Z",
        "voteCount": 1,
        "content": "maybe b"
      },
      {
        "date": "2023-11-05T14:34:00.000Z",
        "voteCount": 1,
        "content": "The answer is D."
      },
      {
        "date": "2023-10-11T03:04:00.000Z",
        "voteCount": 1,
        "content": "D Answer"
      },
      {
        "date": "2023-09-25T09:04:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is D\n\nThis is the fastest and easiest way to deploy a Jenkins server on GCP. GCP Marketplace provides pre-configured and pre-packaged applications that you can launch with just a few clicks.\n\nTo deploy Jenkins from GCP Marketplace, follow these steps:\n\n-&gt;Go to the GCP Marketplace.\n-&gt;Search for \"Jenkins\".\n-&gt;Click the \"Jenkins\" app listing.\n-&gt;Click the \"Launch\" button.\n-&gt;Configure the Jenkins deployment options, such as the machine type and region.\n-&gt;Click the \"Deploy\" button.\n\nGCP Marketplace will deploy a Jenkins server to your GCP project. Once the deployment is complete, you can access the Jenkins web UI at the URL provided in the deployment details."
      },
      {
        "date": "2023-09-15T00:48:00.000Z",
        "voteCount": 1,
        "content": "D. Use GCP Marketplace to launch the Jenkins solution: This is the most straightforward and efficient method to deploy Jenkins on Google Cloud Platform"
      },
      {
        "date": "2023-09-01T09:30:00.000Z",
        "voteCount": 1,
        "content": "D is the correct answer , fewest steps possible"
      },
      {
        "date": "2023-04-16T19:42:00.000Z",
        "voteCount": 1,
        "content": "D IS CORRECT"
      },
      {
        "date": "2023-03-22T03:46:00.000Z",
        "voteCount": 1,
        "content": "Marketplace gives jenkins image to use"
      },
      {
        "date": "2023-03-02T04:37:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2023-01-20T20:57:00.000Z",
        "voteCount": 1,
        "content": "D is correct answer"
      },
      {
        "date": "2022-12-14T08:48:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2022-12-10T07:06:00.000Z",
        "voteCount": 1,
        "content": "D is the correct answer"
      },
      {
        "date": "2022-12-03T02:43:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2022-11-30T04:15:00.000Z",
        "voteCount": 1,
        "content": "D is correct.\nMarket place makes things easier."
      },
      {
        "date": "2022-11-16T06:38:00.000Z",
        "voteCount": 1,
        "content": "D is correct since Jenkins is a produce on GCP"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 14,
    "url": "https://www.examtopics.com/discussions/google/view/18828-exam-associate-cloud-engineer-topic-1-question-14-discussion/",
    "body": "You need to update a deployment in Deployment Manager without any resource downtime in the deployment. Which command should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tgcloud deployment-manager deployments create --config &lt;deployment-config-path&gt;",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tgcloud deployment-manager deployments update --config &lt;deployment-config-path&gt;\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tgcloud deployment-manager resources create --config &lt;deployment-config-path&gt;",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tgcloud deployment-manager resources update --config &lt;deployment-config-path&gt;"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 17,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-09-10T04:44:00.000Z",
        "voteCount": 48,
        "content": "B is correct Additional tip, update and create resource is not even a command under deployment management service."
      },
      {
        "date": "2020-04-20T20:18:00.000Z",
        "voteCount": 39,
        "content": "B is correct"
      },
      {
        "date": "2023-11-05T14:36:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is B"
      },
      {
        "date": "2023-10-11T03:07:00.000Z",
        "voteCount": 1,
        "content": "Answer B is Correct"
      },
      {
        "date": "2023-09-25T09:45:00.000Z",
        "voteCount": 3,
        "content": "The correct answer is B. \n\nThis command updates an existing deployment with the configuration specified in the deployment-config-path file. Deployment Manager automatically determines whether resources need to be created, updated, or deleted in order to apply the changes.\n\nHere is an example of how to use the gcloud deployment-manager deployments update command to update a deployment without any resource downtime:\n\ngcloud deployment-manager deployments update my-deployment --config deployment.yaml\n\nThis command will update the my-deployment deployment with the configuration specified in the deployment.yaml file. Deployment Manager will automatically determine whether resources need to be created, updated, or deleted in order to apply the changes."
      },
      {
        "date": "2023-09-01T09:31:00.000Z",
        "voteCount": 1,
        "content": "b is the answer, clearly"
      },
      {
        "date": "2023-04-11T08:59:00.000Z",
        "voteCount": 1,
        "content": "I agree B is the \ud83d\udd11"
      },
      {
        "date": "2023-03-22T03:48:00.000Z",
        "voteCount": 2,
        "content": "gcloud deployment-manager deployments update --config is correct"
      },
      {
        "date": "2023-02-13T23:04:00.000Z",
        "voteCount": 1,
        "content": "B will be correct answer"
      },
      {
        "date": "2023-02-03T23:38:00.000Z",
        "voteCount": 1,
        "content": "should not use 'create', so B or D is correct.... keywords 'resource' is invalid, so B is the ans."
      },
      {
        "date": "2022-12-29T14:13:00.000Z",
        "voteCount": 5,
        "content": "The correct answer is Option B: `gcloud deployment-manager deployments update --config &lt;deployment-config-path&gt;`. This command updates an existing deployment with the configuration specified in the `deployment-config-path` file. It allows you to make changes to the deployment without any downtime in the resources.\n\nhttps://cloud.google.com/sdk/gcloud/reference/deployment-manager/"
      },
      {
        "date": "2022-12-03T02:45:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2022-10-23T02:09:00.000Z",
        "voteCount": 4,
        "content": "should not use 'create', so B or D is correct\nand keywords 'resource' is invalid, so B is the ans"
      },
      {
        "date": "2022-10-10T15:20:00.000Z",
        "voteCount": 2,
        "content": "B is correct"
      },
      {
        "date": "2022-10-09T06:44:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2022-07-15T02:48:00.000Z",
        "voteCount": 2,
        "content": "B is correct. C and D are invalid commands and A is to create a new deployment."
      },
      {
        "date": "2022-07-01T23:28:00.000Z",
        "voteCount": 1,
        "content": "Answer is B"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 15,
    "url": "https://www.examtopics.com/discussions/google/view/19441-exam-associate-cloud-engineer-topic-1-question-15-discussion/",
    "body": "You need to run an important query in BigQuery but expect it to return a lot of records. You want to find out how much it will cost to run the query. You are using on-demand pricing. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tArrange to switch to Flat-Rate pricing for this query, then move back to on-demand.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the command line to run a dry run query to estimate the number of bytes read. Then convert that bytes estimate to dollars using the Pricing Calculator.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the command line to run a dry run query to estimate the number of bytes returned. Then convert that bytes estimate to dollars using the Pricing Calculator.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun a select count (*) to get an idea of how many records your query will look through. Then convert that number of rows to dollars using the Pricing Calculator."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 22,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-08-22T15:53:00.000Z",
        "voteCount": 44,
        "content": "Correct Answers is (B):\n\nOn-demand pricing\nUnder on-demand pricing, BigQuery charges for queries by using one metric: the number of bytes processed (also referred to as bytes read). You are charged for the number of bytes processed whether the data is stored in BigQuery or in an external data source such as Cloud Storage, Drive, or Cloud Bigtable. On-demand pricing is based solely on usage.\n\nhttps://cloud.google.com/bigquery/pricing#on_demand_pricing"
      },
      {
        "date": "2020-05-01T17:14:00.000Z",
        "voteCount": 35,
        "content": "B is Correct"
      },
      {
        "date": "2023-11-05T14:38:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is B"
      },
      {
        "date": "2023-10-11T03:08:00.000Z",
        "voteCount": 1,
        "content": "Answer B"
      },
      {
        "date": "2023-09-25T09:43:00.000Z",
        "voteCount": 4,
        "content": "The correct answer is B. \n\nThis option is the most accurate way to estimate the cost of your query, because it takes into account the actual number of bytes that will be processed by BigQuery.\n\nHere is an example of how to run a dry run query in BigQuery:\n\nbq dry run --query \"SELECT * FROM &lt;dataset&gt;.&lt;table&gt; WHERE &lt;condition&gt;\"\n\nThis command will print the estimated number of bytes that will be processed by BigQuery. You can then use the Pricing Calculator to convert this bytes estimate to dollars.\n\nOnce you have estimated the cost of your query, you can decide whether or not to proceed with running it. If you decide to proceed, you can monitor the cost of your query using the BigQuery Monitoring Console."
      },
      {
        "date": "2023-09-01T09:32:00.000Z",
        "voteCount": 1,
        "content": "yes B is the correct answer, biq query charges for queries by using one metric"
      },
      {
        "date": "2023-06-08T22:09:00.000Z",
        "voteCount": 1,
        "content": "B is Correct"
      },
      {
        "date": "2023-03-22T03:51:00.000Z",
        "voteCount": 3,
        "content": "Calculation should be on bytes read"
      },
      {
        "date": "2023-02-03T17:05:00.000Z",
        "voteCount": 1,
        "content": "Calculation on bytes read."
      },
      {
        "date": "2023-01-05T14:38:00.000Z",
        "voteCount": 2,
        "content": "B, You need check bytes read"
      },
      {
        "date": "2023-06-08T22:53:00.000Z",
        "voteCount": 1,
        "content": "Obviously ,the best answer here is B and not c."
      },
      {
        "date": "2022-12-29T14:23:00.000Z",
        "voteCount": 6,
        "content": "The correct answer is Option B. Running a dry run query using the command line can estimate the number of bytes read by the query, which can then be used to estimate the cost of running the query using the Pricing Calculator. To estimate the cost of a BigQuery query, you can use the `bq` command-line tool to run a dry-run query.\n\nOption A: Arranging to switch to Flat-Rate pricing will not help you estimate the cost of running the query using on-demand pricing.\n\nOption C: Estimating the number of bytes returned by the query will not give you an accurate estimate of the cost of running the query using on-demand pricing.\n\nOption D: Estimating the number of rows that the query will look through will not give you an accurate estimate of the cost of running the query using on-demand pricing.\n\nhttps://cloud.google.com/bigquery/docs/estimate-costs"
      },
      {
        "date": "2022-12-01T17:42:00.000Z",
        "voteCount": 2,
        "content": "number of bytes read"
      },
      {
        "date": "2022-10-23T02:19:00.000Z",
        "voteCount": 2,
        "content": "B, READ but not RETURNED"
      },
      {
        "date": "2022-07-01T23:29:00.000Z",
        "voteCount": 2,
        "content": "Answer is B"
      },
      {
        "date": "2022-06-22T14:51:00.000Z",
        "voteCount": 2,
        "content": "B is right . number bytes read instead of number of byte returned."
      },
      {
        "date": "2022-05-22T03:36:00.000Z",
        "voteCount": 3,
        "content": "Go for B"
      },
      {
        "date": "2022-04-26T06:44:00.000Z",
        "voteCount": 2,
        "content": "B Is correct"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 16,
    "url": "https://www.examtopics.com/discussions/google/view/16684-exam-associate-cloud-engineer-topic-1-question-16-discussion/",
    "body": "You have a single binary application that you want to run on Google Cloud Platform. You decided to automatically scale the application based on underlying infrastructure CPU usage. Your organizational policies require you to use virtual machines directly. You need to ensure that the application scaling is operationally efficient and completed as quickly as possible. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Google Kubernetes Engine cluster, and use horizontal pod autoscaling to scale the application.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an instance template, and use the template in a managed instance group with autoscaling configured.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an instance template, and use the template in a managed instance group that scales up and down based on the time of day.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a set of third-party tools to build automation around scaling the application up and down, based on Stackdriver CPU usage monitoring."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 17,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-12-27T09:23:00.000Z",
        "voteCount": 67,
        "content": "I'll take a simple and logical approach for answering this.\nLet's first break down the question into key requirements - \n1. automatically scale the application based on underlying infrastructure CPU usage.\n2. use virtual machines directly.\n \nA. Not feasible because VMs are not used directly here.\nB. This is the correct answer.\nC. Time of Day... Easy elimination because this does not scale on CPU usage and time of day is mentioned NOWHERE. \nD. Third Party Tools.... Nobody would use GCP if they needed third party tools to do something as simple as scaling based on CPU usage. all popular cloud providers have native solutions for this including GCP."
      },
      {
        "date": "2021-04-10T16:28:00.000Z",
        "voteCount": 18,
        "content": "and also D is out because why would I use a third party tool when is a GCP exam"
      },
      {
        "date": "2021-10-08T20:04:00.000Z",
        "voteCount": 5,
        "content": "If the resource/solution is not available. It's a possibility."
      },
      {
        "date": "2020-03-15T10:46:00.000Z",
        "voteCount": 54,
        "content": "correct is B as you have to use VM instances directly."
      },
      {
        "date": "2024-06-30T10:29:00.000Z",
        "voteCount": 1,
        "content": "The correct option is B as it manges the VMs"
      },
      {
        "date": "2023-11-05T14:41:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is B"
      },
      {
        "date": "2023-11-05T05:02:00.000Z",
        "voteCount": 1,
        "content": "B is the answers you can autoscale based on CPU Usage.  D is wrong as it suggests that the triggering is time of day"
      },
      {
        "date": "2023-10-11T03:14:00.000Z",
        "voteCount": 1,
        "content": "Answer C"
      },
      {
        "date": "2023-09-25T09:50:00.000Z",
        "voteCount": 2,
        "content": "The correct answer is B. \n\nThis option is the most efficient way to scale your application based on CPU usage, because it uses Google Cloud's built-in autoscaling capabilities. Autoscaling allows you to specify a minimum and maximum number of instances, and Google Cloud will automatically add or remove instances as needed to maintain your desired CPU utilization.\n\nOptions A, C, and D are not as efficient, because they require more manual intervention to scale your application.\n\nHere are the steps to create a managed instance group with autoscaling configured:\n\n1. Create an instance template.\n2. Create a managed instance group from the instance template.\n3. Configure autoscaling for the managed instance group.\n\nOnce you have configured autoscaling, the managed instance group will automatically add or remove instances as needed to maintain your desired CPU utilization."
      },
      {
        "date": "2023-09-14T04:17:00.000Z",
        "voteCount": 1,
        "content": "It sounds appropriate."
      },
      {
        "date": "2023-09-01T09:35:00.000Z",
        "voteCount": 1,
        "content": "B, is the right answer because, in b you can use the vm's directly and it will be autoconfigured"
      },
      {
        "date": "2023-07-16T04:23:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2023-02-03T17:09:00.000Z",
        "voteCount": 2,
        "content": "B correct answer."
      },
      {
        "date": "2022-12-29T14:30:00.000Z",
        "voteCount": 5,
        "content": "The correct answer is Option B. Creating an instance template and using it in a managed instance group with autoscaling configured will allow you to automatically scale the application based on underlying infrastructure CPU usage and will be operationally efficient and completed quickly. \nOption A is incorrect because it involves using Kubernetes, which is not required in this scenario. \n\nOption C is incorrect because it involves scaling based on the time of day, which is not specified as a requirement. \n\nOption D involves using third-party tools and is not necessary for this scenario."
      },
      {
        "date": "2022-12-08T08:50:00.000Z",
        "voteCount": 1,
        "content": "B, as MIG with autoscaling is best choice"
      },
      {
        "date": "2022-12-03T02:52:00.000Z",
        "voteCount": 1,
        "content": "correct is B"
      },
      {
        "date": "2022-11-30T04:18:00.000Z",
        "voteCount": 1,
        "content": "automatic scale"
      },
      {
        "date": "2022-10-23T02:20:00.000Z",
        "voteCount": 1,
        "content": "B, managed instance group (VM instances) with autoscaling"
      },
      {
        "date": "2022-08-16T00:32:00.000Z",
        "voteCount": 4,
        "content": "Our requirements are as per the question \n\n1. Use Virtual Machines directly (i.e. not container-based)\n2. Scale Automatically\n3. Scaling is efficient &amp; is quick\n\nB is correct \n\nManaged instance groups offer autoscaling capabilities that let you automatically add or delete instances from a managed instance group based on increases or decreases in load (CPU Utilization in this case). Autoscaling helps your apps gracefully handle increases in traffic and reduce costs when the need for resources is lower. You define the autoscaling policy and the autoscaler performs automatic scaling based on the measured load (CPU Utilization in this case). Autoscaling works by adding more instances to your instance group when there is more load (upscaling), and deleting instances when the need for instances is lowered (downscaling).\n\nRef: https://cloud.google.com/compute/docs/autoscaler"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 17,
    "url": "https://www.examtopics.com/discussions/google/view/15898-exam-associate-cloud-engineer-topic-1-question-17-discussion/",
    "body": "You are analyzing Google Cloud Platform service costs from three separate projects. You want to use this information to create service cost estimates by service type, daily and monthly, for the next six months using standard query syntax. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExport your bill to a Cloud Storage bucket, and then import into Cloud Bigtable for analysis.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExport your bill to a Cloud Storage bucket, and then import into Google Sheets for analysis.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExport your transactions to a local file, and perform analysis with a desktop tool.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExport your bill to a BigQuery dataset, and then write time window-based SQL queries for analysis.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 16,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-12-27T14:31:00.000Z",
        "voteCount": 105,
        "content": "Solving this by first eliminating the options that don't suit us. By breaking down the question into the key requirements-\n\n1. Analyzing Google Cloud Platform service costs from three separate projects. \n2. Using standard query syntax. -&gt; (Relational data and SQL)\n\nA. 'Cloud Storage bucket'........'Cloud Bigtable'. Not feasible, mainly because cloud BigTable is not good for Structured Data (or Relational Data on which we can run SQL queries as per the question's requirements). BigTable is better suited for Semi Structured data and NoSQL data.\nB. 'Cloud Storage bucket'.....'Google Sheets'. Not Feasible because there is no use of SQL in this option, which is one of the requirements.\nC. Local file, external tools... this is automatically eliminated because the operation we need is simple, and there has to be a GCP native solution for this. We shouldn't need to rely on going out of the cloud for such a simple thing.\nD. 'BigQuery'.....'SQL queries' -&gt; This is the right answer."
      },
      {
        "date": "2022-07-30T15:21:00.000Z",
        "voteCount": 15,
        "content": "Cloud billing data can only be exported to a JSON local file and to Bigquery. So, using Cloud Storage to export cloud billing data is not possible to do.\n\nhttps://cloud.google.com/billing/docs/how-to/export-data-bigquery"
      },
      {
        "date": "2020-03-08T20:43:00.000Z",
        "voteCount": 18,
        "content": "Agreed, BigQuery"
      },
      {
        "date": "2020-12-23T15:33:00.000Z",
        "voteCount": 3,
        "content": "the key is standard query syntax"
      },
      {
        "date": "2024-01-31T10:39:00.000Z",
        "voteCount": 1,
        "content": "Try it here: https://lookerstudio.google.com/reporting/0B7GT7ZlyzUmCZHFhNDlKVENHYmc/page/tLtE"
      },
      {
        "date": "2023-11-05T14:43:00.000Z",
        "voteCount": 2,
        "content": "The correct answer is D"
      },
      {
        "date": "2023-10-11T03:15:00.000Z",
        "voteCount": 1,
        "content": "D is the Answer"
      },
      {
        "date": "2023-09-25T09:56:00.000Z",
        "voteCount": 2,
        "content": "The correct answer is D. \n\nBigQuery is a fully-managed, petabyte-scale analytics data warehouse that enables businesses to analyze all their data very quickly. It is also a good choice for analyzing cost data because it can handle large amounts of data and can perform complex queries quickly.\n\nTime window-based SQL queries allow you to analyze data over a specific time period. For example, you could write a query to calculate the total cost of a particular service for each day of the month.\n\nHere is an example of a BigQuery query that you could use to calculate the total cost of a particular service for each day of the month:\nsql\nSELECT\n  cost,\n  DATE(usage_start_time) AS date\nFROM\n  `[PROJECT_ID].billing.dataset`\nWHERE\n  service_id = 'YOUR_SERVICE_ID'\nGROUP BY\n  date\nORDER BY\n  date\nThis query will return a table with two columns: `cost` and `date`. The `cost` column will contain the total cost of the service for each day of the month. The `date` column will contain the date for each row."
      },
      {
        "date": "2023-09-01T09:37:00.000Z",
        "voteCount": 1,
        "content": "yes D, is the correct answer, because we only use bigquery for semi or structured data"
      },
      {
        "date": "2023-04-03T05:34:00.000Z",
        "voteCount": 3,
        "content": "In GCP, Always use Big Query to export Billing. And Big Query is best for Analyzing"
      },
      {
        "date": "2023-03-22T03:55:00.000Z",
        "voteCount": 2,
        "content": "Big Query will allow sql analysis"
      },
      {
        "date": "2022-12-29T14:49:00.000Z",
        "voteCount": 5,
        "content": "The correct answer is Option D. Exporting the bill to a BigQuery dataset allows you to use SQL queries to analyze the data and create service cost estimates by service type, daily and monthly, for the next six months. This is an efficient and effective way to analyze the data, especially if you are familiar with SQL syntax. \n\nOption A, importing the bill into Cloud Bigtable, may be more complex and may not offer the same level of flexibility as using SQL queries in BigQuery. \n\nOption B, importing the bill into Google Sheets, may be more suitable for simple analysis, but may not be as efficient for more complex analysis. \n\nOption C, exporting the transactions to a local file and using a desktop tool, may not be as efficient or effective as using a cloud-based solution like BigQuery.\n\nhttps://cloud.google.com/billing/docs/how-to/export-data-bigquery\nhttps://cloud.google.com/bigquery/docs/reference/standard-sql/"
      },
      {
        "date": "2022-11-06T02:34:00.000Z",
        "voteCount": 1,
        "content": "Agreed, BigQuery"
      },
      {
        "date": "2022-10-23T02:21:00.000Z",
        "voteCount": 1,
        "content": "D, using standard query syntax so use BQ"
      },
      {
        "date": "2022-07-20T01:28:00.000Z",
        "voteCount": 1,
        "content": "Agreed, BigQuery \nAns: D"
      },
      {
        "date": "2022-07-01T23:31:00.000Z",
        "voteCount": 1,
        "content": "Answer is D"
      },
      {
        "date": "2022-06-22T14:49:00.000Z",
        "voteCount": 1,
        "content": "D is the right answer."
      },
      {
        "date": "2022-06-12T16:25:00.000Z",
        "voteCount": 1,
        "content": "I agreed, BigQuery Answer D is correct"
      },
      {
        "date": "2022-05-23T07:18:00.000Z",
        "voteCount": 1,
        "content": "answer is Option D"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 18,
    "url": "https://www.examtopics.com/discussions/google/view/21682-exam-associate-cloud-engineer-topic-1-question-18-discussion/",
    "body": "You need to set up a policy so that videos stored in a specific Cloud Storage Regional bucket are moved to Coldline after 90 days, and then deleted after one year from their creation. How should you set up the policy?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Storage Object Lifecycle Management using Age conditions with SetStorageClass and Delete actions. Set the SetStorageClass action to 90 days and the Delete action to 275 days (365 \u05d2\u20ac\" 90)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Storage Object Lifecycle Management using Age conditions with SetStorageClass and Delete actions. Set the SetStorageClass action to 90 days and the Delete action to 365 days.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse gsutil rewrite and set the Delete action to 275 days (365-90).",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse gsutil rewrite and set the Delete action to 365 days."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 63,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 21,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-05-31T13:54:00.000Z",
        "voteCount": 59,
        "content": "Answer is B. There should be no reason to recalculate the time needed to delete after a year."
      },
      {
        "date": "2020-12-01T19:17:00.000Z",
        "voteCount": 5,
        "content": "The correct ans is A."
      },
      {
        "date": "2021-05-03T07:05:00.000Z",
        "voteCount": 13,
        "content": "Right answer is clearly B, \"A\" does not make any sense."
      },
      {
        "date": "2020-06-25T01:30:00.000Z",
        "voteCount": 44,
        "content": "Correct is B.\nYou only re-calculate expiry date when objects are re-written using re-write option to another storage class in which case creation date is rest.\nBut in this case objects is moveed to Coldline class after 90 days and then we want to delete the object after 365 days."
      },
      {
        "date": "2020-08-29T02:22:00.000Z",
        "voteCount": 15,
        "content": "You can change the storage class of an existing object either by rewriting the object or by using Object Lifecycle Management...Since Object Life cycle management was used there was no need to recalculate the expiration date and delete action still remains 365 days.\n\nhttps://cloud.google.com/storage/docs/storage-classes"
      },
      {
        "date": "2024-09-18T20:28:00.000Z",
        "voteCount": 1,
        "content": "La gesti\u00f3n del ciclo de vida de objetos en Google Cloud Storage permite automatizar el cambio de la clase de almacenamiento y la eliminaci\u00f3n de los objetos en funci\u00f3n de su antig\u00fcedad. Para este caso:\n\nDespu\u00e9s de 90 d\u00edas, se debe cambiar la clase de almacenamiento a Coldline, lo que se hace usando la acci\u00f3n SetStorageClass.\n\nLuego, despu\u00e9s de 365 d\u00edas (1 a\u00f1o), los objetos deben ser eliminados usando la acci\u00f3n Delete.\n\nA es incorrecta porque sugiere eliminar los objetos a los 275 d\u00edas, lo cual no coincide con el requisito de eliminar los videos despu\u00e9s de un a\u00f1o.\n\nC y D son incorrectas porque gsutil rewrite no es la herramienta correcta para gestionar pol\u00edticas de ciclo de vida."
      },
      {
        "date": "2024-09-16T14:22:00.000Z",
        "voteCount": 2,
        "content": "option A can be misguiding but you don't have to specify 275 days since the rule would be implemented based on the creation of object not after the effect of the previous rule. Hence option B"
      },
      {
        "date": "2024-06-30T10:34:00.000Z",
        "voteCount": 1,
        "content": "Option A\nCloud Storage Object Lifecycle Management allows you to define a set of rules that manage the lifecycle of your objects. The Age condition specifies the number of days since the object's creation.\nThe SetStorageClass action changes the storage class of objects within the bucket. Setting it to 90 days means that 90 days after the object's creation, it will be moved to Coldline Storage.\nThe Delete action specifies when the object should be deleted. Because the SetStorageClass action occurs at 90 days, you would set the Delete action at 275 days (365 total days from the creation - 90 days already passed until the class change), resulting in the object being deleted one year from its creation."
      },
      {
        "date": "2024-02-29T04:19:00.000Z",
        "voteCount": 1,
        "content": "A is wrong cause there is no need of re-calculating. B is the right choice"
      },
      {
        "date": "2023-11-10T14:05:00.000Z",
        "voteCount": 1,
        "content": "Answer is B"
      },
      {
        "date": "2023-11-01T02:16:00.000Z",
        "voteCount": 2,
        "content": "Time of creation is the reference NOT the time of movement"
      },
      {
        "date": "2023-10-11T03:16:00.000Z",
        "voteCount": 1,
        "content": "Answer B"
      },
      {
        "date": "2023-09-25T09:59:00.000Z",
        "voteCount": 7,
        "content": "The correct answer is B. \n\nCloud Storage Object Lifecycle Management is a feature that allows you to automatically transition objects to different storage classes or delete them based on user-defined rules.\n\nTo set up a lifecycle management policy to move videos to Coldline after 90 days and then delete them after one year, you would create a rule with the following conditions and actions:\n\n* Condition: Age is greater than 90 days\n* Action: Set storage class to Coldline\n* Condition: Age is greater than 365 days\n* Action: Delete\n\nThis policy will ensure that your videos are automatically moved to Coldline after 90 days, where they will be stored at a lower cost. After one year, the videos will be automatically deleted.\n\nHere is an example of how to create a lifecycle management policy using the gcloud command-line tool:\n\n\ngcloud storage lifecycle management policies set my-bucket my-policy --action-set-storage-class coldline --condition-age-days 90 --action-delete --condition-age-days 365"
      },
      {
        "date": "2023-09-08T08:51:00.000Z",
        "voteCount": 1,
        "content": "\"... and then deleted after one year FROM THEIR CREATION\". I vote for A."
      },
      {
        "date": "2023-07-20T04:50:00.000Z",
        "voteCount": 1,
        "content": "Answer is B"
      },
      {
        "date": "2023-07-19T07:52:00.000Z",
        "voteCount": 1,
        "content": "Answer is B"
      },
      {
        "date": "2023-06-01T23:40:00.000Z",
        "voteCount": 1,
        "content": "Answer is B . \nAge condition :\nThe age condition is satisfied when a resource reaches the specified age (in days). Age is measured from the resource's creation time.\nFor objects, the creation time is the time when the object is successfully written to the bucket, such as when an upload completes.\nhttps://cloud.google.com/storage/docs/lifecycle#age"
      },
      {
        "date": "2023-04-03T05:44:00.000Z",
        "voteCount": 2,
        "content": "In GCP Storage, any rule either life cycle management or retention policy applies based on the creation date and time of the object. Even in the question they mention \" from their creation\". So ans is B."
      },
      {
        "date": "2023-03-24T14:31:00.000Z",
        "voteCount": 3,
        "content": "The answer is B. When using Age, it is calculated from when the object is written to the bucket. The age do not get affected when you change the storage class .https://cloud.google.com/storage/docs/lifecycle#age"
      },
      {
        "date": "2023-03-22T03:57:00.000Z",
        "voteCount": 3,
        "content": "NO need to recalculate as it is from object original creation date"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 19,
    "url": "https://www.examtopics.com/discussions/google/view/19444-exam-associate-cloud-engineer-topic-1-question-19-discussion/",
    "body": "You have a Linux VM that must connect to Cloud SQL. You created a service account with the appropriate access rights. You want to make sure that the VM uses this service account instead of the default Compute Engine service account. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWhen creating the VM via the web console, specify the service account under the 'Identity and API Access' section.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDownload a JSON Private Key for the service account. On the Project Metadata, add that JSON as the value for the key compute-engine-service- account.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDownload a JSON Private Key for the service account. On the Custom Metadata of the VM, add that JSON as the value for the key compute-engine- service-account.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDownload a JSON Private Key for the service account. After creating the VM, ssh into the VM and save the JSON under ~/.gcloud/compute-engine-service- account.json."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 40,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 22,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-05-01T17:19:00.000Z",
        "voteCount": 58,
        "content": "A is correct"
      },
      {
        "date": "2021-08-17T01:34:00.000Z",
        "voteCount": 18,
        "content": "I vote A\nhttps://cloud.google.com/compute/docs/access/create-enable-service-accounts-for-instances\n\nChanging the service account and access scopes for an instance\nIf you want to run the VM as a different identity, or you determine that the instance needs a different set of scopes to call the required APIs, you can change the service account and the access scopes of an existing instance. For example, you can change access scopes to grant access to a new API, or change an instance so that it runs as a service account that you created, instead of the Compute Engine default service account. However, Google recommends that you use the fine-grained IAM policies instead of relying on access scopes to control resource access for the service account.\n\nTo change an instance's service account and access scopes, the instance must be temporarily stopped. To stop your instance, read the documentation for Stopping an instance. After changing the service account or access scopes, remember to restart the instance. Use one of the following methods to the change service account or access scopes of the stopped instance.\n\nHope this helps :)"
      },
      {
        "date": "2021-06-09T16:08:00.000Z",
        "voteCount": 12,
        "content": "How can this be? It says you HAVE a VM, meaning it's already created. A cannot be the solution."
      },
      {
        "date": "2021-12-09T08:57:00.000Z",
        "voteCount": 3,
        "content": "As the comment says: \n\n\"To change an instance's service account and access scopes, the instance must be temporarily stopped ... After changing the service account or access scopes, remember to restart the instance.\" So we can stop the instance, change the service account, then start it up again."
      },
      {
        "date": "2021-09-23T08:23:00.000Z",
        "voteCount": 6,
        "content": "A seems legit, the answer is worded poorly but is the most correct. \n---\nhttps://cloud.google.com/compute/docs/access/create-enable-service-accounts-for-instances#changeserviceaccountandscopes\n---\n\"To change an instance's service account and access scopes, the instance must be temporarily stopped ... After changing the service account or access scopes, remember to restart the instance.\" So we can stop the instance, change the service account, then start it up again."
      },
      {
        "date": "2021-12-12T03:00:00.000Z",
        "voteCount": 12,
        "content": "Either the question or the answers are wrong. The question says that we HAVE a Linux VM, so we should strike all the answers that include \"when creating the VM..\" - on the other hand, adding JSON Tokens to VM metadata is terrible because it's readable in clear-text for everyone. So, what do we need to do here?"
      },
      {
        "date": "2024-09-20T04:10:00.000Z",
        "voteCount": 2,
        "content": "A is correct because the easiest solution is specifying the service account while creating the vm. if you dont specify the default compute engine account is chosen."
      },
      {
        "date": "2024-09-18T20:36:00.000Z",
        "voteCount": 1,
        "content": "Para asegurarse de que la m\u00e1quina virtual utilice una cuenta de servicio espec\u00edfica en lugar de la predeterminada de Compute Engine, debes especificar la cuenta de servicio correcta al crear la m\u00e1quina virtual. En la secci\u00f3n \"Identidad y acceso a API\", puedes seleccionar la cuenta de servicio adecuada para garantizar que las solicitudes y accesos a otros servicios, como Cloud SQL, se realicen usando los permisos asociados a esa cuenta de servicio.\n\nB, C y D implican el uso de claves privadas JSON, lo cual no es una pr\u00e1ctica recomendada debido a los riesgos de seguridad asociados al manejo de claves manualmente."
      },
      {
        "date": "2024-08-28T11:17:00.000Z",
        "voteCount": 1,
        "content": "A is correct."
      },
      {
        "date": "2024-06-17T23:36:00.000Z",
        "voteCount": 1,
        "content": "The question implies that a Linux VM already exists and needs to be configured to use a specific service account instead of the default Compute Engine service account. This is crucial because it eliminates option A, which focuses on setting the service account during VM creation.\n\nWhy Option C is Correct:\n\nCustom Metadata : Custom metadata is designed for VM-specific configuration. It's the ideal place to store service account credentials.\ncompute-engine-service-account : This is the specific metadata key used to tell the VM which service account to use.\nJSON Private Key : This is the standard format for storing service account credentials."
      },
      {
        "date": "2024-05-28T10:01:00.000Z",
        "voteCount": 1,
        "content": "select the service account directly in vm options, when creating or editing the VM. \n\nJSON private key? what are you talking about. You are all wrong"
      },
      {
        "date": "2024-01-16T04:00:00.000Z",
        "voteCount": 1,
        "content": "What documentation do you have on B, C, and D?"
      },
      {
        "date": "2023-12-27T13:18:00.000Z",
        "voteCount": 1,
        "content": "A is recommended way. C is correct but A is the recommended approach."
      },
      {
        "date": "2023-10-26T12:34:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2023-10-20T08:23:00.000Z",
        "voteCount": 1,
        "content": "the key is not directly provided to the VM (normally) only Service account to use \nhttps://docs.bridgecrew.io/docs/bc_gcp_iam_2"
      },
      {
        "date": "2023-10-11T03:18:00.000Z",
        "voteCount": 1,
        "content": "A is correct Answer"
      },
      {
        "date": "2023-09-25T10:08:00.000Z",
        "voteCount": 2,
        "content": "The correct answer is D. \n\nThis is the recommended approach, because it allows you to specify the service account that you want to use without having to modify the VM's metadata.\n\nThe other options are not as good:\n\nOption A is not as good, because it requires you to specify the service account when creating the VM. This can be inconvenient if you need to update the service account later.\n\nOption B is not as good, because it requires you to modify the VM's metadata. This can be complex and error-prone.\n\nOption C is not as good, because it requires you to modify the VM's custom metadata. This is not a recommended approach, because custom metadata is intended for use by custom applications."
      },
      {
        "date": "2023-08-25T15:00:00.000Z",
        "voteCount": 1,
        "content": "we have to use the newly created account rather VM default/attached SA."
      },
      {
        "date": "2023-07-20T04:52:00.000Z",
        "voteCount": 3,
        "content": "A is correct"
      },
      {
        "date": "2023-07-19T07:53:00.000Z",
        "voteCount": 2,
        "content": "A is correct"
      },
      {
        "date": "2023-07-18T07:52:00.000Z",
        "voteCount": 3,
        "content": "A is correct. No idea why you'd add anything to metadata of an instance https://cloud.google.com/compute/docs/metadata/overview\n\nThe SA can be specified in the web console during creation of the VM and also if the VM is stopped. This SA will then be used for everything that VM does.  Therefore, A is correct."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 20,
    "url": "https://www.examtopics.com/discussions/google/view/15899-exam-associate-cloud-engineer-topic-1-question-20-discussion/",
    "body": "You created an instance of SQL Server 2017 on Compute Engine to test features in the new version. You want to connect to this instance using the fewest number of steps. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall a RDP client on your desktop. Verify that a firewall rule for port 3389 exists.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall a RDP client in your desktop. Set a Windows username and password in the GCP Console. Use the credentials to log in to the instance.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet a Windows password in the GCP Console. Verify that a firewall rule for port 22 exists. Click the RDP button in the GCP Console and supply the credentials to log in.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet a Windows username and password in the GCP Console. Verify that a firewall rule for port 3389 exists. Click the RDP button in the GCP Console, and supply the credentials to log in."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 41,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 37,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-05-02T21:15:00.000Z",
        "voteCount": 56,
        "content": "I would say B is correct. RDP is enabled by default when you crate a Windows instance (no need to chek on it). Just make sure you install an RDP client ( chrome ext or RDP) and set windows password."
      },
      {
        "date": "2021-07-06T12:54:00.000Z",
        "voteCount": 18,
        "content": "Obviously, B is not the answer because you have to install an RDP client which is an extra step. D is the answer because you can connect directly using the RDP button in the GCP console."
      },
      {
        "date": "2021-11-29T07:15:00.000Z",
        "voteCount": 20,
        "content": "Tested it myself. At least on my Machines, I was asked to First install a RDP Client."
      },
      {
        "date": "2021-09-02T01:51:00.000Z",
        "voteCount": 26,
        "content": "No, we can't connect using RDP directly in the GCP console. When we click on it, it asks us to install RDP client. So ultimately, B is more accurate."
      },
      {
        "date": "2022-07-30T16:58:00.000Z",
        "voteCount": 5,
        "content": "The firewall rule for port 3389 is created by default if you create windows server on Compute Engine. So, no need to verify it.\n\nhttps://cloud.google.com/compute/docs/instances/connecting-to-windows#before-you-begin"
      },
      {
        "date": "2022-10-24T00:54:00.000Z",
        "voteCount": 6,
        "content": "In the link you provided the first step is to verify firewall rule was created, even if it is the default option!, so D is the most accurate even by the link."
      },
      {
        "date": "2024-02-06T06:17:00.000Z",
        "voteCount": 1,
        "content": "I disagree. The Q says that you have just created this VM, which means the FW rules for RDP has also been created. I believe the documentations says it a bit wonky as it is something to check if things aren't working after you've done everything right, as someone could have deleted that rule if this server has been up for a while, rather than having to check this right after you've made a new VM.\nI'd say B is better answer than D."
      },
      {
        "date": "2023-09-10T08:55:00.000Z",
        "voteCount": 1,
        "content": "You need firewall ingress rule for port 3398 in your VPC. It is not created by default."
      },
      {
        "date": "2020-05-01T17:17:00.000Z",
        "voteCount": 35,
        "content": "D seems more correct"
      },
      {
        "date": "2022-02-27T23:52:00.000Z",
        "voteCount": 21,
        "content": "I tested this on on Compute Engine today by deploying a new instance.  D is not correct.  When you click the RDP button, you are asked to install a client or use the Windows RDP client if you are running Windows.  There is no option to enter credentials or get an RDP session through the web interface."
      },
      {
        "date": "2024-10-18T00:21:00.000Z",
        "voteCount": 1,
        "content": "Option D is correct because it includes setting up a Windows username and password, verifying that port 3389 is open for RDP in the firewall, and then using the RDP button in the GCP Console to log in, ensuring all necessary steps for a successful connection are completed."
      },
      {
        "date": "2024-10-15T01:21:00.000Z",
        "voteCount": 1,
        "content": "D ... direct connection, no client installation and simplified network configurations"
      },
      {
        "date": "2024-09-24T03:23:00.000Z",
        "voteCount": 1,
        "content": "This option provides a clear set of steps: you set the necessary credentials, ensure the firewall allows RDP traffic, and then use the built-in RDP feature of the GCP Console to connect directly to the instance."
      },
      {
        "date": "2024-08-28T11:20:00.000Z",
        "voteCount": 1,
        "content": "Google Cloud Console provides an easy-to-use RDP button that allows you to directly connect to your instance without needing to separately install and configure an RDP client. This reduces the steps required for connection."
      },
      {
        "date": "2024-08-02T06:25:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2024-07-19T04:21:00.000Z",
        "voteCount": 2,
        "content": "Documentation says \"By default, Compute Engine creates firewall rules that allow RDP access on TCP port 3389. Verify that these firewall rules exist by visiting the firewall rules page in the Google Cloud console and looking for firewall rules that allow tcp:3389 connections.\""
      },
      {
        "date": "2024-07-20T12:54:00.000Z",
        "voteCount": 1,
        "content": "I think that the Q already not exist because much easier to do it by CLI and command which provides on created page of SQL instance?"
      },
      {
        "date": "2024-05-28T09:58:00.000Z",
        "voteCount": 1,
        "content": "who say D, better to change to other cloud.\nIf you click rdp button in gcp windows instance, create username and password to use in a remote desktop software. There is not one of this in gcp console. Yo need something external, like IAP Desktop or de Remote desktop programa in Windows pcs."
      },
      {
        "date": "2024-01-16T04:06:00.000Z",
        "voteCount": 1,
        "content": "The following sites do not describe RDP client installation.\nhttps://cloud.google.com/compute/docs/instances/connecting-to-windows"
      },
      {
        "date": "2024-01-07T01:38:00.000Z",
        "voteCount": 3,
        "content": "In general, both B and D options are correct. But, as per the question we need to identify which one has a minimum number of steps involved. As per Google documents, when you create a Windows Instance, the RDP Port 3389 is enabled by default, so here we need not \ncheck for this PORT status. Therefore answer is B. Here is a reference from GCP Documents \"Be sure the VM allows access through Remote Desktop Protocol (RDP). By default, Compute Engine creates firewall rules that allow RDP access on TCP port 3389. Verify that these firewall rules exist by visiting the firewall rules page in the Google Cloud console and looking for firewall rules that allow tcp:3389 connections.\""
      },
      {
        "date": "2024-01-07T01:02:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/compute/docs/instances/connecting-to-windows#remote-desktop-connection-app"
      },
      {
        "date": "2023-11-22T04:40:00.000Z",
        "voteCount": 3,
        "content": "B + D.\nIn B, Verify that a firewall rule for Port 3389 exists - is missing. \nin D, Install a RDP client in your desktop - is missing. \n\nGoing with B as 3389 port should be enabled to connect to a Windows Instance. However, the step of installing the RDP client is important and by clicking on RDP button in the GCP Console will let you download the &lt;instance name&gt;.rdp file which is a shortcut to open the Windows VM instance using RDP / mstsc."
      },
      {
        "date": "2023-11-21T04:49:00.000Z",
        "voteCount": 1,
        "content": "I would say D as the question says fewest number of steps. So three steps\n1. Set a Windows username and password in the GCP Console.\n2. Verify that a firewall rule for port 3389 (RDP) exists.\n3. Use the RDP button in the GCP Console to connect to the instance, supplying the credentials for login.\nIn option B it says install RDP client. Which I think not the headache of GCP if you have installed RDP or not."
      },
      {
        "date": "2023-10-11T03:22:00.000Z",
        "voteCount": 1,
        "content": "Answer D"
      },
      {
        "date": "2023-10-10T17:47:00.000Z",
        "voteCount": 1,
        "content": "Given the options:\n\nThe first option mentions port 22, which is for SSH (used typically for Linux instances) and is irrelevant in this context.\nThe second option matches the typical process of setting a Windows username/password, ensuring the firewall rule, and then using the GCP Console's RDP button.\nThe third option misses the step about setting a Windows username/password.\nThe fourth option involves the manual process of setting up an RDP client and then connecting.\nGiven the emphasis on \"fewest number of steps\", the best option is: D"
      },
      {
        "date": "2023-09-25T10:12:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is D. \n\nThis option does not require you to install any additional software on your desktop, and it allows you to connect to the instance with just a few clicks.\n\nThe other options are not as good:\n\nOption A requires you to install a RDP client on your desktop.\n\nOption B requires you to install a RDP client on your desktop and set a Windows username and password in the GCP Console.\n\nOption C requires you to set a Windows password in the GCP Console, verify that a firewall rule for port 22 exists, and then click the RDP button in the GCP Console and supply the credentials to log in."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 21,
    "url": "https://www.examtopics.com/discussions/google/view/16692-exam-associate-cloud-engineer-topic-1-question-21-discussion/",
    "body": "You have one GCP account running in your default region and zone and another account running in a non-default region and zone. You want to start a new<br>Compute Engine instance in these two Google Cloud Platform accounts using the command line interface. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate two configurations using gcloud config configurations create [NAME]. Run gcloud config configurations activate [NAME] to switch between accounts when running the commands to start the Compute Engine instances.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate two configurations using gcloud config configurations create [NAME]. Run gcloud configurations list to start the Compute Engine instances.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tActivate two configurations using gcloud configurations activate [NAME]. Run gcloud config list to start the Compute Engine instances.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tActivate two configurations using gcloud configurations activate [NAME]. Run gcloud configurations list to start the Compute Engine instances."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 26,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-04-30T11:35:00.000Z",
        "voteCount": 37,
        "content": "Correct answer is A as you can create different configurations for each account and create compute instances in each account by activating the respective account.Refer GCP documentation - Configurations Create &amp;amp; Activate Options B, C &amp;amp; D are wrong as gcloud config configurations list does not help create instances. It would only lists existing named configurations."
      },
      {
        "date": "2020-03-15T12:06:00.000Z",
        "voteCount": 28,
        "content": "A is the correct option"
      },
      {
        "date": "2024-09-18T23:05:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2023-11-05T15:12:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is A"
      },
      {
        "date": "2023-09-27T07:57:00.000Z",
        "voteCount": 4,
        "content": "The correct answer is A\n\nThis option allows you to create and activate different configurations for your GCP accounts. This way, you can easily switch between accounts and run commands without having to re-enter your credentials.\n\nThe other options are not as good:\n\n* Option B does not specify how to switch between accounts when running the commands to start the Compute Engine instances.\n* Option C does not specify how to create configurations for your GCP accounts.\n* Option D does not specify how to start the Compute Engine instances."
      },
      {
        "date": "2023-09-01T09:47:00.000Z",
        "voteCount": 1,
        "content": "Yes A seems to be more correct"
      },
      {
        "date": "2023-07-20T04:56:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2023-07-16T23:12:00.000Z",
        "voteCount": 2,
        "content": "Yes A is the correct Answer. using the gcloud configurations,we can create the we need to run the another compute engine instance"
      },
      {
        "date": "2023-06-06T14:51:00.000Z",
        "voteCount": 1,
        "content": "A is the correct answer uncontested. The rest state 'list' which is inaccurate."
      },
      {
        "date": "2023-03-22T04:12:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2023-03-05T01:20:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is A."
      },
      {
        "date": "2023-02-18T20:07:00.000Z",
        "voteCount": 7,
        "content": "Answer A is the correct answer.\n\nTo start a new Compute Engine instance in different GCP accounts using the command line interface, you can create two configurations using gcloud config configurations create [NAME], where [NAME] is the name of the configuration for each account. Then, you can switch between accounts using gcloud config configurations and activate [NAME] before running the gcloud compute instances create command to create the instance.\n\nAnswer B is incorrect because running gcloud configurations list only lists the available configurations and does not start Compute Engine instances.\n\nAnswer C is incorrect because activating configurations using gcloud configurations activate [NAME] only switches the current configuration, but does not start Compute Engine instances.\n\nAnswer D is incorrect because running gcloud configurations list only lists the available configurations and does not start Compute Engine instances."
      },
      {
        "date": "2022-11-30T04:38:00.000Z",
        "voteCount": 1,
        "content": "A is correct."
      },
      {
        "date": "2022-10-21T02:41:00.000Z",
        "voteCount": 1,
        "content": "A is correct answer"
      },
      {
        "date": "2022-10-10T15:37:00.000Z",
        "voteCount": 1,
        "content": "a is correct"
      },
      {
        "date": "2022-10-03T21:03:00.000Z",
        "voteCount": 1,
        "content": "A is correct bro!"
      },
      {
        "date": "2022-07-01T23:42:00.000Z",
        "voteCount": 1,
        "content": "Answer is A"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 22,
    "url": "https://www.examtopics.com/discussions/google/view/19550-exam-associate-cloud-engineer-topic-1-question-22-discussion/",
    "body": "You significantly changed a complex Deployment Manager template and want to confirm that the dependencies of all defined resources are properly met before committing it to the project. You want the most rapid feedback on your changes. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse granular logging statements within a Deployment Manager template authored in Python.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMonitor activity of the Deployment Manager execution on the Stackdriver Logging page of the GCP Console.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExecute the Deployment Manager template against a separate project with the same configuration, and monitor for failures.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExecute the Deployment Manager template using the \u05d2\u20ac\"-preview option in the same project, and observe the state of interdependent resources.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 24,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-05-03T21:45:00.000Z",
        "voteCount": 46,
        "content": "Correct answer is D as Deployment Manager provides the preview feature to check on what resources would be created"
      },
      {
        "date": "2023-02-18T20:15:00.000Z",
        "voteCount": 13,
        "content": "Answer D is the most appropriate choice for getting rapid feedback on changes to a Deployment Manager template.\n\nThe preview command in Deployment Manager creates a preview deployment of the resources defined in the configuration, without actually creating or modifying any resources. This allows you to quickly test and validate changes to the template before committing them to the project. During the preview, you can observe the state of interdependent resources and ensure that their dependencies are properly met. This provides rapid feedback on your changes, without actually creating any resources or incurring any costs."
      },
      {
        "date": "2023-12-03T01:13:00.000Z",
        "voteCount": 3,
        "content": "D is right based on this part of doc: https://cloud.google.com/deployment-manager/docs/deployments/updating-deployments#optional_preview_an_updated_configuration"
      },
      {
        "date": "2023-11-05T15:13:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is D."
      },
      {
        "date": "2023-09-27T08:00:00.000Z",
        "voteCount": 2,
        "content": "The correct option is D\n\nThe `--preview` option will preview the changes that will be made to the deployment without actually making them. This allows you to see how the changes will affect the deployment and to identify any potential problems.\n\nThe other options are not as good:\n\n* Option A is not as good, because it requires you to add logging statements to your Deployment Manager template. This can be time-consuming and error-prone.\n* Option B is not as good, because it requires you to monitor the Stackdriver Logging page of the GCP Console. This can be difficult to do, especially for complex deployments.\n* Option C is not as good, because it requires you to create a separate project with the same configuration. This can be time-consuming and expensive."
      },
      {
        "date": "2023-09-01T09:49:00.000Z",
        "voteCount": 1,
        "content": "D is the right answer aas deployment manager provides the preview option to check the templates"
      },
      {
        "date": "2023-07-20T04:57:00.000Z",
        "voteCount": 1,
        "content": "D is right"
      },
      {
        "date": "2023-07-18T20:01:00.000Z",
        "voteCount": 1,
        "content": "D is correct. Execute the Deployment Manager template using the \u05d2\u20ac\"-preview option in the same project, and observe the state of interdependent resources."
      },
      {
        "date": "2023-03-22T05:27:00.000Z",
        "voteCount": 1,
        "content": "D as it gives preview option to check"
      },
      {
        "date": "2022-11-30T04:44:00.000Z",
        "voteCount": 3,
        "content": "From: https://cloud.google.com/deployment-manager/docs/deployments/updating-deployments#optional_preview_an_updated_configuration\n\nPreview an updated configuration\nYou can preview the update you want to make before committing any changes, with the Google Cloud CLI or the API. The Deployment Manager service previews the configuration by expanding the full configuration and creating \"shell\" resources.\n\nDeployment Manager does not instantiate any actual resources when you preview a configuration, giving you the opportunity to see the deployment before committing to it.\n\ngcloud\nAPI\nWith the Google Cloud CLI, make an update request with the --preview parameter:\n\n\ngcloud deployment-manager deployments update example-deployment \\\n    --config configuration-file.yaml \\\n    --preview"
      },
      {
        "date": "2022-11-05T11:32:00.000Z",
        "voteCount": 1,
        "content": "deployment Manager provides the preview feature"
      },
      {
        "date": "2022-10-23T03:16:00.000Z",
        "voteCount": 1,
        "content": "D Preview mode"
      },
      {
        "date": "2022-07-01T23:43:00.000Z",
        "voteCount": 1,
        "content": "Answer is D"
      },
      {
        "date": "2022-06-22T14:41:00.000Z",
        "voteCount": 2,
        "content": "D is right.. Preview mode is best to verification"
      },
      {
        "date": "2022-06-18T12:37:00.000Z",
        "voteCount": 1,
        "content": "D FOR SURE"
      },
      {
        "date": "2022-05-22T12:31:00.000Z",
        "voteCount": 1,
        "content": "Go for D"
      },
      {
        "date": "2022-02-11T06:02:00.000Z",
        "voteCount": 2,
        "content": "Preview feature is available"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 23,
    "url": "https://www.examtopics.com/discussions/google/view/24012-exam-associate-cloud-engineer-topic-1-question-23-discussion/",
    "body": "You are building a pipeline to process time-series data. Which Google Cloud Platform services should you put in boxes 1,2,3, and 4?<br><img src=\"/assets/media/exam-media/04338/0001200001.jpg\" class=\"in-exam-image\"><br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Pub/Sub, Cloud Dataflow, Cloud Datastore, BigQuery",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFirebase Messages, Cloud Pub/Sub, Cloud Spanner, BigQuery",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Pub/Sub, Cloud Storage, BigQuery, Cloud Bigtable",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Pub/Sub, Cloud Dataflow, Cloud Bigtable, BigQuery\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 51,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-06-25T02:12:00.000Z",
        "voteCount": 68,
        "content": "Without a doubt D.\nWhenever we want to process timeseries data look for BigTable.\nAlso you want to perform analystics in Box 4 ..look for BigQuery\n\nOnly D provides this option."
      },
      {
        "date": "2020-07-09T04:21:00.000Z",
        "voteCount": 3,
        "content": "Speaker also looks like an IoT device so D not A"
      },
      {
        "date": "2020-12-03T17:42:00.000Z",
        "voteCount": 2,
        "content": "are we considering bigtable as storage in here , since they expecting some storage"
      },
      {
        "date": "2023-02-18T20:24:00.000Z",
        "voteCount": 42,
        "content": "The correct process for building a pipeline to process time-series data. Here's how each of the components is used:\n\n1. Cloud Pub/Sub: receives and distributes time-series data from different sources.\n2. Cloud Dataflow: processes the data by applying transformations and analytics.\n3. Cloud Bigtable: stores and manages the processed data as a NoSQL database.\n4. BigQuery: provides a SQL-like interface to analyze the data and extract insights.\n\nBy combining these components, you can create a scalable and reliable pipeline to process and analyze time-series data in real time."
      },
      {
        "date": "2023-03-18T10:24:00.000Z",
        "voteCount": 5,
        "content": "Many thanks Buruguduy.. for the extensive explanation you always give to your choice of answers. It's really helpful to understand the concept."
      },
      {
        "date": "2023-11-05T15:14:00.000Z",
        "voteCount": 1,
        "content": "Correct answer D"
      },
      {
        "date": "2023-09-27T10:59:00.000Z",
        "voteCount": 3,
        "content": "The correct answer is D\n\nThis diagram shows a typical pipeline for processing time-series data:\n\n1. **Cloud Pub/Sub:** A messaging service that allows you to send and receive messages between independent applications.\n2. **Cloud Dataflow:** A fully-managed service for transforming and processing data streams.\n3. **Cloud Bigtable:** A wide-column, distributed NoSQL database that is optimized for storing and analyzing large amounts of data.\n4. **BigQuery:** A fully-managed, petabyte-scale analytics data warehouse that enables businesses to analyze all their data very quickly.\n\nIn this pipeline, the following happens:\n\n1. Time-series data is sent to Cloud Pub/Sub.\n2. Cloud Dataflow reads the data from Cloud Pub/Sub and performs any necessary transformations or processing.\n3. Cloud Dataflow writes the transformed data to Cloud Bigtable.\n4. BigQuery queries the data in Cloud Bigtable to generate insights."
      },
      {
        "date": "2023-09-01T09:51:00.000Z",
        "voteCount": 1,
        "content": "big table for 3 , so d is the correct answer"
      },
      {
        "date": "2023-07-18T20:15:00.000Z",
        "voteCount": 1,
        "content": "D. Cloud Pub/Sub, Cloud Dataflow, Cloud Bigtable, BigQuery"
      },
      {
        "date": "2023-03-22T05:31:00.000Z",
        "voteCount": 1,
        "content": "Cloud Bigtable for time series data storage and BigQuery for analysis"
      },
      {
        "date": "2022-12-11T05:49:00.000Z",
        "voteCount": 1,
        "content": "Option D"
      },
      {
        "date": "2022-10-27T14:05:00.000Z",
        "voteCount": 2,
        "content": "I know the answer is D, but it's misleading.  \"Storage\" would suggest cloud storage, not BigTable."
      },
      {
        "date": "2022-11-15T01:07:00.000Z",
        "voteCount": 1,
        "content": "You can't store timeseries data in cloud storage. Even if you store it how you are gonna access it?"
      },
      {
        "date": "2022-09-09T01:35:00.000Z",
        "voteCount": 1,
        "content": "whenever IOT is present go with DataStore"
      },
      {
        "date": "2022-07-01T23:44:00.000Z",
        "voteCount": 1,
        "content": "Answer is D"
      },
      {
        "date": "2022-06-22T14:39:00.000Z",
        "voteCount": 1,
        "content": "D is right"
      },
      {
        "date": "2022-06-21T22:14:00.000Z",
        "voteCount": 1,
        "content": "D is the right answer"
      },
      {
        "date": "2022-06-19T23:00:00.000Z",
        "voteCount": 2,
        "content": "Look the picture posted on this page here\nhttps://cloud.google.com/dataflow#section-7"
      },
      {
        "date": "2022-06-19T22:59:00.000Z",
        "voteCount": 6,
        "content": "IoT = Unstructured data -&gt; eliminated Datastore + Timeseries requirement = BigTable\nIngestion point = Pub/Sub (Firebase messaging is a push notification service to client)\nProcess data + realtime or batch = Data flow\nAnalytics = BigQuery\nHence D."
      },
      {
        "date": "2022-06-07T01:18:00.000Z",
        "voteCount": 1,
        "content": "Answer is D"
      },
      {
        "date": "2022-05-22T12:32:00.000Z",
        "voteCount": 1,
        "content": "Go for D"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 24,
    "url": "https://www.examtopics.com/discussions/google/view/16693-exam-associate-cloud-engineer-topic-1-question-24-discussion/",
    "body": "You have a project for your App Engine application that serves a development environment. The required testing has succeeded and you want to create a new project to serve as your production environment. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse gcloud to create the new project, and then deploy your application to the new project.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse gcloud to create the new project and to copy the deployed application to the new project.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Deployment Manager configuration file that copies the current App Engine deployment into a new project.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy your application again using gcloud and specify the project parameter with the new project name to create the new project."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 15,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-03-15T12:13:00.000Z",
        "voteCount": 76,
        "content": "Correct is A.\nOption B is wrong as the option to use gcloud app cp does not exist.\nOption C is wrong as Deployment Manager does not copy the application, but allows you to specify all the resources needed for your application in a declarative format using yaml\nOption D is wrong as gcloud app deploy would not create a new project. The project should be created before usage"
      },
      {
        "date": "2020-07-06T11:45:00.000Z",
        "voteCount": 6,
        "content": "you're missing one thing. D isn't about using deployment manager to copy the configuration, instead, using the configuration file to copy the configuration from test project."
      },
      {
        "date": "2023-09-21T18:11:00.000Z",
        "voteCount": 1,
        "content": "A is correct since the documentation here explicitly mentioned the roles of external editors https://cloud.google.com/iam/docs/job-functions/auditing#scenario_external_auditors"
      },
      {
        "date": "2020-04-30T11:40:00.000Z",
        "voteCount": 14,
        "content": "Correct answer is A as gcloud can be used to create a new project and the gcloud app deploy can point to the new project.Refer GCP documentation - GCloud App Deploy.    \nOption B is wrong as the option to use gcloud app cp does not exist\n.Option C is wrong as Deployment Manager does not copy the application, but allows you to specify all the resources needed for your application in a declarative format using yaml\nOption D is wrong as gcloud app deploy would not create a new project. The project should be created before usage."
      },
      {
        "date": "2023-12-06T10:07:00.000Z",
        "voteCount": 1,
        "content": "the correct answer is C"
      },
      {
        "date": "2023-09-27T12:11:00.000Z",
        "voteCount": 3,
        "content": "The correct answer is A.\n\nThis is the simplest and most straightforward way to create a new production environment. It is also the most efficient way, because it does not require you to copy the deployed application to the new project.\n\nThe other options are not as good:\n\n* Option B is not as good, because it requires you to copy the deployed application to the new project. This can be time-consuming and error-prone.\n* Option C is not as good, because it requires you to create a Deployment Manager configuration file. This can be complex and time-consuming.\n* Option D is not as good, because it does not allow you to create a new project."
      },
      {
        "date": "2023-09-01T09:53:00.000Z",
        "voteCount": 1,
        "content": "option A seems more correct as in B copy command don't exist"
      },
      {
        "date": "2023-04-16T11:57:00.000Z",
        "voteCount": 2,
        "content": "As this is test on cloud infra then definitely the question is about creating instance with all the configuration which was present in the development environment. I think C is correct because we can copy the existing configuration from deployment manager to the new project."
      },
      {
        "date": "2023-02-18T20:33:00.000Z",
        "voteCount": 9,
        "content": "The correct answer is A. Use gcloud to create the new project, and then deploy your application to the new project.\n\nAnswer B, copying the deployed application to the new project, is not a recommended method for creating a new project since it can lead to inconsistencies and compatibility issues between the old and new projects.\n\nAnswer C, using a Deployment Manager configuration file to copy the current App Engine deployment to a new project, is also not recommended. Deployment Manager is a service that automates the deployment of infrastructure and applications, but it's not intended to copy a deployed application to a new project.\n\nAnswer D, deploying your application again using gcloud and specifying the project parameter with the new project name, is not recommended. This method will create a new deployment of your application in the same project as the development environment, rather than in a separate project for the production environment."
      },
      {
        "date": "2022-10-23T04:39:00.000Z",
        "voteCount": 2,
        "content": "A, one project can only have one deployment"
      },
      {
        "date": "2022-09-09T02:15:00.000Z",
        "voteCount": 1,
        "content": "cloud can be used to create a new project and the gcloud app deploy can point to the new project."
      },
      {
        "date": "2022-08-16T00:28:00.000Z",
        "voteCount": 2,
        "content": "A is correct \n\nYou can deploy to a different project by using \u2013project flag.\nBy default, the service is deployed the current project configured via:\n$ gcloud config set core/project PROJECT\nTo override this value for a single deployment, use the \u2013project flag:\n$ gcloud app deploy ~/my_app/app.yaml \u2013project=PROJECT\n\nRef: https://cloud.google.com/sdk/gcloud/reference/app/deploy"
      },
      {
        "date": "2022-06-22T14:37:00.000Z",
        "voteCount": 1,
        "content": "A is right"
      },
      {
        "date": "2022-05-22T15:11:00.000Z",
        "voteCount": 1,
        "content": "go for A"
      },
      {
        "date": "2022-01-01T07:40:00.000Z",
        "voteCount": 1,
        "content": "Think A also."
      },
      {
        "date": "2021-12-11T08:38:00.000Z",
        "voteCount": 3,
        "content": "Although the answer is not worded correctly, I think C is the right answer . Command is \ngcloud app deploy ~/my_app/app.yaml --project=PROJECT (https://cloud.google.com/sdk/gcloud/reference/app/deploy)"
      },
      {
        "date": "2021-11-19T00:54:00.000Z",
        "voteCount": 1,
        "content": "A. Use gcloud to create the new project, and then deploy your application to the new project."
      },
      {
        "date": "2021-11-18T23:16:00.000Z",
        "voteCount": 1,
        "content": "A. Use gcloud to create the new project, and then deploy your application to the new project."
      },
      {
        "date": "2021-11-18T15:08:00.000Z",
        "voteCount": 1,
        "content": "Correct is A"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 25,
    "url": "https://www.examtopics.com/discussions/google/view/16694-exam-associate-cloud-engineer-topic-1-question-25-discussion/",
    "body": "You need to configure IAM access audit logging in BigQuery for external auditors. You want to follow Google-recommended practices. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd the auditors group to the 'logging.viewer' and 'bigQuery.dataViewer' predefined IAM roles.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd the auditors group to two new custom IAM roles.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd the auditor user accounts to the 'logging.viewer' and 'bigQuery.dataViewer' predefined IAM roles.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd the auditor user accounts to two new custom IAM roles."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 47,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 10,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 4,
        "isMostVoted": false
      },
      {
        "answer": "\u3042",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-03-15T12:20:00.000Z",
        "voteCount": 82,
        "content": "Correct is A.\nAs per google best practices it is recommended to use predefined roles and create groups to control access to multiple users with same responsibility"
      },
      {
        "date": "2020-07-04T07:55:00.000Z",
        "voteCount": 5,
        "content": "You assume Auditors Group = External Auditors only. Auditors Group may contain both Internal and External Auditors."
      },
      {
        "date": "2020-12-01T13:07:00.000Z",
        "voteCount": 14,
        "content": "The question literally says - External Auditors"
      },
      {
        "date": "2021-03-24T02:26:00.000Z",
        "voteCount": 2,
        "content": "I can create External group and Internal group Auditors"
      },
      {
        "date": "2020-07-27T09:59:00.000Z",
        "voteCount": 46,
        "content": "Correct answer is A as per:\nhttps://cloud.google.com/iam/docs/job-functions/auditing#scenario_external_auditors"
      },
      {
        "date": "2023-10-17T11:53:00.000Z",
        "voteCount": 1,
        "content": "very useful\n\nThe organization creates a Google group for these external auditors and adds the current auditor to the group. This group is monitored and is typically granted access to the dashboard application"
      },
      {
        "date": "2024-09-22T10:36:00.000Z",
        "voteCount": 1,
        "content": "I will go A"
      },
      {
        "date": "2024-07-30T19:13:00.000Z",
        "voteCount": 1,
        "content": "The organization creates a Google group for these external auditors and adds the current auditor to the group. This group is monitored and is typically granted access to the dashboard application.\nhttps://cloud.google.com/iam/docs/job-functions/auditing#scenario_external_auditors"
      },
      {
        "date": "2024-07-24T04:34:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer is A. you should add a role to the group of users instead of adding particular users in IAM"
      },
      {
        "date": "2024-06-10T06:14:00.000Z",
        "voteCount": 1,
        "content": "correct answer is A"
      },
      {
        "date": "2024-01-16T04:16:00.000Z",
        "voteCount": 1,
        "content": "auditors group"
      },
      {
        "date": "2023-12-24T01:17:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/iam/docs/job-functions/auditing?hl=ja#scenario_external_auditors"
      },
      {
        "date": "2023-11-22T05:21:00.000Z",
        "voteCount": 2,
        "content": "A\nCreate a group with the auditors, grant 'logging.viewer' and 'bigQuery.dataViewer roles to the group on a table / view with the required data."
      },
      {
        "date": "2023-11-01T02:23:00.000Z",
        "voteCount": 2,
        "content": "AS per Google best practices the roles should be assigned to a group &amp; not to individual users"
      },
      {
        "date": "2023-10-17T11:52:00.000Z",
        "voteCount": 4,
        "content": "A is correct. 1st you should know this is a exam. Google recommend xxx means you should choose group first."
      },
      {
        "date": "2023-09-27T12:23:00.000Z",
        "voteCount": 5,
        "content": "The correct answer is A. \n\nThis option follows Google-recommended practices, because it allows you to grant auditors access to view audit logs without granting them access to other resources in your project.\n\nThe other options are not as good:\n\n* Option B is not as good, because it requires you to create two new custom IAM roles. This can be complex and time-consuming.\n* Option C is not as good, because it grants auditors access to all audit logs in your project, including audit logs for resources that they do not need access to.\n* Option D is not as good, because it grants auditors access to all data in your BigQuery datasets, including data that they do not need access to."
      },
      {
        "date": "2023-09-01T09:55:00.000Z",
        "voteCount": 3,
        "content": "Google Recommended Practice A is the correct Answer add the users in the group then grant them the access"
      },
      {
        "date": "2023-08-09T08:20:00.000Z",
        "voteCount": 1,
        "content": "C\nOption A, which suggests adding the auditors group to predefined roles, might not be as appropriate as using individual auditor user accounts. It's generally a best practice to assign permissions to specific users rather than groups, as it provides better granularity and control over access."
      },
      {
        "date": "2023-07-20T05:02:00.000Z",
        "voteCount": 2,
        "content": "Correct answer is A"
      },
      {
        "date": "2023-07-15T15:43:00.000Z",
        "voteCount": 1,
        "content": "There is no group created already, so  C is the right answer."
      },
      {
        "date": "2023-03-18T20:25:00.000Z",
        "voteCount": 5,
        "content": "Correct Answer is B\nBy creating a custom IAM role, you can specify the exact permissions that the auditors need, and avoid granting them unnecessary permissions that come with predefined IAM roles. In this case, you can create two custom IAM roles: one for 'logging.viewer' and one for 'bigQuery.dataViewer', and grant the corresponding permissions to each role. Then, you can add the auditors group to these custom roles to give them access to the required logs and data."
      },
      {
        "date": "2023-03-18T20:27:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer is B\nOption A is incorrect because the logging.viewer and bigQuery.dataViewer roles only grant read access to logs and data in BigQuery, respectively. These roles do not provide audit logging capabilities.\n\nOption C is incorrect because it suggests adding individual user accounts to the roles, whereas the question specifically asks for adding an auditors group. In addition, adding individual user accounts can be difficult to manage and does not scale well as the number of auditors increases. It is generally recommended to use groups for managing access whenever possible. \nOption D suggests adding the auditor user accounts to two new custom IAM roles, which could work. However, the question specifically asks for following Google-recommended practices. The recommended practice is to use predefined roles over custom roles whenever possible. Therefore, option B, which suggests adding the auditors group to two new custom IAM roles, is not recommended."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 26,
    "url": "https://www.examtopics.com/discussions/google/view/16698-exam-associate-cloud-engineer-topic-1-question-26-discussion/",
    "body": "You need to set up permissions for a set of Compute Engine instances to enable them to write data into a particular Cloud Storage bucket. You want to follow<br>Google-recommended practices. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a service account with an access scope. Use the access scope 'https://www.googleapis.com/auth/devstorage.write_only'.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a service account with an access scope. Use the access scope 'https://www.googleapis.com/auth/cloud-platform'.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a service account and add it to the IAM role 'storage.objectCreator' for that bucket.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a service account and add it to the IAM role 'storage.objectAdmin' for that bucket."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 41,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-03-15T14:11:00.000Z",
        "voteCount": 58,
        "content": "As per as the least privileage recommended by google, C is the correct Option, A is incorrect because the scope doesnt exist. B incorrect because it will give him full of control"
      },
      {
        "date": "2022-06-26T23:20:00.000Z",
        "voteCount": 1,
        "content": "Check here, it is A-&gt; https://cloud.google.com/storage/docs/authentication\nhttps://cloud.google.com/storage/docs/authentication"
      },
      {
        "date": "2022-12-26T09:17:00.000Z",
        "voteCount": 1,
        "content": "In the Document, it includes https://www.googleapis.com/auth/devstorage.read_write scope"
      },
      {
        "date": "2023-05-28T22:17:00.000Z",
        "voteCount": 1,
        "content": "There is no scope called write-only, as per the reference document."
      },
      {
        "date": "2023-06-14T07:40:00.000Z",
        "voteCount": 2,
        "content": "In the Document, 'write -only' does not exist. Just read-only"
      },
      {
        "date": "2020-12-02T11:32:00.000Z",
        "voteCount": 2,
        "content": "The scope does exist - https://download.huihoo.com/google/gdgdevkit/DVD1/developers.google.com/compute/docs/api/how-tos/authorization.html"
      },
      {
        "date": "2022-04-04T11:10:00.000Z",
        "voteCount": 2,
        "content": "it doesn't exist. show us this on official google website"
      },
      {
        "date": "2021-09-16T05:22:00.000Z",
        "voteCount": 4,
        "content": "No it doesn't. You have read-only, read-write, full-control and others... but \"write-only\" is not a thing.\n\nhttps://cloud.google.com/storage/docs/authentication"
      },
      {
        "date": "2020-07-07T03:17:00.000Z",
        "voteCount": 18,
        "content": "In reviewing this, it looks to be a multiple answer question.  According to Best Practices in this Google Doc (https://cloud.google.com/compute/docs/access/create-enable-service-accounts-for-instances#best_practices) you grant the instance the scope and the permissions are determined by the IAM roles of the service account.  In this case, you would grant the instance the scope and the role (storage.objectCreator) to the service account.\n\nAns B and C\n\nRole from GCP Console:\nID = roles/storage.objectCreator\nRole launch stage = General Availability\nDescription = Access to create objects in GCS.\n\n3 assigned permissions\nresourcemanager.projects.get\nresourcemanager.projects.list\nstorage.objects.create"
      },
      {
        "date": "2022-08-01T23:01:00.000Z",
        "voteCount": 1,
        "content": "There are many access scopes available to choose from, but a best practice is to set the cloud-platform access scope, which is an OAuth scope for most Google Cloud services, and then control the service account's access by granting it IAM roles..you have an app that reads and writes files on Cloud Storage, it must first authenticate to the Cloud Storage API. You can create an instance with the cloud-platform scope and attach a service account to the instance\nhttps://cloud.google.com/compute/docs/access/service-accounts"
      },
      {
        "date": "2022-08-02T01:38:00.000Z",
        "voteCount": 2,
        "content": "Reading the second point of the best practice. You should grant your VM the https://www.googleapis.com/auth/cloud-platform scope to allow access to most of Google Cloud APIs.\n\nSo, that the IAM permissions are completely determined by the IAM roles you granted to the service account.\n\nThe conclusion is you should not mess up with the VM scopes to grant access to Google Services, instead you should grant the access via IAM roles of the service account you attached to the VM.\n\nhttps://cloud.google.com/compute/docs/access/create-enable-service-accounts-for-instances#best_practices"
      },
      {
        "date": "2024-09-22T10:46:00.000Z",
        "voteCount": 1,
        "content": "C because of  'storage.objectCreator'"
      },
      {
        "date": "2024-06-10T06:15:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is C"
      },
      {
        "date": "2023-11-05T15:31:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is D"
      },
      {
        "date": "2023-11-01T02:25:00.000Z",
        "voteCount": 1,
        "content": "storage.objectCreator contains sufficient privileges to do the job &amp; so admin is not required"
      },
      {
        "date": "2023-09-28T07:44:00.000Z",
        "voteCount": 4,
        "content": "The correct answer is C. \n\nThe other options are not accurate and go against the principle of giving least required access. \n\nA is incorrect as there is no role as write_only\nB is not a good option as it gives full control of google cloud services where as we are looking for write data into a particular cloud storage bucket\nD. is not a good option as it gives full control over objects\n\nSources:\nhttps://cloud.google.com/storage/docs/authentication\nhttps://cloud.google.com/storage/docs/access-control/iam-roles"
      },
      {
        "date": "2023-09-01T09:57:00.000Z",
        "voteCount": 1,
        "content": "c seems more correct, you need to go iam to provide the permissions , b and d will give it more or full access"
      },
      {
        "date": "2023-07-20T21:32:00.000Z",
        "voteCount": 2,
        "content": "Associate Cloud Engineer exam booked very soon. kindly share the all the questions and any other support exam to clear this"
      },
      {
        "date": "2023-08-03T03:01:00.000Z",
        "voteCount": 1,
        "content": "Hi Neha, Please let me know how your exam was? I am taking the exam soon. Thanks"
      },
      {
        "date": "2023-07-20T05:04:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2023-07-19T08:25:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2023-05-28T22:15:00.000Z",
        "voteCount": 3,
        "content": "C is correct answer"
      },
      {
        "date": "2023-05-28T09:56:00.000Z",
        "voteCount": 1,
        "content": "The ask is how the \u201cCompute Engine instances to enable them to write data into a particular Cloud Storage bucket\u201d. A&nbsp;service account&nbsp;is a special kind of account used by an application or compute workload, rather than a person. When you set up an instance to run as a service account, you determine the level of access the service account has by the IAM roles that you&nbsp;grant to the service account. If the service account has no IAM roles, then no resources can be accessed using the service account on that instance.\nThe best Practice suggested by Google is refer in this link: https://cloud.google.com/compute/docs/access/service-accounts#scopes_best_practice \u2028https://cloud.google.com/storage/docs/access-control/iam-roles shows that storage.objectCreator is best choice of the role for this problem statement."
      },
      {
        "date": "2023-04-25T13:59:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is C. \n\nThere is no role as write only its read only hence A is incorrect."
      },
      {
        "date": "2023-04-03T21:56:00.000Z",
        "voteCount": 1,
        "content": "IAM Work on Principal of least privilege,"
      },
      {
        "date": "2023-03-18T20:31:00.000Z",
        "voteCount": 2,
        "content": "Option C is the correct answer. To grant a set of Compute Engine instances permissions to write data to a particular Cloud Storage bucket, you should create a service account and add it to the IAM role 'storage.objectCreator' for that bucket. This IAM role allows the service account to create new objects in the bucket, but it does not allow it to modify or delete existing objects. Option A is incorrect because the access scope 'https://www.googleapis.com/auth/devstorage.write_only' does not exist. Option B is incorrect because the access scope 'https://www.googleapis.com/auth/cloud-platform' grants permissions for all Google Cloud Platform services, which is overly broad and not recommended. Option D is incorrect because the IAM role 'storage.objectAdmin' provides full control over the bucket, which is more access than necessary to allow the Compute Engine instances to write data to the bucket."
      },
      {
        "date": "2023-03-18T01:27:00.000Z",
        "voteCount": 1,
        "content": "According to least priviliges, the correct answer is C"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 27,
    "url": "https://www.examtopics.com/discussions/google/view/16701-exam-associate-cloud-engineer-topic-1-question-27-discussion/",
    "body": "You have sensitive data stored in three Cloud Storage buckets and have enabled data access logging. You want to verify activities for a particular user for these buckets, using the fewest possible steps. You need to verify the addition of metadata labels and which files have been viewed from those buckets. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUsing the GCP Console, filter the Activity log to view the information.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUsing the GCP Console, filter the Stackdriver log to view the information.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tView the bucket in the Storage section of the GCP Console.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a trace in Stackdriver to view the information."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 34,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 34,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-05-23T23:30:00.000Z",
        "voteCount": 50,
        "content": "A is correct. As mentioned in the question, data access logging is enabled. I tried to download a file from a bucket and was able to view this information in Activity tab in console"
      },
      {
        "date": "2021-12-19T06:23:00.000Z",
        "voteCount": 1,
        "content": "I did all the configuration enabling data access logging but I still not able to see the logs when uploading or downloading a file. Does someone here has done it with a different result?"
      },
      {
        "date": "2022-08-02T02:15:00.000Z",
        "voteCount": 1,
        "content": "I agree with liyux21 and vito9630. In this reference link below says:\n\nIn the Activity page, where the identity performing logged actions is redacted from the audit log entry, User (anonymized) is displayed.\n\nBeacause of this, I think you can't verify the addition of metadata labels through Activity Logs.\n\nhttps://cloud.google.com/logging/docs/audit#view-activity"
      },
      {
        "date": "2022-10-06T11:11:00.000Z",
        "voteCount": 3,
        "content": "activity log is deprecated:\n\nhttps://cloud.google.com/compute/docs/logging/activity-logs"
      },
      {
        "date": "2022-12-07T06:06:00.000Z",
        "voteCount": 1,
        "content": "You need to see here, https://cloud.google.com/compute/docs/logging/audit-logging. Admin activity audit logs."
      },
      {
        "date": "2023-02-27T19:11:00.000Z",
        "voteCount": 2,
        "content": "Yes, it is deprecated. However, it became the audit log which is exactly what this question is referring to. Option A is correct in my opinion."
      },
      {
        "date": "2020-05-28T05:24:00.000Z",
        "voteCount": 24,
        "content": "data access logging don't provide information about addition of metada, so B is correct"
      },
      {
        "date": "2020-12-01T20:59:00.000Z",
        "voteCount": 15,
        "content": "Answer is A. Activity log does indeed show information about metadata.\nI agree with Eshkrkrkr based on https://cloud.google.com/storage/docs/audit-logs Admin Activity logs: Entries for operations that modify the configuration or metadata of a project, bucket, or object."
      },
      {
        "date": "2022-03-02T11:48:00.000Z",
        "voteCount": 2,
        "content": "'Admin activity logs' capture metadata modification, but its different from 'Data Access logging', right ?"
      },
      {
        "date": "2024-10-17T11:44:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is A. Using the GCP Console, filter the Activity log to view the information.\n\nData access logs for Cloud Storage buckets are not stored in Stackdriver Logging (formerly Stackdriver). They are stored in the Activity Log, a centralized log for all GCP activity. The Activity Log allows filtering by resource (the specific buckets), user, and activity type (adding metadata labels, viewing files). Options B, C, and D won't show this detailed access information. Therefore, directly querying the Activity Log provides the most efficient and accurate way to find the required information."
      },
      {
        "date": "2024-10-16T01:37:00.000Z",
        "voteCount": 1,
        "content": "Cloud Audit Logs (Activity Logs):These logs track who did what, where, and when within Google Cloud resources, which is crucial for security and compliance."
      },
      {
        "date": "2024-10-06T08:17:00.000Z",
        "voteCount": 1,
        "content": "B is incorrect because data access logs can be viewed from Activity logs and not Stackdriver logs. Filtering the logs in Stackdriver logging may provide some information, but it may not be as specific and targeted as viewing the Audit log in the GCP console. Stackdriver logging is a more general log management and analysis tool, whereas the Audit log is specifically designed for tracking and monitoring activities in Google Cloud services."
      },
      {
        "date": "2024-10-01T23:51:00.000Z",
        "voteCount": 1,
        "content": "Apparently Stackdriver logging is depreciated and new is Cloud logging as per below answer I got from ChatGPI : Filter the logs in Cloud Logging (formerly Stackdriver Logging) for that specific user and the bucket resources."
      },
      {
        "date": "2024-09-23T05:31:00.000Z",
        "voteCount": 1,
        "content": "Using the GCP Console, filter the Activity log\nto view the information. This is because Google Cloud Platform\u2019s Activity log is\nspecifically designed to record and store actions performed by users, such as the\naddition of metadata labels and file access in Cloud Storage buckets. Filtering\nthe Activity log by the specific user will help you quickly verify the actions they\nhave taken, fulfilling the requirement with the fewest possible steps."
      },
      {
        "date": "2024-09-18T22:51:00.000Z",
        "voteCount": 1,
        "content": "Los registros de acceso a los datos para Cloud Storage se almacenan en Cloud Logging (anteriormente Stackdriver Logging). Para verificar actividades espec\u00edficas, como la adici\u00f3n de etiquetas de metadatos y el acceso a archivos en dep\u00f3sitos, debes filtrar los registros en Cloud Logging. Esto te permitir\u00e1 ver las acciones realizadas por un usuario en particular de forma detallada.\n\nA es incorrecta, ya que la consola de actividad solo muestra actividades administrativas y no incluye los registros de acceso a los datos.\nC no te permite ver un historial de actividades o registros de acceso.\nD es innecesario, ya que puedes acceder directamente a los registros en Cloud Logging sin necesidad de crear un seguimiento adicional."
      },
      {
        "date": "2024-09-10T15:07:00.000Z",
        "voteCount": 1,
        "content": "B. Using the GCP Console, filter the Stackdriver log to view the information.\n\nHere's why:\n\nData Access Logging: Data access logging captures information about actions performed on Cloud Storage buckets, including metadata changes and file views.\nStackdriver Logging: This service collects and stores logs from various Google Cloud services, including Cloud Storage data access logs.\nFiltering: You can use Stackdriver Logging's filtering capabilities to narrow down the logs to the specific user and activities you're interested in (adding metadata labels and viewing files).\nThis approach provides a direct and efficient way to verify the desired information without requiring additional steps or tools."
      },
      {
        "date": "2024-08-28T11:57:00.000Z",
        "voteCount": 1,
        "content": "Stackdriver now known as Cloud Logging captures detailed logs for various GCP services, including Cloud Storage. Since you've enabled data access logging, the logs should contain records of actions such as viewing files, modifying metadata, and other access activities. This method is more comprehensive than the Activity log (which primarily tracks high-level operations) and gives you the ability to drill down into specific events, which is necessary for sensitive data verification."
      },
      {
        "date": "2024-07-20T13:16:00.000Z",
        "voteCount": 1,
        "content": "Addition of metadata labels."
      },
      {
        "date": "2024-07-12T12:46:00.000Z",
        "voteCount": 2,
        "content": "Activity Log\nUsage: Useful for auditing and tracking changes made by users or service accounts to the GCP resources.\nStackdriver Log\nUsage: Useful for monitoring the behavior of applications, diagnosing issues, and understanding the operational state of your system"
      },
      {
        "date": "2024-05-29T17:16:00.000Z",
        "voteCount": 2,
        "content": "Answer is B. Explanation: Activity Logs are essential for tracking administrative and system actions for security and compliance, while Cloud Logging (formerly known as Stackdriver Logging) offers a comprehensive solution for collecting, analyzing, and monitoring a wide range of logs from various sources. Also, the addition of metadata labels can be verified in Cloud Logging only."
      },
      {
        "date": "2024-05-15T06:17:00.000Z",
        "voteCount": 1,
        "content": "B is the correct answer"
      },
      {
        "date": "2024-05-05T00:15:00.000Z",
        "voteCount": 1,
        "content": "After asking Microsoft Copilot, I got B as a correct answer, but here is what Google Gemini replied:\n\n\"\nThe best option to verify activities for a particular user with the fewest steps is to:\n* A. Using the GCP Console, filter the Activity log to view the information.\n\nHere's why the other options are less suitable:\n\n* B. Stackdriver Logs: While Stackdriver Logs can be used for broader GCP activity logging, Cloud Storage access logs are specifically captured in the Activity log.  \n* C. Viewing the Bucket: This option only shows the current state of the bucket and doesn't provide historical activity information.\n* D. Stackdriver Trace:  Creating a trace is for monitoring service calls and dependencies, not user activity on Cloud Storage buckets.\n\nBy filtering the Activity log for the specific user and the relevant Cloud Storage buckets, you can efficiently see actions like adding metadata labels and viewing files.\n\""
      },
      {
        "date": "2024-04-03T07:56:00.000Z",
        "voteCount": 1,
        "content": "The correct option to verify activities for a particular user, including the addition of metadata labels and viewed files from Cloud Storage buckets, with the fewest possible steps is:\n\nB. Using the GCP Console, filter the Stackdriver log to view the information.\n\nExplanation:\n\nStackdriver logging captures logs for various GCP services, including Cloud Storage. By filtering the Stackdriver logs, you can easily view activities such as metadata changes and file accesses for the specified user across multiple buckets. This method allows you to access the relevant information in a centralized manner, minimizing the number of steps required to gather the required insights."
      },
      {
        "date": "2024-01-07T01:26:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/logging/docs/audit#view-logs"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 28,
    "url": "https://www.examtopics.com/discussions/google/view/18517-exam-associate-cloud-engineer-topic-1-question-28-discussion/",
    "body": "You are the project owner of a GCP project and want to delegate control to colleagues to manage buckets and files in Cloud Storage. You want to follow Google- recommended practices. Which IAM roles should you grant your colleagues?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tProject Editor",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStorage Admin\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStorage Object Admin",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStorage Object Creator"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 24,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-08-22T19:31:00.000Z",
        "voteCount": 53,
        "content": "Correct Answer is (B):\n\nStorage Admin (roles/storage.admin)\tGrants full control of buckets and objects.\nWhen applied to an individual bucket, control applies only to the specified bucket and objects within the bucket.\n\nfirebase.projects.get\nresourcemanager.projects.get\nresourcemanager.projects.list\nstorage.buckets.*\nstorage.objects.*"
      },
      {
        "date": "2021-10-12T10:15:00.000Z",
        "voteCount": 3,
        "content": "why not storage object admin?"
      },
      {
        "date": "2021-11-01T15:16:00.000Z",
        "voteCount": 25,
        "content": "Because the objet admin don't have control over buckets and you need it"
      },
      {
        "date": "2022-01-11T11:03:00.000Z",
        "voteCount": 5,
        "content": "Exactly, you want to give someone right to edit storages not just objects. Google does this kind of answers to confuse us."
      },
      {
        "date": "2022-02-23T12:52:00.000Z",
        "voteCount": 3,
        "content": "Question states \"Buckets and Objects\""
      },
      {
        "date": "2020-04-15T14:03:00.000Z",
        "voteCount": 14,
        "content": "B is correct"
      },
      {
        "date": "2023-09-28T09:18:00.000Z",
        "voteCount": 6,
        "content": "The correct answer is B\n\nThis role allows users to create, manage, and delete buckets and files in Cloud Storage. It also allows users to set permissions on buckets and files.\n\nThe other options are not as good:\nA gives users too much power, as it allows them to manage all resources in a project, including Cloud Storage buckets and files\nC gives users too much power, as it allows them to manage all objects in a bucket, including the permissions on those objects\nD does not give users enough power, as it does not allow them to manage buckets or set permissions on buckets and objects\n\nSteps to grant Storage Admin IAM role:\n1 Go to the Google Cloud Console\n2 Click on the IAM &amp; Admin menu\n3 Click on the Roles tab\n4 Click on the Storage Admin role\n5 Click on the Add members button\n6 Type the email addresses of your colleagues in the Members field\n7 Click on the Add button"
      },
      {
        "date": "2023-09-01T10:00:00.000Z",
        "voteCount": 1,
        "content": "B is more correct, as it give you the both access"
      },
      {
        "date": "2023-08-29T01:53:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2023-07-24T00:24:00.000Z",
        "voteCount": 1,
        "content": "B.\nStorage Admin (roles/storage.admin) - Grants full control of buckets and objects.\nhttps://cloud.google.com/storage/docs/access-control/iam-roles"
      },
      {
        "date": "2023-03-22T05:50:00.000Z",
        "voteCount": 1,
        "content": "Correct option B"
      },
      {
        "date": "2023-02-18T21:54:00.000Z",
        "voteCount": 4,
        "content": "Answer B, \"Storage Admin,\" is the correct answer because it grants permissions to manage Cloud Storage resources at the project level, including creating and deleting buckets, changing bucket settings, and assigning permissions to buckets and their contents. This role also includes the permissions of the \"Storage Object Admin\" and \"Storage Object Creator\" roles, which allow managing objects and uploading new ones.\n\nAnswer A, \"Project Editor,\" is a higher-level role that includes permissions to manage not only Cloud Storage but also other GCP services in the project. Granting this role may not be appropriate if the colleagues only need to manage Cloud Storage resources.\n\nAnswers C and D may not be sufficient if the colleagues need to create or delete buckets or change their settings."
      },
      {
        "date": "2022-12-06T10:32:00.000Z",
        "voteCount": 2,
        "content": "Storage Admin (roles/storage.admin)\t\nGrants full control of buckets and objects.\nWhen applied to an individual bucket, control applies only to the specified bucket and objects within the bucket.\n\nWhile\nStorage Object Admin (roles/storage.objectAdmin)\t\nGrants full control over objects, including listing, creating, viewing, and deleting objects."
      },
      {
        "date": "2022-10-23T05:31:00.000Z",
        "voteCount": 1,
        "content": "B. Storage Admin"
      },
      {
        "date": "2022-10-18T19:39:00.000Z",
        "voteCount": 1,
        "content": "According to the question, your colleagues need to manage \"buckets\" in Cloud Storage(storage.objects.* permission), so (B) is correct.\n(C) doesn't have control over the buckets. \nCloud document:\nhttps://cloud.google.com/storage/docs/access-control/iam-roles#standard-roles\n---&gt;Storage Object Admin (Cannot find \"storage.buckets.*\" Permission)\n----&gt;Storage Admin ( Has \"storage.buckets.*\" Permission)"
      },
      {
        "date": "2022-09-19T05:35:00.000Z",
        "voteCount": 1,
        "content": "the ans is definitively B"
      },
      {
        "date": "2022-09-01T11:23:00.000Z",
        "voteCount": 1,
        "content": "Storage Admin option A - As we are supposed to create buckets as well"
      },
      {
        "date": "2022-07-02T00:03:00.000Z",
        "voteCount": 1,
        "content": "Answer is B"
      },
      {
        "date": "2022-06-22T14:28:00.000Z",
        "voteCount": 1,
        "content": "Storage Admin  is right.. B is correct"
      },
      {
        "date": "2022-05-23T01:36:00.000Z",
        "voteCount": 1,
        "content": "Go for B"
      },
      {
        "date": "2022-05-23T01:37:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/storage/docs/access-control/iam-roles"
      },
      {
        "date": "2022-04-04T11:13:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/storage/docs/access-control/iam-roles"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 29,
    "url": "https://www.examtopics.com/discussions/google/view/18518-exam-associate-cloud-engineer-topic-1-question-29-discussion/",
    "body": "You have an object in a Cloud Storage bucket that you want to share with an external company. The object contains sensitive data. You want access to the content to be removed after four hours. The external company does not have a Google account to which you can grant specific user-based access privileges. You want to use the most secure method that requires the fewest steps. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a signed URL with a four-hour expiration and share the URL with the company.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet object access to 'public' and use object lifecycle management to remove the object after four hours.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the storage bucket as a static website and furnish the object's URL to the company. Delete the object from the storage bucket after four hours.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new Cloud Storage bucket specifically for the external company to access. Copy the object to that bucket. Delete the bucket after four hours have passed."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 20,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-10-05T17:31:00.000Z",
        "voteCount": 41,
        "content": "A.\nSigned URLs are used to give time-limited resource access to anyone in possession of the URL, regardless of whether they have a Google account.\nhttps://cloud.google.com/storage/docs/access-control/signed-urls"
      },
      {
        "date": "2020-04-15T14:04:00.000Z",
        "voteCount": 9,
        "content": "A is correct"
      },
      {
        "date": "2024-04-27T23:40:00.000Z",
        "voteCount": 1,
        "content": "A is correct option"
      },
      {
        "date": "2023-12-18T06:14:00.000Z",
        "voteCount": 1,
        "content": "Option A"
      },
      {
        "date": "2023-11-05T10:13:00.000Z",
        "voteCount": 1,
        "content": "signed URL for timed access"
      },
      {
        "date": "2023-09-28T09:41:00.000Z",
        "voteCount": 6,
        "content": "The correct answer is A\n\nTo do this, follow these steps:\n\n1 Go to the Google Cloud Console\n2 Click on the Storage menu\n3 Click on the Browser tab\n4 Click on the name of the bucket containing the object that you want to share\n5 Click on the name of the object that you want to share\n6 Click on the Create signed URL button\n7 In the Expires field, enter 4\n8 Click on the Create button\n\nThe company will be able to access the object using the signed URL for four hours. After four hours, the signed URL will expire and the company will no longer be able to access the object.\n\nThe other options are not as secure or efficient:\nB not as secure because it makes the object accessible to anyone who has the URL of the object\nC requires you to configure the storage bucket as a static website and to delete the object after four hours\nD requires you to create a new Cloud Storage bucket and to copy the object to that bucket\n\nhttps://cloud.google.com/storage/docs/access-control/signed-urls"
      },
      {
        "date": "2023-09-01T10:01:00.000Z",
        "voteCount": 1,
        "content": "A seems more legit"
      },
      {
        "date": "2023-07-24T02:14:00.000Z",
        "voteCount": 1,
        "content": "A. This page provides an overview of signed URLs, which give time-limited access to a specific Cloud Storage resource. Anyone in possession of the signed URL can use it while it's active, regardless of whether they have a Google account"
      },
      {
        "date": "2023-07-20T05:09:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2023-02-18T22:01:00.000Z",
        "voteCount": 3,
        "content": "Answer A is correct. Creating a signed URL with a short expiration time is a secure way to share objects in a Cloud Storage bucket with an external party, especially when the external company does not have a Google account.\n\nAnswer B is incorrect because setting object access to 'public' could potentially expose sensitive data to anyone who knows the object URL.\n\nAnswer C is incorrect because configuring the storage bucket as a static website would make the entire bucket contents available publicly, which is not secure.\n\nAnswer D is incorrect because creating a new Cloud Storage bucket for the external company to access, and then deleting the bucket after four hours, is a cumbersome and unnecessary approach. It also adds additional complexity to your environment."
      },
      {
        "date": "2023-02-04T02:23:00.000Z",
        "voteCount": 1,
        "content": "A minimal... B C D too much steps"
      },
      {
        "date": "2022-11-30T05:30:00.000Z",
        "voteCount": 1,
        "content": "A. Signed URL for a certain period of time."
      },
      {
        "date": "2022-10-23T07:36:00.000Z",
        "voteCount": 1,
        "content": "A. signed URL"
      },
      {
        "date": "2022-10-18T19:48:00.000Z",
        "voteCount": 1,
        "content": "Go for (A)\nAnyone in possession of the signed URL can use it while it's active, regardless of whether they have a Google account. It gives time-limited access to a specific Cloud Storage resource.\n\nReference:\nhttps://cloud.google.com/storage/docs/access-control/signed-urls"
      },
      {
        "date": "2022-10-17T04:41:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2022-07-02T00:04:00.000Z",
        "voteCount": 1,
        "content": "Answer is A"
      },
      {
        "date": "2022-06-30T04:39:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 30,
    "url": "https://www.examtopics.com/discussions/google/view/18519-exam-associate-cloud-engineer-topic-1-question-30-discussion/",
    "body": "You are creating a Google Kubernetes Engine (GKE) cluster with a cluster autoscaler feature enabled. You need to make sure that each node of the cluster will run a monitoring pod that sends container metrics to a third-party monitoring solution. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the monitoring pod in a StatefulSet object.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the monitoring pod in a DaemonSet object.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReference the monitoring pod in a Deployment object.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReference the monitoring pod in a cluster initializer at the GKE cluster creation time."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 34,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-02-26T20:17:00.000Z",
        "voteCount": 45,
        "content": "B is right: https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/\nSome typical uses of a DaemonSet are:\n\nrunning a cluster storage daemon on every node\nrunning a logs collection daemon on every node\nrunning a node monitoring daemon on every node"
      },
      {
        "date": "2020-04-15T14:04:00.000Z",
        "voteCount": 25,
        "content": "B is correct"
      },
      {
        "date": "2024-08-27T04:31:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is B"
      },
      {
        "date": "2023-12-18T06:22:00.000Z",
        "voteCount": 1,
        "content": "Option B"
      },
      {
        "date": "2023-09-28T09:58:00.000Z",
        "voteCount": 5,
        "content": "The correct answer is B\n\nDaemonSets ensure that one running instance of the pod is scheduled on every node in the cluster. This means that even if the cluster autoscaler scales the cluster up or down, the monitoring pod will continue to run on each node.\n\nThe other options are not as good:\nA This is not necessary for the monitoring pod, and it can make it more difficult to scale the cluster\nC This is not necessary for the monitoring pod, and it can make it more difficult to scale the cluster\nD This is not the best way to deploy the monitoring pod, because it makes it difficult to update the monitoring pod and it does not ensure that the monitoring pod is running on every node in the cluster.\n\nhttps://kubernetes.io/docs/concepts/workloads/controllers/daemonset/"
      },
      {
        "date": "2023-09-01T10:02:00.000Z",
        "voteCount": 1,
        "content": "B seems more correcnt"
      },
      {
        "date": "2023-03-15T07:28:00.000Z",
        "voteCount": 4,
        "content": "daemonset- makes sure that a fixed number of pods is running in every moment in every node of the cluster"
      },
      {
        "date": "2023-02-18T22:06:00.000Z",
        "voteCount": 13,
        "content": "Answer B, Deploy the monitoring pod in a DaemonSet object, is the correct answer.\n\nA DaemonSet ensures that all (or some) nodes in a cluster run a copy of a specific Pod. By deploying the monitoring pod in a DaemonSet object, a copy of the pod will run on each node of the cluster. This ensures that the metrics for all containers running on each node will be sent to the third-party monitoring solution.\n\nA. StatefulSet is used for stateful applications that require unique network identifiers, stable storage, and ordered deployment and scaling.\n\nC. A Deployment object is used to manage a set of replica Pods in a declarative way.\n\nD. Cluster initializers are deprecated and no longer recommended for use in Kubernetes."
      },
      {
        "date": "2022-10-23T07:36:00.000Z",
        "voteCount": 1,
        "content": "B  DaemonSet"
      },
      {
        "date": "2022-06-22T14:26:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2022-05-29T04:53:00.000Z",
        "voteCount": 7,
        "content": "DaemonSets attempt to adhere to a one-Pod-per-node model, either across the entire cluster or a subset of nodes. As you add nodes to a node pool, DaemonSets automatically add Pods to the new nodes as needed."
      },
      {
        "date": "2022-05-24T19:30:00.000Z",
        "voteCount": 1,
        "content": "go for B"
      },
      {
        "date": "2022-05-23T01:49:00.000Z",
        "voteCount": 1,
        "content": "Go for B"
      },
      {
        "date": "2022-05-05T10:40:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2022-03-01T00:48:00.000Z",
        "voteCount": 2,
        "content": "\"Every Node\" is the keyword here, which is what DaemonSet is used for"
      },
      {
        "date": "2022-01-26T18:02:00.000Z",
        "voteCount": 1,
        "content": "B 2000%"
      },
      {
        "date": "2021-12-01T07:54:00.000Z",
        "voteCount": 3,
        "content": "I will vote B"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 31,
    "url": "https://www.examtopics.com/discussions/google/view/21111-exam-associate-cloud-engineer-topic-1-question-31-discussion/",
    "body": "You want to send and consume Cloud Pub/Sub messages from your App Engine application. The Cloud Pub/Sub API is currently disabled. You will use a service account to authenticate your application to the API. You want to make sure your application can use Cloud Pub/Sub. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable the Cloud Pub/Sub API in the API Library on the GCP Console.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRely on the automatic enablement of the Cloud Pub/Sub API when the Service Account accesses it.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Deployment Manager to deploy your application. Rely on the automatic enablement of all APIs used by the application being deployed.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGrant the App Engine Default service account the role of Cloud Pub/Sub Admin. Have your application enable the API on the first connection to Cloud Pub/ Sub."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 17,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-08-22T19:59:00.000Z",
        "voteCount": 32,
        "content": "Correct Answer is (A)\n\nQuickstart: using the Google Cloud Console\nThis page shows you how to perform basic tasks in Pub/Sub using the Google Cloud Console.\n\nNote: If you are new to Pub/Sub, we recommend that you start with the interactive tutorial.\nBefore you begin\nSet up a Cloud Console project.\nSet up a project\n\nClick to:\n\nCreate or select a project.\nEnable the Pub/Sub API for that project.\nYou can view and manage these resources at any time in the Cloud Console.\n\nInstall and initialize the Cloud SDK.\nNote: You can run the gcloud tool in the Cloud Console without installing the Cloud SDK. To run the gcloud tool in the Cloud Console, use Cloud Shell .\n\nhttps://cloud.google.com/pubsub/docs/quickstart-console"
      },
      {
        "date": "2020-05-21T16:15:00.000Z",
        "voteCount": 24,
        "content": "We need to enable the pub/sub API, if we are going to use it in your project... then APP engine can able to access it with required ServiceAccount"
      },
      {
        "date": "2023-11-27T08:35:00.000Z",
        "voteCount": 1,
        "content": "hello test"
      },
      {
        "date": "2023-09-01T10:04:00.000Z",
        "voteCount": 1,
        "content": "Yes A, is more correct as first you need to enable the API itself"
      },
      {
        "date": "2023-05-29T05:02:00.000Z",
        "voteCount": 2,
        "content": "ANSWER SHOULD BE A."
      },
      {
        "date": "2023-02-18T22:11:00.000Z",
        "voteCount": 8,
        "content": "Answer A is correct. Enable the Cloud Pub/Sub API in the API Library on the GCP Console.\n\nSince the Cloud Pub/Sub API is currently disabled, the first step is to enable it. This can be done through the API Library on the GCP Console. Once the API is enabled, the service account can be used to authenticate the App Engine application to the Cloud Pub/Sub API.\n\nAnswer B is incorrect because there is no automatic enablement of APIs when a service account accesses them. The API needs to be enabled manually in the API Library or through the command-line interface.\n\nAnswer C is incorrect because enabling APIs through Deployment Manager requires that the APIs be enabled in the project before Deployment Manager can use them.\n\nAnswer D is incorrect because granting the App Engine Default service account the Cloud Pub/Sub Admin role could be a security risk, and it is not necessary to enable the API."
      },
      {
        "date": "2022-06-22T14:21:00.000Z",
        "voteCount": 1,
        "content": "A is right"
      },
      {
        "date": "2022-06-02T07:58:00.000Z",
        "voteCount": 3,
        "content": "Yup its A"
      },
      {
        "date": "2022-05-24T19:33:00.000Z",
        "voteCount": 1,
        "content": "go for A"
      },
      {
        "date": "2022-05-23T01:53:00.000Z",
        "voteCount": 1,
        "content": "Go for A"
      },
      {
        "date": "2022-05-17T12:28:00.000Z",
        "voteCount": 2,
        "content": "A is correct, although there are options to activate APIs programmatically (gcloud, curl) https://cloud.google.com/service-usage/docs/enable-disable\nB and C are incorrect because \"rely on ...\" is never a good option\nD could be possible, but to much permissions are given to the app"
      },
      {
        "date": "2022-04-04T11:29:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2022-03-20T11:30:00.000Z",
        "voteCount": 1,
        "content": "Enable the Cloud Pub/Sub API in the API Library on the GCP Console."
      },
      {
        "date": "2022-03-01T00:51:00.000Z",
        "voteCount": 2,
        "content": "This question seems to be asking \"Can you automatically enable the API or do you have to do it manually?\" and I think the answer is that there's no automatic enablement of the API."
      },
      {
        "date": "2021-11-19T01:04:00.000Z",
        "voteCount": 2,
        "content": "A. Enable the Cloud Pub/Sub API in the API Library on the GCP Console."
      },
      {
        "date": "2021-11-18T23:31:00.000Z",
        "voteCount": 1,
        "content": "A. Enable the Cloud Pub/Sub API in the API Library on the GCP Console."
      },
      {
        "date": "2021-11-18T15:28:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 32,
    "url": "https://www.examtopics.com/discussions/google/view/16702-exam-associate-cloud-engineer-topic-1-question-32-discussion/",
    "body": "You need to monitor resources that are distributed over different projects in Google Cloud Platform. You want to consolidate reporting under the same Stackdriver<br>Monitoring dashboard. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Shared VPC to connect all projects, and link Stackdriver to one of the projects.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFor each project, create a Stackdriver account. In each project, create a service account for that project and grant it the role of Stackdriver Account Editor in all other projects.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a single Stackdriver account, and link all projects to the same account.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a single Stackdriver account for one of the projects. In Stackdriver, create a Group and add the other project names as criteria for that Group."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 25,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 5,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-12-23T09:49:00.000Z",
        "voteCount": 98,
        "content": "First of all D is incorrect, Groups are used to define alerts on set of resources(such as VM instances, databases, and load balancers). FYI tried adding Two projects into a group it did not allowed me as the \"AND\"/\"OR\" criteria for the group failed with this combination of resources.\n\nC is correct because,\nWhen you intially click on Monitoring(Stackdriver Monitoring) it creates a workspac(a stackdriver account) linked to the ACTIVE(CURRENT) Project from which it was clicked. \n\nNow if you change the project and again click onto Monitoring it would create an another workspace(a stackdriver account) linked to the changed ACTIVE(CURRENT) Project, we don't want this as this would not consolidate our result into a single dashboard(workspace/stackdriver account). \n\nIf you have accidently created two diff workspaces merge them under Monitoring &gt; Settings &gt;  Merge Workspaces &gt; MERGE.\n\nIf we have only one workspace and two projects we can simply add other GCP Project under \nMonitoring &gt; Settings &gt; GCP Projects &gt; Add GCP Projects.\n\nIn both of these cases we did not create a GROUP, we just linked GCP Project to the workspace(stackdriver account)."
      },
      {
        "date": "2020-03-15T14:31:00.000Z",
        "voteCount": 35,
        "content": "C is correct not D"
      },
      {
        "date": "2024-10-17T12:01:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is **C. Configure a single Stackdriver account, and link all projects to the same account.**\n\nStackdriver Monitoring (now Google Cloud Monitoring) doesn't work on a per-project account basis.  There's a single Monitoring service that spans your entire Google Cloud organization.  All projects within the organization can automatically report metrics to this single Monitoring instance.  You don't need to create separate accounts or use complex workarounds like Shared VPC or inter-project service accounts.  Option C directly addresses this by leveraging the inherent design of Cloud Monitoring.\n\nOptions A, B, and D are incorrect because they introduce unnecessary complexity and don't utilize the built-in functionality of Google Cloud Monitoring for consolidated reporting across multiple projects.  They might even lead to permission issues and difficulties in maintaining a unified view of your resources."
      },
      {
        "date": "2024-09-10T15:15:00.000Z",
        "voteCount": 1,
        "content": "This method provides a centralized and flexible way to monitor resources across different projects, without requiring complex network configurations or additional accounts."
      },
      {
        "date": "2024-01-31T08:02:00.000Z",
        "voteCount": 2,
        "content": "D is incorrect because a stackdriver group can group resources but not projects. Creating a single Stackdriver account in one project and creating a Group with other project names as criteria does not automatically aggregate and monitor metrics from different projects in a single dashboard. While creating a Group can help organize and manage metrics, it does not provide a solution for linking and monitoring metrics from different projects. You would still need to separately configure and link each project to the Stackdriver account to monitor their respective metrics."
      },
      {
        "date": "2023-11-01T02:29:00.000Z",
        "voteCount": 2,
        "content": "C is more correct, just link them into one"
      },
      {
        "date": "2023-09-05T06:09:00.000Z",
        "voteCount": 1,
        "content": "Option C is the recommended approach because it allows you to configure a single Stackdriver account and link all your projects to this account. This way, you can centralise monitoring, create custom dashboards, set up alerts, and gain a unified view of your resources distributed across different projects in a more straightforward and consolidated manner. It provides a single point of access and management for monitoring across all projects, which is typically the desired outcome for multi-project environments."
      },
      {
        "date": "2023-09-01T10:07:00.000Z",
        "voteCount": 1,
        "content": "C is more correct, just link them into one"
      },
      {
        "date": "2023-08-24T13:01:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/monitoring/settings\nBest practices for scoping projects\nWe recommend that you use a new Google Cloud project or one without resources as the scoping project when you want to view metrics for multiple Google Cloud projects or AWS accounts.\n\nWhen a metrics scope contains monitored projects, to chart or monitor only those metrics stored in the scoping project, you must specify filters that exclude metrics from the monitored projects. The requirement to use filters increases the complexity of chart and alerting policy, and it increases the possibility of a configuration error. The recommendation ensures that these scoping projects don't generate metrics, so there are no metrics in the projects to chart or monitor."
      },
      {
        "date": "2023-07-22T07:18:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2023-03-19T07:37:00.000Z",
        "voteCount": 1,
        "content": "C is correct because you can monitor multiple GCP projects from a single stackdriver account"
      },
      {
        "date": "2023-03-02T04:13:00.000Z",
        "voteCount": 1,
        "content": "c should be correct"
      },
      {
        "date": "2023-02-18T22:45:00.000Z",
        "voteCount": 5,
        "content": "Answer C. Configure a single Stackdriver account and link all projects to the same account.\n\nBy linking all projects to the same Stackdriver account, you can consolidate monitoring and reporting for all resources across multiple projects. This can be done by following the instructions provided by Stackdriver on how to add projects to an account. \n\nAnswer A is not the best solution since Shared VPC does not consolidate the monitoring and reporting of resources distributed over different projects. \n\nAnswer B is not an efficient solution as it requires creating multiple Stackdriver accounts, which can lead to confusion and inefficiency.  \n\nAnswer D is not the best solution as it only groups the projects under one account, but it does not consolidate monitoring and reporting."
      },
      {
        "date": "2022-12-05T21:51:00.000Z",
        "voteCount": 1,
        "content": "Have anybody all questions and want top share? Thanks!"
      },
      {
        "date": "2022-11-05T16:26:00.000Z",
        "voteCount": 1,
        "content": "If anyone is the contributor please share the dumps on shah.sana@northeastern.edu"
      },
      {
        "date": "2022-10-23T07:42:00.000Z",
        "voteCount": 1,
        "content": "C. Configure a single Stackdriver account, and link all projects to the same account."
      },
      {
        "date": "2022-09-01T11:27:00.000Z",
        "voteCount": 1,
        "content": "Option C"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 33,
    "url": "https://www.examtopics.com/discussions/google/view/20724-exam-associate-cloud-engineer-topic-1-question-33-discussion/",
    "body": "You are deploying an application to a Compute Engine VM in a managed instance group. The application must be running at all times, but only a single instance of the VM should run per GCP project. How should you configure the instance group?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet autoscaling to On, set the minimum number of instances to 1, and then set the maximum number of instances to 1.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet autoscaling to Off, set the minimum number of instances to 1, and then set the maximum number of instances to 1.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet autoscaling to On, set the minimum number of instances to 1, and then set the maximum number of instances to 2.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet autoscaling to Off, set the minimum number of instances to 1, and then set the maximum number of instances to 2."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 22,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 19,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-07-07T05:24:00.000Z",
        "voteCount": 184,
        "content": "In my GCP console, I created a managed instance group for each answer.  For each answer I deleted the instance that was created as a simple test to prove or disprove each answer.  \n\nIn answer A, another instance was created after I deleted the instance  \nIn answer B, no other instance was created after I deleted the instance\nIn answer C, another instance was created after I deleted the instance\nIn answer D, no other instance was created after I deleted the instance\n\nMy observation is A is the correct Answer.\n\nA - Correct - It correctly solves the problem with only a single instance at one time\nB - Incorrect - Does not fit the requirement because AFTER the deletion of the instance, no other instance was created\nC - Incorrect - It creates another instance after the delete HOWEVER it 2 VM's could be created even if the target is exceeded\nD - Incorrect - Does not fit the requirement because AFTER the deletion of the instance, no other instance was created"
      },
      {
        "date": "2020-10-06T06:30:00.000Z",
        "voteCount": 2,
        "content": "thanks for confirming"
      },
      {
        "date": "2020-07-29T12:16:00.000Z",
        "voteCount": 3,
        "content": "Thanks for confirming this for us."
      },
      {
        "date": "2020-11-10T08:52:00.000Z",
        "voteCount": 1,
        "content": "Thank you very much for this detail testing and explaination."
      },
      {
        "date": "2020-11-07T11:48:00.000Z",
        "voteCount": 4,
        "content": "Wrong! https://cloud.google.com/compute/docs/instance-groups MIGs offer the following advantages:\n\n    High availability.\n        Keeping VM instances running. If a VM in the group stops, crashes, or is deleted by an action other than an instance group management command (for example, an intentional scale in), the MIG automatically recreates that VM in accordance with the original instance's specification (same VM name, same template) so that the VM can resume its work."
      },
      {
        "date": "2020-12-13T18:00:00.000Z",
        "voteCount": 1,
        "content": "Why are you talking about MIG when the options are about AutoScaling on or off?"
      },
      {
        "date": "2020-12-21T03:58:00.000Z",
        "voteCount": 4,
        "content": "Because the question states MIG: \"you are deploying an application to a Compute Engine VM in a managed instance group \"\n\nI am wondering if  XRiddlerX  got a MIG with the autohealing configured.\nhttps://cloud.google.com/compute/docs/autoscaler\n\nAutoscaling works independently from autohealing. \n\"If you configure autohealing for your group and an instance fails the health check, \nthe autohealer attempts to recreate the instance. Recreating an instance can cause the number of instances in the group to fall \nbelow the autoscaling threshold (minNumReplicas) that you specify.\""
      },
      {
        "date": "2020-12-13T18:07:00.000Z",
        "voteCount": 1,
        "content": "I mean why don't you elaborate on Autoscaling part. If it is set to off will the MIG keep instance health?"
      },
      {
        "date": "2021-04-29T15:15:00.000Z",
        "voteCount": 8,
        "content": "Good point but this questions says nothing about auto-healing so we need to treat this question within context. The reason that auto-healing is not the correct way to implement this is because you can fall under your min instance number with auto-healing and not with auto-scaling. See below:\nAutoscaling works independently from autohealing. If you configure autohealing for your group and an instance fails the health check, the autohealer attempts to recreate the instance. Recreating an instance can cause the number of instances in the group to fall below the autoscaling threshold (minNumReplicas) that you specify.\n\nIf you autoscale a regional MIG, an instance can be added then immediately deleted from one of the zones. This happens when the utilization in the zone triggers a scale out but the overall utilization in the regional MIG does not require the additional instance or the additional instance is required in a different zone.\n\nSource: https://cloud.google.com/compute/docs/autoscaler\n\nIn conclusion, the answer is A."
      },
      {
        "date": "2020-06-20T23:40:00.000Z",
        "voteCount": 30,
        "content": "We want the application running at all times. If the VM crashes due to any underlying hardware failure, we want another instance to be added so we need autoscaling ON\nCorrect answer is A"
      },
      {
        "date": "2020-12-21T03:34:00.000Z",
        "voteCount": 1,
        "content": "As said in other coment, you can fix this with Autohealing, autoscaling means more machines, autohealing means re creating of VM's"
      },
      {
        "date": "2021-02-06T12:15:00.000Z",
        "voteCount": 2,
        "content": "Even if I agree with your response, it still does not meet the requirements asked in the question, which is \"the application should be running all the time\", because in your case when the VM instance for whatever reason stops, that mean the application will experience a downtime."
      },
      {
        "date": "2020-06-22T16:19:00.000Z",
        "voteCount": 6,
        "content": "A is wrong - What you are talking about is Autohealing. Autoscale will not rebuild the VM on the crash."
      },
      {
        "date": "2020-12-20T23:02:00.000Z",
        "voteCount": 2,
        "content": "won't auto healing take time to recreate that VM? and there should be one VM running all time"
      },
      {
        "date": "2021-03-21T23:26:00.000Z",
        "voteCount": 7,
        "content": "Read this warning message: \"The minimum number of instances is equal to maximum number of instances. This means the autoscaler cannot add or remove instances from the instance group. Make sure this is the correct setting.\"\nSo when minimum is equal to maximum, it does not matter whether autoscaling is on or off. So now the question is who takes care of running the MINIMUM instances: MIG itself."
      },
      {
        "date": "2022-12-28T13:16:00.000Z",
        "voteCount": 1,
        "content": "nice explanation"
      },
      {
        "date": "2024-10-14T03:17:00.000Z",
        "voteCount": 1,
        "content": "A is correct."
      },
      {
        "date": "2024-09-10T15:17:00.000Z",
        "voteCount": 1,
        "content": "Autoscaling Off: By setting autoscaling to Off, you ensure that the instance group will not automatically scale up or down based on demand. This is important because you only want a single instance of the VM running.\nMinimum instances 1: Setting the minimum number of instances to 1 guarantees that at least one instance of the VM will always be running. This ensures that your application is always available.\nMaximum instances 1: Setting the maximum number of instances to 1 prevents additional instances from being created, ensuring that only a single instance of the VM is running at any given time."
      },
      {
        "date": "2024-08-17T11:14:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is A\nBy setting autoscaling to \"On\" and configuring both the minimum and maximum number of instances to 1, you ensure that the managed instance group will always maintain exactly one running instance. If the instance fails or is terminated, the autoscaler will automatically recreate it, ensuring that the application remains available at all times."
      },
      {
        "date": "2024-04-04T04:27:00.000Z",
        "voteCount": 3,
        "content": "Option A, setting autoscaling to On and configuring both the minimum and maximum number of instances to 1, may seem initially plausible, but it doesn't align with the requirement of having only a single instance of the VM running per GCP project.\n\nWith autoscaling set to On and both minimum and maximum instances set to 1, the managed instance group would indeed maintain a single instance of the VM. However, if for any reason that instance were to terminate unexpectedly (e.g., due to maintenance events, instance failure, etc.), the autoscaling mechanism would immediately attempt to launch another instance to maintain the desired minimum of 1 instance.\n\nThis behavior would potentially lead to multiple instances being spawned over time, violating the requirement of having only one instance running per GCP project."
      },
      {
        "date": "2024-04-04T04:27:00.000Z",
        "voteCount": 2,
        "content": "Therefore, option B, with autoscaling set to Off and both minimum and maximum instances set to 1, ensures strict enforcement of having only one instance running at all times, aligning better with the specified requirement."
      },
      {
        "date": "2024-03-18T06:13:00.000Z",
        "voteCount": 2,
        "content": "B is the answer."
      },
      {
        "date": "2024-03-04T16:28:00.000Z",
        "voteCount": 1,
        "content": "A is the best answer. No Autoscaling = Risk. Turning off autoscaling removes the automatic health checks and recovery mechanisms, making your single instance vulnerable."
      },
      {
        "date": "2024-02-17T18:46:00.000Z",
        "voteCount": 3,
        "content": "B is correct. This is a trick question. Don't confuse \"autoSCALING\" with \"autoHEALING\""
      },
      {
        "date": "2024-01-31T08:06:00.000Z",
        "voteCount": 2,
        "content": "Another instance will be created after a crash of the running instance"
      },
      {
        "date": "2024-01-24T20:29:00.000Z",
        "voteCount": 3,
        "content": "A is the correct answer.\n\nA. When autoscaling is on, we can set minimum and maximum no of instances.\nB. When autoscaling is off, we cannot set minimum or maximum no of instances. Instead we set no of instances.\nC. Max no of instances can't be 2 as per question.\nD. Max no of instances can't be 2 as per question."
      },
      {
        "date": "2024-01-16T10:00:00.000Z",
        "voteCount": 1,
        "content": "A - Correct"
      },
      {
        "date": "2024-01-07T16:21:00.000Z",
        "voteCount": 1,
        "content": "A is the correct answer"
      },
      {
        "date": "2024-01-03T13:16:00.000Z",
        "voteCount": 2,
        "content": "B - Autoscaling has no effect so A is wrong."
      },
      {
        "date": "2023-12-31T12:07:00.000Z",
        "voteCount": 1,
        "content": "Correct ans is B as autoscaling will create more instance and requirement is to run only single instance."
      },
      {
        "date": "2023-12-21T18:05:00.000Z",
        "voteCount": 1,
        "content": "A. Because we want app to be fail/crash safe."
      },
      {
        "date": "2023-12-18T06:39:00.000Z",
        "voteCount": 1,
        "content": "Option A, min 1 n max 1 with autoscaling"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 34,
    "url": "https://www.examtopics.com/discussions/google/view/16703-exam-associate-cloud-engineer-topic-1-question-34-discussion/",
    "body": "You want to verify the IAM users and roles assigned within a GCP project named my-project. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun gcloud iam roles list. Review the output section.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun gcloud iam service-accounts list. Review the output section.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNavigate to the project and then to the IAM section in the GCP Console. Review the members and roles.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNavigate to the project and then to the Roles section in the GCP Console. Review the roles and status."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-03-15T14:34:00.000Z",
        "voteCount": 65,
        "content": "Correct answer is C as IAM section provides the list of both Members and Roles.Option A is wrong as it would provide information about the roles only.Option B is wrong as it would provide only the service accounts.Option D is wrong as it would provide information about the roles only."
      },
      {
        "date": "2020-04-15T16:47:00.000Z",
        "voteCount": 10,
        "content": "C is the correct answer"
      },
      {
        "date": "2024-01-07T16:22:00.000Z",
        "voteCount": 1,
        "content": "C is the correct answer as it will details regarding the users as well as the Roles"
      },
      {
        "date": "2023-12-18T06:51:00.000Z",
        "voteCount": 1,
        "content": "Option c"
      },
      {
        "date": "2023-09-01T10:11:00.000Z",
        "voteCount": 1,
        "content": "c seems more legit"
      },
      {
        "date": "2023-05-07T00:39:00.000Z",
        "voteCount": 1,
        "content": "C is correct answer"
      },
      {
        "date": "2023-02-18T23:37:00.000Z",
        "voteCount": 2,
        "content": "Answer C is the correct answer.\n\nTo verify the IAM users and roles assigned within a GCP project, you can navigate to the project and then to the IAM section in the GCP Console. In the IAM section, you can review the members and roles assigned to the project. This will allow you to see who has what level of access to the project resources.\n\nAnswer A is incorrect because it lists the roles available in the project, but it does not show the IAM users and roles assigned to those roles.\n\nAnswer B is incorrect because it lists the service accounts in the project, but it does not show the IAM users and roles assigned to those service accounts.\n\nAnswer D is incorrect because it lists the roles available in the project, but it does not show the IAM users and roles assigned to those roles."
      },
      {
        "date": "2022-10-23T07:43:00.000Z",
        "voteCount": 1,
        "content": "C. Navigate to the project and then to the IAM section in the GCP Console. Review the members and roles."
      },
      {
        "date": "2022-07-13T10:01:00.000Z",
        "voteCount": 1,
        "content": "C is the only logical answers. If you go IAM &amp; Admin &gt; IAM: You can see Principals and Roles. Users, groups, service accounts are Principals"
      },
      {
        "date": "2022-07-06T18:28:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2022-06-22T14:08:00.000Z",
        "voteCount": 1,
        "content": "C is right! A &amp; B doesn't satisfy the requirement."
      },
      {
        "date": "2022-05-25T05:16:00.000Z",
        "voteCount": 2,
        "content": "Correct answer is C as IAM section provides the list of both Members and Roles.Option A is wrong as it would provide information about the roles only.Option B is wrong as it would provide only the service accounts.Option D is wrong"
      },
      {
        "date": "2022-05-24T19:40:00.000Z",
        "voteCount": 1,
        "content": "go for C"
      },
      {
        "date": "2022-05-23T04:20:00.000Z",
        "voteCount": 1,
        "content": "Go for C"
      },
      {
        "date": "2022-04-26T08:52:00.000Z",
        "voteCount": 2,
        "content": "C is correct"
      },
      {
        "date": "2022-03-02T03:50:00.000Z",
        "voteCount": 6,
        "content": "I chose C after experimenting in the console.\n\nA. Wrong, this just shows you all the roles, not users etc.  It's not useful.\nB. Wrong, This will just show service accounts, not users and roles like the question asks\nC. Correct.  I logged onto console and followed the steps and was able to see all the assigned users and roles.\nD.  Wrong.  We need to see how the roles are used.  I couldn't even see a \"Roles\" option directly, you need to browse to IAM then to the Roles subsection, which is not useful anyway."
      },
      {
        "date": "2021-11-19T01:08:00.000Z",
        "voteCount": 2,
        "content": "C. Navigate to the project and then to the IAM section in the GCP Console. Review the members and roles."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 35,
    "url": "https://www.examtopics.com/discussions/google/view/16705-exam-associate-cloud-engineer-topic-1-question-35-discussion/",
    "body": "You need to create a new billing account and then link it with an existing Google Cloud Platform project. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tVerify that you are Project Billing Manager for the GCP project. Update the existing project to link it to the existing billing account.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tVerify that you are Project Billing Manager for the GCP project. Create a new billing account and link the new billing account to the existing project.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tVerify that you are Billing Administrator for the billing account. Create a new project and link the new project to the existing billing account.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tVerify that you are Billing Administrator for the billing account. Update the existing project to link it to the existing billing account."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 56,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 9,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 7,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-03-15T14:38:00.000Z",
        "voteCount": 46,
        "content": "D is correct as the project is already created"
      },
      {
        "date": "2022-02-01T03:09:00.000Z",
        "voteCount": 27,
        "content": "The Question clearly says \"Create a new Billing account\" and a Bill Administrator cannot create A billing account so according to the question the clear Answer is B."
      },
      {
        "date": "2023-05-29T01:14:00.000Z",
        "voteCount": 6,
        "content": "Only options which says Create New billing account is B. This is what asked in the question. However Billing Account Administrator cannot create a new Billing account --&gt; https://cloud.google.com/billing/docs/how-to/billing-access \nBilling account Administrator --&gt; Manage billing accounts (but not create them).\n\nNone of the answers are correct."
      },
      {
        "date": "2021-06-24T02:01:00.000Z",
        "voteCount": 3,
        "content": "billing administration can not create billing account\nso A"
      },
      {
        "date": "2022-01-18T06:05:00.000Z",
        "voteCount": 8,
        "content": "B is the correct answer. Existing project with new billing account."
      },
      {
        "date": "2021-05-08T09:59:00.000Z",
        "voteCount": 30,
        "content": "How come? Option D says \"update the existing project to link it to the EXISTING billing account\", whereas the task is clearly saying \"you need to create a NEW billing account\".\n\nOnly B meets that criteria."
      },
      {
        "date": "2021-05-14T23:13:00.000Z",
        "voteCount": 1,
        "content": "check all options have mentioned is as \"EXISTING billing account\", because they are creating it and so it will exist."
      },
      {
        "date": "2022-04-04T01:55:00.000Z",
        "voteCount": 4,
        "content": "I JUST TESTED TO SEE IF THE ROLE OF PROJECT BILLING MANAGER CAN CREATE A NEW BILLING ACCOUNT,  I GOT THIS ERROR `You do not have sufficient permissions to view this page`"
      },
      {
        "date": "2020-04-15T17:59:00.000Z",
        "voteCount": 31,
        "content": "Project Billing Manager cannot create a billing account, there is nothing like Billing Administrator it is Billing Account Administrator. Both Project Billing Manager and Billing Account Administrator cannot create a billing account.  A is the only answer that make sense. We have to assume the the billing acount is already created"
      },
      {
        "date": "2021-07-14T09:37:00.000Z",
        "voteCount": 8,
        "content": "Wrong because \"You need to create a new billing account\". You can't assume the billing acount is already created."
      },
      {
        "date": "2020-12-18T13:33:00.000Z",
        "voteCount": 5,
        "content": "The answer is A, yes"
      },
      {
        "date": "2021-09-25T23:19:00.000Z",
        "voteCount": 1,
        "content": "Project Billing Manager can't create a billing account, still, A is the only feasible answer."
      },
      {
        "date": "2020-07-02T01:35:00.000Z",
        "voteCount": 7,
        "content": "'Verify that you are Billing Administrator for the billing account' - aka Billing Account Administrator. A and B do not have privilege to make a new billing account (https://cloud.google.com/billing/docs/how-to/billing-access). C is correct, this has been incorrectly up voted"
      },
      {
        "date": "2023-01-21T04:49:00.000Z",
        "voteCount": 2,
        "content": "The document says - https://cloud.google.com/billing/docs/how-to/billing-access\nBilling Account Administrator:\nBy default, the person who creates the Cloud Billing account is a Billing Account Administrator for the Cloud Billing account.\nCab link and unlink projects and manage other user roles on the billing account.\n So the Answer is clearly C"
      },
      {
        "date": "2023-01-21T04:50:00.000Z",
        "voteCount": 1,
        "content": "Also, If you are a billing administrator on only one Cloud Billing account, new projects you create are automatically linked to your existing Cloud Billing account.\n\nWhich clearly means, Billing Account Admin has the access to create a project"
      },
      {
        "date": "2024-10-14T03:20:00.000Z",
        "voteCount": 1,
        "content": "B is correct."
      },
      {
        "date": "2024-06-30T11:02:00.000Z",
        "voteCount": 1,
        "content": "B\nVerify that you are Project Billing Manager for the GCP project.\nCreate a new billing account and link the new billing account to the existing project."
      },
      {
        "date": "2024-02-17T19:07:00.000Z",
        "voteCount": 1,
        "content": "Hmm doesn't seem like any of these answers are correct. \n\nBut the Billing Account Admin has the same privileges and additional ones when compared to the Project Billing Manager.  YET, B is the only answer that says create a new billing account. This is a tough one, but I would go with answer D if it showed on the test."
      },
      {
        "date": "2024-02-17T19:08:00.000Z",
        "voteCount": 1,
        "content": "...additionally, doesn't seem like either role can create a new billing account"
      },
      {
        "date": "2024-02-17T19:09:00.000Z",
        "voteCount": 1,
        "content": "...additionally, doesn't seem like either role can create a new billing account when inspecting the role priveleges in IAM"
      },
      {
        "date": "2024-01-31T08:25:00.000Z",
        "voteCount": 1,
        "content": "All are more or less wrong. We need both Billing Account User and Project Billing Manager.\nBut if I have to choose I choose B (it's the only option that creates an account)\nhttps://cloud.google.com/billing/docs/how-to/billing-access\nit [Billing Account User] has very restricted permissions, so you can grant it broadly. When granted in combination with Project Creator, the two roles allow a user to create new projects linked to the billing account on which the Billing Account User role is granted. Or, when granted in combination with the Project Billing Manager role, the two roles allow a user to link and unlink projects on the billing account on which the Billing Account User role is granted."
      },
      {
        "date": "2023-11-25T05:22:00.000Z",
        "voteCount": 1,
        "content": "D is correct as my opinion because: This role is an owner role for a billing account. Use it to manage payment instruments, configure billing exports, view cost information, link and \nunlink projects and manage other user roles on the billing account. By default, the person who creates the Cloud Billing account is a Billing Account Administrator for the Cloud Billing account. and  \nWhen granted in combination with the Billing Account User role, the Project Billing Manager role allows a user to attach the project to the billing account, but does not grant any rights over resources. Project Owners can use this role to allow someone else to manage the billing for the project without granting them resource access. \n\nCHECK OUT ON THIS LINK: https://cloud.google.com/billing/docs/how-to/billing-access"
      },
      {
        "date": "2023-10-13T02:06:00.000Z",
        "voteCount": 1,
        "content": "Answer is B"
      },
      {
        "date": "2023-10-13T02:05:00.000Z",
        "voteCount": 1,
        "content": "Answer is B"
      },
      {
        "date": "2023-09-23T17:13:00.000Z",
        "voteCount": 1,
        "content": "I'm quite confused since the question is explicitly mentioning \"Creating New Billing Account\" however neither \"Project billing manager\" nor \"Billing Administrator\" has the permission to create a billing account. Only the billing creator can!!"
      },
      {
        "date": "2023-09-18T06:48:00.000Z",
        "voteCount": 1,
        "content": "I think its A. C and D refers to Billing Administrator and there is only Billing Account Administrator, so C abd D are out. \n\nOnly left is A and B. Both Project Billing Manafer cannot create billing accounts, he can only link and unlink billing accounts to projects, NOT create them. \n\nThus, the option left is A, and it makes sense, cause that role can link projects to billing accounts and update the exisiting project with that new billing accoubt."
      },
      {
        "date": "2023-09-18T06:50:00.000Z",
        "voteCount": 1,
        "content": "***to create billing accounts the role required is Billing Account Creator.\nLink: https://cloud.google.com/billing/docs/how-to/billing-access"
      },
      {
        "date": "2023-09-01T10:12:00.000Z",
        "voteCount": 2,
        "content": "b seems more legit"
      },
      {
        "date": "2023-08-03T05:48:00.000Z",
        "voteCount": 2,
        "content": "Option B is the accurate approach. As a Project Billing Manager, you have the authority to create a new billing account and link it to an existing Google Cloud Platform project. This way, you maintain control over both the project and its associated billing.\n\nOption A involves updating the existing project to link it to the existing billing account, which is not the correct sequence for creating a new billing account and linking it to the project."
      },
      {
        "date": "2023-08-01T22:52:00.000Z",
        "voteCount": 1,
        "content": "B is correct answer.Bill Administrator cannot create A billing account"
      },
      {
        "date": "2023-08-01T17:31:00.000Z",
        "voteCount": 1,
        "content": "Project Billing Manager\n(roles/billing.projectManager)\tLink/unlink the project to/from a billing account.\tOrganization, folder, or project.\tWhen granted in combination with the Billing Account User role, the Project Billing Manager role allows a user to attach the project to the billing account, but does not grant any rights over resources. Project Owners can use this role to allow someone else to manage the billing for the project without granting them resource access."
      },
      {
        "date": "2023-07-20T05:19:00.000Z",
        "voteCount": 2,
        "content": "B is the correct answer"
      },
      {
        "date": "2023-06-04T03:38:00.000Z",
        "voteCount": 3,
        "content": "D is the correct answer \nAs per GCP docs neither the Billing Administrator nor Project Billing Manager can create billing account.\nso by assuming that account is already created , we only have to link it\nProject Billing Manager role has to be combined with the Billing Account User role to be able to link a project to billing account \nso the correct answer must be D .\nhttps://cloud.google.com/billing/docs/how-to/billing-access#relationships-between-resources"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 36,
    "url": "https://www.examtopics.com/discussions/google/view/23110-exam-associate-cloud-engineer-topic-1-question-36-discussion/",
    "body": "You have one project called proj-sa where you manage all your service accounts. You want to be able to use a service account from this project to take snapshots of VMs running in another project called proj-vm. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDownload the private key from the service account, and add it to each VMs custom metadata.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDownload the private key from the service account, and add the private key to each VM's SSH keys.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGrant the service account the IAM Role of Compute Storage Admin in the project called proj-vm.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWhen creating the VMs, set the service account's API scope for Compute Engine to read/write."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-07-23T13:12:00.000Z",
        "voteCount": 43,
        "content": "C is the correct answer.\nIt took me a while to figure it out because I didn't understand how service accounts work across project. This article made it clear for me. https://gtseres.medium.com/using-service-accounts-across-projects-in-gcp-cf9473fef8f0\n\nYou create the service account in proj-sa and take note of the service account email, then you go to proj-vm in IAM &gt; ADD and add the service account's email as new member and give it the Compute Storage Admin role."
      },
      {
        "date": "2022-03-26T15:12:00.000Z",
        "voteCount": 2,
        "content": "As of now, service accounts may be impersonated (new-term). AKA, you can create a service account in one project and then impersonate it in others. Essentially, it involves the same steps as what the medium article suggests (create a service account in the principal (main) project and then add the email of the main project to the project you want to impersonate) https://cloud.google.com/iam/docs/impersonating-service-accounts#impersonate-sa-level"
      },
      {
        "date": "2022-07-13T11:15:00.000Z",
        "voteCount": 1,
        "content": "I have tried C, it doesn't work. Also, this refers to a different Principal (user) impersonating a Service Account which is a different case from what is in the question."
      },
      {
        "date": "2020-06-14T01:24:00.000Z",
        "voteCount": 22,
        "content": "Option C is the right one"
      },
      {
        "date": "2023-09-01T10:14:00.000Z",
        "voteCount": 1,
        "content": "C seems more correct, because you want to use it, you need access for it"
      },
      {
        "date": "2023-08-09T08:56:00.000Z",
        "voteCount": 1,
        "content": "C is the answer"
      },
      {
        "date": "2023-08-01T17:53:00.000Z",
        "voteCount": 1,
        "content": "C is the correct answer.\n\nCompute Storage Admin (roles/compute.storageAdmin) has permissions to create, modify, and delete disks, images, and snapshots.\n\nFor example, if your company has someone who manages project images and you don't want them to have the editor role on the project, then grant this role to their account on the project.\n\nThe most common way to let an application authenticate as a service account is to attach a service account to the resource running the application. For example, you can attach a service account to a Compute Engine instance so that applications running on that instance can authenticate as the service account. Then, you can grant the service account IAM roles to let the service account\u2014and, by extension, applications on the instance\u2014access Google Cloud resources."
      },
      {
        "date": "2023-02-19T00:04:00.000Z",
        "voteCount": 5,
        "content": "Answer C is correct. Grant the service account the IAM Role of Compute Storage Admin in the project called proj-vm.\n\nTo take snapshots of VMs running in another project, you need to grant the service account that will take the snapshots the necessary IAM role to perform the action. In this case, granting the service account in the proj-sa project the Compute Storage Admin role in the proj-vm project will allow it to take snapshots of VMs running in that project. \n\nAnswers A and B are incorrect because they involve downloading and adding the private key of the service account to each VM, which is not necessary and potentially risky. \n\nAnswer D is also incorrect because setting the service account's API scope for Compute Engine to read/write only grants it permission to perform actions on resources within the same project.\n\nhttps://cloud.google.com/iam/docs/creating-managing-service-accounts\n\nhttps://cloud.google.com/iam/docs/granting-roles-to-service-accounts"
      },
      {
        "date": "2022-10-23T07:57:00.000Z",
        "voteCount": 1,
        "content": "C. Grant the service account the IAM Role of Compute Storage Admin in the project called proj-vm."
      },
      {
        "date": "2022-08-18T07:50:00.000Z",
        "voteCount": 3,
        "content": "Safe to eliminate any options that demand transferring of private keys. NOT SAFE\n\nHence, C."
      },
      {
        "date": "2022-09-29T20:18:00.000Z",
        "voteCount": 1,
        "content": "highly agree with this thoughts! transferring private keys is a big no no here."
      },
      {
        "date": "2022-07-02T07:13:00.000Z",
        "voteCount": 1,
        "content": "Answer is C"
      },
      {
        "date": "2022-06-22T14:02:00.000Z",
        "voteCount": 4,
        "content": "C. is the correct answer\n\nCompute Storage Admin\n(roles/compute.storageAdmin)\t\nPermissions to create, modify, and delete disks, images, and snapshots.\n\nFor example, if your company has someone who manages project images and you don't want them to have the editor role on the project, then grant this role to their account on the project.\n\nLowest-level resources where you can grant this role:\n\nDisk\nImage\nSnapshot Beta"
      },
      {
        "date": "2022-05-24T19:46:00.000Z",
        "voteCount": 1,
        "content": "go for C"
      },
      {
        "date": "2022-03-03T04:02:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/compute/docs/access/iam#compute.storageAdmin"
      },
      {
        "date": "2021-12-07T05:51:00.000Z",
        "voteCount": 2,
        "content": "When a service account is in one project, and it accesses a resource in another project, you usually must enable the API for that resource in both projects. For example, if you have a service account in the project my-service-accounts and a Cloud SQL instance in the project my-application, you must enable the Cloud SQL API in both my-service-accounts and my-application."
      },
      {
        "date": "2021-11-19T01:18:00.000Z",
        "voteCount": 1,
        "content": "C. Grant the service account the IAM Role of Compute Storage Admin in the project called proj-vm."
      },
      {
        "date": "2021-11-18T23:48:00.000Z",
        "voteCount": 1,
        "content": "C. Grant the service account the IAM Role of Compute Storage Admin in the project called proj-vm."
      },
      {
        "date": "2021-11-18T15:44:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2021-07-07T07:45:00.000Z",
        "voteCount": 4,
        "content": "C. Compute Storage Admin role has this: compute.snapshots.*"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 37,
    "url": "https://www.examtopics.com/discussions/google/view/17331-exam-associate-cloud-engineer-topic-1-question-37-discussion/",
    "body": "You created a Google Cloud Platform project with an App Engine application inside the project. You initially configured the application to be served from the us- central region. Now you want the application to be served from the asia-northeast1 region. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the default region property setting in the existing GCP project to asia-northeast1.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the region property setting in the existing App Engine application from us-central to asia-northeast1.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a second App Engine application in the existing GCP project and specify asia-northeast1 as the region to serve your application.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new GCP project and create an App Engine application inside this new project. Specify asia-northeast1 as the region to serve your application.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 49,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-03-23T21:19:00.000Z",
        "voteCount": 91,
        "content": "Option D is correct, as there can be only one App Engine application inside a project . C is incorrect, as GCP can't have two app engine applications.."
      },
      {
        "date": "2021-07-14T09:46:00.000Z",
        "voteCount": 25,
        "content": "Yes, and you can't change an App Engine application region once created"
      },
      {
        "date": "2020-06-04T08:32:00.000Z",
        "voteCount": 39,
        "content": "Each Cloud project can contain only a single App Engine application, and once created you cannot change the location of your App Engine application.\n\nhttps://cloud.google.com/appengine/docs/flexible/nodejs/managing-projects-apps-billing#create"
      },
      {
        "date": "2024-07-24T15:05:00.000Z",
        "voteCount": 1,
        "content": "Why showing correct answer as C????"
      },
      {
        "date": "2024-03-14T12:23:00.000Z",
        "voteCount": 1,
        "content": "why the correct answer is D?"
      },
      {
        "date": "2024-03-14T12:24:00.000Z",
        "voteCount": 1,
        "content": "and why here is mentioning that C is correct?"
      },
      {
        "date": "2023-09-01T10:16:00.000Z",
        "voteCount": 2,
        "content": "D seems more correct , as a project can only have a single app engine application"
      },
      {
        "date": "2023-08-31T05:22:00.000Z",
        "voteCount": 4,
        "content": "Each Google Cloud project can contain only a single App Engine application, and once created you cannot change the location of your App Engine application"
      },
      {
        "date": "2023-08-09T09:00:00.000Z",
        "voteCount": 1,
        "content": "D as you cannot have more than one APP engine"
      },
      {
        "date": "2023-08-03T21:24:00.000Z",
        "voteCount": 1,
        "content": "Option D is correct.\n\nThere can be only one App Engine application inside a project\nBesides, you cannot change an app's region after you set it.\nhttps://cloud.google.com/appengine/docs/standard/locations"
      },
      {
        "date": "2023-08-01T18:16:00.000Z",
        "voteCount": 1,
        "content": "Option D is correct.\n\nThere can be only one App Engine application inside a project \nBesides, you cannot change an app's region after you set it. \nhttps://cloud.google.com/appengine/docs/standard/locations"
      },
      {
        "date": "2023-07-24T08:33:00.000Z",
        "voteCount": 1,
        "content": "Option D is correct"
      },
      {
        "date": "2023-05-25T12:06:00.000Z",
        "voteCount": 2,
        "content": "To change the region for serving your existing App Engine application from the us-central region to the asia-northeast1 region, you should:\n\nB. Change the region property setting in the existing App Engine application from us-central to asia-northeast1.\n\nHere's why this option is the correct choice:\n\nRegion Property Setting: The region property determines the geographical location where your App Engine application is served from. By changing the region property setting from us-central to asia-northeast1, you instruct App Engine to serve your application from the desired region.\n\nExisting App Engine Application: Since you already have an existing App Engine application, there's no need to create a new project or a new application. You can simply modify the configuration of the existing application to switch the serving region.\n\nBy following option B and changing the region property setting in the existing App Engine application from us-central to asia-northeast1, you ensure that your application is served from the desired region."
      },
      {
        "date": "2024-03-14T12:37:00.000Z",
        "voteCount": 1,
        "content": "why B?\nhttps://cloud.google.com/appengine/docs/standard/locations#:~:text=Using%20services%20across%20multiple%20locations,region%20after%20you%20set%20it."
      },
      {
        "date": "2023-05-25T07:19:00.000Z",
        "voteCount": 2,
        "content": "Definitely D is correct\nhttps://cloud.google.com/appengine/docs/flexible/managing-projects-apps-billing#create"
      },
      {
        "date": "2023-04-21T12:01:00.000Z",
        "voteCount": 2,
        "content": "D is all over the internet and on the official docs"
      },
      {
        "date": "2023-03-22T21:11:00.000Z",
        "voteCount": 2,
        "content": "one project can contain only a single app engine"
      },
      {
        "date": "2023-02-20T04:32:00.000Z",
        "voteCount": 2,
        "content": "Cannot create multiple app engine application inside one project"
      },
      {
        "date": "2023-02-19T00:14:00.000Z",
        "voteCount": 5,
        "content": "The answer is D: Create a new GCP project and create an App Engine application inside this new project. Specify asia-northeast1 as the region to serve your application.\n\nTherefore, answer D is the correct answer as it involves creating a new GCP project and App Engine application with the desired region specified from the start.\n\nAnswer A is incorrect because changing the default region property setting in the existing GCP project does not automatically change the region in which your App Engine application is served.\n\nAnswer B is incorrect because changing the region property setting in the existing App Engine application from us-central to asia-northeast1 only changes the default region for the App Engine app, but it does not actually move the app to the new region. The app would continue to be served from the original region until it is moved to the new region.\n\nAnswer C is incorrect because creating a second App Engine application in the existing GCP project does not change the region in which the original App Engine application is served."
      },
      {
        "date": "2023-05-18T21:51:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/appengine/docs/standard/locations\nYou cannot change an app's region after you set it."
      },
      {
        "date": "2023-01-18T02:42:00.000Z",
        "voteCount": 2,
        "content": "It's D because each cloud project can contain only one App Engine application so C is wrong!\nAnd also it's not possibile to change the location of an existing app engine."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 38,
    "url": "https://www.examtopics.com/discussions/google/view/17345-exam-associate-cloud-engineer-topic-1-question-38-discussion/",
    "body": "You need to grant access for three users so that they can view and edit table data on a Cloud Spanner instance. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun gcloud iam roles describe roles/spanner.databaseUser. Add the users to the role.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun gcloud iam roles describe roles/spanner.databaseUser. Add the users to a new group. Add the group to the role.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun gcloud iam roles describe roles/spanner.viewer - -project my-project. Add the users to the role.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun gcloud iam roles describe roles/spanner.viewer - -project my-project. Add the users to a new group. Add the group to the role."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 41,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-03-24T05:34:00.000Z",
        "voteCount": 73,
        "content": "I think it should be B, setup a group first are suggested way from Google."
      },
      {
        "date": "2020-04-17T09:10:00.000Z",
        "voteCount": 26,
        "content": "B is the correct option"
      },
      {
        "date": "2023-11-22T07:19:00.000Z",
        "voteCount": 2,
        "content": "B \nas per the best practice."
      },
      {
        "date": "2023-11-01T02:42:00.000Z",
        "voteCount": 1,
        "content": "B is correct Because adding a group instead to the user is a GCP best practice"
      },
      {
        "date": "2023-09-01T10:18:00.000Z",
        "voteCount": 2,
        "content": "b seems more legit as it will add in the group and they need edit access also"
      },
      {
        "date": "2023-08-25T10:01:00.000Z",
        "voteCount": 1,
        "content": "Setup group and add role to it"
      },
      {
        "date": "2023-08-09T09:01:00.000Z",
        "voteCount": 1,
        "content": "I go with B but TO have more controlled access, A is correct as well"
      },
      {
        "date": "2023-08-01T18:30:00.000Z",
        "voteCount": 2,
        "content": "Google groups can help you manage users at scale. Each member of a Google group inherits the Identity and Access Management (IAM) roles granted to that group. This inheritance means that you can use a group's membership to manage users' roles instead of granting IAM roles to individual users.\n\nhttps://cloud.google.com/iam/docs/groups-in-cloud-console#:~:text=To%20add%20members%3A%20Click%20person,add%20them%20to%20the%20group."
      },
      {
        "date": "2023-07-16T01:33:00.000Z",
        "voteCount": 1,
        "content": "B is the correct option as spanner users are grouped into a single group and can be added to the IAM role. Easy for management work."
      },
      {
        "date": "2023-02-19T00:43:00.000Z",
        "voteCount": 8,
        "content": "Answer C is incorrect because the `roles/spanner.viewer` role only allows read-only access to Spanner instances, whereas the question asks for users to be able to view and edit table data.\n\nAnswer D is also incorrect for the same reason as answer C. The `roles/spanner.viewer` role does not provide the necessary permissions for editing table data.\n\nTherefore, answers A and B are the only options that provide the `roles/spanner.databaseUser` role, which includes the necessary permissions to view and edit table data on a Cloud Spanner instance. \n\nHowever, answer B is arguably better since it involves creating a new group and adding the users to that group, which can simplify the management of permissions in the future."
      },
      {
        "date": "2023-01-21T05:35:00.000Z",
        "voteCount": 2,
        "content": "There's no mention of the 3 users being related, so why create a group? A seems best."
      },
      {
        "date": "2023-01-28T14:57:00.000Z",
        "voteCount": 3,
        "content": "Because the best practice is to create groups and assign the role to the group, not to the users directly."
      },
      {
        "date": "2023-01-21T05:35:00.000Z",
        "voteCount": 1,
        "content": "There's no mention of the 3 users being related, so why create a group? A seems best."
      },
      {
        "date": "2023-01-18T02:45:00.000Z",
        "voteCount": 1,
        "content": "I think it's B, because GC best practices suggest to create groups and associate users to it."
      },
      {
        "date": "2023-01-28T16:21:00.000Z",
        "voteCount": 2,
        "content": "You know what? I also thought it was B but there's no mention about best practices, and Google Groups can only be created if there's a google workspace organization. If you go to Google &gt; Groups, you can't even create groups and the platform asks to choose an organization (from Google workspace). That has got me confused."
      },
      {
        "date": "2022-11-30T06:08:00.000Z",
        "voteCount": 1,
        "content": "Bis the correct answer"
      },
      {
        "date": "2022-10-18T03:43:00.000Z",
        "voteCount": 1,
        "content": "B is the most correct based on this \nhttps://cloud.google.com/spanner/docs/iam#spanner.databaseUser"
      },
      {
        "date": "2022-10-12T04:20:00.000Z",
        "voteCount": 1,
        "content": "Ans is B"
      },
      {
        "date": "2022-10-07T09:22:00.000Z",
        "voteCount": 1,
        "content": "Yeah, option B if we involve best practices. But the question does not say anything about any group. Thus option A is the correct one"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 39,
    "url": "https://www.examtopics.com/discussions/google/view/19613-exam-associate-cloud-engineer-topic-1-question-39-discussion/",
    "body": "You create a new Google Kubernetes Engine (GKE) cluster and want to make sure that it always runs a supported and stable version of Kubernetes. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable the Node Auto-Repair feature for your GKE cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable the Node Auto-Upgrades feature for your GKE cluster.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSelect the latest available cluster version for your GKE cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSelect \u05d2\u20acContainer-Optimized OS (cos)\u05d2\u20ac as a node image for your GKE cluster."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 13,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-05-10T05:30:00.000Z",
        "voteCount": 41,
        "content": "The answer is B\nhttps://cloud.google.com/kubernetes-engine/versioning-and-upgrades"
      },
      {
        "date": "2020-06-23T15:22:00.000Z",
        "voteCount": 32,
        "content": "\"Creating or upgrading a cluster by specifying the version as &lt;latest&gt; does not provide automatic upgrades. Enable automatic node upgrades to ensure that the nodes in your cluster up to date with the latest stable version.\" --source: https://cloud.google.com/kubernetes-engine/versioning-and-upgrades\n\n-Correct answer: B"
      },
      {
        "date": "2023-09-01T10:21:00.000Z",
        "voteCount": 2,
        "content": "b is corrent , as auto updates provide the more stable version"
      },
      {
        "date": "2023-02-19T00:47:00.000Z",
        "voteCount": 9,
        "content": "Answer B is correct. Google Kubernetes Engine (GKE) supports multiple versions of Kubernetes, and new versions are regularly released. To ensure that your GKE cluster runs a supported and stable version of Kubernetes, it is recommended to enable the Node Auto-Upgrades feature. This feature automatically upgrades the Kubernetes version of each node in the cluster to the latest stable version.\n\nAnswer A, enabling the Node Auto-Repair feature, is focused on repairing or replacing nodes in case they become unresponsive, but it doesn't address the need for running a supported and stable version of Kubernetes.\n\nAnswer C, selecting the latest available cluster version, may not always be the best option as new versions may have bugs or issues that have not yet been identified.\n\nAnswer D, selecting Container-Optimized OS (cos) as a node image, is focused on using a lightweight and secure operating system optimized for running containers, but it doesn't address the need for running a supported and stable version of Kubernetes."
      },
      {
        "date": "2023-08-09T09:02:00.000Z",
        "voteCount": 1,
        "content": "True. B is the correct choice"
      },
      {
        "date": "2022-10-23T08:13:00.000Z",
        "voteCount": 1,
        "content": "B. Enable the Node Auto-Upgrades"
      },
      {
        "date": "2022-06-22T13:53:00.000Z",
        "voteCount": 2,
        "content": "B is correct.\nCreating or upgrading a cluster by specifying the version as latest does not provide automatic upgrades. Enable node auto-upgrades to ensure that the nodes in your cluster are up-to-date with the latest stable version."
      },
      {
        "date": "2022-05-29T05:31:00.000Z",
        "voteCount": 2,
        "content": "Node auto-upgrades help you keep the nodes in your cluster up-to-date with the cluster control plane version when your control plane is updated on your behalf. When you create a new cluster or node pool with Google Cloud console or the gcloud command, node auto-upgrade is enabled by default."
      },
      {
        "date": "2022-05-09T15:47:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/kubernetes-engine/versioning-and-upgrades"
      },
      {
        "date": "2021-11-19T01:21:00.000Z",
        "voteCount": 4,
        "content": "B. Enable the Node Auto-Upgrades feature for your GKE cluster."
      },
      {
        "date": "2021-11-18T23:51:00.000Z",
        "voteCount": 1,
        "content": "B. Enable the Node Auto-Upgrades feature for your GKE cluster."
      },
      {
        "date": "2021-11-18T15:51:00.000Z",
        "voteCount": 1,
        "content": "Ans is B"
      },
      {
        "date": "2021-10-28T15:49:00.000Z",
        "voteCount": 2,
        "content": "Ans B- With auto-upgrades, GKE automatically ensures that security updates are applied and kept up to date. Ease of use: Provides a simple way to keep your nodes up to date with the latest Kubernetes features."
      },
      {
        "date": "2021-05-12T10:12:00.000Z",
        "voteCount": 2,
        "content": "I was trying to create GKE but I dont see that option in Console"
      },
      {
        "date": "2021-05-11T21:08:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2021-03-24T13:47:00.000Z",
        "voteCount": 1,
        "content": "B is correct. Enable the Node Auto-Upgrades feature for your GKE cluster."
      },
      {
        "date": "2021-03-23T17:53:00.000Z",
        "voteCount": 1,
        "content": "answer is B . https://cloud.google.com/kubernetes-engine/versioning in 'specifying cluster version' section"
      },
      {
        "date": "2021-03-15T19:46:00.000Z",
        "voteCount": 1,
        "content": "Vote for B"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 40,
    "url": "https://www.examtopics.com/discussions/google/view/16973-exam-associate-cloud-engineer-topic-1-question-40-discussion/",
    "body": "You have an instance group that you want to load balance. You want the load balancer to terminate the client SSL session. The instance group is used to serve a public web application over HTTPS. You want to follow Google-recommended practices. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an HTTP(S) load balancer.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an internal TCP load balancer.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an external SSL proxy load balancer.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an external TCP proxy load balancer."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 26,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-04-02T23:09:00.000Z",
        "voteCount": 62,
        "content": "According to the documentation of SSL Proxy Load Balacing on Google, \"SSL Proxy Load Balancing is intended for non-HTTP(S) traffic. For HTTP(S) traffic, we recommend that you use HTTP(S) Load Balancing.\" in my opinion A should be the most suitable choice."
      },
      {
        "date": "2021-05-03T07:49:00.000Z",
        "voteCount": 16,
        "content": "Agree with you but, A is not the most suitable choice, it is the only choice, as the other Load  Balancers cannot route HTTP(S) traffic."
      },
      {
        "date": "2020-04-17T09:35:00.000Z",
        "voteCount": 29,
        "content": "For HTTP(s) Load balancer, the client SSL session terminates at the load balancer. A is the correct option."
      },
      {
        "date": "2024-05-21T06:41:00.000Z",
        "voteCount": 1,
        "content": "why A is correcte ?"
      },
      {
        "date": "2024-09-17T09:45:00.000Z",
        "voteCount": 2,
        "content": "In opt B, Internal tcp only deals with ip and ports in a vpc i.e 'internally' and does not deal with http/s or ssl/tls\nIn opt C external SSL can serve application over https but not the ssl termination part.\nIn opt D external TCP only deals with web application over HTTP and not the secured one. To top it off, it can't even terminate SSL session. \nHence the only viable option is HTTP(S) load balancer."
      },
      {
        "date": "2023-09-01T10:22:00.000Z",
        "voteCount": 1,
        "content": "google recommend https for terminate the ssl session so A seems more legit"
      },
      {
        "date": "2023-08-09T09:04:00.000Z",
        "voteCount": 2,
        "content": "Question mentions HTTPS, SO A is the correct answer."
      },
      {
        "date": "2023-02-19T01:04:00.000Z",
        "voteCount": 22,
        "content": "Answer A is correct. Google recommends using an HTTP(S) load balancer for terminating SSL sessions and load-balancing traffic to an instance group serving a public web application over HTTPS. \n\nAnswer B is incorrect because it is an internal load balancer, which is not suitable for serving public web applications. Internal load balancers are used for private/internal applications.\n\nAnswer C is incorrect because SSL proxy load balancers do not terminate the SSL session, instead they pass the SSL traffic directly to the backends without decrypting it. SSL proxy load balancers are used when you need to ensure that SSL is used end-to-end between the client and the backend, and when you want to offload SSL processing from the backends.\n\nAnswer D is incorrect because TCP proxy load balancers do not terminate SSL sessions. TCP proxy load balancers are used for non-HTTP traffic and can balance traffic at the TCP layer, but they do not have the ability to terminate SSL sessions."
      },
      {
        "date": "2023-03-21T13:37:00.000Z",
        "voteCount": 1,
        "content": "When using External SSL Proxy Load Balancing for your SSL traffic, user SSL (TLS) connections are terminated at the load balancing layer, and then proxied to the closest available backend instances by using either SSL (recommended) or TCP. For the types of backends that are supported, see Backends.\n\nhttps://cloud.google.com/load-balancing/docs/ssl/"
      },
      {
        "date": "2023-04-16T06:49:00.000Z",
        "voteCount": 1,
        "content": "yo never fail us, my lord!"
      },
      {
        "date": "2022-10-23T08:14:00.000Z",
        "voteCount": 1,
        "content": "HTTP(S) load balancer."
      },
      {
        "date": "2022-06-22T13:52:00.000Z",
        "voteCount": 1,
        "content": "I will go with A"
      },
      {
        "date": "2022-06-06T04:33:00.000Z",
        "voteCount": 1,
        "content": "A is correct.\nAccording to this guide for setting up an HTTP (S) load balancer in GCP: The client SSL session terminates at the load balancer. Sessions between the load balancer and the instance can either be HTTPS (recommended) or HTTP."
      },
      {
        "date": "2022-05-24T20:32:00.000Z",
        "voteCount": 1,
        "content": "Go for C.\nIt dont say Global balancer."
      },
      {
        "date": "2022-05-04T00:01:00.000Z",
        "voteCount": 2,
        "content": "Correct A"
      },
      {
        "date": "2021-12-03T03:46:00.000Z",
        "voteCount": 5,
        "content": "Correct answer is A - HTTP LB \nfor SSL proxy, its doest support HTTP traffic \n\"SSL Proxy Load Balancing is intended for non-HTTP(S) traffic. For HTTP(S) traffic, we recommend that you use HTTP(S) Load Balancing.\n\"\nhttps://cloud.google.com/load-balancing/docs/ssl"
      },
      {
        "date": "2021-11-19T01:21:00.000Z",
        "voteCount": 1,
        "content": "A. Configure an HTTP(S) load balancer."
      },
      {
        "date": "2021-11-18T23:52:00.000Z",
        "voteCount": 1,
        "content": "A. Configure an HTTP(S) load balancer."
      },
      {
        "date": "2021-11-18T15:52:00.000Z",
        "voteCount": 1,
        "content": "Ans is A"
      },
      {
        "date": "2021-10-28T15:59:00.000Z",
        "voteCount": 2,
        "content": "A-  please refer the this link - https://cloud.google.com/load-balancing/docs/choosing-load-balancer"
      },
      {
        "date": "2021-07-02T08:07:00.000Z",
        "voteCount": 2,
        "content": "No doubt its answer is (A) in Azure its application gateway. These both are layer 7 traffic load balancer"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 41,
    "url": "https://www.examtopics.com/discussions/google/view/17901-exam-associate-cloud-engineer-topic-1-question-41-discussion/",
    "body": "You have 32 GB of data in a single file that you need to upload to a Nearline Storage bucket. The WAN connection you are using is rated at 1 Gbps, and you are the only one on the connection. You want to use as much of the rated 1 Gbps as possible to transfer the file rapidly. How should you upload the file?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the GCP Console to transfer the file instead of gsutil.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable parallel composite uploads using gsutil on the file transfer.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDecrease the TCP window size on the machine initiating the transfer.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the storage class of the bucket from Nearline to Multi-Regional."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 22,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-04-28T11:14:00.000Z",
        "voteCount": 45,
        "content": "Correct answer is B as the bandwidth is good and its a single file, gsutil parallel composite uploads can be used to split the large file and upload in parallel.Refer GCP documentation - Transferring Data to GCP &amp;amp"
      },
      {
        "date": "2020-05-25T03:02:00.000Z",
        "voteCount": 17,
        "content": "Truly B is absolutely correct"
      },
      {
        "date": "2024-06-30T11:11:00.000Z",
        "voteCount": 2,
        "content": "B- Enable parallel composite uploads using gsutil on the file transfer.\n\nThis option is correct because parallel composite uploads can break down a large file into smaller components, upload them in parallel, and recombine them into a single object in the cloud. This method takes advantage of the available bandwidth more efficiently than serial uploads, as it can simultaneously transmit multiple parts of the file over the network. The gsutil tool has a -o option that allows enabling of parallel composite uploads."
      },
      {
        "date": "2024-05-23T11:39:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer: B"
      },
      {
        "date": "2023-09-11T04:11:00.000Z",
        "voteCount": 5,
        "content": "The best option to upload the file is B. Enable parallel composite uploads using gsutil on the file transfer. This is because parallel composite uploads can speed up the upload of large files by dividing them into chue upload time. \n\nThe other options are not as effective or feasible as option B:\nOption A. Use the GCP Console to transfer the file instead of gsutil is not a good choice because the GCP Console has a limit of 5 GB per file upload. \n\nOption C. Decrease the TCP window size on the machine initiating the transfer is not advisable because it would reduce the amount of data that can be sent before receiving an acknowledgment, which could lead to lower throughput and higher latency. \n\nOption D. Change the storage class of the bucket from Nearline to Multi-Regional is not relevant to the upload speed, as it only affects the availability and cost of storing and accessing the data."
      },
      {
        "date": "2023-09-01T10:23:00.000Z",
        "voteCount": 1,
        "content": "b is legit correct as it helps you more to increase the speed."
      },
      {
        "date": "2023-08-09T09:08:00.000Z",
        "voteCount": 1,
        "content": "Parallel composite is the right ans"
      },
      {
        "date": "2023-03-22T21:35:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2023-02-19T12:07:00.000Z",
        "voteCount": 10,
        "content": "Answer B is correct. Enable parallel composite uploads using gsutil on the file transfer.\n\nThe most efficient way to upload the large file to Nearline Storage bucket using a single WAN connection rated at 1 Gbps is to enable parallel composite uploads using gsutil. By default, gsutil uses a single thread to upload a single object. But with parallel composite uploads, gsutil will split the file into smaller chunks and upload these chunks in parallel using multiple threads. This will allow the file to be uploaded faster and more efficiently.\n\nhttps://cloud.google.com/storage/docs/parallel-composite-uploads"
      },
      {
        "date": "2023-02-04T05:28:00.000Z",
        "voteCount": 1,
        "content": "B is correct Answer"
      },
      {
        "date": "2023-01-08T18:13:00.000Z",
        "voteCount": 3,
        "content": "Just passed my exam and this question is on the exam, the correct answer is B"
      },
      {
        "date": "2022-10-23T08:15:00.000Z",
        "voteCount": 1,
        "content": "parallel composite uploads"
      },
      {
        "date": "2022-06-22T13:29:00.000Z",
        "voteCount": 3,
        "content": "B is right\nhttps://cloud.google.com/storage/docs/parallel-composite-uploads"
      },
      {
        "date": "2022-05-04T00:02:00.000Z",
        "voteCount": 2,
        "content": "b is right"
      },
      {
        "date": "2022-04-03T07:53:00.000Z",
        "voteCount": 3,
        "content": "youtube.com/watch?v=NlevtGlo-E0  slice upload elephant file"
      },
      {
        "date": "2021-11-19T10:38:00.000Z",
        "voteCount": 2,
        "content": "Ans : B"
      },
      {
        "date": "2021-11-19T07:22:00.000Z",
        "voteCount": 2,
        "content": ". Enable parallel composite uploads using gsutil on the file transfer."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 42,
    "url": "https://www.examtopics.com/discussions/google/view/21685-exam-associate-cloud-engineer-topic-1-question-42-discussion/",
    "body": "You've deployed a microservice called myapp1 to a Google Kubernetes Engine cluster using the YAML file specified below:<br><img src=\"/assets/media/exam-media/04338/0002100001.jpg\" class=\"in-exam-image\"><br>You need to refactor this configuration so that the database password is not stored in plain text. You want to follow Google-recommended practices. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the database password inside the Docker image of the container, not in the YAML file.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the database password inside a Secret object. Modify the YAML file to populate the DB_PASSWORD environment variable from the Secret.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the database password inside a ConfigMap object. Modify the YAML file to populate the DB_PASSWORD environment variable from the ConfigMap.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the database password in a file inside a Kubernetes persistent volume, and use a persistent volume claim to mount the volume to the container."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 30,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-05-31T19:01:00.000Z",
        "voteCount": 72,
        "content": "it is good practice to use Secrets for confidential data (like API keys) and ConfigMaps for non-confidential data (like port numbers).  B is correct."
      },
      {
        "date": "2020-06-04T09:04:00.000Z",
        "voteCount": 39,
        "content": "B is correct answer\nhttps://cloud.google.com/kubernetes-engine/docs/concepts/secret"
      },
      {
        "date": "2021-07-27T07:27:00.000Z",
        "voteCount": 9,
        "content": "\"Storing sensitive data in Secrets is more secure than in plaintext ConfigMaps or in Pod specifications\""
      },
      {
        "date": "2024-06-05T10:54:00.000Z",
        "voteCount": 2,
        "content": "How could this possibly be C over B?\n\n\"ConfigMap is similar to Secret except that you use a Secret for sensitive information and you use a ConfigMap to store non-sensitive data such as connection strings, public credentials, hostnames, and URLs.\""
      },
      {
        "date": "2024-05-23T11:39:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer: C"
      },
      {
        "date": "2023-12-27T13:00:00.000Z",
        "voteCount": 1,
        "content": "In my opinion also B is correct answer as secret manager will keep secret of all credentials and confidentiality."
      },
      {
        "date": "2023-12-25T07:52:00.000Z",
        "voteCount": 3,
        "content": "why most answer by examtopics are wrong"
      },
      {
        "date": "2023-11-01T02:44:00.000Z",
        "voteCount": 1,
        "content": "B is correct because storing passwords in secrets is the GKE best practice"
      },
      {
        "date": "2023-09-24T09:43:00.000Z",
        "voteCount": 1,
        "content": "Storing database passwords, or any sensitive credentials, inside a ConfigMap is not recommended from a security standpoint. \"B\" it is!"
      },
      {
        "date": "2023-09-01T10:25:00.000Z",
        "voteCount": 1,
        "content": "b is correct as it good pracits to use secrrets for the passwords"
      },
      {
        "date": "2023-08-31T05:35:00.000Z",
        "voteCount": 1,
        "content": "correct answer is B, as secrets are used to store credentials and configmap is used to store the configuration"
      },
      {
        "date": "2023-08-09T09:12:00.000Z",
        "voteCount": 1,
        "content": "B is the right approach"
      },
      {
        "date": "2023-07-20T05:34:00.000Z",
        "voteCount": 1,
        "content": "B is correct."
      },
      {
        "date": "2023-03-09T04:49:00.000Z",
        "voteCount": 1,
        "content": "B is the correct answer as configmap is configurations non confidential."
      },
      {
        "date": "2023-02-27T04:40:00.000Z",
        "voteCount": 1,
        "content": "A common use case for a service is to use ConfigMaps to separate application code from configuration. ConfigMap is similar to Secret except that you use a Secret for sensitive information and you use a ConfigMap to store non-sensitive data such as connection strings, public credentials, hostnames, and URLs."
      },
      {
        "date": "2023-02-19T12:30:00.000Z",
        "voteCount": 1,
        "content": "Answer B is the correct choice as it recommends storing the database password inside a Secret object, which is designed to securely store sensitive data like passwords, and then modifying the YAML file to populate the DB_PASSWORD environment variable from the Secret.\n\nStoring sensitive information such as passwords in plain text inside configuration files is not secure and violates Google's security best practices. Instead, secrets should be stored separately and securely. In Kubernetes, secrets are designed to store sensitive information such as passwords, API keys, and tokens. Secrets are encrypted and can be used to pass sensitive data to containers in a safe manner.\n\nTo implement this in the given configuration, you can create a secret object and store the database password as a key-value pair. Then, modify the YAML file to populate the DB_PASSWORD environment variable from the secret.\n\nhttps://kubernetes.io/docs/concepts/configuration/secret/"
      },
      {
        "date": "2023-01-29T07:59:00.000Z",
        "voteCount": 1,
        "content": "B is the correct answer\nA Secret is an object that contains a small amount of sensitive data such as a password, a token, or a key. Such information might otherwise be put in a Pod specification or in a container image. Using a Secret means that you don't need to include confidential data in your application code."
      },
      {
        "date": "2023-01-25T15:39:00.000Z",
        "voteCount": 1,
        "content": "B not C read here that configmap don't encrypt \nhttps://kubernetes.io/docs/concepts/configuration/configmap/"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 43,
    "url": "https://www.examtopics.com/discussions/google/view/21292-exam-associate-cloud-engineer-topic-1-question-43-discussion/",
    "body": "You are running an application on multiple virtual machines within a managed instance group and have autoscaling enabled. The autoscaling policy is configured so that additional instances are added to the group if the CPU utilization of instances goes above 80%. VMs are added until the instance group reaches its maximum limit of five VMs or until CPU utilization of instances lowers to 80%. The initial delay for HTTP health checks against the instances is set to 30 seconds.<br>The virtual machine instances take around three minutes to become available for users. You observe that when the instance group autoscales, it adds more instances then necessary to support the levels of end-user traffic. You want to properly maintain instance group sizes when autoscaling. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the maximum number of instances to 1.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDecrease the maximum number of instances to 3.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a TCP health check instead of an HTTP health check.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the initial delay of the HTTP health check to 200 seconds.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 20,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-10-25T06:12:00.000Z",
        "voteCount": 30,
        "content": "I think exam will be ended by the time you finish reading the question"
      },
      {
        "date": "2020-05-25T03:10:00.000Z",
        "voteCount": 24,
        "content": "D is correct answer."
      },
      {
        "date": "2024-05-23T11:40:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer: D"
      },
      {
        "date": "2023-09-01T10:27:00.000Z",
        "voteCount": 1,
        "content": "D is more correct because it gives you time to check the available instace"
      },
      {
        "date": "2023-04-04T01:34:00.000Z",
        "voteCount": 4,
        "content": "First Health check must be done after proper boot of VM."
      },
      {
        "date": "2023-03-22T21:39:00.000Z",
        "voteCount": 2,
        "content": "Increase delay to check all instances are available"
      },
      {
        "date": "2023-02-19T12:42:00.000Z",
        "voteCount": 8,
        "content": "Answer D is the correct solution to maintain instance group sizes when autoscaling.\n\nWhen autoscaling is enabled, new instances are added based on a metric or metrics (such as CPU utilization) when certain thresholds are met. When adding new instances, it is important to ensure that only the necessary number of instances are added to the instance group and that the group size is properly maintained to prevent overprovisioning and unnecessary costs.\n\nIn this scenario, the instance group is adding more instances than necessary when autoscaling due to the initial delay of HTTP health checks. Increasing the initial delay to 200 seconds will ensure that the health check properly reflects the actual availability of the instances and prevent overprovisioning.\n\nAnswers A and B limit the maximum number of instances, which could cause issues when scaling to support higher levels of end-user traffic.\n\nAnswer C suggests using a TCP health check instead of an HTTP health check, but it does not address the issue of overprovisioning when autoscaling."
      },
      {
        "date": "2022-10-23T08:17:00.000Z",
        "voteCount": 1,
        "content": "Increase the initial delay"
      },
      {
        "date": "2022-09-12T09:43:00.000Z",
        "voteCount": 1,
        "content": "D is correct answer."
      },
      {
        "date": "2022-08-06T09:01:00.000Z",
        "voteCount": 1,
        "content": "Increase initial delay"
      },
      {
        "date": "2022-07-02T21:08:00.000Z",
        "voteCount": 1,
        "content": "D is correct one"
      },
      {
        "date": "2022-06-22T13:25:00.000Z",
        "voteCount": 1,
        "content": "D is more appropriate for this question"
      },
      {
        "date": "2022-05-31T18:21:00.000Z",
        "voteCount": 1,
        "content": "Go for D"
      },
      {
        "date": "2022-05-26T03:32:00.000Z",
        "voteCount": 1,
        "content": "Go for D"
      },
      {
        "date": "2022-03-18T04:38:00.000Z",
        "voteCount": 2,
        "content": "logically first 3 options are out of context, so d is right"
      },
      {
        "date": "2021-11-19T00:05:00.000Z",
        "voteCount": 2,
        "content": "D. Increase the initial delay of the HTTP health check to 200 seconds."
      },
      {
        "date": "2021-11-18T16:01:00.000Z",
        "voteCount": 1,
        "content": "Ans is D"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 44,
    "url": "https://www.examtopics.com/discussions/google/view/19510-exam-associate-cloud-engineer-topic-1-question-44-discussion/",
    "body": "You need to select and configure compute resources for a set of batch processing jobs. These jobs take around 2 hours to complete and are run nightly. You want to minimize service costs. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSelect Google Kubernetes Engine. Use a single-node cluster with a small instance type.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSelect Google Kubernetes Engine. Use a three-node cluster with micro instance types.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSelect Compute Engine. Use preemptible VM instances of the appropriate standard machine type.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSelect Compute Engine. Use VM instance types that support micro bursting."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-10-14T10:55:00.000Z",
        "voteCount": 63,
        "content": "As everyone has said the answer is C but here is the source for the information. \"For example, batch processing jobs can run on preemptible instances. If some of those instances stop during processing, the job slows but does not completely stop. Preemptible instances complete your batch processing tasks without placing additional workload on your existing instances and without requiring you to pay full price for additional normal instances.\"\n\nsrouce: https://cloud.google.com/compute/docs/instances/preemptible"
      },
      {
        "date": "2020-05-02T21:27:00.000Z",
        "voteCount": 29,
        "content": "I woud say C is the correct answer"
      },
      {
        "date": "2024-05-23T11:40:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer: C"
      },
      {
        "date": "2023-09-01T10:28:00.000Z",
        "voteCount": 1,
        "content": "c is the correct answer, use preemptible for the compute engine"
      },
      {
        "date": "2023-02-19T12:53:00.000Z",
        "voteCount": 7,
        "content": "ANSWER C. Select Compute Engine. Use preemptible VM instances of the appropriate standard machine type.\n\nPreemptible VM instances offer the lowest cost for batch processing jobs in the Google Cloud Platform. Preemptible VM instances are computed instances that can run for a maximum of 24 hours and provide no availability guarantees. Preemptible VM instances are up to 80% cheaper than standard compute instances, making them an excellent choice for batch-processing workloads that can be interrupted.\n\nThe small instance type in a single-node cluster (ANSWER A) would not provide enough resources for batch processing jobs, and the micro instance types in a three-node cluster (ANSWER B) may not provide enough resources for the batch processing jobs to complete within the allotted time. VM instance types that support micro-bursting (ANSWER D) may not provide enough sustained CPU performance to complete batch processing jobs within the desired time frame."
      },
      {
        "date": "2022-09-07T02:44:00.000Z",
        "voteCount": 3,
        "content": "batch processing jobs can run on preemptible instances. if some of those instances stop during processing, the job slows but does not completely stop. preemptible instances camplete your batch processing tasks without placing additional worklods  on your existing instances and without requring you to pay full price for additional normal instances\""
      },
      {
        "date": "2022-08-20T02:49:00.000Z",
        "voteCount": 3,
        "content": "hey guys tell me one important thing i am learning GCP but did not get anything i mean whatever you guys are discussing in this forum"
      },
      {
        "date": "2022-12-07T04:10:00.000Z",
        "voteCount": 2,
        "content": "as you said you are learning, it takes time to master"
      },
      {
        "date": "2022-06-22T13:21:00.000Z",
        "voteCount": 3,
        "content": "C is right .\n\nIf your apps are fault-tolerant and can withstand possible instance preemptions, then preemptible instances can reduce your Compute Engine costs significantly. For example, batch processing jobs can run on preemptible instances. If some of those instances stop during processing, the job slows but does not completely stop. Preemptible instances complete your batch processing tasks without placing additional workload on your existing instances and without requiring you to pay full price for additional normal instances."
      },
      {
        "date": "2022-06-02T08:51:00.000Z",
        "voteCount": 3,
        "content": "Yup, C for batch and cost"
      },
      {
        "date": "2022-05-31T18:23:00.000Z",
        "voteCount": 1,
        "content": "Go for C"
      },
      {
        "date": "2022-05-26T03:34:00.000Z",
        "voteCount": 1,
        "content": "Go for C"
      },
      {
        "date": "2022-01-10T06:22:00.000Z",
        "voteCount": 3,
        "content": "I read that C is the right answer, but the question doesn't say that batch can be stopped and restarted."
      },
      {
        "date": "2022-02-10T23:18:00.000Z",
        "voteCount": 1,
        "content": "Batch will not be stopped and load will be shifted to another instances."
      },
      {
        "date": "2021-11-19T00:05:00.000Z",
        "voteCount": 3,
        "content": "C. Select Compute Engine. Use preemptible VM instances of the appropriate standard machine type."
      },
      {
        "date": "2021-11-18T16:02:00.000Z",
        "voteCount": 3,
        "content": "Ans is C"
      },
      {
        "date": "2021-09-01T02:21:00.000Z",
        "voteCount": 2,
        "content": "C - corectly"
      },
      {
        "date": "2021-06-12T10:15:00.000Z",
        "voteCount": 3,
        "content": "Option C is the correct answer"
      },
      {
        "date": "2021-05-11T21:21:00.000Z",
        "voteCount": 1,
        "content": "C is correct...For cost-saving &amp; not immediate fault-tolerant workloads like batch jobs use Preemptible VM instances"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 45,
    "url": "https://www.examtopics.com/discussions/google/view/16707-exam-associate-cloud-engineer-topic-1-question-45-discussion/",
    "body": "You recently deployed a new version of an application to App Engine and then discovered a bug in the release. You need to immediately revert to the prior version of the application. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun gcloud app restore.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOn the App Engine page of the GCP Console, select the application that needs to be reverted and click Revert.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOn the App Engine Versions page of the GCP Console, route 100% of the traffic to the previous version.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the original version as a separate application. Then go to App Engine settings and split traffic between applications so that the original version serves 100% of the requests."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 32,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-03-15T14:59:00.000Z",
        "voteCount": 77,
        "content": "correct is C NOT D.\nOption A is wrong as gcloud app restore was used for backup and restore and has been deprecated.Option B is wrong as there is no application revert functionality available.Option D is wrong as App Engine maintains version and need not be redeployed."
      },
      {
        "date": "2020-03-23T21:38:00.000Z",
        "voteCount": 29,
        "content": "App engine maintains versions and to revert back to previous version, traffic can be set to 100% for the prior version.. hence correct answer is C"
      },
      {
        "date": "2024-09-22T12:48:00.000Z",
        "voteCount": 1,
        "content": "C  is the correct answer"
      },
      {
        "date": "2023-11-01T02:48:00.000Z",
        "voteCount": 1,
        "content": "C is correct because app engine maintains version"
      },
      {
        "date": "2023-09-01T10:29:00.000Z",
        "voteCount": 1,
        "content": "c is the correct answer"
      },
      {
        "date": "2023-08-30T10:14:00.000Z",
        "voteCount": 1,
        "content": "Correct option is C."
      },
      {
        "date": "2023-08-29T01:08:00.000Z",
        "voteCount": 1,
        "content": "App Engine Version page and route 100% to the previous version"
      },
      {
        "date": "2023-08-09T09:16:00.000Z",
        "voteCount": 1,
        "content": "C is faster. Stick with C"
      },
      {
        "date": "2023-03-22T21:42:00.000Z",
        "voteCount": 1,
        "content": "App engine allows versioning"
      },
      {
        "date": "2023-02-19T13:30:00.000Z",
        "voteCount": 7,
        "content": "ANSWER C. On the App Engine Versions page of the GCP Console, route 100% of the traffic to the previous version.\n\nTo immediately revert to the prior version of an application in App Engine, you can route 100% of the traffic to the previous version. This can be done through the App Engine Versions page of the GCP Console by selecting the desired version and selecting \"Migrate traffic\" and moving the slider to 100%. This will ensure that all traffic is directed to the prior version until the bug is fixed and the new version can be safely redeployed.\n\nhttps://cloud.google.com/appengine/docs/flexible/migrating-traffic\n\nANSWER A (Run gcloud app restore) and ANSWER B (Click Revert on GCP Console) are not valid actions to revert to the prior version of the application. ANSWER D (Deploy the original version as a separate application) is not necessary and would complicate the environment by requiring a split traffic configuration."
      },
      {
        "date": "2022-12-27T14:43:00.000Z",
        "voteCount": 1,
        "content": "NO NEED OF REDEPLOY"
      },
      {
        "date": "2022-12-21T02:15:00.000Z",
        "voteCount": 2,
        "content": "which ans need to take most voted or correct option pls suggest"
      },
      {
        "date": "2024-05-16T02:01:00.000Z",
        "voteCount": 1,
        "content": "The most voted"
      },
      {
        "date": "2022-12-05T09:09:00.000Z",
        "voteCount": 1,
        "content": "correct is C"
      },
      {
        "date": "2022-10-24T07:11:00.000Z",
        "voteCount": 2,
        "content": "route 100% of the traffic to the previous version"
      },
      {
        "date": "2022-09-08T10:09:00.000Z",
        "voteCount": 2,
        "content": "Correct option is C.\nAppEngine already creates a version for you. Also you do not create a application as one project is associated with one AppEngine application."
      },
      {
        "date": "2022-08-11T05:00:00.000Z",
        "voteCount": 1,
        "content": "App engine is a version control tool for your running app"
      },
      {
        "date": "2022-07-02T07:21:00.000Z",
        "voteCount": 1,
        "content": "Answer is C"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 46,
    "url": "https://www.examtopics.com/discussions/google/view/16708-exam-associate-cloud-engineer-topic-1-question-46-discussion/",
    "body": "You deployed an App Engine application using gcloud app deploy, but it did not deploy to the intended project. You want to find out why this happened and where the application deployed. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCheck the app.yaml file for your application and check project settings.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCheck the web-application.xml file for your application and check project settings.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGo to Deployment Manager and review settings for deployment of applications.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGo to Cloud Shell and run gcloud config list to review the Google Cloud configuration used for deployment.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 31,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-03-23T21:40:00.000Z",
        "voteCount": 57,
        "content": "I would opt option D : as it would help to check the config details and Option A is not correct, as app.yaml would have only the runtime and script to run parameters and not the Project details"
      },
      {
        "date": "2021-09-27T07:32:00.000Z",
        "voteCount": 2,
        "content": "Why would you choose Cloud Shell if its not even mention on the question? (what if the person did the command on its own computer?, this would not work)"
      },
      {
        "date": "2024-07-31T08:30:00.000Z",
        "voteCount": 1,
        "content": "The first line of the question says: \"You deployed an App Engine application using gcloud app deploy\""
      },
      {
        "date": "2021-10-17T01:18:00.000Z",
        "voteCount": 13,
        "content": "gcloud app deploy means sdk"
      },
      {
        "date": "2022-07-05T14:03:00.000Z",
        "voteCount": 1,
        "content": "Regardless if you use your computer or cloud shell, you have to use SDK for gcloud command-line interface. gcloud uses a configuration file which contains default project, region and zone details so that command line can omit these parameters and use default."
      },
      {
        "date": "2020-04-05T06:07:00.000Z",
        "voteCount": 39,
        "content": "Option D - The config list will give the name of the project\nC:\\GCP\\appeng&gt;gcloud config list\n[core]\naccount = xxx@gmail.com\ndisable_usage_reporting = False\nproject = my-first-demo-xxxx"
      },
      {
        "date": "2024-09-22T12:53:00.000Z",
        "voteCount": 1,
        "content": "D is the correct answer."
      },
      {
        "date": "2023-11-22T08:07:00.000Z",
        "voteCount": 1,
        "content": "A\napp.yaml will have the project details."
      },
      {
        "date": "2024-09-17T14:52:00.000Z",
        "voteCount": 1,
        "content": "nope, app.yaml has only runtime and script to run the parameters"
      },
      {
        "date": "2023-11-01T02:49:00.000Z",
        "voteCount": 1,
        "content": "D is correct because gcloud config list will give you the current project name &amp; rest all options talks about examining the yaml file which is not a best practice"
      },
      {
        "date": "2023-09-01T10:31:00.000Z",
        "voteCount": 2,
        "content": "option d as it will give you full information why it dont get deployed to the intended project"
      },
      {
        "date": "2023-08-03T06:26:00.000Z",
        "voteCount": 6,
        "content": "Option D is the appropriate choice for diagnosing why the App Engine application did not deploy to the intended project. By running gcloud config list in Cloud Shell, you can view the current configuration settings, including the project ID, region, and other relevant settings used for deployment.\nOptions A and B involve checking the configuration files for the application (app.yaml and web-application.xml), but they may not directly provide information about where the application deployed or why it didn't deploy to the intended project.\nOption C involves Deployment Manager, which is a tool for creating, deploying, and managing resources in Google Cloud Platform, but it's not specifically related to App Engine deployments and may not provide the necessary insights in this context."
      },
      {
        "date": "2023-05-01T03:54:00.000Z",
        "voteCount": 1,
        "content": "D is CORRECT"
      },
      {
        "date": "2023-03-09T03:51:00.000Z",
        "voteCount": 1,
        "content": "I VOTE FOR D"
      },
      {
        "date": "2023-02-19T13:43:00.000Z",
        "voteCount": 9,
        "content": "ANSWER D. CORRECT. Go to Cloud Shell and run gcloud config list to review the Google Cloud configuration used for deployment.\n\nRunning gcloud config list in the Cloud Shell will show the currently active configuration that was used for the deployment. This can help identify if the wrong project was selected or if the configuration was set up incorrectly.\n\nhttps://cloud.google.com/sdk/gcloud/reference/config/list\n\nANSWER A may be helpful to ensure that the project and deployment settings are correctly specified, but it does not provide information on where the application was actually deployed.\nANSWER B is not relevant for App Engine deployments as this is an XML configuration file typically used in Java web applications deployed to servlet containers.\nANSWER C is also not relevant for App Engine deployments, as Deployment Manager is typically used to create and manage deployments of cloud infrastructure resources such as virtual machines, load balancers, and databases."
      },
      {
        "date": "2022-12-21T02:11:00.000Z",
        "voteCount": 1,
        "content": "which and i need to prefer correct or voted"
      },
      {
        "date": "2022-12-05T09:19:00.000Z",
        "voteCount": 1,
        "content": "option D"
      },
      {
        "date": "2022-10-24T07:35:00.000Z",
        "voteCount": 1,
        "content": "D. check project setting by gcloud config list"
      },
      {
        "date": "2022-09-08T10:21:00.000Z",
        "voteCount": 9,
        "content": "Both A and D seem correct. \nLots of people mentioned that app.yaml does not contain project id. That is an incorrect statement. Project is contained in app.YAML for \"Standard\" app engine application but gcloud config list is has to be used for \"FLEXIBLE\" application. Since the questions does not inform us whether it is a standard or flexible app, Option D is correct. Look at the link here. https://cloud.google.com/appengine/docs/flexible/python/reference/app-yaml#app-id .\n\nThe doc states:\n\"In some App Engine standard environment runtimes, you might have specified the Cloud Platform project ID (sometimes called \"app ID\") in the project's app.yaml file.\n\nHowever, in the flexible environment, the project ID (app ID) is specified either:\n\n    By using gcloud init when you install the Google Cloud CLI. To view the default project ID of the gcloud CLI, run gcloud config list.\n    By using the gcloud config set project [YOUR_PROJECT_ID] command to set the default project ID of the gcloud CLI.\n    By using the --project flag when you deploy your app, for example: gcloud app deploy --project [YOUR_PROJECT_ID]\n\""
      },
      {
        "date": "2022-08-20T03:05:00.000Z",
        "voteCount": 2,
        "content": "Why C is incorrect"
      },
      {
        "date": "2022-09-22T14:37:00.000Z",
        "voteCount": 1,
        "content": "App Engine Does not use deployment manager"
      },
      {
        "date": "2022-08-09T20:10:00.000Z",
        "voteCount": 1,
        "content": "D\nhttps://cloud.google.com/endpoints/docs/openapi/troubleshoot-gce-deployment"
      },
      {
        "date": "2022-07-02T07:21:00.000Z",
        "voteCount": 1,
        "content": "Answer is D"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 47,
    "url": "https://www.examtopics.com/discussions/google/view/46362-exam-associate-cloud-engineer-topic-1-question-47-discussion/",
    "body": "You want to configure 10 Compute Engine instances for availability when maintenance occurs. Your requirements state that these instances should attempt to automatically restart if they crash. Also, the instances should be highly available including during system maintenance. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an instance template for the instances. Set the 'Automatic Restart' to on. Set the 'On-host maintenance' to Migrate VM instance. Add the instance template to an instance group.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an instance template for the instances. Set 'Automatic Restart' to off. Set 'On-host maintenance' to Terminate VM instances. Add the instance template to an instance group.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an instance group for the instances. Set the 'Autohealing' health check to healthy (HTTP).",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an instance group for the instance. Verify that the 'Advanced creation options' setting for 'do not retry machine creation' is set to off."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 38,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-03-11T19:24:00.000Z",
        "voteCount": 43,
        "content": "A\nhttps://cloud.google.com/compute/docs/instances/setting-instance-scheduling-options\n\nonHostMaintenance: Determines the behavior when a maintenance event occurs that might cause your instance to reboot.\n\n[Default] MIGRATE, which causes Compute Engine to live migrate an instance when there is a maintenance event.\nTERMINATE, which stops an instance instead of migrating it.\nautomaticRestart: Determines the behavior when an instance crashes or is stopped by the system.\n\n[Default] true, so Compute Engine restarts an instance if the instance crashes or is stopped.\nfalse, so Compute Engine does not restart an instance if the instance crashes or is stopped."
      },
      {
        "date": "2021-10-11T05:20:00.000Z",
        "voteCount": 28,
        "content": "Seems like it was a very obvious option i.e. A...Who selected B, I want to know his/her location?"
      },
      {
        "date": "2021-11-18T11:58:00.000Z",
        "voteCount": 4,
        "content": "Nikki singh."
      },
      {
        "date": "2022-12-13T22:59:00.000Z",
        "voteCount": 4,
        "content": "lol, yeah i want to examine their brain as well"
      },
      {
        "date": "2024-09-03T12:15:00.000Z",
        "voteCount": 1,
        "content": "You don't even need to set anything because 'Automatic Restart' = 'On' and 'On-host maintenance' = 'Migrate' will be set BY DEFAULT!"
      },
      {
        "date": "2024-08-15T00:19:00.000Z",
        "voteCount": 1,
        "content": "B IS correct since if you set automatic restart to on , then your instance would be shut down during maintenance event, which Cancels the migrate on maintenance event setting that IS required for availibility purpose."
      },
      {
        "date": "2024-07-10T10:00:00.000Z",
        "voteCount": 1,
        "content": "A looks obvious. B doesnt sound correct"
      },
      {
        "date": "2023-12-31T03:13:00.000Z",
        "voteCount": 4,
        "content": "simple... \nAutomatic Restart ON vs OFF (obvious ON), \nOn Host Maintainence MIGRATE vs TERMINATE (really??!)"
      },
      {
        "date": "2023-12-21T18:28:00.000Z",
        "voteCount": 1,
        "content": "A is correct because automatic restart will restart the instance if it crashes and setting on host maintenance to migrate the instance will not let the application go down during maintenance. It fulfills the requirements of automatically restarting the instances if they crash and ensuring that they are not lost during system maintenance activity. By setting the 'Automatic Restart' to on, the instances will attempt to automatically restart if they crash. By setting the 'On-host maintenance' to Migrate VM instance, the instances will be migrated to another host during system maintenance, preventing any downtime."
      },
      {
        "date": "2023-11-01T02:52:00.000Z",
        "voteCount": 1,
        "content": "A is correct because you need HA of VMs during mainetence"
      },
      {
        "date": "2023-10-14T01:43:00.000Z",
        "voteCount": 4,
        "content": "1\nThe best answer is A. Create an instance template for the instances. Set the 'Automatic Restart' to on. Set the 'On-host maintenance' to Migrate VM instance. Add the instance template to an instance group.\n\nThis will ensure that your instances are automatically restarted if they crash and that they are migrated during system maintenance, which will keep them highly available.\n\nThe other options are not as effective:\n\nOption B is not as effective because it will prevent your instances from being automatically restarted if they crash.\nOption C is not as effective because it will not migrate your instances during system maintenance, which could lead to downtime.\nOption D is not as effective because it does not guarantee that your instances will be automatically migrated during system maintenance."
      },
      {
        "date": "2023-09-13T06:57:00.000Z",
        "voteCount": 1,
        "content": "I choose A"
      },
      {
        "date": "2023-09-01T10:33:00.000Z",
        "voteCount": 1,
        "content": "A seems more legit rightt ?"
      },
      {
        "date": "2023-08-09T09:19:00.000Z",
        "voteCount": 1,
        "content": "The correct option is:\n\nA. Create an instance template for the instances. Set the 'Automatic Restart' to on. Set the 'On-host maintenance' to Migrate VM instance. Add the instance template to an instance group.\n\nExplanation:\nThe scenario requires configuring Compute Engine instances for availability during maintenance while also enabling automatic restart if they crash. Google Cloud Platform provides features and settings to achieve high availability and automatic restart for instances.\n\nOption A outlines the appropriate steps"
      },
      {
        "date": "2023-05-01T03:53:00.000Z",
        "voteCount": 1,
        "content": "Create an instance template for the instances. Set the 'Automatic Restart' to on. Set the 'On-host maintenance' to Migrate VM instance. Add the instance template to an instance group."
      },
      {
        "date": "2023-04-04T09:53:00.000Z",
        "voteCount": 1,
        "content": "A is correct for sure -  yt-  the Technobie"
      },
      {
        "date": "2023-03-29T10:10:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2023-03-09T03:53:00.000Z",
        "voteCount": 1,
        "content": "I vote for D"
      },
      {
        "date": "2023-02-19T13:50:00.000Z",
        "voteCount": 8,
        "content": "ANSWER A. CORRECT. Create an instance template for the instances. Set the 'Automatic Restart' to on. Set the 'On-host maintenance' to Migrate VM instance. Add the instance template to an instance group.\n\nTo ensure the availability of instances during maintenance and restart instances in the event of a crash, you should create an instance group with an instance template that specifies 'Automatic Restart' to on. This will allow your instance to restart in the event of a crash. \n\nAdditionally, you should set the 'On-host maintenance' to Migrate VM instance so that VM instances are live migrated to another host in the event of an infrastructure maintenance event. \n\nThe instance group should have a health check configured to verify that the instances are healthy. By using an instance group, you can also take advantage of the autoscaling and load-balancing capabilities that come with instance groups."
      },
      {
        "date": "2023-03-19T08:38:00.000Z",
        "voteCount": 3,
        "content": "Hi Buru, as always, amazing explanation. many thanks!!"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 48,
    "url": "https://www.examtopics.com/discussions/google/view/21294-exam-associate-cloud-engineer-topic-1-question-48-discussion/",
    "body": "You host a static website on Cloud Storage. Recently, you began to include links to PDF files on this site. Currently, when users click on the links to these PDF files, their browsers prompt them to save the file onto their local system. Instead, you want the clicked PDF files to be displayed within the browser window directly, without prompting the user to save the file locally. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Cloud CDN on the website frontend.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable 'Share publicly' on the PDF file objects.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet Content-Type metadata to application/pdf on the PDF file objects.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a label to the storage bucket with a key of Content-Type and value of application/pdf."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 40,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-02-19T13:55:00.000Z",
        "voteCount": 34,
        "content": "ANSWER A, enabling Cloud CDN on the website frontend, is not relevant to displaying PDF files in the browser. Cloud CDN is a content delivery network that caches content at edge locations around the world to reduce latency and improve website performance.\n\nANSWER B, enabling \"Share publicly\" on the PDF file objects, only controls whether or not the files are accessible to users without authentication. It does not affect how the files are displayed in the browser.\n\nANSWER D, adding a label to the storage bucket with a key of Content-Type and value of application/pdf, is not the correct way to set the Content-Type metadata for individual objects. Labels are used for organizing resources, while metadata is used to provide information about the data itself.\n\nTherefore, ANSWER C, setting Content-Type metadata to application/pdf on the PDF file objects, is the correct answer."
      },
      {
        "date": "2023-03-21T13:49:00.000Z",
        "voteCount": 2,
        "content": "Many thanks for clear explanations! :)"
      },
      {
        "date": "2020-05-25T03:15:00.000Z",
        "voteCount": 25,
        "content": "C is correct"
      },
      {
        "date": "2024-01-07T17:13:00.000Z",
        "voteCount": 1,
        "content": "C makes much sense out of the remaining"
      },
      {
        "date": "2023-09-01T10:35:00.000Z",
        "voteCount": 1,
        "content": "answer is c , as other are not relevant"
      },
      {
        "date": "2023-08-30T10:52:00.000Z",
        "voteCount": 7,
        "content": "C. Set Content-Type metadata to application/pdf on the PDF file objects.\n\nExplanation: The Content-Type metadata indicates the media type of the content and helps the browser understand how to handle the file. In this case, by setting the Content-Type metadata of the PDF files to \"application/pdf,\" you're informing the browser that the files are in PDF format, and the browser will attempt to display them directly within the browser window, rather than prompting the user to download them."
      },
      {
        "date": "2023-08-03T06:30:00.000Z",
        "voteCount": 1,
        "content": "C is correct\nSetting the Content-Type metadata to application/pdf on the PDF file objects instructs the web browser to treat these files as PDF documents and display them inline, rather than prompting the user to download them."
      },
      {
        "date": "2022-12-09T05:01:00.000Z",
        "voteCount": 13,
        "content": "Option C:\nTo display PDF files directly within the browser window on a website hosted on Cloud Storage, you can follow these steps:\n\nIn the Google Cloud Console, navigate to the Cloud Storage section and select the \"Buckets\" page.\n\nSelect the bucket that contains the static website and the PDF files.\n\nFrom the \"Actions\" menu, select \"Edit bucket\" and then go to the \"Website\" tab.\n\nIn the \"Website Configuration\" section, select the \"Serve objects with this content type\" option and enter \"application/pdf\" in the text field. This will cause PDF files to be served with the correct content type.\n\nSave the changes to the bucket configuration.\n\nAfter completing these steps, the PDF files on your website will be served with the correct content type and will be displayed directly within the browser window when clicked, without prompting the user to save the file locally."
      },
      {
        "date": "2022-10-24T07:42:00.000Z",
        "voteCount": 1,
        "content": "C. Set Content-Type metadata to application/pdf"
      },
      {
        "date": "2022-10-21T04:51:00.000Z",
        "voteCount": 1,
        "content": "FYI\nImportance of setting the correct MIME type\nhttps://developer.mozilla.org/en-US/docs/Web/HTTP/Basics_of_HTTP/MIME_Types#importance_of_setting_the_correct_mime_type"
      },
      {
        "date": "2022-06-22T13:11:00.000Z",
        "voteCount": 3,
        "content": "C is correct. Edit the PDF objects in Cloud Storage and reconfigure their Content-Type metadata into application/pdf."
      },
      {
        "date": "2022-05-28T10:24:00.000Z",
        "voteCount": 2,
        "content": "Ans: C"
      },
      {
        "date": "2022-01-29T11:50:00.000Z",
        "voteCount": 1,
        "content": "C IS CORRECT"
      },
      {
        "date": "2021-11-19T00:09:00.000Z",
        "voteCount": 1,
        "content": "C. Set Content-Type metadata to application/pdf on the PDF file objects."
      },
      {
        "date": "2021-11-18T16:10:00.000Z",
        "voteCount": 1,
        "content": "Ans - C"
      },
      {
        "date": "2021-06-19T16:20:00.000Z",
        "voteCount": 11,
        "content": "C is correct.\nhttps://cloud.google.com/storage/docs/metadata#content-type"
      },
      {
        "date": "2021-06-10T02:55:00.000Z",
        "voteCount": 1,
        "content": "c is correct"
      },
      {
        "date": "2021-03-24T13:56:00.000Z",
        "voteCount": 1,
        "content": "C is correct. Set Content-Type metadata to application/pdf on the PDF file objects."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 49,
    "url": "https://www.examtopics.com/discussions/google/view/16710-exam-associate-cloud-engineer-topic-1-question-49-discussion/",
    "body": "You have a virtual machine that is currently configured with 2 vCPUs and 4 GB of memory. It is running out of memory. You want to upgrade the virtual machine to have 8 GB of memory. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRely on live migration to move the workload to a machine with more memory.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse gcloud to add metadata to the VM. Set the key to required-memory-size and the value to 8 GB.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStop the VM, change the machine type to n1-standard-8, and start the VM.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStop the VM, increase the memory to 8 GB, and start the VM.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 21,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-03-22T09:11:00.000Z",
        "voteCount": 65,
        "content": "coldpar, why are you getting the people confused? you need to stop teh VM and modify the RAM, that's all"
      },
      {
        "date": "2021-10-13T19:14:00.000Z",
        "voteCount": 7,
        "content": "who is coldpar"
      },
      {
        "date": "2022-07-21T22:50:00.000Z",
        "voteCount": 1,
        "content": "He deleted his comment"
      },
      {
        "date": "2023-04-16T07:49:00.000Z",
        "voteCount": 2,
        "content": "i dont think we can delete our comments"
      },
      {
        "date": "2020-04-10T05:22:00.000Z",
        "voteCount": 44,
        "content": "D is correct. If you pay attention to the question, option C mentions n1-standard-8. That instance type has 8vCPUs and 30 GB RAM, and we only need 8GB. On top of that, it is possible to use custom machine type to adjust current VM RAM to the value we need. Got the answer from this course I did to prepare the exam: https://www.udemy.com/course/google-cloud-associate-engineer-exam-practice-tests/?couponCode=21CDE6A4C2B95F79BD97\ngood luck!"
      },
      {
        "date": "2022-06-25T03:38:00.000Z",
        "voteCount": 2,
        "content": "How to use coupon code carls"
      },
      {
        "date": "2023-09-01T10:36:00.000Z",
        "voteCount": 1,
        "content": "D is the correct answer"
      },
      {
        "date": "2023-08-16T04:49:00.000Z",
        "voteCount": 2,
        "content": "You can add extended memory only to custom machine types. Predefined machine types are not supported."
      },
      {
        "date": "2023-03-26T07:00:00.000Z",
        "voteCount": 2,
        "content": "D is correct. \nC.n1 standard8 has 30GB RAM.\nA&amp;B- No vm instance stop, Hence can't be updated."
      },
      {
        "date": "2023-02-19T14:06:00.000Z",
        "voteCount": 16,
        "content": "ANSWER D is correct because it is the correct process to follow to increase the memory of a virtual machine in the Google Cloud Platform.\n\nTo increase the memory of a virtual machine, you need to first stop the VM, since it is not possible to modify the memory of a running VM. Then, you can increase the memory of the VM by editing the machine type and selecting a machine type with more memory. Once you have made the change, you can start the VM again.\n\nANSWER A is not the best approach as it relies on live migration which can be a risky operation.\n\nANSWER B is incorrect because adding metadata to the VM will not change the amount of memory allocated to the VM.\n\nANSWER C is incorrect because changing the machine type to n1-standard-8 would also increase the number of vCPUs to 8, which may not be necessary and could result in overprovisioning of resources. In addition, changing the machine type would also affect the cost of the VM instance, which may not be desired. Since the primary concern in this scenario is to increase memory."
      },
      {
        "date": "2022-12-05T10:43:00.000Z",
        "voteCount": 1,
        "content": "Option D"
      },
      {
        "date": "2022-10-24T07:53:00.000Z",
        "voteCount": 2,
        "content": "C is incorrect because  the 8 in \"n1-standard-8\" means 8 cpus instead of 8 gb of ram"
      },
      {
        "date": "2022-08-02T06:43:00.000Z",
        "voteCount": 1,
        "content": "D is right one"
      },
      {
        "date": "2022-06-01T03:21:00.000Z",
        "voteCount": 1,
        "content": "Go for D"
      },
      {
        "date": "2022-03-30T17:53:00.000Z",
        "voteCount": 5,
        "content": "The reason A is not correct is because live migration simply moves an existing VM between hosts, no attributes or properties are changed otherwise. Hence, you cannot live migrate from 1 VM type to another.\nhttps://cloud.google.com/compute/docs/instances/live-migration"
      },
      {
        "date": "2021-12-20T17:08:00.000Z",
        "voteCount": 2,
        "content": "To be exact... There is no \"n1\" option that has 2vCPU and 4GB. The closest would be n1-standard-2 with 2vCPU and 7.50GB RAM.  This machine is already custom. So, option D."
      },
      {
        "date": "2021-11-19T00:09:00.000Z",
        "voteCount": 5,
        "content": "D. Stop the VM, increase the memory to 8 GB, and start the VM."
      },
      {
        "date": "2021-11-18T16:42:00.000Z",
        "voteCount": 1,
        "content": "Ans - D"
      },
      {
        "date": "2021-08-06T00:17:00.000Z",
        "voteCount": 2,
        "content": "We do not have the option to increase memory in GCP VM directly. Instead, machine type needs to be changed. However, given the choices, D is the best answer here."
      },
      {
        "date": "2021-07-30T03:20:00.000Z",
        "voteCount": 4,
        "content": "D is correct \nhttps://cloud.google.com/compute/docs/instances/creating-instance-with-custom-machine-type#add_extended_memory_to_an_existing_vm_instance"
      },
      {
        "date": "2021-06-06T04:57:00.000Z",
        "voteCount": 4,
        "content": "why do we want to stop the VM when live migration is an option I would go for Option A"
      },
      {
        "date": "2021-07-24T00:38:00.000Z",
        "voteCount": 1,
        "content": "I don't find how to manually initiate a live migration, its apparently a google managed process"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 50,
    "url": "https://www.examtopics.com/discussions/google/view/17484-exam-associate-cloud-engineer-topic-1-question-50-discussion/",
    "body": "You have production and test workloads that you want to deploy on Compute Engine. Production VMs need to be in a different subnet than the test VMs. All the<br>VMs must be able to reach each other over Internal IP without creating additional routes. You need to set up VPC and the 2 subnets. Which configuration meets these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a single custom VPC with 2 subnets. Create each subnet in a different region and with a different CIDR range.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a single custom VPC with 2 subnets. Create each subnet in the same region and with the same CIDR range.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate 2 custom VPCs, each with a single subnet. Create each subnet in a different region and with a different CIDR range.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate 2 custom VPCs, each with a single subnet. Create each subnet in the same region and with the same CIDR range."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 46,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-03-25T08:17:00.000Z",
        "voteCount": 35,
        "content": "A is correct"
      },
      {
        "date": "2020-10-26T01:25:00.000Z",
        "voteCount": 22,
        "content": "Vote A\nhttps://cloud.google.com/vpc/docs/using-vpc#subnet-rules\nPrimary and secondary ranges for subnets cannot overlap with any allocated range, any primary or secondary range of another subnet in the same network, or any IP ranges of subnets in peered networks."
      },
      {
        "date": "2024-05-11T04:16:00.000Z",
        "voteCount": 2,
        "content": "Wrong question with wrong answer"
      },
      {
        "date": "2023-09-25T04:08:00.000Z",
        "voteCount": 4,
        "content": "A is correct\nIf you create more than one subnet in a VPC, the CIDR blocks of the subnets cannot overlap. For example, if you create a VPC with CIDR block 10.0. 0.0/24 , it supports 256 IP addresses. You can break this CIDR block into two subnets, each supporting 128 IP addresses."
      },
      {
        "date": "2023-09-12T01:25:00.000Z",
        "voteCount": 7,
        "content": "For anyone new to the business, prod and test networks should never talk to each other....  The requirement in this question (that both envs can reach each other) is completely against best practice and common sense... There should always be complete network isolation between prod and non-prod environments."
      },
      {
        "date": "2024-09-03T12:23:00.000Z",
        "voteCount": 1,
        "content": "in reality, business leads will push you to make such a connection, for example, because the test environment doesn't have enough data for testing..."
      },
      {
        "date": "2023-09-06T23:49:00.000Z",
        "voteCount": 3,
        "content": "I tried to create a VPC with 2 subnets in same regione and same CIDR\nI got the following error\nOperation type [insert] failed with message \"Invalid IPCidrRange: 10.0.0.0/28 conflicts with existing subnetwork 'subnet-1' in region 'asia-east1'.\""
      },
      {
        "date": "2024-07-31T08:47:00.000Z",
        "voteCount": 1,
        "content": "\"same CIDR\" means the same range of addresses. You cannot have two networks overlapping anywhere because the IPs will conflict."
      },
      {
        "date": "2023-09-01T10:37:00.000Z",
        "voteCount": 1,
        "content": "A is correct as it help to make sure they have a diffenret subnets"
      },
      {
        "date": "2023-03-13T00:01:00.000Z",
        "voteCount": 5,
        "content": "A is the correct Answer. People voting for B need to improve their networking knowledge."
      },
      {
        "date": "2023-02-19T14:17:00.000Z",
        "voteCount": 14,
        "content": "ANSWER A meets the requirement because it creates a single custom VPC with 2 subnets, with each subnet in a different region and with a different CIDR range. This ensures that the production and test VMs are in separate subnets and that they can communicate with each other over Internal IP without creating additional routes. Since the subnets are in different regions, they will also have different internal routing tables, which can help isolate the traffic between the two subnets. This configuration provides the necessary network isolation and connectivity required by the production and test workloads.\n\nANSWER B suggests creating a single custom VPC with two subnets in the same region and with the same CIDR range. However, the requirement is that production VMs need to be in a different subnet than the test VMs. With the subnets in the same region and with the same CIDR range, it would not be possible to separate the production and test VMs into different subnets. Therefore, ANSWER B does not meet the requirement."
      },
      {
        "date": "2023-02-08T13:26:00.000Z",
        "voteCount": 2,
        "content": "https://www.youtube.com/watch?v=XLaFU1t9pM8\n\n8:15"
      },
      {
        "date": "2023-01-21T18:59:00.000Z",
        "voteCount": 2,
        "content": "I feel the Answer is A it has to be in the same VPC to talk to each other but on 2 different subnet but is there any sense to have it different region? It should be fine with different CIDR for same region i feel"
      },
      {
        "date": "2022-10-24T08:03:00.000Z",
        "voteCount": 1,
        "content": "A, same VPC network with different CIDR range"
      },
      {
        "date": "2022-10-18T03:55:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2022-10-13T07:54:00.000Z",
        "voteCount": 4,
        "content": "It has to be A: one VPC, two subnets in different regions and different CIDR range. (It would also be valid to have both subnets in the same region.)\n\nWhat about option B? We know that \"each primary or secondary IPv4 range for all subnets in a VPC network must be a unique valid CIDR block\" (read here https://cloud.google.com/vpc/docs/subnets#ipv4-ranges ). Thus, prod and test subnets cannot overlap -&gt; option B is not valid.\n\nOptions C and D are not valid neither, because \"all the VMs must be able to reach each other\" - this will not happen if we distribute the VMs across two VPC."
      },
      {
        "date": "2022-10-04T11:16:00.000Z",
        "voteCount": 1,
        "content": "Leutenant_Ololo, I test and checked. It is B answer"
      },
      {
        "date": "2023-01-29T08:53:00.000Z",
        "voteCount": 1,
        "content": "how do you want to have 2 subnets with same CIDR? Not only in GCP but anywhere..."
      },
      {
        "date": "2022-08-10T03:16:00.000Z",
        "voteCount": 4,
        "content": "Option A is incorrect, see what us says \"create each subnet in a different region and with a different CIDR range\" in this case routing is required due different CIDR range.\n\nOption B is correct, If you create 2 subnets in same CIDR you can communicate over internal IP without additional routing."
      },
      {
        "date": "2022-10-03T19:08:00.000Z",
        "voteCount": 2,
        "content": "routers will be created automatically. Just go create a new VPC with 2 subnets and then check routes. https://cloud.google.com/vpc/docs/routes#subnet-routes"
      },
      {
        "date": "2022-07-27T12:30:00.000Z",
        "voteCount": 4,
        "content": "Different regions is something odd, but the main reason why its A is cause the CIDR range.\nCIDR is the short for Classless Inter-Domain Routing. So, if we have 2 subnets, they CAN NOT BE the use the same CIDR. \n\nIPv4 subnet ranges\n\"Each primary or secondary IPv4 range for all subnets in a VPC network must be a unique valid CIDR block. Refer to the per network limits for the number of secondary IP ranges you can define.\"\nhttps://cloud.google.com/vpc/docs/vpc"
      },
      {
        "date": "2022-08-06T03:39:00.000Z",
        "voteCount": 3,
        "content": "You should understand that the VPC is a Global Resource. You can create a VPC and having subnets accross regions. This subnets could communicate to each other through their Private IP.\n\nhttps://cloud.google.com/sql/docs/mysql/private-ip#overview"
      },
      {
        "date": "2023-09-24T09:54:00.000Z",
        "voteCount": 1,
        "content": "It seems that it doesn't have to be different regions, but the CIDR range should be different.\nThat's the key"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 51,
    "url": "https://www.examtopics.com/discussions/google/view/17258-exam-associate-cloud-engineer-topic-1-question-51-discussion/",
    "body": "You need to create an autoscaling managed instance group for an HTTPS web application. You want to make sure that unhealthy VMs are recreated. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a health check on port 443 and use that when creating the Managed Instance Group.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSelect Multi-Zone instead of Single-Zone when creating the Managed Instance Group.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the Instance Template, add the label 'health-check'.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the Instance Template, add a startup script that sends a heartbeat to the metadata server."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 46,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 6,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-03-22T09:22:00.000Z",
        "voteCount": 66,
        "content": "I'll go with A, MIGs support autohealing, load balancing, autoscaling, and auto-updating. no the Images templates, this is set up in the MIG"
      },
      {
        "date": "2021-06-08T10:16:00.000Z",
        "voteCount": 14,
        "content": "The correct answer is A. Please, modify it."
      },
      {
        "date": "2024-09-23T02:22:00.000Z",
        "voteCount": 1,
        "content": "A is the correct answer."
      },
      {
        "date": "2024-03-16T00:32:00.000Z",
        "voteCount": 3,
        "content": "The correct answer is A. Create a health check on port 443 and use that when creating the Managed Instance Group.\n\nOption B is related to the availability and distribution of instances across multiple zones, but it does not directly address the requirement of recreating unhealthy VMs.\nOption C is incorrect because labels are not used for configuring health checks in GCP.\nOption D is an alternative method for health checking, but it is not as straightforward as using the built-in health check functionality provided by GCP for managed instance groups."
      },
      {
        "date": "2023-09-06T23:54:00.000Z",
        "voteCount": 2,
        "content": "Only A answer has some sense"
      },
      {
        "date": "2023-09-01T22:04:00.000Z",
        "voteCount": 1,
        "content": "443 means http A seems more correct"
      },
      {
        "date": "2023-08-09T09:29:00.000Z",
        "voteCount": 1,
        "content": "C is incomplete. A  all the way"
      },
      {
        "date": "2023-08-04T00:18:00.000Z",
        "voteCount": 2,
        "content": "Option A is the correct choice because it involves creating a health check specifically on port 443, which is the standard port for HTTPS traffic. This health check will monitor the health of the instances based on their ability to respond to HTTPS requests. When creating the Managed Instance Group, you would configure it to use this health check to determine the health of the instances\nOptions B, C, and D are not directly related to setting up proper health checks for autoscaling and ensuring unhealthy instances are recreated"
      },
      {
        "date": "2023-06-06T02:42:00.000Z",
        "voteCount": 1,
        "content": "A for A-Game let's goooo!"
      },
      {
        "date": "2023-05-01T04:30:00.000Z",
        "voteCount": 1,
        "content": "C is definitely incorrect. Adding a label does not recreate unhealthy VMs.\nA is CORRECT."
      },
      {
        "date": "2023-04-04T08:47:00.000Z",
        "voteCount": 1,
        "content": "As web application is on Https, so will set port 443 to check health of instance."
      },
      {
        "date": "2023-03-13T00:04:00.000Z",
        "voteCount": 2,
        "content": "A is the right answer.\nANSWER C is incorrect, adding a label to the instance template, has no direct impact on the ability to recreate unhealthy VMs."
      },
      {
        "date": "2023-02-19T20:39:00.000Z",
        "voteCount": 11,
        "content": "ANSWER A. Create a health check on port 443 and use that when creating the Managed Instance Group.\n\nTo ensure that unhealthy VMs are recreated, a health check should be created to monitor the instances in the managed instance group. This health check should be configured to check the appropriate endpoint for the web application, which in this case would be port 443 for HTTPS. If an instance is determined to be unhealthy, the instance group will automatically recreate it.\n\nINCORRECT:\n\nANSWER B is not directly related to recreating unhealthy VMs, but instead ensures that the instance group spans multiple zones for increased availability.\n\nANSWER C, adding a label to the instance template, has no direct impact on the ability to recreate unhealthy VMs.\n\nANSWER D, adding a startup script to send a heartbeat to the metadata server, can help detect and recover from application-level failures, but it does not directly ensure that unhealthy VMs are recreated."
      },
      {
        "date": "2022-12-13T22:54:00.000Z",
        "voteCount": 1,
        "content": "I go with A, default port is 443 for https"
      },
      {
        "date": "2022-10-27T15:17:00.000Z",
        "voteCount": 1,
        "content": "This shows the importance of reading exactly what it says.  The label can throw you off.   A is the answer."
      },
      {
        "date": "2022-10-24T08:16:00.000Z",
        "voteCount": 1,
        "content": "A obviously, port 443 for HTTPS application"
      },
      {
        "date": "2022-10-21T05:20:00.000Z",
        "voteCount": 2,
        "content": "A is correct.\nhttps://cloud.google.com/compute/docs/instance-groups/autohealing-instances-in-migs#setting_up_an_autohealing_policy"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 52,
    "url": "https://www.examtopics.com/discussions/google/view/18612-exam-associate-cloud-engineer-topic-1-question-52-discussion/",
    "body": "Your company has a Google Cloud Platform project that uses BigQuery for data warehousing. Your data science team changes frequently and has few members.<br>You need to allow members of this team to perform queries. You want to follow Google-recommended practices. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create an IAM entry for each data scientist's user account. 2. Assign the BigQuery jobUser role to the group.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create an IAM entry for each data scientist's user account. 2. Assign the BigQuery dataViewer user role to the group.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a dedicated Google group in Cloud Identity. 2. Add each data scientist's user account to the group. 3. Assign the BigQuery jobUser role to the group.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a dedicated Google group in Cloud Identity. 2. Add each data scientist's user account to the group. 3. Assign the BigQuery dataViewer user role to the group."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 52,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 16,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-05-03T03:34:00.000Z",
        "voteCount": 98,
        "content": "C is correct because dataViewer does not allow user to perform queries. jobUser can."
      },
      {
        "date": "2024-03-18T06:42:00.000Z",
        "voteCount": 2,
        "content": "The dataViewer role gives permissions to run read-only queries, and that\u2019s all data scientists will need so D is correct\nC will work but update and delete permissions are not required here."
      },
      {
        "date": "2020-05-04T16:58:00.000Z",
        "voteCount": 6,
        "content": "dataviewer can perform queries as well. D is correct"
      },
      {
        "date": "2020-12-19T03:23:00.000Z",
        "voteCount": 2,
        "content": "incorrect"
      },
      {
        "date": "2020-12-21T08:33:00.000Z",
        "voteCount": 2,
        "content": "no it does not. \nBigQuery Job User\tProvides permissions to run jobs, including queries, within the project."
      },
      {
        "date": "2022-01-14T17:06:00.000Z",
        "voteCount": 5,
        "content": "it can not. go through the below documentation:\nhttps://cloud.google.com/iam/docs/understanding-roles#predefined_roles"
      },
      {
        "date": "2024-05-17T01:16:00.000Z",
        "voteCount": 1,
        "content": "I checked this. This is what it says about BigQuery Data Viewer:\nWhen applied to a dataset, this role provides permissions to list all of the resources in the dataset (such as tables, views, snapshots, models, and routines) and to read their data and metadata with applicable APIs \"and in queries\". \nIt explicitly says \"permission to read data in queries\"\nAlso notice that jobUser will give unnecessary permissions (at project level) to a group of data scientists that frequently change."
      },
      {
        "date": "2020-05-06T14:15:00.000Z",
        "voteCount": 46,
        "content": "C is correct, doc's said: When applied to a dataset, dataViewer provides permissions to:\n\nRead the dataset's metadata and to list tables in the dataset.\nRead data and metadata from the dataset's tables.\nWhen applied at the project or organization level, this role can also enumerate all datasets in the project. Additional roles, however, are necessary to allow the running of jobs."
      },
      {
        "date": "2020-07-02T02:51:00.000Z",
        "voteCount": 2,
        "content": "listen to this guy, he's right"
      },
      {
        "date": "2022-07-06T05:51:00.000Z",
        "voteCount": 1,
        "content": "according to the principle of least priviledge that Google instills on it's IAM offerings, answer c would be correct!"
      },
      {
        "date": "2020-07-02T02:50:00.000Z",
        "voteCount": 7,
        "content": "Ref 'D' data viewer = When applied at the project or organization level, this role can also enumerate all datasets in the project. Additional roles, however, are necessary to allow the running of jobs."
      },
      {
        "date": "2024-04-12T01:59:00.000Z",
        "voteCount": 1,
        "content": "The Google-recommended practice for managing permissions in Google Cloud Platform (GCP) is to use IAM (Identity and Access Management) roles and groups for better organization and control.\n\nOption C aligns with this best practice:\n\nC. 1. Create a dedicated Google group in Cloud Identity. 2. Add each data scientist's user account to the group. 3. Assign the BigQuery jobUser role to the group.\n\nThis approach allows for central management of permissions by adding and removing users from the Google group as needed, rather than individually managing permissions for each user. The jobUser role provides the necessary permissions for running queries in BigQuery, allowing the data science team to perform their tasks without granting unnecessary permissions."
      },
      {
        "date": "2024-04-04T06:44:00.000Z",
        "voteCount": 1,
        "content": "This is correct"
      },
      {
        "date": "2024-03-18T06:37:00.000Z",
        "voteCount": 2,
        "content": "D is correct as dataViewer role in BigQuery provides read-only access to datasets and allows users to run select queries."
      },
      {
        "date": "2024-03-16T00:39:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is C.\n\nAssign the BigQuery jobUser role to individual user accounts, which can be complicated to manage as teams change frequently.\nThe BigQuery dataViewer role only allows viewing data in BigQuery data sets and tables, but does not grant permission to run queries or create jobs."
      },
      {
        "date": "2024-02-18T08:20:00.000Z",
        "voteCount": 2,
        "content": "Per documentation:\njobUser: \"Provides permissions to run jobs, including queries, within the project.\"\n\ndataViewer can only \"view\" data, but can't run queries:\nhttps://cloud.google.com/bigquery/docs/access-control#bigquery.dataViewer"
      },
      {
        "date": "2024-02-08T10:47:00.000Z",
        "voteCount": 4,
        "content": "Answer is D. \ndataViewer Role: The dataViewer role in BigQuery provides read-only access to datasets. It allows users to run queries on the datasets but does not grant permissions to modify or delete data. This is generally appropriate for data scientists who need to analyze data but not make structural changes to datasets.\n\nThis approach follows the principle of least privilege by granting the minimum necessary permissions for the data science team to perform their tasks without exposing unnecessary capabilities.\n\nOption C could also work, but using the dataViewer role is generally more appropriate for users who only need to query data without making structural changes to datasets."
      },
      {
        "date": "2023-11-22T18:24:00.000Z",
        "voteCount": 3,
        "content": "C\nAs per the documentation, jobuser is required for querying: \nhttps://cloud.google.com/bigquery/docs/access-control#bigquery.dataViewer\nhttps://cloud.google.com/bigquery/docs/access-control#bigquery.jobUser"
      },
      {
        "date": "2023-11-01T03:26:00.000Z",
        "voteCount": 1,
        "content": "C is correct because dataViewer does not allow user to perform queries. jobUser can"
      },
      {
        "date": "2023-09-01T22:06:00.000Z",
        "voteCount": 1,
        "content": "C is the corrcect answer as per the , google recommended practise add them into the group then assign the role"
      },
      {
        "date": "2023-08-31T21:59:00.000Z",
        "voteCount": 1,
        "content": "C is correct."
      },
      {
        "date": "2023-08-25T18:55:00.000Z",
        "voteCount": 1,
        "content": "UPDATES and INSERTS are queries and can not be performed with dataviewer. So C is correct Answer"
      },
      {
        "date": "2023-08-04T00:22:00.000Z",
        "voteCount": 1,
        "content": "C is correct\nD is incorrect as the dataViewer role provides more access than needed for just performing queries. This role allows users to view data in datasets and tables, which might not be necessary or appropriate for your data science team."
      },
      {
        "date": "2023-06-25T14:58:00.000Z",
        "voteCount": 2,
        "content": "dataViewer only allows vieweing. Pretty obvious!"
      },
      {
        "date": "2023-06-24T06:02:00.000Z",
        "voteCount": 2,
        "content": "The both BigQuery jobUser and dataViewer role can execute queries.  But the key here, is that \"data science team changes frequently\", so the dataViewer role has less permissions.   So, D is correct."
      },
      {
        "date": "2024-04-26T14:46:00.000Z",
        "voteCount": 1,
        "content": "dataViewer can't execute queries \"Additional roles, however, are necessary to allow the running of jobs.\" https://cloud.google.com/iam/docs/understanding-roles#bigquery.dataViewer"
      },
      {
        "date": "2023-06-09T21:03:00.000Z",
        "voteCount": 1,
        "content": "C is correct, refer https://cloud.google.com/bigquery/docs/access-control"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 53,
    "url": "https://www.examtopics.com/discussions/google/view/46621-exam-associate-cloud-engineer-topic-1-question-53-discussion/",
    "body": "Your company has a 3-tier solution running on Compute Engine. The configuration of the current infrastructure is shown below.<br><img src=\"/assets/media/exam-media/04338/0002700001.png\" class=\"in-exam-image\"><br>Each tier has a service account that is associated with all instances within it. You need to enable communication on TCP port 8080 between tiers as follows:<br>* Instances in tier #1 must communicate with tier #2.<br>* Instances in tier #2 must communicate with tier #3.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create an ingress firewall rule with the following settings: \u05d2\u20ac\u00a2 Targets: all instances \u05d2\u20ac\u00a2 Source filter: IP ranges (with the range set to 10.0.2.0/24) \u05d2\u20ac\u00a2 Protocols: allow all 2. Create an ingress firewall rule with the following settings: \u05d2\u20ac\u00a2 Targets: all instances \u05d2\u20ac\u00a2 Source filter: IP ranges (with the range set to 10.0.1.0/24) \u05d2\u20ac\u00a2 Protocols: allow all",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create an ingress firewall rule with the following settings: \u05d2\u20ac\u00a2 Targets: all instances with tier #2 service account \u05d2\u20ac\u00a2 Source filter: all instances with tier #1 service account \u05d2\u20ac\u00a2 Protocols: allow TCP:8080 2. Create an ingress firewall rule with the following settings: \u05d2\u20ac\u00a2 Targets: all instances with tier #3 service account \u05d2\u20ac\u00a2 Source filter: all instances with tier #2 service account \u05d2\u20ac\u00a2 Protocols: allow TCP: 8080\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create an ingress firewall rule with the following settings: \u05d2\u20ac\u00a2 Targets: all instances with tier #2 service account \u05d2\u20ac\u00a2 Source filter: all instances with tier #1 service account \u05d2\u20ac\u00a2 Protocols: allow all 2. Create an ingress firewall rule with the following settings: \u05d2\u20ac\u00a2 Targets: all instances with tier #3 service account \u05d2\u20ac\u00a2 Source filter: all instances with tier #2 service account \u05d2\u20ac\u00a2 Protocols: allow all",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create an egress firewall rule with the following settings: \u05d2\u20ac\u00a2 Targets: all instances \u05d2\u20ac\u00a2 Source filter: IP ranges (with the range set to 10.0.2.0/24) \u05d2\u20ac\u00a2 Protocols: allow TCP: 8080 2. Create an egress firewall rule with the following settings: \u05d2\u20ac\u00a2 Targets: all instances \u05d2\u20ac\u00a2 Source filter: IP ranges (with the range set to 10.0.1.0/24) \u05d2\u20ac\u00a2 Protocols: allow TCP: 8080"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-03-15T23:48:00.000Z",
        "voteCount": 90,
        "content": "This question is designed to waste your time during the exam by making you read all those long answers.   Remember that part of exam technique is not about knowing the product at all, but understanding multiple choice questions. \n\nFor example when two answers are very similar to each other, this can increase the likelihood that the correct answer is one of those two.\n\nIn this case it's an easy process of elimination as all answers are similar, we just need to filter out the wrong ones (and whacking the wrong answer in an exam is sometimes the best way to find the right one). \n\nTwo answers mention port 8080, and two mention all ports.  Obviously we just need port 8080, so we can immediately eliminate those two questions that want all ports open.  That gives us a 50/50 chance of getting this question right. \n\nOf the remaining answers, one says \"ingress\" and the other \"egress\".  We know that by default egress is permitted and ingress is not, so that makes \"b\" the only surviving choice."
      },
      {
        "date": "2023-03-19T09:28:00.000Z",
        "voteCount": 4,
        "content": "Hi Obey, many thanks for the exam tricks"
      },
      {
        "date": "2023-11-05T11:18:00.000Z",
        "voteCount": 3,
        "content": "obeythefist would 100% survive an apocalypse"
      },
      {
        "date": "2024-02-18T08:24:00.000Z",
        "voteCount": 2,
        "content": "obeythefist broke it down EXACTLY how I did it before viewing discussion. This exam is starting to seem easier and easier :)"
      },
      {
        "date": "2021-04-11T19:21:00.000Z",
        "voteCount": 27,
        "content": "if you see closely, port 8080 and service account is required so B is the answer without reading all answers"
      },
      {
        "date": "2022-01-26T10:53:00.000Z",
        "voteCount": 4,
        "content": "Love they way you think, drill down to the important details."
      },
      {
        "date": "2023-09-01T22:08:00.000Z",
        "voteCount": 1,
        "content": "b is the correct answer"
      },
      {
        "date": "2023-02-19T22:14:00.000Z",
        "voteCount": 1,
        "content": "ANSWER B is the correct answer because it creates ingress firewall rules that allow communication between the instances in the different tiers on TCP port 8080, based on their associated service accounts. The first rule allows traffic from instances in Tier#1 with the Tier#1 service account to instances in Tier#2 with the Tier#2 service account. The second rule allows traffic from instances in Tier#2 with the Tier#2 service account to instances in Tier#3 with the Tier#3 service account. This ensures that only the appropriate instances can communicate with each other."
      },
      {
        "date": "2023-02-19T22:14:00.000Z",
        "voteCount": 4,
        "content": "ANSWER A is incorrect because it creates ingress firewall rules to allow communication between instances based on the IP ranges of their respective subnets. However, this doesn't guarantee that only instances in the desired tiers will be able to communicate with each other. Other instances outside the desired tiers that happen to be in the same subnet ranges will also be able to communicate.\n\nANSWER C is incorrect because it also allows all protocols for communication between instances in the desired tiers. This may not be desirable from a security standpoint, as it can potentially allow for unauthorized access or communication between instances.\n\nANSWER D is incorrect because it creates egress firewall rules instead of ingress rules. Egress rules control outbound traffic from instances, whereas ingress rules control inbound traffic. In this case, we need to control inbound traffic to allow communication between tiers on TCP port 8080."
      },
      {
        "date": "2022-12-06T05:58:00.000Z",
        "voteCount": 1,
        "content": "B is the correct answer"
      },
      {
        "date": "2022-10-24T08:28:00.000Z",
        "voteCount": 1,
        "content": "B is correct obviously"
      },
      {
        "date": "2022-08-01T13:09:00.000Z",
        "voteCount": 1,
        "content": "B is the correct answer"
      },
      {
        "date": "2022-06-23T08:58:00.000Z",
        "voteCount": 2,
        "content": "B is right, We need to open firewall rules to allow port 8080 and It shouldn't be wide open... like /24 network."
      },
      {
        "date": "2022-06-01T23:56:00.000Z",
        "voteCount": 1,
        "content": "go for B"
      },
      {
        "date": "2022-01-31T06:48:00.000Z",
        "voteCount": 2,
        "content": "B is correct"
      },
      {
        "date": "2022-01-04T21:09:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2021-11-19T20:02:00.000Z",
        "voteCount": 1,
        "content": "B. 1. Create an ingress firewall rule with the following settings: \u05d2\u20ac\u00a2 Targets: all instances with tier #2 service account \u05d2\u20ac\u00a2 Source filter: all instances with tier #1 service account \u05d2\u20ac\u00a2 Protocols: allow TCP:8080 2. Create an ingress firewall rule with the following settings: \u05d2\u20ac\u00a2 Targets: all instances with tier #3 service account \u05d2\u20ac\u00a2 Source filter: all instances with tier #2 service account \u05d2\u20ac\u00a2 Protocols: allow TCP: 8080"
      },
      {
        "date": "2021-11-18T16:51:00.000Z",
        "voteCount": 1,
        "content": "Ans - B"
      },
      {
        "date": "2021-06-19T16:32:00.000Z",
        "voteCount": 10,
        "content": "B is correct"
      },
      {
        "date": "2021-06-10T03:19:00.000Z",
        "voteCount": 2,
        "content": "B is correct"
      },
      {
        "date": "2021-05-26T05:49:00.000Z",
        "voteCount": 4,
        "content": "B is correct"
      },
      {
        "date": "2021-05-11T23:43:00.000Z",
        "voteCount": 3,
        "content": "B is correct"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 54,
    "url": "https://www.examtopics.com/discussions/google/view/18614-exam-associate-cloud-engineer-topic-1-question-54-discussion/",
    "body": "You are given a project with a single Virtual Private Cloud (VPC) and a single subnetwork in the us-central1 region. There is a Compute Engine instance hosting an application in this subnetwork. You need to deploy a new instance in the same project in the europe-west1 region. This new instance needs access to the application. You want to follow Google-recommended practices. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a subnetwork in the same VPC, in europe-west1. 2. Create the new instance in the new subnetwork and use the first instance's private address as the endpoint.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a VPC and a subnetwork in europe-west1. 2. Expose the application with an internal load balancer. 3. Create the new instance in the new subnetwork and use the load balancer's address as the endpoint.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a subnetwork in the same VPC, in europe-west1. 2. Use Cloud VPN to connect the two subnetworks. 3. Create the new instance in the new subnetwork and use the first instance's private address as the endpoint.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a VPC and a subnetwork in europe-west1. 2. Peer the 2 VPCs. 3. Create the new instance in the new subnetwork and use the first instance's private address as the endpoint."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 58,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-06-03T16:22:00.000Z",
        "voteCount": 43,
        "content": "JUST PASS THE EXAM THIS MORING , THIS ONE IS THERE AND I CHOOSE A"
      },
      {
        "date": "2020-04-17T10:47:00.000Z",
        "voteCount": 40,
        "content": "A is correct"
      },
      {
        "date": "2024-01-23T04:07:00.000Z",
        "voteCount": 1,
        "content": "I read that we cannot span VPC in more than one region so how can we use same VPC subnet in different region?\nSo how can answer be A?"
      },
      {
        "date": "2024-02-09T04:09:00.000Z",
        "voteCount": 2,
        "content": "Networks (VPCs) with Google are multi-regional by default, so you can have resources anywhere in the world under one VPC and multiple subnets. (probably not the best explanation, but it's true in the end)"
      },
      {
        "date": "2023-06-06T02:52:00.000Z",
        "voteCount": 1,
        "content": "A-team FTW!"
      },
      {
        "date": "2023-03-30T04:52:00.000Z",
        "voteCount": 3,
        "content": "B. 1. Create a VPC and a subnetwork in europe-west1. 2. Expose the application with an internal load balancer. 3. Create the new instance in the new subnetwork and use the load balancer's address as the endpoint.\n\nThis option follows Google-recommended practices by creating a new VPC and subnetwork in the region where the new instance will be deployed. The application is exposed using an internal load balancer, which allows the new instance to access the application using the load balancer's private IP address as the endpoint. This approach provides a secure and scalable way to connect instances across regions. Option A is incorrect because it creates a new subnetwork in the same VPC, which may cause issues with network latency and scalability. Option C is incorrect because it uses Cloud VPN, which is typically used for connecting on-premises networks to GCP, and may not be the most efficient option for connecting instances within GCP. Option D is incorrect because it peers two VPCs, which may not be the most efficient option for connecting instances within the same project.\n\nthis is what GPT said, does it make sense?"
      },
      {
        "date": "2023-04-20T01:47:00.000Z",
        "voteCount": 1,
        "content": "GPT also told me B is correct. If asked \"in gcp, can instances in the same vpc but in different subnets communicate using internal ip in different regions?\" chatGPT answers \"Yes, instances in the same VPC but in different subnets can communicate using internal IP even if they are in different regions. As long as the VPC network is set up properly, the instances can communicate with each other using their internal IP addresses, regardless of the region. However, it's important to note that traffic between regions will incur additional network egress charges, so it's important to consider the cost implications when designing your network architecture.\"\nSo I think the correct answer would still be A."
      },
      {
        "date": "2023-05-20T00:02:00.000Z",
        "voteCount": 8,
        "content": "If you trust ChatGPT damn sure you are gonna fail the exam."
      },
      {
        "date": "2024-02-09T04:11:00.000Z",
        "voteCount": 1,
        "content": "VPCs are multi-regional by default, so you can create another subnetwork (which is regional) in another region and that's it. B would require use of an LB which is not required for this simple thing."
      },
      {
        "date": "2023-02-19T22:28:00.000Z",
        "voteCount": 11,
        "content": "ANSWER A is the correct answer because it follows Google's recommended practices of using a single VPC per project and creating a new subnetwork in the same VPC in the europe-west1 region. This allows the new instance to communicate with the existing instance using its private IP address as the endpoint."
      },
      {
        "date": "2023-02-19T22:28:00.000Z",
        "voteCount": 6,
        "content": "ANSWER B is incorrect because creating a new VPC and subnetwork in the europe-west1 region is not necessary and goes against Google's recommended practices of using a single VPC per project. Additionally, using an internal load balancer to expose the application is not necessary since the new instance will be in the same project and can communicate directly with the existing instance.\n\nANSWER C is also incorrect because Cloud VPN is used to establish a secure connection between a VPC and an external network, such as an on-premises data center or another cloud provider. It is not designed to enable communication between subnetworks in the same VPC, especially not across different regions.\n\nANSWER D is incorrect because VPC peering only works between VPCs in the same region, so it would not be possible to peer the existing VPC in us-central1 with a new VPC in europe-west1."
      },
      {
        "date": "2022-12-31T06:56:00.000Z",
        "voteCount": 1,
        "content": "Answer A is correct."
      },
      {
        "date": "2022-12-06T08:58:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2022-10-24T08:33:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2022-09-26T19:22:00.000Z",
        "voteCount": 2,
        "content": "if A is correct can someone explain what \"use the first instance's private address as the endpoint\" means? Does it mean to use IP from previous subnet? or does it mean use first IP from new subnet?"
      },
      {
        "date": "2022-10-03T21:01:00.000Z",
        "voteCount": 2,
        "content": "A is correct.  VPC allows you to spawn multiple subnets in different zones.  Routing is handled automatically (because Routers are created automatically).\n\n\"use the first instance's private address as the endpoint\" means that this new instance will be accessing the app via first intance's private IP (so there should be some routing rules created). Question says: \"This new instance needs access to the application.\" .."
      },
      {
        "date": "2022-09-24T10:21:00.000Z",
        "voteCount": 1,
        "content": "had this question today"
      },
      {
        "date": "2022-06-02T00:12:00.000Z",
        "voteCount": 2,
        "content": "Go for A"
      },
      {
        "date": "2022-04-09T01:24:00.000Z",
        "voteCount": 1,
        "content": "A cannot be good, I mean guys you are not good at networking, if you have two different subnets, you cannot use an IP from the other subnet, just randomly, you have to \"give acess\" which means you have to connect to the two subnets somehow, it would be better with rouing, but VPN does the job...\nhttps://cloud.google.com/network-connectivity/docs/vpn/concepts/overview\nCloud VPN securely connects your peer network to your Virtual Private Cloud (VPC) network through an IPsec VPN connection. Traffic traveling between the two networks is encrypted by one VPN gateway and then decrypted by the other VPN gateway. This action protects your data as it travels over the internet. You can also connect two instances of Cloud VPN to each other."
      },
      {
        "date": "2022-04-22T14:56:00.000Z",
        "voteCount": 20,
        "content": "In GCP, VPC's are global - and subnets across different regions can be accessed using private IP's (no VPN setup required)."
      },
      {
        "date": "2022-06-23T09:37:00.000Z",
        "voteCount": 4,
        "content": "There is no need of setting VPN as you mentioned. AWS is different .."
      },
      {
        "date": "2022-10-03T21:04:00.000Z",
        "voteCount": 3,
        "content": "I mean guys you are not good at networking &lt;-- but we are glad you are profficient with it :-D \n\nRouting between subnets in GCP is not the same as in \"regular\" networking. \nhttps://cloud.google.com/vpc/docs/routes#subnet-routes  :  \"When you add a subnet, Google Cloud creates a corresponding subnet route for the subnet's primary IP address range.\"  RTFM.."
      },
      {
        "date": "2021-12-06T06:18:00.000Z",
        "voteCount": 2,
        "content": "A is perfect"
      },
      {
        "date": "2021-11-19T20:06:00.000Z",
        "voteCount": 2,
        "content": "A. 1. Create a subnetwork in the same VPC, in europe-west1. 2. Create the new instance in the new subnetwork and use the first instance's private address as the endpoint."
      },
      {
        "date": "2021-11-18T16:53:00.000Z",
        "voteCount": 2,
        "content": "Ans - A"
      },
      {
        "date": "2021-10-20T10:11:00.000Z",
        "voteCount": 3,
        "content": "Subnets are global. A is correct"
      },
      {
        "date": "2022-05-21T07:08:00.000Z",
        "voteCount": 1,
        "content": "subnets are regional!"
      },
      {
        "date": "2022-05-23T01:25:00.000Z",
        "voteCount": 4,
        "content": "VPC ARE GLOBA,SUBNETS ARE REGIONAL"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 55,
    "url": "https://www.examtopics.com/discussions/google/view/19419-exam-associate-cloud-engineer-topic-1-question-55-discussion/",
    "body": "Your projects incurred more costs than you expected last month. Your research reveals that a development GKE container emitted a huge number of logs, which resulted in higher costs. You want to disable the logs quickly using the minimum number of steps. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Go to the Logs ingestion window in Stackdriver Logging, and disable the log source for the GKE container resource.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Go to the Logs ingestion window in Stackdriver Logging, and disable the log source for the GKE Cluster Operations resource.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Go to the GKE console, and delete existing clusters. 2. Recreate a new cluster. 3. Clear the option to enable legacy Stackdriver Logging.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Go to the GKE console, and delete existing clusters. 2. Recreate a new cluster. 3. Clear the option to enable legacy Stackdriver Monitoring."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 17,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-05-04T02:41:00.000Z",
        "voteCount": 53,
        "content": "The question mentioned that \"GKE container emitted a huge number of logs\", in my opinion A is correct."
      },
      {
        "date": "2021-01-31T11:53:00.000Z",
        "voteCount": 18,
        "content": "I think A is right. \nhttps://cloud.google.com/logging/docs/api/v2/resource-list\n\nGKE Containers have more log than GKE Cluster Operations:\n\n.-GKE Containe:\ncluster_name: An immutable name for the cluster the container is running in.\nnamespace_id: Immutable ID of the cluster namespace the container is running in.\ninstance_id: Immutable ID of the GCE instance the container is running in.\npod_id: Immutable ID of the pod the container is running in.\ncontainer_name: Immutable name of the container.\nzone: The GCE zone in which the instance is running.\n\nVS\n\n.-GKE Cluster Operations\nproject_id: The identifier of the GCP project associated with this resource, such as \"my-project\".\ncluster_name: The name of the GKE Cluster.\nlocation: The location in which the GKE Cluster is running."
      },
      {
        "date": "2024-09-07T04:21:00.000Z",
        "voteCount": 1,
        "content": "Your projects incurred more costs than you expected last month. Your research reveals that a development GKE container emitted a huge number of logs, which resulted in higher costs. You want to disable the logs quickly using the minimum number of steps. What should you do?\nA. 1. Go to the Logs ingestion window in Stackdriver Logging, and disable the log source for the GKE container resource.\nB. 1. Go to the Logs ingestion window in Stackdriver Logging, and disable the log source for the GKE Cluster Operations resource.\nC. 1. Go to the GKE console, and delete existing clusters. 2. Recreate a new cluster. 3. Clear the option to enable legacy Stackdriver Logging.\nD. 1. Go to the GKE console, and delete existing clusters. 2. Recreate a new cluster. 3. Clear the option to enable legacy Stackdriver Monitoring."
      },
      {
        "date": "2023-09-01T22:14:00.000Z",
        "voteCount": 2,
        "content": "A seems more correct as it also uses the fewes steps also"
      },
      {
        "date": "2023-04-04T09:34:00.000Z",
        "voteCount": 8,
        "content": "Just as a side note. Stackdriver is now Google Cloud Operations Suite."
      },
      {
        "date": "2023-02-19T22:31:00.000Z",
        "voteCount": 6,
        "content": "The correct answer is A.\n\nTo disable the logs quickly using the minimum number of steps, you should go to the Logs ingestion window in Stackdriver Logging and disable the log source for the GKE container resource. This will prevent the GKE container from emitting logs, which will in turn reduce the amount of log data generated and lower the costs incurred.\n\nAnswer B is incorrect because disabling the log source for the GKE Cluster Operations resource would not stop the logs emitted by the GKE container, and would not reduce the amount of log data generated.\n\nAnswers C and D are incorrect because deleting and recreating the GKE cluster is not a recommended approach for disabling logs. Additionally, clearing the option to enable legacy Stackdriver Logging or Monitoring would not disable the logs emitted by the GKE container."
      },
      {
        "date": "2022-12-06T09:03:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2022-10-24T08:34:00.000Z",
        "voteCount": 1,
        "content": "A can do it with fewest steps"
      },
      {
        "date": "2022-10-21T06:22:00.000Z",
        "voteCount": 1,
        "content": "Currently B is correct. (Stackdriver Logging has been named Cloud Logging.)\nhttps://cloud.google.com/stackdriver/docs/solutions/gke/installing#migrating"
      },
      {
        "date": "2022-08-06T08:14:00.000Z",
        "voteCount": 1,
        "content": "Go for A"
      },
      {
        "date": "2022-06-23T09:48:00.000Z",
        "voteCount": 1,
        "content": "C,D are straight forward you can eliminate and decide between A &amp; B .. A makes more sense .. logical thinking required to solve it quickly."
      },
      {
        "date": "2022-06-23T09:48:00.000Z",
        "voteCount": 1,
        "content": "A right"
      },
      {
        "date": "2022-06-02T00:15:00.000Z",
        "voteCount": 2,
        "content": "Go for A"
      },
      {
        "date": "2022-06-02T00:54:00.000Z",
        "voteCount": 2,
        "content": "Beginning with GKE version 1.15.7, you can configure Cloud Operations for GKE to only capture system logs and not collect application logs.\nhttps://cloud.google.com/stackdriver/docs/solutions/gke/managing-logs"
      },
      {
        "date": "2022-05-07T14:45:00.000Z",
        "voteCount": 1,
        "content": "A is correct. Target GKE containers since that is what is emitting the most logs"
      },
      {
        "date": "2022-03-21T21:55:00.000Z",
        "voteCount": 2,
        "content": "Go to the Logs ingestion window in Stackdriver Logging, and disable the log source for the GKE container resource."
      },
      {
        "date": "2022-01-29T10:29:00.000Z",
        "voteCount": 1,
        "content": "A is right answer"
      },
      {
        "date": "2022-01-23T23:08:00.000Z",
        "voteCount": 1,
        "content": "A is ans"
      },
      {
        "date": "2021-11-19T20:10:00.000Z",
        "voteCount": 2,
        "content": "A. 1. Go to the Logs ingestion window in Stackdriver Logging, and disable the log source for the GKE container resource."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 56,
    "url": "https://www.examtopics.com/discussions/google/view/17514-exam-associate-cloud-engineer-topic-1-question-56-discussion/",
    "body": "You have a website hosted on App Engine standard environment. You want 1% of your users to see a new test version of the website. You want to minimize complexity. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the new version in the same application and use the --migrate option.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the new version in the same application and use the --splits option to give a weight of 99 to the current version and a weight of 1 to the new version.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new App Engine application in the same project. Deploy the new version in that application. Use the App Engine library to proxy 1% of the requests to the new version.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new App Engine application in the same project. Deploy the new version in that application. Configure your network load balancer to send 1% of the traffic to that new application."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 19,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-03-26T06:37:00.000Z",
        "voteCount": 56,
        "content": "I will prefer B as the answer.. why we need create new application?"
      },
      {
        "date": "2021-06-01T02:16:00.000Z",
        "voteCount": 9,
        "content": "Agree B is correct. creating a new application in the same project for app engine is anyways not possible."
      },
      {
        "date": "2021-07-24T02:20:00.000Z",
        "voteCount": 20,
        "content": "more over, in app engine we cannot create \"new application\", we have to create a new Project to do that, an app engine projet has 1 application (which can have multiple versions and services)"
      },
      {
        "date": "2023-11-05T12:10:00.000Z",
        "voteCount": 2,
        "content": "\"yasu\"...nana?"
      },
      {
        "date": "2020-04-02T23:41:00.000Z",
        "voteCount": 17,
        "content": "I agree with yasu. And only one app engine can exist in one project. B is the best choice, simple and easy."
      },
      {
        "date": "2023-09-01T22:19:00.000Z",
        "voteCount": 3,
        "content": "B is the correct answer"
      },
      {
        "date": "2023-02-19T22:33:00.000Z",
        "voteCount": 11,
        "content": "The correct answer is B.\n\nBy using the App Engine's traffic splitting feature, we can easily direct a certain percentage of traffic to a specific version of our application. In this case, we want to send 1% of traffic to the new test version and keep the remaining 99% on the current version. This can be achieved by deploying the new version in the same application and using the `--splits` option to give a weight of 99 to the current version and a weight of 1 to the new version.\n\nAnswer A is incorrect because the `--migrate` option is used for migrating traffic to a new version after it has been fully tested and is ready for full deployment.\n\nAnswer C is incorrect because it requires additional configuration to proxy requests to the new version, increasing complexity unnecessarily.\n\nAnswer D is incorrect because it involves configuring a network load balancer, which is not necessary for this use case and adds unnecessary complexity."
      },
      {
        "date": "2023-04-28T13:24:00.000Z",
        "voteCount": 4,
        "content": "While I agree with your choice and your explanation of B. I also believe C and D are wrong simply because you can only have one App Engine within a project https://cloud.google.com/appengine/docs/flexible/managing-projects-apps-billing#:~:text=Important%3A%20Each%20Cloud%20project%20can,of%20your%20App%20Engine%20application."
      },
      {
        "date": "2022-12-06T09:07:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2022-10-24T08:36:00.000Z",
        "voteCount": 1,
        "content": "B. deploy new version with --splits option"
      },
      {
        "date": "2022-09-24T10:21:00.000Z",
        "voteCount": 2,
        "content": "had this question today"
      },
      {
        "date": "2022-08-08T08:33:00.000Z",
        "voteCount": 1,
        "content": "B! A very natural answer\u2026 Perfect for switching users over to new version. Imagine creating multiple projects to update App Engine  deployments, isn\u2019t that logically unnecessary?"
      },
      {
        "date": "2022-06-25T22:41:00.000Z",
        "voteCount": 4,
        "content": "Hint:\nOne app engine per project. So Option C,D eliminated. this hint will help in many similar questions.\nSplitting traffic hint will help as well"
      },
      {
        "date": "2022-06-23T09:50:00.000Z",
        "voteCount": 1,
        "content": "B is right."
      },
      {
        "date": "2022-06-07T21:14:00.000Z",
        "voteCount": 3,
        "content": "b is my answer.\na: --migrate is for enabling gradual traffic migration as opposed to migrating traffic immediately\nc &amp; d: no need to create a project. You can split the traffic any time"
      },
      {
        "date": "2022-06-02T10:18:00.000Z",
        "voteCount": 1,
        "content": "Go for B"
      },
      {
        "date": "2022-01-29T10:31:00.000Z",
        "voteCount": 2,
        "content": "B is the correct answer as there is only one app engine can be created per project"
      },
      {
        "date": "2022-01-17T00:27:00.000Z",
        "voteCount": 2,
        "content": "This question was there , go with community answers."
      },
      {
        "date": "2022-01-04T21:18:00.000Z",
        "voteCount": 2,
        "content": "B,\n\n--splits exists for such requirements"
      },
      {
        "date": "2021-11-19T20:11:00.000Z",
        "voteCount": 1,
        "content": "B. Deploy the new version in the same application and use the --splits option to give a weight of 99 to the current version and a weight of 1 to the new version."
      },
      {
        "date": "2021-11-18T16:55:00.000Z",
        "voteCount": 2,
        "content": "Ans - B"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 57,
    "url": "https://www.examtopics.com/discussions/google/view/18197-exam-associate-cloud-engineer-topic-1-question-57-discussion/",
    "body": "You have a web application deployed as a managed instance group. You have a new version of the application to gradually deploy. Your web application is currently receiving live web traffic. You want to ensure that the available capacity does not decrease during the deployment. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPerform a rolling-action start-update with maxSurge set to 0 and maxUnavailable set to 1.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPerform a rolling-action start-update with maxSurge set to 1 and maxUnavailable set to 0.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new managed instance group with an updated instance template. Add the group to the backend service for the load balancer. When all instances in the new managed instance group are healthy, delete the old managed instance group.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new instance template with the new application version. Update the existing managed instance group with the new instance template. Delete the instances in the managed instance group to allow the managed instance group to recreate the instance using the new instance template."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 30,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-04-11T00:25:00.000Z",
        "voteCount": 94,
        "content": "Correct option is B. We need to ensure the global capacity remains intact, for that reason we need to establish maxUnavailable to 0. On the other hand, we need to ensure new instances can be created. We do that by establishing the maxSurge to 1. Option C is more expensive and more difficult to set up and option D won't meet requirements since it won't keep global capacity intact."
      },
      {
        "date": "2022-12-09T14:39:00.000Z",
        "voteCount": 12,
        "content": "maxSurge- configure how many new instances the MIG can create above its targetSize during an automated update. For example, if you set maxSurge to 5, the MIG uses the new instance template to create up to 5 new instances above your target size. Setting a higher maxSurge value speeds up your update, at the cost of additional instances"
      },
      {
        "date": "2023-02-25T10:32:00.000Z",
        "voteCount": 1,
        "content": "Thanks for this.\nAnd setting it to one makes sense, seeing that we want a gradual update"
      },
      {
        "date": "2020-09-09T09:09:00.000Z",
        "voteCount": 20,
        "content": "I take my own previous comment back. It's definitely B."
      },
      {
        "date": "2024-02-08T11:23:00.000Z",
        "voteCount": 1,
        "content": "I vote for A. \nA rolling update with maxSurge set to 0 ensures that no additional instances beyond the desired size are created during the update.\nBy setting maxUnavailable to 1, only one instance is taken down at a time, minimizing the impact on the available capacity during the deployment.\nThis approach allows the rolling deployment to proceed in a controlled manner, with each new version gradually replacing instances without decreasing the overall capacity and with zero downtime.\n\nNot Option B \nbecause setting maxSurge to 1 and maxUnavailable to 0, could lead to temporarily increased capacity during the update, and it might result in higher resource usage than necessary. This option may not guarantee zero downtime or minimize the impact on the available capacity during the deployment."
      },
      {
        "date": "2023-11-22T20:36:00.000Z",
        "voteCount": 2,
        "content": "B is the clean way: https://medium.com/@bubu.tripathy/understanding-maxsurge-and-maxunavailable-4966dfafc8ba"
      },
      {
        "date": "2023-09-01T22:20:00.000Z",
        "voteCount": 1,
        "content": "B is the correct answer"
      },
      {
        "date": "2023-02-19T22:44:00.000Z",
        "voteCount": 10,
        "content": "Answer B is the correct answer because it allows for a safe and controlled rolling deployment with zero downtime and without reducing the available capacity during the deployment.\n\nThe `maxSurge` parameter controls the maximum number of new instances that can be created above the desired number of instances during the update process. By setting `maxSurge` to 1, the new version of the application can be gradually rolled out while maintaining the same number of available instances.\n\nThe `maxUnavailable` parameter controls the maximum number of instances that can be unavailable during the update process. By setting `maxUnavailable` to 0, at least one instance of the previous version will be available at all times, ensuring that there is no decrease in available capacity during the deployment.\n\nBy performing a rolling update with `maxSurge` set to 1 and `maxUnavailable` set to 0, the new version of the application can be gradually deployed with zero downtime and no decrease in available capacity."
      },
      {
        "date": "2023-02-19T22:44:00.000Z",
        "voteCount": 3,
        "content": "Answer A is incorrect because setting maxSurge to 0 means that no additional instances are created beyond the existing number of instances in the group, which can potentially lead to a decrease in capacity. Also, setting maxUnavailable to 1 means that one instance can be unavailable at any given time, which can potentially lead to some users experiencing downtime.\n\nAnswer C is incorrect because creating a new managed instance group would require adding the new group to the backend service, which can take time and potentially cause downtime. Also, deleting the old managed instance group before ensuring that the new group is healthy can cause a decrease in capacity.\n\nAnswer D is incorrect because deleting instances in the managed instance group can cause a temporary decrease in capacity, and it may take some time for new instances to be created with the new instance template. Also, the new instances may take time to warm up, which can cause a delay in serving traffic."
      },
      {
        "date": "2023-01-06T22:49:00.000Z",
        "voteCount": 3,
        "content": "If you do not want any unavailable machines during an update, set the maxUnavailable value to 0 and the maxSurge value to greater than 0. With these settings, Compute Engine removes each old machine only after its replacement new machine is created and running.\n\nhttps://cloud.google.com/compute/docs/instance-groups/rolling-out-updates-to-managed-instance-groups#max_unavailable"
      },
      {
        "date": "2022-12-30T10:11:00.000Z",
        "voteCount": 1,
        "content": "If you do not want any unavailable machines during an update, set the maxUnavailable value to 0 and the maxSurge value to greater than 0. With these settings, Compute Engine removes each old machine only after its replacement new machine is created and running."
      },
      {
        "date": "2022-12-14T09:04:00.000Z",
        "voteCount": 2,
        "content": "HI all,\n\nif you guys have all the questions and answers please mail it to\nuntranslatable[dot]character@gmail[dot]com\n\nThanks in advance."
      },
      {
        "date": "2022-12-07T21:40:00.000Z",
        "voteCount": 1,
        "content": "B is Correct"
      },
      {
        "date": "2022-12-01T04:30:00.000Z",
        "voteCount": 1,
        "content": "this this this this this"
      },
      {
        "date": "2022-11-18T22:31:00.000Z",
        "voteCount": 2,
        "content": "I have seen this question in others websites, and in all of them, the answer is B B B!!."
      },
      {
        "date": "2022-10-30T08:00:00.000Z",
        "voteCount": 1,
        "content": "B. maxUnavailable set to 0 is the key"
      },
      {
        "date": "2022-09-26T19:25:00.000Z",
        "voteCount": 1,
        "content": "it should be B. if we change template it cause outage and question said no outage allowed."
      },
      {
        "date": "2022-07-02T07:36:00.000Z",
        "voteCount": 1,
        "content": "Answer is B"
      },
      {
        "date": "2022-06-23T09:52:00.000Z",
        "voteCount": 1,
        "content": "Correct option is B.  Perform a rolling-action start-update with maxSurge set to 1 and maxUnavailable set to 0. This is also present on Tutorial Dojo practice questions."
      },
      {
        "date": "2023-06-06T03:01:00.000Z",
        "voteCount": 1,
        "content": "Sir, this is a GCP certification platform.."
      },
      {
        "date": "2022-06-03T16:23:00.000Z",
        "voteCount": 3,
        "content": "WAS IN MY EXAM GO WITY THE MAJORITY"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 58,
    "url": "https://www.examtopics.com/discussions/google/view/16872-exam-associate-cloud-engineer-topic-1-question-58-discussion/",
    "body": "You are building an application that stores relational data from users. Users across the globe will use this application. Your CTO is concerned about the scaling requirements because the size of the user base is unknown. You need to implement a database solution that can scale with your user growth with minimum configuration changes. Which storage solution should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud SQL",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Spanner\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Firestore",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Datastore"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 21,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-08-03T02:10:00.000Z",
        "voteCount": 109,
        "content": "B\nCloud SQL for small relational data, scaled manually\nCloud Spanner for relational data, scaled automatically\nCloud Firestore for app-based data(?)\nCloud Datastore for non-relational data\nCorrect me if i'm wrong"
      },
      {
        "date": "2022-06-06T18:42:00.000Z",
        "voteCount": 16,
        "content": "Just one detail: Cloud Firestore for non relational data (noSql)"
      },
      {
        "date": "2023-06-06T03:02:00.000Z",
        "voteCount": 8,
        "content": "N E R D"
      },
      {
        "date": "2023-03-28T00:21:00.000Z",
        "voteCount": 1,
        "content": "'small relational data' as in 3 TB for Shared core or 64 TB for Dedicated core in Cloud SQL"
      },
      {
        "date": "2020-03-17T09:04:00.000Z",
        "voteCount": 29,
        "content": "in my opinion correct is B"
      },
      {
        "date": "2024-06-17T00:37:00.000Z",
        "voteCount": 1,
        "content": "Cloud spanner is correct as it will scaled automatically"
      },
      {
        "date": "2023-11-27T21:41:00.000Z",
        "voteCount": 2,
        "content": "B is Correct Ans. Cloud Spanner provides a scalable online transaction processing (OLTP) database with high availability and strong consistency at a global scale."
      },
      {
        "date": "2023-09-01T22:23:00.000Z",
        "voteCount": 1,
        "content": "B, for large and automatically scaled"
      },
      {
        "date": "2023-05-13T05:04:00.000Z",
        "voteCount": 3,
        "content": "Correct ans is B... Focus on two words \"Relational\" which means option C &amp; D has been eliminated bcz these are non-relational DB.And another word 'Globally' which means Option A also eliminated bcz Cloud Sql does not support global deployments."
      },
      {
        "date": "2023-02-19T22:47:00.000Z",
        "voteCount": 9,
        "content": "The best storage solution for this scenario would be Cloud Spanner. Cloud Spanner is a fully managed, scalable, relational database that is designed to handle global deployments with ease. It can handle large amounts of data and high transactional volumes. It also provides automatic sharding and synchronous replication, ensuring high availability and durability of data. Cloud Spanner supports SQL semantics and provides a familiar relational database experience to developers, which would make it easy to adopt in existing workflows. \n\nCloud SQL, on the other hand, has limits on scalability and does not support global deployments as well as Cloud Spanner. \n\nCloud Firestore and Cloud Datastore are NoSQL databases that are better suited for document-based data storage and not optimized for relational data storage."
      },
      {
        "date": "2022-12-06T10:56:00.000Z",
        "voteCount": 1,
        "content": "correct is B"
      },
      {
        "date": "2022-11-14T11:30:00.000Z",
        "voteCount": 1,
        "content": "could someone please explain why the answer is not A"
      },
      {
        "date": "2022-10-30T08:02:00.000Z",
        "voteCount": 1,
        "content": "B. Spanner for autoscale"
      },
      {
        "date": "2022-09-18T09:18:00.000Z",
        "voteCount": 2,
        "content": "B.   \nkeywords:  \"global\" and \"relational\""
      },
      {
        "date": "2022-08-20T06:03:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is B\nCloud Firestore and Cloud Datastore - can easily be eliminated doesn't fall under Relational DB"
      },
      {
        "date": "2022-07-25T11:28:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2022-06-28T19:18:00.000Z",
        "voteCount": 1,
        "content": "B as many have mentioned already"
      },
      {
        "date": "2022-06-23T09:53:00.000Z",
        "voteCount": 5,
        "content": "Global is keyword in this question, Cloud Spanner is the right option..  B"
      },
      {
        "date": "2022-06-02T11:16:00.000Z",
        "voteCount": 1,
        "content": "Go for B"
      },
      {
        "date": "2022-02-08T12:32:00.000Z",
        "voteCount": 2,
        "content": "B is correct"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 59,
    "url": "https://www.examtopics.com/discussions/google/view/17333-exam-associate-cloud-engineer-topic-1-question-59-discussion/",
    "body": "You are the organization and billing administrator for your company. The engineering team has the Project Creator role on the organization. You do not want the engineering team to be able to link projects to the billing account. Only the finance team should be able to link a project to a billing account, but they should not be able to make any other changes to projects. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAssign the finance team only the Billing Account User role on the billing account.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAssign the engineering team only the Billing Account User role on the billing account.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAssign the finance team the Billing Account User role on the billing account and the Project Billing Manager role on the organization.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAssign the engineering team the Billing Account User role on the billing account and the Project Billing Manager role on the organization."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 88,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 66,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-03-23T21:33:00.000Z",
        "voteCount": 89,
        "content": "Option A is correct, as we don't want the engineering team to link projects to billing account and want only the Finance team. Billing Account User role will help to link projects to the billing account..."
      },
      {
        "date": "2021-10-20T18:32:00.000Z",
        "voteCount": 8,
        "content": "I would also go with A. I would think they are trying to get a quick answer from you as \"Billing Administrator\": engineering team already has the project creator role; you just would want finance team to link (and only) link projects to billing accounts, nothing else. Maybe the key phrase here is \"but they should not be able to make any other changes to projects\" and that would include the action of unlinking projects."
      },
      {
        "date": "2021-06-07T23:02:00.000Z",
        "voteCount": 1,
        "content": "Billing Account User also enables the user to make changes in resources."
      },
      {
        "date": "2022-07-03T22:06:00.000Z",
        "voteCount": 2,
        "content": "Billing Account User Role  when granted in combination with the Project Billing Manager role, the two roles allow a user to link and unlink projects on the billing account on which the Billing Account User role is granted"
      },
      {
        "date": "2020-09-08T02:57:00.000Z",
        "voteCount": 11,
        "content": "Option A makes the most sense since Billing Account User can link projects to the billing account and the question reinforces principle of least privilege. Source: https://cloud.google.com/billing/docs/how-to/billing-access"
      },
      {
        "date": "2024-05-17T06:40:00.000Z",
        "voteCount": 1,
        "content": "Yes, but in combination with Project Billing Manager. Also these two roles won't grant rights on any other resources, which is also intended in the question."
      },
      {
        "date": "2020-05-31T08:38:00.000Z",
        "voteCount": 63,
        "content": "for me is C:\nhttps://cloud.google.com/billing/docs/how-to/modify-project#permissions_required_for_this_task_2\n\"Roles with adequate permissions to perform this task:\n   * Project Owner or Project Billing Manager on the project, AND Billing Account Administrator or Billing Account User for the target Cloud Billing account.\""
      },
      {
        "date": "2024-10-03T02:13:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      },
      {
        "date": "2022-02-27T23:15:00.000Z",
        "voteCount": 3,
        "content": "The question states that the finance group should not be able to make changes to existing projects.  Granting the finance team organizational level Billing Account Administrator will allow them to make changes to other projects.  C cannot be correct."
      },
      {
        "date": "2022-10-11T23:54:00.000Z",
        "voteCount": 3,
        "content": "Project Billing Manager does not allow to make any changes to projects. It's just about linking+unlinking projects to billing accounts\n\nOn the other hand, the single role \"billing account user\" does not grant any right to view projects. Even less likely to link them to any billing account. (see https://cloud.google.com/iam/docs/job-functions/billing \"The Billing Account User role gives the service account the permissions to enable billing (associate projects with the organization's billing account for all projects in the organization) and thereby permit the service account to enable APIs that require billing to be enabled.\"). Thus A is not the correct answer.\n\nThe right answer is C, without any kind of doubt"
      },
      {
        "date": "2022-11-02T08:25:00.000Z",
        "voteCount": 4,
        "content": "Are you blind ? you posted link where its clearly stated in billing account user description: (associate projects with the organization's billing account for all projects in the organization)\n\nSo you literarly posted link with clarification that answer A is correct.\nanswer C will give finance team additional permission to unlink billing account from projects and question clearly states that finance team should not be able to make any other changes to projects so C without any kind of doubt is wrong."
      },
      {
        "date": "2022-12-30T05:21:00.000Z",
        "voteCount": 1,
        "content": "Billing Account User\nPrincipal:\tService account that is used for automating project creation.\nIt is for service account, so C is correct"
      },
      {
        "date": "2023-04-19T05:46:00.000Z",
        "voteCount": 1,
        "content": "\"Project Billing Manager does not allow to make any changes to projects. It's just about linking+unlinking projects to billing accounts\"\n\nCorrect, but the problem states \"... You do not want the engineering team to be able to link projects to the billing account.\" So in that case, wouldn't it be option A?"
      },
      {
        "date": "2022-11-18T08:21:00.000Z",
        "voteCount": 4,
        "content": "We are assigning the finance team the Billing Account User role on the billing account, which allows them to create new projects linked to the billing account on which the role is granted. We are also assigning them the Project Billing Manager role on the organization (trickles down to the project as well) which lets them attach the project to the billing account, but does not grant any rights over resources."
      },
      {
        "date": "2024-10-08T15:42:00.000Z",
        "voteCount": 1,
        "content": "You do not want the engineering team to be able to link projects to the billing account. \nThink about this, the answer is clearly A"
      },
      {
        "date": "2024-09-19T20:32:00.000Z",
        "voteCount": 1,
        "content": "El rol de Usuario de cuenta de facturaci\u00f3n permite al equipo de finanzas vincular proyectos a la cuenta de facturaci\u00f3n, sin otorgarles otros permisos sobre la administraci\u00f3n de proyectos.\nEl rol de Gerente de facturaci\u00f3n del proyecto otorga al equipo de finanzas la capacidad de gestionar la facturaci\u00f3n de proyectos individuales, pero no permite otros cambios a nivel de proyecto o de organizaci\u00f3n.\nEsta combinaci\u00f3n permite que solo el equipo de finanzas pueda vincular proyectos a la cuenta de facturaci\u00f3n, cumpliendo con la pol\u00edtica que el equipo de ingenier\u00eda no debe poder hacerlo."
      },
      {
        "date": "2024-06-16T23:40:00.000Z",
        "voteCount": 1,
        "content": "The Billing Account User role in GCP allows a user to view and manage billing accounts, but does not grant permissions to associate projects with billing accounts. The Project Billing Manager role is more focused on associating GCP projects with billing accounts."
      },
      {
        "date": "2024-06-03T01:54:00.000Z",
        "voteCount": 1,
        "content": "C is not good because in this link https://cloud.google.com/billing/docs/how-to/billing-access, the role Project Billing Manager allows \"Link/unlink the project to/from a billing account\". Unlink is not good in this situation."
      },
      {
        "date": "2024-05-17T06:44:00.000Z",
        "voteCount": 1,
        "content": "When granted in combination with the Billing Account User role, the Project Billing Manager role allows a user to attach the project to the billing account, but does not grant any rights over resources."
      },
      {
        "date": "2024-04-12T02:29:00.000Z",
        "voteCount": 1,
        "content": "To achieve the desired level of access control, where only the finance team can link projects to the billing account while preventing them from making other changes to projects, you should follow these steps: C\nExplanation:\n\nAssigning the finance team the Billing Account User role on the billing account allows them to link projects to the billing account, which is necessary for managing billing.\nAssigning the Project Billing Manager role on the organization to the finance team allows them to manage billing for projects within the organization without granting them additional permissions to modify projects themselves.\nThis approach ensures that the finance team has the necessary permissions to manage billing-related tasks while restricting their access to project management functionalities, such as creating or deleting projects, which are typically associated with the Project Creator role."
      },
      {
        "date": "2024-02-18T09:21:00.000Z",
        "voteCount": 1,
        "content": "Answer is A. 100%. They are asking for Finance to ONLY be able to link projects to billing (not unlink, etc.). This role has the billing.resourceAssociations.create permission\n\nPer documentation for Billing Account User: \n\"When granted in conjunction with the Project Owner role or Project Billing Manager role, provides access to associate projects with billing accounts.\"\nhttps://cloud.google.com/billing/docs/access-control#billing.user"
      },
      {
        "date": "2024-03-05T08:28:00.000Z",
        "voteCount": 1,
        "content": "Negative, GhostRider. A has insufficient permissions. The Billing Account User role alone doesn't provide the ability to manage the billing association of projects within the organization."
      },
      {
        "date": "2024-02-18T09:22:00.000Z",
        "voteCount": 2,
        "content": "*Answer is C. 100%"
      },
      {
        "date": "2024-02-10T00:33:00.000Z",
        "voteCount": 1,
        "content": "Option C\nAs stated in docs - &gt;\u00a7https://cloud.google.com/billing/docs/access-control#tbl_perm\nprojects.updateBillingInfo - &gt; \n\nBilling Account Administrator or Billing Account User, AND Project Billing Manager"
      },
      {
        "date": "2024-01-19T01:48:00.000Z",
        "voteCount": 1,
        "content": "Option C is not correct because the Project Billing Manager role would give the finance team permissions to manage billing on projects, which is more access than you want to provide. The Billing Account User role is sufficient for the finance team to link projects to the billing account"
      },
      {
        "date": "2024-01-04T06:01:00.000Z",
        "voteCount": 1,
        "content": "C is definitely the right answer here. \"Cloud Billing Account User when granted in combination with the Project Billing Manager role, the two roles allow a user to link and unlink projects on the billing account on which the Billing Account User role is granted.\"\n\nSource: https://cloud.google.com/billing/docs/how-to/billing-access"
      },
      {
        "date": "2024-01-12T06:15:00.000Z",
        "voteCount": 1,
        "content": "Yeah, with those roles the user is able to \"link and unlink projects\", but we are asked to give the financial team access to link only ONE project to a billing account (pay attention that doesn't say unlink, so it's not necessary) if you read the permissions of the billing account user in the link that you sent, you will see that this role can link a project to a billing account, that is just what we were asked to, so option A is correct."
      },
      {
        "date": "2024-01-02T20:15:00.000Z",
        "voteCount": 2,
        "content": "I tried asking Bard from google and I got this:\n\nBilling Account User: This role grants basic read-only access to billing information for a specific billing account. This ensures the finance team can see costs associated with projects but not modify any project details.\nProject Billing Manager: This role allows linking projects to a billing account and managing billing settings for those projects. However, it doesn't grant broader project editing permissions like creating, deleting, or modifying resources within the project.\n\nI think I'll go with option C"
      },
      {
        "date": "2023-12-04T11:06:00.000Z",
        "voteCount": 1,
        "content": "Option A is enough, https://cloud.google.com/billing/docs/how-to/billing-access"
      },
      {
        "date": "2023-11-22T21:51:00.000Z",
        "voteCount": 2,
        "content": "As per the below, C is the correct answer: https://cloud.google.com/billing/docs/how-to/billing-access#overview-of-cloud-billing-roles-in-cloud-iam"
      },
      {
        "date": "2023-11-22T09:29:00.000Z",
        "voteCount": 1,
        "content": "C is correct as the finance team need roles/billing.projectManager to allow them to manage the billing for the project without granting them resource access.\nD is incorrect because the finance team need to have Project Creator role or similar role to have resource access before use roles/billing.user to link project to billing account\nDocument: https://cloud.google.com/billing/docs/how-to/billing-access#overview-of-cloud-billing-roles-in-cloud-iam"
      },
      {
        "date": "2023-11-01T03:40:00.000Z",
        "voteCount": 1,
        "content": "A is correct because the questions tricks you with the engineering team input, it doesn't need to perform anything on the engineering team"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 60,
    "url": "https://www.examtopics.com/discussions/google/view/20773-exam-associate-cloud-engineer-topic-1-question-60-discussion/",
    "body": "You have an application running in Google Kubernetes Engine (GKE) with cluster autoscaling enabled. The application exposes a TCP endpoint. There are several replicas of this application. You have a Compute Engine instance in the same region, but in another Virtual Private Cloud (VPC), called gce-network, that has no overlapping IP ranges with the first VPC. This instance needs to connect to the application on GKE. You want to minimize effort. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. In GKE, create a Service of type LoadBalancer that uses the application's Pods as backend. 2. Set the service's externalTrafficPolicy to Cluster. 3. Configure the Compute Engine instance to use the address of the load balancer that has been created.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. In GKE, create a Service of type NodePort that uses the application's Pods as backend. 2. Create a Compute Engine instance called proxy with 2 network interfaces, one in each VPC. 3. Use iptables on this instance to forward traffic from gce-network to the GKE nodes. 4. Configure the Compute Engine instance to use the address of proxy in gce-network as endpoint.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. In GKE, create a Service of type LoadBalancer that uses the application's Pods as backend. 2. Add an annotation to this service: cloud.google.com/load-balancer-type: Internal 3. Peer the two VPCs together. 4. Configure the Compute Engine instance to use the address of the load balancer that has been created.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. In GKE, create a Service of type LoadBalancer that uses the application's Pods as backend. 2. Add a Cloud Armor Security Policy to the load balancer that whitelists the internal IPs of the MIG's instances. 3. Configure the Compute Engine instance to use the address of the load balancer that has been created."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 37,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 30,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-08-01T10:36:00.000Z",
        "voteCount": 61,
        "content": "I believe it's A. It's never mentioned in the question that traffic cannot go through the Internet but it's mentioned that effort should be minimized. A requires a lot less effort than C to accomplish the same (no VPC peering, per example)."
      },
      {
        "date": "2023-10-18T12:42:00.000Z",
        "voteCount": 3,
        "content": "A,C are ok for me. But this is a exam. Why the question mention the same region, no overlapping IP ranges means they suggest you to use VPC rather than public traffic. I 99% sure, if there is an official explaniation, there would be A is not correct there is a risk or error prone, sth like this."
      },
      {
        "date": "2022-02-25T12:34:00.000Z",
        "voteCount": 2,
        "content": "Totally agree. I had the same thought and looked through the question for any indication that the traffic must be private."
      },
      {
        "date": "2021-03-05T11:20:00.000Z",
        "voteCount": 13,
        "content": "Ans: A . This sounds correct and avoids unnecessary steps in C. C is also correct but compared to it, A is much easier to achieve.  Go over Kubernetes Loadbalancer concepts to get more details. Initially i was thinking C is the Answer. but after putting some time on K8's Network - changed my mind to A."
      },
      {
        "date": "2020-08-23T10:39:00.000Z",
        "voteCount": 10,
        "content": "Yeah, I feel the same. Nowhere does it say that the traffic has to be internal. But it does say \"minimal effort\" which I feel is option A."
      },
      {
        "date": "2020-05-17T10:23:00.000Z",
        "voteCount": 46,
        "content": "i think C is better solution, the solution A pass trafic trought public internet, also C by internal network and the \"no overlap ips\" in the statament suggest that."
      },
      {
        "date": "2024-05-23T11:55:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer: A"
      },
      {
        "date": "2024-04-13T04:34:00.000Z",
        "voteCount": 3,
        "content": "Here's why Option A might be preferred over Option C:\n\nSimplicity: Option A requires creating a LoadBalancer service in GKE and configuring the Compute Engine instance to use the load balancer's address. This is a straightforward setup and does not involve additional networking configurations.\n\nReduced Complexity: Peering two VPCs involves setting up and managing VPC peering configurations, which can be complex, especially if there are overlapping IP ranges. It also requires additional permissions and coordination between different teams.\n\nDirect Connectivity: Option A provides direct connectivity between the Compute Engine instance and the application running in GKE through the load balancer. Peering VPCs might introduce additional network hops, potentially impacting latency and network performance.\n\nScalability and Flexibility: Using a LoadBalancer service in GKE allows for scalability and flexibility, as the load balancer can automatically scale to handle increased traffic and can be easily configured to adapt to changing requirements."
      },
      {
        "date": "2024-02-01T00:48:00.000Z",
        "voteCount": 2,
        "content": "Not A, exposing the service with an external LoadBalancer (externalTrafficPolicy set to Cluster) and not peering VPCs or using an internal load balancer unnecessarily exposes the service to the internet, which is not required for inter-VPC communication and could lead to security concerns.\nAll the details in the question are pushing to answer C."
      },
      {
        "date": "2023-10-12T12:22:00.000Z",
        "voteCount": 4,
        "content": "Option A suggests creating an external LoadBalancer. This is not the most efficient method because you're exposing your GKE application to the internet just to allow communication between two internal resources.\n\nOption C suggests creating an internal LoadBalancer, which is the right approach. By using an internal LoadBalancer, the service is only exposed within the Google Cloud environment and won't be accessible from the internet. Peering the two VPCs ensures the two resources can communicate across the VPCs."
      },
      {
        "date": "2023-10-05T04:29:00.000Z",
        "voteCount": 2,
        "content": "C. 1. In GKE, create a Service of type LoadBalancer that uses the application's Pods as backend. 2. Add an annotation to this service: cloud.google.com/load-balancer-type: Internal 3. Peer the two VPCs together. 4. Configure the Compute Engine instance to use the address of the load balancer that has been created."
      },
      {
        "date": "2023-10-03T06:12:00.000Z",
        "voteCount": 3,
        "content": "Correct Answer is C\nOption A suggests setting the service's externalTrafficPolicy to Cluster. While this is a valid configuration, it's not directly related to the scenario described.\n\nIn the given scenario, the goal is to connect a Compute Engine instance from a different VPC to the application running in GKE. This involves networking configurations, peering the VPCs, and potentially setting up a LoadBalancer.\n\nSetting the externalTrafficPolicy to Cluster primarily affects how traffic is balanced across Pods within the cluster, but it doesn't directly address the requirement of connecting an external instance from a different VPC."
      },
      {
        "date": "2023-08-29T02:29:00.000Z",
        "voteCount": 2,
        "content": "Minimal effort is the point."
      },
      {
        "date": "2023-07-20T06:12:00.000Z",
        "voteCount": 1,
        "content": "Option C."
      },
      {
        "date": "2023-06-04T22:31:00.000Z",
        "voteCount": 3,
        "content": "A is not correct \nBecause the GKE cluster and the instance are not in the same vpc , so without vpc peering traffic can't be established .\nC is the correct answer. \nTraffic still internal not exposed to internet , as they mentioned creating internal tcp loadbalancer not public one and created vpc peering . so no additional steps are needed."
      },
      {
        "date": "2023-06-13T13:13:00.000Z",
        "voteCount": 1,
        "content": "C is also correct but as the question states, minima effort. In A you dont need vpc peering since you will be using loadbalancer to expose the application, therefore, traffic can still be establishd b/n the two."
      },
      {
        "date": "2023-06-03T21:05:00.000Z",
        "voteCount": 3,
        "content": "Correct is C\nplease refer\nhttps://www.youtube.com/watch?v=qx8PEmxKYzg"
      },
      {
        "date": "2023-04-03T19:03:00.000Z",
        "voteCount": 1,
        "content": "The answer is C as two VPC needed to Peer first"
      },
      {
        "date": "2023-03-30T08:07:00.000Z",
        "voteCount": 1,
        "content": "same region, but in another Virtual Private Cloud (VPC), called gce-network, that has no overlapping IP ranges with the first VPC.... need to read question again and again"
      },
      {
        "date": "2023-03-13T00:50:00.000Z",
        "voteCount": 1,
        "content": "peering is lot easier effort but with dependent on not having overlapping IPs and that was clearly stated on the question. So C without any doubt is the correct answer here IMO."
      },
      {
        "date": "2023-02-19T23:14:00.000Z",
        "voteCount": 6,
        "content": "Answer A is the correct solution.\n\nIn Answer A, we can create a Service of the type LoadBalancer in GKE that uses the application's Pods as a backend. This will create a Google Cloud load balancer with an external IP address that can be used to connect to the application. We can set the service's externalTrafficPolicy to Cluster to ensure that traffic is routed only to the nodes running the application. Then we can configure the Compute Engine instance to use the address of the load balancer that has been created."
      },
      {
        "date": "2023-02-19T23:14:00.000Z",
        "voteCount": 6,
        "content": "Answer B is not recommended because it requires the creation of an additional instance called a proxy, and the use of iptables to forward traffic from gce-network to the GKE nodes. This solution introduces additional complexity and potential points of failure.\n\nAnswer C is not recommended because it requires the peering of two VPCs. This solution is also more complex and requires additional configuration.\n\nAnswer D is not recommended because it involves using Cloud Armor to whitelist the internal IPs of the MIG's instances. This solution introduces additional complexity and potential security risks.\n\nTherefore, Answer A is the most straightforward and least complex solution to connect the Compute Engine instance to the application running on GKE."
      },
      {
        "date": "2023-06-28T10:01:00.000Z",
        "voteCount": 2,
        "content": "externalTrafficPolicy is supported for internal LoadBalancer Services (via the TCP/UDP load balancer), but load balancing behavior depends on where traffic originates from and the configured traffic policy. Hence Answer is C as per link: https://cloud.google.com/kubernetes-engine/docs/how-to/service-parameters#externalTrafficPolicy"
      },
      {
        "date": "2023-03-20T21:53:00.000Z",
        "voteCount": 2,
        "content": "Option C requires less effort compared to option A.\n\nIn option A, you need to set the service's externalTrafficPolicy to Cluster, which means that the traffic will be load balanced across all nodes in the cluster, including those outside of the VPC network. You will also need to configure the Compute Engine instance to use the address of the load balancer that has been created.\n\nIn option C, you only need to add an annotation to the service with the value of \"Internal\", which will create an internal load balancer that is only accessible from within the VPC network. You will also need to peer the two VPCs together and configure the Compute Engine instance to use the address of the load balancer that has been created.\n\nTherefore, option C requires less effort as it involves fewer steps and less configuration."
      },
      {
        "date": "2023-02-12T20:36:00.000Z",
        "voteCount": 3,
        "content": "A is correct\nOption A is the best solution to minimize effort. In GKE, creating a Service of type LoadBalancer that uses the application's Pods as backend and setting the service's externalTrafficPolicy to Cluster will expose the TCP endpoint of the application with a public IP address. Then, configuring the Compute Engine instance to use the address of the load balancer that has been created will allow it to connect to the application on GKE. Option B requires creating a separate instance as a proxy and using iptables to forward traffic, which adds unnecessary complexity. Option C involves peering the two VPCs together, which may not be desirable or feasible in all cases. Option D adds additional complexity by adding a Cloud Armor Security Policy to the load balancer."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 61,
    "url": "https://www.examtopics.com/discussions/google/view/17515-exam-associate-cloud-engineer-topic-1-question-61-discussion/",
    "body": "Your organization is a financial company that needs to store audit log files for 3 years. Your organization has hundreds of Google Cloud projects. You need to implement a cost-effective approach for log file retention. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an export to the sink that saves logs from Cloud Audit to BigQuery.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an export to the sink that saves logs from Cloud Audit to a Coldline Storage bucket.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite a custom script that uses logging API to copy the logs from Stackdriver logs to BigQuery.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExport these logs to Cloud Pub/Sub and write a Cloud Dataflow pipeline to store logs to Cloud SQL."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 28,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-03-26T06:55:00.000Z",
        "voteCount": 56,
        "content": "Why not B? cost effective"
      },
      {
        "date": "2021-10-22T06:03:00.000Z",
        "voteCount": 1,
        "content": "BigQuery data after 90 days has the same cost for storage as Cloud Storage Nearline. Storing it in Cloud Storage adds more costs for data retrival if the class is i.e archival"
      },
      {
        "date": "2023-06-04T22:52:00.000Z",
        "voteCount": 2,
        "content": "the options have cold-line storage not nearline.\nso B is the cheapest option."
      },
      {
        "date": "2023-04-04T11:34:00.000Z",
        "voteCount": 1,
        "content": "That seems to be the one..."
      },
      {
        "date": "2021-12-25T04:10:00.000Z",
        "voteCount": 6,
        "content": "B is correct because Coldline Storage is the perfect service to store audit logs from all the projects and is very cost-efficient as well. Coldline Storage is a very low-cost, highly durable storage service for storing infrequently accessed data."
      },
      {
        "date": "2020-04-03T00:18:00.000Z",
        "voteCount": 20,
        "content": "if it is all about cost, B is the best. However, speaking of \"audit\" you probably need to access the data once in a while, which Coldline storage might not be ideal for this case I guess? I would go for A in the exam though."
      },
      {
        "date": "2020-09-04T00:50:00.000Z",
        "voteCount": 15,
        "content": "Be strong!!! If B is the best, go for B!!!"
      },
      {
        "date": "2021-08-10T16:55:00.000Z",
        "voteCount": 7,
        "content": "The question is clearly saying cost effect. BQ is one of the most expensive services in GCP."
      },
      {
        "date": "2021-09-25T13:23:00.000Z",
        "voteCount": 3,
        "content": "I would play it safe and interpret the question literally, implying that they will only store the audit logs and not be accessing them a lot."
      },
      {
        "date": "2024-08-29T05:38:00.000Z",
        "voteCount": 1,
        "content": "as its logs for 3 years , coldline is the correct"
      },
      {
        "date": "2024-04-26T22:19:00.000Z",
        "voteCount": 1,
        "content": "But the retention period of Coldline storage is 90 days, it's not meeting the requirement mentioned in ques to store for 3 years."
      },
      {
        "date": "2024-04-26T22:14:00.000Z",
        "voteCount": 1,
        "content": "But retention period of coldline storage is 90 days only, in ques they've mentioned for 3 years?"
      },
      {
        "date": "2024-04-04T11:26:00.000Z",
        "voteCount": 2,
        "content": "Using Google Gemini, it suggests Option B."
      },
      {
        "date": "2024-01-21T10:05:00.000Z",
        "voteCount": 1,
        "content": "Both options (exporting to BigQuery and exporting to Coldline Storage) have their merits, and the choice depends on specific use cases, access patterns, and organizational preferences. If your organization values a more structured and analyzable format with SQL-like querying capabilities, BigQuery might be preferable. If the priority is on long-term, infrequent access with cost optimization, then Coldline Storage could be a suitable choice."
      },
      {
        "date": "2023-11-06T01:43:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is B."
      },
      {
        "date": "2023-11-01T04:06:00.000Z",
        "voteCount": 1,
        "content": "B is correct. coldline storage it is cost-effective and for long-term storage"
      },
      {
        "date": "2023-09-21T06:04:00.000Z",
        "voteCount": 1,
        "content": "B is correct. coldline storage it is cost-effective and for long-term storage"
      },
      {
        "date": "2023-04-17T00:51:00.000Z",
        "voteCount": 1,
        "content": "answer B"
      },
      {
        "date": "2023-02-20T13:22:00.000Z",
        "voteCount": 7,
        "content": "Answer B. Create an export to the sink that saves logs from Cloud Audit to a Coldline Storage bucket.\n\nTo meet the requirement of retaining log files for 3 years, a cost-effective approach is to export logs to a cheaper storage option like Cloud Storage Coldline storage class. The coldline storage class is designed for cold data storage and offers lower storage costs and higher retrieval costs when compared to other storage classes.\n\nExporting logs from Cloud Audit to a Coldline Storage bucket can be done by creating an export sink. This is a straightforward process that can be done via the Cloud Console, Cloud SDK, or REST API."
      },
      {
        "date": "2023-02-20T13:23:00.000Z",
        "voteCount": 8,
        "content": "INCORRECT:\n\nAnswer A is incorrect because while it does store logs in BigQuery, it is a more expensive option than storing logs in Coldline storage.\n\nAnswer C is incorrect because writing a custom script to copy logs to BigQuery would be complex and more difficult to maintain compared to using an export sink.\n\nAnswer D is incorrect because while logs can be exported to Pub/Sub, writing a Cloud Dataflow pipeline to store logs in Cloud SQL would require additional configuration and might not be as cost-effective as exporting logs to Coldline storage."
      },
      {
        "date": "2023-09-06T03:48:00.000Z",
        "voteCount": 4,
        "content": "Buru, Thank you so much from whole GCP community for your efforts, you are incredible man !!!"
      },
      {
        "date": "2023-09-14T00:17:00.000Z",
        "voteCount": 1,
        "content": "I agree thanks a lot!!!!"
      },
      {
        "date": "2023-02-03T03:06:00.000Z",
        "voteCount": 1,
        "content": "B. Create an export to the sink that saves logs from Cloud Audit to a Coldline Storage bucket. This is the most cost-effective approach for log file retention as Coldline Storage has lower storage costs compared to other storage classes, making it suitable for infrequently accessed data that still needs to be retained for a long time."
      },
      {
        "date": "2023-01-03T07:17:00.000Z",
        "voteCount": 1,
        "content": "B is the most cost effective"
      },
      {
        "date": "2022-12-25T14:08:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/logging/docs/export/configure_export_v2\nI go with B"
      },
      {
        "date": "2022-12-06T11:12:00.000Z",
        "voteCount": 1,
        "content": "B is the best"
      },
      {
        "date": "2022-11-18T22:51:00.000Z",
        "voteCount": 1,
        "content": "B is the answer, because he wants&nbsp;a&nbsp;cost-effective solution so B is the&nbsp;cheapest option."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 62,
    "url": "https://www.examtopics.com/discussions/google/view/21981-exam-associate-cloud-engineer-topic-1-question-62-discussion/",
    "body": "You want to run a single caching HTTP reverse proxy on GCP for a latency-sensitive website. This specific reverse proxy consumes almost no CPU. You want to have a 30-GB in-memory cache, and need an additional 2 GB of memory for the rest of the processes. You want to minimize cost. How should you run this reverse proxy?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Cloud Memorystore for Redis instance with 32-GB capacity.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun it on Compute Engine, and choose a custom instance type with 6 vCPUs and 32 GB of memory.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPackage it in a container image, and run it on Kubernetes Engine, using n1-standard-32 instances as nodes.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun it on Compute Engine, choose the instance type n1-standard-1, and add an SSD persistent disk of 32 GB."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 21,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-08-18T09:02:00.000Z",
        "voteCount": 55,
        "content": "Go to cloud console and create instance\nselect Memorystore with Basic tier, select us-central1 and us-central1-a, and capacity 32GB, the cost estimate is $0.023/GB/hr\n\nselect VM instance with custom machine type with 6 vCPUs and 32 GB memory, the same region and zone as Memorystore setting, the cost estimate is $0.239/hr\n\nOption B will definitely cost more as it adds on CPU usage cost even it uses little in this scenario, but still charge you. So answer is A from real practice example."
      },
      {
        "date": "2020-08-20T09:15:00.000Z",
        "voteCount": 1,
        "content": "I agree with you"
      },
      {
        "date": "2021-10-15T23:04:00.000Z",
        "voteCount": 2,
        "content": "and what about HTTP, how are you supporting that with Redis?"
      },
      {
        "date": "2022-02-28T23:11:00.000Z",
        "voteCount": 2,
        "content": "A quick Bing search shows a number of solutions for caching HTTP services with Redis."
      },
      {
        "date": "2022-06-25T07:49:00.000Z",
        "voteCount": 12,
        "content": "Who uses Bing at this present day and age?"
      },
      {
        "date": "2022-11-17T10:06:00.000Z",
        "voteCount": 1,
        "content": "believe me it is very good and clean. When I was doing my research I have used both google and bing. and find bing as more trusted and complete answer."
      },
      {
        "date": "2020-09-10T03:41:00.000Z",
        "voteCount": 7,
        "content": "Using pricing calculator matching 730 hrs per month for both.. Memorystore is 537.28 per month and vm (6 cpus 32 gb memory) is 174.41 per month. So vm is still cheaper even with 6 cpus."
      },
      {
        "date": "2020-09-10T12:13:00.000Z",
        "voteCount": 3,
        "content": "I agree its cheaper, but 2 drawbacks, 1 hit latency, 2 you need to install cache plain won't help even if check. So still think A."
      },
      {
        "date": "2020-09-20T01:31:00.000Z",
        "voteCount": 3,
        "content": "Typo correct my statements. There are 2 issues If you create a VM how every cheap there will be a hit with latency when communicating with it. Second, you will need to install/implement a caching system on that. \n\nWhereas cloud memorystore for redis is designed for the sole purpose of HTTP caching which has very low latency compared to any other solution we are thinking of doing."
      },
      {
        "date": "2021-01-12T16:13:00.000Z",
        "voteCount": 4,
        "content": "$0.023  * 32 = $0.736\nis it cheaper?"
      },
      {
        "date": "2020-08-16T16:57:00.000Z",
        "voteCount": 35,
        "content": "Correct Answer should be A:\nThe question mention \"You want to have a 30-GB in-memory cache, and\nneed an additional 2 GB of memory for the rest of the processes\" \n\nWhat is Google Cloud Memorystore?\nOverview. Cloud Memorystore for Redis is a fully managed Redis service for Google Cloud Platform. Applications running on Google Cloud Platform can achieve extreme performance by leveraging the highly scalable, highly available, and secure Redis service without the burden of managing complex Redis deployments."
      },
      {
        "date": "2020-08-23T11:10:00.000Z",
        "voteCount": 20,
        "content": "Just to complement the answer:\nWe are looking for \"latency-sensitive website\" \n\nWhat it's good for\nMemorystore for Redis provides a fast, in-memory store for use cases that require fast, real-time processing of data. From simple caching use cases to real time analytics, Memorystore for Redis provides the performance you need.\n\nCaching: Cache is an integral part of modern application architectures. Memorystore for Redis provides low latency access and high throughput for heavily accessed data, compared to accessing the data from a disk based backend store. Session management, frequently accessed queries, scripts, and pages are common examples of caching.\n\nhttps://cloud.google.com/memorystore/docs/redis/redis-overview#what_its_good_for"
      },
      {
        "date": "2020-09-08T08:11:00.000Z",
        "voteCount": 2,
        "content": "I agree with your reasoning. Given that the question stresses that this is for a 'latency sensitive website', that's a clue that Redis is part of the answer. Even if spinning up a similarly sized VM were more cost effective, I can't find any documentation that this would provide sufficiently low latency as a memory cache. Yes, you want to keep costs low, but not if it causes your latency-sensitive website problems. Thus I agree that option A is the answer."
      },
      {
        "date": "2020-12-29T19:08:00.000Z",
        "voteCount": 2,
        "content": "Agree, don't think anything you create with the Compute Engine will meet the \"You want to have a 30-GB in-memory cache\" requirement...that's a very different technology"
      },
      {
        "date": "2024-09-20T19:15:00.000Z",
        "voteCount": 1,
        "content": "La n1-standard-1 es una instancia de bajo costo con 1 vCPU y 3.75 GB de memoria, suficiente para los procesos adicionales del proxy. Dado que el proxy inverso pr\u00e1cticamente no consume CPU, no es necesario optar por una instancia m\u00e1s grande. El disco persistente SSD de 32 GB puede actuar como almacenamiento para la cach\u00e9 en lugar de usar costosas soluciones en memoria, lo que ayuda a minimizar costos, mientras proporciona un almacenamiento r\u00e1pido, suficiente para el sitio sensible a la latencia."
      },
      {
        "date": "2024-07-30T05:22:00.000Z",
        "voteCount": 1,
        "content": "A might be a fine answer, except that Redis is not an http reverse proxy.  It is a data cache.  So A, regardless of the cost, does not work for this use case."
      },
      {
        "date": "2024-05-23T11:56:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer:B"
      },
      {
        "date": "2023-11-01T04:47:00.000Z",
        "voteCount": 1,
        "content": "A is correct because The question mention \"You want to have a 30-GB in-memory cache, and Redis is inmemory"
      },
      {
        "date": "2023-10-05T04:39:00.000Z",
        "voteCount": 1,
        "content": "B. Run it on Compute Engine, and choose a custom instance type with 6 vCPUs and 32 GB of memory."
      },
      {
        "date": "2023-09-01T22:32:00.000Z",
        "voteCount": 1,
        "content": "A is the correct as redis for low latency"
      },
      {
        "date": "2023-06-09T03:46:00.000Z",
        "voteCount": 2,
        "content": "Low latency should be A."
      },
      {
        "date": "2023-06-03T21:09:00.000Z",
        "voteCount": 1,
        "content": "correct answer is A\nhttps://www.youtube.com/watch?v=a1p1pB375Ik"
      },
      {
        "date": "2023-02-20T13:30:00.000Z",
        "voteCount": 7,
        "content": "ANSWER A is the most cost-effective solution for running a caching HTTP reverse proxy on GCP. Cloud Memorystore for Redis is a managed service that provides an in-memory cache for your applications. It offers a high throughput and low latency access to the Redis protocol. Cloud Memorystore offers an SLA of 99.9% availability and automatic failover for Redis instances. In this case, a 32-GB Redis instance is sufficient to accommodate the 30-GB cache and the additional 2 GB of memory required for the rest of the processes. This solution is highly scalable and allows you to increase the size of the Redis instance as your needs grow."
      },
      {
        "date": "2023-02-20T13:31:00.000Z",
        "voteCount": 4,
        "content": "INCORRECT:\n\nANSWER B is not a cost-effective solution since it requires a custom instance type with 6 vCPUs and 32 GB of memory, which is over-provisioned for a caching HTTP reverse proxy.\n\nANSWER C is also not a cost-effective solution since it uses Kubernetes Engine, which has a higher management overhead and may not be necessary for a single caching HTTP reverse proxy. Additionally, using n1-standard-32 instances as nodes is over-provisioned for the requirements of the caching HTTP reverse proxy.\n\nANSWER D is not a viable solution since the instance type n1-standard-1 only provides 3.75 GB of memory, which is insufficient for the 30-GB cache and the additional 2 GB of memory required for the rest of the processes. Adding an SSD persistent disk of 32 GB will not provide enough memory for the reverse proxy."
      },
      {
        "date": "2023-02-03T03:08:00.000Z",
        "voteCount": 2,
        "content": "A. Create a Cloud Memorystore for Redis instance with 32-GB capacity is the recommended option. This option provides the required memory and is cost-effective since the proxy requires almost no CPU. Cloud Memorystore for Redis is designed specifically for in-memory caching, making it the best choice for your use case."
      },
      {
        "date": "2022-12-06T11:20:00.000Z",
        "voteCount": 1,
        "content": "answer is A"
      },
      {
        "date": "2022-11-29T07:44:00.000Z",
        "voteCount": 1,
        "content": "While Redis is definitely the easiest and best solution for a latency sensitive workload, the question is worded in such a way to emphasize the requirement of cost. \"You want to have a 30-GB in-memory cache, and need an additional 2 GB of memory for the rest of the processes. You want to minimize cost\". Given this, the answer has to be B, even if that's no the best technical solution for the problem."
      },
      {
        "date": "2022-11-18T23:03:00.000Z",
        "voteCount": 1,
        "content": "A is correct, he only wants to have memory capacity, and doesn't care about CPU at all. In addition Memory-store is already configured to use is a cache memory."
      },
      {
        "date": "2022-10-27T17:14:00.000Z",
        "voteCount": 1,
        "content": "How do you figure the correct answer here?   The votes overwhelmingly say one thing, but the correct answer is another."
      },
      {
        "date": "2022-10-18T04:25:00.000Z",
        "voteCount": 1,
        "content": "I changed my mind - cost effective is B (while A is easiest)"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 63,
    "url": "https://www.examtopics.com/discussions/google/view/22725-exam-associate-cloud-engineer-topic-1-question-63-discussion/",
    "body": "You are hosting an application on bare-metal servers in your own data center. The application needs access to Cloud Storage. However, security policies prevent the servers hosting the application from having public IP addresses or access to the internet. You want to follow Google-recommended practices to provide the application with access to Cloud Storage. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Use nslookup to get the IP address for storage.googleapis.com. 2. Negotiate with the security team to be able to give a public IP address to the servers. 3. Only allow egress traffic from those servers to the IP addresses for storage.googleapis.com.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Using Cloud VPN, create a VPN tunnel to a Virtual Private Cloud (VPC) in Google Cloud. 2. In this VPC, create a Compute Engine instance and install the Squid proxy server on this instance. 3. Configure your servers to use that instance as a proxy to access Cloud Storage.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Use Migrate for Compute Engine (formerly known as Velostrata) to migrate those servers to Compute Engine. 2. Create an internal load balancer (ILB) that uses storage.googleapis.com as backend. 3. Configure your new instances to use this ILB as proxy.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Using Cloud VPN or Interconnect, create a tunnel to a VPC in Google Cloud. 2. Use Cloud Router to create a custom route advertisement for 199.36.153.4/30. Announce that network to your on-premises network through the VPN tunnel. 3. In your on-premises network, configure your DNS server to resolve *.googleapis.com as a CNAME to restricted.googleapis.com.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 21,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-06-10T04:19:00.000Z",
        "voteCount": 54,
        "content": "D is the correct one as per Ref: https://cloud.google.com/vpc/docs/configure-private-google-access-hybrid"
      },
      {
        "date": "2022-02-28T23:18:00.000Z",
        "voteCount": 23,
        "content": "What messy answers!  I chose D and here is my reasoning per answer.\n\nA. It's bad practice to use nslookup to try find a permanent IP address because IPs can change.  That's what DNS is for!  Also, the security team aren't going to budge... this is just a silly answer.  \nB. We're getting warmer.  Any time a question mentions on-prem and cloud, Google wants you to think about Cloud VPN.  This solution might even work, but installing Squid?  This is a messy solution to a more simple problem.\nC.  Talk about using a sledge hammer to swat a mosquito.   I think this could work, but migrating servers to cloud to solve a simple networking problem?\nD. Once more Google's favorite Cloud VPN is in the answer.  I'm not sure about the networking component of this question."
      },
      {
        "date": "2022-02-28T23:21:00.000Z",
        "voteCount": 15,
        "content": "Edit: Of course the reason D: is correct is because 199.36.153.4/30 is the network segment that you can direct traffic to if you want to use Google services \"internally\".   So your on prem servers will resolve storage.googleapis.com to something in this 199.36.153.4/30 range.  Then they will route using Cloud Router and your VPN tunnel into Google Cloud privately."
      },
      {
        "date": "2024-05-23T11:56:00.000Z",
        "voteCount": 1,
        "content": "C is the correct Answer"
      },
      {
        "date": "2023-11-06T01:48:00.000Z",
        "voteCount": 2,
        "content": "The correct answer is D"
      },
      {
        "date": "2023-11-01T04:50:00.000Z",
        "voteCount": 1,
        "content": "D as per google recommened pratcises"
      },
      {
        "date": "2023-09-01T22:35:00.000Z",
        "voteCount": 1,
        "content": "D as per google recommened pratcises"
      },
      {
        "date": "2023-02-20T13:50:00.000Z",
        "voteCount": 9,
        "content": "ANSWER D is the recommended solution because it provides a secure and direct connection to Cloud Storage without requiring internet access or exposing the servers to public IP addresses. \n\n   * By setting up a VPN or Interconnect tunnel, the on-premises servers can access Google \n      Cloud resources over a private and encrypted connection.\n\n   * The custom route advertisement for 199.36.153.4/30 ensures that traffic is routed \n     correctly between the on-premises network and Google Cloud.\n\n   * Configuring the DNS server to resolve *.googleapis.com as a CNAME to \n      restricted.googleapis.com ensures that requests are directed to Google Cloud over the \n      VPN or Interconnect tunnel."
      },
      {
        "date": "2023-01-02T01:39:00.000Z",
        "voteCount": 5,
        "content": "D but anyone wanna try to explain how the hell you can have a VPN connection without accessing the public internet? The only option for D should be using Interconnect for a direct private wire from your data center to GCP. VPN doesn't make any sense."
      },
      {
        "date": "2023-03-07T06:55:00.000Z",
        "voteCount": 1,
        "content": "The machine hosting the application cannot access directly the public internet. So to go to Google Cloud it must go through a VPN."
      },
      {
        "date": "2023-01-06T06:54:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/vpc/docs/configure-private-google-access-hybrid"
      },
      {
        "date": "2022-12-06T11:24:00.000Z",
        "voteCount": 1,
        "content": "D is the correct"
      },
      {
        "date": "2022-09-26T19:36:00.000Z",
        "voteCount": 1,
        "content": "correct answer is D. why will cx migrate it env. to GCP. easiest and faster approach is to have Cloud VPN setup and advertise route o cloud router"
      },
      {
        "date": "2022-09-26T19:38:00.000Z",
        "voteCount": 1,
        "content": "in my above comment I meant advertise routes to local router used on-prem"
      },
      {
        "date": "2022-09-24T10:24:00.000Z",
        "voteCount": 3,
        "content": "had this question today"
      },
      {
        "date": "2022-08-21T02:15:00.000Z",
        "voteCount": 5,
        "content": "how you all know that everybody  \n\nmere to upar se nikal raha hai sab kuch pls help guys"
      },
      {
        "date": "2022-10-01T23:18:00.000Z",
        "voteCount": 2,
        "content": "tu akela nhi hai bro"
      },
      {
        "date": "2022-10-21T22:11:00.000Z",
        "voteCount": 1,
        "content": "Me too bro"
      },
      {
        "date": "2022-08-06T07:44:00.000Z",
        "voteCount": 1,
        "content": "Answer is D"
      },
      {
        "date": "2022-07-02T07:54:00.000Z",
        "voteCount": 1,
        "content": "Answer is D"
      },
      {
        "date": "2022-06-23T10:06:00.000Z",
        "voteCount": 1,
        "content": "D is most appropriate, This is part of Tutorial Dojo practice questions."
      },
      {
        "date": "2022-06-03T02:54:00.000Z",
        "voteCount": 1,
        "content": "Go for D"
      },
      {
        "date": "2022-02-21T11:00:00.000Z",
        "voteCount": 2,
        "content": "D is the correct answer"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 64,
    "url": "https://www.examtopics.com/discussions/google/view/22259-exam-associate-cloud-engineer-topic-1-question-64-discussion/",
    "body": "You want to deploy an application on Cloud Run that processes messages from a Cloud Pub/Sub topic. You want to follow Google-recommended practices. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a Cloud Function that uses a Cloud Pub/Sub trigger on that topic. 2. Call your application on Cloud Run from the Cloud Function for every message.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Grant the Pub/Sub Subscriber role to the service account used by Cloud Run. 2. Create a Cloud Pub/Sub subscription for that topic. 3. Make your application pull messages from that subscription.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a service account. 2. Give the Cloud Run Invoker role to that service account for your Cloud Run application. 3. Create a Cloud Pub/Sub subscription that uses that service account and uses your Cloud Run application as the push endpoint.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Deploy your application on Cloud Run on GKE with the connectivity set to Internal. 2. Create a Cloud Pub/Sub subscription for that topic. 3. In the same Google Kubernetes Engine cluster as your application, deploy a container that takes the messages and sends them to your application."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 17,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 9,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-06-05T18:43:00.000Z",
        "voteCount": 57,
        "content": "C looks right for me as per https://cloud.google.com/run/docs/tutorials/pubsub#integrating-pubsub"
      },
      {
        "date": "2021-07-25T17:02:00.000Z",
        "voteCount": 6,
        "content": "great doc, its' C"
      },
      {
        "date": "2020-12-19T01:35:00.000Z",
        "voteCount": 40,
        "content": "why c ?\nexplained&gt;&gt;\n You can use Pub/Sub to push messages to the endpoint of your Cloud Run service, where the messages are subsequently delivered to containers as HTTP requests. You cannot use Pub/Sub pull subscriptions because Cloud Run only allocates CPU during the processing of a request."
      },
      {
        "date": "2024-09-27T20:14:00.000Z",
        "voteCount": 1,
        "content": "C is right Answer"
      },
      {
        "date": "2024-05-23T11:57:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is D"
      },
      {
        "date": "2024-04-13T05:19:00.000Z",
        "voteCount": 3,
        "content": "Here's why Option B might be preferred over Option C:\nService Account Permissions: Option C involves creating a separate service account and granting it the Cloud Run Invoker role.\n\nDirect Subscription Configuration: Option B allows for a direct configuration of a Cloud Pub/Sub subscription to pull messages from the topic eliminating the need to manage service accounts and roles separately.\n\nStandard Practice: Granting the Pub/Sub Subscriber role directly to the service account used by Cloud Run is a standard practice for allowing Cloud Run services to access Pub/Sub topics follows the principle of least privilege by granting only the necessary permissions to the service account.\n\nPush vs. Pull Model: Option C uses a push model where Cloud Pub/Sub sends messages directly to the Cloud Run service. While this model can work, it requires additional setup for configuring the push endpoint \n\nOverall, Option B provides a simpler and more direct approach to integrating Cloud Run with Cloud Pub/Sub, aligning well with Google-recommended practices"
      },
      {
        "date": "2023-11-22T22:51:00.000Z",
        "voteCount": 1,
        "content": "C looks the correct option."
      },
      {
        "date": "2023-05-04T20:10:00.000Z",
        "voteCount": 3,
        "content": "it is explicated recommended use \" push\" : \nNote: Google recommends using push subscriptions to consume messages from a Pub/Sub topic on Cloud Run. Although it is possible to use Pub/Sub pull subscriptions, pull subscriptions require you to monitor message delivery latency and manually scale the number of instances to maintain a healthy delivery latency. If you want to use pull subscriptions, use the CPU always allocated setting along with a number of minimum instances.\nhttps://cloud.google.com/run/docs/triggering/pubsub-push"
      },
      {
        "date": "2023-04-20T20:31:00.000Z",
        "voteCount": 1,
        "content": "what is the actual answer people who are voting or actual answer?"
      },
      {
        "date": "2023-04-04T06:33:00.000Z",
        "voteCount": 3,
        "content": "ExamPro also explains it's C."
      },
      {
        "date": "2023-03-12T07:38:00.000Z",
        "voteCount": 2,
        "content": "\"processes messages from a Cloud Pub/Sub topic\"\nshould be pull subscription\ni will go with B"
      },
      {
        "date": "2023-02-20T14:06:00.000Z",
        "voteCount": 4,
        "content": "Answer B, I would say is the correct answer for me.\n\nWhen you want to deploy an application on Cloud Run that processes messages from a Cloud Pub/Sub topic, you should follow Google-recommended practices to ensure that your application can securely and reliably process the messages.\n\nAnswer A is not the correct answer because using a Cloud Function to call your application on Cloud Run for every message adds additional complexity and potential points of failure."
      },
      {
        "date": "2023-02-20T14:07:00.000Z",
        "voteCount": 1,
        "content": "Answer C is not the correct answer ALTHOUGH it is possible to create a subscription that uses Cloud Run as the push endpoint, it requires an additional setup that is not necessary for this use case. Additionally, this approach requires your Cloud Run service to be publicly accessible.\n\nAnswer D is not the correct answer because it requires deploying a container in a Kubernetes cluster to handle the Pub/Sub messages and forward them to Cloud Run. This adds additional complexity to the architecture, and Cloud Run can directly subscribe to a Pub/Sub topic without needing a proxy service in between.\n\nTherefore, the recommended approach is to grant the Pub/Sub Subscriber role to the service account used by Cloud Run and create a Cloud Pub/Sub subscription for that topic. This approach is straightforward and aligns with Google's best practices for integrating Pub/Sub with Cloud Run."
      },
      {
        "date": "2023-02-26T02:17:00.000Z",
        "voteCount": 1,
        "content": "The advantage of Cloud Run scaling to zero might be a better practice like Shwom suggested.\nI also thought B cus of it's straightforward approach, but C seems better now."
      },
      {
        "date": "2023-02-02T12:57:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2022-12-08T22:27:00.000Z",
        "voteCount": 1,
        "content": "C is the right answer"
      },
      {
        "date": "2022-11-30T08:04:00.000Z",
        "voteCount": 6,
        "content": "Ans is C (push subscription). \n \nI think B (pull subscription) can work, but it does not follow best practices. With pull subscription, you cannot take advantage of scale-to-zero for Cloud Run. Why is this the case? Because you need to keep your subscriber client (Cloud Run in option B) running, otherwise you don't know if there are new messages. (Whereas if you use push subscription instead, you can scale to zero and rest assured that Cloud Run will be \"notified\" of new messages).\n\nSee 3:15 and 4:10 of this video: https://www.youtube.com/watch?v=KObJkda4ZfY\n\nRemember this:\nPush subscription -&gt; Pub/Sub server initiates a request to your subscriber client to deliver messages\nPull subscription -&gt; Your subscriber client initiates requests to a Pub/Sub server to retrieve messages\n\nDocumentation: https://cloud.google.com/pubsub/docs/subscriber"
      },
      {
        "date": "2023-09-30T07:02:00.000Z",
        "voteCount": 1,
        "content": "Explained well enough. Recommended"
      },
      {
        "date": "2022-11-30T07:52:00.000Z",
        "voteCount": 1,
        "content": "Ans is C (push subscription). \n\nI think B (pull subscription) can work, but it does not follow best practices. You cannot take advantage of scale-to-zero for Cloud Run. See 3:15 and 4:10 of this video: https://www.youtube.com/watch?v=KObJkda4ZfY"
      },
      {
        "date": "2022-10-30T08:33:00.000Z",
        "voteCount": 1,
        "content": "C obviously"
      },
      {
        "date": "2022-07-02T07:58:00.000Z",
        "voteCount": 1,
        "content": "Answer is C"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 65,
    "url": "https://www.examtopics.com/discussions/google/view/22624-exam-associate-cloud-engineer-topic-1-question-65-discussion/",
    "body": "You need to deploy an application, which is packaged in a container image, in a new project. The application exposes an HTTP endpoint and receives very few requests per day. You want to minimize costs. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the container on Cloud Run.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the container on Cloud Run on GKE.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the container on App Engine Flexible.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the container on GKE with cluster autoscaling and horizontal pod autoscaling enabled."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 36,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-06-13T13:42:00.000Z",
        "voteCount": 71,
        "content": "A should be cheapest as no infra needed."
      },
      {
        "date": "2020-07-02T05:16:00.000Z",
        "voteCount": 32,
        "content": "Listen to this guy. Google says \"Cloud Run abstracts away all infrastructure management by automatically scaling up and down from zero almost instantaneously\u2014depending on traffic. Cloud Run only charges you for the exact resources you use.\""
      },
      {
        "date": "2020-08-16T17:22:00.000Z",
        "voteCount": 27,
        "content": "Correct Answer should be A:\n\nCloud Run takes any container images and pairs great with the container ecosystem: Cloud Build, Artifact Registry, Docker. ... No infrastructure to manage: once deployed, Cloud Run manages your services so you can sleep well. Fast autoscaling. Cloud Run automatically scales up or down from zero to N depending on traffic.\n\nhttps://cloud.google.com/run"
      },
      {
        "date": "2024-09-20T22:05:00.000Z",
        "voteCount": 1,
        "content": "Cloud Run es una plataforma serverless que permite ejecutar contenedores de manera altamente escalable y a un costo muy bajo, ya que solo se paga por las solicitudes recibidas y el tiempo de ejecuci\u00f3n. Dado que la aplicaci\u00f3n recibe pocas solicitudes por d\u00eda, Cloud Run es la opci\u00f3n m\u00e1s rentable, ya que no incurre en costos cuando no hay tr\u00e1fico. Adem\u00e1s, es f\u00e1cil de implementar y mantiene la infraestructura al m\u00ednimo, lo que optimiza tanto costos como administraci\u00f3n."
      },
      {
        "date": "2024-06-17T00:33:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2024-06-13T04:34:00.000Z",
        "voteCount": 1,
        "content": "A is the correct answer"
      },
      {
        "date": "2024-05-23T11:57:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer: B"
      },
      {
        "date": "2023-11-06T02:03:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is A"
      },
      {
        "date": "2023-11-06T02:00:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is A"
      },
      {
        "date": "2023-09-01T22:40:00.000Z",
        "voteCount": 1,
        "content": "A, as it does not include the infra services and its cheaper"
      },
      {
        "date": "2023-08-09T11:29:00.000Z",
        "voteCount": 1,
        "content": "Should be A"
      },
      {
        "date": "2023-05-23T03:04:00.000Z",
        "voteCount": 1,
        "content": "Can anyone confirm these questions are the exact questions that would come in the exam? I can see there are 201 questions mentioned but I got only 197. Also, with each question, its mentioned topic 1. Does that mean there are more topics for ACE exam?"
      },
      {
        "date": "2023-02-20T14:23:00.000Z",
        "voteCount": 7,
        "content": "Definitely the correct answer is A, which is deploying the container on Cloud Run, is the most cost-effective option to deploy a container image that exposes an HTTP endpoint and receives very few requests per day. \n\nCloud Run is a fully managed serverless platform that allows you to run stateless containers and automatically scales up or down based on incoming requests. \n\nWith Cloud Run, you only pay for the actual usage, so if your application receives very few requests per day, your costs will be minimal."
      },
      {
        "date": "2022-12-06T11:31:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer should be A"
      },
      {
        "date": "2022-11-14T08:30:00.000Z",
        "voteCount": 2,
        "content": "A is correct, it's scaled automatically and service-lees and more cheaper than GKE."
      },
      {
        "date": "2022-11-11T06:53:00.000Z",
        "voteCount": 1,
        "content": "Serverless option: Cloud Run is the cheapest"
      },
      {
        "date": "2022-09-24T10:24:00.000Z",
        "voteCount": 7,
        "content": "had this question today"
      },
      {
        "date": "2022-07-31T05:46:00.000Z",
        "voteCount": 1,
        "content": "Don't forget that cloud run invoking rules\n\"Cloud Run redirects all HTTP requests to HTTPS but terminates TLS before they reach your web service.\"\nin last case when i use cloud run with POST request and HTTP method, it will generate bug which redirect the request to GET https header :)\nhttps://cloud.google.com/run/docs/triggering/https-request#"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 66,
    "url": "https://www.examtopics.com/discussions/google/view/22104-exam-associate-cloud-engineer-topic-1-question-66-discussion/",
    "body": "Your company has an existing GCP organization with hundreds of projects and a billing account. Your company recently acquired another company that also has hundreds of projects and its own billing account. You would like to consolidate all GCP costs of both GCP organizations onto a single invoice. You would like to consolidate all costs as of tomorrow. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLink the acquired company's projects to your company's billing account.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the acquired company's billing account and your company's billing account to export the billing data into the same BigQuery dataset.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the acquired company's projects into your company's GCP organization. Link the migrated projects to your company's billing account.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new GCP organization and a new billing account. Migrate the acquired company's projects and your company's projects into the new GCP organization and link the projects to the new billing account."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 24,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 7,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-06-23T03:50:00.000Z",
        "voteCount": 56,
        "content": "To me, A looks correct. projects are linked to another organization as well in the acquired company so migrating would need google cloud support. we can not do ourselves. however, we can link other company projects to an existing billing account to generate total cost.\nhttps://medium.com/google-cloud/google-cloud-platform-cross-org-billing-41c5db8fefa6"
      },
      {
        "date": "2020-07-02T05:23:00.000Z",
        "voteCount": 10,
        "content": "Listen to this guy. It's 'A' as moving projects can take some time from Google. There's no need to create a new organisation and other options don't make any sense"
      },
      {
        "date": "2021-05-09T01:23:00.000Z",
        "voteCount": 2,
        "content": "You're saying it as if \"moving projects\" was a viable option. What about B?"
      },
      {
        "date": "2022-08-07T22:31:00.000Z",
        "voteCount": 3,
        "content": "I think B is not make sense. You don't want to do statistical analytic to the billing data. You want to consolidate all the costs as of tomorrow. So, the costs as of tomorrow should be billed in one billing account. That's what I've understand from the question."
      },
      {
        "date": "2021-12-25T04:28:00.000Z",
        "voteCount": 5,
        "content": "A is correct because linking all projects of the acquired organization to the main organization\u2019s billing account will generate a single bill for all projects.\nD is incorrect because there is no need to create a new organization for this."
      },
      {
        "date": "2020-08-07T01:45:00.000Z",
        "voteCount": 47,
        "content": "I could be missing something but where does it say in the question that the two orgs want to migrate projects?  I believe the question and key points are \"consolidate all GCP costs\" and \"consolidate all costs as of tomorrow\".  With that said, C and D would not be a 24 hour task and seems a bit cumbersome to perform for something simple as \"creating a single invoice\" AND that's a migration and not a consolidation of cost.  With A, I can't find anywhere in GCP docs that this is a best practice, only a medium.com blog.  IMHO, I won't go down this route because \"Just because you can do something, doesn't mean you should.\" and I would consult GCP support for best practices on A before I do something like that.\n\nThat leaves B which is to export both detailed billing to BigQuery and create a invoice/report.  This would be a temporary solution until you migrate Organizations.  IMHO\n\nI go with B."
      },
      {
        "date": "2021-08-14T11:43:00.000Z",
        "voteCount": 5,
        "content": "I also vote B,\nwhy?\nagree with this technical explanation and my finance team not gonna pay some newly acquired company bill by tomorrow :)"
      },
      {
        "date": "2021-12-06T01:23:00.000Z",
        "voteCount": 1,
        "content": "Are you sure you can do all steps by tomorrow? \n(You would like to consolidate all costs as of tomorrow)"
      },
      {
        "date": "2022-12-06T06:09:00.000Z",
        "voteCount": 4,
        "content": "What does exporting data to BigQuery have to do with creating an Invoice?"
      },
      {
        "date": "2021-02-18T10:11:00.000Z",
        "voteCount": 17,
        "content": "I am not sure that exporting some statistical data to BigQuery means anything for Google who creates the invoice.\nWith \"A\" you are right, that is not the best practice, but the key word \"for tomorrow\" allows this custom approach. \nSo the answer is \"A\""
      },
      {
        "date": "2022-04-01T20:04:00.000Z",
        "voteCount": 1,
        "content": "I will go with A in the exam as well, but just wondering, they are two different organisations, how can you link all projects from org2 to org1's billing account without the help of GCP support??"
      },
      {
        "date": "2023-01-24T06:30:00.000Z",
        "voteCount": 1,
        "content": "Cloud Billing accounts can be used across organization resources. However, organization resource moves often also include a requirement to move to a new billing account. To get the permissions that you need to change the project's billing account, ask your administrator to grant you the following IAM roles:\n\nBilling Account User (roles/billing.user) on the destination billing account\nProject billing manager (roles/billing.projectManager) on the project\n\nhttps://cloud.google.com/resource-manager/docs/project-migration#permissions-billing"
      },
      {
        "date": "2024-09-23T05:49:00.000Z",
        "voteCount": 1,
        "content": "I will go for C"
      },
      {
        "date": "2024-04-13T05:29:00.000Z",
        "voteCount": 1,
        "content": "A. Link the acquired company's projects to your company's billing account.\n\nExplanation:\n\nBilling Account Linking: By linking the acquired company's projects to your company's billing account, you can consolidate all costs onto a single invoice. This allows for centralized billing management and easier tracking of expenses.\n\nImmediate Consolidation: This action can be implemented quickly and efficiently, allowing for cost consolidation as of tomorrow, as specified in the requirement.\n\nMinimal Disruption: Linking projects to a different billing account does not require significant changes to the existing project configurations or organizational structure. It allows both companies to maintain their separate GCP organizations and project structures while consolidating billing.\n\nCost Tracking: With all costs consolidated onto a single invoice, it becomes easier to track expenses and manage budgets effectively."
      },
      {
        "date": "2024-02-20T14:39:00.000Z",
        "voteCount": 1,
        "content": "C. Migrate the acquired company's projects into your company's GCP organization. Link the migrated projects to your company's billing account.\n\nHere's why:\n\nOption A: Linking projects to a different billing account doesn't consolidate costs onto a single invoice.\nOption B: Exporting data to a shared BigQuery dataset allows analysis but doesn't consolidate billing itself.\nOption C: This approach achieves your goal efficiently:\nMigration: Moving acquired company projects into your organization allows centralized management and cost consolidation.\nLinking to existing billing account: Ensures all project costs appear on your existing invoice starting from the day of migration.\nTiming: Given the urgency of same-day consolidation, this is the fastest option."
      },
      {
        "date": "2024-02-14T14:18:00.000Z",
        "voteCount": 4,
        "content": "This question is outdated: As of October 26, 2023, Google Cloud Platform does not allow directly linking projects between separate organizations to a single billing account. Each organization must have its own billing account, and resource costs cannot be directly consolidated across distinct organizations."
      },
      {
        "date": "2023-12-30T04:44:00.000Z",
        "voteCount": 1,
        "content": "C should be be correct as per me"
      },
      {
        "date": "2023-11-06T02:05:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is D"
      },
      {
        "date": "2023-09-01T22:44:00.000Z",
        "voteCount": 1,
        "content": "A is the correct answer"
      },
      {
        "date": "2023-08-09T11:36:00.000Z",
        "voteCount": 1,
        "content": "To consolidate all GCP costs of both organizations onto a single invoice, you would need to set up a new GCP organization and a new billing account. This new billing account would be used to aggregate the costs of both companies' projects.\n\nOPTION D"
      },
      {
        "date": "2023-07-20T12:05:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is B. By configuring both billing accounts to export their billing data into the same BigQuery dataset, you can perform cost analysis and generate consolidated reports. This will enable you to have a comprehensive view of the costs incurred by both organizations and facilitate the process of managing expenses and making financial decisions."
      },
      {
        "date": "2023-05-02T02:07:00.000Z",
        "voteCount": 1,
        "content": "A would be the quickest"
      },
      {
        "date": "2023-05-23T03:07:00.000Z",
        "voteCount": 1,
        "content": "Hi Krishna: I have a query:\nCould you confirm if these questions are the exact questions that would come in the exam? I can see there are 201 questions mentioned but I got only 197. Also, with each question, its mentioned topic 1. Does that mean there are more topics for ACE exam?"
      },
      {
        "date": "2023-04-12T17:06:00.000Z",
        "voteCount": 3,
        "content": "option D is not the best option we can have since it involves additional effort and resources, .i.e migrating all projects from both companies which is a hassle and time consuming. \n\nour best option is A since we acquire new company we just add it to our \"company\" for the purpose of putting it onto single invoice"
      },
      {
        "date": "2023-04-05T11:32:00.000Z",
        "voteCount": 1,
        "content": "Hello everyone"
      },
      {
        "date": "2023-02-20T14:42:00.000Z",
        "voteCount": 5,
        "content": "ANSWER \"A\" would be the quickest way to consolidate all costs onto a single invoice as of tomorrow. As in \"tomorrow\" or else your BOSS will fire you for not providing within 24 hours. LOL! Just kidding. By linking the acquired company's projects to your company's billing account, you can ensure that all costs for both GCP organizations are billed to a single billing account.\n\nANSWER \"C\" would be the proper way to consolidate the GCP organizations in the long term, but it would take more time to migrate the acquired company's projects into your company's GCP organization and link the projects to your company's billing account.\n\nANSWER \"B\" suggests configuring both billing accounts to export billing data into the same BigQuery dataset. While this approach can help consolidate billing data and provide a more comprehensive view of costs, it does not provide a single invoice for all costs as the question requires."
      },
      {
        "date": "2023-02-20T14:43:00.000Z",
        "voteCount": 4,
        "content": "ANSWER \"D\" involves creating a new GCP organization and a new billing account and migrating both companies' projects into the new organization and billing account. While this approach can consolidate costs onto a single invoice, it is a more complex and time-consuming process that may not be feasible to complete within 24 hours as the question requires. It is also not ideal if the two companies' projects have different access control requirements or billing policies that need to be maintained separately.\n\nTherefore, in the context of the given question and the requirement to consolidate costs as of tomorrow onto a single invoice, ANSWER \"A\", linking the acquired company's projects to the existing billing account, is the most appropriate and practical solution."
      },
      {
        "date": "2023-02-03T03:36:00.000Z",
        "voteCount": 2,
        "content": "C. Migrate the acquired company's projects into your company's GCP organization. Link the migrated projects to your company's billing account.\n\nTo consolidate costs onto a single invoice, you need to bring all projects under the same GCP organization and link them to the same billing account. The best way to achieve this is to migrate the acquired company's projects into your company's GCP organization and link them to your company's billing account. This will ensure that all costs are consolidated onto a single invoice and can be easily managed and monitored from a single location."
      },
      {
        "date": "2022-12-06T11:33:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer should be A"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 67,
    "url": "https://www.examtopics.com/discussions/google/view/22190-exam-associate-cloud-engineer-topic-1-question-67-discussion/",
    "body": "You built an application on Google Cloud that uses Cloud Spanner. Your support team needs to monitor the environment but should not have access to table data.<br>You need a streamlined solution to grant the correct permissions to your support team, and you want to follow Google-recommended practices. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd the support team group to the roles/monitoring.viewer role\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd the support team group to the roles/spanner.databaseUser role.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd the support team group to the roles/spanner.databaseReader role.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd the support team group to the roles/stackdriver.accounts.viewer role."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 23,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-06-10T04:24:00.000Z",
        "voteCount": 46,
        "content": "its A, As you need to monitor only"
      },
      {
        "date": "2021-07-13T06:15:00.000Z",
        "voteCount": 15,
        "content": "A, right, correct answer.\nB and C are incorrect because allow to read data. \nD also incorrect: Not for monitoring. roles/stackdriver.accounts.viewer\tStackdriver Accounts Viewer: \nRead-only access to get and list information about Stackdriver account structure (resourcemanager.projects.get, resourcemanager.projects.list and stackdriver.projects.get)"
      },
      {
        "date": "2021-07-13T06:15:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/iam/docs/understanding-roles"
      },
      {
        "date": "2020-06-13T14:47:00.000Z",
        "voteCount": 19,
        "content": "A is correct as user should not have any access to data, so B and C cant be used in this scenario."
      },
      {
        "date": "2024-06-24T22:30:00.000Z",
        "voteCount": 1,
        "content": "Its D.\nStackdriver roles in GCP (Google Cloud Platform) are predefined sets of permissions that control access to monitoring and logging data within Stackdriver, a suite of tools for monitoring and logging applications and infrastructure in GCP.\n\nThese roles determine what users or groups can see and do within Stackdriver.  They allow you to grant granular access levels, ensuring users have the necessary permissions to perform their tasks without exposing sensitive data or granting unnecessary control."
      },
      {
        "date": "2023-10-06T00:38:00.000Z",
        "voteCount": 2,
        "content": "A. Add the support team group to the roles/monitoring.viewer role"
      },
      {
        "date": "2023-09-30T07:08:00.000Z",
        "voteCount": 1,
        "content": "Makes sense for me"
      },
      {
        "date": "2023-09-01T22:45:00.000Z",
        "voteCount": 1,
        "content": "A as you only need the monitor access"
      },
      {
        "date": "2023-04-12T17:15:00.000Z",
        "voteCount": 1,
        "content": "the goal of support team  is to MONITOR the environment only. therefore  roles/monitoring.viewer role is the best option we have \n\nhttps://cloud.google.com/spanner/docs/iam#roles"
      },
      {
        "date": "2023-02-20T15:19:00.000Z",
        "voteCount": 8,
        "content": "Answer A, adding the support team group to the roles/monitoring.viewer role, is the CORRECT answer. This role grants read-only access to monitoring data for all resources in a project, which allows the support team to monitor the environment but not access the table data.\n\nAnswer B, adding the support team group to the roles/spanner.databaseUser role, grants read and write access to all tables in the specified database, which is NOT required for the support team to monitor the environment.\n\nAnswer C, adding the support team group to the roles/spanner.databaseReader role, grants read-only access to all tables in the specified database, which would give the support team access to the table data.\n\nAnswer D, adding the support team group to the roles/stackdriver.accounts.viewer role, grants permissions to view Stackdriver data for all resources in a project, which is NOT directly related to monitoring the Cloud Spanner environment."
      },
      {
        "date": "2022-12-06T11:42:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2022-11-18T23:53:00.000Z",
        "voteCount": 1,
        "content": "A is correct, the team need to monitor the environment not read the data."
      },
      {
        "date": "2022-09-24T10:25:00.000Z",
        "voteCount": 3,
        "content": "had this question today"
      },
      {
        "date": "2022-09-16T01:34:00.000Z",
        "voteCount": 1,
        "content": "B is wrong because it grants write access also we only need monitoring access."
      },
      {
        "date": "2022-07-02T08:05:00.000Z",
        "voteCount": 2,
        "content": "A. This is the only role that provides read-only access to get and list information about all monitoring data and configurations."
      },
      {
        "date": "2022-06-23T10:15:00.000Z",
        "voteCount": 1,
        "content": "You only need to monitor so A is correct!"
      },
      {
        "date": "2022-06-23T10:16:00.000Z",
        "voteCount": 1,
        "content": "roles/monitoring.viewer\nMonitoring Viewer\tGrants read-only access to Monitoring in the Google Cloud console and API."
      },
      {
        "date": "2022-06-06T00:14:00.000Z",
        "voteCount": 2,
        "content": "A is correct"
      },
      {
        "date": "2022-06-03T23:58:00.000Z",
        "voteCount": 1,
        "content": "Go for A"
      },
      {
        "date": "2022-01-17T01:05:00.000Z",
        "voteCount": 8,
        "content": "This was there in exam, go with community answers."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 68,
    "url": "https://www.examtopics.com/discussions/google/view/47413-exam-associate-cloud-engineer-topic-1-question-68-discussion/",
    "body": "For analysis purposes, you need to send all the logs from all of your Compute Engine instances to a BigQuery dataset called platform-logs. You have already installed the Cloud Logging agent on all the instances. You want to minimize cost. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Give the BigQuery Data Editor role on the platform-logs dataset to the service accounts used by your instances. 2. Update your instances' metadata to add the following value: logs-destination: bq://platform-logs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. In Cloud Logging, create a logs export with a Cloud Pub/Sub topic called logs as a sink. 2. Create a Cloud Function that is triggered by messages in the logs topic. 3. Configure that Cloud Function to drop logs that are not from Compute Engine and to insert Compute Engine logs in the platform-logs dataset.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. In Cloud Logging, create a filter to view only Compute Engine logs. 2. Click Create Export. 3. Choose BigQuery as Sink Service, and the platform-logs dataset as Sink Destination.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a Cloud Function that has the BigQuery User role on the platform-logs dataset. 2. Configure this Cloud Function to create a BigQuery Job that executes this query: INSERT INTO dataset.platform-logs (timestamp, log) SELECT timestamp, log FROM compute.logs WHERE timestamp &gt; DATE_SUB(CURRENT_DATE(), INTERVAL 1 DAY) 3. Use Cloud Scheduler to trigger this Cloud Function once a day."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 15,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-03-16T14:30:00.000Z",
        "voteCount": 22,
        "content": "vote for ''C\"\n\nhttps://cloud.google.com/logging/docs/export/configure_export_v2"
      },
      {
        "date": "2021-06-11T08:53:00.000Z",
        "voteCount": 7,
        "content": "I vote for C"
      },
      {
        "date": "2024-01-30T04:12:00.000Z",
        "voteCount": 1,
        "content": "Option C"
      },
      {
        "date": "2023-11-06T02:30:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is C"
      },
      {
        "date": "2023-09-30T07:10:00.000Z",
        "voteCount": 2,
        "content": "C, it's simple enough"
      },
      {
        "date": "2023-09-01T22:47:00.000Z",
        "voteCount": 1,
        "content": "C , it minimizes the cost"
      },
      {
        "date": "2023-07-20T12:09:00.000Z",
        "voteCount": 3,
        "content": "Answer: C. Option C allows you to create a log export from Cloud Logging to BigQuery with minimal setup and cost. By creating a filter to view only Compute Engine logs, you ensure that only the relevant logs are exported to BigQuery, reducing unnecessary data transfer and storage costs."
      },
      {
        "date": "2023-05-30T07:13:00.000Z",
        "voteCount": 1,
        "content": "B looks like the most cost effective option since filtering out only the logs you need will reduce storage and data transfer costs."
      },
      {
        "date": "2023-06-09T11:19:00.000Z",
        "voteCount": 1,
        "content": "Correction - C"
      },
      {
        "date": "2023-02-20T15:26:00.000Z",
        "voteCount": 3,
        "content": "The most cost-effective and recommended solution to send logs from Compute Engine instances to BigQuery is to use the Cloud Logging agent with a sink that streams the logs to BigQuery.\n\nAnswer C is the most appropriate solution. In Cloud Logging, create a filter to view only Compute Engine logs. Click Create Export. Choose BigQuery as Sink Service, and the platform-logs dataset as Sink Destination. This will allow all Compute Engine instance logs to be exported to BigQuery with minimal complexity and cost."
      },
      {
        "date": "2022-12-13T08:52:00.000Z",
        "voteCount": 1,
        "content": "Mi vote is for \"c\""
      },
      {
        "date": "2022-12-06T11:44:00.000Z",
        "voteCount": 1,
        "content": "vote for ''C\""
      },
      {
        "date": "2022-10-05T04:00:00.000Z",
        "voteCount": 3,
        "content": "C. is correct, Sinks control how Cloud Logging routes logs. Using sinks, you can route some or all of your logs to supported destinations."
      },
      {
        "date": "2022-06-23T10:18:00.000Z",
        "voteCount": 1,
        "content": "I will go with C.."
      },
      {
        "date": "2022-06-04T00:32:00.000Z",
        "voteCount": 1,
        "content": "Go for C"
      },
      {
        "date": "2022-03-16T05:08:00.000Z",
        "voteCount": 5,
        "content": "Outdated question. It's now about Cloud Sink, but C is the closest option"
      },
      {
        "date": "2022-01-12T00:24:00.000Z",
        "voteCount": 4,
        "content": "I think it's C as all the other ones seem to get logs from everywhere not just Compute Engine!"
      },
      {
        "date": "2021-11-23T14:12:00.000Z",
        "voteCount": 4,
        "content": "I vote for C"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 69,
    "url": "https://www.examtopics.com/discussions/google/view/22969-exam-associate-cloud-engineer-topic-1-question-69-discussion/",
    "body": "You are using Deployment Manager to create a Google Kubernetes Engine cluster. Using the same Deployment Manager deployment, you also want to create a<br>DaemonSet in the kube-system namespace of the cluster. You want a solution that uses the fewest possible services. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd the cluster's API as a new Type Provider in Deployment Manager, and use the new type to create the DaemonSet.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Deployment Manager Runtime Configurator to create a new Config resource that contains the DaemonSet definition.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWith Deployment Manager, create a Compute Engine instance with a startup script that uses kubectl to create the DaemonSet.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the cluster's definition in Deployment Manager, add a metadata that has kube-system as key and the DaemonSet manifest as value."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 18,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-08-23T12:45:00.000Z",
        "voteCount": 76,
        "content": "Correct Answer is (A):\n\nAdding an API as a type provider\nThis page describes how to add an API to Google Cloud Deployment Manager as a type provider. To learn more about types and type providers, read the Types overview documentation.\n\nA type provider exposes all of the resources of a third-party API to Deployment Manager as base types that you can use in your configurations. These types must be directly served by a RESTful API that supports Create, Read, Update, and Delete (CRUD).\n\nIf you want to use an API that is not automatically provided by Google with Deployment Manager, you must add the API as a type provider. \n\nhttps://cloud.google.com/deployment-manager/docs/configuration/type-providers/creating-type-provider"
      },
      {
        "date": "2020-12-30T07:55:00.000Z",
        "voteCount": 7,
        "content": "very good find, sounds like you hit the nail in the head"
      },
      {
        "date": "2020-06-14T01:39:00.000Z",
        "voteCount": 10,
        "content": "Option A is the right answer"
      },
      {
        "date": "2023-09-01T22:51:00.000Z",
        "voteCount": 2,
        "content": "A is the correct , bcoz it help you contact directly to the gke cluster to create daemon"
      },
      {
        "date": "2023-08-09T11:39:00.000Z",
        "voteCount": 1,
        "content": "Should have been D"
      },
      {
        "date": "2023-04-12T17:26:00.000Z",
        "voteCount": 1,
        "content": "option A is the right answer because it lets you directly interact with the Kubernetes API to create the Daemonset using the same deployment Manager Deployment"
      },
      {
        "date": "2023-02-20T15:57:00.000Z",
        "voteCount": 7,
        "content": "I would say both Answer A and Answer D are valid solutions, and it depends on your preference and requirements.\n\nAnswer A involves adding the cluster's API as a new Type Provider in Deployment Manager and using the new type to create the DaemonSet. This solution would allow you to create and manage the DaemonSet and the cluster in the same Deployment Manager deployment.\n\nAnswer D involves adding a metadata block to the Deployment Manager deployment of the cluster, which will create the DaemonSet in the kube-system namespace of the cluster. This solution would allow you to create the DaemonSet in a simple way and avoid the need to create a new Type of Provider.\n\nIn conclusion, I would choose Answer A to be considered the answer that uses the fewest possible services, as it only involves adding the cluster's API as a new Type Provider in Deployment Manager, which is a lightweight solution."
      },
      {
        "date": "2023-02-12T22:02:00.000Z",
        "voteCount": 3,
        "content": "D. In the cluster's definition in Deployment Manager, add a metadata that has kube-system as key and the DaemonSet manifest as value.\n\nThis approach involves adding the DaemonSet manifest directly as a metadata entry in the cluster's definition in Deployment Manager. When the cluster is created, the DaemonSet is automatically created in the kube-system namespace. This approach is the simplest and requires the fewest number of services. Option A is also a viable solution but requires more work to set up a Type Provider. Option B is not suitable because it involves a separate service (Runtime Configurator). Option C is also not recommended because it involves creating a Compute Engine instance and using kubectl to create the DaemonSet, which is more complicated and less efficient than the other options."
      },
      {
        "date": "2022-12-25T18:04:00.000Z",
        "voteCount": 1,
        "content": "Option A is the right answer"
      },
      {
        "date": "2022-07-02T08:10:00.000Z",
        "voteCount": 1,
        "content": "Answer is A."
      },
      {
        "date": "2022-06-23T10:20:00.000Z",
        "voteCount": 1,
        "content": "go with A as per ESP_SAP explanations.."
      },
      {
        "date": "2022-06-04T00:40:00.000Z",
        "voteCount": 1,
        "content": "Go for A"
      },
      {
        "date": "2022-02-21T11:14:00.000Z",
        "voteCount": 1,
        "content": "A is the correct answe, the API need to be added as a type provider"
      },
      {
        "date": "2022-01-26T09:42:00.000Z",
        "voteCount": 1,
        "content": "A should be correct one"
      },
      {
        "date": "2021-12-26T10:30:00.000Z",
        "voteCount": 2,
        "content": "https://medium.com/google-cloud/cloud-deployment-manager-kubernetes-2dd9b8124223"
      },
      {
        "date": "2023-09-30T07:23:00.000Z",
        "voteCount": 1,
        "content": "Good reference. Thanks for it. Recomended"
      },
      {
        "date": "2021-12-04T10:30:00.000Z",
        "voteCount": 2,
        "content": "Correct Answer is (A)"
      },
      {
        "date": "2021-11-23T15:10:00.000Z",
        "voteCount": 10,
        "content": "couldn't be more confusing"
      },
      {
        "date": "2021-11-20T04:47:00.000Z",
        "voteCount": 1,
        "content": "A. Add the cluster\u05d2\u20ac\u2122s API as a new Type Provider in Deployment Manager, and use the new type to create the DaemonSet."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 70,
    "url": "https://www.examtopics.com/discussions/google/view/24342-exam-associate-cloud-engineer-topic-1-question-70-discussion/",
    "body": "You are building an application that will run in your data center. The application will use Google Cloud Platform (GCP) services like AutoML. You created a service account that has appropriate access to AutoML. You need to enable authentication to the APIs from your on-premises environment. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse service account credentials in your on-premises application.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse gcloud to create a key file for the service account that has appropriate permissions.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up direct interconnect between your data center and Google Cloud Platform to enable authentication for your on-premises applications.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGo to the IAM &amp; admin console, grant a user account permissions similar to the service account permissions, and use this user account for authentication from your data center."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 15,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-08-16T21:22:00.000Z",
        "voteCount": 53,
        "content": "Correct answer should be  (B):\n\nTo use a service account outside of Google Cloud, such as on other platforms or on-premises, you must first establish the identity of the service account. Public/private key pairs provide a secure way of accomplishing this goal.\n\nhttps://cloud.google.com/iam/docs/creating-managing-service-account-keys"
      },
      {
        "date": "2023-02-20T16:38:00.000Z",
        "voteCount": 6,
        "content": "The recommended approach for enabling authentication from an on-premises environment to Google Cloud Platform (GCP) services like AutoML is to use a service account and generate a JSON key file for the service account. This key file can then be used to authenticate and authorize API calls from your on-premises environment to GCP.\n\nTherefore, the correct answer is B. Use gcloud to create a key file for the service account that has appropriate permissions."
      },
      {
        "date": "2023-11-23T00:08:00.000Z",
        "voteCount": 4,
        "content": "B\nAs per the documentation: https://cloud.google.com/iam/docs/keys-create-delete#creating"
      },
      {
        "date": "2023-11-06T02:50:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is B"
      },
      {
        "date": "2023-10-14T14:36:00.000Z",
        "voteCount": 1,
        "content": "A. Use service account credentials in your on-premises application.\n\nExplanation:\n\n    Service accounts are the recommended way to authenticate your application and authorize it to access GCP services.\n    You can create and use service account credentials to authenticate your application running in your on-premises environment and access GCP services like AutoML.\n\nOption B (using gcloud to create a key file for the service account) is a valid approach to generate credentials for a service account, but using those credentials in your application is essential, which aligns with option A.\n\nOptions C and D are not directly related to enabling authentication for on-premises applications using service account credentials. Setting up direct interconnect (option C) is about networking, and granting permissions to a user account (option D) is not the standard approach for authenticating an application running on-premises to GCP services"
      },
      {
        "date": "2023-09-01T22:53:00.000Z",
        "voteCount": 1,
        "content": "B is the correct answer, as to access the out side the google cloud , you need the key"
      },
      {
        "date": "2023-02-12T22:16:00.000Z",
        "voteCount": 2,
        "content": "A. Use service account credentials in your on-premises application.\n\nTo enable authentication to GCP services from your on-premises environment, you can use service account credentials in your on-premises application. This involves creating a service account that has appropriate access to the required GCP services, downloading the service account key file, and using the key file to authenticate the API requests in your on-premises application. This is a secure way to authenticate to GCP services as it does not require direct access to your GCP project or credentials from your on-premises environment."
      },
      {
        "date": "2023-02-20T16:33:00.000Z",
        "voteCount": 3,
        "content": "Cloud Security/Auditor doesn't like Answer \"A\". Using service account credentials in your on-premises application could be a security risk if the credentials are compromised. If the key file is stolen or leaked, an attacker could use it to access your GCP resources, potentially causing data breaches, service disruptions, or financial losses.\n\nI would select Answer \"B\". Use gcloud to create a key file for the service account that has appropriate permissions and let Security Auditor stay away from my back. Never-ending \"You cannot do this, you cannot do that\" on Answer A."
      },
      {
        "date": "2022-12-06T11:49:00.000Z",
        "voteCount": 1,
        "content": "B it is."
      },
      {
        "date": "2022-11-27T14:53:00.000Z",
        "voteCount": 1,
        "content": "B it is."
      },
      {
        "date": "2022-11-05T15:27:00.000Z",
        "voteCount": 1,
        "content": "Correct answer should be (B):"
      },
      {
        "date": "2022-06-23T10:23:00.000Z",
        "voteCount": 4,
        "content": "B is right\nTo use a service account from outside of Google Cloud, such as on other platforms or on-premises, you must first establish the identity of the service account. Public/private key pairs provide a secure way of accomplishing this goal. When you create a service account key, the public portion is stored on Google Cloud, while the private portion is available only to you. For more information about public/private key pairs, see Service account keys."
      },
      {
        "date": "2022-06-04T04:12:00.000Z",
        "voteCount": 1,
        "content": "Go for B"
      },
      {
        "date": "2022-01-12T00:35:00.000Z",
        "voteCount": 1,
        "content": "Even thought A and B seem to be doing the same thing the best practice is to create a key so B is the right answer \n!"
      },
      {
        "date": "2021-11-20T04:48:00.000Z",
        "voteCount": 3,
        "content": "B. Use gcloud to create a key file for the service account that has appropriate permissions."
      },
      {
        "date": "2021-11-19T07:59:00.000Z",
        "voteCount": 3,
        "content": "B. Use gcloud to create a key file for the service account that has appropriate permissions."
      },
      {
        "date": "2021-09-26T03:09:00.000Z",
        "voteCount": 3,
        "content": "Why not A? Aren't  A and B getting the same key file?"
      },
      {
        "date": "2021-11-20T04:15:00.000Z",
        "voteCount": 1,
        "content": "A is not really telling you the steps to accomplish the task, it's only telling you the result of it (creating a SA with sufficient permissions and then use Console / gcloud to create a JSON token for it)"
      },
      {
        "date": "2021-07-01T20:46:00.000Z",
        "voteCount": 3,
        "content": "B is correct.\n\nCreating service account keys\nTo use a service account from outside of Google Cloud, such as on other platforms or on-premises, you must first establish the identity of the service account. Public/private key pairs provide a secure way of accomplishing this goal. When you create a service account key, the public portion is stored on Google Cloud, while the private portion is available only to you. For more information about public/private key pairs, see Service account keys."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 71,
    "url": "https://www.examtopics.com/discussions/google/view/22192-exam-associate-cloud-engineer-topic-1-question-71-discussion/",
    "body": "You are using Container Registry to centrally store your company's container images in a separate project. In another project, you want to create a Google<br>Kubernetes Engine (GKE) cluster. You want to ensure that Kubernetes can download images from Container Registry. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the project where the images are stored, grant the Storage Object Viewer IAM role to the service account used by the Kubernetes nodes.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWhen you create the GKE cluster, choose the Allow full access to all Cloud APIs option under 'Access scopes'.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a service account, and give it access to Cloud Storage. Create a P12 key for this service account and use it as an imagePullSecrets in Kubernetes.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the ACLs on each image in Cloud Storage to give read-only access to the default Compute Engine service account."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 15,
        "isMostVoted": true
      },
      {
        "answer": "R",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-08-16T21:35:00.000Z",
        "voteCount": 64,
        "content": "Correct Answer (A):\nIAM permissions\nIAM permissions determine who can access resources. All users, service accounts, and other identities that interact with Container Registry must have the appropriate Cloud Storage permissions.\n\nBy default, Google Cloud use default service accounts to interact with resources within the same project. For example, the Cloud Build service account can both push and pull images when Container Registry is in the same project.\n\nYou must configure or modify permissions yourself if:\n\nYou are using a service account in one project to access Container Registry in a different project\nYou are using a default service account with read-only access to storage, but you want to both pull and push images\nYou are using a custom service account to interact with Container Registry\n\nhttps://cloud.google.com/container-registry/docs/access-control"
      },
      {
        "date": "2020-09-20T06:49:00.000Z",
        "voteCount": 11,
        "content": "A is correct, practical implementation in video https://www.youtube.com/watch?v=R16z7Sjrkxs"
      },
      {
        "date": "2020-08-12T11:39:00.000Z",
        "voteCount": 21,
        "content": "A is correct...\nContainer Registry uses Cloud Storage buckets as the underlying storage for container images. You control access to your images by granting appropriate Cloud Storage permissions to a user, group, service account, or other identity.\n\nIf the service account needs to access Container Registry in another project, you must grant the required permissions in the project with Container Registry.\n\nReference:\nhttps://cloud.google.com/container-registry/docs/access-control#permissions"
      },
      {
        "date": "2024-09-23T06:26:00.000Z",
        "voteCount": 1,
        "content": "A  is correct"
      },
      {
        "date": "2024-05-15T17:08:00.000Z",
        "voteCount": 1,
        "content": "As per https://cloud.google.com/container-registry/docs/access-control.\n\nContainer Registry is deprecated and scheduled for shutdown. After May 15, 2024, Artifact Registry will host images for the gcr.io domain in Google Cloud projects without previous Container Registry usage. After March 18, 2025, Container Registry will be shut down.\n\nArtifact Registry is the recommended service for container image storage and management on Google Cloud."
      },
      {
        "date": "2023-09-01T23:02:00.000Z",
        "voteCount": 2,
        "content": "A is the correct answer , as granting this role allow to download the image"
      },
      {
        "date": "2023-04-12T17:43:00.000Z",
        "voteCount": 1,
        "content": "Grating Storage Object Viewer  IAM Role to the service account  used by Kubernetes nodes allow the nodes to download the images from  Container registry."
      },
      {
        "date": "2023-02-20T16:51:00.000Z",
        "voteCount": 5,
        "content": "Answer A. In the project where the images are stored, grant the Storage Object Viewer IAM role to the service account used by the Kubernetes nodes.\n\nTo ensure that Kubernetes can download container images from Container Registry, you need to grant the necessary permissions to the service account used by the Kubernetes nodes. In this case, you would need to grant the Storage Object Viewer IAM role to the service account used by the Kubernetes nodes in the project where the images are stored. This role allows the service account to read objects from Cloud Storage buckets, including the container images in Container Registry."
      },
      {
        "date": "2023-01-29T12:29:00.000Z",
        "voteCount": 1,
        "content": "Definitely A seems more practical and accurate."
      },
      {
        "date": "2022-10-15T02:14:00.000Z",
        "voteCount": 1,
        "content": "CORRET ANS is A"
      },
      {
        "date": "2022-09-11T06:48:00.000Z",
        "voteCount": 1,
        "content": "Answer A is correct.\nStorage Object Viewer (roles/storage.objectViewer)  -- Grant the role on the registry storage bucket.\nhttps://cloud.google.com/container-registry/docs/access-control"
      },
      {
        "date": "2022-06-23T10:25:00.000Z",
        "voteCount": 1,
        "content": "Storage Object viewer is enough, A is right."
      },
      {
        "date": "2022-06-04T04:24:00.000Z",
        "voteCount": 1,
        "content": "Go for A.\nthe option A is specific with the role that will be used. In GCP , the recommendations is using the specific permission. The others options not are specific and are not correct ."
      },
      {
        "date": "2022-03-17T06:20:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/container-registry/docs/access-control"
      },
      {
        "date": "2022-02-21T11:18:00.000Z",
        "voteCount": 2,
        "content": "A is the correct answe because it follows google recommended practices to grant permissions to service accounts, also the object viewer is the appropiate role for the GKE to pull the image"
      },
      {
        "date": "2022-02-07T15:31:00.000Z",
        "voteCount": 1,
        "content": "A is the most obvious"
      },
      {
        "date": "2022-01-26T09:47:00.000Z",
        "voteCount": 2,
        "content": "A should be correct \nhttps://cloud.google.com/container-registry/docs/pushing-and-pulling#pulling_images_from_a_registry\nhttps://cloud.google.com/storage/docs/access-control/iam-roles#standard-roles"
      },
      {
        "date": "2022-01-12T00:42:00.000Z",
        "voteCount": 1,
        "content": "It's A as you need Storage Object Viewer IAM role in order to have access to the images!"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 72,
    "url": "https://www.examtopics.com/discussions/google/view/22899-exam-associate-cloud-engineer-topic-1-question-72-discussion/",
    "body": "You deployed a new application inside your Google Kubernetes Engine cluster using the YAML file specified below.<br><img src=\"/assets/media/exam-media/04338/0003900001.png\" class=\"in-exam-image\"><br>You check the status of the deployed pods and notice that one of them is still in PENDING status:<br><img src=\"/assets/media/exam-media/04338/0003900002.png\" class=\"in-exam-image\"><br>You want to find out why the pod is stuck in pending status. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReview details of the myapp-service Service object and check for error messages.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReview details of the myapp-deployment Deployment object and check for error messages.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReview details of myapp-deployment-58ddbbb995-lp86m Pod and check for warning messages.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tView logs of the container in myapp-deployment-58ddbbb995-lp86m pod and check for warning messages."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 9,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-07-02T06:13:00.000Z",
        "voteCount": 31,
        "content": "It's C - https://kubernetes.io/docs/tasks/debug-application-cluster/debug-application/#debugging-pods"
      },
      {
        "date": "2020-07-12T12:03:00.000Z",
        "voteCount": 24,
        "content": "Answer is C - You can't view logs of a pod that isn't deployed, so D is incorrect.\nC allows you to check the pod deployment messages and look for errors"
      },
      {
        "date": "2021-07-04T04:22:00.000Z",
        "voteCount": 3,
        "content": "What u said is incorrect you can view pod's log even in pending state.\nkubectl logs &lt;pon-name&gt; -n &lt;namespace&gt;"
      },
      {
        "date": "2023-09-01T23:04:00.000Z",
        "voteCount": 2,
        "content": "C is correct, as its help you to check the error"
      },
      {
        "date": "2023-02-20T17:04:00.000Z",
        "voteCount": 3,
        "content": "Answer C. Review details of myapp-deployment-58ddbbb995-lp86m Pod and check for warning messages.\n\nTo find out why a pod is stuck in pending status, you can review the details of the pod and check for any warning messages. Answer C is the correct answer because it suggests reviewing the details of the specific pod that is stuck in pending status. You can use the kubectl describe pod &lt;pod-name&gt; command to view detailed information about the pod, including any warning messages that might indicate why the pod is not scheduled."
      },
      {
        "date": "2023-01-08T22:30:00.000Z",
        "voteCount": 1,
        "content": "D : we first check logs"
      },
      {
        "date": "2022-11-19T00:22:00.000Z",
        "voteCount": 1,
        "content": "I vote C; because if we imagine that we will go to a main menu that display the errors of the all deployment object as hole, we will surely navigate thin to the pod menu ! so the C option will direct take us to the second menu."
      },
      {
        "date": "2022-10-05T04:32:00.000Z",
        "voteCount": 1,
        "content": "C is correct,\nDebugging Pods\nThe first step in debugging a Pod is taking a look at it. Check the current state of the Pod and recent events with the following command:\n\nkubectl describe pods ${POD_NAME}"
      },
      {
        "date": "2022-09-11T10:21:00.000Z",
        "voteCount": 1,
        "content": "Definitely"
      },
      {
        "date": "2022-08-14T11:04:00.000Z",
        "voteCount": 1,
        "content": "I guess it's B. its deployment that creates the pod and it has the information why it is not able to create. it shows the information if you describe the deployment ( kubectl describe deployment )"
      },
      {
        "date": "2022-08-06T07:46:00.000Z",
        "voteCount": 1,
        "content": "Answer is C"
      },
      {
        "date": "2022-07-02T08:18:00.000Z",
        "voteCount": 1,
        "content": "Answer is C"
      },
      {
        "date": "2022-06-23T10:27:00.000Z",
        "voteCount": 1,
        "content": "C is right"
      },
      {
        "date": "2022-06-04T04:29:00.000Z",
        "voteCount": 1,
        "content": "Go for C"
      },
      {
        "date": "2022-02-20T00:21:00.000Z",
        "voteCount": 1,
        "content": "C is the correct answer: The first step in debugging a Pod is taking a look at it. Check the current state of the Pod and recent events with the following command:\n\nkubectl describe pods ${POD_NAME}\nhttps://kubernetes.io/docs/tasks/debug-application-cluster/debug-application/"
      },
      {
        "date": "2022-02-20T00:18:00.000Z",
        "voteCount": 2,
        "content": "C is the correct answer.\nIf a Pod is stuck in Pending it means that it can not be scheduled onto a node. Generally this is because there are insufficient resources of one type or another that prevent scheduling. Look at the output of the kubectl describe ... command above. There should be messages from the scheduler about why it can not schedule your Pod.\nhttps://kubernetes.io/docs/tasks/debug-application-cluster/debug-application/"
      },
      {
        "date": "2021-11-20T04:55:00.000Z",
        "voteCount": 1,
        "content": "C. Review details of myapp-deployment-58ddbbb995-lp86m Pod and check for warning messages."
      },
      {
        "date": "2021-11-19T08:01:00.000Z",
        "voteCount": 2,
        "content": "C. Review details of myapp-deployment-58ddbbb995-lp86m Pod and check for warning messages."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 73,
    "url": "https://www.examtopics.com/discussions/google/view/22023-exam-associate-cloud-engineer-topic-1-question-73-discussion/",
    "body": "You are setting up a Windows VM on Compute Engine and want to make sure you can log in to the VM via RDP. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAfter the VM has been created, use your Google Account credentials to log in into the VM.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAfter the VM has been created, use gcloud compute reset-windows-password to retrieve the login credentials for the VM.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWhen creating the VM, add metadata to the instance using 'windows-password' as the key and a password as the value.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAfter the VM has been created, download the JSON private key for the default Compute Engine service account. Use the credentials in the JSON file to log in to the VM."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 18,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-06-03T12:45:00.000Z",
        "voteCount": 63,
        "content": "Correct Answer is B. \nB. After the VM has been created, use gcloud compute reset-windows-password to retrieve the login credentials for the VM.\n\nhttps://cloud.google.com/sdk/gcloud/reference/beta/compute/reset-windows-password"
      },
      {
        "date": "2020-08-11T15:00:00.000Z",
        "voteCount": 4,
        "content": "Correct answer is B.\ngcloud beta compute reset-windows-password allows a user to reset and retrieve a password for a Windows virtual machine instance. If the Windows account does not exist, this command will cause the account to be created and the password for that new account will be returned."
      },
      {
        "date": "2020-06-26T03:26:00.000Z",
        "voteCount": 5,
        "content": "Yes! \"If the Windows account does not exist, this command will cause the account to be created and the password for that new account will be returned.\""
      },
      {
        "date": "2020-06-05T07:46:00.000Z",
        "voteCount": 4,
        "content": "did you even look at the link you provide ? it clearly say gcloud beta compute reset-windows-password my-instance and not gcloud compute reset-windows-password. D is correct - https://cloud.google.com/iam/docs/creating-managing-service-account-keys"
      },
      {
        "date": "2020-06-05T07:48:00.000Z",
        "voteCount": 1,
        "content": "nobody talk on reset the password but how to access the Windows - best way - Service Account"
      },
      {
        "date": "2023-06-05T01:43:00.000Z",
        "voteCount": 1,
        "content": "Service accounts shouldn't be used for RDP , they are used to machine authentication with services."
      },
      {
        "date": "2023-06-05T01:43:00.000Z",
        "voteCount": 1,
        "content": "Also how you will RDP with Service account and private key\nYou need username and password , it is not ssh"
      },
      {
        "date": "2021-05-09T02:39:00.000Z",
        "voteCount": 7,
        "content": "Oh yes? Then what about this link (for non-beta command)?\nhttps://cloud.google.com/sdk/gcloud/reference/compute/reset-windows-password\n\n\"If the Windows account does not exist, this command will cause the account to be created and the password for that new account will be returned.\"\n\nThe answer is obviously B. Just test it and it'll become very clear"
      },
      {
        "date": "2023-02-20T17:33:00.000Z",
        "voteCount": 8,
        "content": "CORRECT:\nAnswer B. After the VM has been created, use gcloud compute reset-windows-password to retrieve the login credentials for the VM.\n\nINCORRECT:\nAnswer A is not correct because Google Account credentials cannot be used to log in to Windows VMs.\n\nAnswer C is not correct because metadata can be used to specify some settings for a VM, but the 'windows-password' metadata key is not used for specifying the login password for a Windows VM.\n\nAnswer D is not correct because the JSON private key for the default Compute Engine service account is not used for logging in to a Windows VM."
      },
      {
        "date": "2023-12-09T23:47:00.000Z",
        "voteCount": 2,
        "content": "Why this website has published all incorrect answers"
      },
      {
        "date": "2023-11-06T03:10:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is B."
      },
      {
        "date": "2023-11-05T10:11:00.000Z",
        "voteCount": 1,
        "content": "what is rdp meaning please ?"
      },
      {
        "date": "2024-02-24T13:24:00.000Z",
        "voteCount": 1,
        "content": "remote desktop protocal used to connect and login into windows machine, similar to ssh protocal for linux machine"
      },
      {
        "date": "2023-09-01T23:07:00.000Z",
        "voteCount": 1,
        "content": "Yes , B is the corrrect answer"
      },
      {
        "date": "2023-04-12T18:05:00.000Z",
        "voteCount": 1,
        "content": "after creating Windows VM on COmpute Engine it has a local user account as well. This acct is used to login to the VM via RDP. If you forget the password you can use gcloud compute reset-windows-password to reset it. This command generates a new password and sets it for the user account on the VM."
      },
      {
        "date": "2023-03-23T05:12:00.000Z",
        "voteCount": 1,
        "content": "Correct option B"
      },
      {
        "date": "2023-03-21T13:36:00.000Z",
        "voteCount": 2,
        "content": "Option B is correct. After creating a Windows VM instance on Compute Engine, you need to use the gcloud compute reset-windows-password command to retrieve the login credentials for the VM. This command generates a new Windows password and displays it in the output of the command. You can then use this password to log in to the VM via RDP.\n\nOption A is incorrect because logging in to the VM using your Google Account credentials is not a supported method for Windows VM instances.\n\nOption C is also incorrect because 'windows-password' is not a recognized metadata key for Windows VM instances.\n\nOption D is incorrect because you cannot use the JSON private key for the default Compute Engine service account to log in to a Windows VM instance via RDP."
      },
      {
        "date": "2022-12-07T11:06:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer is B.\nB. After the VM has been created, use gcloud compute reset-windows-password to retrieve the login credentials for the VM"
      },
      {
        "date": "2022-09-16T09:55:00.000Z",
        "voteCount": 1,
        "content": "After the VM has been created, use gcloud compute reset-windows-password to retrieve the login credentials for the VM."
      },
      {
        "date": "2022-06-23T10:28:00.000Z",
        "voteCount": 1,
        "content": "B is perfect for this question.."
      },
      {
        "date": "2022-06-04T05:08:00.000Z",
        "voteCount": 2,
        "content": "Go for B\nhttps://cloud.google.com/sdk/gcloud/reference/compute/reset-windows-password"
      },
      {
        "date": "2022-05-28T02:03:00.000Z",
        "voteCount": 1,
        "content": "B is correct \nafter VM creating , you can reset the password"
      },
      {
        "date": "2022-05-10T07:29:00.000Z",
        "voteCount": 2,
        "content": "After the VM has been created, use gcloud compute reset-windows-password to retrieve the login credentials for the VM"
      },
      {
        "date": "2022-03-15T23:26:00.000Z",
        "voteCount": 1,
        "content": "b is currect"
      },
      {
        "date": "2022-02-21T11:22:00.000Z",
        "voteCount": 1,
        "content": "B is the correct answer, the command is the correct one"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 74,
    "url": "https://www.examtopics.com/discussions/google/view/22730-exam-associate-cloud-engineer-topic-1-question-74-discussion/",
    "body": "You want to configure an SSH connection to a single Compute Engine instance for users in the dev1 group. This instance is the only resource in this particular<br>Google Cloud Platform project that the dev1 users should be able to connect to. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet metadata to enable-oslogin=true for the instance. Grant the dev1 group the compute.osLogin role. Direct them to use the Cloud Shell to ssh to that instance.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet metadata to enable-oslogin=true for the instance. Set the service account to no service account for that instance. Direct them to use the Cloud Shell to ssh to that instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable block project wide keys for the instance. Generate an SSH key for each user in the dev1 group. Distribute the keys to dev1 users and direct them to use their third-party tools to connect.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable block project wide keys for the instance. Generate an SSH key and associate the key with that instance. Distribute the key to dev1 users and direct them to use their third-party tools to connect."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 26,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-06-10T04:40:00.000Z",
        "voteCount": 46,
        "content": "A correct one"
      },
      {
        "date": "2020-06-15T10:52:00.000Z",
        "voteCount": 3,
        "content": "Agree with that"
      },
      {
        "date": "2020-07-02T06:20:00.000Z",
        "voteCount": 8,
        "content": "For further evidence... https://cloud.google.com/compute/docs/instances/managing-instance-access"
      },
      {
        "date": "2020-10-21T06:37:00.000Z",
        "voteCount": 11,
        "content": "Pure from logic thinking: A can't be right. If the group get access to that instance with enable-oslogin=true, then they could have access to every instance that has enable-oslogin=true. Or do I miss something?"
      },
      {
        "date": "2020-12-30T09:07:00.000Z",
        "voteCount": 1,
        "content": "I'm convinced with this logic"
      },
      {
        "date": "2021-12-12T23:46:00.000Z",
        "voteCount": 2,
        "content": "clearly, question say \"the only ressource they need to access in this project\" \nas you said, all ressources will be available if we set the role"
      },
      {
        "date": "2023-01-29T13:04:00.000Z",
        "voteCount": 3,
        "content": "That's not necessarily true - https://cloud.google.com/compute/docs/oslogin/set-up-oslogin. The doc says \"If you want enable OS Login for all VMs in a project, set the metadata at the project-level. If you want to enable OS Login for a single VM, set the metadata at the instance-level.\" \n\nThat means you can do it at the instance level, so there shouldn't be a problem with following A."
      },
      {
        "date": "2022-05-22T00:04:00.000Z",
        "voteCount": 6,
        "content": "Note the sentence \"Set metadata to enable-oslogin=true for the instance.\" This means the metadata for oslogin has been set to that particular instance only, and not for all."
      },
      {
        "date": "2023-12-30T17:35:00.000Z",
        "voteCount": 5,
        "content": "OS Login Feature:\n\nOS Login is a feature in GCP that manages SSH access to your Compute Engine instances using IAM (Identity and Access Management) roles. When OS Login is enabled, it allows you to use IAM roles to grant or revoke SSH access to your instances, which can be more secure and manageable than traditional SSH key management.\nEnabling OS Login:\n\nSetting the instance metadata enable-oslogin=true enables the OS Login feature on that specific Compute Engine instance.\nWhen OS Login is enabled, traditional SSH keys defined in the project or instance metadata are ignored, and the instance instead relies on IAM roles for SSH access."
      },
      {
        "date": "2023-09-01T23:10:00.000Z",
        "voteCount": 3,
        "content": "A is correct as , it gives the only specific access"
      },
      {
        "date": "2023-06-05T01:52:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/compute/docs/oslogin/set-up-oslogin"
      },
      {
        "date": "2023-04-12T18:24:00.000Z",
        "voteCount": 1,
        "content": "Enabling OSLogin allow user to login to Google Cloud credentials to authenticate to instance, instead of SSH key.\nGranting 'compute.osLogin' to the dev1 lets them login using OSLogin to the resourcee\n\nBD are incorrect because \"block project-wide SSH keys\" is an advance security features taht is used for high secured environment where more granular control over SSH us required\n\nC is hassle because it manually ditribute keys to each user in Dev1 which is time consuming"
      },
      {
        "date": "2023-02-20T21:13:00.000Z",
        "voteCount": 3,
        "content": "Answer B is incorrect because setting the service account to no service account has no impact on SSH access to the VM instance.\n\nAnswer C is incorrect because generating an SSH key for each user in the dev1 group and distributing them is cumbersome and not scalable, especially if you have many users.\n\nAnswer D is incorrect because generating a single SSH key and distributing it to multiple users undermines security, as it means any of the users in possession of the key can access the VM instance."
      },
      {
        "date": "2023-01-29T13:57:00.000Z",
        "voteCount": 1,
        "content": "In my opinion A would be best, but they have to use this and only this 1 instance. You don't know if any other instances has this metadata set up - if they do, dev1 team has also access to this instances, what invalidates the answer. To make sure they are using only this 1 instance, I'd say D."
      },
      {
        "date": "2023-01-29T13:04:00.000Z",
        "voteCount": 1,
        "content": "Based on https://cloud.google.com/compute/docs/oslogin/set-up-oslogin I'd go for A."
      },
      {
        "date": "2023-01-08T03:28:00.000Z",
        "voteCount": 1,
        "content": "The dev1 users should be able to connect only to this VM instance"
      },
      {
        "date": "2022-12-07T21:02:00.000Z",
        "voteCount": 1,
        "content": "A correct one"
      },
      {
        "date": "2022-10-05T04:59:00.000Z",
        "voteCount": 2,
        "content": "A is the correct answer\nGranting OS Login IAM roles\nAfter you enable OS Login on one or more instances in your project, those VMs accept connections only from user accounts that have the necessary IAM roles in your project or organization.\n\nroles/compute.osLogin, which doesn't grant administrator permissions"
      },
      {
        "date": "2022-09-24T10:26:00.000Z",
        "voteCount": 3,
        "content": "had this question today"
      },
      {
        "date": "2022-07-02T08:20:00.000Z",
        "voteCount": 1,
        "content": "Answer is A"
      },
      {
        "date": "2022-06-23T10:30:00.000Z",
        "voteCount": 1,
        "content": "A is correct.."
      },
      {
        "date": "2022-05-23T05:59:00.000Z",
        "voteCount": 2,
        "content": "For further evidence... https://cloud.google.com/compute/docs/instances/managing-instance-access"
      },
      {
        "date": "2022-05-09T18:20:00.000Z",
        "voteCount": 1,
        "content": "A is correct and recommended option.\nD is incorrect because block project-wide restrict access to this instance, evidence: https://cloud.google.com/compute/docs/connect/restrict-ssh-keyshttps://cloud.google.com/compute/docs/connect/restrict-ssh-keys"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 75,
    "url": "https://www.examtopics.com/discussions/google/view/22193-exam-associate-cloud-engineer-topic-1-question-75-discussion/",
    "body": "You need to produce a list of the enabled Google Cloud Platform APIs for a GCP project using the gcloud command line in the Cloud Shell. The project name is my-project. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun gcloud projects list to get the project ID, and then run gcloud services list --project &lt;project ID&gt;.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun gcloud init to set the current project to my-project, and then run gcloud services list --available.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun gcloud info to view the account value, and then run gcloud services list --account &lt;Account&gt;.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun gcloud projects describe &lt;project ID&gt; to verify the project value, and then run gcloud services list --available."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 9,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-06-06T17:52:00.000Z",
        "voteCount": 50,
        "content": "A is the correct answer, log to gcloud and run the commands, doesnt make sense to run cloud init and gcloud services list --available gives you the full services that are available."
      },
      {
        "date": "2020-06-13T21:33:00.000Z",
        "voteCount": 6,
        "content": "Yes, Answer A correct. it shows only enabled services of API"
      },
      {
        "date": "2021-05-09T02:56:00.000Z",
        "voteCount": 12,
        "content": "\"A\" is correct.\n\nFor those, who have doubts:\n\n`gcloud services list --available` returns not only the enabled services in the project but also services that CAN be enabled. Therefore, option B is incorrect.\n\nhttps://cloud.google.com/sdk/gcloud/reference/services/list#--available"
      },
      {
        "date": "2021-06-05T05:30:00.000Z",
        "voteCount": 1,
        "content": "Best answer!"
      },
      {
        "date": "2024-03-13T09:57:00.000Z",
        "voteCount": 1,
        "content": "A is correct!"
      },
      {
        "date": "2023-04-17T03:14:00.000Z",
        "voteCount": 1,
        "content": "Answer: A"
      },
      {
        "date": "2023-02-20T22:22:00.000Z",
        "voteCount": 4,
        "content": "Let's do a side-by-side analysis of Answer A and Answer D to clear our doubts:\n\nAnswer A: Run gcloud projects list to get the project ID, and then run gcloud services list --project &lt;project ID&gt;.\n\nThis option first retrieves the project ID by running the gcloud projects list command.\nThen, it uses the gcloud services list command with the --project flag to list the enabled APIs for the specified project."
      },
      {
        "date": "2023-02-20T22:23:00.000Z",
        "voteCount": 3,
        "content": "Answer D: Run gcloud projects describe &lt;project ID&gt; to verify the project value, and then run gcloud services list --available.\n\nThis option uses the gcloud projects describe command with the project ID to retrieve information about the specified project, including the project ID.\nThen, it uses the gcloud services list command with the --available flag to list all available APIs, not just the ones that are enabled for the specified project.\n\nBased on the scenario presented in the question, we want to produce a list of the enabled APIs for a GCP project, NOT a list of all available APIs. Therefore, Answer A is more appropriate because it specifically lists the enabled APIs for the specified project.\n\nAnswer D lists all available APIs which may include APIs that are not enabled in the project, which could cause confusion or unnecessary information.\n\nTherefore, Answer A is the correct option in this case."
      },
      {
        "date": "2023-02-12T23:03:00.000Z",
        "voteCount": 1,
        "content": "D. Run gcloud projects describe &lt;project ID&gt; to verify the project value, and then run gcloud services list --available.\n\nTo produce a list of enabled Google Cloud Platform APIs for a GCP project using the gcloud command line, you can first run gcloud projects describe &lt;project ID&gt; to verify the project ID for the project in question. Then, you can run gcloud services list --available to list all the available APIs and see which ones are enabled for the project. This command shows the full list of services and their status, including whether they are enabled, disabled, or ready for use. Option A is incorrect because it lists all the available services, regardless of whether they are enabled or not. Option B is incorrect because it lists only the available services, which might not be enabled in the project. Option C is incorrect because it shows account information and not service information."
      },
      {
        "date": "2022-10-05T05:09:00.000Z",
        "voteCount": 1,
        "content": "A is correct answer,\nRun the following command to list the enabled APIs and services in your current project:\n\n\ngcloud services list\n\nwhereas, Run the following command to list the APIs and services available to you in your current project:\n\n\ngcloud services list --available"
      },
      {
        "date": "2022-06-23T10:31:00.000Z",
        "voteCount": 1,
        "content": "A is the correct answer"
      },
      {
        "date": "2022-06-04T06:44:00.000Z",
        "voteCount": 1,
        "content": "Go for A"
      },
      {
        "date": "2022-02-21T11:26:00.000Z",
        "voteCount": 2,
        "content": "A is the correct answer"
      },
      {
        "date": "2022-02-20T01:36:00.000Z",
        "voteCount": 2,
        "content": "A  is the correct answer.\n\nhttps://cloud.google.com/sdk/gcloud/reference/services/list#--available\n--available\nReturn the services available to the project to enable. This list will include any services that the project has already enabled.\n\nTo list the services the current project has enabled for consumption, run:\ngcloud services list --enabled\n\nTo list the services the current project can enable for consumption, run:\ngcloud services list --available"
      },
      {
        "date": "2022-01-04T22:22:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2021-11-20T05:03:00.000Z",
        "voteCount": 1,
        "content": "A. Run gcloud projects list to get the project ID, and then run gcloud services list --project &lt;project ID&gt;."
      },
      {
        "date": "2021-11-19T08:03:00.000Z",
        "voteCount": 1,
        "content": "A. Run gcloud projects list to get the project ID, and then run gcloud services list --project &lt;project ID&gt;."
      },
      {
        "date": "2021-10-02T09:44:00.000Z",
        "voteCount": 2,
        "content": "A Correct"
      },
      {
        "date": "2021-05-12T18:58:00.000Z",
        "voteCount": 2,
        "content": "A is correct"
      },
      {
        "date": "2021-04-17T05:18:00.000Z",
        "voteCount": 1,
        "content": "A is correct answer --project option available in gcloud command"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 76,
    "url": "https://www.examtopics.com/discussions/google/view/22059-exam-associate-cloud-engineer-topic-1-question-76-discussion/",
    "body": "You are building a new version of an application hosted in an App Engine environment. You want to test the new version with 1% of users before you completely switch your application over to the new version. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy a new version of your application in Google Kubernetes Engine instead of App Engine and then use GCP Console to split traffic.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy a new version of your application in a Compute Engine instance instead of App Engine and then use GCP Console to split traffic.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy a new version as a separate app in App Engine. Then configure App Engine using GCP Console to split traffic between the two apps.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy a new version of your application in App Engine. Then go to App Engine settings in GCP Console and split traffic between the current version and newly deployed versions accordingly.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 13,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-06-03T22:07:00.000Z",
        "voteCount": 54,
        "content": "Correct answer is D"
      },
      {
        "date": "2020-12-28T08:48:00.000Z",
        "voteCount": 33,
        "content": "Splitting the question to the key requirements\n\n1. new version of an application hosted in an App Engine environment.\n2. test the new version with 1% of users\n\nApp engine supports versioning and traffic splitting so no need to involve anything else\n(source - https://cloud.google.com/appengine#all-features)\n\nA. ....'Google Kubernetes Engine'.... - No need to involve GKE. Not the right option\nB. ....'Compute Engine instance'.... - No need to involve Compute Engine.\nC. ....'Separate app in App Engine'....- No need to deploy as a separate app. versioning is supported already. Not the right option.\nD. This is the right answer."
      },
      {
        "date": "2022-05-22T00:09:00.000Z",
        "voteCount": 5,
        "content": "Just to add, for option C you cannot have two applications deployed inside an app engine project. In order to do so, you need to create the application inside a new project.\nSo, we just eliminate option C."
      },
      {
        "date": "2023-12-28T17:53:00.000Z",
        "voteCount": 1,
        "content": "the correct answer is D"
      },
      {
        "date": "2023-11-06T03:28:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is D."
      },
      {
        "date": "2023-09-02T02:35:00.000Z",
        "voteCount": 2,
        "content": "D is correct , because you cannot create a sepearte app in the same app engine"
      },
      {
        "date": "2023-08-25T09:27:00.000Z",
        "voteCount": 1,
        "content": "D. Deploy a new version of your application in App Engine. Then go to App Engine settings in GCP Console and split traffic between the current version and newly deployed versions accordingly"
      },
      {
        "date": "2023-06-29T17:50:00.000Z",
        "voteCount": 1,
        "content": "App engine provides out of box functionality to split the traffic between multiple versions"
      },
      {
        "date": "2023-04-17T03:17:00.000Z",
        "voteCount": 1,
        "content": "Answer D is the correct answer"
      },
      {
        "date": "2023-03-09T03:59:00.000Z",
        "voteCount": 1,
        "content": "D correct, no more than 1 app engine per project GKE or CE doesnt make sense, the right approach is deploying new version and splitting traffic"
      },
      {
        "date": "2023-02-20T22:31:00.000Z",
        "voteCount": 3,
        "content": "Answer D is the correct answer as App Engine is designed to allow you to deploy multiple versions of the same application and route traffic between them. To test the new version with 1% of users, you can deploy the new version alongside the current version and then use the App Engine Traffic Splitting feature to gradually increase the percentage of users who are routed to the new version. This can be done easily using GCP Console.\n\nAnswers A and B are not the optimal solutions as Kubernetes Engine and Compute Engine do not offer the same level of built-in traffic splitting and routing features as App Engine.\n\nAnswer C is also POSSIBLE but may not be the best approach since deploying a separate app requires additional configuration and maintenance. It is simpler to deploy multiple versions of the same application and use App Engine Traffic Splitting to route traffic between them."
      },
      {
        "date": "2022-12-07T21:24:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is D"
      },
      {
        "date": "2022-12-07T11:11:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is D"
      },
      {
        "date": "2022-11-11T23:59:00.000Z",
        "voteCount": 1,
        "content": "D. Deploy a new version of your application in App Engine"
      },
      {
        "date": "2022-09-26T19:48:00.000Z",
        "voteCount": 1,
        "content": "without any doubt answer is D"
      },
      {
        "date": "2022-09-20T09:46:00.000Z",
        "voteCount": 1,
        "content": "D is correct\nA. no need to use GKE\nB. Compute engine not-relevant.\nC. cant deploy 2 applications in one project.\nD. --splits &amp;  --split-by flags are available for gcloud app deploy."
      },
      {
        "date": "2022-06-23T10:34:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2022-06-08T00:03:00.000Z",
        "voteCount": 1,
        "content": "D is the correct answer"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 77,
    "url": "https://www.examtopics.com/discussions/google/view/22218-exam-associate-cloud-engineer-topic-1-question-77-discussion/",
    "body": "You need to provide a cost estimate for a Kubernetes cluster using the GCP pricing calculator for Kubernetes. Your workload requires high IOPs, and you will also be using disk snapshots. You start by entering the number of nodes, average hours, and average days. What should you do next?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFill in local SSD. Fill in persistent disk storage and snapshot storage.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFill in local SSD. Add estimated cost for cluster management.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSelect Add GPUs. Fill in persistent disk storage and snapshot storage.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSelect Add GPUs. Add estimated cost for cluster management."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 22,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-06-05T09:56:00.000Z",
        "voteCount": 63,
        "content": "This one is Tricky,  local SSD is require for High IOPS - https://cloud.google.com/compute/docs/disks/local-ssd , but it say using disk snapshots.  A is correct."
      },
      {
        "date": "2020-06-10T04:44:00.000Z",
        "voteCount": 16,
        "content": "A is correct ."
      },
      {
        "date": "2023-12-29T04:42:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2023-12-28T17:56:00.000Z",
        "voteCount": 1,
        "content": "Obviously the correct answer is A"
      },
      {
        "date": "2023-11-06T03:30:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is C"
      },
      {
        "date": "2023-09-02T02:38:00.000Z",
        "voteCount": 2,
        "content": "A is correct , as ssd requires the High IOPS"
      },
      {
        "date": "2023-06-29T17:51:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2023-04-12T20:46:00.000Z",
        "voteCount": 1,
        "content": "since the requirement is high IOPs Local SSD is our best option. that makes:\n C and D as irrelevant. \nB Add estimated cost for cluster management is not related to storage requirement mentioned in the scenario"
      },
      {
        "date": "2023-02-21T09:04:00.000Z",
        "voteCount": 4,
        "content": "To provide a cost estimate for a Kubernetes cluster using the GCP pricing calculator for Kubernetes, after entering the number of nodes, average hours, and average days, you should fill in the required storage and snapshot details.\n\nGiven that your workload requires high IOPs and will also be using disk snapshots, the appropriate option would be;\n\nA. Fill in local SSD. Fill in persistent disk storage and snapshot storage."
      },
      {
        "date": "2023-04-17T05:57:00.000Z",
        "voteCount": 2,
        "content": "my lord! you're always right"
      },
      {
        "date": "2023-02-09T12:14:00.000Z",
        "voteCount": 1,
        "content": "Why do I need to fill out persistent disk storage and snapshot storage, it is already populated.\nfilling out local ssd should suffice. going with B"
      },
      {
        "date": "2023-02-21T09:11:00.000Z",
        "voteCount": 3,
        "content": "If persistent disk storage and snapshot storage are already populated in the GCP pricing calculator, you do not need to fill them out again. In that case, selecting local SSD and adding an estimated cost for cluster management, as suggested in Answer B, would be sufficient.\n\nHowever, it is important to note that the cost estimate may not be accurate if any of the details in the GCP pricing calculator are incorrect or do not match your requirements. Therefore, it is always a good practice to review all the details and ensure that they are accurate and up to date before finalizing the cost estimate.\n\nIn summary, if persistent disk storage and snapshot storage are already populated, and you only require local SSD and an estimated cost for cluster management, then Answer B is a valid choice. Still, I go with Answer A my friend. :)"
      },
      {
        "date": "2022-12-07T21:26:00.000Z",
        "voteCount": 1,
        "content": "A is correct ."
      },
      {
        "date": "2022-11-12T00:21:00.000Z",
        "voteCount": 1,
        "content": "A. local SSD for high iops"
      },
      {
        "date": "2022-10-16T07:22:00.000Z",
        "voteCount": 1,
        "content": "The question is about storage then A is the correct answer."
      },
      {
        "date": "2022-10-14T04:07:00.000Z",
        "voteCount": 1,
        "content": "for high IOPS needs local ssd and snapshot wording is clear enough so A is correct"
      },
      {
        "date": "2022-09-26T19:49:00.000Z",
        "voteCount": 2,
        "content": "we should not use GPU in this case. SSD is correct solution for storage . correct answer is A"
      },
      {
        "date": "2022-09-03T03:03:00.000Z",
        "voteCount": 1,
        "content": "Option A is correct"
      },
      {
        "date": "2022-06-23T10:36:00.000Z",
        "voteCount": 1,
        "content": "A is correct, Local SSD provides high performance."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 78,
    "url": "https://www.examtopics.com/discussions/google/view/22220-exam-associate-cloud-engineer-topic-1-question-78-discussion/",
    "body": "You are using Google Kubernetes Engine with autoscaling enabled to host a new application. You want to expose this new application to the public, using HTTPS on a public IP address. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Kubernetes Service of type NodePort for your application, and a Kubernetes Ingress to expose this Service via a Cloud Load Balancer.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Kubernetes Service of type ClusterIP for your application. Configure the public DNS name of your application using the IP of this Service.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Kubernetes Service of type NodePort to expose the application on port 443 of each node of the Kubernetes cluster. Configure the public DNS name of your application with the IP of every node of the cluster to achieve load-balancing.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a HAProxy pod in the cluster to load-balance the traffic to all the pods of the application. Forward the public traffic to HAProxy with an iptable rule. Configure the DNS name of your application using the public IP of the node HAProxy is running on."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 21,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-02-18T17:28:00.000Z",
        "voteCount": 53,
        "content": "HAProxy is HTTP only, doesnt support HTTPS, so you can reject option D\nhttps://www.haproxy.org/#desc\n\n\nCluster IP - is an internal IP, you cannot expose public externally. reject option B\n\n\nout of option A and C\n\nC, port 443 is https but public DNS is not going to give you a load balancing \nA is the right choice, \nkubernets ingress exposes HTTPS\nhttps://kubernetes.io/docs/concepts/services-networking/ingress/\n\nand cloud load balancer is the right choice which will help to expose the app to public"
      },
      {
        "date": "2022-01-12T01:24:00.000Z",
        "voteCount": 1,
        "content": "Pretty sure that option D works more from on premise then cloud because with cloud you pretty much don't have to configure your ip tables !"
      },
      {
        "date": "2020-06-05T10:11:00.000Z",
        "voteCount": 33,
        "content": "A is correct."
      },
      {
        "date": "2020-12-30T11:07:00.000Z",
        "voteCount": 21,
        "content": "Saw this which provides good context https://medium.com/google-cloud/kubernetes-nodeport-vs-loadbalancer-vs-ingress-when-should-i-use-what-922f010849e0"
      },
      {
        "date": "2021-02-22T20:40:00.000Z",
        "voteCount": 2,
        "content": "you nailed it."
      },
      {
        "date": "2024-07-25T00:08:00.000Z",
        "voteCount": 1,
        "content": "option A is correct, but do not use it in real deployments, it is a bad practice.  I am wondering why they didn't mention Cluster IP and exposing it via an ingress or at least a service of type loadbalancer"
      },
      {
        "date": "2023-12-30T17:56:00.000Z",
        "voteCount": 2,
        "content": "In Kubernetes, a Service of type NodePort is a way to expose your applications to external traffic. It's one of the several types of Services available in Kubernetes to control how external sources can access services running within the cluster. Here's what a NodePort service entails:\n\nExposing Services Outside the Cluster:\n\nA NodePort service makes your application accessible from outside the Kubernetes cluster by opening a specific port (the NodePort) on all the nodes (VMs) in your cluster. This port is randomly selected from a defined range (default: 30000-32767) unless you specify a particular port."
      },
      {
        "date": "2023-12-30T17:56:00.000Z",
        "voteCount": 1,
        "content": "When a NodePort service is created, each node in the cluster allocates the specified NodePort. External traffic can access the service by hitting any node's IP address at the NodePort, regardless of whether that node is actually running a pod for the service.\nKubernetes internally routes that traffic to the appropriate pods, even if they are running on different nodes."
      },
      {
        "date": "2023-11-06T03:48:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is A"
      },
      {
        "date": "2023-09-02T02:40:00.000Z",
        "voteCount": 2,
        "content": "option A is correct as you need load balancing and in option c dns will not give you load balancing"
      },
      {
        "date": "2023-08-18T23:13:00.000Z",
        "voteCount": 2,
        "content": "I didnt know, that ClusterIP is an internal IP and you cannot expose public externally..\n\nThanks !"
      },
      {
        "date": "2023-06-29T17:52:00.000Z",
        "voteCount": 1,
        "content": "A is very easy solution"
      },
      {
        "date": "2023-02-21T09:18:00.000Z",
        "voteCount": 1,
        "content": "To expose a new application hosted on Google Kubernetes Engine with autoscaling enabled to the public using HTTPS on a public IP address, the most appropriate option would be;\n\nA. Create a Kubernetes Service of type NodePort for your application, and a Kubernetes Ingress to expose this Service via a Cloud Load Balancer."
      },
      {
        "date": "2023-01-20T03:45:00.000Z",
        "voteCount": 1,
        "content": "A works and is correct, but service type should be ClusterIP"
      },
      {
        "date": "2022-12-07T21:27:00.000Z",
        "voteCount": 2,
        "content": "A is correct ."
      },
      {
        "date": "2022-09-26T19:50:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is A. Create a Kubernetes Service of type NodePort for your application, and a Kubernetes Ingress to expose this Service via a Cloud Load Balancer."
      },
      {
        "date": "2022-09-24T10:28:00.000Z",
        "voteCount": 1,
        "content": "had this question today"
      },
      {
        "date": "2022-07-06T08:10:00.000Z",
        "voteCount": 1,
        "content": "A is the correct"
      },
      {
        "date": "2022-06-23T10:37:00.000Z",
        "voteCount": 1,
        "content": "This is A without any second thought."
      },
      {
        "date": "2022-06-04T07:12:00.000Z",
        "voteCount": 1,
        "content": "Go for A"
      },
      {
        "date": "2022-04-21T09:27:00.000Z",
        "voteCount": 1,
        "content": "option A is an correct answer"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 79,
    "url": "https://www.examtopics.com/discussions/google/view/22222-exam-associate-cloud-engineer-topic-1-question-79-discussion/",
    "body": "You need to enable traffic between multiple groups of Compute Engine instances that are currently running two different GCP projects. Each group of Compute<br>Engine instances is running in its own VPC. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tVerify that both projects are in a GCP Organization. Create a new VPC and add all instances.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tVerify that both projects are in a GCP Organization. Share the VPC from one project and request that the Compute Engine instances in the other project use this shared VPC.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tVerify that you are the Project Administrator of both projects. Create two new VPCs and add all instances.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tVerify that you are the Project Administrator of both projects. Create a new VPC and add all instances."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 12,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-06-05T10:15:00.000Z",
        "voteCount": 34,
        "content": "B - https://cloud.google.com/vpc/docs/shared-vpc"
      },
      {
        "date": "2020-10-11T05:45:00.000Z",
        "voteCount": 10,
        "content": "B. Verify that both projects are in a GCP Organization. Share the VPC from one project and request that the Compute Engine instances in the other project use this shared VPC."
      },
      {
        "date": "2023-06-29T17:52:00.000Z",
        "voteCount": 2,
        "content": "shared-vpc is right option"
      },
      {
        "date": "2023-04-17T03:25:00.000Z",
        "voteCount": 1,
        "content": "Use shared VPC"
      },
      {
        "date": "2023-03-23T05:28:00.000Z",
        "voteCount": 1,
        "content": "Shared VPC is the correct choice here"
      },
      {
        "date": "2023-02-21T09:36:00.000Z",
        "voteCount": 4,
        "content": "To enable traffic between multiple groups of Compute Engine instances running in different VPCs of different GCP projects, the best option would be;\n\nB. Verify that both projects are in a GCP Organization. Share the VPC from one project and request that the Compute Engine instances in the other project use this shared VPC."
      },
      {
        "date": "2022-12-18T11:27:00.000Z",
        "voteCount": 1,
        "content": "B correct"
      },
      {
        "date": "2022-12-07T21:32:00.000Z",
        "voteCount": 1,
        "content": "B is correc"
      },
      {
        "date": "2022-11-19T00:56:00.000Z",
        "voteCount": 1,
        "content": "B is correct because is the concept of the shared VPC."
      },
      {
        "date": "2022-09-16T03:13:00.000Z",
        "voteCount": 1,
        "content": "ok, is B, but that means that the VMs in the \"other project\" have to change their ip?"
      },
      {
        "date": "2022-06-23T10:39:00.000Z",
        "voteCount": 1,
        "content": "It is shared VPC concept. Go with B"
      },
      {
        "date": "2022-06-23T10:39:00.000Z",
        "voteCount": 8,
        "content": "Shared VPC allows an organization to connect resources from multiple projects to a common Virtual Private Cloud (VPC) network, so that they can communicate with each other securely and efficiently using internal IPs from that network. When you use Shared VPC, you designate a project as a host project and attach one or more other service projects to it. The VPC networks in the host project are called Shared VPC networks. Eligible resources from service projects can use subnets in the Shared VPC network"
      },
      {
        "date": "2022-06-08T00:15:00.000Z",
        "voteCount": 1,
        "content": "shared vpc is the answer"
      },
      {
        "date": "2022-06-04T07:14:00.000Z",
        "voteCount": 1,
        "content": "Go for B"
      },
      {
        "date": "2022-04-11T13:26:00.000Z",
        "voteCount": 2,
        "content": "B. You use \"Shared VPC Network\" to share a network across several projects in your GCP organization. You designate a project as a host project, and attach one or more other service projects to it. Only works within the same organization. Only works across projects."
      },
      {
        "date": "2021-11-20T05:13:00.000Z",
        "voteCount": 1,
        "content": "B. Verify that both projects are in a GCP Organization. Share the VPC from one project and request that the Compute Engine instances in the other project use this shared VPC."
      },
      {
        "date": "2021-11-19T08:06:00.000Z",
        "voteCount": 1,
        "content": "B. Verify that both projects are in a GCP Organization. Share the VPC from one project and request that the Compute Engine instances in the other project use this shared VPC."
      },
      {
        "date": "2021-10-02T09:51:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 80,
    "url": "https://www.examtopics.com/discussions/google/view/24056-exam-associate-cloud-engineer-topic-1-question-80-discussion/",
    "body": "You want to add a new auditor to a Google Cloud Platform project. The auditor should be allowed to read, but not modify, all project items.<br>How should you configure the auditor's permissions?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a custom role with view-only project permissions. Add the user's account to the custom role.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a custom role with view-only service permissions. Add the user's account to the custom role.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSelect the built-in IAM project Viewer role. Add the user's account to this role.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSelect the built-in IAM service Viewer role. Add the user's account to this role."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 25,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 8,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-06-25T11:42:00.000Z",
        "voteCount": 45,
        "content": "C is correct\nroles/Viewer role provides access to all resources under the projects but do not alter the state of these resources"
      },
      {
        "date": "2022-08-06T15:35:00.000Z",
        "voteCount": 5,
        "content": "It should be A.\n\nhttps://cloud.google.com/iam/docs/faq#when_would_i_use_basic_roles\nWhen would I use basic roles?\nYou can use basic roles in development and test environments, where it might be appropriate for some principals to have wide-ranging permissions. Avoid basic roles in production environments."
      },
      {
        "date": "2022-08-06T15:36:00.000Z",
        "voteCount": 2,
        "content": "Principle of least privilege"
      },
      {
        "date": "2023-06-17T21:01:00.000Z",
        "voteCount": 1,
        "content": "i disagree."
      },
      {
        "date": "2023-01-29T13:39:00.000Z",
        "voteCount": 1,
        "content": "But in this case we're not asked to follow any best practices. Besides, the help article says \"In production environments, do not grant basic roles unless there is no alternative.\", and in this case there's no alternative since we need to grant access to all resources."
      },
      {
        "date": "2020-10-11T05:48:00.000Z",
        "voteCount": 13,
        "content": "C. Select the built-in IAM project Viewer role. Add the user's account to this role."
      },
      {
        "date": "2024-04-13T06:36:00.000Z",
        "voteCount": 4,
        "content": "C. Select the built-in IAM project Viewer role. Add the user's account to this role.\n\nExplanation:\n\nIAM Project Viewer Role: The IAM project Viewer role provides read-only access to all resources within a Google Cloud Platform project. This role allows the user to view project items, including resources and configurations, but does not grant permissions to modify them. This aligns with the requirement of allowing the auditor to read, but not modify, all project items.\n\nBuilt-in Role: The IAM project Viewer role is a built-in role provided by Google Cloud Platform. It is specifically designed for users who need read-only access to project resources.\n\nLeast Privilege: Selecting the IAM project Viewer role ensures that the auditor has the necessary permissions to perform their tasks without granting them unnecessary privileges. It follows the principle of least privilege, providing only the permissions required to fulfill their role."
      },
      {
        "date": "2024-02-26T23:52:00.000Z",
        "voteCount": 1,
        "content": "I think C is more correct ."
      },
      {
        "date": "2023-11-23T01:47:00.000Z",
        "voteCount": 2,
        "content": "C is better though it is a basic role, as the question says all the project items."
      },
      {
        "date": "2023-11-06T03:54:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is C"
      },
      {
        "date": "2023-10-19T14:32:00.000Z",
        "voteCount": 1,
        "content": "C is correct. Project viewer provide read-only permissions to all resources; no permission to change resources."
      },
      {
        "date": "2023-10-15T06:47:00.000Z",
        "voteCount": 1,
        "content": "To grant an auditor read-only access to all project items on Google Cloud Platform, you should choose option A:\n\nA. Create a custom role with view-only project permissions. Add the user's account to the custom role.\n\nExplanation:\n- Creating a custom role allows you to define specific permissions tailored to your needs, in this case, view-only access to project items.\n- By selecting the necessary read-only project permissions for the custom role, you can provide the auditor with the appropriate level of access without allowing modifications.\n- Adding the user's account to this custom role will grant them the specified permissions.\n\nOption B refers to \"view-only service permissions,\" which may not provide the desired level of access to all project items.\n\nOptions C and D suggest using built-in roles, but they may have more permissions than needed for a read-only auditor role. Custom roles offer a more precise approach for achieving the specified permissions."
      },
      {
        "date": "2023-07-20T21:20:00.000Z",
        "voteCount": 1,
        "content": "Answer is C. Select the built-in IAM project Viewer role. Add the user's account to this role.\n\nThe IAM project Viewer role is a built-in role in Google Cloud that provides read-only access to all resources within a project. This role allows users to view project items, configurations, and metadata but does not grant any permission to modify or make changes to the resources."
      },
      {
        "date": "2023-06-29T17:54:00.000Z",
        "voteCount": 1,
        "content": "with principle of leastprivilege should be A\nAlso, question is asking to set permission on single project. Basic principles grants permissions on all project."
      },
      {
        "date": "2023-04-18T08:16:00.000Z",
        "voteCount": 2,
        "content": "It is option A. I just referred here!\nhttps://cloud.google.com/iam/docs/roles-overview\nCaution: Basic roles include thousands of permissions across all Google Cloud services. In production environments, do not grant basic roles unless there is no alternative. Instead, grant the most limited predefined roles or custom roles that meet your needs"
      },
      {
        "date": "2023-04-17T03:27:00.000Z",
        "voteCount": 1,
        "content": "we should avoid basic roles"
      },
      {
        "date": "2023-04-14T13:37:00.000Z",
        "voteCount": 9,
        "content": "Go for C.\n\nThe debate is between A and C. From auditor accessibility perspective they are the same, but from practical perspective C is the only option. For people who vote for A, you must never work with auditors in an enterprise level project. There are hundred if not thousands of permission you need to set one by one if you create custom role by yourself. And they will come to you and ask for permission every single day. And this is an \"there's no alternative\" situation where using Basic role is practical."
      },
      {
        "date": "2023-03-09T04:06:00.000Z",
        "voteCount": 1,
        "content": "I would go with C, A and C are equally correct, with principle of leastprivilege should be A, with recomendation of not using custom roles becasue they are not maintained by gcp it should be C, since its not stating its a production env its a little bit ambiguous"
      },
      {
        "date": "2023-02-21T10:14:00.000Z",
        "voteCount": 3,
        "content": "To allow the new auditor to read, but not modify, all project items in a Google Cloud Platform project, the best option would be;\n\nC. Select the built-in IAM project Viewer role. Add the user's account to this role."
      },
      {
        "date": "2023-02-21T08:42:00.000Z",
        "voteCount": 1,
        "content": "Google recommends: \nBasic roles include thousands of permissions across all Google Cloud services. In production environments, do not grant basic roles unless there is no alternative. Instead, grant the most limited predefined roles or custom roles that meet your needs."
      },
      {
        "date": "2023-02-24T04:52:00.000Z",
        "voteCount": 1,
        "content": "I didn't find in question about production"
      },
      {
        "date": "2023-03-10T12:18:00.000Z",
        "voteCount": 4,
        "content": "auditor works on a production project. nothing to do them in test and development projects"
      },
      {
        "date": "2023-02-02T19:49:00.000Z",
        "voteCount": 2,
        "content": "Avoid basic roles in production environments."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 81,
    "url": "https://www.examtopics.com/discussions/google/view/22026-exam-associate-cloud-engineer-topic-1-question-81-discussion/",
    "body": "You are operating a Google Kubernetes Engine (GKE) cluster for your company where different teams can run non-production workloads. Your Machine Learning<br>(ML) team needs access to Nvidia Tesla P100 GPUs to train their models. You want to minimize effort and cost. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAsk your ML team to add the \u05d2\u20acaccelerator: gpu\u05d2\u20ac annotation to their pod specification.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRecreate all the nodes of the GKE cluster to enable GPUs on all of them.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate your own Kubernetes cluster on top of Compute Engine with nodes that have GPUs. Dedicate this cluster to your ML team.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a new, GPU-enabled, node pool to the GKE cluster. Ask your ML team to add the cloud.google.com/gke -accelerator: nvidia-tesla-p100 nodeSelector to their pod specification.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 22,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-06-03T13:22:00.000Z",
        "voteCount": 47,
        "content": "D is the correct answer.\n\nhttps://cloud.google.com/kubernetes-engine/docs/how-to/gpus"
      },
      {
        "date": "2020-08-06T17:32:00.000Z",
        "voteCount": 10,
        "content": "the documentation states \"Limitations\nBefore using GPUs on GKE, keep in mind the following limitations:\n\nYou cannot add GPUs to existing node pools.\nGPU nodes cannot be live migrated during maintenance events.\""
      },
      {
        "date": "2020-12-13T09:50:00.000Z",
        "voteCount": 16,
        "content": "In this case it is about adding a GPU enabled node pool not a GPU to an existing node-pool"
      },
      {
        "date": "2022-11-29T10:34:00.000Z",
        "voteCount": 1,
        "content": "You're correct that D says that, except that the question also says to use the most cost-effective method. Two node-pools would be more expensive than rebuilding the current one with GPU enabled."
      },
      {
        "date": "2023-01-11T01:00:00.000Z",
        "voteCount": 1,
        "content": "It also says to minimize effort, wouldn't recreating all the pools take way longer than just adding 1?"
      },
      {
        "date": "2020-10-11T06:03:00.000Z",
        "voteCount": 15,
        "content": "D. Add a new, GPU-enabled, node pool to the GKE cluster. Ask your ML team to add the cloud.google.com/gke -accelerator: nvidia-tesla-p100 nodeSelector to their pod specification."
      },
      {
        "date": "2023-11-06T04:01:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is D"
      },
      {
        "date": "2023-06-29T17:55:00.000Z",
        "voteCount": 1,
        "content": "D is the correct answer."
      },
      {
        "date": "2023-04-17T03:29:00.000Z",
        "voteCount": 1,
        "content": "Option D is good"
      },
      {
        "date": "2023-04-12T21:18:00.000Z",
        "voteCount": 2,
        "content": "Creating  new node pool w/ GPU-enabled instances is cost - saving solution. This way ML team workload will GPU instance for their ML and other team workload will run smoothly"
      },
      {
        "date": "2023-03-30T15:08:00.000Z",
        "voteCount": 1,
        "content": "D makes more cost effective"
      },
      {
        "date": "2023-02-22T11:28:00.000Z",
        "voteCount": 6,
        "content": "Answer D. Add a new, GPU-enabled, node pool to the GKE cluster. Ask your ML team to add the cloud.google.com/gke-accelerator: nvidia-tesla-p100 nodeSelector to their pod specification.\n\nAdding a new node pool with GPUs is the best option because it allows for a separate set of nodes that can be specifically allocated to workloads that require GPU acceleration, such as the Machine Learning (ML) team's workloads. This approach will not affect other workloads running on the original nodes, keeping the costs low and the overall cluster performance stable."
      },
      {
        "date": "2022-12-07T21:40:00.000Z",
        "voteCount": 1,
        "content": "D is the correct answer."
      },
      {
        "date": "2022-11-12T01:20:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2022-09-16T04:35:00.000Z",
        "voteCount": 1,
        "content": "D is  correct"
      },
      {
        "date": "2022-09-14T06:23:00.000Z",
        "voteCount": 2,
        "content": "D is correct\nBecause if you create entirely new node pool then its not cost efficient and also the pods which not require that much high GPU is get scheduled into it. So instead of that add a new node pool with GPU and in the pod YAML file mention the node affinity to get scheduled into the GPU enabled node pool."
      },
      {
        "date": "2022-06-23T11:19:00.000Z",
        "voteCount": 2,
        "content": "By looking at all answers first 3 can be eliminated without any second thought. D is correct."
      },
      {
        "date": "2022-06-04T07:28:00.000Z",
        "voteCount": 1,
        "content": "Go for D"
      },
      {
        "date": "2022-05-23T09:20:00.000Z",
        "voteCount": 2,
        "content": "In this case it is about adding a GPU enabled node pool not a GPU to an existing node-pool"
      },
      {
        "date": "2022-02-21T12:16:00.000Z",
        "voteCount": 1,
        "content": "D is the correct answer"
      },
      {
        "date": "2021-12-12T22:58:00.000Z",
        "voteCount": 3,
        "content": "D would be the right option when there is possibility to add GPUs without recreating the nodes."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 82,
    "url": "https://www.examtopics.com/discussions/google/view/22525-exam-associate-cloud-engineer-topic-1-question-82-discussion/",
    "body": "Your VMs are running in a subnet that has a subnet mask of 255.255.255.240. The current subnet has no more free IP addresses and you require an additional<br>10 IP addresses for new VMs. The existing and new VMs should all be able to reach each other without additional routes. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse gcloud to expand the IP range of the current subnet.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDelete the subnet, and recreate it using a wider range of IP addresses.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new project. Use Shared VPC to share the current network with the new project.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new subnet with the same starting IP but a wider range to overwrite the current subnet."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 28,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-06-07T14:42:00.000Z",
        "voteCount": 56,
        "content": "A: Expand the existing subnet. \n https://cloud.google.com/sdk/gcloud/reference/compute/networks/subnets/expand-ip-range"
      },
      {
        "date": "2020-10-11T07:23:00.000Z",
        "voteCount": 11,
        "content": "A. Use gcloud to expand the IP range of the current subnet."
      },
      {
        "date": "2024-01-09T19:44:00.000Z",
        "voteCount": 2,
        "content": "The correct answer is A. Use gcloud to expand the IP range of the current subnet.\n\nExpanding the primary IPv4 address range of a subnet does not cause a break or gap in network connectivity2. DHCP leases are not broken. IP addresses of running VMs at the time of the expansion do not change. You cannot \u201cun-expand\u201d or contract the range after it\u2019s expanded. Expansion is permanent."
      },
      {
        "date": "2023-11-06T04:05:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is A"
      },
      {
        "date": "2023-09-02T02:56:00.000Z",
        "voteCount": 2,
        "content": "A is correct , as you need to expand it"
      },
      {
        "date": "2023-06-29T17:55:00.000Z",
        "voteCount": 2,
        "content": "gcloud compute networks subnets expand-ip-range NAME --prefix-length=PREFIX_LENGTH [--region=REGION] [GCLOUD_WIDE_FLAG \u2026]"
      },
      {
        "date": "2023-03-27T11:53:00.000Z",
        "voteCount": 3,
        "content": "You can't expand the subnet  because the question states \"The current subnet has no more free IP addresses\", correct answer is C."
      },
      {
        "date": "2023-06-05T02:22:00.000Z",
        "voteCount": 8,
        "content": "that's exactly why you expand the subnet =D"
      },
      {
        "date": "2023-03-18T01:37:00.000Z",
        "voteCount": 1,
        "content": "Use gcloud to expand the IP range of the current subnet."
      },
      {
        "date": "2023-02-28T09:27:00.000Z",
        "voteCount": 1,
        "content": "A. Use gcloud to expand the IP range of the current subnet."
      },
      {
        "date": "2023-02-22T11:43:00.000Z",
        "voteCount": 3,
        "content": "Answer A (Use gcloud to expand the IP range of the current subnet): This option is correct because it allows you to expand the primary IP range of the existing subnet to accommodate the additional 10 IP addresses required for the new VMs. This can be done without deleting or recreating the subnet, which saves time and avoids disrupting any existing resources that are using the subnet."
      },
      {
        "date": "2023-02-12T23:30:00.000Z",
        "voteCount": 3,
        "content": "A\nTo get an additional 10 IP addresses for new VMs running in a subnet that has no more free IP addresses and where existing and new VMs should be able to reach each other without additional routes, you should use gcloud to expand the IP range of the current subnet. This can be achieved by modifying the IP range of the subnet with the gcloud command-line tool. You can use the following command to increase the range of the subnet:\n\n\ngcloud compute networks subnets expand [SUBNET_NAME] --range=[NEW_IP_RANGE]\n\n\nMake sure to specify the name of the subnet you want to modify, and choose a new IP range that includes the current range and 10 additional IP addresses."
      },
      {
        "date": "2022-12-10T06:36:00.000Z",
        "voteCount": 1,
        "content": "A 100%"
      },
      {
        "date": "2022-12-07T22:33:00.000Z",
        "voteCount": 1,
        "content": "A: Expand the existing subnet."
      },
      {
        "date": "2022-11-12T01:22:00.000Z",
        "voteCount": 1,
        "content": "expand the IP range of the current subnet"
      },
      {
        "date": "2022-10-28T06:12:00.000Z",
        "voteCount": 1,
        "content": "Why in the world would C be the answer?   A is def the answer."
      },
      {
        "date": "2022-10-05T13:28:00.000Z",
        "voteCount": 1,
        "content": "A. is correct,\n\ngcloud compute networks subnets expand-ip-range - expand the IP range of a Compute Engine subnetwork\n\ngcloud compute networks subnets expand-ip-range NAME --prefix-length=PREFIX_LENGTH [--region=REGION] [GCLOUD_WIDE_FLAG \u2026]"
      },
      {
        "date": "2022-09-26T19:53:00.000Z",
        "voteCount": 1,
        "content": "correct answer is A. Use gcloud to expand the IP range of the current subnet."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 83,
    "url": "https://www.examtopics.com/discussions/google/view/22857-exam-associate-cloud-engineer-topic-1-question-83-discussion/",
    "body": "Your organization uses G Suite for communication and collaboration. All users in your organization have a G Suite account. You want to grant some G Suite users access to your Cloud Platform project. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Cloud Identity in the GCP Console for your domain.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGrant them the required IAM roles using their G Suite email address.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a CSV sheet with all users' email addresses. Use the gcloud command line tool to convert them into Google Cloud Platform accounts.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the G Suite console, add the users to a special group called cloud-console-users@yourdomain.com. Rely on the default behavior of the Cloud Platform to grant users access if they are members of this group."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 16,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-06-11T07:38:00.000Z",
        "voteCount": 30,
        "content": "B is correct"
      },
      {
        "date": "2020-06-20T06:24:00.000Z",
        "voteCount": 22,
        "content": "B is correct: To actively adopt the Organization resource, the G Suite or Cloud Identity super admins need to assign the Organization Administrator Cloud IAM role to a user or group"
      },
      {
        "date": "2023-11-06T04:10:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is B"
      },
      {
        "date": "2023-09-02T02:57:00.000Z",
        "voteCount": 3,
        "content": "B seems more correct as per thhe google pratcices"
      },
      {
        "date": "2023-06-29T17:56:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2023-04-02T21:53:00.000Z",
        "voteCount": 1,
        "content": "Dear friends,\nGreat responses in question.\nCan someone with contributor access, send me the remaining questions to this email: zwwdplay@hotmail.com"
      },
      {
        "date": "2023-02-22T11:47:00.000Z",
        "voteCount": 9,
        "content": "Answer B. Grant them the required IAM roles using their G Suite email address.\n\nTo grant G Suite users access to a Cloud Platform project, you should use their G Suite email addresses to grant them the required IAM roles. \n\nAnswer A is incorrect because enabling Cloud Identity is not necessary for granting G Suite users access to a Cloud Platform project. Cloud Identity provides a centralized identity management system for G Suite and Cloud Platform, but it is not required for this use case.\n\nAnswer C is incorrect because there is no need to convert G Suite email addresses into Google Cloud Platform accounts. G Suite users already have Google accounts and can be granted access to Cloud Platform using their G Suite email addresses.\n\nAnswer D is incorrect because there is no default behavior in the Cloud Platform to grant access to users who are members of a particular group. Access to Cloud Platform resources is granted based on IAM roles and policies, not group membership."
      },
      {
        "date": "2022-11-12T01:25:00.000Z",
        "voteCount": 1,
        "content": "Grant them the required IAM roles using their G Suite email address"
      },
      {
        "date": "2022-06-23T11:22:00.000Z",
        "voteCount": 1,
        "content": "B is right"
      },
      {
        "date": "2022-06-04T07:54:00.000Z",
        "voteCount": 1,
        "content": "Go for B"
      },
      {
        "date": "2022-02-18T05:38:00.000Z",
        "voteCount": 2,
        "content": "B as per the comments"
      },
      {
        "date": "2021-11-20T05:22:00.000Z",
        "voteCount": 2,
        "content": "B. Grant them the required IAM roles using their G Suite email address."
      },
      {
        "date": "2021-11-19T12:13:00.000Z",
        "voteCount": 1,
        "content": "B is Correct"
      },
      {
        "date": "2021-10-11T03:44:00.000Z",
        "voteCount": 2,
        "content": "B is correct"
      },
      {
        "date": "2021-05-13T03:44:00.000Z",
        "voteCount": 2,
        "content": "B is correct"
      },
      {
        "date": "2021-03-27T09:14:00.000Z",
        "voteCount": 2,
        "content": "B. Grant them the required IAM roles using their G Suite email address."
      },
      {
        "date": "2021-03-25T01:53:00.000Z",
        "voteCount": 3,
        "content": "B is correct. Grant them the required IAM roles using their G Suite email address."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 84,
    "url": "https://www.examtopics.com/discussions/google/view/24058-exam-associate-cloud-engineer-topic-1-question-84-discussion/",
    "body": "You have a Google Cloud Platform account with access to both production and development projects. You need to create an automated process to list all compute instances in development and production projects on a daily basis. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate two configurations using gcloud config. Write a script that sets configurations as active, individually. For each configuration, use gcloud compute instances list to get a list of compute resources.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate two configurations using gsutil config. Write a script that sets configurations as active, individually. For each configuration, use gsutil compute instances list to get a list of compute resources.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGo to Cloud Shell and export this information to Cloud Storage on a daily basis.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGo to GCP Console and export this information to Cloud SQL on a daily basis."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 22,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-06-25T11:55:00.000Z",
        "voteCount": 38,
        "content": "A is correct"
      },
      {
        "date": "2020-10-11T08:15:00.000Z",
        "voteCount": 15,
        "content": "A. Create two configurations using gcloud config. Write a script that sets configurations as active, individually. For each configuration, use gcloud compute instances list to get a list of compute resources."
      },
      {
        "date": "2023-11-06T04:13:00.000Z",
        "voteCount": 1,
        "content": "A is the best"
      },
      {
        "date": "2023-09-02T03:08:00.000Z",
        "voteCount": 3,
        "content": "A is correct , first list , then acitvate it"
      },
      {
        "date": "2023-08-25T06:12:00.000Z",
        "voteCount": 1,
        "content": "A. Create two configurations using gcloud config. Write a script that sets configurations as active, individually. For each configuration, use gcloud compute instances list to get a list of compute resources."
      },
      {
        "date": "2023-06-29T17:58:00.000Z",
        "voteCount": 1,
        "content": "Activate each config and list the instances"
      },
      {
        "date": "2023-03-23T05:34:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2023-02-22T11:56:00.000Z",
        "voteCount": 10,
        "content": "Answer A is the correct answer.\n\nThe most straightforward way to list all compute instances in development and production projects is to use gcloud compute instances list command. However, since the account has access to both production and development projects, it's necessary to create two configurations with different project IDs. \n\nAnswer B is incorrect because gsutil is used for object storage operations and not compute instances. (DISTRACTOR)\n\nAnswers C and D are incorrect because they do not provide a straightforward solution for listing compute instances in multiple projects."
      },
      {
        "date": "2023-02-02T15:35:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2022-12-07T23:15:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2022-11-12T01:28:00.000Z",
        "voteCount": 1,
        "content": "gcloud instead of gsutil"
      },
      {
        "date": "2022-09-15T05:50:00.000Z",
        "voteCount": 3,
        "content": "\"gcloud\" can create and manage Google Cloud resources while \"gsutil\" cannot do so. \"gsutil\" can manipulate buckets, bucket's objects and bucket ACLs on GCS(Google Cloud Storage) while \"gcloud\" cannot do so"
      },
      {
        "date": "2022-09-19T23:00:00.000Z",
        "voteCount": 2,
        "content": "Which one is correct in all cases , suggested one or community one . Im confused totally for all questions"
      },
      {
        "date": "2022-08-08T20:53:00.000Z",
        "voteCount": 1,
        "content": "A. Gsutil is used for cloud storage bucket"
      },
      {
        "date": "2022-06-23T11:36:00.000Z",
        "voteCount": 4,
        "content": "A is right, This is part of Tutorial Dojo practice questions"
      },
      {
        "date": "2022-06-04T08:08:00.000Z",
        "voteCount": 1,
        "content": "Go for A"
      },
      {
        "date": "2022-04-10T10:19:00.000Z",
        "voteCount": 1,
        "content": "Obviously A is correct"
      },
      {
        "date": "2021-12-21T16:37:00.000Z",
        "voteCount": 1,
        "content": "This looks to be a multiple-choice question. The answer A is correct, and the C completes the task... Does it make sense?"
      },
      {
        "date": "2021-12-29T09:44:00.000Z",
        "voteCount": 3,
        "content": "C is not automated task \n\nA is correct"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 85,
    "url": "https://www.examtopics.com/discussions/google/view/22875-exam-associate-cloud-engineer-topic-1-question-85-discussion/",
    "body": "You have a large 5-TB AVRO file stored in a Cloud Storage bucket. Your analysts are proficient only in SQL and need access to the data stored in this file. You want to find a cost-effective way to complete their request as soon as possible. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLoad data in Cloud Datastore and run a SQL query against it.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a BigQuery table and load data in BigQuery. Run a SQL query on this table and drop this table after you complete your request.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate external tables in BigQuery that point to Cloud Storage buckets and run a SQL query on these external tables to complete your request.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Hadoop cluster and copy the AVRO file to NDFS by compressing it. Load the file in a hive table and provide access to your analysts so that they can run SQL queries."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 17,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-12-28T09:16:00.000Z",
        "voteCount": 154,
        "content": "Breaking down the question into key points - \n\n\n1. 5-TB AVRO file stored in a Cloud Storage bucket.\n2. Analysts are proficient only in SQL \n3. cost-effective way to complete their request as soon as possible\n\n\nA. ....Load data in Cloud Datastore... (Not Correct because Cloud Datastore is not a good option to run SQL Queries)\n\nB. ...Load data in BigQuery.... (Not Cost Effective because loading the data which is already present in the bucket into BigQuery again is expensive)\n\nC. Create external tables in BigQuery that point to Cloud Storage buckets and run a SQL query on these external tables to complete your request.\n(This is the right answer as it meets all the requirements from the question)\n\nD. Create a Hadoop cluster and copy the AVRO file to NDFS by compressing it. Load the file in a hive table and provide access to your analysts so that they can run SQL queries.\n(Too roundabout and indirect. Not the right option)"
      },
      {
        "date": "2021-03-21T19:18:00.000Z",
        "voteCount": 17,
        "content": "listem this guy"
      },
      {
        "date": "2020-06-20T23:29:00.000Z",
        "voteCount": 23,
        "content": "C is correct: https://cloud.google.com/bigquery/external-data-sources"
      },
      {
        "date": "2024-04-12T14:15:00.000Z",
        "voteCount": 1,
        "content": "D - The ONLY answer that provides data for the analysts is D. The question states that the analysts need access to the data and that they only know SQL (not that *you* only know SQL). The other 3 answers don't provide the data to analysts. You might have to fill in the blanks that you will pass it to them in a spreadsheet format, but that very likely won't satisfy their needs to query the data using SQL and at 5TB size, that isn't ideal. Therefore D is the only answer that satisfies the requirement."
      },
      {
        "date": "2024-01-09T19:50:00.000Z",
        "voteCount": 1,
        "content": "The most cost-effective and efficient option would be Option C: Create external tables in BigQuery that point to Cloud Storage buckets and run a SQL query on these external tables to complete your request.\n\nThis approach allows you to query data directly from the AVRO file stored in the Cloud Storage bucket without having to load the data into BigQuery first. This saves both time and money as you are not charged for the storage of data within BigQuery. Plus, BigQuery is designed to be able to handle large datasets, making it a suitable choice for a 5-TB AVRO file. Your analysts, who are proficient in SQL, can easily work with BigQuery as it uses a SQL interface."
      },
      {
        "date": "2023-11-06T04:16:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is C"
      },
      {
        "date": "2023-09-02T03:12:00.000Z",
        "voteCount": 1,
        "content": "C is the right answer , as"
      },
      {
        "date": "2023-08-25T06:08:00.000Z",
        "voteCount": 2,
        "content": "C. Create external tables in BigQuery that point to Cloud Storage buckets and run a SQL query on these external tables to complete your request."
      },
      {
        "date": "2023-08-25T06:01:00.000Z",
        "voteCount": 1,
        "content": "C. Create external tables in BigQuery that point to Cloud Storage buckets and run a SQL query on these external tables to complete your request."
      },
      {
        "date": "2023-02-22T12:02:00.000Z",
        "voteCount": 7,
        "content": "Answer C is the most cost-effective and efficient way to provide analysts access to the data stored in the 5-TB AVRO file in Cloud Storage. \n\nHere's why:\n\nYou can create external tables in BigQuery that point to the 5-TB AVRO file stored in Cloud Storage. External tables allow you to query data stored in Cloud Storage without the need to load the data into BigQuery. This is a cost-effective way to provide your analysts' access to the data they need, and it is also an efficient solution since you can run SQL queries on the data directly in BigQuery."
      },
      {
        "date": "2023-01-02T01:54:00.000Z",
        "voteCount": 1,
        "content": "External tables in BigQuery"
      },
      {
        "date": "2022-12-07T23:18:00.000Z",
        "voteCount": 1,
        "content": "C is correct: https://cloud.google.com/bigquery/external-data-sources"
      },
      {
        "date": "2022-11-12T02:06:00.000Z",
        "voteCount": 1,
        "content": "external tables in BigQuery"
      },
      {
        "date": "2022-10-22T07:01:00.000Z",
        "voteCount": 1,
        "content": "Similar to Athena"
      },
      {
        "date": "2022-10-05T13:49:00.000Z",
        "voteCount": 2,
        "content": "C. is correct,\nAn external data source is a data source that you can query directly from BigQuery, even though the data is not stored in BigQuery storage.\n\nBigQuery supports the following external data sources:\n\nAmazon S3\nAzure Storage\nCloud Bigtable\nCloud Spanner\nCloud SQL\nCloud Storage\nDrive"
      },
      {
        "date": "2022-09-12T04:29:00.000Z",
        "voteCount": 1,
        "content": "answer is c"
      },
      {
        "date": "2022-06-23T11:40:00.000Z",
        "voteCount": 1,
        "content": "mohdafiuddin explanation is very detailed .. C is right"
      },
      {
        "date": "2022-06-04T08:10:00.000Z",
        "voteCount": 2,
        "content": "Go for C"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 86,
    "url": "https://www.examtopics.com/discussions/google/view/22028-exam-associate-cloud-engineer-topic-1-question-86-discussion/",
    "body": "You need to verify that a Google Cloud Platform service account was created at a particular time. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFilter the Activity log to view the Configuration category. Filter the Resource type to Service Account.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFilter the Activity log to view the Configuration category. Filter the Resource type to Google Project.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFilter the Activity log to view the Data Access category. Filter the Resource type to Service Account.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFilter the Activity log to view the Data Access category. Filter the Resource type to Google Project."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 34,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-06-03T13:45:00.000Z",
        "voteCount": 79,
        "content": "Correct Answer is A.\nFilter the Activity log to view the Configuration category. Filter the Resource type to Service Account."
      },
      {
        "date": "2020-06-09T10:26:00.000Z",
        "voteCount": 10,
        "content": "I agree A"
      },
      {
        "date": "2023-11-05T21:10:00.000Z",
        "voteCount": 1,
        "content": "ayyy its mlantonis pre-fame"
      },
      {
        "date": "2020-07-14T16:29:00.000Z",
        "voteCount": 29,
        "content": "A - I reproduced in my project."
      },
      {
        "date": "2024-03-13T15:38:00.000Z",
        "voteCount": 1,
        "content": "A baby"
      },
      {
        "date": "2023-11-27T16:40:00.000Z",
        "voteCount": 3,
        "content": "Why is the correct answer almost always different from the community answers?"
      },
      {
        "date": "2024-04-12T14:20:00.000Z",
        "voteCount": 1,
        "content": "It could be a way for Google to identify people who have used this resource and invalidate their exam results. Best just not to click on the \"solution\" at all."
      },
      {
        "date": "2024-02-15T08:38:00.000Z",
        "voteCount": 2,
        "content": "Funskies!"
      },
      {
        "date": "2023-11-06T04:19:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is A"
      },
      {
        "date": "2023-09-02T03:18:00.000Z",
        "voteCount": 3,
        "content": "A is the correct answer, you can see it in configuration category"
      },
      {
        "date": "2023-08-25T01:46:00.000Z",
        "voteCount": 1,
        "content": "A. Filter the Activity log to view the Configuration category. Filter the Resource type to Service Account."
      },
      {
        "date": "2023-02-22T12:31:00.000Z",
        "voteCount": 5,
        "content": "Answer A. Filter the Activity log to view the Configuration category. Filter the Resource type to Service Account.\n\nThe Activity log is the primary tool for viewing and analyzing activity within a Google Cloud project, including all Service Account-related activity. By filtering the Activity log to view the Configuration category and filtering the Resource type to Service Account, you can see when a Service Account was created, updated, or deleted, along with other related metadata such as the user who performed the action and the IP address from which the action was performed."
      },
      {
        "date": "2022-12-28T08:54:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer is A."
      },
      {
        "date": "2022-12-10T06:45:00.000Z",
        "voteCount": 1,
        "content": "A 100%"
      },
      {
        "date": "2022-12-07T23:19:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer is A."
      },
      {
        "date": "2022-12-02T21:44:00.000Z",
        "voteCount": 10,
        "content": "Thank God, we have a discussion community and not just the solutions from this site lol."
      },
      {
        "date": "2022-11-19T01:16:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2022-11-12T02:17:00.000Z",
        "voteCount": 1,
        "content": "A. Filter the Activity log to view the Configuration category. Filter the Resource type to Service Account."
      },
      {
        "date": "2022-11-07T13:50:00.000Z",
        "voteCount": 1,
        "content": "Answer is A"
      },
      {
        "date": "2022-11-03T03:16:00.000Z",
        "voteCount": 2,
        "content": "for sure, i simulated here."
      },
      {
        "date": "2022-09-11T22:35:00.000Z",
        "voteCount": 1,
        "content": "A key word is configuration"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 87,
    "url": "https://www.examtopics.com/discussions/google/view/22876-exam-associate-cloud-engineer-topic-1-question-87-discussion/",
    "body": "You deployed an LDAP server on Compute Engine that is reachable via TLS through port 636 using UDP. You want to make sure it is reachable by clients over that port. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd the network tag allow-udp-636 to the VM instance running the LDAP server.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a route called allow-udp-636 and set the next hop to be the VM instance running the LDAP server.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a network tag of your choice to the instance. Create a firewall rule to allow ingress on UDP port 636 for that network tag.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a network tag of your choice to the instance running the LDAP server. Create a firewall rule to allow egress on UDP port 636 for that network tag."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-06-14T01:46:00.000Z",
        "voteCount": 33,
        "content": "Option C is the right one"
      },
      {
        "date": "2020-06-25T12:03:00.000Z",
        "voteCount": 25,
        "content": "C is correct\nYou tag the instances ,then create ingress firewall rules to allow udp on desired port for target-tags name applied to instances"
      },
      {
        "date": "2023-11-27T16:41:00.000Z",
        "voteCount": 2,
        "content": "Option C, agree"
      },
      {
        "date": "2023-11-06T04:22:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is C"
      },
      {
        "date": "2023-09-02T03:20:00.000Z",
        "voteCount": 1,
        "content": "C is correct bcoz of ingress"
      },
      {
        "date": "2023-03-23T05:46:00.000Z",
        "voteCount": 1,
        "content": "Firewall rule for ingress is correct"
      },
      {
        "date": "2023-02-22T12:40:00.000Z",
        "voteCount": 4,
        "content": "Answer C is correct: Add a network tag of your choice to the instance. Create a firewall rule to allow ingress on UDP port 636 for that network tag.\n\nTo make sure that the LDAP server is reachable by clients over port 636 using UDP, you need to allow ingress traffic on that port. You can achieve this by adding a network tag to the instance running the LDAP server and then creating a firewall rule that allows ingress traffic on UDP port 636 for that network tag."
      },
      {
        "date": "2022-12-25T05:58:00.000Z",
        "voteCount": 1,
        "content": "can anyone please suggest why D is not correct? thanks"
      },
      {
        "date": "2024-04-04T00:29:00.000Z",
        "voteCount": 2,
        "content": "egress!"
      },
      {
        "date": "2023-02-22T12:39:00.000Z",
        "voteCount": 4,
        "content": "Answer D is incorrect because adding a network tag of your choice to the instance running the LDAP server and creating a firewall rule to allow egress traffic on UDP port 636 for that network tag would not allow incoming traffic on that port. You need to create a firewall rule that allows ingress traffic on that port."
      },
      {
        "date": "2022-11-12T02:22:00.000Z",
        "voteCount": 1,
        "content": "allow ingress"
      },
      {
        "date": "2022-06-23T11:44:00.000Z",
        "voteCount": 1,
        "content": "C is right."
      },
      {
        "date": "2022-06-04T12:17:00.000Z",
        "voteCount": 1,
        "content": "Go for C"
      },
      {
        "date": "2022-01-29T04:39:00.000Z",
        "voteCount": 4,
        "content": "You are developing a new web application that will be deployed on Google Cloud Platform. As part of your release cycle, you want to test updates to your application on a small portion of real user traffic. The majority of the users should still be directed towards a stable version of your application. What should you do?\n\nA. Deploy me application on App Engine For each update, create a new version of the same service Configure traffic splitting to send a small percentage of traffic to the new version\nB. Deploy the application on App Engine For each update, create a new service Configure traffic splitting to send a small percentage of traffic to the new service.\nC. Deploy the application on Kubernetes Engine For a new release, update the deployment to use the new version\nD. Deploy the application on Kubernetes Engine For a now release, create a new deployment for the new version Update the service e to use the now deployment."
      },
      {
        "date": "2022-02-03T06:51:00.000Z",
        "voteCount": 1,
        "content": "A create new version"
      },
      {
        "date": "2022-05-22T06:02:00.000Z",
        "voteCount": 1,
        "content": "A without any doubt."
      },
      {
        "date": "2023-02-05T12:30:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2022-01-28T19:09:00.000Z",
        "voteCount": 2,
        "content": "C is correct as you can use tags and then set firewall rules for instances with such tag."
      },
      {
        "date": "2021-12-06T06:45:00.000Z",
        "voteCount": 2,
        "content": "C IS PERFECT"
      },
      {
        "date": "2021-11-20T05:28:00.000Z",
        "voteCount": 2,
        "content": "C. Add a network tag of your choice to the instance. Create a firewall rule to allow ingress on UDP port 636 for that network tag."
      },
      {
        "date": "2021-11-19T12:42:00.000Z",
        "voteCount": 2,
        "content": "C is Correct"
      },
      {
        "date": "2021-06-03T13:24:00.000Z",
        "voteCount": 2,
        "content": "C A tag is simply a character string added to a tags field in a resource, such as Compute Engine virtual machine (VM) instances or instance templates. A tag is not a separate resource, so you cannot create it separately. All resources with that string are considered to have that tag. Tags enable you to make firewall rules and routes applicable to specific VM instances."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 88,
    "url": "https://www.examtopics.com/discussions/google/view/22714-exam-associate-cloud-engineer-topic-1-question-88-discussion/",
    "body": "You need to set a budget alert for use of Compute Engineer services on one of the three Google Cloud Platform projects that you manage. All three projects are linked to a single billing account. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tVerify that you are the project billing administrator. Select the associated billing account and create a budget and alert for the appropriate project.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tVerify that you are the project billing administrator. Select the associated billing account and create a budget and a custom alert.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tVerify that you are the project administrator. Select the associated billing account and create a budget for the appropriate project.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tVerify that you are project administrator. Select the associated billing account and create a budget and a custom alert."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 15,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-06-10T02:45:00.000Z",
        "voteCount": 48,
        "content": "I think the answer is A, You can rely on default alert. No need for custom alert"
      },
      {
        "date": "2020-09-26T18:07:00.000Z",
        "voteCount": 1,
        "content": "Right its not asking to set custom alert"
      },
      {
        "date": "2020-11-12T01:34:00.000Z",
        "voteCount": 6,
        "content": "One point - there is no such role as Project Billing Administrator - it should be Project Billing Manager but he can't create budgets, the only one who can - Billing Account Administrator. Nor Project Administrator exists. Very tricky question, maybe the option a wrong, hope smb will catch it on exam and pass some light on real variants.\nhttps://cloud.google.com/iam/docs/understanding-roles#billing-roles"
      },
      {
        "date": "2020-12-21T03:39:00.000Z",
        "voteCount": 12,
        "content": "Eshkrkrkr read the question calmly. The role there is Billing Administrator.  Not Project Billing Administrator. \n\nIt's more like: \u201cVerify you are the project; billing administrator\u201d"
      },
      {
        "date": "2022-08-04T02:28:00.000Z",
        "voteCount": 7,
        "content": "more like: \"\u201cVerify you are the project's billing administrator\u201d\""
      },
      {
        "date": "2021-10-21T11:33:00.000Z",
        "voteCount": 7,
        "content": "I agree. If I'm not wrong, project admin doesn't have billing permissions so C and D discarded. Between A and B, option B looks like it works but we would be creating a budget and alert receiving info about billing as a whole; so A delimits billing for the project you want to get info from."
      },
      {
        "date": "2020-06-20T23:39:00.000Z",
        "voteCount": 17,
        "content": "A is correct, as you can set a default alert also on a single project: https://cloud.google.com/billing/docs/how-to/budgets"
      },
      {
        "date": "2022-08-10T03:59:00.000Z",
        "voteCount": 2,
        "content": "why nobody is talking about  \"set a budget alert for use of Compute Engineer services\" only.. why not custom alert ?how default alert ?"
      },
      {
        "date": "2023-12-05T08:07:00.000Z",
        "voteCount": 2,
        "content": "Is that not a typo? \"*Compute Engine\""
      },
      {
        "date": "2020-09-11T03:13:00.000Z",
        "voteCount": 3,
        "content": "Agreed. Per the link included: \"To create a budget for your Cloud Billing account, you must be a Billing Account Administrator on the Cloud Billing account.\" So that eliminates C &amp; D. Then no need for custom alert, eliminating B. The answer is A."
      },
      {
        "date": "2023-11-06T04:25:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is A"
      },
      {
        "date": "2023-09-02T03:24:00.000Z",
        "voteCount": 3,
        "content": "A is correct because, there is default alert, no need oof custom alert"
      },
      {
        "date": "2023-08-25T03:59:00.000Z",
        "voteCount": 1,
        "content": "A. Verify that you are the project billing administrator. Select the associated billing account and create a budget and alert for the appropriate project."
      },
      {
        "date": "2023-07-20T21:42:00.000Z",
        "voteCount": 1,
        "content": "Answer is B.\n\nExplanation: In Google Cloud, budget alerts are associated with billing accounts, not individual projects. Since all three projects are linked to a single billing account, you need to be the billing administrator to set up a budget and alert for that billing account."
      },
      {
        "date": "2023-02-22T12:52:00.000Z",
        "voteCount": 5,
        "content": "Answer A. Verify that you are the project billing administrator. Select the associated billing account and create a budget and alert for the appropriate project.\n\nIn this scenario, you need to create a budget alert for the use of Compute Engine services on a specific project. Since all three projects are linked to a single billing account, you need to make sure that you are the billing administrator for that account. Once verified, you can create a budget and alert for the specific project by selecting the associated billing account and setting the budget and alert for the appropriate project."
      },
      {
        "date": "2023-02-22T12:53:00.000Z",
        "voteCount": 1,
        "content": "INCORRECT\n\nAnswer B is incorrect because a custom alert is not necessary for this scenario. A budget alert alone is sufficient to notify you when your spending reaches a certain threshold.\n\nAnswer C is incorrect because, while a project administrator can create a budget for the project, they cannot set a budget alert. Only a billing administrator has the necessary permissions to create a budget alert.\n\nAnswer D is incorrect because a project administrator cannot create a custom alert on the associated billing account. Custom alerts can only be created by billing administrators."
      },
      {
        "date": "2023-01-29T15:24:00.000Z",
        "voteCount": 1,
        "content": "I don't even think there's an option for custom budget alert since all budget alerts are kind of the same and we can only customize (with the actual word \"customize\") the recipients. A should be correct."
      },
      {
        "date": "2022-12-07T00:24:00.000Z",
        "voteCount": 1,
        "content": "I think it's B because you can choose how to get alerts, by mail, slack, phone, etc."
      },
      {
        "date": "2022-11-25T01:11:00.000Z",
        "voteCount": 1,
        "content": "Compute Engineer services, please read as Compute Engine services. Ans is A"
      },
      {
        "date": "2022-11-19T01:54:00.000Z",
        "voteCount": 1,
        "content": "I will choice B, It's clear in the question that he only wants to have a specific budget for the VMs instance and have alert of the cost of these&nbsp;instances go out the budget."
      },
      {
        "date": "2022-10-05T14:01:00.000Z",
        "voteCount": 1,
        "content": "A. is correct,\nYou can define the scope of the budget. For example, you can scope the budget to apply to the spend in an entire Cloud Billing account, or narrow the scope to one or more projects, and/or one or more services, and/or other budget filters applicable to your Cloud Billing account."
      },
      {
        "date": "2022-07-14T13:54:00.000Z",
        "voteCount": 1,
        "content": "Billing Administrator is in between organization and projects, So he can chose and select appropriate project."
      },
      {
        "date": "2022-06-23T11:47:00.000Z",
        "voteCount": 1,
        "content": "You need billing admin not a project admin .. A is right"
      },
      {
        "date": "2022-06-09T02:39:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2022-06-04T12:20:00.000Z",
        "voteCount": 1,
        "content": "Go for A"
      },
      {
        "date": "2022-04-20T03:36:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 89,
    "url": "https://www.examtopics.com/discussions/google/view/22237-exam-associate-cloud-engineer-topic-1-question-89-discussion/",
    "body": "You are migrating a production-critical on-premises application that requires 96 vCPUs to perform its task. You want to make sure the application runs in a similar environment on GCP. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWhen creating the VM, use machine type n1-standard-96.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWhen creating the VM, use Intel Skylake as the CPU platform.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate the VM using Compute Engine default settings. Use gcloud to modify the running instance to have 96 vCPUs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStart the VM using Compute Engine default settings, and adjust as you go based on Rightsizing Recommendations."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 26,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-06-05T13:12:00.000Z",
        "voteCount": 52,
        "content": "A is correct - https://cloud.google.com/compute/docs/machine-types"
      },
      {
        "date": "2023-08-18T22:28:00.000Z",
        "voteCount": 2,
        "content": "Indeed, there is a n1-standard-96 machine type in the machine types list here https://cloud.google.com/compute/docs/general-purpose-machines"
      },
      {
        "date": "2020-10-12T06:06:00.000Z",
        "voteCount": 12,
        "content": "A. When creating the VM, use machine type n1-standard-96."
      },
      {
        "date": "2024-09-23T11:34:00.000Z",
        "voteCount": 1,
        "content": "why B ?\n Option A is the most straightforward and reliable way to ensure your application runs in a similar environment with the required 96 vCPUs"
      },
      {
        "date": "2023-11-06T04:29:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is A"
      },
      {
        "date": "2023-09-29T18:55:00.000Z",
        "voteCount": 1,
        "content": "A is the correct answer"
      },
      {
        "date": "2023-09-02T03:26:00.000Z",
        "voteCount": 3,
        "content": "A is correct, use machine type n1-standard-96 while creating the VM"
      },
      {
        "date": "2023-04-12T22:03:00.000Z",
        "voteCount": 1,
        "content": "the goal is to have an equivalent of this app in GCP. therefore A is the best shot we have"
      },
      {
        "date": "2023-02-22T13:00:00.000Z",
        "voteCount": 8,
        "content": "A. When creating the VM, use machine type n1-standard-96.\n\nAnswer A is the correct answer as it directly addresses the requirement to have 96 vCPUs by selecting the n1-standard-96 machine type. This machine type offers 96 vCPUs, 360 GB of memory, and up to 2,400 GB of local SSD storage.\n\nhttps://cloud.google.com/compute/docs/machine-resource\n\nAnswer B is incorrect because selecting a CPU platform alone will not guarantee the availability of the required number of vCPUs.\n\nAnswer C is incorrect because it is not possible to modify a running Compute Engine instance to add vCPUs. vCPUs can only be added or removed during instance creation or by stopping the instance first.\n\nAnswer D is incorrect because while Rightsizing Recommendations can help optimize compute resources, they will not guarantee that the application has the required 96 vCPUs to function properly."
      },
      {
        "date": "2022-12-08T00:35:00.000Z",
        "voteCount": 2,
        "content": "A is correct - https://cloud.google.com/compute/docs/machine-types"
      },
      {
        "date": "2022-11-29T10:52:00.000Z",
        "voteCount": 3,
        "content": "the instance name for 96 vcpu N1 is \"n1-highcpu-96\", not n1-standard-96.\n\nPossible that has been updated since this question came out?"
      },
      {
        "date": "2022-11-12T02:33:00.000Z",
        "voteCount": 1,
        "content": "keyword:  n1-standard-96"
      },
      {
        "date": "2022-10-05T14:13:00.000Z",
        "voteCount": 1,
        "content": "A. is correct,\nN1\n* CPU types --&gt; Skylake, Broadwell, Haswell, Sandy Bridge, and Ivy Bridge\n* Architecture --&gt; x86\n* vCPUs\t1 to 96"
      },
      {
        "date": "2022-07-16T23:18:00.000Z",
        "voteCount": 1,
        "content": "Go with A, It is the Correct one ."
      },
      {
        "date": "2022-07-14T14:00:00.000Z",
        "voteCount": 1,
        "content": "SKYLAKE IS 96VCPU. B is correct."
      },
      {
        "date": "2022-08-23T07:59:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/compute/docs/machine-types\nN1 machine series have up to 96 vCPUs, 6.5 GB of memory per vCPU, and are available on Intel Sandy Bridge, Ivy Bridge, Haswell, Broadwell, and Skylake CPU platforms."
      },
      {
        "date": "2022-07-02T11:41:00.000Z",
        "voteCount": 1,
        "content": "Go with A"
      },
      {
        "date": "2022-06-23T11:52:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/compute/docs/general-purpose-machines  - A is correct"
      },
      {
        "date": "2022-06-04T16:42:00.000Z",
        "voteCount": 2,
        "content": "Go for A\nhttps://cloud.google.com/compute/all-pricing?hl=es"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 90,
    "url": "https://www.examtopics.com/discussions/google/view/22961-exam-associate-cloud-engineer-topic-1-question-90-discussion/",
    "body": "You want to configure a solution for archiving data in a Cloud Storage bucket. The solution must be cost-effective. Data with multiple versions should be archived after 30 days. Previous versions are accessed once a month for reporting. This archive data is also occasionally updated at month-end. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a bucket lifecycle rule that archives data with newer versions after 30 days to Coldline Storage.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a bucket lifecycle rule that archives data with newer versions after 30 days to Nearline Storage.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a bucket lifecycle rule that archives data from regional storage after 30 days to Coldline Storage.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a bucket lifecycle rule that archives data from regional storage after 30 days to Nearline Storage."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 21,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-06-12T05:25:00.000Z",
        "voteCount": 33,
        "content": "B is correct"
      },
      {
        "date": "2020-08-23T19:46:00.000Z",
        "voteCount": 24,
        "content": "Correct Answer (B):\n\nNumberOfNewerVersions\nThe NumberOfNewerVersions condition is typically only used in conjunction with Object Versioning. If the value of this condition is set to N, an object version satisfies the condition when there are at least N versions (including the live version) newer than it. For a live object version, the number of newer versions is considered to be 0. For the most recent noncurrent version, the number of newer versions is 1 (or 0 if there is no live object version), and so on.\n\nImportant: When specifying this condition in a .json configuration file, you must use numNewerVersions instead of NumberOfNewerVersions.\n\nhttps://cloud.google.com/storage/docs/lifecycle#numberofnewerversions"
      },
      {
        "date": "2023-11-06T04:33:00.000Z",
        "voteCount": 2,
        "content": "The correct answer is B"
      },
      {
        "date": "2023-09-02T03:31:00.000Z",
        "voteCount": 4,
        "content": "B is the right answer, because of data is accessing infrequently and nearline storage is good for it"
      },
      {
        "date": "2023-08-28T16:50:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2023-07-20T22:05:00.000Z",
        "voteCount": 1,
        "content": "Correct ans is A.\n\nExplanation: In this scenario, you need to archive data after 30 days, which implies that the data with multiple versions is considered for archiving. Since you need to access previous versions once a month for reporting, using Coldline Storage is the most cost-effective option."
      },
      {
        "date": "2024-02-13T10:51:00.000Z",
        "voteCount": 1,
        "content": "Retrieval fees is more expensive in Coldline.\n\nStandard storage\tNearline storage\tColdline storage\tArchive storage\n$0 per GB\t        $0.01 per GB\t       $0.02 per GB\t       $0.05 per GB"
      },
      {
        "date": "2023-03-23T05:53:00.000Z",
        "voteCount": 3,
        "content": "since accessed frequently it will be nearline"
      },
      {
        "date": "2023-11-27T16:51:00.000Z",
        "voteCount": 1,
        "content": "the logic is simple :) i agree with you"
      },
      {
        "date": "2023-02-22T13:44:00.000Z",
        "voteCount": 10,
        "content": "Answer B, adding a bucket lifecycle rule that archives data with newer versions after 30 days to Nearline Storage, is the correct answer for this scenario.\n\nNearline Storage is designed for data that is accessed less frequently, such as for backup and archival purposes. It has a minimum storage duration of 30 days, which makes it suitable for archiving data that needs to be kept for a long time but is accessed infrequently. Additionally, Nearline Storage has lower storage costs than Coldline Storage, making it more cost-effective for this use case.\n\nBy adding a bucket lifecycle rule that archives data with newer versions after 30 days to Nearline Storage, you can ensure that the data is automatically moved to a more cost-effective storage class while still being easily accessible for reporting purposes."
      },
      {
        "date": "2023-03-23T10:26:00.000Z",
        "voteCount": 4,
        "content": "You're incorrect. Coldline storage has a lower costs that Nearline Storage. https://cloud.google.com/storage/docs/storage-classes."
      },
      {
        "date": "2023-03-25T16:48:00.000Z",
        "voteCount": 1,
        "content": "The question tells us that the previous versions are accessed once a month for reporting. So, nearline makes more sense in this case. 'Buruguduystunstugudunstuy' has mentioned that nearline has lower storage costs for only this 'use case'"
      },
      {
        "date": "2023-04-17T07:17:00.000Z",
        "voteCount": 5,
        "content": "just FYI that my lord, @Buruguduystunstugudunstuy, is always right!!!!1"
      },
      {
        "date": "2022-11-12T02:34:00.000Z",
        "voteCount": 1,
        "content": "archives data with newer versions after 30 days to Nearline Storage."
      },
      {
        "date": "2022-09-19T01:45:00.000Z",
        "voteCount": 1,
        "content": "B should be correct:\n\nNearline has min storage of 30 days, while Coldline has 90 days.\n\nSince \"archive data is also occasionally updated at month-end\", updating object before min storage period is allowed but causes early deletion fees as if the object was stored for the min duration, so using Coldline will always charge for 90 days and not likely to save cost.\n\nhttps://cloud.google.com/storage/pricing#early-delete"
      },
      {
        "date": "2022-06-23T11:54:00.000Z",
        "voteCount": 1,
        "content": "B is right and straight forward."
      },
      {
        "date": "2022-06-23T11:54:00.000Z",
        "voteCount": 1,
        "content": "regional storage after 30 days to Nearline Storage option is trick you :)"
      },
      {
        "date": "2022-06-04T16:45:00.000Z",
        "voteCount": 1,
        "content": "Go for B"
      },
      {
        "date": "2022-03-17T01:55:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2021-12-06T06:48:00.000Z",
        "voteCount": 2,
        "content": "B is perfect"
      },
      {
        "date": "2021-11-20T05:35:00.000Z",
        "voteCount": 1,
        "content": "B. Add a bucket lifecycle rule that archives data with newer versions after 30 days to Nearline Storage."
      },
      {
        "date": "2021-11-19T08:20:00.000Z",
        "voteCount": 1,
        "content": "B. Add a bucket lifecycle rule that archives data with newer versions after 30 days to Nearline Storage."
      },
      {
        "date": "2021-08-28T05:18:00.000Z",
        "voteCount": 2,
        "content": "Agree B is the correct"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 91,
    "url": "https://www.examtopics.com/discussions/google/view/22127-exam-associate-cloud-engineer-topic-1-question-91-discussion/",
    "body": "Your company's infrastructure is on-premises, but all machines are running at maximum capacity. You want to burst to Google Cloud. The workloads on Google<br>Cloud must be able to directly communicate to the workloads on-premises using a private IP range. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn Google Cloud, configure the VPC as a host for Shared VPC.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn Google Cloud, configure the VPC for VPC Network Peering.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate bastion hosts both in your on-premises environment and on Google Cloud. Configure both as proxy servers using their public IP addresses.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up Cloud VPN between the infrastructure on-premises and Google Cloud.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 23,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-06-04T15:40:00.000Z",
        "voteCount": 57,
        "content": "I believe D is the right answer"
      },
      {
        "date": "2020-06-06T12:52:00.000Z",
        "voteCount": 2,
        "content": "B is correct - https://cloud.google.com/solutions/best-practices-vpc-design . this answer also on all machines are running at maximum capacity."
      },
      {
        "date": "2020-06-07T16:09:00.000Z",
        "voteCount": 27,
        "content": "vpc network peering does not connect to on-prem. Cloud VPN is the correct solution. https://cloud.google.com/vpn/docs/concepts/overview"
      },
      {
        "date": "2020-06-09T10:36:00.000Z",
        "voteCount": 15,
        "content": "You need VPN, so D is the correct. VPC network peering is between VPCs."
      },
      {
        "date": "2022-07-31T23:56:00.000Z",
        "voteCount": 7,
        "content": "\"Google Cloud VPC Network Peering allows internal IP address connectivity across two Virtual Private Cloud (VPC) networks regardless of whether they belong to the same project or the same organization.\" \nhttps://cloud.google.com/vpc/docs/vpc-peering\n\nwhile \n\"Cloud Interconnect provides low latency, high availability connections that enable you to reliably transfer data between your on-premises and Google Cloud Virtual Private Cloud (VPC) networks.\"\nhttps://cloud.google.com/network-connectivity/docs/interconnect/concepts/overview\n\nand\n\"HA VPN is a high-availability (HA) Cloud VPN solution that lets you securely connect your on-premises network to your VPC network through an IPsec VPN connection in a single region.\"\nhttps://cloud.google.com/network-connectivity/docs/vpn/concepts/overview\n\nso, cloud vpn is the best answer for the question requirement"
      },
      {
        "date": "2020-08-17T19:05:00.000Z",
        "voteCount": 35,
        "content": "Correct Answer is (D):\n\nAccess internal IPs directly\nYour VPC network's internal (RFC 1918) IP addresses are directly accessible from your on-premises network with peering, no NAT device or VPN tunnel required.\n\nHybrid made easy\nToday\u2019s business climate demands flexibility. Connecting your on-premises resources to your cloud resources seamlessly, with minimum latency or interruption, is a business-critical requirement. The speed and reliability of Cloud Interconnect lets you extend your organization\u2019s data center network into Google Cloud, simply and easily, while options such as Cloud VPN provide flexibility for all your workloads. This unlocks the potential of hybrid app development and all the benefits the cloud has to offer.\n\nIn the graphic below: What GCP Connection is right for you? shows clearly what is the method for extend your on premise network (IP Private communication).\nWhat GCP Connection is right for you?\nhttps://cloud.google.com/hybrid-connectivity"
      },
      {
        "date": "2023-11-06T04:38:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is D"
      },
      {
        "date": "2023-09-02T23:23:00.000Z",
        "voteCount": 4,
        "content": "D is the right answer as they need  the private range and the machine are also on high working load"
      },
      {
        "date": "2023-05-28T01:43:00.000Z",
        "voteCount": 1,
        "content": "d is right"
      },
      {
        "date": "2023-04-17T04:00:00.000Z",
        "voteCount": 2,
        "content": "VPN to connect your on-premise network to the cloud"
      },
      {
        "date": "2023-03-23T05:52:00.000Z",
        "voteCount": 2,
        "content": "VPN for on premise connection to GCP"
      },
      {
        "date": "2023-02-22T14:12:00.000Z",
        "voteCount": 8,
        "content": "Answer D. Set up Cloud VPN between the infrastructure on-premises and Google Cloud.\n\nTo burst into Google Cloud from the on-premises infrastructure, a VPN connection can be established between the on-premises network and Google Cloud. VPN provides a secure, private tunnel to transfer data between on-premises infrastructure and Google Cloud. Cloud VPN would allow workloads on Google Cloud to communicate with workloads on-premises over private IP addresses, making it a suitable option for this scenario. \n\nAnswer A (Shared VPC) and Answer B (VPC Network Peering) do not address the requirement of communicating over a private IP range between on-premises and Google Cloud. \n\nAnswer C (bastion hosts) involves the use of public IP addresses, which may not be suitable for a private, secure connection."
      },
      {
        "date": "2022-12-08T00:53:00.000Z",
        "voteCount": 1,
        "content": "D is the right answer"
      },
      {
        "date": "2022-11-12T02:38:00.000Z",
        "voteCount": 1,
        "content": "Cloud VPN"
      },
      {
        "date": "2022-09-26T18:14:00.000Z",
        "voteCount": 1,
        "content": "It is the answer"
      },
      {
        "date": "2022-07-02T11:43:00.000Z",
        "voteCount": 1,
        "content": "Go with D"
      },
      {
        "date": "2022-06-23T12:04:00.000Z",
        "voteCount": 1,
        "content": "D is right answer"
      },
      {
        "date": "2022-06-23T11:56:00.000Z",
        "voteCount": 2,
        "content": "Cloud VPN is way to establish connection between on prem to cloud . D is correct."
      },
      {
        "date": "2022-01-13T02:00:00.000Z",
        "voteCount": 2,
        "content": "D is the right answer."
      },
      {
        "date": "2021-12-18T07:14:00.000Z",
        "voteCount": 3,
        "content": "On-premise -&gt; GCP \nThere are 2 ways\n1. Cloud VPN\n2. Interconnect\nSince we have VPN as an option, others is not recommended"
      },
      {
        "date": "2021-12-06T06:48:00.000Z",
        "voteCount": 1,
        "content": "D is perfect"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 92,
    "url": "https://www.examtopics.com/discussions/google/view/22128-exam-associate-cloud-engineer-topic-1-question-92-discussion/",
    "body": "You want to select and configure a solution for storing and archiving data on Google Cloud Platform. You need to support compliance objectives for data from one geographic location. This data is archived after 30 days and needs to be accessed annually. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSelect Multi-Regional Storage. Add a bucket lifecycle rule that archives data after 30 days to Coldline Storage.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSelect Multi-Regional Storage. Add a bucket lifecycle rule that archives data after 30 days to Nearline Storage.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSelect Regional Storage. Add a bucket lifecycle rule that archives data after 30 days to Nearline Storage.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSelect Regional Storage. Add a bucket lifecycle rule that archives data after 30 days to Coldline Storage.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 13,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-06-04T15:45:00.000Z",
        "voteCount": 46,
        "content": "D\nGoogle Cloud Coldline is a new cold-tier storage for archival data with access frequency of less than once per year. Unlike other cold storage options, Nearline has no delays prior to data access, so now it is the leading solution among competitors."
      },
      {
        "date": "2020-06-07T16:24:00.000Z",
        "voteCount": 9,
        "content": "D is correct.  Coldline is a better choice."
      },
      {
        "date": "2020-06-06T12:56:00.000Z",
        "voteCount": 12,
        "content": "C is correct - This data is archived after 30 days - Nearline Storage 30 days , Coldline Storage 90 days https://cloud.google.com/storage/docs/storage-classes"
      },
      {
        "date": "2020-06-09T10:33:00.000Z",
        "voteCount": 1,
        "content": "dan80 is right"
      },
      {
        "date": "2020-09-11T09:00:00.000Z",
        "voteCount": 2,
        "content": "30 day is min retention policy any access before this is chargeable. Here  they are saying the archival data is accessed after 1 yr so cold line. The 30 days statement is to archive data after 30 days."
      },
      {
        "date": "2020-10-15T13:02:00.000Z",
        "voteCount": 5,
        "content": "D, It is saying AFTER 30 days. We should use coldline storage"
      },
      {
        "date": "2020-08-17T19:33:00.000Z",
        "voteCount": 16,
        "content": "Correct Answer is (D):\n\nhttps://cloud.google.com/storage/docs/storage-classes\n\nNearline Storage\nNearline Storage is a low-cost, highly durable storage service for storing infrequently accessed data. Nearline Storage is a better choice than Standard Storage in scenarios where slightly lower availability, a 30-day minimum storage duration, and costs for data access are acceptable trade-offs for lowered at-rest storage costs.\n\nNearline Storage is ideal for data you plan to read or modify on average once per month or less. For example, if you want to continuously add files to Cloud Storage and plan to access those files once a month for analysis, Nearline Storage is a great choice.\n\nNearline Storage is also appropriate for data backup, long-tail multimedia content, and data archiving. Note, however, that for data accessed less frequently than once a quarter, Coldline Storage or Archive Storage are more cost-effective, as they offer lower storage costs.\nhttps://cloud.google.com/storage/docs/storage-classes#nearline"
      },
      {
        "date": "2020-08-23T20:02:00.000Z",
        "voteCount": 13,
        "content": "CORRECTION.\nCorrect Answer is (D):\n\nThe Real description is about Coldline storage Class:\n\nColdline Storage\nColdline Storage is a very-low-cost, highly durable storage service for storing infrequently accessed data. Coldline Storage is a better choice than Standard Storage or Nearline Storage in scenarios where slightly lower availability, a 90-day minimum storage duration, and higher costs for data access are acceptable trade-offs for lowered at-rest storage costs.\n\nColdline Storage is ideal for data you plan to read or modify at most once a quarter. Note, however, that for data being kept entirely for backup or archiving purposes, Archive Storage is more cost-effective, as it offers the lowest storage costs.\n\nhttps://cloud.google.com/storage/docs/storage-classes#coldline"
      },
      {
        "date": "2023-11-27T16:54:00.000Z",
        "voteCount": 2,
        "content": "D is correct.\n\"from one geographic location\" clues the answer"
      },
      {
        "date": "2023-11-06T04:43:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is D"
      },
      {
        "date": "2023-09-02T23:25:00.000Z",
        "voteCount": 2,
        "content": "D is the correrct answer, as the data in access only once a year"
      },
      {
        "date": "2023-04-17T04:02:00.000Z",
        "voteCount": 1,
        "content": "\"This data is archived after 30 days and needs to be accessed annually\"\nideally archive; coldine is the closest."
      },
      {
        "date": "2023-04-05T21:26:00.000Z",
        "voteCount": 1,
        "content": "The best option would be to select Regional Storage and add a bucket lifecycle rule that archives data after 30 days to Nearline Storage. Nearline Storage is designed for data that is accessed less frequently, but still needs to be readily available when accessed. It has a lower storage cost than Regional Storage, and retrieval costs are lower than those of Coldline Storage."
      },
      {
        "date": "2023-04-03T10:26:00.000Z",
        "voteCount": 2,
        "content": "D is correct"
      },
      {
        "date": "2023-02-22T14:26:00.000Z",
        "voteCount": 4,
        "content": "Answer D is the CORRECT answer. The scenario mentioned in the question requires archiving data after 30 days and accessing it annually. As per the Cloud Storage documentation, Coldline storage is ideal for data that is accessed at most once a quarter. Hence, selecting regional storage and adding a bucket lifecycle rule that archives data after 30 days to Coldline Storage is the best solution to meet the compliance objectives and cost-effectiveness requirements."
      },
      {
        "date": "2023-02-22T14:26:00.000Z",
        "voteCount": 1,
        "content": "INCORRECT:\n\nAnswer A, selecting Multi-Regional Storage and adding a bucket lifecycle rule that archives data after 30 days to Coldline Storage, is not a good fit for this scenario because Multi-Regional Storage is more expensive than Regional Storage and it does not provide a clear advantage for this use case.\n\nAnswer B, selecting Multi-Regional Storage and adding a bucket lifecycle rule that archives data after 30 days to Nearline Storage, is also not the best option because Nearline Storage is more appropriate for data that is accessed less than once a month, while in this scenario, the data needs to be accessed at least once a year.\n\nAnswer C, selecting Regional Storage and adding a bucket lifecycle rule that archives data after 30 days to Nearline Storage, is not ideal because Nearline Storage is more suitable for data that is accessed less than once a month. If the data is accessed only once a year, it might be more cost-effective to choose Coldline Storage instead."
      },
      {
        "date": "2022-12-08T00:56:00.000Z",
        "voteCount": 1,
        "content": "The answer is D."
      },
      {
        "date": "2022-11-12T02:40:00.000Z",
        "voteCount": 1,
        "content": "Regional Storage, Coldline Storage."
      },
      {
        "date": "2022-10-17T06:13:00.000Z",
        "voteCount": 1,
        "content": "As in question it is asking for \"one geographic location\" .So multi region options A &amp; B is eliminated. And Between C &amp; D  \"D is correct\" as data will be accessed once a year."
      },
      {
        "date": "2022-09-26T20:03:00.000Z",
        "voteCount": 1,
        "content": "its C because archive data is 30 days and Nearline storage support that. https://cloud.google.com/storage/docs/storage-classes"
      },
      {
        "date": "2022-09-24T10:34:00.000Z",
        "voteCount": 1,
        "content": "had this question today"
      },
      {
        "date": "2022-06-23T12:08:00.000Z",
        "voteCount": 1,
        "content": "D is for sure."
      },
      {
        "date": "2022-06-04T18:35:00.000Z",
        "voteCount": 1,
        "content": "Go for D\n\u201c\u2026 and needs to be accessed annually\u201d\nCold line is the better choice."
      },
      {
        "date": "2022-03-17T02:03:00.000Z",
        "voteCount": 1,
        "content": "D is the right answer."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 93,
    "url": "https://www.examtopics.com/discussions/google/view/22731-exam-associate-cloud-engineer-topic-1-question-93-discussion/",
    "body": "Your company uses BigQuery for data warehousing. Over time, many different business units in your company have created 1000+ datasets across hundreds of projects. Your CIO wants you to examine all datasets to find tables that contain an employee_ssn column. You want to minimize effort in performing this task.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGo to Data Catalog and search for employee_ssn in the search box.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite a shell script that uses the bq command line tool to loop through all the projects in your organization.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite a script that loops through all the projects in your organization and runs a query on INFORMATION_SCHEMA.COLUMNS view to find the employee_ssn column.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite a Cloud Dataflow job that loops through all the projects in your organization and runs a query on INFORMATION_SCHEMA.COLUMNS view to find employee_ssn column."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 38,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-06-10T04:59:00.000Z",
        "voteCount": 40,
        "content": "Its A."
      },
      {
        "date": "2020-07-23T02:42:00.000Z",
        "voteCount": 32,
        "content": "Correct is A.  \nI tested on my account following this procedure: https://cloud.google.com/bigquery/docs/quickstarts/quickstart-web-ui?authuser=4\nI created a data set and through Data Catalog I easily and effortlessly searched for the column name \"gender\""
      },
      {
        "date": "2024-01-10T12:42:00.000Z",
        "voteCount": 3,
        "content": "The best option to minimize effort in performing this task is A. Go to Data Catalog and search for employee_ssn in the search box.\n\nGoogle Cloud\u2019s Data Catalog is a fully managed and scalable metadata management service that empowers organizations to quickly discover, manage, and understand all their data in Google Cloud. It offers a simple and easy-to-use search interface for data discovery. By searching for \u201cemployee_ssn\u201d in the Data Catalog, you can quickly find all tables across all datasets and projects that contain this column. This approach is more efficient and requires less effort compared to writing and maintaining scripts or jobs.\n\nPlease note that access to Data Catalog and the visibility of datasets, tables, and columns are subject to permissions and roles in IAM policy. Make sure you have the necessary permissions to view the metadata."
      },
      {
        "date": "2023-12-05T07:50:00.000Z",
        "voteCount": 4,
        "content": "Data Catalog lets you search and tag entries such as BigQuery tables with metadata. Some examples of metadata that you can use for tagging include public and private tags, data stewards, and rich text overview.\n\nhttps://cloud.google.com/data-catalog/docs/tag-bigquery-dataset"
      },
      {
        "date": "2023-11-09T03:34:00.000Z",
        "voteCount": 2,
        "content": "I am preparing for the GCP-ACE exam, I was able to access 96 questions only, if anyone has the entire questions please share them with my vittoriaprovenza@tiscali.it address. I have exam on next week,pls share Thanks in advance. I would be forever grateful."
      },
      {
        "date": "2023-11-06T05:36:00.000Z",
        "voteCount": 1,
        "content": "I will go for A"
      },
      {
        "date": "2023-11-06T05:31:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is A"
      },
      {
        "date": "2023-09-02T23:28:00.000Z",
        "voteCount": 1,
        "content": "Answer is A, as it requires the less effort and other options are more time consuming and error prone"
      },
      {
        "date": "2023-05-07T17:09:00.000Z",
        "voteCount": 1,
        "content": "The most efficient approach to identify tables that contain an employee_ssn column in BigQuery would be to query the INFORMATION_SCHEMA.COLUMNS view, which provides metadata about all columns in all tables in a given dataset. Therefore, options C and D are both possible solutions.\n\nOption A, searching for the column name in Data Catalog, may not be efficient if there are too many datasets to search through manually.\n\nOption B, writing a shell script to loop through all the projects in your organization, may work, but it would require more effort and time than options C and D. Also, it would be more error-prone since the script would need to handle authentication and authorization, handle exceptions and errors, and collect the results.\n\nTherefore, options C and D are better choices, but option D, using Cloud Dataflow, might be overkill for this specific task. Option C, looping through all projects and querying INFORMATION_SCHEMA.COLUMNS view, is the simplest and most effective solution to minimize effort."
      },
      {
        "date": "2023-04-22T08:44:00.000Z",
        "voteCount": 1,
        "content": "Hi All, I have my GCP ACE exam scheduled for tomorrow. However, I am only being able to access 96 questions. Can anyone kindly share the entire list of questions as I have hardly anytime left before my exam. oniyi6@yahoo.com. Thank you all so much"
      },
      {
        "date": "2023-04-30T04:51:00.000Z",
        "voteCount": 1,
        "content": "Did you pass the exam? If so any questions from here came? Please let us know so that it will be helpful"
      },
      {
        "date": "2023-02-28T07:34:00.000Z",
        "voteCount": 1,
        "content": "C is IMO the correct answer (https://stackoverflow.com/questions/68746567/big-query-find-all-column-name-containing-surname-across-all-tables)"
      },
      {
        "date": "2023-02-22T14:53:00.000Z",
        "voteCount": 8,
        "content": "Answer A is the correct answer. Go to Data Catalog and search for employee_ssn in the search box.\n\nData Catalog is a fully managed and scalable metadata management service that allows you to discover, understand, and manage your data. It provides search functionality that allows you to search for datasets, tables, columns, and other metadata across your organization. Therefore, you can simply go to Data Catalog and search for \"employee_ssn\" in the search box to find all datasets that contain this column. This is the most efficient and straightforward solution to the problem.\n\nAnswers B, C, and D are not ideal solutions. \n\nAnswer B requires writing a shell script and using the bq command line tool to loop through all the projects, which is time-consuming and error-prone. \n\nAnswer C requires writing a script that loops through all the projects and runs a query on INFORMATION_SCHEMA.COLUMNS view, which is also time-consuming and error-prone. \n\nAnswer D involves writing a Cloud Dataflow job, which is unnecessary and OVERKILL for this simple task."
      },
      {
        "date": "2023-01-29T06:41:00.000Z",
        "voteCount": 1,
        "content": "The answer is A"
      },
      {
        "date": "2023-01-19T13:16:00.000Z",
        "voteCount": 2,
        "content": "Hello, I have my GCP ACE exam scheduled early next week. However, I am only being able to access 96 questions. Can anyone kindly share the entire list of questions as I have hardly anytime left before my exam."
      },
      {
        "date": "2022-11-12T03:03:00.000Z",
        "voteCount": 1,
        "content": "Data Catalog"
      },
      {
        "date": "2022-10-06T04:41:00.000Z",
        "voteCount": 4,
        "content": "A is the correct answer, Data Catalog can be used to search the column with keyword:value pair,\n\nFilter your search by adding a keyword:value to your search terms in the search box:\n\nKeyword\tDescription\nname:\tMatch data asset name\n***column:\tMatch column name or nested column name\ndescription:\tMatch table description"
      },
      {
        "date": "2022-09-18T04:04:00.000Z",
        "voteCount": 1,
        "content": "ITS A COLUMN WE ARE SEARCHING FOR AND I DONT SEE DATA CATALOGUE CAN SEARCH FOR COLUMN"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 94,
    "url": "https://www.examtopics.com/discussions/google/view/22847-exam-associate-cloud-engineer-topic-1-question-94-discussion/",
    "body": "You create a Deployment with 2 replicas in a Google Kubernetes Engine cluster that has a single preemptible node pool. After a few minutes, you use kubectl to examine the status of your Pod and observe that one of them is still in Pending status:<br><img src=\"/assets/media/exam-media/04338/0004900001.jpg\" class=\"in-exam-image\"><br>What is the most likely cause?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe pending Pod's resource requests are too large to fit on a single node of the cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tToo many Pods are already running in the cluster, and there are not enough resources left to schedule the pending Pod.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe node pool is configured with a service account that does not have permission to pull the container image used by the pending Pod.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe pending Pod was originally scheduled on a node that has been preempted between the creation of the Deployment and your verification of the Pods' status. It is currently being rescheduled on a new node.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 31,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 30,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-08-17T19:54:00.000Z",
        "voteCount": 55,
        "content": "Correct Answer is  (B):\n\nReasons for a Pod Status Pending:\nTroubleshooting Reason #1: Not enough CPU\nTroubleshooting Reason #2: Not enough memory\nTroubleshooting Reason #3: Not enough CPU and memory\nhttps://managedkube.com/kubernetes/k8sbot/troubleshooting/pending/pod/2019/02/22/pending-pod.html"
      },
      {
        "date": "2020-08-20T01:16:00.000Z",
        "voteCount": 5,
        "content": "I agree with you. The correct answer is B"
      },
      {
        "date": "2020-09-20T10:05:00.000Z",
        "voteCount": 3,
        "content": "D gives you the reason why the resource could not be available a it was preempted"
      },
      {
        "date": "2021-04-02T10:47:00.000Z",
        "voteCount": 3,
        "content": "If it was preempted, then it has to be restarted right? then it will show its failing not pending, check the articles mentioned by ESP_SAP"
      },
      {
        "date": "2023-03-09T04:29:00.000Z",
        "voteCount": 1,
        "content": "No, it will show as pending initially while realocating"
      },
      {
        "date": "2021-06-01T21:08:00.000Z",
        "voteCount": 2,
        "content": "Its in a deployment, the pod will be recreated. There is insufficient resources in the node, not because its preemptible but because there is no memory/cpu......"
      },
      {
        "date": "2021-04-25T07:02:00.000Z",
        "voteCount": 28,
        "content": "The real crux of this question is the mention about \"Pre-emptible Node pool\". That need to take into consider while determining the answer. If we choose B, then the importance of \"Pre-emptible node pool\" is not there. Whether the node pool is pre-emptible or not, resource scarcity can lead to pending pods.\n\nWhen we consider the mention of \"Pre-emptible Node Poll\" , then the answer is obviously D. if a pre-meptible Node get pre-empted there will be a delay in cluster to sync it.\n\nAnswer is D."
      },
      {
        "date": "2024-09-05T12:25:00.000Z",
        "voteCount": 1,
        "content": "they just wanted to confuse us and added \"preemptible\", be strong"
      },
      {
        "date": "2021-09-24T04:40:00.000Z",
        "voteCount": 4,
        "content": "Questions says \"Single Node\" at that case the second pod can't be in running state."
      },
      {
        "date": "2022-02-10T19:54:00.000Z",
        "voteCount": 2,
        "content": "A node can have multiple pods. So that is not a problem."
      },
      {
        "date": "2022-01-20T22:55:00.000Z",
        "voteCount": 9,
        "content": "It says  a single node pool, not a single node. Meaning there can be multiple nodes, right?"
      },
      {
        "date": "2022-02-10T20:04:00.000Z",
        "voteCount": 9,
        "content": "Pre-emptible would have been an issue if the cluster had more than one node. The question clearly states that it is a single node cluster. That means if that single VM was pre-empted, neither of the pods should have been running. Since one pod is running, that means that (the only) VM is running. So, the reason the second pod is still pending because the VM is not having enough resources to run both the pods. Hence B."
      },
      {
        "date": "2022-06-08T01:41:00.000Z",
        "voteCount": 3,
        "content": "Actually the question stated \"single preemptible node pool\" and not \"single node\" so it's possible that there are multiple nodes and one of the node on which the pod was scheduled on was preempted"
      },
      {
        "date": "2020-06-25T12:28:00.000Z",
        "voteCount": 21,
        "content": "D is correct as the node on which pod was scheduled to run was preempted &amp; now this pod is scheduled to run on different preemtible node from the node-pool"
      },
      {
        "date": "2021-09-23T06:08:00.000Z",
        "voteCount": 6,
        "content": "Incorrect. There is a single preemtible instance, if it was preempted then both pods would show as 'Pending'. B is correct."
      },
      {
        "date": "2023-03-09T04:30:00.000Z",
        "voteCount": 2,
        "content": "No, because one of the pods may run on another node that its still up"
      },
      {
        "date": "2022-03-17T03:15:00.000Z",
        "voteCount": 11,
        "content": "&gt; There is a single preemtible instance\n\nWhere does it say that?  It doesn't.  Don't make things up. There's a single pre-emptible node pool.  A single pool is not the same as a single node."
      },
      {
        "date": "2024-09-27T02:27:00.000Z",
        "voteCount": 1,
        "content": "The most likely cause is given in the question. We have a single preemptible node pool."
      },
      {
        "date": "2024-09-05T12:27:00.000Z",
        "voteCount": 1,
        "content": "100% - not enough CPU / memory."
      },
      {
        "date": "2024-08-29T09:55:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is D."
      },
      {
        "date": "2024-07-14T05:36:00.000Z",
        "voteCount": 1,
        "content": "I think so because main word of the question is preemptible pool nodes it can initialize some latency during creating node to find compliance machine."
      },
      {
        "date": "2024-02-28T19:27:00.000Z",
        "voteCount": 1,
        "content": "B is more correct. Maybe Troubleshooting Reason Not enough CPU or Memory or both of them."
      },
      {
        "date": "2024-02-28T11:09:00.000Z",
        "voteCount": 1,
        "content": "I would go for B"
      },
      {
        "date": "2024-02-21T02:33:00.000Z",
        "voteCount": 1,
        "content": "D good choice"
      },
      {
        "date": "2024-02-06T15:49:00.000Z",
        "voteCount": 1,
        "content": "The term 'preemptible node pool' is in the question. D is the answer."
      },
      {
        "date": "2023-12-30T19:26:00.000Z",
        "voteCount": 1,
        "content": "I would go with B.\nWhile D is possible, this scenario is less likely compared to the resource constraint issue, especially if the Pending status is observed consistently over a few minutes. Preemption would usually lead to a quicker rescheduling unless there are resource constraints."
      },
      {
        "date": "2023-12-28T07:53:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2023-11-06T05:37:00.000Z",
        "voteCount": 1,
        "content": "I go with B"
      },
      {
        "date": "2023-11-01T17:37:00.000Z",
        "voteCount": 1,
        "content": "answer is B"
      },
      {
        "date": "2023-11-01T05:21:00.000Z",
        "voteCount": 2,
        "content": "When we consider the mention of \"Pre-emptible Node Poll\" , then the answer is obviously D."
      },
      {
        "date": "2023-10-10T10:21:00.000Z",
        "voteCount": 1,
        "content": "I will pick option D"
      },
      {
        "date": "2023-09-02T23:30:00.000Z",
        "voteCount": 2,
        "content": "B , seems more correct as it dont have enough resources"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 95,
    "url": "https://www.examtopics.com/discussions/google/view/22385-exam-associate-cloud-engineer-topic-1-question-95-discussion/",
    "body": "You want to find out when users were added to Cloud Spanner Identity Access Management (IAM) roles on your Google Cloud Platform (GCP) project. What should you do in the GCP Console?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOpen the Cloud Spanner console to review configurations.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOpen the IAM &amp; admin console to review IAM policies for Cloud Spanner roles.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGo to the Stackdriver Monitoring console and review information for Cloud Spanner.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGo to the Stackdriver Logging console, review admin activity logs, and filter them for Cloud Spanner IAM roles.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 26,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-07-23T13:24:00.000Z",
        "voteCount": 84,
        "content": "Answer = D, I have simple rule; if metrics then Monitoring, if Auditing then Logging."
      },
      {
        "date": "2020-06-06T16:08:00.000Z",
        "voteCount": 44,
        "content": "I think the answer is D"
      },
      {
        "date": "2020-11-09T07:28:00.000Z",
        "voteCount": 3,
        "content": "As per the Cloud Audit logs documentation."
      },
      {
        "date": "2024-05-27T08:06:00.000Z",
        "voteCount": 1,
        "content": "D, but in 2024 there is not stackdriver Logging..."
      },
      {
        "date": "2023-11-09T03:38:00.000Z",
        "voteCount": 2,
        "content": "I am preparing for the GCP-ACE exam, I was able to access 96 questions only, if anyone has the entire questions please share them with my vittoriaprovenza@tiscali.it address. I have exam on next week,pls share Thanks in advance. I would be forever grateful."
      },
      {
        "date": "2023-11-06T05:54:00.000Z",
        "voteCount": 1,
        "content": "I will go with D"
      },
      {
        "date": "2023-11-01T05:22:00.000Z",
        "voteCount": 1,
        "content": "D is the only option that talks about the admin activity"
      },
      {
        "date": "2023-09-02T23:33:00.000Z",
        "voteCount": 2,
        "content": "D , seems more correct as it shows you the history also"
      },
      {
        "date": "2023-06-03T22:41:00.000Z",
        "voteCount": 1,
        "content": "option D is correct \nrefer:\nhttps://www.exam-answer.com/google/ace/question95"
      },
      {
        "date": "2023-04-19T01:08:00.000Z",
        "voteCount": 1,
        "content": "Answer is D"
      },
      {
        "date": "2023-02-23T16:48:00.000Z",
        "voteCount": 6,
        "content": "Answer A is incorrect because the Cloud Spanner console only shows configurations related to Cloud Spanner instances and databases, but not IAM roles.\n\nAnswer B is partially correct in that the IAM &amp; admin console is where IAM policies can be viewed and edited. However, it does not show a history of when users were added to Cloud Spanner IAM roles.\n\nAnswer C is incorrect because Stackdriver Monitoring is used to monitor the performance of Google Cloud resources and applications, and does not provide information about IAM role changes.\n\nOverall, the best answer is D, as Stackdriver Logging provides a comprehensive history of all administrative activity logs, including when users were added to Cloud Spanner IAM roles."
      },
      {
        "date": "2022-12-16T20:04:00.000Z",
        "voteCount": 5,
        "content": "This was on the exam 12/16/2022"
      },
      {
        "date": "2022-11-12T04:11:00.000Z",
        "voteCount": 1,
        "content": "Stackdriver Logging console &gt; admin activity logs &gt; Cloud Spanner IAM role"
      },
      {
        "date": "2022-10-06T04:54:00.000Z",
        "voteCount": 1,
        "content": "D is correct, Activity logs captures the time when the users were given the IAM roles for Cloud Spanner"
      },
      {
        "date": "2022-10-05T21:01:00.000Z",
        "voteCount": 1,
        "content": "You need to see when not what so D."
      },
      {
        "date": "2022-10-05T01:42:00.000Z",
        "voteCount": 1,
        "content": "ANS- D"
      },
      {
        "date": "2022-09-25T06:35:00.000Z",
        "voteCount": 1,
        "content": "Answer should be D"
      },
      {
        "date": "2022-08-03T02:39:00.000Z",
        "voteCount": 2,
        "content": "bow before my divine intellect"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 96,
    "url": "https://www.examtopics.com/discussions/google/view/24255-exam-associate-cloud-engineer-topic-1-question-96-discussion/",
    "body": "Your company implemented BigQuery as an enterprise data warehouse. Users from multiple business units run queries on this data warehouse. However, you notice that query costs for BigQuery are very high, and you need to control costs. Which two methods should you use? (Choose two.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSplit the users from business units to multiple projects.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tApply a user- or project-level custom query quota for BigQuery data warehouse.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate separate copies of your BigQuery data warehouse for each business unit.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSplit your BigQuery data warehouse into multiple data warehouses for each business unit.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange your BigQuery query model from on-demand to flat rate. Apply the appropriate number of slots to each Project.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "BE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BE",
        "count": 18,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 5,
        "isMostVoted": false
      },
      {
        "answer": "E",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-06-28T04:49:00.000Z",
        "voteCount": 46,
        "content": "I'd say B and E. So either you do B or E to reduce costs."
      },
      {
        "date": "2023-03-29T00:50:00.000Z",
        "voteCount": 1,
        "content": "The question says 'which two methods should you use', implying that you want to use *both*. Using quotas together with the flat-rate pricing doesn\u00b4t make any sense.\nBesides that, E is wrong imo. Why? Because flat-rate pricing is very expensive, you pay a fixed high price for something that likely won\u00b4t be used enough by the average business unit. You need to allocate different 'slots' which is inflexible and complex. It would make more sense to rely on quotas, which means you have an upper limit for costs, but don\u00b4t necessarily pay the maximum."
      },
      {
        "date": "2020-11-02T20:30:00.000Z",
        "voteCount": 25,
        "content": "B &amp; E \nRefer below link - first of all you can define quotas on project or user level and 2nd one is you can change from on demand to flat rate model \n and define the parameters based on your requirement --- \n\nhttps://cloud.google.com/bigquery/docs/custom-quotas\nhttps://cloud.google.com/bigquery/pricing#flat_rate_pricing"
      },
      {
        "date": "2024-01-07T17:33:00.000Z",
        "voteCount": 1,
        "content": "What happens if you only choose one here? Do you get half the points of 0 points?"
      },
      {
        "date": "2023-11-06T05:56:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is BE"
      },
      {
        "date": "2023-09-02T23:38:00.000Z",
        "voteCount": 1,
        "content": "BE seems more legit"
      },
      {
        "date": "2023-08-25T01:28:00.000Z",
        "voteCount": 1,
        "content": "BE is correct Answer"
      },
      {
        "date": "2023-08-21T23:43:00.000Z",
        "voteCount": 2,
        "content": "B, after reading the specs of BigQuery pricing. C and E doesn't meet the cost requirements. E, Google is no longer offering the flat-rate anymore. E, doesn't meet the criteria and B offers the option to set User-level and project-level custom cost controls."
      },
      {
        "date": "2023-08-10T10:02:00.000Z",
        "voteCount": 3,
        "content": "Flat rate is no longer available for Big Query."
      },
      {
        "date": "2023-07-27T22:33:00.000Z",
        "voteCount": 3,
        "content": "BigQuery Flat-Rate Model is no longer sold as of July 5, 2023\n\"Only available to customers with fixed priced flat-rate prior to end of sale\"\nhttps://cloud.google.com/bigquery/pricing"
      },
      {
        "date": "2023-05-21T01:26:00.000Z",
        "voteCount": 2,
        "content": "Anyone who needs the full PDF message me @dive_into_dev Instagram."
      },
      {
        "date": "2023-04-21T07:17:00.000Z",
        "voteCount": 2,
        "content": "Inorder to get the rest of questions you can type\" Google cloud Ace exam question 97\" in Google and get it.till 197 is there I guess"
      },
      {
        "date": "2023-04-25T03:49:00.000Z",
        "voteCount": 2,
        "content": "That's not right. I have got contributor access and I can assure that this is not the way you can get all the questions and answers."
      },
      {
        "date": "2023-03-28T17:11:00.000Z",
        "voteCount": 1,
        "content": "BE BOTH ARE CORRECT"
      },
      {
        "date": "2023-03-28T07:19:00.000Z",
        "voteCount": 1,
        "content": "Hi guys, I am sitting this exam next Monday, Is it possible that anyone can email me the rest of the questions - rizwanfarooq14@hotmail.com I would really appreciate it!!!"
      },
      {
        "date": "2023-02-25T16:10:00.000Z",
        "voteCount": 1,
        "content": "Dear friends, I am wondering if anyone could kindly send the rest of the questions as pdf to my email: \njing.cecilia.liao@outlook.com\nThank you!"
      },
      {
        "date": "2023-02-23T16:57:00.000Z",
        "voteCount": 5,
        "content": "The two methods that should be used to control BigQuery query costs are:\n\nB. Apply a user- or project-level custom query quota for BigQuery data warehouse: Setting a custom query quota will limit the amount of data a user or project can query within a given period, which helps in controlling costs.\n\nE. Change your BigQuery query model from on-demand to flat rate. Apply the appropriate number of slots to each Project: With a flat-rate pricing model, the user pays a fixed monthly fee based on the number of slots used. This enables better cost control and also ensures better query performance as the slots are reserved for the user's exclusive use.\n\nAnswers A, C, and D are not recommended as they would require a lot of effort and maintenance for managing multiple projects or data warehouses."
      },
      {
        "date": "2023-02-13T12:25:00.000Z",
        "voteCount": 1,
        "content": "B and E seem to be correct."
      },
      {
        "date": "2023-02-05T13:49:00.000Z",
        "voteCount": 1,
        "content": "BE are correct. Setting Project quota and/or flat rate model should be chosen"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 97,
    "url": "https://www.examtopics.com/discussions/google/view/75008-exam-associate-cloud-engineer-topic-1-question-97-discussion/",
    "body": "You are building a product on top of Google Kubernetes Engine (GKE). You have a single GKE cluster. For each of your customers, a Pod is running in that cluster, and your customers can run arbitrary code inside their Pod. You want to maximize the isolation between your customers' Pods. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Binary Authorization and whitelist only the container images used by your customers' Pods.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Container Analysis API to detect vulnerabilities in the containers used by your customers' Pods.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a GKE node pool with a sandbox type configured to gvisor. Add the parameter runtimeClassName: gvisor to the specification of your customers' Pods.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the cos_containerd image for your GKE nodes. Add a nodeSelector with the value cloud.google.com/gke-os-distribution: cos_containerd to the specification of your customers' Pods."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-05-22T06:45:00.000Z",
        "voteCount": 22,
        "content": "Let me be honest, I did not have any clue to answer this question. However, I spotted the keyword, 'isolation', from the question and a keyword, 'sandbox' from the answers and guessed the answer which turned out to be correct.\nSo, yes it is C!"
      },
      {
        "date": "2022-05-06T00:27:00.000Z",
        "voteCount": 11,
        "content": "Correct answer is C: You can enable GKE Sandbox on your cluster to isolate untrusted workloads in sandboxes on the node. GKE Sandbox is built using gVisor, an open source project: https://cloud.google.com/kubernetes-engine/docs/concepts/security-overview?hl=en#protecting_nodes_from_untrusted_workloads"
      },
      {
        "date": "2023-12-30T19:38:00.000Z",
        "voteCount": 6,
        "content": "gVisor is a sandboxing technology that provides an additional layer of isolation between running containers. It's particularly useful in scenarios where containers might be running untrusted or arbitrary code, as it helps in mitigating the risk of kernel exploits.\nBy configuring a node pool with gVisor and specifying runtimeClassName: gvisor in the Pod specifications, each Pod is run within this sandboxed environment, thereby enhancing isolation between the Pods."
      },
      {
        "date": "2023-12-30T19:39:00.000Z",
        "voteCount": 2,
        "content": "A. Binary Authorization: While Binary Authorization is a security control that ensures only trusted container images are deployed on GKE, it doesn't provide isolation between running Pods. It's more about image integrity and compliance.\nB. Container Analysis API: This API is used for scanning container images for vulnerabilities. While important for security, it doesn't directly contribute to runtime isolation between Pods.\nD. Using cos_containerd Image: The Container-Optimized OS with containerd (cos_containerd) is a secure choice for the node image in GKE. However, it doesn't provide the same level of isolation for arbitrary code execution in Pods as gVisor. The nodeSelector parameter is used to schedule Pods on specific nodes but doesn't enhance inter-Pod isolation."
      },
      {
        "date": "2023-12-30T19:45:00.000Z",
        "voteCount": 2,
        "content": "\u2022\tImplementing gVisor can impact the performance of the containers due to the additional layer of abstraction. However, for scenarios requiring high security and isolation, particularly when running arbitrary code, the trade-off can be justified."
      },
      {
        "date": "2023-11-06T06:02:00.000Z",
        "voteCount": 2,
        "content": "The correct answer is C"
      },
      {
        "date": "2022-12-21T08:27:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2022-11-06T16:25:00.000Z",
        "voteCount": 3,
        "content": "GKE Sandbox https://cloud.google.com/kubernetes-engine/docs/concepts/sandbox-pods"
      },
      {
        "date": "2022-08-03T20:21:00.000Z",
        "voteCount": 2,
        "content": "As it has been mentioned already: https://cloud.google.com/kubernetes-engine/docs/how-to/sandbox-pods?hl=en\n\nhttps://cloud.google.com/kubernetes-engine/docs/how-to/sandbox-pods?hl=en#working_with"
      },
      {
        "date": "2022-06-23T12:28:00.000Z",
        "voteCount": 3,
        "content": "gVisor is the way to isolate. Those who already preparing for CKS can answer this question without even thinking further. C is right"
      },
      {
        "date": "2022-06-04T23:48:00.000Z",
        "voteCount": 1,
        "content": "Go for C"
      },
      {
        "date": "2022-04-30T21:29:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/kubernetes-engine/docs/concepts/security-overview?hl=en#protecting_nodes_from_untrusted_workloads"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 98,
    "url": "https://www.examtopics.com/discussions/google/view/79637-exam-associate-cloud-engineer-topic-1-question-98-discussion/",
    "body": "Your customer has implemented a solution that uses Cloud Spanner and notices some read latency-related performance issues on one table. This table is accessed only by their users using a primary key. The table schema is shown below.<br><img src=\"/assets/media/exam-media/04338/0005100001.png\" class=\"in-exam-image\"><br>You want to resolve the issue. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRemove the profile_picture field from the table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a secondary index on the person_id column.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the primary key to not have monotonically increasing values.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a secondary index using the following Data Definition Language (DDL): <img src=\"/assets/media/exam-media/04338/0005200001.png\" class=\"in-exam-image\">"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 29,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 12,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 6,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-09-02T18:51:00.000Z",
        "voteCount": 10,
        "content": "Create a secondary index using the following Data Definition.\nIf we watch the next video, he talks about a change to monotonically when we insert rows.\nFinally when we talk about read and we have a perdormance issues, we must create a index.\nhttps://www.youtube.com/watch?v=r6uj0HMNQNQ"
      },
      {
        "date": "2023-01-31T12:45:00.000Z",
        "voteCount": 3,
        "content": "Adding index for faster retrieval is a basic DBMS concept but why do we need the index on firstname and lastname as per D?"
      },
      {
        "date": "2024-09-05T12:34:00.000Z",
        "voteCount": 1,
        "content": "right, it says: This table is accessed only by a primary key. So B should be the answer"
      },
      {
        "date": "2023-12-30T19:56:00.000Z",
        "voteCount": 6,
        "content": "\u2022\tHotspotting Issue: Cloud Spanner, like many distributed databases, can experience issues with what is known as \"hotspotting.\" This happens when a large portion of read or write operations are concentrated on a specific part of the database. In this case, the sequential nature of the person_id as a primary key can lead to hotspotting because new records are continually added at the \"end\" of the key space, creating a hot node which can cause performance bottlenecks.\n\u2022\tMonotonically Increasing Values: Monotonically increasing values as primary keys can exacerbate the hotspotting effect because each new entry is placed after the last, creating a write hotspot on the last node that handles the upper bound of the key range. Over time, this can lead to unbalanced read/write loads across the nodes."
      },
      {
        "date": "2023-12-30T19:57:00.000Z",
        "voteCount": 3,
        "content": "\u2022\tAlternatives to Monotonic Keys: To mitigate this, you can use a primary key that distributes writes more evenly across the key space. This could be achieved by using a UUID (Universally Unique Identifier) or sharding the monotonically increasing identifier by combining it with another value that has a more random distribution."
      },
      {
        "date": "2024-09-05T12:52:00.000Z",
        "voteCount": 2,
        "content": "Why B: To improve read performance, we should add a secondary index on the primary key. Since we know that this table is accessed only by the primary key, this would optimize read operations.\n\nWhy not C: Monotonically increasing values might create a hotspot, but this would primarily affect write operations. In fact, lookups would be easier with sequential values compared to a UUID. Additionally, it is not mentioned that users are only reading the last inserted data, which would create a read hotspot.\n\nWhy not D: This approach could work, but since the table is accessed only by the primary key, there's no need to add additional fields to the index.\n\nWhy not A: The profile_picture field should not significantly affect read performance."
      },
      {
        "date": "2024-09-05T12:57:00.000Z",
        "voteCount": 2,
        "content": "oh, just found that primary key is automatically indexed by default... so I would vote for A then, hahaha"
      },
      {
        "date": "2024-03-14T08:03:00.000Z",
        "voteCount": 1,
        "content": "D is correct for me"
      },
      {
        "date": "2023-11-06T06:05:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is C"
      },
      {
        "date": "2023-10-17T00:55:00.000Z",
        "voteCount": 2,
        "content": "D makes no sense. It's C"
      },
      {
        "date": "2023-09-02T23:49:00.000Z",
        "voteCount": 2,
        "content": "C seems more correct"
      },
      {
        "date": "2023-03-29T10:11:00.000Z",
        "voteCount": 4,
        "content": "How is this a GCP question?"
      },
      {
        "date": "2023-04-03T08:38:00.000Z",
        "voteCount": 1,
        "content": "Spanner and distribution of primary key"
      },
      {
        "date": "2023-03-18T11:43:00.000Z",
        "voteCount": 2,
        "content": "PK already has index by default, so not B. D - index by 3 fields. but users use person_id for acces, so D is wrong. \nSo C - because monotonically increasing fields is not good candidate for PK(because index degradetion)"
      },
      {
        "date": "2023-03-06T19:50:00.000Z",
        "voteCount": 2,
        "content": "Based on the supplied video by others; https://www.youtube.com/watch?v=r6uj0HMNQNQ, we can see at time 1:47 that due to the slitting of the rows, a sequential primary key will create hotspots. Therefor we need a non-sequential key; e.g. hash-based key"
      },
      {
        "date": "2023-02-13T19:39:00.000Z",
        "voteCount": 4,
        "content": "B. is correct...Add a secondary index on the person_id column.\n\nAdding a secondary index on the person_id column would help resolve the read latency-related performance issues on this table. Since the table is accessed using only the primary key, creating a secondary index on the person_id column would allow Cloud Spanner to retrieve the data using the index, rather than scanning the entire table. This can significantly reduce the read latency for queries that access this table.\n\nRemoving the profile_picture field or changing the primary key to not have monotonically increasing values may not necessarily resolve the performance issues related to read latency. Creating a secondary index is a more targeted solution to address the specific issue at hand.\n\nOption D is incomplete and does not provide enough information to assess its correctness."
      },
      {
        "date": "2022-12-03T23:35:00.000Z",
        "voteCount": 2,
        "content": "Create a secondary index using the following Data Definition.\nIf we watch the next video, he talks about a change to monotonically when we insert rows.\nFinally when we talk about read and we have a perdormance issues, we must create a index.\nhttps://www.youtube.com/watch?v=r6uj0HMNQNQ"
      },
      {
        "date": "2022-12-01T19:41:00.000Z",
        "voteCount": 1,
        "content": "I think it is C"
      },
      {
        "date": "2022-11-30T03:01:00.000Z",
        "voteCount": 1,
        "content": "It's definetely C, D doesn't make sense here"
      },
      {
        "date": "2022-11-13T17:26:00.000Z",
        "voteCount": 3,
        "content": "Choose a primary key to prevent hotspots\nAs mentioned in Schema and data model, you should be careful when choosing a primary key to not accidentally create hotspots in your database. One cause of hotspots is having a column whose value monotonically increases as the first key part, because this results in all inserts occurring at the end of your key space. This pattern is undesirable because Spanner divides data among servers by key ranges, which means all your inserts will be directed at a single server that will end up doing all the work."
      },
      {
        "date": "2022-11-06T16:28:00.000Z",
        "voteCount": 6,
        "content": "C https://cloud.google.com/spanner/docs/schema-design#primary-key-prevent-hotspots"
      },
      {
        "date": "2022-11-06T09:40:00.000Z",
        "voteCount": 4,
        "content": "C is the right answer. Why? \"This table is accessed only by their users using a primary key.\" So adding additional indexes on firstname and lastname won't help."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 99,
    "url": "https://www.examtopics.com/discussions/google/view/22200-exam-associate-cloud-engineer-topic-1-question-99-discussion/",
    "body": "Your finance team wants to view the billing report for your projects. You want to make sure that the finance team does not get additional permissions to the project. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd the group for the finance team to roles/billing user role.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd the group for the finance team to roles/billing admin role.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd the group for the finance team to roles/billing viewer role.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd the group for the finance team to roles/billing project/Manager role."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-06-06T11:59:00.000Z",
        "voteCount": 52,
        "content": "c\n\"Billing Account Viewer access would usually be granted to finance teams, it provides access to spend information, but does not confer the right to link or unlink projects or otherwise manage the properties of the billing account.\"\n\nhttps://cloud.google.com/billing/docs/how-to/billing-access"
      },
      {
        "date": "2020-06-06T19:54:00.000Z",
        "voteCount": 21,
        "content": "Answer is C - Billing Account Viewer access would usually be granted to finance teams, it provides access to spend information, but does not confer the right to link or unlink projects or otherwise manage the properties of the billing account."
      },
      {
        "date": "2021-02-05T17:13:00.000Z",
        "voteCount": 4,
        "content": "Hey, look at this ......:)"
      },
      {
        "date": "2023-11-06T06:10:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is C"
      },
      {
        "date": "2023-09-02T23:50:00.000Z",
        "voteCount": 2,
        "content": "C is the correct answer"
      },
      {
        "date": "2023-07-26T00:03:00.000Z",
        "voteCount": 1,
        "content": "all other will give additional permission"
      },
      {
        "date": "2022-06-23T12:35:00.000Z",
        "voteCount": 1,
        "content": "It is no brainer question, C is right"
      },
      {
        "date": "2022-06-05T00:13:00.000Z",
        "voteCount": 1,
        "content": "Go for C"
      },
      {
        "date": "2022-01-09T11:54:00.000Z",
        "voteCount": 2,
        "content": "C - the only role appropriate answer to view and not change anything in the project is the billing viewer role."
      },
      {
        "date": "2021-12-06T07:41:00.000Z",
        "voteCount": 1,
        "content": "C is perfect"
      },
      {
        "date": "2021-11-20T05:56:00.000Z",
        "voteCount": 1,
        "content": "C. Add the group for the finance team to roles/billing viewer role."
      },
      {
        "date": "2021-08-30T05:09:00.000Z",
        "voteCount": 1,
        "content": "agree c is the correct option. needs only billing viewer role."
      },
      {
        "date": "2021-05-16T03:44:00.000Z",
        "voteCount": 2,
        "content": "C is the correct answer"
      },
      {
        "date": "2021-05-13T07:29:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2021-03-25T03:11:00.000Z",
        "voteCount": 1,
        "content": "C is correct. Add the group for the finance team to roles/billing viewer role."
      },
      {
        "date": "2021-03-12T09:51:00.000Z",
        "voteCount": 1,
        "content": "Obvious choice here is C-- viewer to make sure they dont have additional access.\nSource: this is my job"
      },
      {
        "date": "2021-03-10T21:40:00.000Z",
        "voteCount": 1,
        "content": "ANS - C"
      },
      {
        "date": "2021-02-24T09:37:00.000Z",
        "voteCount": 2,
        "content": "C. Add the group for the finance team to roles/billing viewer role."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 100,
    "url": "https://www.examtopics.com/discussions/google/view/22640-exam-associate-cloud-engineer-topic-1-question-100/",
    "body": "Your organization has strict requirements to control access to Google Cloud projects. You need to enable your Site Reliability Engineers (SREs) to approve requests from the Google Cloud support team when an SRE opens a support case. You want to follow Google-recommended practices. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd your SREs to roles/iam.roleAdmin role.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd your SREs to roles/accessapproval.approver role.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd your SREs to a group and then add this group to roles/iam.roleAdmin.role.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd your SREs to a group and then add this group to roles/accessapproval.approver role.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 16,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-06-09T03:03:00.000Z",
        "voteCount": 60,
        "content": "D. Add your SREs to a group and then add this group to roles/accessapproval approver role.\n-Google recommendation."
      },
      {
        "date": "2022-01-17T01:20:00.000Z",
        "voteCount": 20,
        "content": "This was there in exam, go with community answers."
      },
      {
        "date": "2022-01-21T01:39:00.000Z",
        "voteCount": 4,
        "content": "how was your exam? is this website qts useful?"
      },
      {
        "date": "2024-10-14T08:42:00.000Z",
        "voteCount": 1,
        "content": "Grant the Access Approval Approver (roles/accessapproval.approver) IAM role on the project, folder, or organization to the principal who you want to be able to perform approvals. You can grant the Access Approval Approver IAM role to either an individual user, a service account, or a Google group.\nhttps://cloud.google.com/assured-workloads/access-approval/docs/approve-requests"
      },
      {
        "date": "2023-11-06T06:12:00.000Z",
        "voteCount": 1,
        "content": "D is the correct answer"
      },
      {
        "date": "2023-09-02T23:51:00.000Z",
        "voteCount": 2,
        "content": "D seems more correct"
      },
      {
        "date": "2023-08-29T22:13:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2022-06-23T12:36:00.000Z",
        "voteCount": 2,
        "content": "D is right .."
      },
      {
        "date": "2022-06-05T00:14:00.000Z",
        "voteCount": 1,
        "content": "Go for D"
      },
      {
        "date": "2022-04-30T09:12:00.000Z",
        "voteCount": 3,
        "content": "Answers C and D are correct, but it doesn't say if the SRE already has a group and as it is Google's recommendation to make a group to add users and privileges to the group, the right one is D"
      },
      {
        "date": "2022-04-26T18:24:00.000Z",
        "voteCount": 2,
        "content": "It mentioned more than one SRE, so adding the user to group is most suitable approach, Answer is D."
      },
      {
        "date": "2022-04-08T11:33:00.000Z",
        "voteCount": 16,
        "content": "Passed my exams today. Not because just because of the questions I practiced here, but because of you guys, your knowledge and experience and breakdown of questions. Too bad this site can't go legit. It such an wholesome resource.\n\nSome final words... KEEP MOVING FORWARD UNTIL ALL THE QUESTIONS ARE DESTROYED TATAKAE!!!!!!"
      },
      {
        "date": "2022-03-02T04:18:00.000Z",
        "voteCount": 11,
        "content": "I've seen about 5 questions which are like this, always asking how to grant access and \"follow Google best practice\", and every time it's just making sure you know to use a group to control access to resources for users, and not adding users directly to objects. \n\nRemember that keyword, \"Google best practice\" means \"make sure you use a group\""
      },
      {
        "date": "2022-02-15T20:37:00.000Z",
        "voteCount": 3,
        "content": "D. Add your SREs to a group and then add this group to roles/accessapproval approver role."
      },
      {
        "date": "2021-12-08T06:28:00.000Z",
        "voteCount": 3,
        "content": "D is Correct"
      },
      {
        "date": "2021-12-06T07:41:00.000Z",
        "voteCount": 3,
        "content": "D is perfect"
      },
      {
        "date": "2021-11-19T13:09:00.000Z",
        "voteCount": 4,
        "content": "D Correct"
      },
      {
        "date": "2021-11-19T08:30:00.000Z",
        "voteCount": 2,
        "content": "D. Add your SREs to a group and then add this group to roles/accessapproval.approver role."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 101,
    "url": "https://www.examtopics.com/discussions/google/view/24149-exam-associate-cloud-engineer-topic-1-question-101/",
    "body": "You need to host an application on a Compute Engine instance in a project shared with other teams. You want to prevent the other teams from accidentally causing downtime on that application. Which feature should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a Shielded VM.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a Preemptible VM.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a sole-tenant node.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable deletion protection on the instance.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 33,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 24,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-08-23T21:04:00.000Z",
        "voteCount": 58,
        "content": "Correct Answer is (D):\n\nPreventing Accidental VM Deletion\nThis document describes how to protect specific VM instances from deletion by setting the deletionProtection property on an Instance resource. To learn more about VM instances, read the Instances documentation.\n\nAs part of your workload, there might be certain VM instances that are critical to running your application or services, such as an instance running a SQL server, a server used as a license manager, and so on. These VM instances might need to stay running indefinitely so you need a way to protect these VMs from being deleted.\n\nBy setting the deletionProtection flag, a VM instance can be protected from accidental deletion. If a user attempts to delete a VM instance for which you have set the deletionProtection flag, the request fails. Only a user that has been granted a role with compute.instances.create permission can reset the flag to allow the resource to be deleted.\n\nhttps://cloud.google.com/compute/docs/instances/preventing-accidental-vm-deletion"
      },
      {
        "date": "2020-06-26T07:00:00.000Z",
        "voteCount": 14,
        "content": "Agree with D\n\nYou can enabale Termination protection"
      },
      {
        "date": "2024-09-06T11:41:00.000Z",
        "voteCount": 1,
        "content": "It does't say that someone can delete it and we should prevent deletion."
      },
      {
        "date": "2024-08-29T10:06:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is D."
      },
      {
        "date": "2024-07-25T04:29:00.000Z",
        "voteCount": 1,
        "content": "C. Question says accidental downtime - this could be caused by many other reasons other than just straight deleting things.\n \"Use a sole-tenant node\" allows you to have dedicated hardware for your VM instances, providing isolation from other workloads. This isolation can help prevent other teams' actions from impacting your application's availability."
      },
      {
        "date": "2024-05-26T02:23:00.000Z",
        "voteCount": 1,
        "content": "(Use a sole-tenant node) is a method to ensure that your VMs run on a physical host dedicated to your project (related to licences etc), but it doesn't specifically prevent accidental downtime caused by other teams in a shared environment."
      },
      {
        "date": "2024-01-11T19:35:00.000Z",
        "voteCount": 1,
        "content": "To prevent accidental deletion of a Compute Engine instance, you can enable deletion protection on the instance. This feature prevents the instance from being deleted by any user until deletion protection is disabled.\n\nSo, the correct answer is: D. Enable deletion protection on the instance."
      },
      {
        "date": "2023-11-27T17:08:00.000Z",
        "voteCount": 2,
        "content": "for me, is D"
      },
      {
        "date": "2023-11-23T08:27:00.000Z",
        "voteCount": 2,
        "content": "ANS is D:\nBy setting the deletionProtection flag, a VM instance can be protected from accidental deletion. If a user attempts to delete a VM instance for which you have set the deletion \nProtection flag, the request fails. Only a user that has been granted a role with compute."
      },
      {
        "date": "2023-11-10T05:11:00.000Z",
        "voteCount": 2,
        "content": "C is incorrect as sole-tenant is project-based. The other users in the same project can still cause accidentally deletion of the VM even if using a sole-tenant node."
      },
      {
        "date": "2023-11-06T06:17:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is D"
      },
      {
        "date": "2023-11-03T09:36:00.000Z",
        "voteCount": 1,
        "content": "I will go with Enable deletion protection on the instance as it prevents accidental deletion.\nShielded VMs protects from malicious attack in the software on the VM.\nSole Tenant - it isolates the resources from others, but does not protect from accidental deletion. Also this needs all together a setup from scratch and cannot be just setup on fly easily."
      },
      {
        "date": "2023-10-15T08:47:00.000Z",
        "voteCount": 4,
        "content": "A. Use a Shielded VM.\n\nExplanation:\n\n    Shielded VMs provide additional security features to help protect against rootkits and bootkits, ensuring the integrity of the operating system and your applications.\n    Shielded VMs can help prevent accidental or malicious modifications to the VM that could lead to downtime or security risks.\n\nOption B (Use a Preemptible VM) is not suitable for preventing accidental downtime as preemptible VMs are designed to be temporary and can be terminated at any time.\n\nOption C (Use a sole-tenant node) is a method to ensure that your VMs run on a physical host dedicated to your project, but it doesn't specifically prevent accidental downtime caused by other teams in a shared environment.\n\nOption D (Enable deletion protection on the instance) helps prevent accidental deletion of the instance, but it does not prevent other types of accidental modifications or downtime caused by other teams."
      },
      {
        "date": "2023-10-04T23:44:00.000Z",
        "voteCount": 2,
        "content": "D. Enable deletion protection on the instance."
      },
      {
        "date": "2023-09-19T02:49:00.000Z",
        "voteCount": 1,
        "content": "C. \"Use a sole-tenant node\" allows you to have dedicated hardware for your VM instances, providing isolation from other workloads. This isolation can help prevent other teams' actions from impacting your application's availability."
      },
      {
        "date": "2023-09-07T21:58:00.000Z",
        "voteCount": 2,
        "content": "Answer is D"
      },
      {
        "date": "2023-09-03T00:33:00.000Z",
        "voteCount": 1,
        "content": "option c is correct becuase, it givrs you islotation from other projects, where option d is only prevent if from delete only"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 102,
    "url": "https://www.examtopics.com/discussions/google/view/22304-exam-associate-cloud-engineer-topic-1-question-102/",
    "body": "Your organization needs to grant users access to query datasets in BigQuery but prevent them from accidentally deleting the datasets. You want a solution that follows Google-recommended practices. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd users to roles/bigquery user role only, instead of roles/bigquery dataOwner.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd users to roles/bigquery dataEditor role only, instead of roles/bigquery dataOwner.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a custom role by removing delete permissions, and add users to that role only.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a custom role by removing delete permissions. Add users to the group, and then add the group to the custom role.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 24,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 20,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-07-18T10:00:00.000Z",
        "voteCount": 72,
        "content": "I believe the key part is the \"following Google Best Practices\" phrase.\nA - Works, but doesn't follow GCP best practices\nB - Doesn't work as the role grants permission to delete datasets\nC - Works, but is more complicated than A and doesn't follow Google best practices\nD - Correct, more complicated than A, but it follows Google Best Practices."
      },
      {
        "date": "2021-03-07T15:14:00.000Z",
        "voteCount": 11,
        "content": "Read description carefully \"prevent from accidentally deleting the datasets\".  Not tables, datasets! option B does not allow to delete datesets either.\nCheck dateset permissions in the roles/bigquery.dataEditor:\nbigquery.datasets.create\nbigquery.datasets.get\nbigquery.datasets.getIamPolicy\nbigquery.datasets.updateTag\nYou CANNOT delete dataset with option \"B\""
      },
      {
        "date": "2022-04-10T02:43:00.000Z",
        "voteCount": 1,
        "content": "Netiher with A."
      },
      {
        "date": "2022-08-02T08:15:00.000Z",
        "voteCount": 3,
        "content": "But it means you will have to add the users one by one which doesn't follow Google best practices..."
      },
      {
        "date": "2021-06-12T18:52:00.000Z",
        "voteCount": 12,
        "content": "I think A is the Answer and it follow GCP best practices.\nhttps://cloud.google.com/iam/docs/understanding-roles#bigquery-roles\nWe do have the role - BigQuery User which does the below permissions\nWhen applied to a project, this role also provides the ability to run jobs, including queries, within the project. \nbigquery.datasets.create\nbigquery.datasets.get\nbigquery.datasets.getIamPolicy"
      },
      {
        "date": "2023-04-03T23:01:00.000Z",
        "voteCount": 4,
        "content": "bigquery.datasets.create allows the creation of new datasets within the project; the creator is granted the BigQuery Data Owner role (roles/bigquery.dataOwner) on these new datasets so he can delete these created datasets"
      },
      {
        "date": "2023-04-03T23:01:00.000Z",
        "voteCount": 4,
        "content": "D seems correct"
      },
      {
        "date": "2021-07-24T02:53:00.000Z",
        "voteCount": 13,
        "content": "I don't think A works properly.\nroles/bigquery.user has bigquery.datasets.create. And the documentation states:\n\n&gt; Additional, allows the creation of new datasets within the project; the creator is granted the BigQuery Data Owner role (roles/bigquery.dataOwner) on these new datasets.\n\nIf bigquery.user creates a new dataset, it's likely that bigquery.user will get permission to delete that dataset. This means that bigquery.user may have permission to delete data.\nhttps://cloud.google.com/bigquery/docs/access-control"
      },
      {
        "date": "2022-07-29T05:10:00.000Z",
        "voteCount": 1,
        "content": "See the question carefully \"accidentally deleting the datasets\" it is saying not to delete \"the\" datasets which means original dataset which existed before his creation .So answer is A."
      },
      {
        "date": "2022-12-08T10:27:00.000Z",
        "voteCount": 2,
        "content": "The only way a user can accidentally delete a dataset is if they have the delete permission anyway. So brvinod and kyo's points still stand"
      },
      {
        "date": "2022-02-11T07:25:00.000Z",
        "voteCount": 5,
        "content": "A bigquery.user will get a \"data owner\" role on the datasets he creates. That means he can delete those data sets he created. In that sense A fails to that extent."
      },
      {
        "date": "2020-08-19T10:59:00.000Z",
        "voteCount": 57,
        "content": "Correct Answer is (D):\n\nThe proper answer regarding to bigquery roles is the listed in the options, the proper rol that resolve this requirement is: roles/bigquery.dataViewer\nhttps://cloud.google.com/bigquery/docs/access-control#custom_roles\n\non the other hand, the question explicitly is asking to use the GCP best practices on IAM :\nGCP Best Practices  explain clearly these rules:\nPolicy management\n\u2751 Set organization-level IAM policies to grant access to all projects in your organization.\n\u2751  Grant roles to a Google group instead of individual users when possible. It is easier to add members to and remove members from a Google group instead of updating an IAM policy to add or remove users.\n\u2751  If you need to grant multiple roles to allow a particular task, create a Google group, grant the roles to that group, and then add users to that group.\nhttps://cloud.google.com/iam/docs/using-iam-securely#policy_management"
      },
      {
        "date": "2021-02-18T12:59:00.000Z",
        "voteCount": 8,
        "content": "Other best practice is use predefine roles over custom roles. Maybe A is correct"
      },
      {
        "date": "2021-02-18T13:18:00.000Z",
        "voteCount": 6,
        "content": "I correct myself: https://cloud.google.com/iam/docs/understanding-custom-roles\nKey Point: Custom roles enable you to enforce the principle of least privilege, ensuring that the user and service accounts in your organization have only the permissions essential to performing their intended functions."
      },
      {
        "date": "2020-09-03T12:09:00.000Z",
        "voteCount": 5,
        "content": "Answer is A: roles/bigquery.user is a BigQuery User role which when applied to a project provides the ability to run jobs, including queries, within the project. A member with this role can enumerate their own jobs, cancel their own jobs, and enumerate datasets within a project.\n\nRef: https://cloud.google.com/iam/docs/understanding-roles#bigquery-roles"
      },
      {
        "date": "2021-04-01T19:58:00.000Z",
        "voteCount": 5,
        "content": "you can create data set with bigquery.user role because it has bigquery.datasets.create permissions. And if a user has bigquery.datasets.create permissions, when that user creates a dataset, they are granted bigquery.dataOwner access to it. So A is NOT a choice"
      },
      {
        "date": "2024-09-24T18:34:00.000Z",
        "voteCount": 1,
        "content": "A is correct because the key point is.. users can query the dataset but not delete. For querying, jobs create role required which comes under bigquery user role"
      },
      {
        "date": "2024-08-29T10:07:00.000Z",
        "voteCount": 1,
        "content": "D. follows Google Best Practice. Others do not or are flat out wrong."
      },
      {
        "date": "2024-05-14T12:34:00.000Z",
        "voteCount": 1,
        "content": "GCP Best Practices is to create a group of Users and assign it a custom role with the required permissions (least privilege)."
      },
      {
        "date": "2024-03-09T10:23:00.000Z",
        "voteCount": 1,
        "content": "A IS THE ANWER"
      },
      {
        "date": "2024-02-28T17:09:00.000Z",
        "voteCount": 1,
        "content": "D is solves the problem and follow best practices. With A you will be able to delete data sets you create."
      },
      {
        "date": "2024-02-20T04:41:00.000Z",
        "voteCount": 1,
        "content": "You can create a custom role at the project or organization level. Since users are added to role, it should be A.  https://cloud.google.com/iam/docs/creating-custom-roles"
      },
      {
        "date": "2024-02-15T11:35:00.000Z",
        "voteCount": 1,
        "content": "I think the right answer is A due to a predefine valid role. I mean that biqguery.user role is valid so it's not needed to create a custom role. bigquery.user role can't delete datasets created by anyone. \n\nhttps://cloud.google.com/iam/docs/understanding-roles#bigquery.user\n\nbigquery.datasets.delete\nbigquery.datasets.deleteTagBinding"
      },
      {
        "date": "2024-02-15T11:32:00.000Z",
        "voteCount": 1,
        "content": "I think the right answer is A due to a predefine valid role. I mean that biqguery.user role is valid so it's not needed to create a custom role. bigquery.user role can't delete datasets.\nhttps://cloud.google.com/iam/docs/understanding-roles#bigquery.user"
      },
      {
        "date": "2024-02-14T06:33:00.000Z",
        "voteCount": 1,
        "content": "I think it's A for a couple of reasons.\n1. you don't need to create a custom role if there is already one there, an it's Google best practice to ALWAYS use their roles, not create new ones unless absolutely necessary, which is very few cases and I'd be surprised if they'd put a Q in there that would actually .\n2. the \"roles/bigquery.user\" role already allows for bigquery.datasets.create, bigquery.datasets.get, bigquery.datasets.getIamPolicy, which this Q is asking for."
      },
      {
        "date": "2024-02-12T07:35:00.000Z",
        "voteCount": 1,
        "content": "I have no idea, but I think it's D as you add users to group first, then to a role, which is google best practice, but not sure about custom role in the first place."
      },
      {
        "date": "2023-11-27T17:11:00.000Z",
        "voteCount": 2,
        "content": "I gonna with A and not D as best practices is to use predefined roles."
      },
      {
        "date": "2023-11-25T00:51:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is C. Create a custom role by removing delete permissions, and add users to that role only.\n\nThis is the recommended approach by Google, as it provides a granular level of control over user permissions. By creating a custom role that only allows users to query datasets, you can ensure that they do not have the ability to accidentally delete them."
      },
      {
        "date": "2023-11-06T06:24:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is D"
      },
      {
        "date": "2023-11-03T09:50:00.000Z",
        "voteCount": 1,
        "content": "I will go with A and not D as best practices is to use predefined roles. \nIf the question had \"least previledge\"  then answer would have been D."
      },
      {
        "date": "2023-10-04T23:54:00.000Z",
        "voteCount": 1,
        "content": "A. Add users to roles/bigquery user role only, instead of roles/bigquery dataOwner."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 103,
    "url": "https://www.examtopics.com/discussions/google/view/24150-exam-associate-cloud-engineer-topic-1-question-103/",
    "body": "You have a developer laptop with the Cloud SDK installed on Ubuntu. The Cloud SDK was installed from the Google Cloud Ubuntu package repository. You want to test your application locally on your laptop with Cloud Datastore. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExport Cloud Datastore data using gcloud datastore export.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Cloud Datastore index using gcloud datastore indexes create.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall the google-cloud-sdk-datastore-emulator component using the apt get install command.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall the cloud-datastore-emulator component using the gcloud components install command.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 33,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 31,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-07-18T10:05:00.000Z",
        "voteCount": 46,
        "content": "I believe answer is C\nhttps://cloud.google.com/sdk/docs/downloads-apt-get\n\nThe question is not about the datastore command itself but from where we should run the update command on the Ubuntu to install the component."
      },
      {
        "date": "2020-07-30T03:19:00.000Z",
        "voteCount": 53,
        "content": "I agree with this comment.  The answer is C.\nIf you installed the SDK from the Ubuntu repo and try to do the following:\n$ gcloud components install cloud-datastore-emulator\n\nYou will receive this message:\nERROR: (gcloud.components.install) \nYou cannot perform this action because the Cloud SDK component manager \nis disabled for this installation. You can run the following command \nto achieve the same result for this installation: \n\nsudo apt-get install google-cloud-sdk-datastore-emulator"
      },
      {
        "date": "2020-08-25T09:55:00.000Z",
        "voteCount": 4,
        "content": "it says that in your Ubuntu, you have Cloud SDK installed already. So it should be able to run the command in D"
      },
      {
        "date": "2020-09-07T04:51:00.000Z",
        "voteCount": 14,
        "content": "Yes, but it says that \"The Cloud SDK was installed from the Google Cloud Ubuntu package repository\", then to install datastore emulator you should use the command in Option C."
      },
      {
        "date": "2020-09-07T04:53:00.000Z",
        "voteCount": 8,
        "content": "WOW!!! Today I have learned a new and interesting thing thanks to you..."
      },
      {
        "date": "2021-09-22T04:44:00.000Z",
        "voteCount": 35,
        "content": "absolutely insane if that question comes up during the associate exam, who on earth would know that off the top of their heads?"
      },
      {
        "date": "2020-06-26T07:05:00.000Z",
        "voteCount": 26,
        "content": "Ans is D\n\nhttps://cloud.google.com/datastore/docs/tools/datastore-emulator"
      },
      {
        "date": "2020-11-10T07:30:00.000Z",
        "voteCount": 26,
        "content": "Wrong! The answer is C! When you install SDK using apt Cloud SDK Component Manager is disabled and you need to install extra packages again using apt.\nhttps://cloud.google.com/sdk/docs/components#managing_cloud_sdk_components\nNote: These instructions will not work if you have installed Cloud SDK using a package manager such as APT or yum because Cloud SDK Component Manager is disabled when using that method of installation."
      },
      {
        "date": "2020-12-09T11:32:00.000Z",
        "voteCount": 23,
        "content": "I believe the answer is C...\n\nIt is a tricky question!! The question states, \"The Cloud SDK was installed from the Google Cloud Ubuntu package repository.\" For those, who aren't that familiar with Debian/Ubuntu, D seems like an attractive answer. It works as a way to install Datastore...but it does NOT fit the context of the question.\n\nI recommend looking back to G Cloud SDK installation (Debian/Ubuntu): https://cloud.google.com/sdk/docs/install#deb\n\nRead the \"Installation Steps\" in the documentation. In Step 3, \"sudo apt-get update &amp;&amp; sudo apt-get install google-cloud-sdk\". Then, Step 4 is additionally adding other components, such as \"sudo apt-get install google-cloud-sdk-datastore-emulator\".\n\nProving C the correct answer."
      },
      {
        "date": "2021-03-03T10:46:00.000Z",
        "voteCount": 6,
        "content": "Go With 'C' ... just tried creating a ubuntu server and verified these..  Dont worry about any other options. https://cloud.google.com/sdk/docs/quickstart#deb check this link ."
      },
      {
        "date": "2024-10-16T05:11:00.000Z",
        "voteCount": 1,
        "content": "C is correct because the datastore emulator is installed using apt and not gcloud.\nD is incorrect because the \"cloud-datastore-emulator\" component is a legacy component and is not recommended for use. The correct component to install is \"google-cloud-sdk-datastore-emulator\". \n\nhttps://cloud.google.com/datastore/docs/tools/datastore-emulator"
      },
      {
        "date": "2024-08-29T10:11:00.000Z",
        "voteCount": 2,
        "content": "The correct answer is C. Don't mix your Ubuntu CLI with Gcloud. How is it supposed to work. It's either or. Not mixed."
      },
      {
        "date": "2024-07-02T04:50:00.000Z",
        "voteCount": 2,
        "content": "Looks like it may be \"C\" as there is an error when I try to install emulator by gcloud command. \n===\ngcloud components install cloud-datastore-emulator\nERROR: (gcloud.components.install)\nYou cannot perform this action because the Google Cloud CLI component manager\nis disabled for this installation. You can run the following command\nto achieve the same result for this installation:\nsudo apt-get install google-cloud-cli-datastore-emulator\n==="
      },
      {
        "date": "2024-02-15T10:44:00.000Z",
        "voteCount": 1,
        "content": "C. Install the google-cloud-sdk-datastore-emulator component using the apt get install command.\n\nHere's why:\n\nA. Export Cloud Datastore data: This wouldn't help with local testing as it simply exports your data, not the Datastore environment itself.\nB. Create a Cloud Datastore index: Creating an index wouldn't allow you to run a local emulator for testing.\nC. Install the google-cloud-sdk-datastore-emulator component using apt get install: This method is the recommended way to install the Cloud Datastore Emulator on Ubuntu when the Cloud SDK is installed from the Google Cloud Ubuntu package repository.\nD. Install the cloud-datastore-emulator component using the gcloud components install command: This command would work if you had installed the Cloud SDK using the gcloud installer instead of the Ubuntu package repository."
      },
      {
        "date": "2024-01-03T23:18:00.000Z",
        "voteCount": 2,
        "content": "D -Option C suggests installing the google-cloud-sdk-datastore-emulator component using the apt-get install command. However, the Cloud SDK components are typically managed using the gcloud components command, and the correct component name for the Cloud Datastore Emulator is cloud-datastore-emulator.\n\nSo, to ensure consistency and compatibility with the Cloud SDK's management commands, it's recommended to use the gcloud components install command to install the Cloud Datastore Emulator, making option D the more appropriate choice."
      },
      {
        "date": "2024-01-03T23:17:00.000Z",
        "voteCount": 2,
        "content": "Option C suggests installing the google-cloud-sdk-datastore-emulator component using the apt-get install command. However, the Cloud SDK components are typically managed using the gcloud components command, and the correct component name for the Cloud Datastore Emulator is cloud-datastore-emulator.\n\nSo, to ensure consistency and compatibility with the Cloud SDK's management commands, it's recommended to use the gcloud components install command to install the Cloud Datastore Emulator, making option D the more appropriate choice."
      },
      {
        "date": "2024-01-01T08:58:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/datastore/docs/tools/datastore-emulator"
      },
      {
        "date": "2023-12-29T04:29:00.000Z",
        "voteCount": 1,
        "content": "Ans is C"
      },
      {
        "date": "2023-12-10T02:50:00.000Z",
        "voteCount": 2,
        "content": "Installing the emulator\nThe Datastore emulator is a component of the Google Cloud CLI's gcloud CLI. Use the gcloud components install command to install the Datastore emulator:\n \ngcloud components install cloud-datastore-emulator\n\n//Please go through the documentation and the ans has to be D."
      },
      {
        "date": "2023-11-24T23:48:00.000Z",
        "voteCount": 1,
        "content": "D for sure"
      },
      {
        "date": "2023-11-23T04:10:00.000Z",
        "voteCount": 1,
        "content": "D\nNow that you have the gcloud command - Cloud SDK- better to use it: https://cloud.google.com/datastore/docs/tools/datastore-emulator#installing_the_emulator"
      },
      {
        "date": "2023-11-23T02:09:00.000Z",
        "voteCount": 1,
        "content": "Install the google-cloud-sdk-datastore-emulator component using the apt get install command."
      },
      {
        "date": "2023-11-22T11:55:00.000Z",
        "voteCount": 1,
        "content": "Answer C"
      },
      {
        "date": "2023-11-22T11:55:00.000Z",
        "voteCount": 1,
        "content": "Answer D"
      },
      {
        "date": "2023-11-14T23:22:00.000Z",
        "voteCount": 1,
        "content": "The answer to the question is at,\nhttps://cloud.google.com/datastore/docs/tools/datastore-emulator#installing_the_emulator\ngcloud components install cloud-datastore-emulator"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 104,
    "url": "https://www.examtopics.com/discussions/google/view/22305-exam-associate-cloud-engineer-topic-1-question-104/",
    "body": "Your company set up a complex organizational structure on Google Cloud. The structure includes hundreds of folders and projects. Only a few team members should be able to view the hierarchical structure. You need to assign minimum permissions to these team members, and you want to follow Google-recommended practices. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd the users to roles/browser role.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd the users to roles/iam.roleViewer role.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd the users to a group, and add this group to roles/browser.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd the users to a group, and add this group to roles/iam.roleViewer role."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 10,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-08-20T10:44:00.000Z",
        "voteCount": 31,
        "content": "Correct Answer is (C):\n\nWe need to apply the GCP Best practices. \nroles/browser\tBrowser\tRead access to browse the hierarchy for a project, including the folder, organization, and IAM policy. This role doesn't include permission to view resources in the project.\n\nhttps://cloud.google.com/iam/docs/understanding-roles"
      },
      {
        "date": "2020-06-06T08:42:00.000Z",
        "voteCount": 31,
        "content": "C is the better answer."
      },
      {
        "date": "2024-05-26T02:10:00.000Z",
        "voteCount": 1,
        "content": "(roles/browser)\nRead access to browse the hierarchy for a project, including the folder, organization, and allow policy. This role doesn't include permission to view resources in the project.\n\nhttps://cloud.google.com/resource-manager/docs/access-control-proj#browser\n\nTo view, only, \"view the hierarchical structure\", C. But with browser role they will not be able to view resources, only the structure."
      },
      {
        "date": "2024-02-06T10:06:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/iam/docs/understanding-roles#browser"
      },
      {
        "date": "2024-01-01T09:04:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/iam/docs/understanding-roles"
      },
      {
        "date": "2023-11-25T02:24:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is D. Add the users to a group, and add this group to roles/iam.roleViewer role.\n\nThis approach follows Google-recommended practices by utilizing IAM groups to manage user permissions and ensure granular control over access to the organizational structure. By adding users to a group and assigning the roles/iam.roleViewer role to the group, you can effectively grant the necessary permissions to view the hierarchical structure while minimizing the scope of access."
      },
      {
        "date": "2023-11-06T06:33:00.000Z",
        "voteCount": 1,
        "content": "The answer is D"
      },
      {
        "date": "2023-09-07T07:46:00.000Z",
        "voteCount": 1,
        "content": "C is the correct answer.\nhttps://cloud.google.com/iam/docs/understanding-roles#browser"
      },
      {
        "date": "2023-09-03T00:43:00.000Z",
        "voteCount": 1,
        "content": "C is the correct answer"
      },
      {
        "date": "2023-08-25T00:16:00.000Z",
        "voteCount": 1,
        "content": "C. Add the users to a group, and add this group to roles/browser."
      },
      {
        "date": "2023-07-16T05:22:00.000Z",
        "voteCount": 1,
        "content": "C is the answer, \nBrowser - Read access to browse the hierarchy for a project, including the folder, organization, and allow policy"
      },
      {
        "date": "2023-04-17T04:39:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2022-10-07T13:55:00.000Z",
        "voteCount": 2,
        "content": "C is the correct answer,\nID\nroles/browser\nRole launch stage\nGeneral Availability\nDescription\nAccess to browse GCP resources.\n\n6 assigned permissions\nresourcemanager.folders.get\nresourcemanager.folders.list\nresourcemanager.organizations.get\nresourcemanager.projects.get\nresourcemanager.projects.getIamPolicy\nresourcemanager.projects.list"
      },
      {
        "date": "2022-08-04T20:15:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/iam/docs/understanding-roles"
      },
      {
        "date": "2022-08-03T05:04:00.000Z",
        "voteCount": 1,
        "content": "C is the better answer"
      },
      {
        "date": "2022-08-02T04:42:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2022-06-23T13:47:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer is (C):"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 105,
    "url": "https://www.examtopics.com/discussions/google/view/22732-exam-associate-cloud-engineer-topic-1-question-105/",
    "body": "Your company has a single sign-on (SSO) identity provider that supports Security Assertion Markup Language (SAML) integration with service providers. Your company has users in Cloud Identity. You would like users to authenticate using your company's SSO provider. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn Cloud Identity, set up SSO with Google as an identity provider to access custom SAML apps.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn Cloud Identity, set up SSO with a third-party identity provider with Google as a service provider.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tObtain OAuth 2.0 credentials, configure the user consent screen, and set up OAuth 2.0 for Mobile &amp; Desktop Apps.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tObtain OAuth 2.0 credentials, configure the user consent screen, and set up OAuth 2.0 for Web Server Applications."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 19,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-06-10T05:11:00.000Z",
        "voteCount": 31,
        "content": "For me its B option"
      },
      {
        "date": "2020-12-24T11:29:00.000Z",
        "voteCount": 31,
        "content": "Only option B make sense to me as per - https://support.google.com/cloudidentity/answer/6262987?hl=en&amp;ref_topic=7558767"
      },
      {
        "date": "2021-02-22T21:25:00.000Z",
        "voteCount": 3,
        "content": "you nailed it. B is correct."
      },
      {
        "date": "2023-11-27T17:15:00.000Z",
        "voteCount": 2,
        "content": "For me its B"
      },
      {
        "date": "2023-11-08T12:05:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2023-11-06T06:40:00.000Z",
        "voteCount": 1,
        "content": "Option B is makes sense"
      },
      {
        "date": "2023-10-05T00:26:00.000Z",
        "voteCount": 1,
        "content": "B. In Cloud Identity, set up SSO with a third-party identity provider with Google as a service provider."
      },
      {
        "date": "2023-09-07T07:56:00.000Z",
        "voteCount": 3,
        "content": "When you use SSO for Cloud Identity or Google Workspace, your external IdP is the SAML IdP and Google is the SAML service provider.\nhttps://cloud.google.com/architecture/identity/single-sign-on#single_sign-on_process"
      },
      {
        "date": "2023-09-03T00:48:00.000Z",
        "voteCount": 1,
        "content": "B is the correct answer as c and d are not SAML"
      },
      {
        "date": "2023-09-02T03:21:00.000Z",
        "voteCount": 1,
        "content": "Chat GPT has suggested option A, here's the reason it gave for why B is not the right option-\n\nsetting up SSO with a third-party identity provider with Google as a service provider, is typically used when your organization wants to use an external SSO IdP, not Google Cloud Identity, to authenticate users."
      },
      {
        "date": "2023-08-25T00:18:00.000Z",
        "voteCount": 1,
        "content": "B. In Cloud Identity, set up SSO with a third-party identity provider with Google as a service provider."
      },
      {
        "date": "2023-06-14T19:08:00.000Z",
        "voteCount": 1,
        "content": "B is correct, only one that makes sense.\nC &amp; D are OAuth not SAML, and A says Google as IDP..."
      },
      {
        "date": "2022-11-02T06:44:00.000Z",
        "voteCount": 1,
        "content": "For me b is the ans"
      },
      {
        "date": "2022-10-11T08:30:00.000Z",
        "voteCount": 2,
        "content": "B is the correct answer"
      },
      {
        "date": "2022-10-07T23:14:00.000Z",
        "voteCount": 1,
        "content": "B. In Cloud Identity, set up SSO with a third-party identity provider with Google as a service provider."
      },
      {
        "date": "2022-08-17T11:12:00.000Z",
        "voteCount": 1,
        "content": "Only option B make sense to me"
      },
      {
        "date": "2022-08-08T22:45:00.000Z",
        "voteCount": 1,
        "content": "Same and OAuth were two different standards! Google Account is an IdP on its on, whereas question calls for external IdP. Definitely B."
      },
      {
        "date": "2022-06-23T13:50:00.000Z",
        "voteCount": 1,
        "content": "Option B"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 106,
    "url": "https://www.examtopics.com/discussions/google/view/22207-exam-associate-cloud-engineer-topic-1-question-106/",
    "body": "Your organization has a dedicated person who creates and manages all service accounts for Google Cloud projects. You need to assign this person the minimum role for projects. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd the user to roles/iam.roleAdmin role.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd the user to roles/iam.securityAdmin role.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd the user to roles/iam.serviceAccountUser role.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd the user to roles/iam.serviceAccountAdmin role.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 15,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-12-24T11:35:00.000Z",
        "voteCount": 65,
        "content": "Whoever say C is right answer, please read the question 1000000000 times if not understand - \"Your organization has a dedicated person who creates and manages all service accounts for Google Cloud projects. \" Dedicated person who creates and manages all service... Now read below;\nTo allow a user to manage service accounts, grant one of the following roles:\n\nService Account User (roles/iam.serviceAccountUser): Includes permissions to list service accounts, get details about a service account, and impersonate a service account.\nService Account Admin (roles/iam.serviceAccountAdmin): Includes permissions to list service accounts and get details about a service account. Also includes permissions to create, update, and delete service accounts, and to view or change the IAM policy on a service account.\nNow look in which role mentioned \"CREATE\"?\nObviously - roles/iam.serviceAccountAdmin....... So Answer is????\n1M% - D only"
      },
      {
        "date": "2022-01-18T02:19:00.000Z",
        "voteCount": 28,
        "content": "Calm down Jamal, don't pull out the knife..."
      },
      {
        "date": "2023-06-14T09:22:00.000Z",
        "voteCount": 3,
        "content": "He's right. Calm down. Save that bomb for later."
      },
      {
        "date": "2023-12-01T03:31:00.000Z",
        "voteCount": 1,
        "content": "no, i want to choose C"
      },
      {
        "date": "2024-09-06T12:30:00.000Z",
        "voteCount": 1,
        "content": "obviously C - 1B%"
      },
      {
        "date": "2020-06-05T09:22:00.000Z",
        "voteCount": 42,
        "content": "The right answer is D."
      },
      {
        "date": "2023-11-27T17:18:00.000Z",
        "voteCount": 2,
        "content": "No doubt, is D"
      },
      {
        "date": "2023-11-23T04:32:00.000Z",
        "voteCount": 2,
        "content": "D\n\nAs per the documentation:\nhttps://cloud.google.com/iam/docs/understanding-roles#iam.serviceAccountUser\nhttps://cloud.google.com/iam/docs/understanding-roles#iam.serviceAccountAdmin"
      },
      {
        "date": "2023-11-06T06:47:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is D"
      },
      {
        "date": "2023-09-07T08:02:00.000Z",
        "voteCount": 3,
        "content": "roles/iam.serviceAccountAdmin --&gt; Create and manage service accounts.\nhttps://cloud.google.com/iam/docs/understanding-roles#iam.serviceAccountAdmin\n\nroles/iam.serviceAccountUser --&gt; Run operations as the service account.\nhttps://cloud.google.com/iam/docs/understanding-roles#iam.serviceAccountUser"
      },
      {
        "date": "2023-09-03T00:59:00.000Z",
        "voteCount": 1,
        "content": "D is the right answer, as the persnon needs to create also"
      },
      {
        "date": "2023-08-24T23:32:00.000Z",
        "voteCount": 2,
        "content": "D.Service Account Admin (roles/iam.serviceAccountAdmin): Includes permissions to list service accounts and get details about a service account. Also includes permissions to create, update, and delete service accounts, and to view or change the IAM policy on a service account."
      },
      {
        "date": "2022-06-23T13:54:00.000Z",
        "voteCount": 1,
        "content": "dedicated person who creates and manages all service accounts is key world makes me select D as right answer."
      },
      {
        "date": "2022-06-05T10:44:00.000Z",
        "voteCount": 1,
        "content": "Go for D"
      },
      {
        "date": "2022-02-21T12:45:00.000Z",
        "voteCount": 1,
        "content": "D is the correct answer"
      },
      {
        "date": "2022-01-17T01:23:00.000Z",
        "voteCount": 2,
        "content": "This was there in exam, go with community answers."
      },
      {
        "date": "2022-01-05T14:46:00.000Z",
        "voteCount": 1,
        "content": "D - ServiceAccountUser has no privs on ServiceAccounts so C is wrong."
      },
      {
        "date": "2021-12-13T22:41:00.000Z",
        "voteCount": 1,
        "content": "D is the right option"
      },
      {
        "date": "2021-12-06T04:01:00.000Z",
        "voteCount": 4,
        "content": "To allow a user to manage service accounts, grant one of the following roles:\n\n    Service Account User (roles/iam.serviceAccountUser): Includes permissions to list service accounts, get details about a service account, and impersonate a service account.\n    Service Account Admin (roles/iam.serviceAccountAdmin): Includes permissions to list service accounts and get details about a service account. Also includes permissions to create, update, and delete service accounts, and to view or change the IAM policy on a service account."
      },
      {
        "date": "2021-11-19T13:22:00.000Z",
        "voteCount": 2,
        "content": "D is Correct"
      },
      {
        "date": "2021-11-18T15:05:00.000Z",
        "voteCount": 1,
        "content": "D is correct."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 107,
    "url": "https://www.examtopics.com/discussions/google/view/23068-exam-associate-cloud-engineer-topic-1-question-107/",
    "body": "You are building an archival solution for your data warehouse and have selected Cloud Storage to archive your data. Your users need to be able to access this archived data once a quarter for some regulatory requirements. You want to select a cost-efficient option. Which storage option should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCold Storage",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNearline Storage\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRegional Storage",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMulti-Regional Storage"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 20,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 19,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-07-23T17:21:00.000Z",
        "voteCount": 36,
        "content": "Took ACE last week and the exact question came out. I go with B as i felt A is a trick answer. There is no Cold Storage in GCP."
      },
      {
        "date": "2020-08-13T02:37:00.000Z",
        "voteCount": 1,
        "content": "Hello Teegongkia , \nis the questions are still valid ??\nThanks"
      },
      {
        "date": "2021-04-22T03:34:00.000Z",
        "voteCount": 1,
        "content": "Cold data tiering refers to the storage of less frequently, or sporadically accessed data in low cost media such as HDFS (Hadoop Distributed File System) and cloud storage options including Amazon Web Services (AWS), Google Cloud Platform (GCP), and Azure Data Lake Storage (ADLS) that are managed separately from the SAP HANA database, but still accessible at any time.    blogs.sap.com/2018/12/03/what-is-sap-hana-cold-data-tiering/"
      },
      {
        "date": "2020-06-13T10:32:00.000Z",
        "voteCount": 18,
        "content": "This one is confusing. First, there's no 'Cold' storage. It's Coldline.\nNearline Storage is ideal for data you plan to read or modify on average once per month or less.Coldline Storage is ideal for data you plan to read or modify at most once a quarter.\nhttps://cloud.google.com/storage/docs/storage-classes\n\nSo with the misspelling of 'Cold' and these guys accessing it every 90 days, I'm leaning towards Nearline"
      },
      {
        "date": "2021-02-20T15:16:00.000Z",
        "voteCount": 2,
        "content": "I believe the question is old, when Regional and Multi-Regional were also storage classes of the GCS. \nBefore changes: (Multi-Region, Regional, Nearline, Coldline)\nAfter recent changes we have\n- Storage Classes (Standard, Nearline, Coldline, Archive)\n- Storage Locations (Regional, Dual-region, Multi-Region)\nIt's tricky for exam because we don't to answer according to old version or new version.\nFor the latest version, costs for 1Gb for storing (3 month) + retrieval \nNearline: 0.01 * 3  + 0.01 = 0.04\nColdline: 0.004 * 3 + 0.02 = 0.032\nColdline is more cost effective.\nIf \"Cold\" means Coldline (not Archive) the asnwer is A \nIf \"Cold\" means Archive the answer is B\n \nI hope that \"Cold\" means Coldline. I would try wirh A"
      },
      {
        "date": "2021-05-13T06:24:00.000Z",
        "voteCount": 1,
        "content": "A\uff1a\nGoogle Cloud doc:\nhttps://cloud.google.com/storage/docs/storage-classes#coldline"
      },
      {
        "date": "2022-02-28T00:07:00.000Z",
        "voteCount": 4,
        "content": "Yes, but the question says the data will be accessed once per quarter, Google's documentation tells us that Coldline is most suitable for data accessed less than once per quarter.  This direct part of the question tells us how we must answer."
      },
      {
        "date": "2020-11-10T08:01:00.000Z",
        "voteCount": 4,
        "content": "It's a typo. Google wouldn't force to consume knowledge that is a non-best practice from Google. Asnwer is A."
      },
      {
        "date": "2021-05-09T08:54:00.000Z",
        "voteCount": 8,
        "content": "For Google, these exams are just another business."
      },
      {
        "date": "2021-03-08T11:34:00.000Z",
        "voteCount": 12,
        "content": "\"Cold\" is not a typo. I took the exam today and the answers appeared exactly as listed here."
      },
      {
        "date": "2020-12-14T05:17:00.000Z",
        "voteCount": 17,
        "content": "At the page for data archiving (https://cloud.google.com/storage/archival) the first paragraph says: \"Coldline is also ideal for cold storage\u2014data your business expects to touch less than once a quarter.\"\nSo there is such thing as Cold storage according to Google. \n\nAlso at (https://cloud.google.com/storage/docs/storage-classes#archive) they talk about Cold storage:  \"Cold data storage - Archived data, such as data stored for legal or regulatory reasons, can be stored at low cost as Archive Storage, yet still be available if you need it.\""
      },
      {
        "date": "2021-06-02T02:57:00.000Z",
        "voteCount": 1,
        "content": "Thanks for bringing this up. Really helpfull."
      },
      {
        "date": "2021-06-02T03:04:00.000Z",
        "voteCount": 2,
        "content": "This line indicates that cold storage term is used for  - archival / coldline\n\"With low latency and a consistent API across Cloud Storage, Archive and Coldline introduce cold storage you can actually use\" https://cloud.google.com/storage/archival"
      },
      {
        "date": "2024-09-27T03:31:00.000Z",
        "voteCount": 1,
        "content": "There is no such thing as Cold Storage, only Coldline storage. \nNearline storage is ideal for data you plan to read or modify on average once per month or less. For example, if you want to continuously add files to Cloud Storage and plan to access those files once a month for analysis, Nearline storage is a great choice.\n\nhttps://cloud.google.com/storage/docs/storage-classes"
      },
      {
        "date": "2024-05-26T01:01:00.000Z",
        "voteCount": 1,
        "content": "For data accessed less than once a year, Archive\n\nOk, once a quarter, is 4 times per year, so Cold Storage is not correct for this"
      },
      {
        "date": "2024-05-09T02:45:00.000Z",
        "voteCount": 1,
        "content": "we have to consider it as coldline storage"
      },
      {
        "date": "2024-04-22T01:51:00.000Z",
        "voteCount": 1,
        "content": "i think it's  B"
      },
      {
        "date": "2024-03-23T21:04:00.000Z",
        "voteCount": 2,
        "content": "once a quarter &lt;=&gt; 90 days\n=&gt; A. Cold Storage"
      },
      {
        "date": "2024-02-08T04:48:00.000Z",
        "voteCount": 1,
        "content": "users need to be able to access this archived data once a quarter .\n\nB is correct."
      },
      {
        "date": "2023-11-27T17:20:00.000Z",
        "voteCount": 2,
        "content": "Nearline - 30 days\nColdline - 60 days\nSo, the answer is A"
      },
      {
        "date": "2023-11-19T09:23:00.000Z",
        "voteCount": 1,
        "content": "It actually depends on the company. But in case of this question,  you have to follow what google said in the course which is, \"Nearline\" is when you are accessing once a month when coldline where you want to access the data once every quarter. So its A"
      },
      {
        "date": "2023-11-06T06:58:00.000Z",
        "voteCount": 1,
        "content": "Coldline storage option is the answer"
      },
      {
        "date": "2023-10-28T03:08:00.000Z",
        "voteCount": 1,
        "content": "Its option A : cold storage \nCold data storage - Archived data, such as data stored for legal or regulatory reasons, can be stored at low cost as Archive storage, yet still be available if you need it.\nhttps://cloud.google.com/storage/docs/storage-classes#archive"
      },
      {
        "date": "2023-10-23T01:40:00.000Z",
        "voteCount": 1,
        "content": "Cold Storage is not a correct category, it should be cold line in case if considered as solution"
      },
      {
        "date": "2023-10-17T16:38:00.000Z",
        "voteCount": 1,
        "content": "Cold data storage = Archive storage as per this: https://cloud.google.com/storage/docs/storage-classes#archive so I will suggest Nearline storage as the correct answer"
      },
      {
        "date": "2023-10-05T00:33:00.000Z",
        "voteCount": 1,
        "content": "A. Cold Storage \n Coldline storage is designed for data that is accessed roughly less than once a quarter2. It offers 99% availability and is cost-efficient compared to other storage classes2. This makes it a suitable choice for archival solutions where data needs to be accessed infrequently, such as once a quarter for regulatory requirements"
      },
      {
        "date": "2023-09-07T08:06:00.000Z",
        "voteCount": 2,
        "content": "Most appropriate answer here would be Coldline storage, but since this is not present in the given options, so next best answer is Nearline storage.\nCold storage is not a valid cloud storage class."
      },
      {
        "date": "2023-09-01T18:41:00.000Z",
        "voteCount": 1,
        "content": "Standard storage -- Hot storage\nNearline storage  -- Warm storage\nColdline &amp; Archive storage --- Cold storage"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 108,
    "url": "https://www.examtopics.com/discussions/google/view/22312-exam-associate-cloud-engineer-topic-1-question-108/",
    "body": "A team of data scientists infrequently needs to use a Google Kubernetes Engine (GKE) cluster that you manage. They require GPUs for some long-running, non- restartable jobs. You want to minimize cost. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable node auto-provisioning on the GKE cluster.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a VerticalPodAutscaler for those workloads.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a node pool with preemptible VMs and GPUs attached to those VMs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a node pool of instances with GPUs, and enable autoscaling on this node pool with a minimum size of 1."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 35,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 23,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-06-06T09:06:00.000Z",
        "voteCount": 72,
        "content": "If you need something for long-running, non- restartable jobs you dont use preemptible VMs\n\nThink answer is D."
      },
      {
        "date": "2021-04-10T03:28:00.000Z",
        "voteCount": 25,
        "content": "Incorrect options are  \nB. VerticalPodAutscaler scales PODS based on the app you deploy.\n   For handle infrequently GPU access, you need infrequently GPU nodes \n   VerticalAutscaler Pod deployed on a non GPU node it useless, \n   [We cant have the node always have GPU for infrequent requests]\nC. Preemptible VMs cant last long \nD. For infrequent access, you don't want to have a permanent homogenous cluster.\n\n\nThe correct option is \"A\"\nauto-provisioning = Attaches and deletes node pools to cluster based on the requirements.\nHence creating a GPU node pool, and auto-scaling would be better\n\nhttps://cloud.google.com/kubernetes-engine/docs/how-to/node-auto-provisioning"
      },
      {
        "date": "2021-11-22T06:25:00.000Z",
        "voteCount": 12,
        "content": "A is not correct because you can't add a GPU node to an existing GKE cluster \n\nLimitations\nBefore using GPUs on GKE, keep in mind the following limitations:\n\nYou cannot add GPUs to existing node pools.\nGPU nodes cannot be live migrated during maintenance events.\nGPUs are only supported with general-purpose N1 machine types.\nGPUs are not supported in Windows Server node pools\n\nREF: https://cloud.google.com/kubernetes-engine/docs/how-to/gpus#limitations\n\nSo the answer should be D"
      },
      {
        "date": "2021-12-03T10:09:00.000Z",
        "voteCount": 5,
        "content": "Your reference says existing \"node pools\" not GKE cluster.  Auto-provisioning creates new \"node pools\":   https://cloud.google.com/kubernetes-engine/docs/how-to/node-auto-provisioning"
      },
      {
        "date": "2021-12-07T01:07:00.000Z",
        "voteCount": 3,
        "content": "but node pools are homogenous, so how can we be sure that option A will create a GPU node pool"
      },
      {
        "date": "2022-05-31T04:56:00.000Z",
        "voteCount": 9,
        "content": "https://cloud.google.com/kubernetes-engine/docs/how-to/node-auto-provisioning\nNode auto-provisioning creates node pools based on the following information:\n\nCPU, memory and ephemeral storage resource requests.\nGPU requests\nPending Pods' node affinities and label selectors.\nPending Pods' node taints and tolerations."
      },
      {
        "date": "2021-10-11T22:47:00.000Z",
        "voteCount": 7,
        "content": "I do agree A is the answer. Since this is for infrequent needs, autoscaling in letter D is not cost effective as it will always run min. of 1 instance. If we need to infrequently use a cluster, the nodes should be able to adjust based on the current need.\n\n\"With node auto-provisioning, new node pools are created and deleted automatically.\" https://cloud.google.com/kubernetes-engine/docs/how-to/node-auto-provisioning"
      },
      {
        "date": "2024-02-22T03:39:00.000Z",
        "voteCount": 1,
        "content": "If the answer was \"Create auto-provisioning node pool\"  or demand is not about GPU resources I'll agree with A too, but there is a limitation about existing node pools and GPU, so enabling of auto-provisioning will not create GPU nodes. Need to create separate GPU pool then enable auto-provisioning for it."
      },
      {
        "date": "2021-07-24T03:39:00.000Z",
        "voteCount": 3,
        "content": "I think using NAP is the correct answer.\n\u2192Node Auto Provisioning (NAP a.k.a., Nodepool Auto Provisioning)\nThere is an introduction of NAP described below on the blog.\n\n&gt;The above recommendations optimize for cost. NAP, for instance, reduces costs by taking down nodes during underutilized periods.\n\nhttps://cloud.google.com/blog/products/containers-kubernetes/best-practices-for-creating-a-highly-available-gke-cluster"
      },
      {
        "date": "2021-12-21T17:24:00.000Z",
        "voteCount": 7,
        "content": "they \"require GPUs\" - so after checking in Udemy practice tests there is similar question there. And the D answer seems to be the best fit for our scenario here.\n\n\"This option is the most optimal solution for the requirement. Rather than recreating all nodes, you create a new node pool with GPU enabled. You then modify the pod specification to target particular GPU types by adding node selector to your workload's Pod specification. You still have a single cluster, so you pay Kubernetes cluster management fee for just one cluster, thus minimizing the cost.\" Still better option than creating new GKE cluster with GPUs.\nRef: https://cloud.google.com/kubernetes-engine/docs/how-to/gpus\nRef: https://cloud.google.com/kubernetes-engine/pricing"
      },
      {
        "date": "2024-09-19T19:57:00.000Z",
        "voteCount": 1,
        "content": "Changing the answer"
      },
      {
        "date": "2024-09-19T19:49:00.000Z",
        "voteCount": 2,
        "content": "D is right answer"
      },
      {
        "date": "2024-08-29T10:23:00.000Z",
        "voteCount": 1,
        "content": "It's A. Shit gets only auto-provisioned when your devs actually deploy something that requires a GPU. It doesn't run permanently by default thus saves costs since it only gets provisioned when neededn."
      },
      {
        "date": "2024-08-24T01:54:00.000Z",
        "voteCount": 1,
        "content": "A is correct. If the application requires a GPU then auto-provisioning will provision a vm with a GPU"
      },
      {
        "date": "2024-05-26T00:59:00.000Z",
        "voteCount": 1,
        "content": "\"Enable node auto-provisioning\" with GPU will not works due to limitation \"You cannot add GPUs to existing node pools\""
      },
      {
        "date": "2024-03-18T05:23:00.000Z",
        "voteCount": 1,
        "content": "Selected Answer: D"
      },
      {
        "date": "2024-03-05T11:10:00.000Z",
        "voteCount": 1,
        "content": "You are not able to add GPUs to existing node pools. This significantly impacts the viability of option A.\n\nMy reasoning for D: A dedicated GPU node pool allows configuring those nodes with specific instance types, disk sizes, etc., ensuring the best fit for the long-running jobs. While it incurs some cost even with a minimum size of 1, it might still be more cost-efficient than full auto-provisioning if the jobs are infrequent but require a predictable baseline capacity. Separating GPU and non-GPU workloads can improve resource scheduling and prevent potential conflicts."
      },
      {
        "date": "2024-02-22T02:02:00.000Z",
        "voteCount": 2,
        "content": "In my opinion, more sense has A, but then i read again and again the answer - \"Enable node auto-provisioning\" with GPU will not works due to limitation \"You cannot add GPUs to existing node pools\". If \"A\" was like \"Create GPU node pool with enabled auto-provisioning\" this will be correct answer, in in our case should be D"
      },
      {
        "date": "2024-01-11T19:57:00.000Z",
        "voteCount": 1,
        "content": "The most cost-effective option for your scenario would be **C. Create a node pool with preemptible VMs and GPUs attached to those VMs**.\n\nPreemptible VMs are Google Cloud's excess compute capacity. They are up to 80% cheaper than regular instances, making them a cost-effective choice for fault-tolerant workloads that do not require continuous availability\u00b3. \n\nHowever, please note that preemptible VMs are subject to availability and can be preempted if Google Cloud requires access to those resources, but they will be a good choice if the jobs can tolerate occasional preemptions\u00b3.\n\nWhile options A, B, and D could also be used in certain scenarios, they may not provide the same level of cost-effectiveness for long-running, non-restartable jobs that require GPUs\u2075. Always consider the nature of your workloads and their tolerance for interruptions when choosing the right solution."
      },
      {
        "date": "2024-01-01T13:56:00.000Z",
        "voteCount": 1,
        "content": "The ans is D,since they require gpu"
      },
      {
        "date": "2023-12-28T22:34:00.000Z",
        "voteCount": 1,
        "content": "The correct option is \"D\""
      },
      {
        "date": "2023-12-05T11:16:00.000Z",
        "voteCount": 2,
        "content": "It is A \nNode auto-provisioning creates node pools based on the following information:\n\nCPU, memory, and ephemeral storage resource requests.\nGPU requests.\nPending Pods' node affinities and label selectors.\nPending Pods' node taints and tolerations.\n\nhttps://cloud.google.com/kubernetes-engine/docs/concepts/node-auto-provisioning"
      },
      {
        "date": "2023-11-27T17:22:00.000Z",
        "voteCount": 1,
        "content": "For me is A"
      },
      {
        "date": "2023-11-27T05:34:00.000Z",
        "voteCount": 1,
        "content": "Think answer is D."
      },
      {
        "date": "2023-12-26T14:08:00.000Z",
        "voteCount": 1,
        "content": ".... any reason?"
      },
      {
        "date": "2023-11-23T04:51:00.000Z",
        "voteCount": 1,
        "content": "Best option is A: https://cloud.google.com/kubernetes-engine/docs/concepts/node-auto-provisioning#how-it-works"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 109,
    "url": "https://www.examtopics.com/discussions/google/view/24144-exam-associate-cloud-engineer-topic-1-question-109/",
    "body": "Your organization has user identities in Active Directory. Your organization wants to use Active Directory as their source of truth for identities. Your organization wants to have full control over the Google accounts used by employees for all Google services, including your Google Cloud Platform (GCP) organization. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Google Cloud Directory Sync (GCDS) to synchronize users into Cloud Identity.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the cloud Identity APIs and write a script to synchronize users to Cloud Identity.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExport users from Active Directory as a CSV and import them to Cloud Identity via the Admin Console.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAsk each employee to create a Google account using self signup. Require that each employee use their company email address and password."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 12,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-06-26T06:07:00.000Z",
        "voteCount": 28,
        "content": "Ans is A\n\nhttps://tools.google.com/dlpage/dirsync/"
      },
      {
        "date": "2020-08-20T12:10:00.000Z",
        "voteCount": 21,
        "content": "Correct Answer (A):\n\nDirectory Sync\nGoogle Cloud Directory Sync enables administrators to synchronize users, groups and other data from an Active Directory/LDAP service to their Google Cloud domain directory\n\nhttps://tools.google.com/dlpage/dirsync/"
      },
      {
        "date": "2023-11-06T07:45:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is A"
      },
      {
        "date": "2023-09-07T08:25:00.000Z",
        "voteCount": 2,
        "content": "https://support.google.com/a/answer/106368?hl=en\nhttps://cloud.google.com/architecture/identity/federating-gcp-with-active-directory-synchronizing-user-accounts\nhttps://cloud.google.com/architecture/identity/federating-gcp-with-active-directory-introduction"
      },
      {
        "date": "2023-09-03T01:10:00.000Z",
        "voteCount": 1,
        "content": "A is the correct answer as it help you to sybchronize users"
      },
      {
        "date": "2023-08-25T00:00:00.000Z",
        "voteCount": 1,
        "content": "A. Use Google Cloud Directory Sync (GCDS) to synchronize users into Cloud Identity."
      },
      {
        "date": "2022-12-03T07:39:00.000Z",
        "voteCount": 1,
        "content": "A....is right"
      },
      {
        "date": "2022-06-23T14:04:00.000Z",
        "voteCount": 2,
        "content": "A is right, this is part of Tutorials Dojo practice test"
      },
      {
        "date": "2022-06-05T11:32:00.000Z",
        "voteCount": 1,
        "content": "Go for A"
      },
      {
        "date": "2022-03-14T03:54:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2022-01-07T23:52:00.000Z",
        "voteCount": 3,
        "content": "A is right"
      },
      {
        "date": "2021-11-19T13:31:00.000Z",
        "voteCount": 4,
        "content": "A is Correct"
      },
      {
        "date": "2021-05-13T19:38:00.000Z",
        "voteCount": 3,
        "content": "A is correct"
      },
      {
        "date": "2021-03-25T03:19:00.000Z",
        "voteCount": 1,
        "content": "A is correct. Use Google Cloud Directory Sync (GCDS) to synchronize users into Cloud Identity."
      },
      {
        "date": "2021-02-24T15:33:00.000Z",
        "voteCount": 3,
        "content": "\u2022 A. Use Google Cloud Directory Sync (GCDS) to synchronize users into Cloud Identity."
      },
      {
        "date": "2020-11-24T09:38:00.000Z",
        "voteCount": 1,
        "content": "This is A , you can use Google Cloud Sync"
      },
      {
        "date": "2020-11-22T04:22:00.000Z",
        "voteCount": 1,
        "content": "\u2022\tA. Use Google Cloud Directory Sync (GCDS) to synchronize users into Cloud Identity."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 110,
    "url": "https://www.examtopics.com/discussions/google/view/22733-exam-associate-cloud-engineer-topic-1-question-110/",
    "body": "You have successfully created a development environment in a project for an application. This application uses Compute Engine and Cloud SQL. Now you need to create a production environment for this application. The security team has forbidden the existence of network routes between these 2 environments and has asked you to follow Google-recommended practices. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new project, enable the Compute Engine and Cloud SQL APIs in that project, and replicate the setup you have created in the development environment.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new production subnet in the existing VPC and a new production Cloud SQL instance in your existing project, and deploy your application using those resources.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new project, modify your existing VPC to be a Shared VPC, share that VPC with your new project, and replicate the setup you have in the development environment in that new project in the Shared VPC.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAsk the security team to grant you the Project Editor role in an existing production project used by another division of your company. Once they grant you that role, replicate the setup you have in the development environment in that project."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 9,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-09-09T07:12:00.000Z",
        "voteCount": 41,
        "content": "A - correct. Best practice is to create a new project for each environment, such as production and testing. There are no routes between VPCs in these projects by default, so that satisfies the requirement by the security team. \nB. Nope. not best practice and allows communication. \nC. While this is best practice to create a new project for a different environment, it explicitly breaks the security team's rule of having no path between environments by nature of the shared VPC. The shared VPC allows entities in both VPCs to communicate as if they were in the same VPC. That's definitely wrong. \nD. One - not best practice to replicate in the setup in that project. Two - why do they suddenly need the project editor rule? Just a bad answer. Wrong."
      },
      {
        "date": "2020-06-10T05:14:00.000Z",
        "voteCount": 35,
        "content": "Correct answer is A."
      },
      {
        "date": "2021-07-28T23:45:00.000Z",
        "voteCount": 4,
        "content": "Correct answer!"
      },
      {
        "date": "2023-11-27T17:26:00.000Z",
        "voteCount": 2,
        "content": "A\nGoogle's best practices says \"create a new project for each environment."
      },
      {
        "date": "2023-11-06T07:50:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is A"
      },
      {
        "date": "2023-09-07T08:37:00.000Z",
        "voteCount": 1,
        "content": "According to Google recommended practices, you should create a separate project for different environments (dev, test, and prod). Also, the question has forbidden the existence of these environments so shared VPC cannot be used."
      },
      {
        "date": "2023-09-03T01:12:00.000Z",
        "voteCount": 1,
        "content": "A is the correct answer, as it satisy tyhe requirement of security team , no commiunicatiojn , as  option c allows coummnication"
      },
      {
        "date": "2023-08-25T00:02:00.000Z",
        "voteCount": 1,
        "content": "A. Create a new project, enable the Compute Engine and Cloud SQL APIs in that project, and replicate the setup you have created in the development environment."
      },
      {
        "date": "2022-10-17T18:15:00.000Z",
        "voteCount": 1,
        "content": "Satisfies requirements by the security team"
      },
      {
        "date": "2022-10-13T04:43:00.000Z",
        "voteCount": 1,
        "content": "make sense"
      },
      {
        "date": "2022-08-10T22:58:00.000Z",
        "voteCount": 1,
        "content": "A is definitely the answer."
      },
      {
        "date": "2022-08-06T15:32:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/architecture/framework/system-design/resource-management#decouple"
      },
      {
        "date": "2022-07-19T22:22:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is A."
      },
      {
        "date": "2022-07-08T14:02:00.000Z",
        "voteCount": 1,
        "content": "Technically we should create a new VPC if the network is not shared. Creating resources in a new project even within a new subnet will not separate unless firewall rules are not explicitly denying the traffic. The best answer is to create a shared VPC where DEV and PROD are service projects. \nMy Answer is: C"
      },
      {
        "date": "2022-08-06T15:24:00.000Z",
        "voteCount": 3,
        "content": "Why you want to share environments? they should be isolated. Therefore Answer should be A."
      },
      {
        "date": "2022-06-05T18:16:00.000Z",
        "voteCount": 1,
        "content": "Go for A\nI thought that the correct answer was the C , but the question did not say to communicate both environments."
      },
      {
        "date": "2022-03-18T21:21:00.000Z",
        "voteCount": 1,
        "content": "Ans: A\nAgreed!"
      },
      {
        "date": "2021-06-24T06:42:00.000Z",
        "voteCount": 6,
        "content": "Should be A\nit's a best practice \"to have one project per application per environment.\" - https://cloud.google.com/docs/enterprise/best-practices-for-enterprise-organizations#project-structure"
      },
      {
        "date": "2021-05-20T07:21:00.000Z",
        "voteCount": 1,
        "content": "A answer"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 111,
    "url": "https://www.examtopics.com/discussions/google/view/22213-exam-associate-cloud-engineer-topic-1-question-111/",
    "body": "Your management has asked an external auditor to review all the resources in a specific project. The security team has enabled the Organization Policy called<br>Domain Restricted Sharing on the organization node by specifying only your Cloud Identity domain. You want the auditor to only be able to view, but not modify, the resources in that project. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAsk the auditor for their Google account, and give them the Viewer role on the project.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAsk the auditor for their Google account, and give them the Security Reviewer role on the project.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a temporary account for the auditor in Cloud Identity, and give that account the Viewer role on the project.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a temporary account for the auditor in Cloud Identity, and give that account the Security Reviewer role on the project."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 16,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-06-08T07:41:00.000Z",
        "voteCount": 52,
        "content": "C - https://cloud.google.com/iam/docs/roles-audit-logging#scenario_external_auditors"
      },
      {
        "date": "2020-07-02T07:58:00.000Z",
        "voteCount": 7,
        "content": "This guy is right!"
      },
      {
        "date": "2020-08-20T14:21:00.000Z",
        "voteCount": 21,
        "content": "Correct Answer is (C):\n\nroles/viewer\tRead access to all resources.\tGet and list access for all resources.\n\nUsing primitive roles\nThe following table lists the primitive roles that you can grant to access a project, the description of what the role does, and the permissions bundled within that role. Avoid using primitive roles except when absolutely necessary. These roles are very powerful, and include a large number of permissions across all Google Cloud services. For more details on when you should use primitive roles, see the Identity and Access Management FAQ.\n\nIAM predefined roles are much more granular, and allow you to carefully manage the set of permissions that your users have access to. See Understanding Roles for a list of roles that can be granted at the project level. Creating custom roles can further increase the control you have over user permissions.\n\nhttps://cloud.google.com/resource-manager/docs/access-control-proj#using_primitive_roles"
      },
      {
        "date": "2024-06-23T11:49:00.000Z",
        "voteCount": 1,
        "content": "the key word is \"organisation Policy called Domain Restricted sharing.\" his external google account wont work"
      },
      {
        "date": "2024-03-12T02:11:00.000Z",
        "voteCount": 1,
        "content": "CORRECT ANSWER IS C"
      },
      {
        "date": "2023-12-05T11:46:00.000Z",
        "voteCount": 3,
        "content": "Domain Restricted Sharing: Since your organization has the Domain Restricted Sharing policy enabled, sharing resources with accounts outside your Cloud Identity domain isn't allowed. Therefore, options A and B, which involve using the auditor's Google account, aren't feasible."
      },
      {
        "date": "2023-11-27T17:27:00.000Z",
        "voteCount": 1,
        "content": "C, without doubt"
      },
      {
        "date": "2023-11-23T05:09:00.000Z",
        "voteCount": 2,
        "content": "D\n\nAs per the documentation, Security Reviewer is more narrow role than the basic Viewer role: \nhttps://cloud.google.com/iam/docs/understanding-roles#iam.securityReviewer\nhttps://cloud.google.com/iam/docs/understanding-roles#viewer"
      },
      {
        "date": "2023-11-19T09:31:00.000Z",
        "voteCount": 3,
        "content": "It could be A, But C is more practical and you don't have to give the auditor extra 3 seconds of work, and yourself for deleting him after he finishes"
      },
      {
        "date": "2023-11-06T07:54:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is C"
      },
      {
        "date": "2023-09-07T08:43:00.000Z",
        "voteCount": 1,
        "content": "The Resource Manager provides a domain restriction constraint that can be used in organization policies to limit resource sharing based on domain or organization resource. This constraint allows you to restrict the set of identities that are allowed to be used in Identity and Access Management policies.\nOrganization policies can use this constraint to limit resource sharing to identities that belong to a particular organization resource.\nhttps://cloud.google.com/resource-manager/docs/organization-policy/restricting-domains"
      },
      {
        "date": "2023-09-03T01:19:00.000Z",
        "voteCount": 1,
        "content": "C is more correct"
      },
      {
        "date": "2023-08-24T23:29:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer is (C):"
      },
      {
        "date": "2023-06-11T11:51:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer is (C):  \n\nAnswer A is wrong because we can't use the the auditor Google account, security team has enabled the Organization Policy specifying only one Cloud Identity domain."
      },
      {
        "date": "2023-05-07T11:20:00.000Z",
        "voteCount": 1,
        "content": "Answer is definitely C\nPlease review this as it seems to be looked over in the other comments\nhttps://cloud.google.com/resource-manager/docs/organization-policy/restricting-domains\n(a google account that isn't part of the domain will not work unless you specifically allow exceptions at the project level and that was not defined in the answers)"
      },
      {
        "date": "2023-04-17T05:21:00.000Z",
        "voteCount": 1,
        "content": "i believe it is C"
      },
      {
        "date": "2023-01-11T17:47:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is C. A is not correct. You can not ask someone to create a personal google account. He/she has no obligation to do so"
      },
      {
        "date": "2023-01-07T07:12:00.000Z",
        "voteCount": 2,
        "content": "From: https://cloud.google.com/iam/docs/job-functions/auditing#scenario_external_auditors\n\n\"The organization creates a Google group for these external auditors and adds the current auditor to the group. This group is monitored and is typically granted access to the dashboard application.\n\nDuring normal access, the auditors' Google group is only granted access to view the historic logs stored in BigQuery. If any anomalies are discovered, the group is granted permission to view the actual Cloud Logging Admin Activity logs via the dashboard's elevated access mode. At the end of each audit period, the group's access is then revoked.\""
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 112,
    "url": "https://www.examtopics.com/discussions/google/view/25016-exam-associate-cloud-engineer-topic-1-question-112/",
    "body": "You have a workload running on Compute Engine that is critical to your business. You want to ensure that the data on the boot disk of this workload is backed up regularly. You need to be able to restore a backup as quickly as possible in case of disaster. You also want older backups to be cleaned automatically to save on cost. You want to follow Google-recommended practices. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Cloud Function to create an instance template.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a snapshot schedule for the disk using the desired interval.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a cron job to create a new disk from the disk using gcloud.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Cloud Task to create an image and export it to Cloud Storage."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-08-20T14:37:00.000Z",
        "voteCount": 25,
        "content": "Correct Answer (B):\nBest practices for persistent disk snapshots\nYou can create persistent disk snapshots at any time, but you can create snapshots more quickly and with greater reliability if you use the following best practices.\n\nCreating frequent snapshots efficiently\nUse snapshots to manage your data efficiently.\n\nCreate a snapshot of your data on a regular schedule to minimize data loss due to unexpected failure.\n\nImprove performance by eliminating excessive snapshot downloads and by creating an image and reusing it.\n\nSet your snapshot schedule to off-peak hours to reduce snapshot time.\n\nSnapshot frequency limits\nCreating snapshots from persistent disks\nYou can snapshot your disks at most once every 10 minutes. If you want to issue a burst of requests to snapshot your disks, you can issue at most 6 requests in 60 minutes.\n\nIf the limit is exceeded, the operation fails and returns the following error:\n\nhttps://cloud.google.com/compute/docs/disks/snapshot-best-practices"
      },
      {
        "date": "2020-07-07T07:57:00.000Z",
        "voteCount": 21,
        "content": "B is correct for this question"
      },
      {
        "date": "2020-08-25T10:43:00.000Z",
        "voteCount": 3,
        "content": "Question: One cannot delete the old disk when using snapshot, right?"
      },
      {
        "date": "2020-09-07T10:41:00.000Z",
        "voteCount": 6,
        "content": "Snapshots and disks are independent objects con GCP, you could create a snapshot form disk and then delete the disk, the snapshot will stay in place. Actually, you could use this snapshot to create a new disk, assign to another VM, mount it, and use it (all the information that the original disk had at the time of the snapshot will still be there)."
      },
      {
        "date": "2021-12-07T02:38:00.000Z",
        "voteCount": 6,
        "content": "In snapshot schedule, there is autodelete and you can specify the days after which auto delete can happen"
      },
      {
        "date": "2024-05-26T00:49:00.000Z",
        "voteCount": 3,
        "content": "2020 is B. Now, 2024, better solution es backup&amp;dr"
      },
      {
        "date": "2024-03-12T02:13:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is B. Automatic snapshot and deletion as per the need."
      },
      {
        "date": "2023-11-27T17:29:00.000Z",
        "voteCount": 2,
        "content": "B\nthe others make no sense at all"
      },
      {
        "date": "2023-11-06T08:00:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is B"
      },
      {
        "date": "2023-09-07T08:48:00.000Z",
        "voteCount": 1,
        "content": "create a snapshot schedule to regularly and automatically back up your zonal and regional persistent disks. Use snapshot schedules as a best practice to back up your Compute Engine workloads.\nA snapshot retention policy defines how long you want to keep your snapshots.\nhttps://cloud.google.com/compute/docs/disks/scheduled-snapshots\nhttps://cloud.google.com/compute/docs/disks/scheduled-snapshots#retention_policy"
      },
      {
        "date": "2023-09-03T01:21:00.000Z",
        "voteCount": 1,
        "content": "B is the correct Answer, as you can create the snapshot as per your requirment"
      },
      {
        "date": "2022-08-07T13:25:00.000Z",
        "voteCount": 2,
        "content": "Create a snapshot schedule for the disk using the desired interval."
      },
      {
        "date": "2022-07-08T14:19:00.000Z",
        "voteCount": 1,
        "content": "Snapshot is a better option because they are incremental and you can configure them to consolidate and delete snapshots that are not required for recovery. Image can also provide this functionality but the image is full backup which is inefficient in cases where the content of the file system is changing frequently."
      },
      {
        "date": "2022-06-23T14:09:00.000Z",
        "voteCount": 2,
        "content": "B is right"
      },
      {
        "date": "2022-06-05T19:55:00.000Z",
        "voteCount": 1,
        "content": "Go for B"
      },
      {
        "date": "2022-03-18T21:32:00.000Z",
        "voteCount": 2,
        "content": "Ans: B"
      },
      {
        "date": "2022-03-06T03:07:00.000Z",
        "voteCount": 4,
        "content": "say no more:\nhttps://cloud.google.com/compute/docs/disks/scheduled-snapshots\n\"Use snapshot schedules as a best practice to back up your Compute Engine workloads.\""
      },
      {
        "date": "2021-11-19T13:34:00.000Z",
        "voteCount": 1,
        "content": "The right Ans is : B"
      },
      {
        "date": "2021-05-13T19:59:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2021-03-25T03:25:00.000Z",
        "voteCount": 1,
        "content": "B is correct. Create a snapshot schedule for the disk using the desired interval."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 113,
    "url": "https://www.examtopics.com/discussions/google/view/24973-exam-associate-cloud-engineer-topic-1-question-113/",
    "body": "You need to assign a Cloud Identity and Access Management (Cloud IAM) role to an external auditor. The auditor needs to have permissions to review your<br>Google Cloud Platform (GCP) Audit Logs and also to review your Data Access logs. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAssign the auditor the IAM role roles/logging.privateLogViewer. Perform the export of logs to Cloud Storage.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAssign the auditor the IAM role roles/logging.privateLogViewer. Direct the auditor to also review the logs for changes to Cloud IAM policy.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAssign the auditor's IAM user to a custom role that has logging.privateLogEntries.list permission. Perform the export of logs to Cloud Storage.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAssign the auditor's IAM user to a custom role that has logging.privateLogEntries.list permission. Direct the auditor to also review the logs for changes to Cloud IAM policy."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-08-24T08:52:00.000Z",
        "voteCount": 65,
        "content": "Correct Answer is (B):\n\nBackground\nGoogle Cloud provides Cloud Audit Logs, which is an integral part of Cloud Logging. It consists of two log streams for each project: Admin Activity and Data Access.\nAdmin Activity logs contain log entries for API calls or other administrative actions that modify the configuration or metadata of resources. Admin Activity logs are always enabled. There is no charge for your Admin Activity audit logs.\nData Access logs record API calls that create, modify, or read user-provided data. Data Access audit logs are disabled by default because they can be large.\n\n\nlogging.viewer: The logging.viewer role gives the security admin team the ability to view the Admin Activity logs.\nlogging.privateLogViewer : The logging.privateLogViewer role gives the ability to view the Data Access logs."
      },
      {
        "date": "2020-08-24T08:55:00.000Z",
        "voteCount": 24,
        "content": "Correct Answer is (B): (Continuation).\nScenario: External auditors\nIn this scenario, audit logs for an organization are aggregated and exported to a central sink location. A third-party auditor is granted access several \ntimes a year to review the organization's audit logs. The auditor is not authorized to view PII data in the Admin Activity logs. \n\nDuring normal access, the auditors' Google group is only granted access to view the historic logs stored in BigQuery. If any anomalies are discovered, \nthe group is granted permission to view the actual Cloud Logging Admin Activity logs via the dashboard's elevated access mode. At the end of each audit period,\nthe group's access is then revoked.\n\nData is redacted using Cloud DLP before being made accessible for viewing via the dashboard application."
      },
      {
        "date": "2020-08-24T08:55:00.000Z",
        "voteCount": 21,
        "content": "Correct Answer is (B): (Continuation).\nThe table below explains IAM logging roles that an Organization Administrator can grant to the service account used by the dashboard, \nas well as the resource level at which the role is granted: \n\nlogging.viewer\tOrganization\tDashboard service account\tThe logging.viewer role permits the service account to read the Admin Activity logs in Cloud Logging.\nbigquery.dataViewer\tBigQuery dataset\tDashboard service account\tThe bigquery.dataViewer role permits the service account used by the dashboard application \nto read the exported Admin Activity logs."
      },
      {
        "date": "2020-07-06T23:19:00.000Z",
        "voteCount": 17,
        "content": "for me B is the correct answer.."
      },
      {
        "date": "2020-11-11T03:29:00.000Z",
        "voteCount": 8,
        "content": "Yes, B is correct because:\n1) Question doesn't ask us to export and store logs for any long period of time.\n2) Custom role with only logging.privateLogEntries.list permission won't let the auditor to access Log Exporer at all (https://cloud.google.com/logging/docs/access-control#console_permissions - Minimal read-only access: logging.logEntries.list)"
      },
      {
        "date": "2024-01-01T07:44:00.000Z",
        "voteCount": 2,
        "content": "There is no need to export logs to Cloud Storage for the auditor to review them unless there's a specific requirement or preference for reviewing them outside the GCP environment. The Logging service provides the necessary tools for log viewing and querying within the console.\n\nDirecting the auditor to review logs for changes to Cloud IAM policy is part of their duties to ensure that the IAM policies have been correctly managed and modified. This does not require a separate permission as the privateLogViewer role already provides the necessary access."
      },
      {
        "date": "2023-11-06T08:25:00.000Z",
        "voteCount": 2,
        "content": "The correct answer is B"
      },
      {
        "date": "2023-10-17T01:11:00.000Z",
        "voteCount": 1,
        "content": "No logs in cloud storage since reviewer won't have access to it"
      },
      {
        "date": "2023-09-07T08:56:00.000Z",
        "voteCount": 1,
        "content": "- The Logs Viewer role (roles/logging.viewer) gives you read-only access to Admin Activity, Policy Denied, and System Event audit logs. If you have just this role, you cannot view Data Access audit logs that are in the _Default bucket.\n- The Private Logs Viewer role(roles/logging.privateLogViewer) includes the permissions contained in roles/logging.viewer, plus the ability to read Data Access audit logs in the _Default bucket.\nTherefore, no need to export logs to Cloud storage explicitly, the _Default bucket sink access is already provided from the above role.\nhttps://cloud.google.com/iam/docs/audit-logging#audit_log_permissions"
      },
      {
        "date": "2023-09-03T01:24:00.000Z",
        "voteCount": 1,
        "content": "b is the correect answer"
      },
      {
        "date": "2023-08-24T23:19:00.000Z",
        "voteCount": 1,
        "content": "B. Assign the auditor the IAM role roles/logging.privateLogViewer. Direct the auditor to also review the logs for changes to Cloud IAM policy."
      },
      {
        "date": "2023-08-03T00:17:00.000Z",
        "voteCount": 1,
        "content": "This answer is similar to answer choice B, but it suggests creating a custom role for the auditor that includes the \"logging.privateLogEntries.list\" permission. While this would provide the auditor with access to the necessary logs, directing them to also review Cloud IAM policy logs is not relevant to their request. Therefore, this answer is also not correct."
      },
      {
        "date": "2023-08-03T00:15:00.000Z",
        "voteCount": 1,
        "content": "Correct Ans: B"
      },
      {
        "date": "2022-12-25T23:17:00.000Z",
        "voteCount": 1,
        "content": "I also think B"
      },
      {
        "date": "2022-06-23T14:11:00.000Z",
        "voteCount": 1,
        "content": "B is right. Similar practice question in tutorials dojo"
      },
      {
        "date": "2022-05-23T23:27:00.000Z",
        "voteCount": 1,
        "content": "B is correct ans"
      },
      {
        "date": "2022-02-21T12:59:00.000Z",
        "voteCount": 3,
        "content": "B is the correct answer"
      },
      {
        "date": "2021-12-19T08:19:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer is (B)"
      },
      {
        "date": "2021-11-07T22:25:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer is B."
      },
      {
        "date": "2021-10-09T18:31:00.000Z",
        "voteCount": 3,
        "content": "A is the correct answer. Exporting logging data to Cloud Storage is ideal, and  'Cloud IAM Policy' is not mentioned in this question."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 114,
    "url": "https://www.examtopics.com/discussions/google/view/25234-exam-associate-cloud-engineer-topic-1-question-114/",
    "body": "You are managing several Google Cloud Platform (GCP) projects and need access to all logs for the past 60 days. You want to be able to explore and quickly analyze the log contents. You want to follow Google-recommended practices to obtain the combined logs for all projects. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNavigate to Stackdriver Logging and select resource.labels.project_id=\"*\"",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Stackdriver Logging Export with a Sink destination to a BigQuery dataset. Configure the table expiration to 60 days.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Stackdriver Logging Export with a Sink destination to Cloud Storage. Create a lifecycle rule to delete objects after 60 days.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a Cloud Scheduler job to read from Stackdriver and store the logs in BigQuery. Configure the table expiration to 60 days."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-07-30T12:18:00.000Z",
        "voteCount": 26,
        "content": "Its B."
      },
      {
        "date": "2020-09-12T06:19:00.000Z",
        "voteCount": 11,
        "content": "The question is to view log past 60 days. B, c, D talks about deleting an object or truncation of table data"
      },
      {
        "date": "2020-09-12T06:20:00.000Z",
        "voteCount": 3,
        "content": "Answer should be A"
      },
      {
        "date": "2020-09-12T06:29:00.000Z",
        "voteCount": 4,
        "content": "Also A specifically talks about aggregation"
      },
      {
        "date": "2020-09-20T12:20:00.000Z",
        "voteCount": 2,
        "content": "Also by default, you have a lot of flexibility when viewing logging in stack driver , to filter and query."
      },
      {
        "date": "2020-09-29T08:24:00.000Z",
        "voteCount": 3,
        "content": "what about  minimum retention is 30 days ? is it true ?"
      },
      {
        "date": "2020-10-01T06:23:00.000Z",
        "voteCount": 3,
        "content": "Ur correct so minimally is 30 for data access logs https://cloud.google.com/logging/quotas\nthen B is the way to go."
      },
      {
        "date": "2024-05-26T00:45:00.000Z",
        "voteCount": 3,
        "content": "2024, there is not \"Stackdriver Logging Export, but for 2020 it is B"
      },
      {
        "date": "2024-03-27T05:51:00.000Z",
        "voteCount": 1,
        "content": "resource.labels.project_id=\"*\" is not a correct query because \"*\" returns 0 records so option A is not a correct answer"
      },
      {
        "date": "2024-01-01T07:58:00.000Z",
        "voteCount": 3,
        "content": "When it comes to log data, you're typically dealing with high-volume time-series data that is partitioned by time (e.g., by day). In such cases, setting a partition expiration is often more appropriate because it ensures that you're continuously retaining a rolling window of log data (for example, the last 60 days' worth) and automatically purging older data, rather than deleting the entire table at once after a certain period."
      },
      {
        "date": "2024-01-01T07:59:00.000Z",
        "voteCount": 2,
        "content": "In BigQuery, setting an expiration time for tables can be applied in two contexts:\n\nTable Expiration:\n\nWhen you set a table expiration time at the table level, it applies to the entire table. This means that the entire table will be deleted once the specified expiration time has elapsed since the table's creation time.\nPartition Expiration:\n\nFor partitioned tables, you can set a partition expiration time, which applies to individual partitions within the table. Each partition's data will be deleted once the specified expiration time has elapsed since the creation of that specific partition.\nThis is particularly useful for time-series data, like logs, where you might want to only keep recent data and allow older data to be automatically purged."
      },
      {
        "date": "2023-12-07T09:48:00.000Z",
        "voteCount": 2,
        "content": "I dont get the options"
      },
      {
        "date": "2023-11-27T17:32:00.000Z",
        "voteCount": 2,
        "content": "I guess it's B"
      },
      {
        "date": "2023-11-06T09:32:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is B"
      },
      {
        "date": "2023-09-07T09:20:00.000Z",
        "voteCount": 2,
        "content": "Provides storage of log entries in BigQuery datasets. You can use big data analysis capabilities on the stored logs. Logging sinks stream logging data into BigQuery in small batches, which lets you query data without running a load job.\nYou can set a default table expiration time at the dataset level, or you can set a table's expiration time when the table is created. A table's expiration time is often referred to as \"time to live\" or TTL. When a table expires, it is deleted along with all of the data it contains.\nhttps://cloud.google.com/logging/docs/export/configure_export_v2#overview\nhttps://cloud.google.com/bigquery/docs/managing-tables#updating_a_tables_expiration_time"
      },
      {
        "date": "2023-09-03T01:27:00.000Z",
        "voteCount": 1,
        "content": "B is thecorrect answer, we can use bq to get 60 days logs and analyse"
      },
      {
        "date": "2023-08-24T23:06:00.000Z",
        "voteCount": 1,
        "content": "B. Create a Stackdriver Logging Export with a Sink destination to a BigQuery dataset. Configure the table expiration to 60 days."
      },
      {
        "date": "2023-03-30T18:19:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/architecture/security-log-analytics"
      },
      {
        "date": "2022-07-12T22:12:00.000Z",
        "voteCount": 5,
        "content": "All options are wrong , they are talking about deletion after 60 days, but questions asks us to analyse logs of past 60 days"
      },
      {
        "date": "2023-01-30T01:20:00.000Z",
        "voteCount": 1,
        "content": "You are absolutely wrong - meaning of \"past 60 days\" is same as \"last 60 days\" in that sentence."
      },
      {
        "date": "2022-06-23T14:13:00.000Z",
        "voteCount": 1,
        "content": "B is right for sure"
      },
      {
        "date": "2022-06-10T04:14:00.000Z",
        "voteCount": 2,
        "content": "B is the correct answer."
      },
      {
        "date": "2021-10-11T23:25:00.000Z",
        "voteCount": 6,
        "content": "I believe B is the answer.\n\nAll that matters in this scenario is the logs for the past 60 days. \nWe can use BigQuery to analyze contents so C is incorrect. We need to configure a BQ as the sink for the logs export so we can query and analyze log data in the future. Therefore D is incorrect.\nhttps://cloud.google.com/logging/docs/audit/best-practices#export-best-practices\n\nSince we only care about the logs within 60 days, we can set the expiration time to 60 to retain only the logs within that time frame. Once data is beyond 60 days old, it wouldn't be included in future analyzations. \nhttps://cloud.google.com/bigquery/docs/managing-tables#updating_a_tables_expiration_time"
      },
      {
        "date": "2022-03-06T03:53:00.000Z",
        "voteCount": 1,
        "content": "I think here we have the case described in details:\nhttps://cloud.google.com/architecture/exporting-stackdriver-logging-for-security-and-access-analytics"
      },
      {
        "date": "2021-10-09T20:03:00.000Z",
        "voteCount": 1,
        "content": "D should be the correct answer. To 'quickly analyze', you need to use BQ, next, you always need access to the logs 'for past 60days'. This means you have to export logs on a daily basis. You don't want to do this job manually right?"
      },
      {
        "date": "2021-10-09T20:21:00.000Z",
        "voteCount": 3,
        "content": "My apologies, B is correct... 'Sink' can route logging data to BQ  automatically."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 115,
    "url": "https://www.examtopics.com/discussions/google/view/24983-exam-associate-cloud-engineer-topic-1-question-115/",
    "body": "You need to reduce GCP service costs for a division of your company using the fewest possible steps. You need to turn off all configured services in an existing<br>GCP project. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Verify that you are assigned the Project Owners IAM role for this project. 2. Locate the project in the GCP console, click Shut down and then enter the project ID.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Verify that you are assigned the Project Owners IAM role for this project. 2. Switch to the project in the GCP console, locate the resources and delete them.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Verify that you are assigned the Organizational Administrator IAM role for this project. 2. Locate the project in the GCP console, enter the project ID and then click Shut down.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Verify that you are assigned the Organizational Administrators IAM role for this project. 2. Switch to the project in the GCP console, locate the resources and delete them."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 19,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-07-07T01:06:00.000Z",
        "voteCount": 43,
        "content": "for me is A the correct answer"
      },
      {
        "date": "2020-07-14T17:47:00.000Z",
        "voteCount": 19,
        "content": "A - I reproduced in my project"
      },
      {
        "date": "2023-11-06T09:38:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is A"
      },
      {
        "date": "2023-09-07T09:30:00.000Z",
        "voteCount": 4,
        "content": "https://cloud.google.com/resource-manager/docs/access-control-proj#permissions\nhttps://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects"
      },
      {
        "date": "2023-09-03T01:28:00.000Z",
        "voteCount": 1,
        "content": "a is the correct answer"
      },
      {
        "date": "2023-08-24T22:23:00.000Z",
        "voteCount": 1,
        "content": "Verify that you are assigned the Project Owners IAM role for this project. 2. Locate the project in the GCP console, click Shut down and then enter the project ID."
      },
      {
        "date": "2023-07-10T14:47:00.000Z",
        "voteCount": 1,
        "content": "A is the correct answer"
      },
      {
        "date": "2023-05-07T11:35:00.000Z",
        "voteCount": 2,
        "content": "Answer is A\nhttps://support.google.com/googleapi/answer/6251787?hl=en#zippy=%2Cshut-down-a-project"
      },
      {
        "date": "2023-04-17T23:37:00.000Z",
        "voteCount": 1,
        "content": "option A"
      },
      {
        "date": "2023-04-17T23:36:00.000Z",
        "voteCount": 1,
        "content": "i believe it's option A.\n\nroles/owner\tOwner\tAll Editor permissions and permissions for the following actions:\nManage roles and permissions for a project and all resources within the project.\nSet up billing for a project."
      },
      {
        "date": "2023-01-20T23:38:00.000Z",
        "voteCount": 1,
        "content": "According to \"https://cloud.google.com/resource-manager/docs/access-control-org#resourcemanager.organizationAdmin\" and \"https://cloud.google.com/resource-manager/docs/access-control-proj#basic_roles\", only the project owner (and project deleter (roles/resourcemanager.projectDeleter)) can delete a project.\nSo, answer A is the technically correct one."
      },
      {
        "date": "2022-12-22T12:09:00.000Z",
        "voteCount": 2,
        "content": "project not org , A all d way"
      },
      {
        "date": "2022-07-11T16:30:00.000Z",
        "voteCount": 2,
        "content": "A is right \nHint : You need to turn off all configured services in an ***existing GCP project***.\nSo C and D out from selection"
      },
      {
        "date": "2022-07-02T12:26:00.000Z",
        "voteCount": 1,
        "content": "Tried and tested"
      },
      {
        "date": "2022-07-02T07:41:00.000Z",
        "voteCount": 2,
        "content": "correct ans is A"
      },
      {
        "date": "2022-06-27T14:23:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/run/docs/tutorials/gcloud\n\nClean up\n\nIn the dialog, type the project ID, and then click Shut down to delete the project."
      },
      {
        "date": "2022-06-23T14:16:00.000Z",
        "voteCount": 2,
        "content": "A is right"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 116,
    "url": "https://www.examtopics.com/discussions/google/view/25018-exam-associate-cloud-engineer-topic-1-question-116/",
    "body": "You are configuring service accounts for an application that spans multiple projects. Virtual machines (VMs) running in the web-applications project need access to BigQuery datasets in crm-databases-proj. You want to follow Google-recommended practices to give access to the service account in the web-applications project. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGive \u05d2\u20acproject owner\u05d2\u20ac for web-applications appropriate roles to crm-databases-proj.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGive \u05d2\u20acproject owner\u05d2\u20ac role to crm-databases-proj and the web-applications project.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGive \u05d2\u20acproject owner\u05d2\u20ac role to crm-databases-proj and bigquery.dataViewer role to web-applications.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGive bigquery.dataViewer role to crm-databases-proj and appropriate roles to web-applications.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 24,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 17,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-07-07T14:18:00.000Z",
        "voteCount": 30,
        "content": "D cuz u just need read for DB at the other project"
      },
      {
        "date": "2020-07-08T00:32:00.000Z",
        "voteCount": 4,
        "content": "U re right, D is the correct answee"
      },
      {
        "date": "2021-04-12T23:47:00.000Z",
        "voteCount": 6,
        "content": "See the option correctly, as the web app needs access to the big query datasets we have to give access to the web app the data viewer role to only read the datasets! Hence, C"
      },
      {
        "date": "2020-12-25T02:55:00.000Z",
        "voteCount": 2,
        "content": "Question didn't specify if the required access is Read only or more, its saying \"access\" which could be write permissions as well. I will go with C"
      },
      {
        "date": "2021-10-11T23:45:00.000Z",
        "voteCount": 10,
        "content": "It is D because you're right, the question doesn't specify any specific kind of access, however, we need to follow the principle of least-privilege. Hence, we can only assume that read-only access is needed. \n\nbigquery.dataViewer should be assigned to the group of analysts in the crm-databases-proj project. \nhttps://cloud.google.com/bigquery/docs/access-control-examples#read_access_to_data_in_a_different_project"
      },
      {
        "date": "2020-12-25T02:58:00.000Z",
        "voteCount": 5,
        "content": "U r right, it D. why to give \"project owner\" as stated on C. correct answer is D"
      },
      {
        "date": "2021-06-25T03:43:00.000Z",
        "voteCount": 9,
        "content": "but why giving bigquery.dataViewer to crm-databases-proj. we should give for web-application."
      },
      {
        "date": "2022-03-21T13:48:00.000Z",
        "voteCount": 3,
        "content": "You can technically give bigquery.dataviewer to crm-databases-proj service account then create a Key and use that key on the VMs, there for making it correct to use D as answer but is way to dumb I would prefer C BUUUUUUT WHY would I give Project Owner to crm-databases-proj? they really do not evaluate your knowladge"
      },
      {
        "date": "2020-07-07T08:05:00.000Z",
        "voteCount": 11,
        "content": "C is correct.."
      },
      {
        "date": "2022-03-15T05:19:00.000Z",
        "voteCount": 5,
        "content": "THAT SO DUM"
      },
      {
        "date": "2023-12-10T02:01:00.000Z",
        "voteCount": 2,
        "content": "I meet BigQuery the first time ever personly"
      },
      {
        "date": "2021-06-25T03:40:00.000Z",
        "voteCount": 3,
        "content": "But why giving project owner role to crm-databases-proj ?"
      },
      {
        "date": "2024-10-13T03:14:00.000Z",
        "voteCount": 1,
        "content": "It is D. Tested and approved that you do not need BigQuery permissions on the web-app project to access data on the bq tables stored in the crm-dbs project. You do need bq permissions for the SA on the crm project and compute permissions for the same SA on the web-app project. Then, using this SA on a VM on the web-app server, you can access data from bq on the crm-dbs project"
      },
      {
        "date": "2024-08-19T02:34:00.000Z",
        "voteCount": 1,
        "content": "Explanation:\n    Least Privilege Principle: This approach adheres to the principle of least privilege by granting the minimum necessary permissions.\n    Project Owner: The crm-databases-proj project needs full control to manage its resources.\n    bigquery.dataViewer: The web-applications project only needs read access to BigQuery datasets in the crm-databases-proj project.\n\nWhy other options are less suitable:\n\n    A: Giving project owner to web-applications provides unnecessary permissions.\n    B: Giving project owner to both projects grants excessive permissions.\n    D: Giving bigquery.dataViewer to crm-databases-proj is incorrect as this project needs full control over its resources.\n\nBy following option C, you ensure that the web-applications project has the required access to BigQuery datasets without compromising security."
      },
      {
        "date": "2024-06-25T03:18:00.000Z",
        "voteCount": 1,
        "content": "Let's analyze the options:\n\nA &amp; B: Granting \"project owner\" gives excessive permissions, violating the least privilege principle.\nC: Granting \"project owner\" to crm-databases-proj is unnecessary.\nD: Granting \"bigquery.dataViewer\" to crm-databases-proj allows the VM access to datasets and aligns with least privilege. Granting appropriate roles to web-applications secures the web application itself (not shown in this scenario).\nTherefore, option D is the recommended approach."
      },
      {
        "date": "2024-03-07T06:49:00.000Z",
        "voteCount": 1,
        "content": "Project owner role is not required here, so that leaves us with only Option D"
      },
      {
        "date": "2024-01-03T23:54:00.000Z",
        "voteCount": 1,
        "content": "A, b &amp; c is wrong. Keywords is configuring aervice account. A,b &amp; c concerns user account. Correct answer is D"
      },
      {
        "date": "2024-01-01T08:14:00.000Z",
        "voteCount": 3,
        "content": "None of the options is correct. As for D:\nThis option is unclear and potentially misleading. The bigquery.dataViewer role should be assigned specifically to the service account in the web-applications project, not to the crm-databases-proj project."
      },
      {
        "date": "2024-01-01T08:15:00.000Z",
        "voteCount": 1,
        "content": "The ideal approach (not listed in the options) would be:\n\nCreate a service account in the web-applications project specifically for accessing the BigQuery datasets.\nGrant this service account the bigquery.dataViewer role (or another more specific role if different access is needed) on the crm-databases-proj project's BigQuery datasets.\nUse this service account in your VMs in the web-applications project."
      },
      {
        "date": "2023-11-23T05:48:00.000Z",
        "voteCount": 2,
        "content": "Damn!\nAll the four options are correct :-D for the question given :-)"
      },
      {
        "date": "2023-11-06T10:12:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is C"
      },
      {
        "date": "2023-09-03T01:35:00.000Z",
        "voteCount": 1,
        "content": "D is the correct answer, because all other option giveing access  to project owner"
      },
      {
        "date": "2023-08-30T23:54:00.000Z",
        "voteCount": 2,
        "content": "Corrct Answer is D.\nLets just read the options D this way, then it makes sense\nGive service account the bigquery.dataViewer role to crm-databases-proj and service account the appropriate roles to web-applications."
      },
      {
        "date": "2023-08-19T03:30:00.000Z",
        "voteCount": 1,
        "content": "Thanks guys for making that clear for me.\nNow simply guys, among all the answers,  D is giving to the web-application proj the appropriate role, while giving the crm-databases-proj the least privilege role."
      },
      {
        "date": "2023-08-06T03:53:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is D\nAs basic roles (including Owner) should not be used in production environment:"
      },
      {
        "date": "2023-07-23T05:42:00.000Z",
        "voteCount": 1,
        "content": "C is correct.."
      },
      {
        "date": "2023-06-14T09:48:00.000Z",
        "voteCount": 4,
        "content": "I dont get the question. It says \"web-applications project need access to BigQuery datasets in crm-databases-proj\"\n\nAnd all you folks stating C or D is the correct one. Why would we want to give those permissions to the DB? When the question clearly states that the web-app is the one that needs access to the DB?"
      },
      {
        "date": "2023-06-06T06:44:00.000Z",
        "voteCount": 3,
        "content": "It says 'web-applications project need access to BigQuery datasets in crm-databases-proj'. Therefore, give web-applications the BigQuery Data Viewer role - not the other way around. Why would crm-databases-proj need this role in this situation?"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 117,
    "url": "https://www.examtopics.com/discussions/google/view/74558-exam-associate-cloud-engineer-topic-1-question-117/",
    "body": "An employee was terminated, but their access to Google Cloud was not removed until 2 weeks later. You need to find out if this employee accessed any sensitive customer information after their termination. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tView System Event Logs in Cloud Logging. Search for the user's email as the principal.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tView System Event Logs in Cloud Logging. Search for the service account associated with the user.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tView Data Access audit logs in Cloud Logging. Search for the user's email as the principal.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tView the Admin Activity log in Cloud Logging. Search for the service account associated with the user."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 17,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-04-23T23:12:00.000Z",
        "voteCount": 1,
        "content": "Option C is more correct"
      },
      {
        "date": "2024-01-20T08:18:00.000Z",
        "voteCount": 4,
        "content": "Guys, do you know which answers are the correct answers on this website? The website's answers or the most voted ones? I'm preparing for the exam and I need a quick and certain response. Thank you"
      },
      {
        "date": "2024-07-14T10:40:00.000Z",
        "voteCount": 2,
        "content": "Should use discussion to find out correct answer. Also usually you can find fine explain for question"
      },
      {
        "date": "2023-11-06T10:13:00.000Z",
        "voteCount": 2,
        "content": "The correct answer is C"
      },
      {
        "date": "2023-10-28T22:13:00.000Z",
        "voteCount": 2,
        "content": "C. View Data Access audit logs in Cloud Logging. Search for the user's email as the principal.\n\nData Access audit logs provide detailed information about accesses to your Google Cloud resources. By searching for the terminated employee's email address as the principal in the Data Access audit logs, you can track their access to sensitive customer information after their termination. This approach allows you to specifically focus on data access, which is crucial for identifying any unauthorized or suspicious activities related to customer data."
      },
      {
        "date": "2023-09-03T01:37:00.000Z",
        "voteCount": 2,
        "content": "Option C is more correct , as data access logs contain API , from this you can check for it"
      },
      {
        "date": "2023-04-17T23:59:00.000Z",
        "voteCount": 3,
        "content": "I think option C is correct :\nData Access audit logs contain API calls that read the configuration or metadata of resources, as well as user-driven API calls that create, modify, or read user-provided resource data."
      },
      {
        "date": "2023-02-13T22:47:00.000Z",
        "voteCount": 1,
        "content": "C. View Data Access audit logs in Cloud Logging. Search for the user's email as the principal.\n\nData Access audit logs record all activity related to accessing or modifying data, including reading, writing, and deleting operations. By searching for the terminated employee's email as the principal, you can see if they accessed any sensitive customer information after their termination. System Event Logs and Admin Activity logs may not have the details of the data accessed, so Data Access audit logs are the most appropriate option in this scenario."
      },
      {
        "date": "2023-01-06T12:39:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/logging/docs/audit. \nData Access audit logs are disabled by default"
      },
      {
        "date": "2022-09-24T10:30:00.000Z",
        "voteCount": 3,
        "content": "had this question today"
      },
      {
        "date": "2022-08-07T13:59:00.000Z",
        "voteCount": 2,
        "content": "View Data Access audit logs in Cloud Logging. Search for the user's email as the principal"
      },
      {
        "date": "2022-05-24T00:08:00.000Z",
        "voteCount": 2,
        "content": "ANSWER IS C\nAs we want to find out whether the user has accessed the data or not , so Data Acess Logs would be correct option to view that"
      },
      {
        "date": "2022-05-22T08:07:00.000Z",
        "voteCount": 1,
        "content": "I will go with option C."
      },
      {
        "date": "2022-05-12T07:06:00.000Z",
        "voteCount": 3,
        "content": "C. https://cloud.google.com/logging/docs/audit#data-access"
      },
      {
        "date": "2022-04-27T09:48:00.000Z",
        "voteCount": 4,
        "content": "C is the correct answer. We are trying to find out if any sensitive data was accessed. Data access logs are the only logs that show this. C is the only option that mentions data access logs."
      },
      {
        "date": "2022-04-26T01:25:00.000Z",
        "voteCount": 2,
        "content": "Shouldn't the correct option be A here?\nWhat does service account have to do here."
      },
      {
        "date": "2022-04-28T02:52:00.000Z",
        "voteCount": 3,
        "content": "It should be C User activity generally come under audit logs"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 118,
    "url": "https://www.examtopics.com/discussions/google/view/24908-exam-associate-cloud-engineer-topic-1-question-118/",
    "body": "You need to create a custom IAM role for use with a GCP service. All permissions in the role must be suitable for production use. You also want to clearly share with your organization the status of the custom role. This will be the first version of the custom role. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse permissions in your role that use the 'supported' support level for role permissions. Set the role stage to ALPHA while testing the role permissions.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse permissions in your role that use the 'supported' support level for role permissions. Set the role stage to BETA while testing the role permissions.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse permissions in your role that use the 'testing' support level for role permissions. Set the role stage to ALPHA while testing the role permissions.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse permissions in your role that use the 'testing' support level for role permissions. Set the role stage to BETA while testing the role permissions."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-07-13T15:52:00.000Z",
        "voteCount": 99,
        "content": "You need a custom role with permissions supported in prod and you want to publish the status of the role. \nhttps://cloud.google.com/iam/docs/custom-roles-permissions-support\nSUPPORTED\tThe permission is fully supported in custom roles.\nTESTING\tThe permission is being tested to check its compatibility with custom roles. You can include the permission in custom roles, but you might see unexpected behavior. Not recommended for production use.\nNOT_SUPPORTED\tThe permission is not supported in custom roles.\nYou can't use TESTING as it is not good for prod. And you need first version which should be ALPHA. Answer should be A."
      },
      {
        "date": "2020-08-21T07:29:00.000Z",
        "voteCount": 2,
        "content": "good job"
      },
      {
        "date": "2022-03-15T05:45:00.000Z",
        "voteCount": 1,
        "content": "WAY TO GO. VERY CLEAR EXP INDEED"
      },
      {
        "date": "2020-08-24T09:53:00.000Z",
        "voteCount": 32,
        "content": "Correct Answer is (A):\n\nTesting and deploying\nCustom roles include a launch stage, which is stored in the stage property for the role. The launch stage is informational; it helps you keep track of whether each role is ready for widespread use.\n\nEach custom role can have one of the following launch stages:\n\nLaunch stages\nALPHA\tThe role is still being developed or tested, or it includes permissions for Google Cloud services or features that are not yet public. It is not ready for widespread use.\nBETA\tThe role has been tested on a limited basis, or it includes permissions for Google Cloud services or features that are not generally available.\nGA\tThe role has been widely tested, and all of its permissions are for Google Cloud services or features that are generally available."
      },
      {
        "date": "2020-08-24T09:54:00.000Z",
        "voteCount": 13,
        "content": "Correct Answer is (A): Continuation\n\n\n\nSupport levels for permissions in custom roles\nYou can include many, but not all, Identity and Access Management (IAM) permissions in custom roles. Each permission has one of the following support levels:\n\nSupport level\tDescription\nSUPPORTED\tThe permission is fully supported in custom roles.\nTESTING\tThe permission is being tested to check its compatibility with custom roles. You can include the permission in custom roles, but you might see unexpected behavior. Not recommended for production use.\nNOT_SUPPORTED\tThe permission is not supported in custom roles.\n\n\nThe first version of the Custom Role is ALPHA then suitable to productions all permissions in \"Supported\"..."
      },
      {
        "date": "2021-02-25T10:55:00.000Z",
        "voteCount": 2,
        "content": "ESP_SAP\nThere is a discrepancy between your first post and the second post. Compare these two sentences;\n1st POST - ALPHA The role is still being developed or tested, or it includes permissions for Google Cloud services or features that are not yet public. It is not ready for widespread use.\n2nd POST - SUPPORTED The permission is fully supported in custom roles.\n\nAre you still going to go with A ?"
      },
      {
        "date": "2022-09-23T09:59:00.000Z",
        "voteCount": 2,
        "content": "Here ALPHA is for Google cloud feature, only informational. given to identify whether the feature is fully available as a service.\nand SUPPORTED -- is for a custom role which is supported by Google cloud, meaning  any support is provided by Google cloud"
      },
      {
        "date": "2024-06-25T03:21:00.000Z",
        "voteCount": 1,
        "content": "Let's break down the options:\n\nA &amp; B: \"supported\" is the ideal choice for production roles as they are well-tested and documented. BETA is for pre-release features, not initial testing.\nC: \"testing\" permissions are unstable and not suited for production. ALPHA stage is for internal testing before even BETA.\nD: Same issue as option C - \"testing\" permissions are not for production, and ALPHA is an earlier stage than BETA.\nTherefore, the most suitable approach is:\n\nA. Use permissions in your role that use the 'supported' support level for role permissions. Set the role stage to ALPHA while testing the role permissions."
      },
      {
        "date": "2023-12-06T04:10:00.000Z",
        "voteCount": 2,
        "content": "Its A cause in first place it need to be supported not in testing phase because the question is asking it to be in ready phase secondly then needed to be shared in testing phase"
      },
      {
        "date": "2023-11-06T10:20:00.000Z",
        "voteCount": 1,
        "content": "I will go with A"
      },
      {
        "date": "2023-09-07T19:20:00.000Z",
        "voteCount": 1,
        "content": "A is correct answer.\nhttps://cloud.google.com/iam/docs/roles-overview#custom-role-supported-permissions\nhttps://cloud.google.com/iam/docs/roles-overview#custom-role-testing-deploying"
      },
      {
        "date": "2023-09-03T01:40:00.000Z",
        "voteCount": 1,
        "content": "Answer is A, as you need for production and you dont neeed testing for it and you need first version , so it will be ALPHA , not beta"
      },
      {
        "date": "2023-08-24T21:10:00.000Z",
        "voteCount": 1,
        "content": "You need a custom role with permissions supported in prod and you want to publish the status of the role.\nhttps://cloud.google.com/iam/docs/custom-roles-permissions-support\nSUPPORTED The permission is fully supported in custom roles."
      },
      {
        "date": "2022-11-16T21:26:00.000Z",
        "voteCount": 1,
        "content": "why not B?"
      },
      {
        "date": "2023-03-21T15:04:00.000Z",
        "voteCount": 1,
        "content": "Because ...FIRST VERSION... is ALPHA."
      },
      {
        "date": "2022-08-09T18:07:00.000Z",
        "voteCount": 1,
        "content": "It must be suitable for production so Supported permissions only. Plus, it is your first version of the custom role, so you need to check if all is good, then ALPHA."
      },
      {
        "date": "2022-08-07T14:02:00.000Z",
        "voteCount": 1,
        "content": "Use permissions in your role that use the 'supported' support level for role permissions. Set the role stage to ALPHA while testing the role permissions."
      },
      {
        "date": "2022-06-23T14:23:00.000Z",
        "voteCount": 1,
        "content": "A is right"
      },
      {
        "date": "2022-01-26T10:40:00.000Z",
        "voteCount": 2,
        "content": "A is the only right solution."
      },
      {
        "date": "2021-11-24T03:54:00.000Z",
        "voteCount": 2,
        "content": "A is the correct answer"
      },
      {
        "date": "2021-08-19T17:57:00.000Z",
        "voteCount": 3,
        "content": "A - \nSUPPORTED -The permission is fully supported in custom roles. \nrole stage, the stage transitions from ALPHA \u2013&gt; BETA \u2013&gt; GA\nThe only option that satisfies \u201cALPHA\u201d stage with \u201cSUPPORTED\u201d support level is\nUse permissions in your role that use the SUPPORTED support level for role permissions. Set the role stage to ALPHA while testing the role permissions"
      },
      {
        "date": "2021-05-13T20:08:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2021-03-25T04:00:00.000Z",
        "voteCount": 1,
        "content": "A is correct. Use permissions in your role that use the \u05d2\u20ac\u02dcsupported\u05d2\u20ac\u2122 support level for role permissions. Set the role stage to ALPHA while testing the role permissions."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 119,
    "url": "https://www.examtopics.com/discussions/google/view/25019-exam-associate-cloud-engineer-topic-1-question-119/",
    "body": "Your company has a large quantity of unstructured data in different file formats. You want to perform ETL transformations on the data. You need to make the data accessible on Google Cloud so it can be processed by a Dataflow job. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpload the data to BigQuery using the bq command line tool.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpload the data to Cloud Storage using the gsutil command line tool.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpload the data into Cloud SQL using the import function in the console.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpload the data into Cloud Spanner using the import function in the console."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 21,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-07-07T08:10:00.000Z",
        "voteCount": 25,
        "content": "B looks correct. Key work unstructured data"
      },
      {
        "date": "2022-03-02T02:35:00.000Z",
        "voteCount": 3,
        "content": "Also \"different\" file formats, this further supports B as the correct choice."
      },
      {
        "date": "2023-10-28T22:37:00.000Z",
        "voteCount": 6,
        "content": "Google Cloud Storage (GCS) is the recommended service for storing unstructured data like files, images, and backups. If you have large quantities of unstructured data in different file formats that need to be processed with ETL (Extract, Transform, Load) transformations and then processed by a Dataflow job, the typical workflow is to store the raw data in Cloud Storage.\nOnce the data is in Cloud Storage, you can easily access it and perform ETL transformations using Google"
      },
      {
        "date": "2024-01-20T08:19:00.000Z",
        "voteCount": 1,
        "content": "Guys, do you know which answers are the correct answers on this website? The website's answers or the most voted ones? I'm preparing for the exam and I need a quick and certain response. Thank you"
      },
      {
        "date": "2024-01-20T08:19:00.000Z",
        "voteCount": 2,
        "content": "Guys, do you know which answers are the correct answers on this website? The website's answers or the most voted ones? I'm preparing for the exam and I need a quick and certain response. Thank you"
      },
      {
        "date": "2023-11-06T10:21:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is B"
      },
      {
        "date": "2023-09-07T19:23:00.000Z",
        "voteCount": 1,
        "content": "Key term is \"unstructured data in different file formats\". Except B, remaining options are suitable for structured data. So, correct answer is B."
      },
      {
        "date": "2023-09-03T01:41:00.000Z",
        "voteCount": 2,
        "content": "B is correct for Unstructurd DAta its Cloud storage"
      },
      {
        "date": "2023-08-18T06:19:00.000Z",
        "voteCount": 1,
        "content": "why this page ask for contributer access.. i can not access whole questions"
      },
      {
        "date": "2023-08-06T04:01:00.000Z",
        "voteCount": 2,
        "content": "Option B is correct\nCloud Storage is a scalable and cost-effective object storage service that can hold unstructured data of various file formats. Before performing ETL (Extract, Transform, Load) transformations, it's often beneficial to store the raw data in a centralized location, like Cloud Storage."
      },
      {
        "date": "2023-04-18T00:05:00.000Z",
        "voteCount": 1,
        "content": "Answer B"
      },
      {
        "date": "2022-10-23T04:20:00.000Z",
        "voteCount": 1,
        "content": "Cloud Storage as a datalake"
      },
      {
        "date": "2022-08-24T05:06:00.000Z",
        "voteCount": 3,
        "content": "Answer B : \n\"large quantity\" : Cloud Storage or BigQuery\n\"files\" a file is nothing but an Object\n\nSo Cloud Storage is the better option."
      },
      {
        "date": "2022-08-07T14:10:00.000Z",
        "voteCount": 1,
        "content": "Upload the data to Cloud Storage using the gsutil command line tool."
      },
      {
        "date": "2022-06-23T14:25:00.000Z",
        "voteCount": 1,
        "content": "B looks correct"
      },
      {
        "date": "2022-05-18T12:25:00.000Z",
        "voteCount": 4,
        "content": "For unstructured  data use cloud storage. Use Big Query for analytics, data warehouse with structured data"
      },
      {
        "date": "2022-04-25T07:51:00.000Z",
        "voteCount": 2,
        "content": "B looks correct"
      },
      {
        "date": "2022-04-10T15:58:00.000Z",
        "voteCount": 1,
        "content": "Answer is B, Cloud Storage for unstructured data"
      },
      {
        "date": "2022-03-18T22:41:00.000Z",
        "voteCount": 3,
        "content": "Ans: B\n\nWe can upload unstructured data to Cloud storage not to bigquery."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 120,
    "url": "https://www.examtopics.com/discussions/google/view/46576-exam-associate-cloud-engineer-topic-1-question-120/",
    "body": "You need to manage multiple Google Cloud projects in the fewest steps possible. You want to configure the Google Cloud SDK command line interface (CLI) so that you can easily manage multiple projects. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a configuration for each project you need to manage. 2. Activate the appropriate configuration when you work with each of your assigned Google Cloud projects.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a configuration for each project you need to manage. 2. Use gcloud init to update the configuration values when you need to work with a non-default project",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Use the default configuration for one project you need to manage. 2. Activate the appropriate configuration when you work with each of your assigned Google Cloud projects.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Use the default configuration for one project you need to manage. 2. Use gcloud init to update the configuration values when you need to work with a non-default project."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 19,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-03-11T19:46:00.000Z",
        "voteCount": 34,
        "content": "A\nCloud SDK comes with a default configuration. To create multiple configurations, use gcloud config configurations create, and gcloud config configurations activate to switch between them.\n\nhttps://cloud.google.com/sdk/gcloud/reference/config/set"
      },
      {
        "date": "2021-03-11T13:46:00.000Z",
        "voteCount": 8,
        "content": "A. 1. Create a configuration for each project you need to manage. 2. Activate the appropriate configuration when you work with each of your assigned Google Cloud projects."
      },
      {
        "date": "2023-11-20T21:40:00.000Z",
        "voteCount": 2,
        "content": "The correct answer is A"
      },
      {
        "date": "2023-11-06T10:25:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is A"
      },
      {
        "date": "2023-09-07T19:31:00.000Z",
        "voteCount": 2,
        "content": "The gcloud config command group lets you set, view and unset properties used by Google Cloud CLI. A configuration is a set of properties that govern the behavior of gcloud and other Google Cloud CLI tools. The initial default configuration is set when gcloud init is run. You can create additional named configurations using gcloud init or gcloud config configurations create. To switch between configurations, use gcloud config configurations activate. \nhttps://cloud.google.com/sdk/gcloud/reference/config\nhttps://cloud.google.com/sdk/gcloud/reference/config/configurations/activate"
      },
      {
        "date": "2023-09-03T01:43:00.000Z",
        "voteCount": 1,
        "content": "A is the correct answer, as it comes with the default cofiguration and you dont need to update it"
      },
      {
        "date": "2023-08-24T20:59:00.000Z",
        "voteCount": 1,
        "content": "A. 1. Create a configuration for each project you need to manage. 2. Activate the appropriate configuration when you work with each of your assigned Google Cloud projects. kINDLY SHARE COMPLETE QUESTION"
      },
      {
        "date": "2023-02-08T09:59:00.000Z",
        "voteCount": 6,
        "content": "Google Cloud SDK allows you to create multiple configurations for different projects, and you can easily switch between these configurations as needed. To manage multiple projects efficiently, you can create a separate configuration for each project and activate the appropriate configuration when you work with each assigned project. The gcloud config configurations create and gcloud config configurations activate commands allow you to create and activate different configurations. By using different configurations, you can ensure that your CLI commands are always executed in the correct context and against the correct project, without the need to manually change the configuration each time you switch projects."
      },
      {
        "date": "2022-08-24T05:09:00.000Z",
        "voteCount": 2,
        "content": "1. Generate your configurations with \"gcloud config configurations create &lt;config_id&gt; ...\" then activate the one you need according to the project you are working on with \"gcloud config activate &lt;config_id&gt;\""
      },
      {
        "date": "2022-08-07T14:13:00.000Z",
        "voteCount": 1,
        "content": "Create a configuration for each project you need to manage. 2. Activate the appropriate configuration when you work with each of your assigned Google Cloud projects."
      },
      {
        "date": "2022-06-23T14:29:00.000Z",
        "voteCount": 1,
        "content": "A is right.\ngcloud config set project PROJECT_ID\nhttps://cloud.google.com/sdk/gcloud/reference/config/set"
      },
      {
        "date": "2022-05-24T01:25:00.000Z",
        "voteCount": 1,
        "content": "A is correct Answer"
      },
      {
        "date": "2022-05-18T13:07:00.000Z",
        "voteCount": 2,
        "content": "You have to create a config for each project and activate to use it"
      },
      {
        "date": "2022-02-21T13:08:00.000Z",
        "voteCount": 1,
        "content": "A is the correct answer"
      },
      {
        "date": "2021-12-27T06:22:00.000Z",
        "voteCount": 1,
        "content": "why we need to create config, when project is created so its config ."
      },
      {
        "date": "2021-12-13T23:23:00.000Z",
        "voteCount": 1,
        "content": "A is the right option. Multiple configurations. Activate to switch between configurations."
      },
      {
        "date": "2021-12-07T04:29:00.000Z",
        "voteCount": 1,
        "content": "A, C seems hectic, B and D surely are eliminated"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 121,
    "url": "https://www.examtopics.com/discussions/google/view/24975-exam-associate-cloud-engineer-topic-1-question-121/",
    "body": "Your managed instance group raised an alert stating that new instance creation has failed to create new instances. You need to maintain the number of running instances specified by the template to be able to process expected application traffic. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an instance template that contains valid syntax which will be used by the instance group. Delete any persistent disks with the same name as instance names.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an instance template that contains valid syntax that will be used by the instance group. Verify that the instance name and persistent disk name values are not the same in the template.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tVerify that the instance template being used by the instance group contains valid syntax. Delete any persistent disks with the same name as instance names. Set the disks.autoDelete property to true in the instance template.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDelete the current instance template and replace it with a new instance template. Verify that the instance name and persistent disk name values are not the same in the template. Set the disks.autoDelete property to true in the instance template."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 16,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 8,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-02-21T08:39:00.000Z",
        "voteCount": 60,
        "content": "Really tricky question.\nIdeal scenario would be\n1. create new template, while creating ensure that in the new template  disks.autoDelete=true, 3. delete existing persistent disks, 4. make rolling update ...\nIn order to switch to new template we need \"Rolling update\". Unfortunately, it is not mentioned. \n\nWith current options\nC - not correct, we cannot update existing template\nD - not correct, we cannot delete existing template when it is in use (just checked in GCP) (We need rolling update)\nB - will not solve our problem without Rolling update\nA - This is the only option (I know that it can be temporary) that will work without Rolling update according to \nhttps://cloud.google.com/compute/docs/troubleshooting/troubleshooting-migs"
      },
      {
        "date": "2023-10-13T15:01:00.000Z",
        "voteCount": 2,
        "content": "C - you were not told to update existing template. You were told to verify the syntax is correct."
      },
      {
        "date": "2021-03-06T08:38:00.000Z",
        "voteCount": 2,
        "content": "Well reasoned. I'm also going with  A."
      },
      {
        "date": "2023-01-25T04:35:00.000Z",
        "voteCount": 2,
        "content": "Thank you for providing the link."
      },
      {
        "date": "2020-08-20T20:36:00.000Z",
        "voteCount": 44,
        "content": "Correct Answer is (C):\nYour instance template has set the disks.autoDelete option to false for boot persistent disks so that when a VM has been deleted (for example, because of autohealing), the persistent disk was not deleted. When the managed instance group attempted to recreate the VM with the same name, it ran into the same issue where a persistent disk already exists with the same name. Delete the existing persistent disk to resolve the immediate problem and update the instance template to set the disks.autoDelete to true if you would like boot persistent disks to be deleted alongside the instance\n\nhttps://cloud.google.com/compute/docs/instance-groups/creating-groups-of-managed-instances#troubleshooting"
      },
      {
        "date": "2020-12-31T11:56:00.000Z",
        "voteCount": 19,
        "content": "Can't update instance templates, see below"
      },
      {
        "date": "2021-02-27T22:30:00.000Z",
        "voteCount": 12,
        "content": "https://cloud.google.com/compute/docs/instance-templates#how_to_update_instance_templates"
      },
      {
        "date": "2024-09-09T21:04:00.000Z",
        "voteCount": 1,
        "content": "Ensure the template is valid.\nPersistent disk naming: Avoid naming disks the same as instances.\nAutomatic deletion: Set disks.autoDelete to true in the template for automatic disk removal."
      },
      {
        "date": "2024-05-25T09:48:00.000Z",
        "voteCount": 2,
        "content": "Is not C\n\n\"Instance templates are designed to create instances with identical configurations. So you cannot update an existing instance template or change an instance template after you create it.\"\n\nhttps://cloud.google.com/compute/docs/instance-templates#how_to_update_instance_templates"
      },
      {
        "date": "2024-02-13T08:18:00.000Z",
        "voteCount": 1,
        "content": "I have no idea and looking at options and explanations from everyone here A, C or D could be the answers.\nI thought it was D personally, but all answers seem to not make any sense. Usually you can discard one or 2, but here all are the same to me."
      },
      {
        "date": "2023-10-14T01:44:00.000Z",
        "voteCount": 1,
        "content": "All questions are from here."
      },
      {
        "date": "2023-09-19T06:36:00.000Z",
        "voteCount": 1,
        "content": "correct answer is C. It is not updating an instance template, just validate the syntax. The same instance template was working before, so why would you need to edit it? the issue should be in the disk name."
      },
      {
        "date": "2023-09-07T19:35:00.000Z",
        "voteCount": 2,
        "content": "Correct answer is A : Create an instance template that contains valid syntax which will be used by the instance group. Delete any persistent disks with the same name as instance names."
      },
      {
        "date": "2023-02-10T07:47:00.000Z",
        "voteCount": 1,
        "content": "why we dont prefer option B? it says be sure names are different. but option  A deletes discs. if we can change names why to delete them?"
      },
      {
        "date": "2022-11-13T04:18:00.000Z",
        "voteCount": 3,
        "content": "Answer is A\nAs many have mentioned here C and D is straight NO, C talks about updating the instance template and D talks about deleting an instance template BUT as per google documentation you cannot delete an instance template nor update it if it is in use. So A is the only one the makes sense here.\n\nhttps://cloud.google.com/compute/docs/instance-templates#how_to_update_instance_templates"
      },
      {
        "date": "2022-10-23T04:47:00.000Z",
        "voteCount": 2,
        "content": "Vote C\nhttps://cloud.google.com/compute/docs/troubleshooting/troubleshooting-migs\nhttps://cloud.google.com/sdk/gcloud/reference/compute/instances/set-disk-auto-delete"
      },
      {
        "date": "2022-09-03T01:37:00.000Z",
        "voteCount": 1,
        "content": "C is the correct answer.\n1. Ensure you don\u2019t have any persistent disks with the same name as the VM instance.\n2. Ensure the disk autodelete property is turned on (disks.autoDelete set to true).\n3. Ensure instance template syntax is valid"
      },
      {
        "date": "2022-09-03T01:39:00.000Z",
        "voteCount": 1,
        "content": "As described in this article, \"My managed instance group keeps failing to create a VM. What's going on?\"\nhttps://cloud.google.com/compute/docs/instance-groups/creating-groups-of-managed-instances#troubleshooting"
      },
      {
        "date": "2022-08-03T04:04:00.000Z",
        "voteCount": 5,
        "content": "Answer : A\nAs many have mentioned here C and D is straight NO, C talks about updating the instance template and D talks about deleting an instance template BUT as per google documentation you cannot delete an instance template nor update it if it is in use. So A is the only one the makes sense here.\n\nhttps://cloud.google.com/compute/docs/instance-templates#how_to_update_instance_templates"
      },
      {
        "date": "2022-07-25T09:13:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is A\nYou don't have the option to edit an existing instance."
      },
      {
        "date": "2022-07-18T03:02:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is A\nYou don't have the option to edit an existing instance."
      },
      {
        "date": "2022-07-08T00:44:00.000Z",
        "voteCount": 1,
        "content": "You cannot update a template, need to create a new one."
      },
      {
        "date": "2022-07-02T20:35:00.000Z",
        "voteCount": 2,
        "content": "A is correct answer. [Instance template resources are immutable in GCP.]\n\nHow to update instance templates\nInstance templates are designed to create instances with identical configurations. So you cannot update an existing instance template or change an instance template after you create it.\n\nIf you need to make changes to the configuration, create a new instance template. You can create a template based on an existing instance template, or based on an existing instance. You can also override instance template fields when creating a VM instance from an instance template.\nRef : https://cloud.google.com/compute/docs/instance-templates#:~:text=So%20you%20cannot%20update%20an,based%20on%20an%20existing%20instance."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 122,
    "url": "https://www.examtopics.com/discussions/google/view/24994-exam-associate-cloud-engineer-topic-1-question-122/",
    "body": "Your company is moving from an on-premises environment to Google Cloud. You have multiple development teams that use Cassandra environments as backend databases. They all need a development environment that is isolated from other Cassandra instances. You want to move to Google Cloud quickly and with minimal support effort. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Build an instruction guide to install Cassandra on Google Cloud. 2. Make the instruction guide accessible to your developers.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Advise your developers to go to Cloud Marketplace. 2. Ask the developers to launch a Cassandra image for their development work.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Build a Cassandra Compute Engine instance and take a snapshot of it. 2. Use the snapshot to create instances for your developers.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Build a Cassandra Compute Engine instance and take a snapshot of it. 2. Upload the snapshot to Cloud Storage and make it accessible to your developers. 3. Build instructions to create a Compute Engine instance from the snapshot so that developers can do it themselves."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 16,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-07-07T03:49:00.000Z",
        "voteCount": 28,
        "content": "B is correct for me.. launch a solution from marketplace"
      },
      {
        "date": "2020-08-20T21:32:00.000Z",
        "voteCount": 24,
        "content": "Correct Answer is (B):\n\nhttps://medium.com/google-cloud/how-to-deploy-cassandra-and-connect-on-google-cloud-platform-with-a-few-clicks-11ee3d7001d1"
      },
      {
        "date": "2020-12-16T02:11:00.000Z",
        "voteCount": 1,
        "content": "But we are moving from on premises to gcp"
      },
      {
        "date": "2020-09-11T10:45:00.000Z",
        "voteCount": 5,
        "content": "thanks, i always look for your insight"
      },
      {
        "date": "2024-02-13T08:22:00.000Z",
        "voteCount": 1,
        "content": "For me the answer is B for no other reason than it mentions Marketplace, which I imagine google wants you to think about and it's what the majority had selected as well."
      },
      {
        "date": "2023-11-23T06:40:00.000Z",
        "voteCount": 2,
        "content": "B\nWith minimal support - a click away: \nhttps://console.cloud.google.com/marketplace/product/bitnami-launchpad/cassandra?project=fast-art-401415"
      },
      {
        "date": "2023-09-07T19:37:00.000Z",
        "voteCount": 1,
        "content": "Key term is \"move to Google Cloud quickly and with minimal support effort\". Right away you can think of Google Cloud Marketplace in such situations."
      },
      {
        "date": "2023-09-03T10:10:00.000Z",
        "voteCount": 1,
        "content": "B is the correct answer as , it requires the minimal effoort"
      },
      {
        "date": "2023-06-14T10:08:00.000Z",
        "voteCount": 1,
        "content": "D is the right answer not B. Remember the question states moving from on premise to gcloud. Meaning we already have servers build. We would need those image or snapshot so devs can create their own insance.\n\nA and B - start from scratch\nC - Doesnt give permissions to devs so you'll have to create the instance (more support effort)"
      },
      {
        "date": "2023-07-13T09:43:00.000Z",
        "voteCount": 2,
        "content": "nah - D is not the right answer. D says build a Cassandra CE instance. The answer does not refer to taking a snapshot of the on-prem instances, it refers to a snapshot of a Compute Engine VM which you'd have to manually build.\n\nB is correct"
      },
      {
        "date": "2022-10-23T04:57:00.000Z",
        "voteCount": 3,
        "content": "B\nhttps://cloud.google.com/blog/products/databases/open-source-cassandra-now-managed-on-google-cloud"
      },
      {
        "date": "2022-06-23T14:34:00.000Z",
        "voteCount": 1,
        "content": "B absolutely correct, there is no need of manual installs."
      },
      {
        "date": "2022-05-18T13:46:00.000Z",
        "voteCount": 1,
        "content": "B is correct: You want to move to Google Cloud quickly and with minimal support effort.\nhttps://cloud.google.com/marketplace"
      },
      {
        "date": "2022-03-15T23:41:00.000Z",
        "voteCount": 4,
        "content": "I suspect every time an answer features Google Marketplace, that is the correct answer.  In these kinds of exams the purpose is often just to check general product knowledge."
      },
      {
        "date": "2022-02-21T21:20:00.000Z",
        "voteCount": 2,
        "content": "B is the correct answer"
      },
      {
        "date": "2021-12-27T06:57:00.000Z",
        "voteCount": 1,
        "content": "Minimum support when you have steps laydown to your developer to do task on their own. So its D."
      },
      {
        "date": "2021-12-14T00:38:00.000Z",
        "voteCount": 2,
        "content": "B is the right option."
      },
      {
        "date": "2021-11-24T03:59:00.000Z",
        "voteCount": 3,
        "content": "B. its easier to launch from the marketplace"
      },
      {
        "date": "2021-11-22T10:53:00.000Z",
        "voteCount": 1,
        "content": "For me it's B, on upfront it offers the lowest effort to launch Marketplace solutions."
      },
      {
        "date": "2021-11-19T13:54:00.000Z",
        "voteCount": 1,
        "content": "B is Correct"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 123,
    "url": "https://www.examtopics.com/discussions/google/view/79852-exam-associate-cloud-engineer-topic-1-question-123/",
    "body": "You have a Compute Engine instance hosting a production application. You want to receive an email if the instance consumes more than 90% of its CPU resources for more than 15 minutes. You want to use Google services. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a consumer Gmail account. 2. Write a script that monitors the CPU usage. 3. When the CPU usage exceeds the threshold, have that script send an email using the Gmail account and smtp.gmail.com on port 25 as SMTP server.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a Cloud Monitoring Workspace and associate your Google Cloud Platform (GCP) project with it. 2. Create a Cloud Monitoring Alerting Policy that uses the threshold as a trigger condition. 3. Configure your email address in the notification channel.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a Cloud Monitoring Workspace and associate your GCP project with it. 2. Write a script that monitors the CPU usage and sends it as a custom metric to Cloud Monitoring. 3. Create an uptime check for the instance in Cloud Monitoring.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. In Cloud Logging, create a logs-based metric to extract the CPU usage by using this regular expression: CPU Usage: ([0-9] {1,3})% 2. In Cloud Monitoring, create an Alerting Policy based on this metric. 3. Configure your email address in the notification channel."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 19,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-02-13T23:24:00.000Z",
        "voteCount": 5,
        "content": "B\n\nBy setting up a Cloud Monitoring Alerting Policy and configuring the notification channel to an email address, you can receive email alerts when the CPU usage exceeds the threshold that you set. This can be done by using Google Cloud's built-in monitoring and alerting features, which can be more reliable and easier to manage than setting up a custom script or using external email services."
      },
      {
        "date": "2024-05-25T09:37:00.000Z",
        "voteCount": 2,
        "content": "now, 2024, I think is not necessary to create workspace in monitoring..."
      },
      {
        "date": "2024-05-25T09:38:00.000Z",
        "voteCount": 1,
        "content": "Cloud Monitoring metric scope is now"
      },
      {
        "date": "2024-01-11T11:08:00.000Z",
        "voteCount": 2,
        "content": "What is up with blatantly wrong options being marked as the \"correct answer\"??"
      },
      {
        "date": "2023-11-05T00:50:00.000Z",
        "voteCount": 3,
        "content": "The most appropriate and efficient method to set up an alert for high CPU usage on a Google Compute Engine instance using Google's services would be:\nB. \n\n1. Create a Cloud Monitoring Workspace and associate your Google Cloud Platform (GCP) project with it. \n2. Create a Cloud Monitoring Alerting Policy that uses the threshold as a trigger condition. 3. Configure your email address in the notification channel.\nThis option utilizes Google Cloud's built-in Cloud Monitoring service to track the CPU usage and send alerts based on predefined conditions without the need for scripting or managing an email server."
      },
      {
        "date": "2023-01-19T15:28:00.000Z",
        "voteCount": 1,
        "content": "D. 1. In Cloud Logging, create a logs-based metric to extract the CPU usage by using this regular expression: CPU Usage: ([0-9] {1,3})% 2. In Cloud Monitoring, create an Alerting Policy based on this metric. 3. Configure your email address in the notification channel."
      },
      {
        "date": "2023-01-06T05:34:00.000Z",
        "voteCount": 2,
        "content": "Agree, B works"
      },
      {
        "date": "2022-12-01T08:05:00.000Z",
        "voteCount": 1,
        "content": "Agree it's B if we're talking about StackDriver."
      },
      {
        "date": "2022-10-13T13:17:00.000Z",
        "voteCount": 1,
        "content": "is correct"
      },
      {
        "date": "2022-09-21T06:29:00.000Z",
        "voteCount": 3,
        "content": "answer is B, but I would write it this way as stackdriver is deprecated and Operation Suite uses scopes now.\n1. Create a Cloud Monitoring metric scope and associate your Google Cloud Platform (GCP) project with it. \n\n2. Create a Cloud Monitoring Alerting Policy that uses the threshold as a trigger condition. \n\n3. Configure your email address in the notification channel."
      },
      {
        "date": "2022-09-03T10:50:00.000Z",
        "voteCount": 4,
        "content": "The dump has been changed some days ago. This answer was the old and best version:\n1. Create a Stackdriver Workspace, and associate your Google Cloud Platform (GCP) project with it. \n2. Create an Alerting Policy in Stackdriver that uses the threshold as a trigger condition. \n3. Configure your email address in the notification channel."
      },
      {
        "date": "2022-09-18T17:28:00.000Z",
        "voteCount": 1,
        "content": "And the answer is still the same?"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 124,
    "url": "https://www.examtopics.com/discussions/google/view/46586-exam-associate-cloud-engineer-topic-1-question-124/",
    "body": "You have an application that uses Cloud Spanner as a backend database. The application has a very predictable traffic pattern. You want to automatically scale up or down the number of Spanner nodes depending on traffic. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a cron job that runs on a scheduled basis to review Cloud Monitoring metrics, and then resize the Spanner instance accordingly.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Cloud Monitoring alerting policy to send an alert to oncall SRE emails when Cloud Spanner CPU exceeds the threshold. SREs would scale resources up or down accordingly.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Cloud Monitoring alerting policy to send an alert to Google Cloud Support email when Cloud Spanner CPU exceeds your threshold. Google support would scale resources up or down accordingly.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Cloud Monitoring alerting policy to send an alert to webhook when Cloud Spanner CPU is over or under your threshold. Create a Cloud Function that listens to HTTP and resizes Spanner resources accordingly.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 14,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-03-11T14:54:00.000Z",
        "voteCount": 26,
        "content": "D. Create a Cloud Monitoring alerting policy to send an alert to webhook when Cloud Spanner CPU is over or under your threshold. Create a Cloud Function that listens to HTTP and resizes Spanner resources accordingly."
      },
      {
        "date": "2022-08-11T19:12:00.000Z",
        "voteCount": 9,
        "content": "Without knowing that much, you can discard easily B,C as they don't make any sense. Automation should be a key in this answer. Also you should discard \"A\" as with a CronJob you won't spann on time as it will be a fixed time checking. So the only one that is left is D, as just creating an alert and sending it to \"something else\" (in this case a webhook)  in an automated way, should be the common sense way of handling this."
      },
      {
        "date": "2023-01-30T01:59:00.000Z",
        "voteCount": 1,
        "content": "Isn't \"fixed time chacking\" appropriate for quote : very predictable traffic pattern?"
      },
      {
        "date": "2023-02-26T07:34:00.000Z",
        "voteCount": 1,
        "content": "Crossed my mind too, but why check every time when you can trigger a response when it happens.\nPredictability can also be used to determine the threshold."
      },
      {
        "date": "2024-07-21T07:53:00.000Z",
        "voteCount": 1,
        "content": "Why not A if traffic very predictable?"
      },
      {
        "date": "2023-11-13T03:34:00.000Z",
        "voteCount": 2,
        "content": "The most suitable approach to automatically scale the number of Cloud Spanner nodes based on predictable traffic patterns is:\n\nD. Create a Cloud Monitoring alerting policy to send an alert to a webhook when Cloud Spanner CPU is over or under your threshold. Create a Cloud Function that listens to HTTP and resizes Spanner resources accordingly.\n\nThis option utilizes Cloud Monitoring alerts and Cloud Functions to dynamically scale Cloud Spanner resources based on CPU thresholds, providing an automated and responsive solution."
      },
      {
        "date": "2023-09-07T19:40:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is D."
      },
      {
        "date": "2023-09-03T10:19:00.000Z",
        "voteCount": 1,
        "content": "D is the correct Answer as B or C does not do it automatically, and a doesnot use for long spanning"
      },
      {
        "date": "2022-12-24T15:54:00.000Z",
        "voteCount": 3,
        "content": "Why not A is correct as question suggested specific time where as D is like an unpredectiable time?"
      },
      {
        "date": "2023-01-29T18:48:00.000Z",
        "voteCount": 3,
        "content": "Because even though the traffic has a clear pattern, if the traffic changes one day (like a special holiday for ecommerce websites), you wouldn't be able to serve accordingly. It's never a good practice use fixed jobs for time-based traffic issues."
      },
      {
        "date": "2022-12-18T12:57:00.000Z",
        "voteCount": 1,
        "content": "D is definitely correct .. people"
      },
      {
        "date": "2022-07-27T21:00:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is D"
      },
      {
        "date": "2022-06-23T14:37:00.000Z",
        "voteCount": 1,
        "content": "D is correct, It is part of Tutorials Dojo practice test"
      },
      {
        "date": "2022-04-20T16:34:00.000Z",
        "voteCount": 3,
        "content": "Answer is D. The keyword to look for is \"automatically\". A, B, C all have steps that are not automatic. Thus, only D is left."
      },
      {
        "date": "2021-11-19T13:59:00.000Z",
        "voteCount": 3,
        "content": "D is the answer"
      },
      {
        "date": "2021-11-09T09:02:00.000Z",
        "voteCount": 5,
        "content": "https://cloud.google.com/architecture/autoscaling-cloud-spanner"
      },
      {
        "date": "2021-10-13T00:39:00.000Z",
        "voteCount": 3,
        "content": "D is the answer"
      },
      {
        "date": "2021-10-10T18:49:00.000Z",
        "voteCount": 4,
        "content": "It's a tricky question. The answer is 'A'.\nQuestion says traffic pattern is predictable. This means you can schedule the scalability.\nYou can achieve this easily by using Cloud Spanner's API.\n\nhttps://cloud.google.com/spanner/docs/reference/rest/v1/projects.instances/patch"
      },
      {
        "date": "2021-10-24T15:09:00.000Z",
        "voteCount": 6,
        "content": "Really tricky indeed...But if you think about it and you don't stick religiously to \"traffic pattern is predictable\" sentence (that sentence is to trick you), you would think that despite the pattern is very predictable, it could vary at some point in time. With a cron job you can schedule a job using a fixed starting point and a a fixed ending, can't you? Well, what would happen if the app traffic suddenly spikes before your cron job starts running? Or after?. At this point, I go for D, making scalability automatic thanks to Cloud Funtions."
      },
      {
        "date": "2024-02-14T07:56:00.000Z",
        "voteCount": 1,
        "content": "I would say A also, as your explanation doesn't satisfy me, as the question is really clear that traffic is predictable. It doesn't mention any wiggle room."
      },
      {
        "date": "2021-09-29T19:32:00.000Z",
        "voteCount": 5,
        "content": "D 205%"
      },
      {
        "date": "2021-10-12T03:55:00.000Z",
        "voteCount": 4,
        "content": "seriously? 205%?"
      },
      {
        "date": "2021-05-28T16:59:00.000Z",
        "voteCount": 2,
        "content": "Option D. There's an official repository that does something similar to provide autoscaling to Cloud Spanner. https://github.com/cloudspannerecosystem/autoscaler"
      },
      {
        "date": "2021-10-20T00:57:00.000Z",
        "voteCount": 2,
        "content": "Which uses a Cloud Scheduler so answer A"
      },
      {
        "date": "2022-08-06T03:05:00.000Z",
        "voteCount": 1,
        "content": "\"...and then resize the Spanner instance accordingly.\" I think the last sentence says that the resize action is done manually. If so, then D is still the right answer."
      },
      {
        "date": "2022-08-06T03:08:00.000Z",
        "voteCount": 1,
        "content": "For the D option, \"Create a Cloud Function that listens to HTTP and resizes Spanner resources accordingly.\" The listen and resize actions is done by Cloud Function."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 125,
    "url": "https://www.examtopics.com/discussions/google/view/46589-exam-associate-cloud-engineer-topic-1-question-125/",
    "body": "Your company publishes large files on an Apache web server that runs on a Compute Engine instance. The Apache web server is not the only application running in the project. You want to receive an email when the egress network costs for the server exceed 100 dollars for the current month as measured by Google Cloud.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up a budget alert on the project with an amount of 100 dollars, a threshold of 100%, and notification type of \u05d2\u20acemail.\u05d2\u20ac",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up a budget alert on the billing account with an amount of 100 dollars, a threshold of 100%, and notification type of \u05d2\u20acemail.\u05d2\u20ac",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExport the billing data to BigQuery. Create a Cloud Function that uses BigQuery to sum the egress network costs of the exported billing data for the Apache web server for the current month and sends an email if it is over 100 dollars. Schedule the Cloud Function using Cloud Scheduler to run hourly.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Cloud Logging Agent to export the Apache web server logs to Cloud Logging. Create a Cloud Function that uses BigQuery to parse the HTTP response log data in Cloud Logging for the current month and sends an email if the size of all HTTP responses, multiplied by current Google Cloud egress prices, totals over 100 dollars. Schedule the Cloud Function using Cloud Scheduler to run hourly."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 21,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-03-11T15:00:00.000Z",
        "voteCount": 24,
        "content": "C. Export the billing data to BigQuery. Create a Cloud Function that uses BigQuery to sum the egress network costs of the exported billing data for the Apache web server for the current month and sends an email if it is over 100 dollars. Schedule the Cloud Function using Cloud Scheduler to run hourly."
      },
      {
        "date": "2021-03-31T01:41:00.000Z",
        "voteCount": 13,
        "content": "[C]\nI think the keyword here is \"as measured by Google Cloud\". In Answer D you calculate the price yourself, in C you use the billing provided by GCP. Thus I think the Answer is C."
      },
      {
        "date": "2024-02-13T08:29:00.000Z",
        "voteCount": 5,
        "content": "I would never had guessed C or D. I thought it's A, as it's the most straight forward. C or D are ridiculously complex to me."
      },
      {
        "date": "2024-05-13T15:02:00.000Z",
        "voteCount": 2,
        "content": "You need to filter only Network egress costs, so C make more sense..."
      },
      {
        "date": "2023-09-07T19:56:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is C."
      },
      {
        "date": "2023-09-03T10:26:00.000Z",
        "voteCount": 1,
        "content": "C is the correct answer , as it gives you everything the question wants"
      },
      {
        "date": "2023-05-07T12:22:00.000Z",
        "voteCount": 1,
        "content": "Answer is A\n\nFrom this Link:\nhttps://cloud.google.com/load-balancing/docs/ssl\n\nit states this:\nExternal SSL proxy load balancers are intended for non-HTTP(S) traffic. For HTTP(S) traffic, we recommend that you use an external HTTP(S) load balancer.\n\n443 is HTTPS traffic\n\nfor those saying 443 isn't https\nhttps://www.iana.org/assignments/service-names-port-numbers/service-names-port-numbers.xhtml?search=443"
      },
      {
        "date": "2023-05-07T12:25:00.000Z",
        "voteCount": 1,
        "content": "Disregard this was supposed to be for question 127"
      },
      {
        "date": "2022-12-16T21:29:00.000Z",
        "voteCount": 9,
        "content": "Took the exam this week this question is on there, C is the correct answer you need to remember to filter."
      },
      {
        "date": "2022-10-08T06:31:00.000Z",
        "voteCount": 1,
        "content": "C is the correct answer,\nYou export the bill to BigQuery and filter for the Egress cost for the particular application, and send an email if the cost is over 100 dollars, to send an email you need to use cloud function to monitor and trigger based on the conditions."
      },
      {
        "date": "2022-09-19T01:30:00.000Z",
        "voteCount": 2,
        "content": "it can only be the C because \"it's not the only app running\""
      },
      {
        "date": "2022-08-24T05:22:00.000Z",
        "voteCount": 1,
        "content": "[D]\nComplex but works for resume network egres cost of ONLY the Apache instance"
      },
      {
        "date": "2022-08-11T19:25:00.000Z",
        "voteCount": 1,
        "content": "I would say this is the approach without knowing that much:\nA &amp; B discarded as they are not resourced oriented. We need to charge for the apache server (so focusing on the VM where it is hosted) in order to charge this server.\n\nD can't be as you are not charged in this case for that Response payload received."
      },
      {
        "date": "2022-08-07T23:34:00.000Z",
        "voteCount": 3,
        "content": "Answer C and D are correct to me. You can calculate the current Google Cloud egress prices using Cloud billing catalog API which can be used in D option.\n\nref: https://cloud.google.com/blog/topics/cost-management/introducing-cloud-billing-catalog-api-gcp-pricing-in-real-time\n\nBut, if you choose option C, then you will get the usage cost directly from cloud billing data. You should add a label to the Apache web server in order to select its cost.\n\nD has the more complex step, using more services too which is Cloud Logging to store the logging data of the VM and Cloud Billing Catalog API. The using of more services makes the D option to have more expensive costs."
      },
      {
        "date": "2022-07-18T19:38:00.000Z",
        "voteCount": 1,
        "content": "C is best options"
      },
      {
        "date": "2022-06-23T14:50:00.000Z",
        "voteCount": 1,
        "content": "C is best options"
      },
      {
        "date": "2022-04-30T06:20:00.000Z",
        "voteCount": 2,
        "content": "C cannot be the right answer, you export the data to big Query only once. What is the use of a cloud function running every hour on the same data? It doesn't say \"you export the data to BigQuery hourly\"."
      },
      {
        "date": "2022-04-30T23:03:00.000Z",
        "voteCount": 6,
        "content": "I was wrong, when you configure an export to Bigquery billing data are updated periodically, so answer is actually C.\nhttps://cloud.google.com/billing/docs/how-to/export-data-bigquery-tables?hl=en"
      },
      {
        "date": "2022-04-01T23:21:00.000Z",
        "voteCount": 2,
        "content": "I feel the answer should be B as projects can be specified and also services."
      },
      {
        "date": "2024-02-26T13:58:00.000Z",
        "voteCount": 1,
        "content": "Project and billing account level budgets are too broad. They would include all costs within those scopes, not just egress from the Apache server."
      },
      {
        "date": "2022-01-26T13:49:00.000Z",
        "voteCount": 2,
        "content": "It's C"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 126,
    "url": "https://www.examtopics.com/discussions/google/view/46590-exam-associate-cloud-engineer-topic-1-question-126/",
    "body": "You have designed a solution on Google Cloud that uses multiple Google Cloud products. Your company has asked you to estimate the costs of the solution. You need to provide estimates for the monthly total cost. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFor each Google Cloud product in the solution, review the pricing details on the products pricing page. Use the pricing calculator to total the monthly costs for each Google Cloud product.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFor each Google Cloud product in the solution, review the pricing details on the products pricing page. Create a Google Sheet that summarizes the expected monthly costs for each product.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tProvision the solution on Google Cloud. Leave the solution provisioned for 1 week. Navigate to the Billing Report page in the Cloud Console. Multiply the 1 week cost to determine the monthly costs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tProvision the solution on Google Cloud. Leave the solution provisioned for 1 week. Use Cloud Monitoring to determine the provisioned and used resource amounts. Multiply the 1 week cost to determine the monthly costs."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 14,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-03-15T17:04:00.000Z",
        "voteCount": 29,
        "content": "Yes I agree with A. Makes more sense."
      },
      {
        "date": "2021-10-24T15:57:00.000Z",
        "voteCount": 2,
        "content": "Why not B?? Even though answer A makes sense, they are also stating to provide estimates for \"monthly total costs\". One would think that it is not only necessary to get estimates from every resource, but also consolidate them to inform the monthly total cost required."
      },
      {
        "date": "2021-10-24T16:00:00.000Z",
        "voteCount": 8,
        "content": "Ignore the comment folks. Sentence B is missing the little thing about the pricing calculator. I go with A."
      },
      {
        "date": "2021-03-11T15:02:00.000Z",
        "voteCount": 6,
        "content": "A. For each Google Cloud product in the solution, review the pricing details on the products pricing page. Use the pricing calculator to total the monthly costs for each Google Cloud product."
      },
      {
        "date": "2024-01-24T17:02:00.000Z",
        "voteCount": 1,
        "content": "voy con A, si alguien tiene el listado completo de preguntas, me las envia a escotorin83@hotmail.com , rindo en 10 dias gracias!"
      },
      {
        "date": "2023-09-07T20:15:00.000Z",
        "voteCount": 2,
        "content": "Correct answer is A. Use GCP pricing calculator."
      },
      {
        "date": "2023-09-03T10:27:00.000Z",
        "voteCount": 2,
        "content": "A seems more correct"
      },
      {
        "date": "2022-10-23T05:45:00.000Z",
        "voteCount": 2,
        "content": "Vote A\nhttps://cloud.google.com/free/docs/estimate-costs-google-cloud-platform"
      },
      {
        "date": "2022-10-08T06:39:00.000Z",
        "voteCount": 1,
        "content": "A is the correct answer, use the pricing calculator to estimate the pricing for a month and download the estimate to csv file, or you can share the URL of the pricing calculator or email the estimate to the respective people in the company."
      },
      {
        "date": "2022-08-24T05:24:00.000Z",
        "voteCount": 1,
        "content": "[A]\nWhen \"estimate\" you need to read \"price calculator\""
      },
      {
        "date": "2022-06-23T14:52:00.000Z",
        "voteCount": 1,
        "content": "I agree with A"
      },
      {
        "date": "2022-01-17T21:46:00.000Z",
        "voteCount": 4,
        "content": "Answer is C\n\nThe question says 'cost of solution of the design' means, how it is going to work in practical, means the traffic load, no of users, uploads, downloads, transcations etc etc. \nIn this case, the more nearer option is to run for a week, then calculate.\nAnswer A is not correct, becoz A is applicable when the cost of the product is to be determined."
      },
      {
        "date": "2024-02-19T10:37:00.000Z",
        "voteCount": 1,
        "content": "C and D are not optimal due to deployment costs. Provisioning the solution for an extended period just to estimate cost incurs actual expenses. Additionally, a week may not adequately capture fluctuations in usage patterns over a full month."
      },
      {
        "date": "2022-11-07T11:58:00.000Z",
        "voteCount": 3,
        "content": "Question has asked to estimate the costs of the solution i.e. the initial setup cost not the running cost. Therefore, C and D discarded and B is of no use without pricing calculator. Question is testing whether you are aware of ths calculator service from Google."
      },
      {
        "date": "2021-11-24T04:05:00.000Z",
        "voteCount": 2,
        "content": "answer is a"
      },
      {
        "date": "2021-11-19T14:06:00.000Z",
        "voteCount": 3,
        "content": "The Right Ans: A"
      },
      {
        "date": "2021-11-19T09:06:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2021-09-22T02:33:00.000Z",
        "voteCount": 2,
        "content": "As per GCP best practices, A makes more sense"
      },
      {
        "date": "2021-07-16T05:15:00.000Z",
        "voteCount": 5,
        "content": "Correct is A. It is the main purpose of Pricing calculator. You have to check pricing details on products pages to understand what charges apply and how to size it.\n\nB. It works bur it is more complicated than A.\nC. It works but you need to wait one week and you pay for this. Moreoever, you won't be in production so somes charges may lack like net egress charges\nD. No, because Cloud Monitoring can't be used for billing purpose"
      },
      {
        "date": "2021-06-09T13:53:00.000Z",
        "voteCount": 2,
        "content": "The correct answer is A. Please, modify it."
      },
      {
        "date": "2021-05-20T07:24:00.000Z",
        "voteCount": 1,
        "content": "A, is best option"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 127,
    "url": "https://www.examtopics.com/discussions/google/view/24898-exam-associate-cloud-engineer-topic-1-question-127/",
    "body": "You have an application that receives SSL-encrypted TCP traffic on port 443. Clients for this application are located all over the world. You want to minimize latency for the clients. Which load balancing option should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tHTTPS Load Balancer",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNetwork Load Balancer",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSSL Proxy Load Balancer\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInternal TCP/UDP Load Balancer. Add a firewall rule allowing ingress traffic from 0.0.0.0/0 on the target instances."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 37,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 15,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-07-26T01:45:00.000Z",
        "voteCount": 32,
        "content": "SSL Proxy Load Balancing support for the following ports: 25, 43, 110, 143, 195, 443, 465, 587, 700, 993, 995, 1883, 3389, 5222, 5432, 5671, 5672, 5900, 5901, 6379, 8085, 8099, 9092, 9200, and 9300. When you use Google- managed SSL certificates with SSL Proxy Load Balancing, the frontend port for traffic must be 443 to enable the Google-managed SSL certificates to be provisioned and renewed."
      },
      {
        "date": "2020-07-06T08:30:00.000Z",
        "voteCount": 19,
        "content": "C is correct"
      },
      {
        "date": "2024-09-30T01:16:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/load-balancing/docs/tcp\nNote: Although external proxy Network Load Balancers can support HTTPS traffic, you should use an external Application Load Balancer for HTTPS traffic instead. External Application Load Balancers support a number of HTTP-specific features, including routing by HTTP request path and balancing by request rate."
      },
      {
        "date": "2024-07-14T14:36:00.000Z",
        "voteCount": 2,
        "content": "I remember compliance question was in \"Associate Cloud Engineer Certification Learning Path\". And I answered SSL Proxy Load Balancer but it was incorrect. Correct answer is A."
      },
      {
        "date": "2023-11-13T04:47:00.000Z",
        "voteCount": 4,
        "content": "C. SSL Proxy Load Balancer\n\nThe SSL Proxy Load Balancer is designed specifically for SSL-encrypted traffic and provides SSL termination, minimizing latency for clients worldwide by handling SSL connections efficiently. This load balancer is suitable for applications that receive SSL-encrypted TCP traffic on port 443, making it a good choice for the scenario."
      },
      {
        "date": "2023-09-07T20:25:00.000Z",
        "voteCount": 2,
        "content": "Correct answer is C.\nExternal proxy load balancer supports global and regional scope. While external passthrough network load balancer supports regional scope."
      },
      {
        "date": "2023-09-03T10:32:00.000Z",
        "voteCount": 2,
        "content": "C is the right answer , read it carefully  TCP traffic"
      },
      {
        "date": "2024-02-19T04:46:00.000Z",
        "voteCount": 1,
        "content": "It's not about TCP Traffic. It's about minimizing latency, so in order to achieve that we need to use SSL termination which is a feature of the SSL Proxy LB."
      },
      {
        "date": "2024-02-19T04:58:00.000Z",
        "voteCount": 1,
        "content": "I think i'm wrong cuz HTTPS use TCP and SSL, because HTTPS is HTTP over TLS/SSL(now) so the ans in my pov is A"
      },
      {
        "date": "2023-08-03T19:59:00.000Z",
        "voteCount": 1,
        "content": "443 is HTTTPs"
      },
      {
        "date": "2023-05-07T12:25:00.000Z",
        "voteCount": 3,
        "content": "Answer is A\n\nFrom this Link:\nhttps://cloud.google.com/load-balancing/docs/ssl\n\nit states this:\nExternal SSL proxy load balancers are intended for non-HTTP(S) traffic. For HTTP(S) traffic, we recommend that you use an external HTTP(S) load balancer.\n\n443 is HTTPS traffic\n\nfor those saying 443 isn't https\nhttps://www.iana.org/assignments/service-names-port-numbers/service-names-port-numbers.xhtml?search=443"
      },
      {
        "date": "2023-05-01T09:46:00.000Z",
        "voteCount": 9,
        "content": "Go for C. We are taking exam so read the question smart. For HTTPS traffic use HTTPS load balancer. For non-HTTP traffic, use SSL Proxy Load Balancer. (https://cloud.google.com/load-balancing/docs/ssl). Network Load Balancer (Extenral TCP/UDP network load balancer) for regional pass through traffic (https://cloud.google.com/load-balancing/docs/network). Here it mentioned SSL-encrypted TCP traffic all over the world, go for SSL proxy load balancer. Read the keyword \"SSL-encrypted TCP\" and \"all over the world\". Keep it simple, don't over complicated yourself"
      },
      {
        "date": "2023-04-30T12:25:00.000Z",
        "voteCount": 1,
        "content": "The answer is C. \n\n\"SSL encrypted TCP\"  traffic is not exactly SSL traffic. SSL encrypted TCP traffic is usually used for non HTTP application. \nPort 443 is supported by SSL proxy load balancer so this isn't what will differentiate the two for us. The main part here is \"SSL encrypted TCP\" traffic."
      },
      {
        "date": "2023-02-13T23:51:00.000Z",
        "voteCount": 4,
        "content": "C. SSL Proxy Load Balancer would be the best option for minimizing latency for the clients, as it terminates SSL traffic and forwards unencrypted traffic directly to the backend instances. This reduces the amount of processing and latency associated with SSL encryption/decryption. Additionally, because the clients are located all over the world, using a global SSL Proxy Load Balancer can distribute traffic to the closest backend instances for the clients, further reducing latency."
      },
      {
        "date": "2023-02-10T10:05:00.000Z",
        "voteCount": 4,
        "content": "A. HTTPS Load Balancer\n\nThe HTTPS Load Balancer is the best option for minimizing latency for clients located all over the world. HTTPS Load Balancer provides a global solution for load balancing secure (SSL/TLS) traffic, including the ability to distribute traffic to backend instances based on IP address, based on request content, or both. It is designed to handle encrypted traffic and can terminate SSL/TLS connections, making it the optimal choice for an application that receives SSL-encrypted TCP traffic on port 443. Additionally, HTTPS Load Balancer has built-in features to minimize latency, such as support for HTTP/2 and connection multiplexing, which can reduce the number of connections and round trips required to complete a request."
      },
      {
        "date": "2023-02-09T03:54:00.000Z",
        "voteCount": 2,
        "content": "A:\n\nAlthough both HTTPS and SSL Proxy load balancers can both be used for global external load balancing, for HTTP(S) traffic, Google recommends that you use HTTP(S) Load Balancing.\n\nhttps://cloud.google.com/load-balancing/docs/ssl"
      },
      {
        "date": "2023-02-09T04:00:00.000Z",
        "voteCount": 2,
        "content": "To add further clarity, Google documentation says:\n\n\"External SSL Proxy Load Balancing is intended for non-HTTP(S) traffic. For HTTP(S) traffic, we recommend that you use HTTP(S) Load Balancing.\"\n\nPort 443 (mentioned in the question)  is for HTTPS\n\nhttps://cloud.google.com/load-balancing/docs/ssl"
      },
      {
        "date": "2023-03-07T13:06:00.000Z",
        "voteCount": 2,
        "content": "You are assuming  that this is HTTPS traffic. But from question we know that it is SSL-encrypted TCP , so we can't use HTTPS load balancer. Moreover we will be missing HTTP data for URL map"
      },
      {
        "date": "2023-01-04T02:23:00.000Z",
        "voteCount": 2,
        "content": "Correct answer: (A)\nGlobal https load balancer\nhttps://cloud.google.com/load-balancing/docs/choosing-load-balancer"
      },
      {
        "date": "2023-01-08T04:36:00.000Z",
        "voteCount": 2,
        "content": "The question does not mention the layer 7 protocol (HTTP/S), only that is TCP (layer 4). This is way it should be C"
      },
      {
        "date": "2023-01-30T02:10:00.000Z",
        "voteCount": 1,
        "content": "HTTPs uses both SSL encryption and TCP layer as well. 443 port is HTTPs default port, which suggests  A."
      },
      {
        "date": "2023-03-07T13:10:00.000Z",
        "voteCount": 1,
        "content": "443 is default port for SSL/TLS communication. I can be HTTPs or it can be somethink else."
      },
      {
        "date": "2022-11-19T05:08:00.000Z",
        "voteCount": 3,
        "content": "This is a tricky question. \n\nFirst point to consider is the port. TCP 443 port is used for HTTPS traffic.\nSecond  : SSL Proxy LB is intended for non-HTTPs traffic and for HTTPs traffic, it should be global HTTPs LB.\n\nAnswer is A : HTTPS Load Balancer."
      },
      {
        "date": "2022-10-08T06:56:00.000Z",
        "voteCount": 2,
        "content": "C is the correct Answer,\nSSL proxy load balancer with Traffic Type - TCP with SSL offload, for global IPv4, IPv6, external ports for load balancing - 25,43,110,143,195,443,465,587,700,993,995,1883,5222"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 128,
    "url": "https://www.examtopics.com/discussions/google/view/25199-exam-associate-cloud-engineer-topic-1-question-128/",
    "body": "You have an application on a general-purpose Compute Engine instance that is experiencing excessive disk read throttling on its Zonal SSD Persistent Disk. The application primarily reads large files from disk. The disk size is currently 350 GB. You want to provide the maximum amount of throughput while minimizing costs.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the size of the disk to 1 TB.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the allocated CPU to the instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate to use a Local SSD on the instance.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate to use a Regional SSD on the instance."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 13,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-07-09T08:08:00.000Z",
        "voteCount": 36,
        "content": "C is correct, local SSD has more IOPS"
      },
      {
        "date": "2021-02-21T13:43:00.000Z",
        "voteCount": 13,
        "content": "Agree. This is also cheaper than having 350 Gb persistent SSD:\nHere are calculations (taken from GCP when creating instance)\n350 Gb SSD Persistent disk: 59.50$/month, read IOPS: 10 500      with n1-standard-1\n1000 Gb SSD Persistent disk: 170.00$/month, read IOPS: 15 000      with n1-standard-1\n375 Gb Local SSD (NVMe):  30.00$/month, read IOPS: 170 000      with n1-standard-1"
      },
      {
        "date": "2021-12-22T18:02:00.000Z",
        "voteCount": 7,
        "content": "trick question...  locak-ssd is not persistent. increasing the size of of the disk will also increase the iops. A is correct imho."
      },
      {
        "date": "2022-01-23T08:53:00.000Z",
        "voteCount": 2,
        "content": "Local SSDs have higher throughput and lower latency than standard persistent disks or SSD persistent disks. The data that you store on a local SSD persists only until the instance is stopped or deleted."
      },
      {
        "date": "2022-03-02T05:13:00.000Z",
        "voteCount": 8,
        "content": "I once thought that A was the correct response because of the persistence problem, but reading the question carefully, we must choose C over A.\n\nThe question does not stipulate that the local files must be persistent, and this is the only reason why you would choose \"A\" over \"C\". \n\nAlso, the question has an important key word: Minimising costs. \n\n1TB of zonal persistent disk costs a huge amount more than 350GB of local disk. \n\nWe should choose C."
      },
      {
        "date": "2022-05-19T15:26:00.000Z",
        "voteCount": 1,
        "content": "Very Nice, agree C is correct"
      },
      {
        "date": "2022-09-23T16:08:00.000Z",
        "voteCount": 4,
        "content": "they have been using persistent disk....there was a reason during the initial design.....persistent  disk was a req....we should find a solution without affecting that...increasing the size would be the best solution for this scenario."
      },
      {
        "date": "2020-08-21T16:12:00.000Z",
        "voteCount": 16,
        "content": "Correct Answer is (C):\n\nPerformance\n\nStandard persistent disks are efficient and economical for handling sequential read/write operations, but they aren't optimized to handle high rates of random input/output operations per second (IOPS). If your apps require high rates of random IOPS, use SSD persistent disks. SSD persistent disks are designed for single-digit millisecond latencies. Observed latency is application specific.\n\nhttps://cloud.google.com/compute/docs/disks#performance"
      },
      {
        "date": "2022-02-04T12:18:00.000Z",
        "voteCount": 4,
        "content": "A local SSD is not the same as an SSD persistent disk. \n\n\"Local SSDs are physically attached to the server that hosts your VM instance. Local SSDs have higher throughput and lower latency than standard persistent disks or SSD persistent disks. The data that you store on a local SSD persists only until the instance is stopped or deleted.\"\nhttps://cloud.google.com/compute/docs/disks#localssds\n\nThe answer is C."
      },
      {
        "date": "2022-02-04T12:18:00.000Z",
        "voteCount": 1,
        "content": "Sorry, A not C."
      },
      {
        "date": "2023-11-13T05:28:00.000Z",
        "voteCount": 3,
        "content": "C. Migrate to use a Local SSD on the instance.\n\nLocal SSDs provide high-throughput, low-latency storage that is physically attached to the instance. This can be beneficial for applications experiencing excessive disk read throttling, especially when dealing with large files. Local SSDs are ideal for temporary data that can be recomputed or regenerated if lost, and they can offer improved performance compared to Zonal SSD Persistent Disks.\n(https://cloud.google.com/compute/docs/disks/local-ssd#create_local_ssd_instance) for detailed instructions."
      },
      {
        "date": "2023-09-07T20:31:00.000Z",
        "voteCount": 1,
        "content": "Local SSD provides more throughput than Persistent disks and is cost effective solution. So, correct answer is C."
      },
      {
        "date": "2023-02-13T23:54:00.000Z",
        "voteCount": 3,
        "content": "C. Migrate to use a Local SSD on the instance. Local SSDs provide higher throughput and lower latency compared to Zonal SSD Persistent Disks, and are optimized for use cases that require high-speed, temporary storage. They are physically attached to the instance, so network latencies are minimized. However, they are not intended for long-term storage and may not provide the same level of durability as persistent disks. Since the application is primarily reading large files from disk and experiencing disk read throttling, using a Local SSD should provide a significant improvement in performance while minimizing costs. Increasing the size of the Zonal SSD Persistent Disk or increasing the allocated CPU to the instance may provide some improvement, but are unlikely to fully address the disk read throttling issue. Migrating to a Regional SSD is also not likely to improve performance significantly, as the disk is still separate from the instance and network latencies can impact performance."
      },
      {
        "date": "2023-01-21T00:51:00.000Z",
        "voteCount": 1,
        "content": "According to the page containing the tables and to the tables \"https://cloud.google.com/compute/docs/disks/performance#n1_vms\" and \"https://cloud.google.com/compute/docs/disks/performance#n2_vms\", the number of CPU cores greatly influence the available max. read THROUGHPUT (\"excessive disk read throttling on its Zonal SSD Persistent Disk\", \"The application primarily reads large files from disk.\") on general purpose VMs.\nThe question also requires minimizing the costs, however, as Local SSDs are EPHEMERAL, they are out of question for the scenario at hand.\nSo, answer \"B\" seems to be the correct one."
      },
      {
        "date": "2022-10-12T09:30:00.000Z",
        "voteCount": 1,
        "content": "C. Migrate to use a Local SSD on the instance"
      },
      {
        "date": "2022-10-08T07:26:00.000Z",
        "voteCount": 1,
        "content": "C is the correct Answer,\n\nLocal SSDs\nLocal SSDs are physically attached to the server that hosts your VM instance. Local SSDs have higher throughput and lower latency than standard persistent disks or SSD persistent disks. The data that you store on a local SSD persists only until the instance is stopped or deleted. Each local SSD is 375 GB in size, but you can attach a maximum of 24 local SSD partitions for a total of 9 TB per instance.\n\nPerformance\nLocal SSDs are designed to offer very high IOPS and low latency. Unlike persistent disks, you must manage the striping on local SSDs yourself. Combine multiple local SSD partitions into a single logical volume to achieve the best local SSD performance per instance, or format local SSD partitions individually.\n\nLocal SSD performance depends on which interface you select. Local SSDs are available through both SCSI and NVMe interfaces."
      },
      {
        "date": "2022-09-24T10:28:00.000Z",
        "voteCount": 2,
        "content": "had this question today"
      },
      {
        "date": "2022-08-16T00:25:00.000Z",
        "voteCount": 1,
        "content": "C is correct \n\nLocal SSDs are physically attached to the server that hosts your VM instance. Local SSDs have higher throughput and lower latency than standard persistent disks or SSD persistent disks. The performance gains from local SSDs require certain trade-offs in availability, durability, and flexibility. Because of these trade-offs, Local SSD storage isn\u2019t automatically replicated and all data on the local SSD might be lost if the instance terminates for any reason.\n\nRef: https://cloud.google.com/compute/docs/disks#localssds\nRef: https://cloud.google.com/compute/docs/disks/performance#type_comparison"
      },
      {
        "date": "2022-08-11T20:19:00.000Z",
        "voteCount": 1,
        "content": "C can\u00b4t B for the reasons explained here in the answers (local disk info will be totally deleted if you restart or delete your vm). Then A or B. Google recommends to increase the size of the disk as performance of the disk is linear to the size of the same. But Im wondering if 1TB is too much (it is almost 2 times more the original size of the disk, and that sounds like too much, plus if with 350GB the files already fit there, then increasing the  size of the disk \"a little bit more\"  should have been a better approach to test how the performance increases. Then Google Also recommends to add more CPU to get a better IOPS, it might be cheaper than option A if the right machine with more CPU is chosen.\n\nAnswer: B"
      },
      {
        "date": "2022-06-23T14:55:00.000Z",
        "voteCount": 1,
        "content": "Local SSD .. C is right"
      },
      {
        "date": "2022-06-09T03:30:00.000Z",
        "voteCount": 1,
        "content": "C is correct."
      },
      {
        "date": "2022-05-24T02:03:00.000Z",
        "voteCount": 1,
        "content": "Go with C\nAs Local SSDs have high IOPS"
      },
      {
        "date": "2022-04-10T02:51:00.000Z",
        "voteCount": 1,
        "content": "C for me.\nhttps://cloud.google.com/compute/docs/disks#performance"
      },
      {
        "date": "2022-04-06T05:51:00.000Z",
        "voteCount": 3,
        "content": "Correct Answer is B\nFrom both tables:\nhttps://cloud.google.com/compute/docs/disks/performance#performance_by_disk_size\nhttps://cloud.google.com/compute/docs/disks/performance#machine-type-disk-limits\n\nThe general purpose vm limit is less than ssd persistent disk size limit of 250-500"
      },
      {
        "date": "2022-02-25T07:13:00.000Z",
        "voteCount": 2,
        "content": "Increasing Disk size would increase cost while question is asking for minimising the cost"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 129,
    "url": "https://www.examtopics.com/discussions/google/view/27827-exam-associate-cloud-engineer-topic-1-question-129/",
    "body": "Your Dataproc cluster runs in a single Virtual Private Cloud (VPC) network in a single subnet with range 172.16.20.128/25. There are no private IP addresses available in the VPC network. You want to add new VMs to communicate with your cluster using the minimum number of steps. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the existing subnet range to 172.16.20.0/24.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new Secondary IP Range in the VPC and configure the VMs to use that range.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new VPC network for the VMs. Enable VPC Peering between the VMs' VPC network and the Dataproc cluster VPC network.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new VPC network for the VMs with a subnet of 172.32.0.0/16. Enable VPC network Peering between the Dataproc VPC network and the VMs VPC network. Configure a custom Route exchange."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 38,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 37,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-08-21T17:38:00.000Z",
        "voteCount": 49,
        "content": "Correction.\nCorrect Answers is (A): \n\ngcloud compute networks subnets expand-ip-range\nNAME\ngcloud compute networks subnets expand-ip-range - expand the IP range of a Compute Engine subnetwork"
      },
      {
        "date": "2022-08-27T03:57:00.000Z",
        "voteCount": 3,
        "content": "I think, you can't expand ip range subnet, if there isn't space in VPC. I read this question a lot, VPC CIDR like with 172.16.20.128/25 and there's only one subnet 172.16.20.128/25 inside this VPC, so you can't expand nothing. for me, there's Letter C and D works, but letter D is necessary extra work. LETTER C is right."
      },
      {
        "date": "2023-01-30T02:16:00.000Z",
        "voteCount": 2,
        "content": "There's no information about VPC CIDR, only subnet. You can't tell that there's no space"
      },
      {
        "date": "2023-06-21T14:29:00.000Z",
        "voteCount": 3,
        "content": "VPC's DO NOT have IP range limitations. \nYou can only object if 172.16.20.0/25 is in use in the same VPC or in a VPC that this VPC is already peered with. \n\n.128/25 expands to .0/24 (i.e. \"backwards\") So as long as it's free, you're good.\n\nIn a question like this, it's obvious that the simple answer is the right one, i.e. A - Expansion."
      },
      {
        "date": "2023-09-24T08:10:00.000Z",
        "voteCount": 1,
        "content": "- The statement is clear with point 1A and 2A:\n1A.- \"single Virtual Private Cloud (VPC) network in a single subnet\"\n2A.- \"There are no private IP addresses available in the VPC network.\"\n\n- Question: How can you expand if there is a single VPC with a single subnet\nand there are no private IP addresses available in the only VPC network ???\n\n- Result: Yes it has limitation, this question is clear, this is and exam, not the real life, we cannot verify anything else and we have the limitation os the statement."
      },
      {
        "date": "2020-08-09T20:43:00.000Z",
        "voteCount": 37,
        "content": "I think is A,"
      },
      {
        "date": "2020-08-21T07:46:00.000Z",
        "voteCount": 4,
        "content": "thank you"
      },
      {
        "date": "2022-10-05T07:42:00.000Z",
        "voteCount": 3,
        "content": "No it can't be as you can't modify ip address but can expand. There is no ip in the existing vpc so you have to create a new vpc and connect it using peering."
      },
      {
        "date": "2024-10-17T07:57:00.000Z",
        "voteCount": 1,
        "content": "The question asks for the \"minimum number of steps\", not what is wrong or right. A is the \"minimum number of steps\"."
      },
      {
        "date": "2024-09-30T01:23:00.000Z",
        "voteCount": 1,
        "content": "gcloud compute networks subnets expand-ip-range - expand the IP range of a Compute Engine subnetwork\n\nhttps://cloud.google.com/sdk/gcloud/reference/compute/networks/subnets/expand-ip-range"
      },
      {
        "date": "2024-09-24T03:17:00.000Z",
        "voteCount": 1,
        "content": "A is correct answer"
      },
      {
        "date": "2024-09-20T11:47:00.000Z",
        "voteCount": 1,
        "content": "Expand the subnet"
      },
      {
        "date": "2024-09-03T23:55:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is A because increasing the disk size is the simplest way to address the issue. Option C is overly complex and unnecessary."
      },
      {
        "date": "2024-09-03T23:51:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is A, because increasing the disk size is the simplest way to address the issue. Option C is overly complex and unnecessary."
      },
      {
        "date": "2024-08-28T14:51:00.000Z",
        "voteCount": 1,
        "content": "\"minimum number of steps\". I think A is correct"
      },
      {
        "date": "2024-08-24T12:11:00.000Z",
        "voteCount": 1,
        "content": "You can always expand subnets. You can shrink them. So A is the correct answer."
      },
      {
        "date": "2024-07-25T08:25:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is A, in GCP, it is different from AWS or Azure, there is no CIDR assigned for the VPC itself, but each subnet inside the VPC has its own unique CIDR range, that you can expand (with some considtions), I saw some answers based on the logic of AWS and Azure"
      },
      {
        "date": "2024-05-25T09:17:00.000Z",
        "voteCount": 1,
        "content": "Somes who vote C are out.\n\nRead:  the minimum number of steps &gt; Modify the existing subnet range"
      },
      {
        "date": "2024-05-13T12:10:00.000Z",
        "voteCount": 1,
        "content": "It should be A - key \"using the minimum number of steps\"..."
      },
      {
        "date": "2024-03-05T12:40:00.000Z",
        "voteCount": 4,
        "content": "A. I gave this a test today and it worked as well."
      },
      {
        "date": "2024-02-21T10:40:00.000Z",
        "voteCount": 1,
        "content": "This is going on a split. Am I missing it? Why couldn't we just expand the IP range, as in Ans A?"
      },
      {
        "date": "2024-02-13T08:36:00.000Z",
        "voteCount": 1,
        "content": "A - correct\n\nBy far the most minimum amount of steps is required by A where you go from 128 to 254 available IPs."
      },
      {
        "date": "2024-01-21T08:58:00.000Z",
        "voteCount": 2,
        "content": "I see many references to the statement \"There are no private IP addresses available in the VPC network\". Modifying the subnet to a /24 adds 128 free addresses to it. I'll go for A."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 130,
    "url": "https://www.examtopics.com/discussions/google/view/28670-exam-associate-cloud-engineer-topic-1-question-130/",
    "body": "You manage an App Engine Service that aggregates and visualizes data from BigQuery. The application is deployed with the default App Engine Service account.<br>The data that needs to be visualized resides in a different project managed by another team. You do not have access to this project, but you want your application to be able to read data from the BigQuery dataset. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAsk the other team to grant your default App Engine Service account the role of BigQuery Job User.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAsk the other team to grant your default App Engine Service account the role of BigQuery Data Viewer.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn Cloud IAM of your project, ensure that the default App Engine service account has the role of BigQuery Data Viewer.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn Cloud IAM of your project, grant a newly created service account from the other team the role of BigQuery Job User in your project."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 44,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 34,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-08-15T09:51:00.000Z",
        "voteCount": 32,
        "content": "I think B is the answer"
      },
      {
        "date": "2023-06-06T22:21:00.000Z",
        "voteCount": 4,
        "content": "'I think'  could you elaborate further please?"
      },
      {
        "date": "2020-08-21T17:52:00.000Z",
        "voteCount": 27,
        "content": "Correct Answer is (B):\nSorry, I copied/pasted the the wrong statement. \nThis is the proper explanation regarding to Big Query Data Viewer Role. \nThe resource that you need to get access is in the other project.\n\nroles/bigquery.dataViewer\tBigQuery Data Viewer\t\nWhen applied to a table or view, this role provides permissions to:\n\nRead data and metadata from the table or view.\nThis role cannot be applied to individual models or routines.\n\nWhen applied to a dataset, this role provides permissions to:\n\nRead the dataset's metadata and list tables in the dataset.\nRead data and metadata from the dataset's tables.\nWhen applied at the project or organization level, this role can also enumerate all datasets in the project. Additional roles, however, are necessary to allow the running of jobs."
      },
      {
        "date": "2020-09-20T22:08:00.000Z",
        "voteCount": 10,
        "content": "A is correct, data viewer role does not allow you to execute query, that can be done if you are the user role."
      },
      {
        "date": "2020-09-20T22:11:00.000Z",
        "voteCount": 3,
        "content": "The question states you want to aggregate and visualize data, that is run aggregate SQL on data before visualizing."
      },
      {
        "date": "2024-09-30T01:30:00.000Z",
        "voteCount": 1,
        "content": "BigQuery Data Viewer \n(roles/bigquery.dataViewer)\n\nWhen applied to a table or view, this role provides permissions to:\n\nRead data and metadata from the table or view.\nThis role cannot be applied to individual models or routines.\n\nhttps://cloud.google.com/bigquery/docs/access-control"
      },
      {
        "date": "2024-09-07T12:49:00.000Z",
        "voteCount": 1,
        "content": "Big Query Data Viewer Role allows to read data only in the Cloud Console, you cannot use SQL to select or aggregate data, I assume the app will need to run jobs. How would you write such an app? Please correct me if I am wrong."
      },
      {
        "date": "2024-09-04T00:01:00.000Z",
        "voteCount": 2,
        "content": "To read data from a BigQuery dataset, you only need to grant the App Engine Service account the role of BigQuery Data Viewer."
      },
      {
        "date": "2024-03-14T14:59:00.000Z",
        "voteCount": 3,
        "content": "B B B B"
      },
      {
        "date": "2024-03-10T06:13:00.000Z",
        "voteCount": 2,
        "content": "As the question clearly says - 'read'\n'You do not have access to this project, but you want your application to be able to READ data from the BigQuery dataset'\nAwnser is B"
      },
      {
        "date": "2024-03-05T12:48:00.000Z",
        "voteCount": 4,
        "content": "A is too permissive. This role includes BigQuery Data Viewer permissions but also the ability to create, run, and manage BigQuery jobs. Your application only needs to read data, not manipulate it. B grants read-only access to BigQuery datasets and tables, which perfectly aligns with the requirement of the App Engine service only needing to visualize data."
      },
      {
        "date": "2024-01-30T02:42:00.000Z",
        "voteCount": 2,
        "content": "To see data, you need either BigQuery User or BigQuery Data Viewer roles\nYou CANNOT see data with BigQuery Job User roles"
      },
      {
        "date": "2024-01-31T00:28:00.000Z",
        "voteCount": 1,
        "content": "It's true, the jobUser role does not contain permissions to get datasets or tables. I think with the phrasing we can assume that the data is aggregated within the app--not BigQuery--so the query permission is less important than reading the data.\n\nhttps://cloud.google.com/bigquery/docs/access-control#bigquery.jobUser\nhttps://cloud.google.com/bigquery/docs/access-control#bigquery.dataViewer"
      },
      {
        "date": "2024-01-24T17:06:00.000Z",
        "voteCount": 1,
        "content": "B es la correcta. \nSi alguien tiene el listado completo de preguntas, me las envia a escotorin83@hotmail.com , rindo en 10 dias gracias!"
      },
      {
        "date": "2024-01-11T04:22:00.000Z",
        "voteCount": 1,
        "content": "role/bigquery.dataViewer"
      },
      {
        "date": "2024-01-05T03:10:00.000Z",
        "voteCount": 2,
        "content": "B - a viewer can query, aggregate and visual data"
      },
      {
        "date": "2024-01-01T14:22:00.000Z",
        "voteCount": 1,
        "content": "Ans is B\n. Ask the other team to grant your default App Engine Service account the role of BigQuery Data Viewer: The BigQuery Data Viewer role provides permissions to view and list data in BigQuery. This includes reading data from tables and views in a dataset. If you need your application to read data from the BigQuery dataset in the other project, this is the appropriate role to be granted to your service account by the other team.\nReason option A is incorrect\n Ask the other team to grant your default App Engine Service account the role of BigQuery Job User: The BigQuery Job User role allows a user to run BigQuery jobs, which includes query jobs. However, this role does not grant permission to read data in BigQuery tables or datasets. It's useful for submitting jobs but not sufficient for accessing data."
      },
      {
        "date": "2024-01-01T11:11:00.000Z",
        "voteCount": 3,
        "content": "Role of BigQuery Data Viewer: The BigQuery Data Viewer role (roles/bigquery.dataViewer) includes permissions to view and query data within BigQuery datasets. This role is sufficient for an application that needs read access to BigQuery datasets, such as for aggregation and visualization purposes."
      },
      {
        "date": "2023-12-20T06:50:00.000Z",
        "voteCount": 1,
        "content": "Selected Answer: B\n\"BigQuery Job User: Provides permissions to run jobs, including queries, within the project.\"\nSo it is not giving access to read BigQuery Dataset!\nhttps://cloud.google.com/bigquery/docs/access-control"
      },
      {
        "date": "2024-09-07T12:51:00.000Z",
        "voteCount": 1,
        "content": "each and every SQL query in BigQuery is a job..."
      },
      {
        "date": "2023-12-19T12:02:00.000Z",
        "voteCount": 1,
        "content": "The requirements are to  - aggregating and visualizing data from BigQuery - the BigQuery Data Viewer (roles/bigquery.dataViewer) role is likely the most appropriate.\n This role will allow the App Engine service to read and visualize the data without permission to modify it.\n\nIf the app service needs more than just read access, such as the ability to modify data or execute specific types of queries, we might consider the Data Editor or Data User roles. \n\nHowever, always adhere to the principle of least privilege, granting only the minimum permissions necessary for your service to function as intended."
      },
      {
        "date": "2023-09-03T22:15:00.000Z",
        "voteCount": 2,
        "content": "A seems more correct , as A give you both required access i.e aggragets and visualizes data from BQ"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 131,
    "url": "https://www.examtopics.com/discussions/google/view/29004-exam-associate-cloud-engineer-topic-1-question-131/",
    "body": "You need to create a copy of a custom Compute Engine virtual machine (VM) to facilitate an expected increase in application traffic due to a business acquisition.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Compute Engine snapshot of your base VM. Create your images from that snapshot.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Compute Engine snapshot of your base VM. Create your instances from that snapshot.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a custom Compute Engine image from a snapshot. Create your images from that image.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a custom Compute Engine image from a snapshot. Create your instances from that image.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 40,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 27,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-08-21T17:57:00.000Z",
        "voteCount": 37,
        "content": "Correct Answer is (D):\n\nPreparing your instance for an image\nYou can create an image from a disk even while it is attached to a running VM instance. However, your image will be more reliable if you put the instance in a state that is easier for the image to capture. Use one of the following processes to prepare your boot disk for the image:\n\nStop the instance so that it can shut down and stop writing any data to the persistent disk.\n\nIf you can't stop your instance before you create the image, minimize the amount of writes to the disk and sync your file system.\n\nPause apps or operating system processes that write data to that persistent disk.\nRun an app flush to disk if necessary. For example, MySQL has a FLUSH statement. Other apps might have similar processes.\nStop your apps from writing to your persistent disk.\nRun sudo sync.\nAfter you prepare the instance, create the image.\n\nhttps://cloud.google.com/compute/docs/images/create-delete-deprecate-private-images#prepare_instance_for_image"
      },
      {
        "date": "2021-03-28T13:23:00.000Z",
        "voteCount": 14,
        "content": "B: \nwe just need to make 'a copy' of the VM, B works well for that.\n\nnot D: Had the question mentioned more copies, we would need to go the way of images...templates etc. D will work but not needed here."
      },
      {
        "date": "2021-06-12T22:34:00.000Z",
        "voteCount": 2,
        "content": "custom images are better a fit if its for a new business workload you just acquired"
      },
      {
        "date": "2022-06-10T06:17:00.000Z",
        "voteCount": 1,
        "content": "What about the answer that says create your instanceS ??"
      },
      {
        "date": "2024-09-07T12:17:00.000Z",
        "voteCount": 2,
        "content": "The correct algorithm is:\n1. gcloud compute snapshots create\n2. gcloud compute images create\n3. gcloud compute instances create \nNow, decide which one sounds more accurate for you:\n- B.&nbsp;Create a Compute Engine snapshot of your base VM. Create your instances from that snapshot.\n- D.&nbsp;Create a custom Compute Engine image from a snapshot. Create your instances from that image.\nFor me, D sounds more accurate, even though we assume we already have a snapshot."
      },
      {
        "date": "2024-05-25T09:09:00.000Z",
        "voteCount": 1,
        "content": "Compute Engine snapshot? What is that?\nThere are snapshots of disks, or there are disk image from disk snapshots...\nbut here we need the VM image. Not the disks..."
      },
      {
        "date": "2024-01-11T04:31:00.000Z",
        "voteCount": 3,
        "content": "\u30a4\u30e1\u30fc\u30b8\u304b\u3089\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9"
      },
      {
        "date": "2024-07-21T10:04:00.000Z",
        "voteCount": 1,
        "content": "Ok bro!"
      },
      {
        "date": "2024-01-05T03:13:00.000Z",
        "voteCount": 1,
        "content": "It is D"
      },
      {
        "date": "2023-12-19T12:09:00.000Z",
        "voteCount": 2,
        "content": "You can create custom images from source disks, images, snapshots, or images stored in Cloud Storage and use these images to create virtual machine (VM) instances. Custom images are ideal for situations where you have created and modified a persistent boot disk or specific image to a certain state and need to save that state for creating VMs.\n\nYou need to create image from the snapshot, so answer is D\nhttps://cloud.google.com/compute/docs/images/create-custom#:~:text=You%20can%20create%20custom%20images,virtual%20machine%20(VM)%20instances."
      },
      {
        "date": "2023-12-07T03:52:00.000Z",
        "voteCount": 5,
        "content": "You cannot directly create Compute Engine instances from a snapshot. Instances are created from images, not snapshots. The snapshot needs to be converted into a custom image first."
      },
      {
        "date": "2023-11-03T14:26:00.000Z",
        "voteCount": 1,
        "content": "Choosing D, assuming there is schedule snapshots taken and we are moving forward using those snapshot for creating image and using that image for creating new VM. It is indeed very confusing, as we need to assume few things."
      },
      {
        "date": "2023-10-31T11:11:00.000Z",
        "voteCount": 1,
        "content": "B&amp;D are creating instance, so A&amp;C are eliminated.\nWe need snapshot first, so answer is B. \nD is out as there is no mention of snapshot from where image will be created."
      },
      {
        "date": "2023-10-18T12:55:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/compute/docs/images/create-delete-deprecate-private-images#prepare_instance_for_image"
      },
      {
        "date": "2023-10-05T11:52:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/compute/docs/images/create-custom#create_image:~:text=You%20can%20create%20disk%20images%20from%20the%20following%20sources%3A"
      },
      {
        "date": "2023-09-05T06:39:00.000Z",
        "voteCount": 1,
        "content": "Correct answer must be B:\nWhile Option D: will work,  there is NO mention of a pre-created snapshot, you cannot assume you already have a snapshot.  For Option D: to work, the answer should read: Create a snapshot, then create a custom image from the snapshot, then create instances from that image...."
      },
      {
        "date": "2023-08-27T16:53:00.000Z",
        "voteCount": 2,
        "content": "The correct answer is B.\nD is talking about \"creating image from a snapshot\" but what snapshot! we don't have one yet. so the B is the only full answer."
      },
      {
        "date": "2023-06-06T05:40:00.000Z",
        "voteCount": 2,
        "content": "Option D says, Create a custom Compute Engine image from a snapshot. For this we have to create a Snapshot first, we would not like to use an old snapshot. If we have to create a Snapshot, then we can directly use that Snapshot to create a VM, which is option B."
      },
      {
        "date": "2023-05-01T09:15:00.000Z",
        "voteCount": 1,
        "content": "use case of snapshot is for DR and backup. Images are more for creating identical VMs. So I would opt for Option D."
      },
      {
        "date": "2023-04-21T09:13:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/compute/docs/instances/create-start-instance\nTo quickly create more than one VM with the same boot disk, create a custom image, then create VMs from that image instead of using a snapshot.\nANS:D"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 132,
    "url": "https://www.examtopics.com/discussions/google/view/28163-exam-associate-cloud-engineer-topic-1-question-132/",
    "body": "You have deployed an application on a single Compute Engine instance. The application writes logs to disk. Users start reporting errors with the application. You want to diagnose the problem. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNavigate to Cloud Logging and view the application logs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConnect to the instance's serial console and read the application logs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a Health Check on the instance and set a Low Healthy Threshold value.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall and configure the Cloud Logging Agent and view the logs from Cloud Logging.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 25,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 21,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-09-10T12:23:00.000Z",
        "voteCount": 27,
        "content": "Answer: D \n\nApp logs can't be visible to Cloud Logging until we install Cloud Logging Agent on GCE"
      },
      {
        "date": "2021-08-16T07:03:00.000Z",
        "voteCount": 3,
        "content": "Hi all\ncheck this document and decide :)\nhttps://cloud.google.com/logging/docs/agent/logging/installation"
      },
      {
        "date": "2020-09-10T12:28:00.000Z",
        "voteCount": 8,
        "content": "Continuation of reasoning. \n\nIf Problem statement is not having this statement \"The application writes logs to disk\", then we might assume that application is writing logs on Cloud Logging with google-fluentd agent API library. However, problem statement is clearly mentioned that logs are writing down on disk, we need agent installed on GCE to fetch those logs from disk to Cloud Logging. If that is not desirable, then option B is left"
      },
      {
        "date": "2020-09-10T12:33:00.000Z",
        "voteCount": 3,
        "content": "(Correction) Answer is A after rethinking and doing some research by focusing words \"App Engine\", which has by default enabled Request Logs which has App logs on each request and those logs are enabled for Cloud Logging .. \nhttps://cloud.google.com/appengine/docs/standard/python/logs#request_logs_vs_application_logs"
      },
      {
        "date": "2020-09-17T05:45:00.000Z",
        "voteCount": 3,
        "content": "Cloud logging without agent only works for App engine as you stated . but the question is about the compute engine which has to be equipped first with Logging Agent in order to write logs into Cloud Logging. so based your assumption the correct answer is \"D\""
      },
      {
        "date": "2020-11-10T13:00:00.000Z",
        "voteCount": 2,
        "content": "Wrong! Request Logs has the LIST of App logs and ONLY associated with that request! Read the links you provide!"
      },
      {
        "date": "2020-11-21T16:01:00.000Z",
        "voteCount": 1,
        "content": "you must still install the agent:\n\nhttps://cloud.google.com/error-reporting/docs/setup/compute-engine#using_logging"
      },
      {
        "date": "2020-08-21T19:00:00.000Z",
        "voteCount": 18,
        "content": "Correct Answer is (D):\n\nIn its default configuration, the Logging agent streams logs from common third-party applications and system software to Logging; review the list of default logs. You can configure the agent to stream additional logs; go to Configuring the Logging agent for details on agent configuration and operation.\n\nIt is a best practice to run the Logging agent on all your VM instances. The agent runs under both Linux and Windows. To install the Logging agent, go to Installing the agent.\n\nhttps://cloud.google.com/logging/docs/agent"
      },
      {
        "date": "2020-08-23T09:30:00.000Z",
        "voteCount": 1,
        "content": "Cloud logging enabled by default for compute engine"
      },
      {
        "date": "2022-07-08T20:53:00.000Z",
        "voteCount": 1,
        "content": "Do you mean the logging agent is installed by default? It depends on the OS you decide. For example, it is installed in Ubuntu but not on RedHat or Windows. Besides installing of the agent is not enough. You have to configure and let the agent know where your application is writing the logs on the disk so that it can monitor and stream the log to cloud monitoring. D is the correct answer"
      },
      {
        "date": "2020-08-24T12:10:00.000Z",
        "voteCount": 11,
        "content": "CORRECTION.\nCorrect Answer is (A):\n\nActivity logging is enabled by default for all Compute Engine projects.\n\nYou can see your project's activity logs through the Logs Viewer in the Google Cloud Console:\n\nIn the Cloud Console, go to the Logging page.\nGo to the Logging page\n\nWhen in the Logs Viewer, select and filter your resource type from the first drop-down list.\nFrom the All logs drop-down list, select compute.googleapis.com/activity_log to see Compute Engine activity logs.\nhttps://cloud.google.com/compute/docs/logging/activity-logs#viewing_logs\n\nBesides:\nActivity logs are provided as part of the Cloud Logging service. For more information about Logging in general, read the Cloud Logging documentation.\n\nhttps://cloud.google.com/compute/docs/logging/activity-logs"
      },
      {
        "date": "2021-08-11T09:06:00.000Z",
        "voteCount": 20,
        "content": "I feel sorry for the woman in your life."
      },
      {
        "date": "2021-09-11T22:10:00.000Z",
        "voteCount": 2,
        "content": "Haha! That was funny"
      },
      {
        "date": "2020-09-02T03:16:00.000Z",
        "voteCount": 9,
        "content": "Activity Logs do not include 2rd party application logs. Activity logs are more related to operations and changes in the infrastructure. This question is tricky, I think it's either D or B, because if it's only an application on a single instance, you can connect to the instance and read the application logs directly and you save the cost of logging agent."
      },
      {
        "date": "2020-09-02T03:21:00.000Z",
        "voteCount": 1,
        "content": "Maybe I was assuming serial console is the same than system console, technically I guess they're not the same, hence I guess D will be my chosen answer."
      },
      {
        "date": "2020-08-24T12:19:00.000Z",
        "voteCount": 3,
        "content": "Additional information about VM Image for AWS EC2:\nThe Logging agent streams logs from your VM instances and from selected third-party software packages to Cloud Logging. It is a best practice to run the Logging agent on all your VM instances.\n\nThe VM images for Compute Engine and Amazon Elastic Compute Cloud (EC2) don't include the Logging agent, so you must complete these steps to install it on those instances. The agent runs under both Linux and Windows.\n\nIf your VMs are running in Google Kubernetes Engine or App Engine, the agent is already included in the VM image, so you can skip this page."
      },
      {
        "date": "2020-12-31T13:27:00.000Z",
        "voteCount": 4,
        "content": "This points to D then"
      },
      {
        "date": "2024-09-04T09:49:00.000Z",
        "voteCount": 1,
        "content": "Questions are tricky but let's reiterate this question. Hints are that there's an error in the application and that log are written to disk. Which means, it continues to write to the disk where the instance is functional. Therefore, correct method should be to install the agent and then analyze further on the output of the logs. So, answer is D"
      },
      {
        "date": "2024-05-25T09:06:00.000Z",
        "voteCount": 1,
        "content": "This question is from 2020, in that year there was an Logging agent, now called Legacy Logging agent. It is the Ops Agent of nowadays.\nWith that agent, yes, you can configurate it to send personalized logs to GCP. But I think this question says that the app is already malfunctioning, so the logs of that are in the disk. For me it is B."
      },
      {
        "date": "2024-05-29T12:06:00.000Z",
        "voteCount": 1,
        "content": "for me is b, but there is a recent question 2023: 231 in examtopics.\nAnd D answer is:\n\"D. Install and configure the Ops agent and view the logs from Cloud Logging.\"\nSo to pass the exam, here select D. But for me, the question is confused.  Because they want to see past logs, so install ops agent will not show past logs of application."
      },
      {
        "date": "2024-02-21T03:37:00.000Z",
        "voteCount": 1,
        "content": "B will be correct if we talk about VM issues (access to an instance's serial console to debug boot and networking issues, troubleshoot malfunctioning instances, interact with the GRand Unified Bootloader (GRUB), and perform other troubleshooting tasks.)"
      },
      {
        "date": "2023-12-20T07:04:00.000Z",
        "voteCount": 1,
        "content": "Selected Answer: D\n\nWhen and why do we need it? Serial console access is useful in the following situations:\nWhen the VM is not booting: You can use serial console access to see the boot messages and identify the problem.\nWhen the VM is hung: You can use serial console access to see what the VM is doing and try to unfreeze it.\nWhen you need to access the VM\u2019s BIOS or UEFI: You can use serial console access to access the VM\u2019s BIOS or UEFI, which can be useful for changing settings or troubleshooting problems.\nResolving issues with the VM\u2019s operating system."
      },
      {
        "date": "2023-12-07T04:03:00.000Z",
        "voteCount": 1,
        "content": "Its B, App logs are not provided by default and requires to have an agent installed.\nhttps://cloud.google.com/compute/docs/troubleshooting/troubleshooting-using-serial-console"
      },
      {
        "date": "2023-09-03T22:25:00.000Z",
        "voteCount": 3,
        "content": "D  makes more sense , as application writes logs to disk and to diagnose it we need the cloud logging agent"
      },
      {
        "date": "2023-04-30T13:17:00.000Z",
        "voteCount": 2,
        "content": "The line \"application writes logs to disk\" is crucial. It means logs are not available in cloud logging to yet. Hence we need to install the logging agent to send the logs to Cloud Logging. \n\nAnswer is D"
      },
      {
        "date": "2024-05-25T09:06:00.000Z",
        "voteCount": 1,
        "content": "yes you are wright, so you need to see de past logs, so it is B. For the future, after installation of Cloud Logging (now Ops Agent) we will be able to see them in Cloud Logging."
      },
      {
        "date": "2023-04-06T10:41:00.000Z",
        "voteCount": 1,
        "content": "Rethinking it again: \nThe serial console provides a text-based interface that allows you to view the boot-up process and access the login prompt of the instance. From there, you can troubleshoot issues, change configuration settings or perform other administrative tasks.\n\nThe serial console is a useful tool for troubleshooting issues with Compute Engine instances, especially in situations where network connectivity is not available or where the operating system is not functioning properly."
      },
      {
        "date": "2023-04-06T10:38:00.000Z",
        "voteCount": 3,
        "content": "Errors are already on the disk, just ssh to the instance and read them"
      },
      {
        "date": "2023-02-11T02:00:00.000Z",
        "voteCount": 2,
        "content": "To configure logging for your GCP VMs, you need to install the Cloud Logging agent on each of your instances. The Cloud Logging agent collects log data from the instance and sends it to the Cloud Logging API, where it is stored and can be viewed in the Cloud Logging console."
      },
      {
        "date": "2023-01-30T02:36:00.000Z",
        "voteCount": 2,
        "content": "We need to diagnose it quickly - simply login to instance and check logs. When there's application outage, it's not good idea to install cloud agent and wait until logs appear in the Cloud Logging"
      },
      {
        "date": "2023-02-02T22:55:00.000Z",
        "voteCount": 1,
        "content": "Serial port is not used for application debugging generally. It is mainly used for boot issues, network connectivity issues, accessing VM not possible by other menthods"
      },
      {
        "date": "2022-12-12T09:54:00.000Z",
        "voteCount": 1,
        "content": "logging agent is deprecated,i find this in documentation\nhttps://cloud.google.com/logging/docs/agent/logging/installation\n\" While this agent is still supported, we recommend that you use the Ops Agent for new Google Cloud workloads and eventually transition your existing Compute Engine VMs to use the new agent. The Ops Agent, which combines the collection of metrics and logging into a single agent, is the eventual replacement for the existing agents.\"\n\nso the question points to the logging agent not to the ops agent so i think correct answer is A"
      },
      {
        "date": "2022-11-30T07:58:00.000Z",
        "voteCount": 1,
        "content": "D \nInstalling the Cloud Logging agent on individual VMs\n\nIf your VMs are running in Google Kubernetes Engine or App Engine, the agent is already included in the VM image, so you can skip this page."
      },
      {
        "date": "2022-10-12T09:45:00.000Z",
        "voteCount": 1,
        "content": "D. Install and configure the Cloud Logging Agent and view the logs from Cloud Logging"
      },
      {
        "date": "2022-10-08T08:08:00.000Z",
        "voteCount": 1,
        "content": "D is the correct answer,\nInstall fluentD logging agent to compute engine and view the logs from cloud logging."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 133,
    "url": "https://www.examtopics.com/discussions/google/view/27834-exam-associate-cloud-engineer-topic-1-question-133/",
    "body": "An application generates daily reports in a Compute Engine virtual machine (VM). The VM is in the project corp-iot-insights. Your team operates only in the project corp-aggregate-reports and needs a copy of the daily exports in the bucket corp-aggregate-reports-storage. You want to configure access so that the daily reports from the VM are available in the bucket corp-aggregate-reports-storage and use as few steps as possible while following Google-recommended practices. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMove both projects under the same folder.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGrant the VM Service Account the role Storage Object Creator on corp-aggregate-reports-storage.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Shared VPC network between both projects. Grant the VM Service Account the role Storage Object Creator on corp-iot-insights.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMake corp-aggregate-reports-storage public and create a folder with a pseudo-randomized suffix name. Share the folder with the IoT team."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 13,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-08-21T19:07:00.000Z",
        "voteCount": 35,
        "content": "Correct Answer is (B):\n\nPredefined roles\nThe following table describes Identity and Access Management (IAM) roles that are associated with Cloud Storage and lists the permissions that are contained in each role. Unless otherwise noted, these roles can be applied either to entire projects or specific buckets.\n\nStorage Object Creator (roles/storage.objectCreator)\tAllows users to create objects. Does not give permission to view, delete, or overwrite objects.\n\nhttps://cloud.google.com/storage/docs/access-control/iam-roles#standard-roles"
      },
      {
        "date": "2020-08-24T12:41:00.000Z",
        "voteCount": 6,
        "content": "Basically, you are giving the  permissions to the VM Service Account to create a copy of the daily report on the bucket that the other team has access."
      },
      {
        "date": "2020-08-09T21:17:00.000Z",
        "voteCount": 14,
        "content": "i think is B"
      },
      {
        "date": "2020-08-09T21:18:00.000Z",
        "voteCount": 1,
        "content": "Object creator cant see object so i think is D"
      },
      {
        "date": "2021-05-09T14:27:00.000Z",
        "voteCount": 6,
        "content": "VM doesn't need to see the obects - just to create them. It's B:\n\nThe VM is located in project \"corp-iot-insights\" - give its SA the Storage Object Creator role for bucket \"corp-aggregate-reports-storage\" that is located in project \"corp-aggregate-reports\", where your team operates."
      },
      {
        "date": "2024-02-02T18:50:00.000Z",
        "voteCount": 1,
        "content": "Guys ..shared VPC is the key to connect projects. Enjoy"
      },
      {
        "date": "2024-02-27T09:47:00.000Z",
        "voteCount": 1,
        "content": "IAM provides granular control over object-level access, which is a better security practice than opening up entire network segments with Shared VPC. Granting the necessary Storage Object Creator permission directly to the source VM's service account is the most streamlined way to achieve the file transfer with the principle of least privilege. Hypothetically, if the VM also needed access to databases or other network resources in the corp-aggregate-reports project, then Shared VPC could be the appropriate solution."
      },
      {
        "date": "2023-09-03T22:33:00.000Z",
        "voteCount": 1,
        "content": "B is the correct as it gives the service account required access"
      },
      {
        "date": "2023-08-14T02:10:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer is B"
      },
      {
        "date": "2023-07-05T19:45:00.000Z",
        "voteCount": 2,
        "content": "Just take below sentence from the question which is added just for confusion :)\n\n\"Your team operates only in the project corp-aggregate-reports\""
      },
      {
        "date": "2022-09-15T00:32:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer is (B)"
      },
      {
        "date": "2022-08-17T18:18:00.000Z",
        "voteCount": 1,
        "content": "If that is the default service Account of the Compute Instance, then we should do nothing. As the role is already included. Either way, we should do nothing as the role is already covered. Also we shouldn\u00b4t modify Compute instance Service account. But again, I will assume it is not the default."
      },
      {
        "date": "2022-06-23T15:12:00.000Z",
        "voteCount": 1,
        "content": "B is right"
      },
      {
        "date": "2022-06-16T08:43:00.000Z",
        "voteCount": 1,
        "content": "b it is"
      },
      {
        "date": "2022-01-27T00:53:00.000Z",
        "voteCount": 4,
        "content": "You should be able to add a service account to another project:\n\nCreate the first service account in project A in the Cloud Console. Activate it using gcloud auth activate-service-account.\n\nIn the Cloud Console, navigate to project B. Find the \"IAM &amp; admin\" &gt; \"IAM\" page. Click the \"Add\" button. In the \"New members\" field paste the name of the service account (it should look like a strange email address) and give it the appropriate role.\n\nRun gcloud commands with --project set to project B. They should succeed (I just manually verified that this will work).\n\nAutomatic creation of service accounts is something that we're hesitant to do until we can work through all of the security ramifications.\n\nhttps://stackoverflow.com/a/35558464"
      },
      {
        "date": "2022-01-23T06:50:00.000Z",
        "voteCount": 3,
        "content": "It's B since bucket names are globally unique so it's enough to refer to them when you've proper role assigned"
      },
      {
        "date": "2021-05-20T07:25:00.000Z",
        "voteCount": 1,
        "content": "B, assign access is less step"
      },
      {
        "date": "2021-03-17T01:58:00.000Z",
        "voteCount": 3,
        "content": "From stackoverflow: Bucket names are globally unique, so your app will refer to an existing bucket in another project in the same way that it refers to buckets in its own project. Hence the shared VPC is not required to access the bucket. Just the IAM role."
      },
      {
        "date": "2021-03-09T02:02:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2021-02-27T12:19:00.000Z",
        "voteCount": 2,
        "content": "B - Grant the VM Service Account the role Storage Object Creator on corp-aggregate-reports-storage."
      },
      {
        "date": "2021-02-10T18:25:00.000Z",
        "voteCount": 1,
        "content": "B - Grant the VM Service Account the role Storage Object Creator on corp-aggregate-reports-storage."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 134,
    "url": "https://www.examtopics.com/discussions/google/view/28090-exam-associate-cloud-engineer-topic-1-question-134/",
    "body": "You built an application on your development laptop that uses Google Cloud services. Your application uses Application Default Credentials for authentication and works fine on your development laptop. You want to migrate this application to a Compute Engine virtual machine (VM) and set up authentication using Google- recommended practices and minimal changes. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAssign appropriate access for Google services to the service account used by the Compute Engine VM.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a service account with appropriate access for Google services, and configure the application to use this account.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore credentials for service accounts with appropriate access for Google services in a config file, and deploy this config file with your application.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore credentials for your user account with appropriate access for Google services in a config file, and deploy this config file with your application."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 19,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 18,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-08-21T19:27:00.000Z",
        "voteCount": 54,
        "content": "Correct Answer is (B):\n\nBest practices\nIn general, Google recommends that each instance that needs to call a Google API should run as a service account with the minimum permissions necessary for that instance to do its job. In practice, this means you should configure service accounts for your instances with the following process:\n\nCreate a new service account rather than using the Compute Engine default service account.\nGrant IAM roles to that service account for only the resources that it needs.\nConfigure the instance to run as that service account.\nGrant the instance the https://www.googleapis.com/auth/cloud-platform scope to allow full access to all Google Cloud APIs, so that the IAM permissions of the instance are completely determined by the IAM roles of the service account.\nAvoid granting more access than necessary and regularly check your service account permissions to make sure they are up-to-date.\n\nhttps://cloud.google.com/compute/docs/access/create-enable-service-accounts-for-instances#best_practices"
      },
      {
        "date": "2021-12-07T23:31:00.000Z",
        "voteCount": 1,
        "content": "you just gave justification for option A which is right"
      },
      {
        "date": "2022-08-06T00:42:00.000Z",
        "voteCount": 5,
        "content": "Maybe for the option A you are modifying the default service account because it's not explain which service account used by the VM, is it the default one or the new one?\n\nThe best practice is to Create a new service account rather than using the Compute Engine default service account.\n\nB still has the bigger prove here as the answer."
      },
      {
        "date": "2022-08-06T00:55:00.000Z",
        "voteCount": 1,
        "content": "You should read lxgywil comment. His comment explains how authentication works to access Google Services in your application.\n\na relevant link also:\nhttps://cloud.google.com/storage/docs/reference/libraries#setting_up_authentication"
      },
      {
        "date": "2020-12-20T08:36:00.000Z",
        "voteCount": 20,
        "content": "From your quote:\nConfigure the \"instance\" to run as that service account.\nFrom answer B:\n and configure the \"application\" to use this account.\n\nYou don't add service accounts to applications, ans A"
      },
      {
        "date": "2020-12-31T14:30:00.000Z",
        "voteCount": 1,
        "content": "wording is the clue :)"
      },
      {
        "date": "2021-02-21T14:21:00.000Z",
        "voteCount": 7,
        "content": "It's dirty play with words... All understand that we need custom SA, grant required permissions and attach this SA to the VM...\nWhy Google does this?"
      },
      {
        "date": "2021-05-09T14:37:00.000Z",
        "voteCount": 3,
        "content": "When you use a GCP service within your app (code), you have to use its client libraries. When you instantiate a client with client libraries you can pass it a Service Account key, which will define on behalf of which SA the client will be acting. That's how you can configure your app to use a particular service account.\n\nE.g. https://cloud.google.com/storage/docs/reference/libraries#using_the_client_library"
      },
      {
        "date": "2022-11-17T06:14:00.000Z",
        "voteCount": 1,
        "content": "In question it's written application uses application default credentials. So taking that as a hint. B is the answer because here we are configuring service account key into the application. Similar approach."
      },
      {
        "date": "2020-08-11T06:01:00.000Z",
        "voteCount": 20,
        "content": "I would choose: A. Assign appropriate access for Google services to the service account used by the Compute Engine VM.\nas there is no need to create a new service account."
      },
      {
        "date": "2020-08-15T11:17:00.000Z",
        "voteCount": 9,
        "content": "I agree, there is no need to create a new service account"
      },
      {
        "date": "2023-02-24T01:11:00.000Z",
        "voteCount": 5,
        "content": "by default a vm uses a default service account. if you grant permission to this service account it will apply to all VMs default service accounts in the project . in this case you need create a new service account and give it appropriate permission"
      },
      {
        "date": "2024-09-30T01:47:00.000Z",
        "voteCount": 1,
        "content": "Create a new user-managed service account rather than using the Compute Engine default service account, and grant IAM roles to that service account for only the resources and operations that it needs.\n\nhttps://cloud.google.com/compute/docs/access/create-enable-service-accounts-for-instances#best_practices"
      },
      {
        "date": "2024-09-21T04:53:00.000Z",
        "voteCount": 1,
        "content": "I think the \"minimal changes\" hint here would make the first option the more suitable one since it doesn't involve creating a new service account."
      },
      {
        "date": "2024-05-25T08:58:00.000Z",
        "voteCount": 1,
        "content": "Is not possible to add the service accounts to the application"
      },
      {
        "date": "2024-03-14T16:40:00.000Z",
        "voteCount": 1,
        "content": "B is the answer"
      },
      {
        "date": "2024-02-27T10:07:00.000Z",
        "voteCount": 3,
        "content": "I'd strongly lean towards Option B (Create a service account with appropriate access for Google services and configure the application to use this account) as the most likely correct answer. \n\nGoogle's exams emphasize secure design principles. The principle of least privilege is a core tenet, and custom service accounts embody this. Option B aligns precisely with the best practice for production environments and demonstrates a clear understanding of IAM concepts. While Option A could be acceptable with careful permission adjustments, exams often favor the solution most demonstrably secure and aligned with recommended practice out of the box.\nI believe option A might be a trap. Default service accounts can have varying levels of access. The exam might purposely use this ambiguity to test your knowledge of security principles. Focusing on the step of creating a custom service account signals your understanding of the correct IAM workflow."
      },
      {
        "date": "2024-01-01T12:49:00.000Z",
        "voteCount": 3,
        "content": "When you create a new Compute Engine VM, it is assigned a default service account, but this default service account is not unique to each VM. Instead, it's a project-wide default service account. \n1.\tProject-Wide Default Service Account:\n\u2022\tThe default service account is typically named something like PROJECT_NUMBER-compute@developer.gserviceaccount.com. It is the same across all VMs in the project that use the default service account.\n\u2022\tPermissions granted to this default service account apply to all VMs using this account, which could lead to potential security risks if not managed carefully, especially in projects with multiple VMs having different access requirements."
      },
      {
        "date": "2024-01-01T12:50:00.000Z",
        "voteCount": 2,
        "content": "2.\tCreating a New Service Account:\n\u2022\tFor better security and to adhere to the principle of least privilege, it's often recommended to create a new service account with just the necessary permissions for your specific application or VM.\n\u2022\tThis approach allows for more granular control over permissions and reduces the risk of inadvertently granting excessive privileges to all VMs using the default service account."
      },
      {
        "date": "2024-01-01T12:36:00.000Z",
        "voteCount": 1,
        "content": "When you run an application on a Compute Engine VM, the VM can use a service account to interact with Google Cloud services. This service account is attached to the VM and can be used to authenticate your application without needing to explicitly manage credentials.\n\n\u2022\tB. Configure the application to use a new service account: While this is a viable approach, it requires more changes to your application to explicitly use a new service account. Using the VM's service account with ADC requires fewer changes."
      },
      {
        "date": "2023-12-08T05:29:00.000Z",
        "voteCount": 2,
        "content": "B as option A is all about using default service account\nwhereas option B is to make new custom service account Google recommended ."
      },
      {
        "date": "2023-11-17T08:05:00.000Z",
        "voteCount": 1,
        "content": "By creating a service account, you can assign appropriate permissions for Google services to the service account and configure your application to use it. This way, your application can access Google services securely without having to store any user credentials or access tokens on the VM.\nsource1:https://cloud.google.com/compute/docs/access/create-enable-service-accounts-for-instances\nsource2: https://www.exam-answer.com/migrate-application-google-cloud-compute-engine-seo-friendly"
      },
      {
        "date": "2023-11-03T16:03:00.000Z",
        "voteCount": 1,
        "content": "I will go with B, by creating new SA and not by A , as SA used by the Compute Engine VM is by first choice a default SA and not user managed SA."
      },
      {
        "date": "2023-09-03T22:37:00.000Z",
        "voteCount": 1,
        "content": "A is the right answer , as after providing the appropriate access to the service account compute Engine"
      },
      {
        "date": "2023-08-06T05:10:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is \"B\".   In general, Google recommends that each instance that needs to call a Google API should run as a service\naccount with the minimum permissions necessary for that instance to do its job. In practice, this means you\nshould configure service accounts for your instances with the following process: Create a new service\naccount rather than using the Compute Engine default service account. Grant IAM roles to that service\naccount for only the resources that it needs. Configure the instance to run as that service account. Grant the\ninstance the https://www.googleapis.com/auth/cloud-platform scope to allow full access to all Google Cloud\nAPIs, so that the IAM permissions of the instance are completely determined by the IAM roles of the service\naccount. Avoid granting more access than necessary and regularly check your service account permissions to\nmake sure they are up-to-date."
      },
      {
        "date": "2023-08-02T03:54:00.000Z",
        "voteCount": 2,
        "content": "B\n\"Many of these Google Cloud services also provide a default service account. Using the default service account is not recommended, because by default the default service account is highly privileged, which violates the principle of least privilege.\"\nhttps://cloud.google.com/docs/authentication/provide-credentials-adc#attached-sa"
      },
      {
        "date": "2023-06-19T16:21:00.000Z",
        "voteCount": 2,
        "content": "Based on the documentation here : https://cloud.google.com/docs/authentication/application-default-credentials looks like the correct answer is \"A\". \n\nThis is exactly why ADC has been created for. You develop your code on your laptop and you using in your APP code ADC as a way to authorize to GCP - then depends on where you would like to test your code on - you simply execute `gcloud auth application-default login` in your system to store right credentials in your ADC on your laptop. \n\nWhen you copy your code into PROD VM, your app without any changes will scan below locations in order to find credentials : \n1. GOOGLE_APPLICATION_CREDENTIALS environment variable\n2. User credentials set up by using the Google Cloud CLI\n3. The attached service account, returned by the metadata server\n\nAs you can see above, your app will not find any credentials in 1. and 2. location but it will go to location 3, which is the credential for Service Account assigned to your VM.\n\nMinimal effort here means, you don't need to change your APP to get right credentials to GCP services."
      },
      {
        "date": "2023-07-14T07:32:00.000Z",
        "voteCount": 1,
        "content": "This seems the best answer. Using ADC enables the App itself to use the VM's SA after migration. No change to the app is needed."
      },
      {
        "date": "2023-05-14T08:57:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is A\n\nApplication Default Credentials (ADC) is not a declaration of permissions, but more of a directive to find permissions already granted to the application's environment. \n\nReferencing the link below, you could certainly perform answer B, but the question specifies \"minimal changes\". We assign appropriate permissions to the instance that will be running the application and the ADC will find them and use them to authenticate. Kind of dumps the permissions responsibility on the cloud engineer to make sure the instance has the proper permissions.\n\nhttps://google-auth.readthedocs.io/en/master/user-guide.html#application-default:~:text=Applications%20running%20on,compute_engine.Credentials%3A"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 135,
    "url": "https://www.examtopics.com/discussions/google/view/28091-exam-associate-cloud-engineer-topic-1-question-135/",
    "body": "You need to create a Compute Engine instance in a new project that doesn't exist yet. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUsing the Cloud SDK, create a new project, enable the Compute Engine API in that project, and then create the instance specifying your new project.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable the Compute Engine API in the Cloud Console, use the Cloud SDK to create the instance, and then use the --project  flag to specify a new project.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUsing the Cloud SDK, create the new instance, and use the --project  flag to specify the new project. Answer yes when prompted by Cloud SDK to enable the Compute Engine API.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable the Compute Engine API in the Cloud Console. Go to the Compute Engine section of the Console to create a new instance, and look for the Create In A New Project option in the creation form."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-08-21T20:21:00.000Z",
        "voteCount": 41,
        "content": "Correct Answer is (A):\nQuickstart: Creating a New Instance Using the Command Line\nBefore you begin\n1.\tIn the Cloud Console, on the project selector page, select or create a Cloud project.\n2.\tMake sure that billing is enabled for your Google Cloud project. Learn how to confirm billing is enabled for your project.\n\nTo use the gcloud command-line tool for this quickstart, you must first install and initialize the Cloud SDK:\n1.\tDownload and install the Cloud SDK using the instructions given on Installing Google Cloud SDK.\n2.\tInitialize the SDK using the instructions given on Initializing Cloud SDK.\nTo use gcloud in Cloud Shell for this quickstart, first activate Cloud Shell using the instructions given on Starting Cloud Shell.\n\nhttps://cloud.google.com/ai-platform/deep-learning-vm/docs/quickstart-cli#before-you-begin"
      },
      {
        "date": "2020-08-11T06:02:00.000Z",
        "voteCount": 13,
        "content": "I would choose A. Using the Cloud SDK, create a new project, enable the Compute Engine API in that project, and then create the instance specifying your new project.\nas first I need to create a project. Instance creation cannot automatically create a project."
      },
      {
        "date": "2024-03-07T19:49:00.000Z",
        "voteCount": 4,
        "content": "I always think of \"PAP\" to help me with this question: Project: A new project must exist as a container for all your resources.\nAPI: Enable the necessary API, in this case, the Compute Engine API, so you can use its services.\nProvision: Now you can actually create the resource, like your Compute Engine instance."
      },
      {
        "date": "2023-09-03T22:42:00.000Z",
        "voteCount": 2,
        "content": "A is the correct answer, as it follow the right path , to create the compute Engine instance"
      },
      {
        "date": "2023-08-14T02:16:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer is A"
      },
      {
        "date": "2021-12-24T04:46:00.000Z",
        "voteCount": 2,
        "content": "Vote A"
      },
      {
        "date": "2021-12-14T02:32:00.000Z",
        "voteCount": 2,
        "content": "A is the correct option"
      },
      {
        "date": "2021-11-19T14:58:00.000Z",
        "voteCount": 2,
        "content": "Correct Ans is : A"
      },
      {
        "date": "2021-05-20T07:26:00.000Z",
        "voteCount": 2,
        "content": "A is correct"
      },
      {
        "date": "2021-04-17T09:40:00.000Z",
        "voteCount": 1,
        "content": "A the way to go"
      },
      {
        "date": "2021-03-22T22:10:00.000Z",
        "voteCount": 1,
        "content": "Vote A"
      },
      {
        "date": "2021-03-15T06:15:00.000Z",
        "voteCount": 1,
        "content": "A for me"
      },
      {
        "date": "2021-03-09T11:00:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2021-03-06T05:53:00.000Z",
        "voteCount": 1,
        "content": "A is the right answer"
      },
      {
        "date": "2021-02-27T12:30:00.000Z",
        "voteCount": 2,
        "content": "A. Using the Cloud SDK, create a new project, enable the Compute Engine API in that project, and then create the instance specifying your new project."
      },
      {
        "date": "2021-02-22T19:09:00.000Z",
        "voteCount": 1,
        "content": "I think is A: https://cloud.google.com/sdk/gcloud/reference/projects/create"
      },
      {
        "date": "2021-02-12T06:16:00.000Z",
        "voteCount": 2,
        "content": "yep has to be A, new project must be created first"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 136,
    "url": "https://www.examtopics.com/discussions/google/view/28092-exam-associate-cloud-engineer-topic-1-question-136/",
    "body": "Your company runs one batch process in an on-premises server that takes around 30 hours to complete. The task runs monthly, can be performed offline, and must be restarted if interrupted. You want to migrate this workload to the cloud while minimizing cost. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the workload to a Compute Engine Preemptible VM.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the workload to a Google Kubernetes Engine cluster with Preemptible nodes.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the workload to a Compute Engine VM. Start and stop the instance as needed.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Instance Template with Preemptible VMs On. Create a Managed Instance Group from the template and adjust Target CPU Utilization. Migrate the workload."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 14,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-08-21T04:10:00.000Z",
        "voteCount": 45,
        "content": "i understand preemptible as a no-go because of \"must be restarted if interrupted\" here meaning \"starting from scratch\" . So C seems right"
      },
      {
        "date": "2021-07-16T05:46:00.000Z",
        "voteCount": 6,
        "content": "I agree, C.\nyou won't run 30 hours job on preemptible instances that can be stopped at any time and can't run more than 24 hours.\nIf the job could be splitted, then preemptible VM is an option."
      },
      {
        "date": "2021-09-27T17:16:00.000Z",
        "voteCount": 1,
        "content": "Preemptible seems fine on batch jobs for at least 24hours, not the case in here"
      },
      {
        "date": "2021-10-12T02:32:00.000Z",
        "voteCount": 6,
        "content": "I agree with C. You can't risk running a processes that take 30 hours on a preemptible VM (Compute Engine always stops preemptible instances after they run for 24 hours). They are good for \"short-lived\" batch jobs. The scenario is NOT fault tolerant as the whole process restarts if interrupted. \n\nhttps://cloud.google.com/compute/docs/instances/preemptible"
      },
      {
        "date": "2020-08-26T06:36:00.000Z",
        "voteCount": 16,
        "content": "Preemptible will be perfect for a batch job that takes less than 24 hours. But it's not in this case."
      },
      {
        "date": "2021-05-01T06:12:00.000Z",
        "voteCount": 5,
        "content": "What if it is a managed group of Pre emptible instances like in D. If one instance stops, another instance will take over.\n\nI choose D."
      },
      {
        "date": "2021-06-03T15:54:00.000Z",
        "voteCount": 1,
        "content": "is there an option to specify Pre emptible instances while creating template? I couldn't find that. If so then D can't be true"
      },
      {
        "date": "2021-06-12T22:56:00.000Z",
        "voteCount": 2,
        "content": "Yes under management&gt; Availability policy &gt; premptibility  ON/OFF"
      },
      {
        "date": "2024-09-24T09:10:00.000Z",
        "voteCount": 1,
        "content": "Option A"
      },
      {
        "date": "2023-09-03T22:49:00.000Z",
        "voteCount": 2,
        "content": "Option C is correct, bcoz the job is running for more than 30 hours"
      },
      {
        "date": "2023-08-14T02:18:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer is C"
      },
      {
        "date": "2023-07-05T19:54:00.000Z",
        "voteCount": 1,
        "content": "Job runs for 30 hours and must be restarted if interrupted are \"indirectly proportional\" to \"Preemptible\"\n\nAns: C"
      },
      {
        "date": "2022-12-06T08:15:00.000Z",
        "voteCount": 4,
        "content": "Preemptible VMs are cheaper, but they will not be available beyond 24hrs"
      },
      {
        "date": "2022-10-08T08:32:00.000Z",
        "voteCount": 2,
        "content": "C is the correct answer,\nInstall the workload in a compute engine VM, start and stop the instance as needed, because as per the question the VM runs for 30 hours, process can be performed offline and should not be interrupted, if interrupted we need to restart the batch process again. Preemptible VMs are cheaper, but they will not be available beyond 24hrs, and if the process gets interrupted the preemptible VM will restart."
      },
      {
        "date": "2022-08-12T11:41:00.000Z",
        "voteCount": 1,
        "content": "C is the correct answer"
      },
      {
        "date": "2022-08-07T21:49:00.000Z",
        "voteCount": 1,
        "content": "The preemptible instance in GKE is same as Compute Engine Instance. They have same behavior that will be last for 24 hours.\n\nAlso, see the key here \"...and must be restarted if interrupted.\". That means the job will start from the scratch again if the preemptible instance terminated. So, you will just wasted your preemptible instances because the job will never be finished.\n\nhttps://cloud.google.com/kubernetes-engine/docs/how-to/preemptible-vms#overview"
      },
      {
        "date": "2022-07-22T08:33:00.000Z",
        "voteCount": 1,
        "content": "Option D will achieve the goal here. If a preemptible VM goes down in 24 hours a new one will be built for running the batch process through the instance group configuration."
      },
      {
        "date": "2022-08-07T21:36:00.000Z",
        "voteCount": 2,
        "content": "\"...and must be restarted if interrupted.\"\n\nThe job will be start again from a scratch, then run again for another 24 hours in a new preemptible instance, the job will be terminated again after 24 hours, then start again from a scratch for another 24 hours in a new preemptible instance. This make the proces to be an infinite-loop process with wasted resources."
      },
      {
        "date": "2022-06-23T15:40:00.000Z",
        "voteCount": 1,
        "content": "A preemptible VM is an instance that you can create and run at a much lower price than normal instances. However, Compute Engine might stop (preempt) these instances if it requires access to those resources for other tasks. Preemptible instances are excess Compute Engine capacity, so their availability varies with usage.\nIf your apps are fault-tolerant and can withstand possible instance preemptions then preemptible instances can reduce your Compute Engine costs significantly. For example, batch processing jobs can run on preemptible instances. If some of those instances stop during processing, the job slows but does not completely stop. Preemptible instances complete your batch processing tasks without placing additional workload on your existing instances and without requiring you to pay full price for additional normal instances.\nHence, the correct answer is: Build an instance template configured to launch a Preemptible VM. Provision a managed instance group (MIG) from the template you just created. Adjust the Target CPU Utilization setting."
      },
      {
        "date": "2022-06-25T12:47:00.000Z",
        "voteCount": 1,
        "content": "C is correct , since job need to run 30 hours ."
      },
      {
        "date": "2022-05-20T10:11:00.000Z",
        "voteCount": 1,
        "content": "A - D are not possible because: For example, preemptible VMs can only run for up to 24 hours at a time (https://cloud.google.com/compute/docs/instances/preemptible)\nI agree with C"
      },
      {
        "date": "2022-04-28T14:19:00.000Z",
        "voteCount": 2,
        "content": "The answer is C. Many people are saying D but this is incorrect. Preemptible instances are finite Compute Engine resources, so they might not always be available. If there is a long running job that must be restarted, it does not make sense to use preemptible VMs as if there are no resources available, we will not be able to even run or restart the job. Thus, C is correct."
      },
      {
        "date": "2022-03-19T04:24:00.000Z",
        "voteCount": 1,
        "content": "auto restart not in Prem VM"
      },
      {
        "date": "2022-01-28T09:21:00.000Z",
        "voteCount": 1,
        "content": "It should be C Because a Preemptible VM can't run more than 24 hours"
      },
      {
        "date": "2022-01-22T01:45:00.000Z",
        "voteCount": 1,
        "content": "Answer is C \n- Since it is run on premise server, the expectation is to run it on Compute Engine. \n- Preemptible VMs are not an option as it runs for 24 hours only continuously. The batch job takes 30 hours and expects it to be restarted incase of interruptions. (Note it is restart &amp; not resume)"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 137,
    "url": "https://www.examtopics.com/discussions/google/view/28093-exam-associate-cloud-engineer-topic-1-question-137/",
    "body": "You are developing a new application and are looking for a Jenkins installation to build and deploy your source code. You want to automate the installation as quickly and easily as possible. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy Jenkins through the Google Cloud Marketplace.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new Compute Engine instance. Run the Jenkins executable.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new Kubernetes Engine cluster. Create a deployment for the Jenkins image.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an instance template with the Jenkins executable. Create a managed instance group with this template."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-08-11T06:09:00.000Z",
        "voteCount": 29,
        "content": "I would choose A. Deploy Jenkins through the Google Cloud Marketplace.\nas this is a well known opportunity on the GCP Marketplace"
      },
      {
        "date": "2020-08-21T20:46:00.000Z",
        "voteCount": 19,
        "content": "Correct Answer is (A):\n\nInstalling Jenkins\nIn this section, you use Cloud Marketplace to provision a Jenkins instance. You customize this instance to use the agent image you created in the previous section.\n\nGo to the Cloud Marketplace solution for Jenkins.\n\nClick Launch on Compute Engine.\n\nChange the Machine Type field to 4 vCPUs 15 GB Memory, n1-standard-4.\n\nMachine type selection for Jenkins deployment.\n\nClick Deploy and wait for your Jenkins instance to finish being provisioned. When it is finished, you will see:\n\nJenkins has been deployed.\n\nhttps://cloud.google.com/solutions/using-jenkins-for-distributed-builds-on-compute-engine#installing_jenkins"
      },
      {
        "date": "2023-09-03T22:50:00.000Z",
        "voteCount": 1,
        "content": "Answer is : A as it's the most quick option"
      },
      {
        "date": "2023-08-14T02:21:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer is C"
      },
      {
        "date": "2023-04-18T01:03:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2022-10-08T08:35:00.000Z",
        "voteCount": 1,
        "content": "A is correct answer,\nTo quickly deploy Jenkins, deploy it through google cloud marketplace."
      },
      {
        "date": "2022-08-12T11:44:00.000Z",
        "voteCount": 1,
        "content": "remember as quickly as possible, also Google encourage things to be performed in minimal steps so A is the quickest and easiest choice"
      },
      {
        "date": "2022-08-12T01:52:00.000Z",
        "voteCount": 1,
        "content": "Go for A, the easiest"
      },
      {
        "date": "2022-07-04T22:46:00.000Z",
        "voteCount": 2,
        "content": "This is Repeat Question."
      },
      {
        "date": "2022-06-23T15:41:00.000Z",
        "voteCount": 1,
        "content": "Cloud Market Place is fastest and best .. A is right"
      },
      {
        "date": "2022-05-20T10:12:00.000Z",
        "voteCount": 1,
        "content": "A - ... as quickly and easily as possible"
      },
      {
        "date": "2022-01-24T16:55:00.000Z",
        "voteCount": 1,
        "content": "A is right the rest is nonsense"
      },
      {
        "date": "2021-12-27T08:58:00.000Z",
        "voteCount": 1,
        "content": "As we need to automate installation of Jenkins as pre-requisite  ( upgrade etc for future), option C coz with Kubernetes  you can automate installation using charts ( easy to change few parameters)"
      },
      {
        "date": "2021-12-14T02:36:00.000Z",
        "voteCount": 1,
        "content": "A is right. Using Cloud Marketplace"
      },
      {
        "date": "2021-11-19T15:01:00.000Z",
        "voteCount": 1,
        "content": "I agree with A"
      },
      {
        "date": "2021-11-08T00:29:00.000Z",
        "voteCount": 1,
        "content": "A. The faster and best solution."
      },
      {
        "date": "2021-10-08T13:17:00.000Z",
        "voteCount": 1,
        "content": "its A."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 138,
    "url": "https://www.examtopics.com/discussions/google/view/28094-exam-associate-cloud-engineer-topic-1-question-138/",
    "body": "You have downloaded and installed the gcloud command line interface (CLI) and have authenticated with your Google Account. Most of your Compute Engine instances in your project run in the europe-west1-d zone. You want to avoid having to specify this zone with each CLI command when managing these instances.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the europe-west1-d zone as the default zone using the gcloud config subcommand.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the Settings page for Compute Engine under Default location, set the zone to europe\u05d2\u20ac\"west1-d.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the CLI installation directory, create a file called default.conf containing zone=europe\u05d2\u20ac\"west1\u05d2\u20ac\"d.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Metadata entry on the Compute Engine page with key compute/zone and value europe\u05d2\u20ac\"west1\u05d2\u20ac\"d."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 13,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-08-21T20:56:00.000Z",
        "voteCount": 29,
        "content": "Correct Answer is (A):\n\nChange your default zone and region in the metadata server\nNote: This only applies to the default configuration.\nYou can change the default zone and region in your metadata server by making a request to the metadata server. For example:\n\ngcloud compute project-info add-metadata \\\n    --metadata google-compute-default-region=europe-west1,google-compute-default-zone=europe-west1-b\n\nThe gcloud command-line tool only picks up on new default zone and region changes after you rerun the gcloud init command. After updating your default metadata, run gcloud init to reinitialize your default configuration.\n\nhttps://cloud.google.com/compute/docs/gcloud-compute#change_your_default_zone_and_region_in_the_metadata_server"
      },
      {
        "date": "2021-02-28T23:06:00.000Z",
        "voteCount": 4,
        "content": "Using gcloud config you can set the zone in your active configuration only. This setting does not apply to other gcloud configurations and does not become the default for the project.\n\nRef: https://cloud.google.com/sdk/gcloud/reference/config/set\n\nSo I believe correct answer is B as per https://cloud.google.com/compute/docs/regions-zones/changing-default-zone-region#console\n\nIn the Cloud Console, go to the Settings page.\nFrom the Zone drop-down menu, select a default zone."
      },
      {
        "date": "2021-04-04T06:59:00.000Z",
        "voteCount": 1,
        "content": "bro, it mentioned going into the console settings, not the compute engine settings!\nTo change your default region or zone:\n\nIn the Cloud Console, go to the Settings page.\n\nGo to the Settings page\n\nFrom the Region drop-down menu, select a default region.\n\nFrom the Zone drop-down menu, select a default zone."
      },
      {
        "date": "2021-07-16T05:53:00.000Z",
        "voteCount": 2,
        "content": "This setting in the Cloud Console won't be taken into account for gcloud on your active config"
      },
      {
        "date": "2020-09-12T14:07:00.000Z",
        "voteCount": 3,
        "content": "does your comment imply that the answer is D ? i'm confused"
      },
      {
        "date": "2022-01-19T03:32:00.000Z",
        "voteCount": 4,
        "content": "every thing is correct in your explanation but instead of using gcloud compute command they used gcloud config."
      },
      {
        "date": "2022-08-03T17:32:00.000Z",
        "voteCount": 4,
        "content": "You can use the gcloud config set command here, https://cloud.google.com/compute/docs/gcloud-compute#set_default_zone_and_region_in_your_local_client"
      },
      {
        "date": "2021-10-12T02:43:00.000Z",
        "voteCount": 3,
        "content": "I agree the answer is A.\n\ngcloud config - view and edit Cloud SDK properties\n\nzone\nDefault zone to use when working with zonal Compute Engine resources. \nhttps://cloud.google.com/sdk/gcloud/reference/config"
      },
      {
        "date": "2020-08-17T09:38:00.000Z",
        "voteCount": 11,
        "content": "I would go with the answer A"
      },
      {
        "date": "2024-02-08T08:49:00.000Z",
        "voteCount": 1,
        "content": "I would choose A\nreference : https://cloud.google.com/sdk/gcloud/reference/config/set"
      },
      {
        "date": "2023-11-03T16:10:00.000Z",
        "voteCount": 1,
        "content": "gcloud config set compute/zone europe-west1-b"
      },
      {
        "date": "2023-09-03T22:52:00.000Z",
        "voteCount": 1,
        "content": "A is the correct answer, just set default in cloud console in the starting by the cloud shell commands"
      },
      {
        "date": "2023-08-14T02:25:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer is A"
      },
      {
        "date": "2023-01-23T20:16:00.000Z",
        "voteCount": 1,
        "content": "We can set the defailt zone using the below CLI command,\n\ngcloud config set compute/zone ZONE\n\nRefer : https://cloud.google.com/compute/docs/gcloud-compute#set_default_zone_and_region_in_your_local_client"
      },
      {
        "date": "2022-08-18T12:38:00.000Z",
        "voteCount": 1,
        "content": "It is clearly mentioned it is to be done via CLI not console"
      },
      {
        "date": "2022-06-23T15:43:00.000Z",
        "voteCount": 1,
        "content": "A is right"
      },
      {
        "date": "2022-06-09T04:06:00.000Z",
        "voteCount": 1,
        "content": "A, it is clearly mentioned it is to be done via CLI not console."
      },
      {
        "date": "2022-05-31T03:00:00.000Z",
        "voteCount": 1,
        "content": "Just run this commando on CLI\ngcloud config set compute/zone ZONE"
      },
      {
        "date": "2022-03-19T07:45:00.000Z",
        "voteCount": 2,
        "content": "Ans: A\n\n\"You want to avoid having to specify this zone with each CLI command\"\nits about CLI not console!"
      },
      {
        "date": "2022-02-28T07:41:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/compute/docs/regions-zones/changing-default-zone-region#console"
      },
      {
        "date": "2022-02-19T06:23:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/compute/docs/gcloud-compute\n\ngcloud config set compute/zone ZONE"
      },
      {
        "date": "2022-01-10T16:35:00.000Z",
        "voteCount": 1,
        "content": "in GCP qwiklabs, this is how they ask you set the zone primarily, so I go with A"
      },
      {
        "date": "2021-12-27T09:06:00.000Z",
        "voteCount": 2,
        "content": "Answer A. There is no file like default.config. In my labs i set default values  Zone/Region/project etc default values stored in the following location \ncat ~/.config/gcloud/configurations/config_default.\nWe can also set zone default using \" gcloud config set compute/zone &lt;zona_name&gt;\""
      },
      {
        "date": "2021-12-14T02:37:00.000Z",
        "voteCount": 1,
        "content": "A is the right option"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 139,
    "url": "https://www.examtopics.com/discussions/google/view/28095-exam-associate-cloud-engineer-topic-1-question-139/",
    "body": "The core business of your company is to rent out construction equipment at large scale. All the equipment that is being rented out has been equipped with multiple sensors that send event information every few seconds. These signals can vary from engine status, distance traveled, fuel level, and more. Customers are billed based on the consumption monitored by these sensors. You expect high throughput `\" up to thousands of events per hour per device `\" and need to retrieve consistent data based on the time of the event. Storing and retrieving individual signals should be atomic. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a file in Cloud Storage per device and append new data to that file.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a file in Cloud Filestore per device and append new data to that file.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIngest the data into Datastore. Store data in an entity group based on the device.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIngest the data into Cloud Bigtable. Create a row key based on the event timestamp.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 10,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-09-11T02:27:00.000Z",
        "voteCount": 48,
        "content": "Answer: D\n\nKeyword need to look for \n- \"High Throughput\", \n- \"Consistent\", \n- \"Property based data insert/fetch like ngine status, distance traveled, fuel level, and more.\" which can be designed in column, \n- \"Large Scale Customer Base + Each Customer has multiple sensor which send event in seconds\" This will go for pera bytes situation, \n- Export data based on the time of the event. \n- Atomic\n\no BigTable will fit all requirement. \no DataStore is not fully Atomic \no CloudStorage is not a option where we can export data based on time of event. We need another solution to do that\no FireStore can be used with MobileSDK. \n\nSo go with Option D: Big Table"
      },
      {
        "date": "2020-09-20T23:05:00.000Z",
        "voteCount": 4,
        "content": "Its File store not firestore. But still, the argument is correct here as file store will not be automatic like cloud store that no SQL.\n\nIf it would be Firestore I would have gone with it, then big data  for throughout v/s cost."
      },
      {
        "date": "2020-08-16T10:21:00.000Z",
        "voteCount": 8,
        "content": "D is the best answer , Cloud Bigtable"
      },
      {
        "date": "2021-02-13T18:36:00.000Z",
        "voteCount": 6,
        "content": "Simple analogy.\nInformation every few seconds --&gt; Time Series --&gt; Big Table"
      },
      {
        "date": "2023-12-20T07:32:00.000Z",
        "voteCount": 1,
        "content": "Selected Answer: D\n\nhttps://cloud.google.com/bigtable/docs/overview"
      },
      {
        "date": "2023-11-30T06:48:00.000Z",
        "voteCount": 2,
        "content": "Answer D is the best for high throughput and IoT, but I concern about creating a row key based on the event timestamp, will it be leading a hotspots issue?"
      },
      {
        "date": "2023-09-03T23:01:00.000Z",
        "voteCount": 1,
        "content": "D is the right answer, as  its can help with Automatically"
      },
      {
        "date": "2023-02-18T01:55:00.000Z",
        "voteCount": 1,
        "content": "ReadModifyWriteRow requests are atomic:\nhttps://cloud.google.com/bigtable/docs/writes"
      },
      {
        "date": "2023-02-15T11:47:00.000Z",
        "voteCount": 1,
        "content": "it say any type of information. it means there can be an image file for instance. so, Bigtable is the best fit for this scenario"
      },
      {
        "date": "2023-01-04T03:14:00.000Z",
        "voteCount": 2,
        "content": "Answer: C\nkey work: atomic transaction\nhttps://cloud.google.com/datastore/docs/concepts/overview"
      },
      {
        "date": "2023-04-13T02:29:00.000Z",
        "voteCount": 1,
        "content": "high throughput == Bigtable, hence the answer is D"
      },
      {
        "date": "2022-09-24T10:34:00.000Z",
        "voteCount": 5,
        "content": "had this question today"
      },
      {
        "date": "2022-07-10T00:44:00.000Z",
        "voteCount": 4,
        "content": "Timeseries + IoT = Bigtable"
      },
      {
        "date": "2022-06-23T15:45:00.000Z",
        "voteCount": 2,
        "content": "This is related to IoT , it is no sql means BigTable. D is right"
      },
      {
        "date": "2022-05-21T11:18:00.000Z",
        "voteCount": 3,
        "content": "D - Ideal for use cases such as personalization, ad tech, fintech, digital media, and IoT\nhttps://cloud.google.com/bigtable"
      },
      {
        "date": "2022-05-10T14:48:00.000Z",
        "voteCount": 1,
        "content": "Big table for the exact reasons as @hitrshrup mentioned. Time related data flowing in and out will point you directly to BigTable without reading anything else"
      },
      {
        "date": "2022-04-23T08:17:00.000Z",
        "voteCount": 1,
        "content": "When we want to store data based on imestamp =&gt; Cloud Bigtable"
      },
      {
        "date": "2022-04-10T06:24:00.000Z",
        "voteCount": 1,
        "content": "Answer is D, large streaming data = bigtable"
      },
      {
        "date": "2022-03-31T15:48:00.000Z",
        "voteCount": 2,
        "content": "I vote c with DataStore (maybe even better with firestore includs pubsub ability)\nLooks like the internet agrees with me."
      },
      {
        "date": "2021-11-19T15:02:00.000Z",
        "voteCount": 2,
        "content": "Answer: D"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 140,
    "url": "https://www.examtopics.com/discussions/google/view/28495-exam-associate-cloud-engineer-topic-1-question-140/",
    "body": "You are asked to set up application performance monitoring on Google Cloud projects A, B, and C as a single pane of glass. You want to monitor CPU, memory, and disk. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable API and then share charts from project A, B, and C.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable API and then give the metrics.reader role to projects A, B, and C.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable API and then use default dashboards to view all projects in sequence.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable API, create a workspace under project A, and then add projects B and C.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 15,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-08-14T12:39:00.000Z",
        "voteCount": 39,
        "content": "D.  workspaces is made for monitoring multiple projects."
      },
      {
        "date": "2020-08-16T10:23:00.000Z",
        "voteCount": 12,
        "content": "D , Workspace to monitor multiple projects."
      },
      {
        "date": "2020-11-11T23:01:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/monitoring/workspaces"
      },
      {
        "date": "2023-09-03T23:05:00.000Z",
        "voteCount": 2,
        "content": "Option D is the correct Answer,  First create the Workspace under A then add it to the Project B and C"
      },
      {
        "date": "2023-02-14T01:06:00.000Z",
        "voteCount": 3,
        "content": "D. Enable API, create a workspace under project A, and then add projects B and C.\n\nTo monitor multiple Google Cloud projects in a single pane of glass, you can use Google Cloud's operations suite, formerly known as Stackdriver. By enabling the Cloud Monitoring API and creating a workspace under project A, you can add projects B and C to the same workspace. This will allow you to view metrics for CPU, memory, and disk usage for all projects in the same workspace. You can also set up alerting policies to be notified of any potential issues across all projects.\n\nEnabling the API alone or giving metrics.reader role to the projects will not provide a single pane of glass view of all the projects. Similarly, using default dashboards will not provide a unified view of all projects in a single dashboard."
      },
      {
        "date": "2022-12-19T21:57:00.000Z",
        "voteCount": 2,
        "content": "In question, mentioned 'as a single pane of glass' and workspace are meant for Monitoring"
      },
      {
        "date": "2022-10-08T15:23:00.000Z",
        "voteCount": 1,
        "content": "D is the correct answer,\nKeep Project A as host project in workspace and Project B and C as Service Project, and monitor the metrics of the Project A for a centralized view."
      },
      {
        "date": "2022-09-23T03:12:00.000Z",
        "voteCount": 2,
        "content": "Stackdriver workspaces are deprecated, now in the monitoring page of the Project you want, you need to select the \"Scopes\". Anyway he closest answer is D.\n\nScopes allow you to monitor multiple projects.\n\nhttps://cloud.google.com/monitoring/settings/multiple-projects"
      },
      {
        "date": "2022-08-12T11:53:00.000Z",
        "voteCount": 1,
        "content": "D should be the correct answer"
      },
      {
        "date": "2022-06-23T15:47:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2022-05-24T02:47:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2022-05-10T15:32:00.000Z",
        "voteCount": 2,
        "content": "D. One project must be the host (A) in this case and all others can be linked (B&amp;C)"
      },
      {
        "date": "2022-03-20T06:07:00.000Z",
        "voteCount": 1,
        "content": "You can add/link multiple project to a single workspace for monitoring"
      },
      {
        "date": "2022-02-11T02:55:00.000Z",
        "voteCount": 5,
        "content": "Wanted to check, if somebody has appeared ACE exam recently and if yes, what is the percentage of questions that come from this site? Thanks for answering."
      },
      {
        "date": "2022-02-07T10:32:00.000Z",
        "voteCount": 2,
        "content": "D is correct\nWorkspace can monitor multiple projects but a Google Cloud project can be monitored by exactly 1 Workspace."
      },
      {
        "date": "2022-02-06T04:39:00.000Z",
        "voteCount": 2,
        "content": "D is the correct"
      },
      {
        "date": "2021-11-26T08:35:00.000Z",
        "voteCount": 1,
        "content": "workspace is mandatory for monitoring"
      },
      {
        "date": "2021-11-19T15:03:00.000Z",
        "voteCount": 2,
        "content": "i Vote D"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 141,
    "url": "https://www.examtopics.com/discussions/google/view/28468-exam-associate-cloud-engineer-topic-1-question-141/",
    "body": "You created several resources in multiple Google Cloud projects. All projects are linked to different billing accounts. To better estimate future charges, you want to have a single visual representation of all costs incurred. You want to include new cost data as soon as possible. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Billing Data Export to BigQuery and visualize the data in Data Studio.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tVisit the Cost Table page to get a CSV export and visualize it using Data Studio.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFill all resources in the Pricing Calculator to get an estimate of the monthly cost.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Reports view in the Cloud Billing Console to view the desired cost information."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 8,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-08-24T01:42:00.000Z",
        "voteCount": 36,
        "content": "Highly recommended website for exam prep.just passed the exam."
      },
      {
        "date": "2021-11-03T05:43:00.000Z",
        "voteCount": 3,
        "content": "Mine is tommorow lol."
      },
      {
        "date": "2020-08-26T11:47:00.000Z",
        "voteCount": 6,
        "content": "Hope the best for my exam tomorrow"
      },
      {
        "date": "2020-09-09T15:57:00.000Z",
        "voteCount": 4,
        "content": "how did it go for you? I sit mine tomorrow."
      },
      {
        "date": "2021-10-12T21:04:00.000Z",
        "voteCount": 3,
        "content": "My exam is tomorrow, hope for the best."
      },
      {
        "date": "2021-11-10T12:37:00.000Z",
        "voteCount": 1,
        "content": "how did things go?"
      },
      {
        "date": "2021-05-20T07:28:00.000Z",
        "voteCount": 10,
        "content": "A is best answer"
      },
      {
        "date": "2022-12-28T10:06:00.000Z",
        "voteCount": 3,
        "content": "A is the best anmswer."
      },
      {
        "date": "2022-12-12T18:04:00.000Z",
        "voteCount": 2,
        "content": "It can also be D, as I checked, the console has a nice view now for reporting and this question is old."
      },
      {
        "date": "2022-10-19T04:18:00.000Z",
        "voteCount": 3,
        "content": "A is more correct as you can show data from multiple billing accounts as well as different projects."
      },
      {
        "date": "2022-10-12T10:01:00.000Z",
        "voteCount": 2,
        "content": "A. Configure Billing Data Export to BigQuery and visualize the data in Data Studio."
      },
      {
        "date": "2022-10-08T16:05:00.000Z",
        "voteCount": 2,
        "content": "D is the correct Answer,\nUse the Reports in the Cloud Billing console to view the billing information for the resources in multiple projects.\n\nhttps://cloud.google.com/billing/docs/how-to/view-linked"
      },
      {
        "date": "2022-12-27T19:46:00.000Z",
        "voteCount": 2,
        "content": "here projects are associated to different billing accounts! Reports view provides trends for a single billing account for date range. So ans is A"
      },
      {
        "date": "2023-02-12T04:38:00.000Z",
        "voteCount": 1,
        "content": "While D is a possible answer, it is not in this case because \"all projects are linked to different billing accounts\". \n\nBased on below quoted line from the link you shared, D would have been the  answer if all projects were linked to the same billing account:\n\n\"On the Account management page, linked projects are listed under Projects linked to this billing account.\""
      },
      {
        "date": "2022-08-12T11:54:00.000Z",
        "voteCount": 1,
        "content": "Would go with A (Configure Billing Data Export to BigQuery and visualize the data in Data Studio)"
      },
      {
        "date": "2022-06-23T15:49:00.000Z",
        "voteCount": 1,
        "content": "A is the best .."
      },
      {
        "date": "2022-06-09T04:14:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2021-03-09T02:41:00.000Z",
        "voteCount": 6,
        "content": "A is correct"
      },
      {
        "date": "2021-02-28T08:56:00.000Z",
        "voteCount": 6,
        "content": "A. Configure Billing Data Export to BigQuery and visualize the data in Data Studio."
      },
      {
        "date": "2021-02-16T21:53:00.000Z",
        "voteCount": 8,
        "content": "A, Any time you see question about billing and has BQ - 99.99% of time correct answer is that has BQ in it."
      },
      {
        "date": "2021-02-04T04:33:00.000Z",
        "voteCount": 3,
        "content": "A - Configure Billing Data Export to BigQuery and visualize the data in Data Studio."
      },
      {
        "date": "2020-12-16T10:47:00.000Z",
        "voteCount": 6,
        "content": "I was thinking A first. \n\nhttps://cloud.google.com/billing/docs/how-to/export-data-bigquery\n\n\"Cloud Billing export to BigQuery enables you to export detailed Google Cloud billing data (such as usage, cost estimates, and pricing data) automatically throughout the day to a BigQuery dataset that you specify.\"\n\nIt also says that:\n\"Be aware that your BigQuery dataset only reflects Google Cloud billing data incurred from the date you set up Cloud Billing export, and after. That is, Google Cloud billing data is not added retroactively, so you won't see Cloud Billing data from before you enable export.\"\n\nBut I am leaning towards alternative D after reading this page: https://cloud.google.com/billing/docs/how-to/reports. \n\nYou don't have to export/import to see data and you can see costs from different projects. All you need is the permission: billing.accounts.getSpendingInformation. It can forecast future costs too. According to the video at the same page, BiqQuery exports to Data Studio is for deeper cost analysis."
      },
      {
        "date": "2020-12-27T04:35:00.000Z",
        "voteCount": 9,
        "content": "As we can read in the 1st paragraph at https://cloud.google.com/billing/docs/how-to/reports, Reports View from Cloud Billing Console shows info about a single Billing Account.\nThis question says:\n- \"all projects are linked to different billing accounts\".\n- \"we want to have a single visual representation\".\n\nSo, in my opinion, the best answer is [A] (BQ+data studio)"
      },
      {
        "date": "2020-11-28T21:27:00.000Z",
        "voteCount": 1,
        "content": "I guess it's A.\nI'm not sure how to get that info for multiple billing accounts with the billing report as they say \"All projects are linked to different billing accounts.\""
      },
      {
        "date": "2020-11-24T04:31:00.000Z",
        "voteCount": 2,
        "content": "A or D  which one to choose ?"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 142,
    "url": "https://www.examtopics.com/discussions/google/view/28236-exam-associate-cloud-engineer-topic-1-question-142/",
    "body": "Your company has workloads running on Compute Engine and on-premises. The Google Cloud Virtual Private Cloud (VPC) is connected to your WAN over a<br>Virtual Private Network (VPN). You need to deploy a new Compute Engine instance and ensure that no public Internet traffic can be routed to it. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate the instance without a public IP address.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate the instance with Private Google Access enabled.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a deny-all egress firewall rule on the VPC network.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a route on the VPC to route all traffic to the instance over the VPN tunnel."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 10,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-09-09T16:02:00.000Z",
        "voteCount": 32,
        "content": "A for sure\n\nB - this allows internal communicaiton, but does nothing to limit public traffic\nC  - deny all is nice, but it's for egress -- we're looking for ingress\nD - this is way to invasive and doesn't explicitly address the issue of preventing public internet traffic from reaching your instance -- if it does, someone let me know how."
      },
      {
        "date": "2020-08-12T01:47:00.000Z",
        "voteCount": 13,
        "content": "A: answer looks right"
      },
      {
        "date": "2023-09-04T06:50:00.000Z",
        "voteCount": 1,
        "content": "A is the correct option, as other not limit the ingrres traffic"
      },
      {
        "date": "2023-02-09T05:39:00.000Z",
        "voteCount": 1,
        "content": "A for sure"
      },
      {
        "date": "2022-12-29T18:49:00.000Z",
        "voteCount": 1,
        "content": "A - for sure"
      },
      {
        "date": "2022-12-28T10:06:00.000Z",
        "voteCount": 1,
        "content": "A is the best anmswer."
      },
      {
        "date": "2022-12-07T20:03:00.000Z",
        "voteCount": 1,
        "content": "A for sure"
      },
      {
        "date": "2022-10-25T17:44:00.000Z",
        "voteCount": 2,
        "content": "should be A. VMs cannot communicate over the internet without a public IP address. Private Google Access permits access to Google APIs and services in Google's production infrastructure.\nhttps://cloud.google.com/vpc/docs/private-google-access"
      },
      {
        "date": "2022-10-11T10:25:00.000Z",
        "voteCount": 1,
        "content": "Elimination"
      },
      {
        "date": "2022-10-08T20:08:00.000Z",
        "voteCount": 1,
        "content": "D is the right answer, with private google access for on-premises host,\n\nPrivate Google Access for on-premises hosts\t\t\nOn-premises hosts with or without external IP addresses.\tConnect to Google APIs and services, from your on-premises network, through a Cloud VPN tunnel or Cloud Interconnect by using one of the Private Google Access-specific domains and VIPs.\tThe Google services that you can access depend on which Private Google Access-specific domain you use.\tUse this option to connect to Google APIs and services through a VPC network. This method doesn't require your on-premises hosts to have external IP addresses.\n\nplease refer to the link below for more insights,\nhttps://cloud.google.com/vpc/docs/private-google-access-hybrid"
      },
      {
        "date": "2022-07-10T03:23:00.000Z",
        "voteCount": 1,
        "content": "Through elimination - A"
      },
      {
        "date": "2022-06-23T15:50:00.000Z",
        "voteCount": 1,
        "content": "A is right"
      },
      {
        "date": "2022-05-07T06:01:00.000Z",
        "voteCount": 1,
        "content": "AAAAAAAAAA"
      },
      {
        "date": "2022-02-02T06:51:00.000Z",
        "voteCount": 3,
        "content": "A is correct"
      },
      {
        "date": "2021-11-28T20:54:00.000Z",
        "voteCount": 2,
        "content": "PRIVATE INSTANCE WITH A PUBLIC LOADBALANCER WOULD GO PUBLIC! IT'S CONFUSING"
      },
      {
        "date": "2021-12-07T11:13:00.000Z",
        "voteCount": 11,
        "content": "Why are you shouting here?"
      },
      {
        "date": "2023-04-18T22:38:00.000Z",
        "voteCount": 3,
        "content": "and why is he bringing Load Balancer in middle of nowhere?"
      },
      {
        "date": "2021-10-26T03:04:00.000Z",
        "voteCount": 5,
        "content": "The question is about ingress traffic from Internet\nA - If the VM does not have public IP it is not routable from Internet. Correct answear\nB - it is about how to access Google Services API. It does not tell about ingress Internet traffic\nC - It is about egress traffic\nD - It could be but we do not know anything about Internet ingress traffic to on prem. What's more default route tells about egress traffic to Internet. Nothing how Internet can access Compute instance.\n\nCorrect answer is A."
      },
      {
        "date": "2021-05-20T07:28:00.000Z",
        "voteCount": 4,
        "content": "A is correct"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 143,
    "url": "https://www.examtopics.com/discussions/google/view/28465-exam-associate-cloud-engineer-topic-1-question-143/",
    "body": "Your team maintains the infrastructure for your organization. The current infrastructure requires changes. You need to share your proposed changes with the rest of the team. You want to follow Google's recommended best practices. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Deployment Manager templates to describe the proposed changes and store them in a Cloud Storage bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Deployment Manager templates to describe the proposed changes and store them in Cloud Source Repositories.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tApply the changes in a development environment, run gcloud compute instances list, and then save the output in a shared Storage bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tApply the changes in a development environment, run gcloud compute instances list, and then save the output in Cloud Source Repositories."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 8,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-08-21T21:55:00.000Z",
        "voteCount": 32,
        "content": "Correct Answer is (A):\n\nConnecting to Cloud Storage buckets\nCloud Storage is a flexible, scalable, and durable storage option for your virtual machine instances. You can read and write files to Cloud Storage buckets from almost anywhere, so you can use buckets as common storage between your instances, App Engine, your on-premises systems, and other cloud services.\n\nhttps://cloud.google.com/compute/docs/disks/gcs-buckets\n\nWhy not (B)?\nCaution\n\nCloud Source Repositories are intended to store only the source code for your app and not user or personal data. Don't store any Core App Engine Customer Data (as defined in your License Agreement) in Cloud Source Repositories.\nhttps://cloud.google.com/source-repositories/docs/features"
      },
      {
        "date": "2021-07-28T10:41:00.000Z",
        "voteCount": 4,
        "content": "B is the ans"
      },
      {
        "date": "2021-07-27T04:26:00.000Z",
        "voteCount": 1,
        "content": "I agree with what are you saying, but the problem that you know how the deployment manager template looks? Is jinja/yaml file that means that are source code, so better to put them inside of an repository.\n\nSo, for my perpective I will go with the B."
      },
      {
        "date": "2021-08-16T08:58:00.000Z",
        "voteCount": 3,
        "content": "maybe below link will help \nhttps://cloud.google.com/deployment-manager/docs/configuration/templates/hosting-templates-externally\n\nfrom that we can take a idea on deciding  cloud storage or repo :),"
      },
      {
        "date": "2020-08-26T12:06:00.000Z",
        "voteCount": 12,
        "content": "you store the sensitive data NOT in the instance template, that is the current best practice. But you need version control like GIT or Google's GIT (Cloud Source Repo) to backup your code somehow and able to roll back if needed."
      },
      {
        "date": "2021-06-30T04:15:00.000Z",
        "voteCount": 9,
        "content": "B is the answer. Deployment Manager Template can be written in either Jinja or Python, this is Infrastructure as Code (IaC) we are talking about here, same as AWS Cloudformation, or Terraform. Therefore, they should be stored on a git repository such as Google Cloud Source Repositories."
      },
      {
        "date": "2020-09-20T23:31:00.000Z",
        "voteCount": 2,
        "content": "You can do all thing you are mentioning in the cloud store also. Ethically answer is cloud store as you are not dealing with a source file but a template. Again here the argument can go that config is also a part of the source so B answers. To make life easy let's call this template/config file as the proposal file, so the best way to share will be cloud store."
      },
      {
        "date": "2021-01-01T08:27:00.000Z",
        "voteCount": 2,
        "content": "Look at my post above, cloud repo is for code, not templates"
      },
      {
        "date": "2020-08-13T06:42:00.000Z",
        "voteCount": 31,
        "content": "B is correct. https://cloud.google.com/source-repositories/docs/features"
      },
      {
        "date": "2020-08-23T15:24:00.000Z",
        "voteCount": 3,
        "content": "Using Cloud Storage Repos, you can add comments and describe your changes to the team.Hence this might be a better option."
      },
      {
        "date": "2021-01-01T08:27:00.000Z",
        "voteCount": 2,
        "content": "I don't see how you can do this when I tried creating:\nAdd code to your repository\ninfo\nYour repository is currently empty. Add some code using a selected method and then refresh your browser. Contents added to this repository can take some time to show up in search results. Learn more.\nSelect an option to push code to your repository:\nPush code from a local Git repository\nClone your repository to a local Git repository"
      },
      {
        "date": "2024-07-15T14:39:00.000Z",
        "voteCount": 2,
        "content": "Effective June 17, 2024, Cloud Source Repositories isn't available to new customers. If your organization hasn't previously used Cloud Source Repositories, you can't enable the API or use Cloud Source Repositories. New projects not connected to an organization can't enable the Cloud Source Repositories API. Organizations that have used Cloud Source Repositories prior to June 17, 2024 are not affected by this change.\n\nI think that the question does not exist already, or A is right answer"
      },
      {
        "date": "2024-01-01T14:22:00.000Z",
        "voteCount": 1,
        "content": "Use of Deployment Manager Templates:\nGoogle Cloud Deployment Manager is a tool that allows you to automate the creation and management of Google Cloud resources. It uses templates written in YAML, Python, or Jinja2 to describe your resources and their configurations.\nBy using Deployment Manager templates, you can provide a clear, codified, and repeatable description of the proposed changes to your infrastructure.\n\nVersion Control and Collaboration:\nCloud Source Repositories provide managed and scalable Git repositories hosted on Google Cloud. Storing your Deployment Manager templates in a source repository enables version control, which is a best practice in software and infrastructure development.\nThis approach facilitates collaboration among team members, allowing for review, commenting, and history tracking of changes to the templates."
      },
      {
        "date": "2024-01-01T14:23:00.000Z",
        "voteCount": 1,
        "content": "A. Store in Cloud Storage Bucket: While storing templates in a Cloud Storage bucket makes them accessible, it does not provide the benefits of version control and collaborative features offered by source control systems."
      },
      {
        "date": "2023-10-12T03:09:00.000Z",
        "voteCount": 2,
        "content": "B is the correct answer"
      },
      {
        "date": "2023-09-04T06:52:00.000Z",
        "voteCount": 2,
        "content": "B is the correct answer"
      },
      {
        "date": "2023-08-08T05:43:00.000Z",
        "voteCount": 1,
        "content": "Option B (storing in Cloud Source Repositories) might be suitable for storing application code, but it's not the best practice for storing infrastructure configuration templates."
      },
      {
        "date": "2023-04-30T13:42:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is B"
      },
      {
        "date": "2022-12-29T08:54:00.000Z",
        "voteCount": 1,
        "content": "B is the best answer."
      },
      {
        "date": "2022-12-12T18:08:00.000Z",
        "voteCount": 1,
        "content": "As 2022 best practice, B"
      },
      {
        "date": "2022-11-14T02:41:00.000Z",
        "voteCount": 1,
        "content": "Correction. 2 years later.\nCorrect Answer is (B):"
      },
      {
        "date": "2022-10-20T03:50:00.000Z",
        "voteCount": 1,
        "content": "Thought it was A for a sec, the realized Cloud Source is similar to Github/CodeCommit... So,  B is a better choice"
      },
      {
        "date": "2022-10-12T10:10:00.000Z",
        "voteCount": 1,
        "content": "B. Use Deployment Manager templates to describe the proposed changes and store them in Cloud Source Repositories"
      },
      {
        "date": "2022-08-12T11:56:00.000Z",
        "voteCount": 1,
        "content": "A is the right choice, Use Deployment Manager templates to describe the proposed changes and store them in a Cloud Storage bucket"
      },
      {
        "date": "2022-07-10T03:36:00.000Z",
        "voteCount": 1,
        "content": "Don't get confused - it is B. IaC is a matter of version control system like GIT."
      },
      {
        "date": "2022-06-23T15:53:00.000Z",
        "voteCount": 3,
        "content": "B is right .. Showing Deployment Manager templates to your team will allow you to define the changes you want to implement in your cloud infrastructure. You can use Cloud Source Repositories to store Deployment Manager templates and collaborate with your team. Cloud Source Repositories are fully-featured, scalable, and private Git repositories you can use to store, manage and track changes to your code.\nHence, the correct answer is: Create Deployment Manager templates to define the proposed changes and save them into Cloud Source Repositories."
      },
      {
        "date": "2022-05-30T04:51:00.000Z",
        "voteCount": 1,
        "content": "Its B."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 144,
    "url": "https://www.examtopics.com/discussions/google/view/28464-exam-associate-cloud-engineer-topic-1-question-144/",
    "body": "You have a Compute Engine instance hosting an application used between 9 AM and 6 PM on weekdays. You want to back up this instance daily for disaster recovery purposes. You want to keep the backups for 30 days. You want the Google-recommended solution with the least management overhead and the least number of services. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Update your instances' metadata to add the following value: snapshot\u05d2\u20ac\"schedule: 0 1 * * * 2. Update your instances' metadata to add the following value: snapshot\u05d2\u20ac\"retention: 30",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. In the Cloud Console, go to the Compute Engine Disks page and select your instance's disk. 2. In the Snapshot Schedule section, select Create Schedule and configure the following parameters: - Schedule frequency: Daily - Start time: 1:00 AM \u05d2\u20ac\" 2:00 AM - Autodelete snapshots after: 30 days\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a Cloud Function that creates a snapshot of your instance's disk. 2. Create a Cloud Function that deletes snapshots that are older than 30 days. 3. Use Cloud Scheduler to trigger both Cloud Functions daily at 1:00 AM.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a bash script in the instance that copies the content of the disk to Cloud Storage. 2. Create a bash script in the instance that deletes data older than 30 days in the backup Cloud Storage bucket. 3. Configure the instance's crontab to execute these scripts daily at 1:00 AM."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-08-21T22:08:00.000Z",
        "voteCount": 43,
        "content": "Correct Answer is (B):\n\nCreating scheduled snapshots for persistent disk\nThis document describes how to create a snapshot schedule to regularly and automatically back up your zonal and regional persistent disks. Use snapshot schedules as a best practice to back up your Compute Engine workloads. After creating a snapshot schedule, you can apply it to one or more persistent disks.\n\nhttps://cloud.google.com/compute/docs/disks/scheduled-snapshots"
      },
      {
        "date": "2020-09-09T18:02:00.000Z",
        "voteCount": 10,
        "content": "Definitely B. \n\nWith something like this, you should not have to write any custom scripts, custom functions, or cron jobs. This is google's way of saying 'hey, we've already built that stuff in to our snapshot schedules feature."
      },
      {
        "date": "2021-12-08T00:36:00.000Z",
        "voteCount": 5,
        "content": "it is b. we cannot define snapshot config in instance metadata.\nVM instance metadata is used only for:\nstartup and shutdown scripts\nhost maintanence\nguest attributes"
      },
      {
        "date": "2024-05-25T08:21:00.000Z",
        "voteCount": 1,
        "content": "Now in 2024, use backup&amp;DR for this. But in 2020 it was B"
      },
      {
        "date": "2024-05-14T04:35:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/compute/docs/instances/schedule-instance-start-stop"
      },
      {
        "date": "2024-05-25T08:22:00.000Z",
        "voteCount": 1,
        "content": "No, it is not about schedule start-stop, is about snapshot schedule of the disks...\nbut yes, it is B."
      },
      {
        "date": "2024-01-22T04:57:00.000Z",
        "voteCount": 1,
        "content": "I think when we think about best practice, we should always think about being practical. The most practical method is usually the best practice with a few exceptions. In this scenario, Answers C and D require a lot of effort. Answer A seems not quite relevant. Answer B is the only correct option."
      },
      {
        "date": "2023-09-04T06:56:00.000Z",
        "voteCount": 2,
        "content": "B is the correct answer"
      },
      {
        "date": "2023-02-04T06:11:00.000Z",
        "voteCount": 1,
        "content": "B is ans"
      },
      {
        "date": "2022-12-28T22:24:00.000Z",
        "voteCount": 3,
        "content": "got this question 2 days ago. B is right."
      },
      {
        "date": "2022-06-23T15:54:00.000Z",
        "voteCount": 1,
        "content": "B is more appropriate"
      },
      {
        "date": "2022-05-21T15:21:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer B"
      },
      {
        "date": "2021-07-28T12:22:00.000Z",
        "voteCount": 3,
        "content": "why not C?"
      },
      {
        "date": "2021-11-29T01:10:00.000Z",
        "voteCount": 3,
        "content": "The question calls for \"Google-recommended solution with the least management overhead and the least number of services\""
      },
      {
        "date": "2021-05-20T07:28:00.000Z",
        "voteCount": 2,
        "content": "B is correct"
      },
      {
        "date": "2021-03-17T05:47:00.000Z",
        "voteCount": 3,
        "content": "B is more realistic approach"
      },
      {
        "date": "2021-03-16T08:46:00.000Z",
        "voteCount": 2,
        "content": "B is the best option so far. However just wonder this: Schedule frequency: Daily \" Start time: 1:00 AM \" 2:00 AM\" Autodelete snapshots: after 30 days; For Saturday and Sunday it will be a waste of resource to create snapshots since the instance is running during weekdays."
      },
      {
        "date": "2021-02-27T17:25:00.000Z",
        "voteCount": 2,
        "content": "B. 1. In the Cloud Console, go to the Compute Engine Disks page and select your instance's disk. 2. In the Snapshot Schedule section, select Create Schedule and configure the following parameters: \"\" Schedule frequency: Daily \"\" Start time: 1:00 AM \"\" 2:00 AM \"\" Autodelete snapshots after 30 days"
      },
      {
        "date": "2021-01-27T23:33:00.000Z",
        "voteCount": 1,
        "content": "It's B"
      },
      {
        "date": "2020-11-24T05:12:00.000Z",
        "voteCount": 1,
        "content": "B for sure, any doubt?"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 145,
    "url": "https://www.examtopics.com/discussions/google/view/46376-exam-associate-cloud-engineer-topic-1-question-145/",
    "body": "Your existing application running in Google Kubernetes Engine (GKE) consists of multiple pods running on four GKE n1`\"standard`\"2 nodes. You need to deploy additional pods requiring n2`\"highmem`\"16 nodes without any downtime. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse gcloud container clusters upgrade. Deploy the new services.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new Node Pool and specify machine type n2\u05d2\u20ac\"highmem\u05d2\u20ac\"16. Deploy the new pods.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new cluster with n2\u05d2\u20ac\"highmem\u05d2\u20ac\"16 nodes. Redeploy the pods and delete the old cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new cluster with both n1\u05d2\u20ac\"standard\u05d2\u20ac\"2 and n2\u05d2\u20ac\"highmem\u05d2\u20ac\"16 nodes. Redeploy the pods and delete the old cluster."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 6,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-03-16T08:09:00.000Z",
        "voteCount": 32,
        "content": "B is correct answer, read below form google docs;\n\nThis tutorial demonstrates how to migrate workloads running on a Google Kubernetes Engine (GKE) cluster to a new set of nodes within the same cluster without incurring downtime for your application. Such a migration can be useful if you want to migrate your workloads to nodes with a different machine type.\n\nBackground\nA node pool is a subset of machines that all have the same configuration, including machine type (CPU and memory) authorization scopes. Node pools represent a subset of nodes within a cluster; a container cluster can contain one or more node pools.\n\nWhen you need to change the machine profile of your Compute Engine cluster, you can create a new node pool and then migrate your workloads over to the new node pool.\n\nTo migrate your workloads without incurring downtime, you need to:\n\nMark the existing node pool as unschedulable.\nDrain the workloads running on the existing node pool.\nDelete the existing node pool.\n\nhttps://cloud.google.com/kubernetes-engine/docs/tutorials/migrating-node-pool#creating_a_node_pool_with_large_machine_type"
      },
      {
        "date": "2023-09-04T06:58:00.000Z",
        "voteCount": 1,
        "content": "B is the right answer , if you need the new, and if you want to old one also then its D"
      },
      {
        "date": "2023-03-12T14:28:00.000Z",
        "voteCount": 1,
        "content": "B is correct, creating another cluster just doesnt make any sense, node pools are intended for this situations"
      },
      {
        "date": "2023-03-07T19:15:00.000Z",
        "voteCount": 1,
        "content": "Answer is obviously B (read @GCP_Student1 and @Bobbybash replies)"
      },
      {
        "date": "2023-02-14T01:28:00.000Z",
        "voteCount": 2,
        "content": "B. Create a new Node Pool and specify machine type n2\"highmem\"16. Deploy the new pods.\n\nCreating a new Node Pool with the required machine type is the correct approach to deploy additional pods without any downtime. This approach allows you to scale the cluster horizontally by adding more nodes to the existing cluster. By creating a new Node Pool, you can add n2\"highmem\"16 nodes to the existing cluster, and deploy new pods on these nodes without affecting the existing services running on the n1\"standard\"2 nodes. This way, you can ensure high availability and zero downtime during the deployment. Option A (gcloud container clusters upgrade) upgrades the entire cluster, and Option C and D (creating a new cluster) involve deleting the existing cluster, which may cause downtime."
      },
      {
        "date": "2023-02-03T20:29:00.000Z",
        "voteCount": 2,
        "content": "The keyword is \"additional\". Answer B is good if you want to replace with the new VMs. In this case you want the existing ones as well as the new ones. Therefore D."
      },
      {
        "date": "2023-02-08T07:53:00.000Z",
        "voteCount": 3,
        "content": "The keyword is \"additional\", in option D you are deleting the old cluster. SO the answer is B"
      },
      {
        "date": "2022-06-23T15:56:00.000Z",
        "voteCount": 1,
        "content": "B makes perfect sense."
      },
      {
        "date": "2022-06-09T04:18:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2021-05-20T07:29:00.000Z",
        "voteCount": 4,
        "content": "B is correct"
      },
      {
        "date": "2021-04-05T03:12:00.000Z",
        "voteCount": 4,
        "content": "ANS : B\n\n1. The title did not say to delete four GKE n1."
      },
      {
        "date": "2021-03-22T23:52:00.000Z",
        "voteCount": 3,
        "content": "B,You need to create new node pool for cluster"
      },
      {
        "date": "2021-03-14T00:51:00.000Z",
        "voteCount": 2,
        "content": "I guess it's B. I couldn't find resize parameter under cluster upgrade. C and D are incorrect because it's no need to create new cluster."
      },
      {
        "date": "2021-03-13T14:47:00.000Z",
        "voteCount": 1,
        "content": "A. Use gcloud container clusters upgrade. Deploy the new services."
      },
      {
        "date": "2021-03-16T08:08:00.000Z",
        "voteCount": 2,
        "content": "I take it back, the correct answer is \"B\"\nB. Create a new Node Pool and specify machine type n2\u05d2\u20ac\"highmem\u05d2\u20ac\"16. Deploy the new pods."
      },
      {
        "date": "2021-03-11T07:00:00.000Z",
        "voteCount": 3,
        "content": "Answer is B - When you need to change the machine profile of your Compute Engine cluster, you can create a new node pool and then migrate your workloads over to the new node pool."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 146,
    "url": "https://www.examtopics.com/discussions/google/view/28459-exam-associate-cloud-engineer-topic-1-question-146/",
    "body": "You have an application that uses Cloud Spanner as a database backend to keep current state information about users. Cloud Bigtable logs all events triggered by users. You export Cloud Spanner data to Cloud Storage during daily backups. One of your analysts asks you to join data from Cloud Spanner and Cloud<br>Bigtable for specific users. You want to complete this ad hoc request as efficiently as possible. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a dataflow job that copies data from Cloud Bigtable and Cloud Storage for specific users.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a dataflow job that copies data from Cloud Bigtable and Cloud Spanner for specific users.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Cloud Dataproc cluster that runs a Spark job to extract data from Cloud Bigtable and Cloud Storage for specific users.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate two separate BigQuery external tables on Cloud Storage and Cloud Bigtable. Use the BigQuery console to join these tables through user fields, and apply appropriate filters.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 7,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-08-19T19:27:00.000Z",
        "voteCount": 38,
        "content": "I think it should be D. https://cloud.google.com/bigquery/external-data-sources"
      },
      {
        "date": "2020-08-20T06:31:00.000Z",
        "voteCount": 3,
        "content": "The question says: \" Join data from Cloud Spanner and Cloud Bigtable for specific users\" You can see the Google documentation in the link https://cloud.google.com/spanner/docs/export"
      },
      {
        "date": "2020-11-10T13:07:00.000Z",
        "voteCount": 4,
        "content": "Oh my god, SSPC read you your links!\nThe process uses Dataflow and exports data to a folder in a Cloud Storage bucket. The resulting folder contains a set of Avro files and JSON manifest files. And what next? I will tell - next you read below: Compute Engine: Before running your export job, you must set up initial quotas for Recommended starting values are:\n\n    CPUs: 200\n    In-use IP addresses: 200\n    Standard persistent disk: 50 TB\nStill think its A?"
      },
      {
        "date": "2020-08-21T22:35:00.000Z",
        "voteCount": 28,
        "content": "Correct Answer is (D):\n\nIntroduction to external data sources\nThis page provides an overview of querying data stored outside of BigQuery.\n\nhttps://cloud.google.com/bigquery/external-data-sources"
      },
      {
        "date": "2020-08-21T22:36:00.000Z",
        "voteCount": 5,
        "content": "BigQuery offers support for querying data directly from:\n\nBigtable\nCloud Storage\nGoogle Drive\nCloud SQL (beta)"
      },
      {
        "date": "2021-06-13T12:54:00.000Z",
        "voteCount": 1,
        "content": "but here we're not talking about  joining Cloud Storage and Cloud Bigtable external tables.\nthe join happens between a distributed relational database (Spanner) and key-value NoSQL Database  (BigTable) . how's converting Spanner to cloud storage an implicit and trivial step."
      },
      {
        "date": "2021-06-13T13:13:00.000Z",
        "voteCount": 3,
        "content": "\"The Cloud Spanner to Cloud Storage Text template is a batch pipeline that reads in data from a Cloud Spanner table, optionally transforms the data via a JavaScript User Defined Function (UDF) that you provide, and writes it to Cloud Storage as CSV text files.\"\nhttps://cloud.google.com/dataflow/docs/guides/templates/provided-batch#cloudspannertogcstext\n\n\"The Dataflow connector for Cloud Spanner lets you read data from and write data to Cloud Spanner in a Dataflow pipeline\"\nhttps://cloud.google.com/spanner/docs/dataflow-connector"
      },
      {
        "date": "2022-03-07T07:00:00.000Z",
        "voteCount": 6,
        "content": "update:\nBigQuery supports the following external data sources:\n    Bigtable\n    Cloud Spanner\n    Cloud SQL\n    Cloud Storage\n    Drive"
      },
      {
        "date": "2020-09-12T10:56:00.000Z",
        "voteCount": 2,
        "content": "As per your comment D is the answer. \nI also agree.\nBut can BigQurey read backed up data? , as we have backup data on Cloud storage, did not get any evidence in the link you shared."
      },
      {
        "date": "2024-02-15T07:40:00.000Z",
        "voteCount": 1,
        "content": "The Q says that an analyst wants to analyze data about a user from 2 different sources, which Dataflow will give you, plus as Google states, it allows you more time analyzing stuff and less time fiddling with setting things up, which option D is talking about, which is wrong per the asked Q."
      },
      {
        "date": "2023-11-23T09:08:00.000Z",
        "voteCount": 2,
        "content": "D is apt and possible."
      },
      {
        "date": "2023-06-06T22:47:00.000Z",
        "voteCount": 3,
        "content": "BigQuery is powerful. If you have data in one of the popular sources like Cloud Storage or Bigtable, it is much more efficient - both for cost and computation - to create an external table on those data sources, than to copy their data around.\n\nBesides that, also keep in mind that table clones and snapshots are much more efficient than full table copy etc."
      },
      {
        "date": "2023-04-30T14:00:00.000Z",
        "voteCount": 1,
        "content": "I go for option B. As in option D, the data is backed up data and not the most recent data."
      },
      {
        "date": "2023-02-14T01:40:00.000Z",
        "voteCount": 2,
        "content": "B. Create a dataflow job that copies data from Cloud Bigtable and Cloud Spanner for specific users.\n\nTo join data from Cloud Spanner and Cloud Bigtable for specific users, creating a dataflow job that copies data from both sources is the most efficient option. This approach allows you to process the data in parallel, and you can take advantage of Cloud Dataflow's autoscaling feature to handle large volumes of data. You can use Cloud Dataflow to read data from Cloud Bigtable and Cloud Spanner, join the data based on the user fields, and write the output to a new location or send it to the analyst. Option A (copying data from Cloud Storage) does not provide data from Cloud Spanner, and option C (running a Spark job on a Dataproc cluster) involves higher overhead costs. Option D (using BigQuery external tables) is not efficient for ad hoc requests, as data is exported from Spanner to Cloud Storage during backups, so there may be a delay in data availability."
      },
      {
        "date": "2022-11-02T09:44:00.000Z",
        "voteCount": 1,
        "content": "I thinks is D, but not 100% sure, because D does not have any infomation about the specific user like others options."
      },
      {
        "date": "2022-10-08T20:43:00.000Z",
        "voteCount": 2,
        "content": "D is the correct answer,\nAn external data source is a data source that you can query directly from BigQuery, even though the data is not stored in BigQuery storage.\n\nBigQuery supports the following external data sources:\n\nAmazon S3\nAzure Storage\nCloud Bigtable\nCloud Spanner\nCloud SQL\nCloud Storage\nDrive"
      },
      {
        "date": "2022-10-06T02:28:00.000Z",
        "voteCount": 1,
        "content": "D makes sense as the BigQuery external tables are made for such use cases. and \"efficient\" keyword makes sense to use this way as resources used are less."
      },
      {
        "date": "2022-08-06T19:21:00.000Z",
        "voteCount": 1,
        "content": "First of all, using Dataflow can perhaps be effective, but NOT efficient, specially because of costs.\n\nSecond:\n\n\u201cTo query Cloud Bigtable data using a permanent external table, you: Create a table definition file (for the API or bq command-line tool); Create a table in BigQuery linked to the external data source; Query the data using the permanent table.\u201d\n\nSource: https://cloud.google.com/bigquery/docs/external-data-bigtable#:~:text=To%20query%20Cloud%20Bigtable%20data,data%20using%20the%20permanent%20table\n\nThird:\n\n\u201cTo query a Cloud Storage external data source, provide the Cloud Storage URI path to your data and create a table that references the data source.\u201d\n\nSource:\nhttps://cloud.google.com/bigquery/docs/external-data-cloud-storage\n\nCorrect answer: D."
      },
      {
        "date": "2022-07-27T08:59:00.000Z",
        "voteCount": 1,
        "content": "\"efficiently as possible\" -&gt; use the least amount of resources and achieve the same result... so I think it's D"
      },
      {
        "date": "2022-07-10T05:21:00.000Z",
        "voteCount": 1,
        "content": "Most \"cloud\" solution is D"
      },
      {
        "date": "2022-03-19T06:13:00.000Z",
        "voteCount": 1,
        "content": "option d"
      },
      {
        "date": "2022-03-18T02:00:00.000Z",
        "voteCount": 1,
        "content": "Option is D"
      },
      {
        "date": "2022-02-04T15:36:00.000Z",
        "voteCount": 3,
        "content": "I think it is B. \nThe data in Cloud storage is not up to date as backup window is daily.  SO, there are chances is missing  one day worth of data.\nAs it is mentioned as \"efficiently\" instead of quickly, I would choose \"B\"."
      },
      {
        "date": "2022-03-02T03:10:00.000Z",
        "voteCount": 1,
        "content": "How does this create a \"join\" between the two tables?"
      },
      {
        "date": "2022-03-16T03:34:00.000Z",
        "voteCount": 1,
        "content": "why do you think one cannot join 2 subsets of data in dataflow Its meant for processing sets of data."
      },
      {
        "date": "2022-02-04T00:58:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 147,
    "url": "https://www.examtopics.com/discussions/google/view/46861-exam-associate-cloud-engineer-topic-1-question-147/",
    "body": "You are hosting an application from Compute Engine virtual machines (VMs) in us`\"central1`\"a. You want to adjust your design to support the failure of a single<br>Compute Engine zone, eliminate downtime, and minimize cost. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\u05d2\u20ac\" Create Compute Engine resources in us\u05d2\u20ac\"central1\u05d2\u20ac\"b. \u05d2\u20ac\" Balance the load across both us\u05d2\u20ac\"central1\u05d2\u20ac\"a and us\u05d2\u20ac\"central1\u05d2\u20ac\"b.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\u05d2\u20ac\" Create a Managed Instance Group and specify us\u05d2\u20ac\"central1\u05d2\u20ac\"a as the zone. \u05d2\u20ac\" Configure the Health Check with a short Health Interval.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\u05d2\u20ac\" Create an HTTP(S) Load Balancer. \u05d2\u20ac\" Create one or more global forwarding rules to direct traffic to your VMs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\u05d2\u20ac\" Perform regular backups of your application. \u05d2\u20ac\" Create a Cloud Monitoring Alert and be notified if your application becomes unavailable. \u05d2\u20ac\" Restore from backups when notified."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-03-13T17:52:00.000Z",
        "voteCount": 17,
        "content": "A. Create Compute Engine resources in us \"central1 \"b. \" Balance the load across both us \"central1\"a and us \"central1\"b."
      },
      {
        "date": "2022-03-01T00:06:00.000Z",
        "voteCount": 13,
        "content": "This seems straightforward. \"A\" is the only answer that involves putting instances in more than one zone!\n\nA. Yes, creating instances in another zone and balancing the loads will fix this problem\nB. Wrong. This keeps all the instances in one zone, but the question says we want to protect against zone failures.\nC. Wrong. This keeps all the instances in one zone, but the question says we want to protect against zone failures.\nD. Wrong. This keeps all the instances in one zone, but the question says we want to protect against zone failures."
      },
      {
        "date": "2024-04-19T06:28:00.000Z",
        "voteCount": 1,
        "content": "the Answer is B. the 2 Main ask are 1. Single Zone and Minimizes Cost \n\noption B is a cost-effective solution that can provide high availability within a single zone. By creating a Managed Instance Group in us-central1-a and configuring a Health Check with a short Health Interval, you can ensure that if one instance becomes unavailable, the Managed Instance Group will automatically create a new instance to replace it. This can help minimize downtime and ensure that your application remains available within the us-central1-a zone."
      },
      {
        "date": "2024-05-25T08:12:00.000Z",
        "voteCount": 1,
        "content": "No, it is not B."
      },
      {
        "date": "2023-09-04T07:08:00.000Z",
        "voteCount": 3,
        "content": "A seems more right as it help with the Zone failure, all other create the same in same zone"
      },
      {
        "date": "2023-04-03T01:05:00.000Z",
        "voteCount": 1,
        "content": "The Answer is B"
      },
      {
        "date": "2022-08-18T22:33:00.000Z",
        "voteCount": 1,
        "content": "A is best option"
      },
      {
        "date": "2022-08-05T15:03:00.000Z",
        "voteCount": 2,
        "content": "A is the best option"
      },
      {
        "date": "2022-06-23T15:58:00.000Z",
        "voteCount": 1,
        "content": "A is fine."
      },
      {
        "date": "2022-02-14T03:20:00.000Z",
        "voteCount": 3,
        "content": "Option A.\nCreate VMs across more than one region and zone so that you have alternative VMs to point to if a zone or region containing one of your VMs is disrupted. If you host all your VMs in the same zone or region, you won't be able to access any of those VMs if that zone or region becomes unreachable.\nhttps://cloud.google.com/compute/docs/tutorials/robustsystems#distribute"
      },
      {
        "date": "2021-12-08T01:07:00.000Z",
        "voteCount": 1,
        "content": "A is correct because we have to eliminate single zone failure problem"
      },
      {
        "date": "2021-11-29T01:46:00.000Z",
        "voteCount": 4,
        "content": "Why not \"B\" selecting \"Regional (multi zone)\" ?\n\"Regional (multiple zone) coverage. Regional MIGs let you spread app load across multiple zones. This replication protects against zonal failures. If that happens, your app can continue serving traffic from instances running in the remaining available zones in the same region.\"\nhttps://cloud.google.com/compute/docs/instance-groups/"
      },
      {
        "date": "2021-11-27T06:48:00.000Z",
        "voteCount": 1,
        "content": "it should be B , but because it specify the one Zone we can't pick this answer , the closest other option is A"
      },
      {
        "date": "2021-05-20T07:29:00.000Z",
        "voteCount": 6,
        "content": "A is best option"
      },
      {
        "date": "2021-04-10T05:43:00.000Z",
        "voteCount": 2,
        "content": "Can someone explain how A?"
      },
      {
        "date": "2021-11-25T04:59:00.000Z",
        "voteCount": 1,
        "content": "Other options do not prepare you for zonal outages"
      },
      {
        "date": "2021-04-18T13:56:00.000Z",
        "voteCount": 10,
        "content": "in order to remediate to the problem of single point of failure, we have to replicate VMs within multiple zones. Only A choice consider this concern"
      },
      {
        "date": "2021-04-08T08:27:00.000Z",
        "voteCount": 1,
        "content": "A?  Really? how?"
      },
      {
        "date": "2021-04-03T11:53:00.000Z",
        "voteCount": 4,
        "content": "A is correct."
      },
      {
        "date": "2021-03-13T03:11:00.000Z",
        "voteCount": 2,
        "content": "A - \u05d2\u20ac\" Create Compute Engine resources in us\u05d2\u20ac\"central1\u05d2\u20ac\"b. \u05d2\u20ac\" Balance the load across both us\u05d2\u20ac\"central1\u05d2\u20ac\"a and us\u05d2\u20ac\"central1\u05d2\u20ac\"b."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 148,
    "url": "https://www.examtopics.com/discussions/google/view/28240-exam-associate-cloud-engineer-topic-1-question-148/",
    "body": "A colleague handed over a Google Cloud Platform project for you to maintain. As part of a security checkup, you want to review who has been granted the Project<br>Owner role. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the console, validate which SSH keys have been stored as project-wide keys.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNavigate to Identity-Aware Proxy and check the permissions for these resources.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Audit Logs on the IAM &amp; admin page for all resources, and validate the results.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the command gcloud projects get\u05d2\u20ac\"iam\u05d2\u20ac\"policy to view the current role assignments.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-08-21T22:48:00.000Z",
        "voteCount": 45,
        "content": "Correct Answer is (D):\n\nA simple approach would be to use the command flags available when listing all the IAM policy for a given project. For instance, the following command:\n\n`gcloud projects get-iam-policy $PROJECT_ID --flatten=\"bindings[].members\" --format=\"table(bindings.members)\" --filter=\"bindings.role:roles/owner\"`\n\noutputs all the users and service accounts associated with the role \u2018roles/owner\u2019 in the project in question. \n\nhttps://groups.google.com/g/google-cloud-dev/c/Z6sZs7TvygQ?pli=1"
      },
      {
        "date": "2020-08-12T02:24:00.000Z",
        "voteCount": 13,
        "content": "D: is the answer"
      },
      {
        "date": "2020-08-13T05:43:00.000Z",
        "voteCount": 3,
        "content": "D is the correct."
      },
      {
        "date": "2020-12-22T23:19:00.000Z",
        "voteCount": 4,
        "content": "D IS THE ANSWER"
      },
      {
        "date": "2024-02-08T09:12:00.000Z",
        "voteCount": 1,
        "content": "The Answer is D. Per documentation: https://cloud.google.com/sdk/gcloud/reference/projects/get-iam-policy.\n\nAlso, just tried in my own account and it brought a list if all users and their roles."
      },
      {
        "date": "2023-09-04T07:09:00.000Z",
        "voteCount": 1,
        "content": "D seems, more correct"
      },
      {
        "date": "2022-07-10T05:25:00.000Z",
        "voteCount": 1,
        "content": "gcloud iam get-iam-policy"
      },
      {
        "date": "2022-06-23T16:00:00.000Z",
        "voteCount": 1,
        "content": "D is right"
      },
      {
        "date": "2022-05-24T03:17:00.000Z",
        "voteCount": 2,
        "content": "Answer is D"
      },
      {
        "date": "2022-03-23T09:16:00.000Z",
        "voteCount": 1,
        "content": "gcloud projects get-iam-policy $PROJECT_ID"
      },
      {
        "date": "2022-03-01T00:10:00.000Z",
        "voteCount": 11,
        "content": "I chose D by a process of elimination.  Here's my take: \n\nA. There's more than one way to access an instance than just the SSH keys, and SSH keys have nothing to do with Project Owner role.\nB. Barking up the wrong tree here, Identity-Aware Proxy is more for remotely accessing resources, rather than Project Owner IAM roles. \nC. This will only work if everyone who is a Project Owner accesses the system so you can see them in the logs.  What if a Project Owner doesn't access the Project for a while? How long will you wait?  Nope. \nD. By elimination, this is the best result."
      },
      {
        "date": "2022-03-16T03:54:00.000Z",
        "voteCount": 1,
        "content": "NICE EXPLANATION; WAY TO G0 D"
      },
      {
        "date": "2022-02-02T07:42:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2021-11-22T15:27:00.000Z",
        "voteCount": 2,
        "content": "how can the admin be so inconsistent throughout with the answers..not good ..its so confusing"
      },
      {
        "date": "2021-12-07T11:38:00.000Z",
        "voteCount": 1,
        "content": "Confusion!! that's the main goal here so that we all go to the docs and Study hard xD"
      },
      {
        "date": "2021-09-29T12:45:00.000Z",
        "voteCount": 1,
        "content": "D is the correct answer"
      },
      {
        "date": "2021-05-20T07:29:00.000Z",
        "voteCount": 2,
        "content": "D is correct"
      },
      {
        "date": "2021-02-28T10:56:00.000Z",
        "voteCount": 3,
        "content": "D. Use the command gcloud projects get\"\"iam\"\"policy to view the current role assignments."
      },
      {
        "date": "2020-12-22T00:06:00.000Z",
        "voteCount": 3,
        "content": "D 200%"
      },
      {
        "date": "2020-11-24T05:23:00.000Z",
        "voteCount": 1,
        "content": "anyone will be confused - solution says one answer \n same time, all you guys have different choices here. what to take from this ?"
      },
      {
        "date": "2020-11-22T05:51:00.000Z",
        "voteCount": 1,
        "content": "D. Use the command gcloud projects get\"\"iam\"\"policy to view the current role assignments."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 149,
    "url": "https://www.examtopics.com/discussions/google/view/28242-exam-associate-cloud-engineer-topic-1-question-149/",
    "body": "You are running multiple VPC-native Google Kubernetes Engine clusters in the same subnet. The IPs available for the nodes are exhausted, and you want to ensure that the clusters can grow in nodes when needed. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new subnet in the same region as the subnet being used.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd an alias IP range to the subnet used by the GKE clusters.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new VPC, and set up VPC peering with the existing VPC.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExpand the CIDR range of the relevant subnet for the cluster.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-08-21T22:52:00.000Z",
        "voteCount": 30,
        "content": "Correct Answer is (D):\n\ngcloud compute networks subnets expand-ip-range\nNAME\ngcloud compute networks subnets expand-ip-range - expand the IP range of a Compute Engine subnetwork\n\nhttps://cloud.google.com/sdk/gcloud/reference/compute/networks/subnets/expand-ip-range"
      },
      {
        "date": "2021-01-02T09:17:00.000Z",
        "voteCount": 8,
        "content": "Ok D it is, here's the GKE specific documentation\nhttps://cloud.google.com/kubernetes-engine/docs/concepts/alias-ips\nEvery subnet must have a primary IP address range. You can expand the primary IP address range at any time, even when Google Cloud resources use the subnet; however, you cannot shrink or change a subnet's primary IP address scheme after the subnet has been created. The first two and last two IP addresses of a primary IP address range are reserved by Google Cloud."
      },
      {
        "date": "2020-08-12T02:25:00.000Z",
        "voteCount": 12,
        "content": "D: is the answer"
      },
      {
        "date": "2020-08-13T03:10:00.000Z",
        "voteCount": 2,
        "content": "I agree with you. https://cloud.google.com/vpc/docs/configure-alias-ip-ranges#gcloud_1"
      },
      {
        "date": "2023-09-04T07:14:00.000Z",
        "voteCount": 2,
        "content": "D is the correct Answer, as you just expand the range"
      },
      {
        "date": "2023-02-14T01:48:00.000Z",
        "voteCount": 2,
        "content": "D. Expand the CIDR range of the relevant subnet for the cluster.\n\nExpanding the CIDR range of the relevant subnet for the cluster would increase the number of available IP addresses and allow the clusters to grow when needed. This can be done by modifying the existing subnet's IP address range in the VPC network settings. Adding a new subnet or VPC peering would not directly address the issue of running out of available IP addresses in the current subnet. Adding an alias IP range to the subnet could provide additional IP addresses, but may not be sufficient for long-term growth."
      },
      {
        "date": "2022-10-12T10:19:00.000Z",
        "voteCount": 1,
        "content": "D. Expand the CIDR range of the relevant subnet for the cluster."
      },
      {
        "date": "2022-09-27T06:53:00.000Z",
        "voteCount": 1,
        "content": "D. Expanding CIDR range is enough."
      },
      {
        "date": "2022-08-01T23:34:00.000Z",
        "voteCount": 1,
        "content": "C is the right answer"
      },
      {
        "date": "2022-08-05T20:13:00.000Z",
        "voteCount": 2,
        "content": "Please provide the reason why you choose C as the right answer. ESP_SAP explains clearly about the reason why he choose D as the right answer even he add Google Documentation link too to prove his answer."
      },
      {
        "date": "2022-07-05T23:31:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is D:\nhttps://cloud.google.com/vpc/docs/create-modify-vpc-networks#expand-subnet\nJust expand your subnet."
      },
      {
        "date": "2022-06-23T16:00:00.000Z",
        "voteCount": 1,
        "content": "D is right"
      },
      {
        "date": "2021-02-28T11:53:00.000Z",
        "voteCount": 6,
        "content": "This might help \n\nNode limiting ranges\nThe maximum number of Pods and Services for a given GKE cluster is limited by the size of the cluster's secondary ranges. The maximum number of nodes in the cluster is limited by the size of the cluster's subnet's primary IP address range and the cluster's Pod address range.\n\nThe Cloud Console shows error messages like the following to indicate that either the subnet's primary IP address range or the cluster's Pod IP address range (the subnet's secondary IP address range for Pods) has been exhausted:\n\n\nInstance [node name] creation failed: IP space of [cluster subnet] is\nexhausted\nNote: Secondary subnets are not visible in Cloud Console. If you can't find the [cluster subnet] reported by the above error message it means that the error is caused by IP exhaustion in a secondary subnet. In this case check the secondary ranges of the primary subnet.\n\nhttps://cloud.google.com/kubernetes-engine/docs/concepts/alias-ips#node_limiters"
      },
      {
        "date": "2021-03-13T18:06:00.000Z",
        "voteCount": 3,
        "content": "By the way the answer is;\n\nD. Expand the CIDR range of the relevant subnet for the cluster."
      },
      {
        "date": "2020-12-27T02:57:00.000Z",
        "voteCount": 3,
        "content": "UHmmm, 1 question. The description of the problem says that the ip's are EXHAUSTED. So, no more IP's available in this subnet.\n\nIt also states that we're having a multi-VPC environment... as allways we should not interpret, just take the questions literally.\n\nIF we do not know the actual size of the deployment it cna be ANY size, adn if IP's are EXHAUSTED, it should BE, BIG as Galactic sized or so....\n\nWith all this I wonder if the right answer it is not C..."
      },
      {
        "date": "2022-01-16T10:12:00.000Z",
        "voteCount": 1,
        "content": "Same doubt! \nVPC peering seems correct to me."
      },
      {
        "date": "2020-11-24T05:26:00.000Z",
        "voteCount": 2,
        "content": "D best option to think here."
      },
      {
        "date": "2021-02-22T03:16:00.000Z",
        "voteCount": 1,
        "content": "totaly agree"
      },
      {
        "date": "2020-11-22T05:52:00.000Z",
        "voteCount": 1,
        "content": "D. Expand the CIDR range of the relevant subnet for the cluster."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 150,
    "url": "https://www.examtopics.com/discussions/google/view/28243-exam-associate-cloud-engineer-topic-1-question-150/",
    "body": "You have a batch workload that runs every night and uses a large number of virtual machines (VMs). It is fault-tolerant and can tolerate some of the VMs being terminated. The current cost of VMs is too high. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun a test using simulated maintenance events. If the test is successful, use preemptible N1 Standard VMs when running future jobs.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun a test using simulated maintenance events. If the test is successful, use N1 Standard VMs when running future jobs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun a test using a managed instance group. If the test is successful, use N1 Standard VMs in the managed instance group when running future jobs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun a test using N1 standard VMs instead of N2. If the test is successful, use N1 Standard VMs when running future jobs."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 14,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-08-21T22:58:00.000Z",
        "voteCount": 42,
        "content": "Correct Answer is (A):\n\nCreating and starting a preemptible VM instance\nThis page explains how to create and use a preemptible virtual machine (VM) instance. A preemptible instance is an instance you can create and run at a much lower price than normal instances. However, Compute Engine might terminate (preempt) these instances if it requires access to those resources for other tasks. Preemptible instances will always terminate after 24 hours. To learn more about preemptible instances, read the preemptible instances documentation.\n\nPreemptible instances are recommended only for fault-tolerant applications that can withstand instance preemptions. Make sure your application can handle preemptions before you decide to create a preemptible instance. To understand the risks and value of preemptible instances, read the preemptible instances documentation.\n\nhttps://cloud.google.com/compute/docs/instances/create-start-preemptible-instance"
      },
      {
        "date": "2020-08-12T02:28:00.000Z",
        "voteCount": 16,
        "content": "A: is the answer"
      },
      {
        "date": "2020-08-18T09:33:00.000Z",
        "voteCount": 3,
        "content": "\"A\" is correct"
      },
      {
        "date": "2020-08-22T07:41:00.000Z",
        "voteCount": 1,
        "content": "What about a mixture of preemptible N1 and normal N1 instances? i can't believe just having preemptible is a good practice"
      },
      {
        "date": "2020-09-08T10:31:00.000Z",
        "voteCount": 8,
        "content": "Good point, in real-world your solution, is the best. For this scenario, the answer is A."
      },
      {
        "date": "2023-02-11T12:43:00.000Z",
        "voteCount": 5,
        "content": "It is specific on Batch workload , runs in less than 24 hrs, is fault tolerant. The best candidate for this is job is a preemptible VM"
      },
      {
        "date": "2022-11-21T20:24:00.000Z",
        "voteCount": 3,
        "content": "Preemptible to save cost and even it is fault tolerant"
      },
      {
        "date": "2022-10-08T21:08:00.000Z",
        "voteCount": 1,
        "content": "A is correct, preemptible VMs reduce cost, and this is recommended to run batch jobs which run less than 24 hours"
      },
      {
        "date": "2022-09-26T11:25:00.000Z",
        "voteCount": 1,
        "content": "I Vote A as it is clearly correct. Whenever something runs in under 24 hours and is fault tolerant we should be looking at preemptible VMs to save costs."
      },
      {
        "date": "2022-08-27T04:47:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2022-06-30T06:40:00.000Z",
        "voteCount": 1,
        "content": "Answer should be A: preemptible VM instances. Because the workload is fault-tolerant and can tolerate some of the VMs being terminated."
      },
      {
        "date": "2022-06-23T16:01:00.000Z",
        "voteCount": 1,
        "content": "A is right .."
      },
      {
        "date": "2022-06-11T13:10:00.000Z",
        "voteCount": 1,
        "content": "I believe it is A"
      },
      {
        "date": "2022-02-17T06:06:00.000Z",
        "voteCount": 1,
        "content": "as per the comments"
      },
      {
        "date": "2021-11-08T17:20:00.000Z",
        "voteCount": 2,
        "content": "I vote A. Preempt VM can costdown more."
      },
      {
        "date": "2021-09-26T17:50:00.000Z",
        "voteCount": 2,
        "content": "A is correct because preemptible VMs can provide up to 80% discount over normal VMs if the workloads are fault-tolerant"
      },
      {
        "date": "2021-06-15T00:59:00.000Z",
        "voteCount": 2,
        "content": "Keyword- Fault tolerant, so answer should be pre emptible VMs, option A"
      },
      {
        "date": "2021-02-28T12:45:00.000Z",
        "voteCount": 3,
        "content": "A. Run a test using simulated maintenance events. If the test is successful, use preemptible N1 Standard VMs when running future jobs"
      },
      {
        "date": "2020-11-22T05:53:00.000Z",
        "voteCount": 2,
        "content": "\u2022\tA. Run a test using simulated maintenance events. If the test is successful, use preemptible N1 Standard VMs when running future jobs."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 151,
    "url": "https://www.examtopics.com/discussions/google/view/28244-exam-associate-cloud-engineer-topic-1-question-151/",
    "body": "You are working with a user to set up an application in a new VPC behind a firewall. The user is concerned about data egress. You want to configure the fewest open egress ports. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up a low-priority (65534) rule that blocks all egress and a high-priority rule (1000) that allows only the appropriate ports.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up a high-priority (1000) rule that pairs both ingress and egress ports.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up a high-priority (1000) rule that blocks all egress and a low-priority (65534) rule that allows only the appropriate ports.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up a high-priority (1000) rule to allow the appropriate ports."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 28,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-08-22T11:45:00.000Z",
        "voteCount": 43,
        "content": "Correct Answer is (A):\n\nImplied rules\nEvery VPC network has two implied firewall rules. These rules exist, but are not shown in the Cloud Console:\n\nImplied allow egress rule. An egress rule whose action is allow, destination is 0.0.0.0/0, and priority is the lowest possible (65535) lets any instance send traffic to any destination, except for traffic blocked by Google Cloud. A higher priority firewall rule may restrict outbound access. Internet access is allowed if no other firewall rules deny outbound traffic and if the instance has an external IP address or uses a Cloud NAT instance. For more information, see Internet access requirements.\n\nImplied deny ingress rule. An ingress rule whose action is deny, source is 0.0.0.0/0, and priority is the lowest possible (65535) protects all instances by blocking incoming connections to them. A higher priority rule might allow incoming access. The default network includes some additional rules that override this one, allowing certain types of incoming connections.\n\nhttps://cloud.google.com/vpc/docs/firewalls#default_firewall_rules"
      },
      {
        "date": "2022-07-03T18:11:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is C"
      },
      {
        "date": "2022-08-05T20:04:00.000Z",
        "voteCount": 2,
        "content": "You should visit the documentation link he attached. He's copy those statements from the Google Docs."
      },
      {
        "date": "2022-07-04T00:39:00.000Z",
        "voteCount": 1,
        "content": "Listen that guy because he is right"
      },
      {
        "date": "2022-08-04T04:30:00.000Z",
        "voteCount": 16,
        "content": "Answer is (A) : \nFirst I was going with C but then I read the question again, let's try to understand both options here, the goal is to deny egress and only allow some ports for some functions to perform. If we go with C, lower the number higher the priority (1000) so the rule with this priority 1000 will overwrite (65534), so If we allow only appropriate ports it will be overwritten with the high-priority (1000) rule and all the egress traffic will be blocked.\nRemember the goal here is to block egress but not all of it since we still want to configure the fewest open ports and this is statefull meaning for open ports traffic will be both ways.\nA fits this condition where it is saying we block all traffic but the required ports are kept open with higher priority which will only allow the required traffic to leave the network."
      },
      {
        "date": "2024-01-01T14:50:00.000Z",
        "voteCount": 2,
        "content": "Default Egress Behavior: In Google Cloud VPCs, the default behavior is to allow all egress traffic. To restrict egress traffic effectively, you need to explicitly set up firewall rules.\n\nBlocking All Egress Traffic: The low-priority rule (priority 65534, near the lowest priority) should be configured to block all egress traffic. This creates a baseline rule that denies all egress traffic by default.\n\nAllowing Specific Ports: The high-priority rule (priority 1000, indicating a higher priority) should be set to allow egress traffic only on the specific ports that are required for the application. Since firewall rules are evaluated in order of priority, this rule will override the default block for these specific ports."
      },
      {
        "date": "2023-11-03T02:02:00.000Z",
        "voteCount": 2,
        "content": "Correct answer is C: By implementing a high-priority rule to block all egress traffic (since it has a lower number than lower-priority rules), and a low-priority rule to selectively allow specific necessary egress ports (with a higher number), you minimize open egress ports to only the required ones while restricting the rest."
      },
      {
        "date": "2023-09-06T04:27:00.000Z",
        "voteCount": 1,
        "content": "The rule is evaluated on higher priority to lower priority and depends first come first serve basis.\nhttps://cloud.google.com/firewall/docs/firewall-policies-overview#rule-evaluation"
      },
      {
        "date": "2023-09-04T23:03:00.000Z",
        "voteCount": 2,
        "content": "A is the correct answer"
      },
      {
        "date": "2022-12-01T11:14:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is A.\n\nAnswer will not be D, because Egress traffic is Allowed by default. You will have to explicitly set the rule blocking outbound traffic."
      },
      {
        "date": "2022-08-05T20:06:00.000Z",
        "voteCount": 1,
        "content": "Read ESP_SAP comment for the explanation. He explains it clearly."
      },
      {
        "date": "2022-08-01T23:37:00.000Z",
        "voteCount": 1,
        "content": "C is the correct answer"
      },
      {
        "date": "2022-07-25T09:41:00.000Z",
        "voteCount": 1,
        "content": "A: is the answer"
      },
      {
        "date": "2022-07-04T00:39:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is A"
      },
      {
        "date": "2022-07-03T18:16:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer is C"
      },
      {
        "date": "2022-07-03T18:20:00.000Z",
        "voteCount": 3,
        "content": "Hint : All rules are stateful.\nVPC firewall rules are stateful. When a connection is allowed through the firewall in either direction, return traffic matching this connection is also allowed. You cannot configure a firewall rule to deny associated response traffic.\n\nAs per question , we want to restrict egress traffic.\nSo focus to restrict egress traffic based on priority of rules.\nAllow incoming traffic for appropriate traffic and block all traffic and allow only which are required.\n\nHence , as per my view C should be correct answer"
      },
      {
        "date": "2022-06-24T00:36:00.000Z",
        "voteCount": 3,
        "content": "A incorrect 65534  that blocks all ingress, not egress (except few default ports)\nD is correct."
      },
      {
        "date": "2022-07-03T18:13:00.000Z",
        "voteCount": 1,
        "content": "But why D is correct ? Why not  C ?\n\nD is more generic , As per question , need to focus on egress traffic"
      },
      {
        "date": "2021-12-09T06:06:00.000Z",
        "voteCount": 3,
        "content": "Correct Answer is A"
      },
      {
        "date": "2021-11-08T17:25:00.000Z",
        "voteCount": 2,
        "content": "I vote A is correct. Block all port in gress and set low-priority."
      },
      {
        "date": "2021-10-13T18:23:00.000Z",
        "voteCount": 2,
        "content": "A is correct"
      },
      {
        "date": "2021-08-22T06:59:00.000Z",
        "voteCount": 2,
        "content": "Correct answer is A. Firewall rules are executed based on the priority."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 152,
    "url": "https://www.examtopics.com/discussions/google/view/28352-exam-associate-cloud-engineer-topic-1-question-152/",
    "body": "Your company runs its Linux workloads on Compute Engine instances. Your company will be working with a new operations partner that does not use Google<br>Accounts. You need to grant access to the instances to your operations partner so they can maintain the installed tooling. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Cloud IAP for the Compute Engine instances, and add the operations partner as a Cloud IAP Tunnel User.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTag all the instances with the same network tag. Create a firewall rule in the VPC to grant TCP access on port 22 for traffic from the operations partner to instances with the network tag.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up Cloud VPN between your Google Cloud VPC and the internal network of the operations partner.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAsk the operations partner to generate SSH key pairs, and add the public keys to the VM instances."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 24,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 7,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-04-11T07:56:00.000Z",
        "voteCount": 34,
        "content": "A - https://cloud.google.com/iap/docs/external-identities"
      },
      {
        "date": "2020-11-24T06:29:00.000Z",
        "voteCount": 23,
        "content": "full of confusions for any reader....\nYou guys all say A, B, C &amp; D but which one is correct  ?"
      },
      {
        "date": "2021-07-15T11:43:00.000Z",
        "voteCount": 7,
        "content": "nothing"
      },
      {
        "date": "2024-10-10T09:08:00.000Z",
        "voteCount": 1,
        "content": "A is incorrect because the operations partner does not have a Google account. Activating and enabling Cloud IAP (Identity-Aware Proxy) for the Compute Engine instances would only allow access to users with Google Accounts. In this case, the third-party service provider does not use Google Accounts, so this option would not enable their access."
      },
      {
        "date": "2024-10-10T00:58:00.000Z",
        "voteCount": 1,
        "content": "IAP controls access to your applications and resources. It leverages user identity and the context of a request to determine if a user should be allowed access. IAP is a building block toward BeyondCorp, an enterprise security model that enables employees to work from untrusted networks without using a VPN.\n\nBy default, IAP uses Google identities and IAM. By leveraging Identity Platform instead, you can authenticate users with a wide range of external identity providers, such as:\n\nEmail/password\nOAuth (Google, Facebook, Twitter, GitHub, Microsoft, etc.)\nSAML\nOIDC\nPhone number\nCustom\nAnonymous\nThis is useful if your application is already using an external authentication system, and migrating your users to Google accounts is impractical.\n\n"
      },
      {
        "date": "2024-09-19T13:41:00.000Z",
        "voteCount": 1,
        "content": "Cloud IAP requires users to authenticate with a Google Account, which the operations partner does not use.\nSo D,\nSSH key authentication is a standard and secure method for granting access to Linux systems. This avoids the need for creating Google Accounts and provides direct access to the instances."
      },
      {
        "date": "2024-08-25T05:49:00.000Z",
        "voteCount": 1,
        "content": "Using SSH keys is an effective and secure way to provide access to your instances without requiring Google Accounts, aligning with the needs of your new operations partner."
      },
      {
        "date": "2024-08-19T23:44:00.000Z",
        "voteCount": 2,
        "content": "D. Ask the operations partner to generate SSH key pairs, and add the public keys to the VM instances.\n\nExplanation:\n\n    Direct SSH access: This method provides direct SSH access to the instances, allowing the operations partner to manage the installed tooling efficiently.\n    No Google account required: The operations partner doesn't need a Google account, as SSH keys are the authentication mechanism.\n    Security: While you'll need to manage SSH keys carefully, this method offers a secure way to grant access to specific users.\n\nWhy not other options:\n\n    A. Enable Cloud IAP: Cloud IAP is primarily for web applications, not SSH access.\n    B. Firewall rule: While this could technically work, it's less secure than SSH keys and more complex to manage.\n    C. Cloud VPN: Setting up a VPN is overkill for this scenario and introduces additional complexity.\n\nBy choosing option D, you provide the operations partner with the necessary access while maintaining security."
      },
      {
        "date": "2024-07-15T12:19:00.000Z",
        "voteCount": 1,
        "content": "If you go with answer A, explain please what credentials will be used to authentication?"
      },
      {
        "date": "2024-05-23T13:03:00.000Z",
        "voteCount": 1,
        "content": "\"new operations partner that does not use Google Accounts.\"\n\nAll answering A, but the question does not say that the new partner is going to use Google Accounts now. So it is D. Is not a good idea to enter with ssh key pairs, but there is not other option if the new partner has to enter vms and does not have Google accounts."
      },
      {
        "date": "2023-11-23T09:35:00.000Z",
        "voteCount": 2,
        "content": "A is clean: https://cloud.google.com/iap/docs/concepts-overview#when_to_use_iap"
      },
      {
        "date": "2023-09-04T23:06:00.000Z",
        "voteCount": 1,
        "content": "A seems more corrrect, as to provide the access"
      },
      {
        "date": "2023-04-30T14:55:00.000Z",
        "voteCount": 1,
        "content": "Answer is A. Although to enable IAP, you do need to create a firewall rule on tcp 22. But if this question wasn't multiple choice then A is correct. \n\n\"IAP is a building block toward BeyondCorp, an enterprise security model that enables employees to work from untrusted networks without using a VPN.\" - So C is not required when A can suffice."
      },
      {
        "date": "2023-04-23T07:52:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/iap/docs/concepts-overview\nIAP is only for google accounts and applies to access to AppEngine, HTTP(s) LB. It explicitly doesn't protect VMs."
      },
      {
        "date": "2024-05-23T13:04:00.000Z",
        "voteCount": 1,
        "content": "ok, if it is C, then the partner is in the same internal network, How can they enter the linux vms? they need ssh access...."
      },
      {
        "date": "2023-04-23T20:14:00.000Z",
        "voteCount": 5,
        "content": "please ignore..Answer should be A..https://cloud.google.com/iap/docs/external-identities..this page shows it works for VMs and non-google accounts."
      },
      {
        "date": "2024-05-23T13:05:00.000Z",
        "voteCount": 1,
        "content": "Ignore you, A is not possible, because the question does not say since now the partner will use google accounts...."
      },
      {
        "date": "2022-12-20T05:50:00.000Z",
        "voteCount": 4,
        "content": "Please watch this video.\nhttps://www.youtube.com/watch?v=jZdXyWQuIW0"
      },
      {
        "date": "2022-12-07T22:19:00.000Z",
        "voteCount": 2,
        "content": "B is the straight forward answer to allow the partner to access via SSH without a Google account. For those suggesting A, carefully read https://cloud.google.com/iap/docs/external-identities and you'll notice that external identity isn't available from IAP out of the box and requires Identity Platform."
      },
      {
        "date": "2022-11-21T20:31:00.000Z",
        "voteCount": 1,
        "content": "Question ask about granting access to new operations partner and that can be done by first option only."
      },
      {
        "date": "2022-10-08T21:25:00.000Z",
        "voteCount": 7,
        "content": "A is the correct answer,\nIAP controls access to your App Engine apps and Compute Engine VMs running on Google Cloud. It leverages user identity and the context of a request to determine if a user should be allowed access. IAP is a building block toward BeyondCorp, an enterprise security model that enables employees to work from untrusted networks without using a VPN.\n\nBy default, IAP uses Google identities and IAM. By leveraging Identity Platform instead, you can authenticate users with a wide range of external identity providers, such as:\n\nEmail/password\nOAuth (Google, Facebook, Twitter, GitHub, Microsoft, etc.)\nSAML\nOIDC\nPhone number\nCustom\nAnonymous\nThis is useful if your application is already using an external authentication system, and migrating your users to Google accounts is impractical."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 153,
    "url": "https://www.examtopics.com/discussions/google/view/28353-exam-associate-cloud-engineer-topic-1-question-153/",
    "body": "You have created a code snippet that should be triggered whenever a new file is uploaded to a Cloud Storage bucket. You want to deploy this code snippet. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse App Engine and configure Cloud Scheduler to trigger the application using Pub/Sub.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Functions and configure the bucket as a trigger resource.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Google Kubernetes Engine and configure a CronJob to trigger the application using Pub/Sub.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Dataflow as a batch job, and configure the bucket as a data source."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-08-22T12:50:00.000Z",
        "voteCount": 42,
        "content": "Correct Answer is (B):\n\nGoogle Cloud Storage Triggers\nCloud Functions can respond to change notifications emerging from Google Cloud Storage. These notifications can be configured to trigger in response to various events inside a bucket\u2014object creation, deletion, archiving and metadata updates.\n\nNote: Cloud Functions can only be triggered by Cloud Storage buckets in the same Google Cloud Platform project.\nEvent types\nCloud Storage events used by Cloud Functions are based on Cloud Pub/Sub Notifications for Google Cloud Storage and can be configured in a similar way.\n\nSupported trigger type values are:\n\ngoogle.storage.object.finalize\n\ngoogle.storage.object.delete\n\ngoogle.storage.object.archive\n\ngoogle.storage.object.metadataUpdate\n\nObject Finalize\nTrigger type value: google.storage.object.finalize\n\nThis event is sent when a new object is created (or an existing object is overwritten, and a new generation of that object is created) in the bucket.\n\nhttps://cloud.google.com/functions/docs/calling/storage#event_types"
      },
      {
        "date": "2020-08-12T18:55:00.000Z",
        "voteCount": 19,
        "content": "The answer is B"
      },
      {
        "date": "2020-08-14T03:24:00.000Z",
        "voteCount": 1,
        "content": "Sure B? Please you could share the link with the Google documentation"
      },
      {
        "date": "2020-09-08T10:36:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/functions/docs/calling/storage"
      },
      {
        "date": "2024-01-01T15:14:00.000Z",
        "voteCount": 1,
        "content": "Google Cloud Functions supports several types of triggers, allowing you to run your functions in response to various events in the Google Cloud environment or via HTTP requests. \n1.\tHTTP Triggers:\n\u2022\tHTTP triggers allow your Cloud Function to be invoked via standard HTTP requests. These are useful for building APIs, webhooks, and other services that are accessible over the internet or within your internal network.\n2.\tCloud Pub/Sub Triggers:\n\u2022\tCloud Functions can be triggered by messages published to Cloud Pub/Sub topics. This is useful for asynchronous event-driven architectures and integrating with systems that publish events to Pub/Sub."
      },
      {
        "date": "2024-01-01T15:15:00.000Z",
        "voteCount": 1,
        "content": "3.\tCloud Storage Triggers:\n\u2022\tFunctions can respond to changes in Google Cloud Storage, such as creating, deleting, or updating objects. This is helpful for processing uploaded files, data backups, and more.\n4.\tFirestore Triggers:\n\u2022\tThese triggers allow functions to execute in response to changes in Google Cloud Firestore data, including document creation, updates, and deletions. They are useful for syncing Firestore data with other data stores, or for handling real-time data updates.\n5.\tFirebase Realtime Database Triggers:\n\u2022\tCloud Functions can be triggered by changes in Firebase Realtime Database. This is similar to Firestore triggers but specific to Firebase's Realtime Database service."
      },
      {
        "date": "2024-01-01T15:15:00.000Z",
        "voteCount": 1,
        "content": "6.\tFirebase Authentication Triggers:\n\u2022\tFunctions can react to Firebase Authentication events, such as user creation, deletion, or attribute updates. These triggers are useful for custom user management workflows and integration with external systems.\n7.\tGoogle Analytics for Firebase Triggers:\n\u2022\tThese triggers enable functions to respond to Analytics events collected by Firebase, useful for custom event processing and integrations.\n8.\tScheduled (Cron) Triggers:\n\u2022\tCloud Scheduler can be used to trigger functions on a time-based schedule (cron). This is ideal for running batch jobs, regular clean-ups, or other scheduled tasks."
      },
      {
        "date": "2023-09-06T04:58:00.000Z",
        "voteCount": 2,
        "content": "Use \"Object finalized\" event of the Cloud Storage bucket as trigger for the Cloud Functions.\nhttps://cloud.google.com/functions/docs/calling/storage"
      },
      {
        "date": "2023-09-04T23:09:00.000Z",
        "voteCount": 1,
        "content": "B seems the correct option, as we can use the cloud functions as per our requirement for the cloud storage bucket.."
      },
      {
        "date": "2022-08-19T00:50:00.000Z",
        "voteCount": 1,
        "content": "The answer is B"
      },
      {
        "date": "2022-06-24T03:40:00.000Z",
        "voteCount": 1,
        "content": "B is correct, it is required on demand when upload happens"
      },
      {
        "date": "2022-01-23T11:16:00.000Z",
        "voteCount": 2,
        "content": "The answer is B"
      },
      {
        "date": "2022-01-08T22:24:00.000Z",
        "voteCount": 2,
        "content": "I vote for B"
      },
      {
        "date": "2021-11-20T03:07:00.000Z",
        "voteCount": 4,
        "content": "Vote For B"
      },
      {
        "date": "2021-09-26T10:34:00.000Z",
        "voteCount": 2,
        "content": "Correct Answer is B - Use Cloud Functions and configure the bucket as a trigger resource."
      },
      {
        "date": "2021-06-18T01:36:00.000Z",
        "voteCount": 1,
        "content": "Question asks:\nYou want to deploy this code snippet. What should you do?\nTo me, none of the answers is relevant to DEPLOYMENT - they all are about how you get the trigger to run ..."
      },
      {
        "date": "2021-03-19T15:45:00.000Z",
        "voteCount": 2,
        "content": "vote for B"
      },
      {
        "date": "2021-03-19T06:11:00.000Z",
        "voteCount": 3,
        "content": "I think is B"
      },
      {
        "date": "2021-03-14T09:47:00.000Z",
        "voteCount": 3,
        "content": "B. Use Cloud Functions and configure the bucket as a trigger resource."
      },
      {
        "date": "2021-02-12T15:49:00.000Z",
        "voteCount": 2,
        "content": "Answer is B, app engine is for applications, in this case it\u2019s just a code snippet which cloud functions is more suitable"
      },
      {
        "date": "2021-02-04T04:59:00.000Z",
        "voteCount": 3,
        "content": "B - Use Cloud Functions and configure the bucket as a trigger resource."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 154,
    "url": "https://www.examtopics.com/discussions/google/view/28130-exam-associate-cloud-engineer-topic-1-question-154/",
    "body": "You have been asked to set up Object Lifecycle Management for objects stored in storage buckets. The objects are written once and accessed frequently for 30 days. After 30 days, the objects are not read again unless there is a special need. The objects should be kept for three years, and you need to minimize cost.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up a policy that uses Nearline storage for 30 days and then moves to Archive storage for three years.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up a policy that uses Standard storage for 30 days and then moves to Archive storage for three years.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up a policy that uses Nearline storage for 30 days, then moves the Coldline for one year, and then moves to Archive storage for two years.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up a policy that uses Standard storage for 30 days, then moves to Coldline for one year, and then moves to Archive storage for two years."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 12,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-08-22T13:07:00.000Z",
        "voteCount": 54,
        "content": "Correct Answer is (B):\n \nThe key to understand the requirement is : \"The objects are written once and accessed frequently for 30 days\" \nStandard Storage\nStandard Storage is best for data that is frequently accessed (\"hot\" data) and/or stored for only brief periods of time.\n\nArchive Storage\nArchive Storage is the lowest-cost, highly durable storage service for data archiving, online backup, and disaster recovery. Unlike the \"coldest\" storage services offered by other Cloud providers, your data is available within milliseconds, not hours or days. Archive Storage is the best choice for data that you plan to access less than once a year.\n\nhttps://cloud.google.com/storage/docs/storage-classes#standard"
      },
      {
        "date": "2020-09-13T04:24:00.000Z",
        "voteCount": 1,
        "content": "What if we chose option D to minimize the cost as asked in the question? What do you think?"
      },
      {
        "date": "2020-10-16T10:08:00.000Z",
        "voteCount": 5,
        "content": "It doesn't minimize the costs. Check the costs of coldline vs archival"
      },
      {
        "date": "2020-08-11T08:34:00.000Z",
        "voteCount": 15,
        "content": "I think the correct one is B. Because Nearline has a 30-day minimum storage duration.\nhttps://cloud.google.com/storage/docs/storage-classes"
      },
      {
        "date": "2020-08-12T11:11:00.000Z",
        "voteCount": 1,
        "content": "The object should be kept for three years, and you need to minimize cost, after 30 days it will be moved to archive, ans A"
      },
      {
        "date": "2020-08-12T11:12:00.000Z",
        "voteCount": 6,
        "content": "Sorry you are right accessed frequently for 30 days, its B"
      },
      {
        "date": "2024-01-21T20:49:00.000Z",
        "voteCount": 1,
        "content": "The objects are written once and accessed frequently for 30 days. Then rarely accessed."
      },
      {
        "date": "2023-11-12T00:02:00.000Z",
        "voteCount": 1,
        "content": "B is the correct answer"
      },
      {
        "date": "2023-09-06T05:01:00.000Z",
        "voteCount": 2,
        "content": "Key terms frequently accessed for 30 days -&gt; Standard storage class.\nNot accessed unless special need for 3 years -&gt; Archive storage class."
      },
      {
        "date": "2023-09-04T23:13:00.000Z",
        "voteCount": 1,
        "content": "B is the correct answer, as the data for first 30 days in accessed frequently so for it we can use the standard , and after it to minimize the cost we can use the archive storage for 3 years"
      },
      {
        "date": "2023-03-10T03:50:00.000Z",
        "voteCount": 1,
        "content": "Answer is B, we cannot select A because data is accedesed frequently and nearline only allows access once per month (you can access more incurring in aditional cost but being not a cost optimized selection)"
      },
      {
        "date": "2023-01-12T19:46:00.000Z",
        "voteCount": 1,
        "content": "Answer is A: there is a retrieval fee for  data access from nearline. Please check https://cloud.google.com/storage/docs/storage-classes. So Standard storage is the cheaper option"
      },
      {
        "date": "2022-10-08T21:33:00.000Z",
        "voteCount": 1,
        "content": "B is the correct Answer,\nFrequently accessed data 'Hot Data' should be stored in Standard Storage for 30 days, \nThen this can be moved to Archive after 30 days for period of three years which is accessed only when a special need arises, to reduce cost."
      },
      {
        "date": "2022-06-28T03:06:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/storage/docs/storage-classes#nearline\n\nNearline storage is ideal for data you plan to read or modify on average once per month or less. For example, if you want to continuously add files to Cloud Storage and plan to access those files once a month for analysis, Nearline storage is a great choice.\n\nNearline storage is also appropriate for data backup, long-tail multimedia content, and data archiving. Note, however, that for data accessed less frequently than once a quarter, Coldline storage or Archive storage are more cost-effective, as they offer lower storage costs."
      },
      {
        "date": "2022-06-28T03:07:00.000Z",
        "voteCount": 1,
        "content": "Nearline storage is a low-cost, highly durable storage service for storing infrequently accessed data. Nearline storage is a better choice than Standard storage in scenarios where slightly lower availability, a 30-day minimum storage duration, and costs for data access are acceptable trade-offs for lowered at-rest storage costs."
      },
      {
        "date": "2022-06-24T03:43:00.000Z",
        "voteCount": 1,
        "content": "B is right"
      },
      {
        "date": "2022-06-25T13:04:00.000Z",
        "voteCount": 2,
        "content": "I am changing it to D.  Set up a policy that uses Standard storage for 30 days, then moves to Coldline for one year, and then moves to Archive storage for two years."
      },
      {
        "date": "2023-04-29T02:10:00.000Z",
        "voteCount": 1,
        "content": "Is B right or D?"
      },
      {
        "date": "2022-03-12T01:56:00.000Z",
        "voteCount": 1,
        "content": "B for sure"
      },
      {
        "date": "2022-01-23T19:58:00.000Z",
        "voteCount": 4,
        "content": "The answer is: B\n\nStandard storage description:\nhttps://cloud.google.com/storage/docs/storage-classes#:~:text=Standard%20Storage%20is%20best%20for%20data%20that%20is%20frequently%20accessed%20(%22hot%22%20data)%20and/or%20stored%20for%20only%20brief%20periods%20of%20time.\n \nNearline storage imp description:\nhttps://cloud.google.com/storage/docs/storage-classes#:~:text=storage%20service%20for-,storing%20infrequently%20accessed%20data.,-Nearline%20Storage%20is\n \nPricing for frequent access:\nhttps://cloud.google.com/storage/pricing#:~:text=Free%20operations-,Standard%20Storage,Free,-Coldline%20Storage"
      },
      {
        "date": "2022-01-08T22:41:00.000Z",
        "voteCount": 4,
        "content": "Standard \u2013 Frequently access and short period\nNearline - Low cost, highly durable for infrequent data access, lower availability\nColdline \u2013Very low cost, highly durable for infrequently accessed, 90 days minimum storage \nArchive  - Lowest cost, highly durable for archiving, backup and DR, lower availability \nI will also go for B"
      },
      {
        "date": "2021-12-22T07:56:00.000Z",
        "voteCount": 1,
        "content": "B. Set up a policy that uses Standard storage for 30 days and then moves to Archive storage for three years. \n _ clearly mentioned data access frequently for 30days and then object not read again until special need ( archive suitable)."
      },
      {
        "date": "2021-12-22T03:52:00.000Z",
        "voteCount": 1,
        "content": "If you access frequently in Nearline, it will cost you more. There is no retrieval cost for Standard. So for frequently accessed object, go with standard. Also in Standard, there is no minimum period to store the object, so don't get confused by 30 days. Ans is B."
      },
      {
        "date": "2021-11-20T03:08:00.000Z",
        "voteCount": 1,
        "content": "Vote For B"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 155,
    "url": "https://www.examtopics.com/discussions/google/view/28354-exam-associate-cloud-engineer-topic-1-question-155/",
    "body": "You are storing sensitive information in a Cloud Storage bucket. For legal reasons, you need to be able to record all requests that read any of the stored data. You want to make sure you comply with these requirements. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable the Identity Aware Proxy API on the project.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tScan the bucket using the Data Loss Prevention API.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAllow only a single Service Account access to read the data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Data Access audit logs for the Cloud Storage API.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-08-22T13:17:00.000Z",
        "voteCount": 32,
        "content": "Correct Answer is (D):\n\nLogged information\nWithin Cloud Audit Logs, there are two types of logs:\n\nAdmin Activity logs: Entries for operations that modify the configuration or metadata of a project, bucket, or object.\n\nData Access logs: Entries for operations that modify objects or read a project, bucket, or object. There are several sub-types of data access logs:\n\nADMIN_READ: Entries for operations that read the configuration or metadata of a project, bucket, or object.\n\nDATA_READ: Entries for operations that read an object.\n\nDATA_WRITE: Entries for operations that create or modify an object.\n\nhttps://cloud.google.com/storage/docs/audit-logs#types"
      },
      {
        "date": "2020-08-12T18:57:00.000Z",
        "voteCount": 19,
        "content": "D is the correct one"
      },
      {
        "date": "2020-08-14T03:00:00.000Z",
        "voteCount": 6,
        "content": "Yes D is the correct"
      },
      {
        "date": "2024-02-28T12:58:00.000Z",
        "voteCount": 2,
        "content": "D is the best answer:\n- Data Access audit logs are specifically designed to track Google Cloud API operations related to data, including reads from Cloud Storage buckets.\n- These logs include details about the user or service account making the request, the time, and the specific data resource accessed.\n- Having this audit trail is essential for demonstrating adherence to regulations around sensitive data handling.\nWhy Others Aren't as Ideal:\nA: Identity-Aware Proxy (IAP): IAP focuses on controlling access to web apps behind firewalls but doesn't inherently log all data read operations.\nB: Data Loss Prevention (DLP): DLP is excellent for identifying sensitive data within your bucket but doesn't provide a continuous audit log of every access.\nC: Restricting Access: While limiting access is a security best practice, it doesn't address the legal requirement to log every read operation."
      },
      {
        "date": "2023-09-06T05:03:00.000Z",
        "voteCount": 2,
        "content": "Enable Data access audit logs for Cloud storage bucket\nhttps://cloud.google.com/storage/docs/audit-logging"
      },
      {
        "date": "2023-09-04T23:14:00.000Z",
        "voteCount": 1,
        "content": "D is the correct answer"
      },
      {
        "date": "2022-11-26T13:02:00.000Z",
        "voteCount": 1,
        "content": "Only logical option"
      },
      {
        "date": "2022-06-24T03:45:00.000Z",
        "voteCount": 1,
        "content": "D is right for this use case"
      },
      {
        "date": "2022-05-15T10:15:00.000Z",
        "voteCount": 2,
        "content": "D is correct as Data Access logs pertaining to Cloud Storage operations are not recorded by default. You have to enable them ...\nhttps://cloud.google.com/storage/docs/audit-logging"
      },
      {
        "date": "2022-04-24T09:11:00.000Z",
        "voteCount": 1,
        "content": "I think it's D"
      },
      {
        "date": "2022-01-08T22:43:00.000Z",
        "voteCount": 2,
        "content": "I also vote for D"
      },
      {
        "date": "2021-08-27T06:51:00.000Z",
        "voteCount": 2,
        "content": "D is the correct Answer"
      },
      {
        "date": "2021-05-23T09:20:00.000Z",
        "voteCount": 1,
        "content": "seems that B is the right!\n\nCloud Data Loss Prevention (DLP) helps you to understand and manage such sensitive data. It provides fast, scalable classification and redaction for sensitive data elements. Using the Data Loss Prevention API and Cloud Functions, you can automatically scan this data before it is uploaded to the shared storage bucket."
      },
      {
        "date": "2021-07-28T01:44:00.000Z",
        "voteCount": 3,
        "content": "the question doesn't ask you to manage or understand sensitive data :\n\" you need to be able to record all requests that read any of the stored data\""
      },
      {
        "date": "2021-03-13T03:20:00.000Z",
        "voteCount": 1,
        "content": "D - Enable Data Access audit logs for the Cloud Storage API."
      },
      {
        "date": "2021-03-09T04:03:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2021-02-28T15:06:00.000Z",
        "voteCount": 2,
        "content": "D. Enable Data Access audit logs for the Cloud Storage API."
      },
      {
        "date": "2020-11-22T06:18:00.000Z",
        "voteCount": 1,
        "content": "\u2022\tD. Enable Data Access audit logs for the Cloud Storage API."
      },
      {
        "date": "2020-10-05T22:18:00.000Z",
        "voteCount": 2,
        "content": "Ans is D"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 156,
    "url": "https://www.examtopics.com/discussions/google/view/28436-exam-associate-cloud-engineer-topic-1-question-156/",
    "body": "You are the team lead of a group of 10 developers. You provided each developer with an individual Google Cloud Project that they can use as their personal sandbox to experiment with different Google Cloud solutions. You want to be notified if any of the developers are spending above $500 per month on their sandbox environment. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a single budget for all projects and configure budget alerts on this budget.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a separate billing account per sandbox project and enable BigQuery billing exports. Create a Data Studio dashboard to plot the spending per billing account.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a budget per project and configure budget alerts on all of these budgets.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a single billing account for all sandbox projects and enable BigQuery billing exports. Create a Data Studio dashboard to plot the spending per project."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-08-22T13:24:00.000Z",
        "voteCount": 48,
        "content": "Correct Answer is (C):\n\nSet budgets and budget alerts\nOverview\nAvoid surprises on your bill by creating Cloud Billing budgets to monitor all of your Google Cloud charges in one place. A budget enables you to track your actual Google Cloud spend against your planned spend. After you've set a budget amount, you set budget alert threshold rules that are used to trigger email notifications. Budget alert emails help you stay informed about how your spend is tracking against your budget.\n\n2. Set budget scope\nSet the budget Scope and then click Next.\n\nIn the Projects field, select one or more projects that you want to apply the budget alert to. To apply the budget alert to all the projects in the Cloud Billing account, choose Select all.\n\nhttps://cloud.google.com/billing/docs/how-to/budgets#budget-scop"
      },
      {
        "date": "2022-02-09T09:14:00.000Z",
        "voteCount": 3,
        "content": "You're the only answer I take seriously \"Thumbs up\""
      },
      {
        "date": "2022-08-04T06:07:00.000Z",
        "voteCount": 1,
        "content": "wait a minute, why not A ?\nAs you said that \n\" In the Projects field, select one or more projects that you want to apply the budget alert to. To apply the budget alert to all the projects in the Cloud Billing account, choose Select all. \"\nAs per this I should be able to create single budget for all the projects and should be able to set alert on that, why create separate budget for all 10 projects ?"
      },
      {
        "date": "2022-09-30T05:56:00.000Z",
        "voteCount": 2,
        "content": "It will be a combined budget that's why it's C"
      },
      {
        "date": "2020-08-19T11:28:00.000Z",
        "voteCount": 10,
        "content": "I think C is the best answer."
      },
      {
        "date": "2024-10-10T01:16:00.000Z",
        "voteCount": 1,
        "content": "C"
      },
      {
        "date": "2024-07-26T02:37:00.000Z",
        "voteCount": 1,
        "content": "Agree, anwser C, as there is no a common billing account mentioned in the question, we need to create a budget by project."
      },
      {
        "date": "2024-01-01T15:31:00.000Z",
        "voteCount": 2,
        "content": "The scope of a budget in GCP can be defined at different levels:\n1.\tProject-Level Budget:\n2.\tBilling Account-Level Budget:\n3.\tSpecific Services or Labels:\n\u2022\tGCP allows you to create budgets for specific services (like Compute Engine, Cloud Storage, etc.) or resources labeled with specific labels within a project or billing account. This level of granularity is useful for tracking costs associated with particular services or resource categories.\n4.\tCredits and Other Filters:\n\u2022\tWhen setting up a budget, you can include or exclude certain types of costs, such as credits, discounts, or taxes, depending on your monitoring needs."
      },
      {
        "date": "2023-09-04T23:17:00.000Z",
        "voteCount": 2,
        "content": "C is the correct answer, because question demands  that which project goes over the 500 per month, to check that you need to create the budget per project"
      },
      {
        "date": "2022-11-13T11:41:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer is (C):"
      },
      {
        "date": "2022-08-19T01:46:00.000Z",
        "voteCount": 2,
        "content": "Key is anyone goes above $500 means it requires project level"
      },
      {
        "date": "2022-06-24T04:00:00.000Z",
        "voteCount": 1,
        "content": "Key is anyone goes above $500 means it requires project level so C is right"
      },
      {
        "date": "2022-04-24T09:08:00.000Z",
        "voteCount": 1,
        "content": "Clearly C is the answer"
      },
      {
        "date": "2021-03-16T23:09:00.000Z",
        "voteCount": 5,
        "content": "Does anyone knows Data Studio can be alert to email?If it can't I'll pick C"
      },
      {
        "date": "2021-02-28T17:56:00.000Z",
        "voteCount": 4,
        "content": "C. Create a budget per project and configure budget alerts on all of these budgets."
      },
      {
        "date": "2021-02-03T05:10:00.000Z",
        "voteCount": 3,
        "content": "I believe is A because of this...\nProjects: In the Projects field, select one or more projects that you want to apply the budget alert to. To apply the budget alert to all of the projects in the Cloud Billing account, choose Select all.\n\n    Some costs are not related to a project, such as the costs of subscriptions or Support costs.\n    In the budget's project scope, in the list of projects you can filter on, [Charges not specific to a project] is not an option you can select.\n    If you choose Select all, then the costs in all projects, including Charges not specific to a project, are included in the budget and cost trend chart cost calculations.\n    If you select one or more projects - but not all projects - then the Charges not specific to a project are not included in the budget and cost trend chart cost calculations.\n    You can view your costs that are not related to a project in the billing reports. Using the projects filter in the reports page, you can select and view [Charges not specific to a project].\n\nURL: https://cloud.google.com/billing/docs/how-to/budgets"
      },
      {
        "date": "2021-12-08T02:56:00.000Z",
        "voteCount": 5,
        "content": "but how will you know who crossed the limit. what if the summation of their usage exceeds 500? the corresponding alert would be a false alarm"
      },
      {
        "date": "2020-11-24T06:41:00.000Z",
        "voteCount": 1,
        "content": "which one correct?"
      },
      {
        "date": "2020-11-22T06:19:00.000Z",
        "voteCount": 1,
        "content": "\u2022\tC. Create a budget per project and configure budget alerts on all of these budgets."
      },
      {
        "date": "2020-09-13T12:24:00.000Z",
        "voteCount": 4,
        "content": "is C, not A\nwith A, i guess if you create a single budget for all projects, together they can easily beat the $500 mark and you need to know if \"one deveoper' did it.\nso one budget per project is the solution."
      },
      {
        "date": "2020-09-12T11:40:00.000Z",
        "voteCount": 3,
        "content": "Yes C is correct, I would have gone with B and D as billing export is the crucial element for billing, but both the option don't talk about notifying about spending.\nAlso, you don't want to combine the billing for all as each one can spend up to 500 so it will be better if they are individual so A is out."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 157,
    "url": "https://www.examtopics.com/discussions/google/view/28327-exam-associate-cloud-engineer-topic-1-question-157/",
    "body": "You are deploying a production application on Compute Engine. You want to prevent anyone from accidentally destroying the instance by clicking the wrong button. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDisable the flag \u05d2\u20acDelete boot disk when instance is deleted.\u05d2\u20ac",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable delete protection on the instance.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDisable Automatic restart on the instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Preemptibility on the instance."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-08-22T13:27:00.000Z",
        "voteCount": 43,
        "content": "Correct Answer is (B):\n\nPreventing Accidental VM Deletion\nThis document describes how to protect specific VM instances from deletion by setting the deletionProtection property on an Instance resource. To learn more about VM instances, read the Instances documentation.\n\nAs part of your workload, there might be certain VM instances that are critical to running your application or services, such as an instance running a SQL server, a server used as a license manager, and so on. These VM instances might need to stay running indefinitely so you need a way to protect these VMs from being deleted.\n\nBy setting the deletionProtection flag, a VM instance can be protected from accidental deletion. If a user attempts to delete a VM instance for which you have set the deletionProtection flag, the request fails. Only a user that has been granted a role with compute.instances.create permission can reset the flag to allow the resource to be deleted.\n\nhttps://cloud.google.com/compute/docs/instances/preventing-accidental-vm-deletion"
      },
      {
        "date": "2023-07-05T10:47:00.000Z",
        "voteCount": 6,
        "content": "Mr.ESP_SAP, your answers are on the spot and I look forward to your notes on all the questions first.. Appreciate your effort and support for this cloud community.. :)"
      },
      {
        "date": "2020-08-12T11:40:00.000Z",
        "voteCount": 11,
        "content": "\"B\" is the answer"
      },
      {
        "date": "2023-11-24T04:49:00.000Z",
        "voteCount": 3,
        "content": "B. Enable delete protection on the instance."
      },
      {
        "date": "2023-11-24T04:50:00.000Z",
        "voteCount": 1,
        "content": "Enabling delete protection helps safeguard your instances from accidental deletion. This means that even if someone attempts to delete the instance through the console or API, they will receive an error, preventing accidental deletion. It acts as an additional layer of protection to avoid critical mistakes."
      },
      {
        "date": "2023-09-06T05:54:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/compute/docs/instances/preventing-accidental-vm-deletion"
      },
      {
        "date": "2023-09-04T23:20:00.000Z",
        "voteCount": 1,
        "content": "B is the correct answer , as it helps to prevent critical instance to get deleted"
      },
      {
        "date": "2022-06-24T04:02:00.000Z",
        "voteCount": 1,
        "content": "This is straight forward question, enable delete protection. B is right"
      },
      {
        "date": "2022-05-28T23:58:00.000Z",
        "voteCount": 2,
        "content": "Preventing Accidental VM Deletion\nThis document describes how to protect specific VM instances from deletion by setting the deletionProtection property on an Instance resource. To learn more about VM instances, read the Instances documentation."
      },
      {
        "date": "2022-01-08T22:50:00.000Z",
        "voteCount": 3,
        "content": "B seems right option"
      },
      {
        "date": "2022-01-05T17:50:00.000Z",
        "voteCount": 2,
        "content": "B - on VM Enable delete protection"
      },
      {
        "date": "2021-12-13T00:46:00.000Z",
        "voteCount": 3,
        "content": "Answer is B. there is an Option in VM instance while creating"
      },
      {
        "date": "2021-08-09T22:01:00.000Z",
        "voteCount": 1,
        "content": "Option A would not prevent , It can be used only after the damage is done. Hence B"
      },
      {
        "date": "2021-04-06T14:14:00.000Z",
        "voteCount": 1,
        "content": "B should be the answer."
      },
      {
        "date": "2021-02-17T19:54:00.000Z",
        "voteCount": 2,
        "content": "B. Enable delete protection on the instance."
      },
      {
        "date": "2021-02-16T09:32:00.000Z",
        "voteCount": 1,
        "content": "B\nThe ans is B"
      },
      {
        "date": "2020-10-05T22:21:00.000Z",
        "voteCount": 2,
        "content": "B for me"
      },
      {
        "date": "2020-09-24T18:05:00.000Z",
        "voteCount": 3,
        "content": "Correct Answer is 'B'"
      },
      {
        "date": "2020-08-13T01:25:00.000Z",
        "voteCount": 5,
        "content": "B is correct answer. https://cloud.google.com/compute/docs/instances/preventing-accidental-vm-deletion"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 158,
    "url": "https://www.examtopics.com/discussions/google/view/28355-exam-associate-cloud-engineer-topic-1-question-158/",
    "body": "Your company uses a large number of Google Cloud services centralized in a single project. All teams have specific projects for testing and development. The<br>DevOps team needs access to all of the production services in order to perform their job. You want to prevent Google Cloud product changes from broadening their permissions in the future. You want to follow Google-recommended practices. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGrant all members of the DevOps team the role of Project Editor on the organization level.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGrant all members of the DevOps team the role of Project Editor on the production project.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a custom role that combines the required permissions. Grant the DevOps team the custom role on the production project.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a custom role that combines the required permissions. Grant the DevOps team the custom role on the organization level."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-08-22T13:39:00.000Z",
        "voteCount": 59,
        "content": "Correct  Answer is (C):\n\nUnderstanding IAM custom roles\n\nKey Point: Custom roles enable you to enforce the principle of least privilege, ensuring that the user and service accounts in your organization have only the permissions essential to performing their intended functions.\n\nBasic concepts\nCustom roles are user-defined, and allow you to bundle one or more supported permissions to meet your specific needs. Custom roles are not maintained by Google; when new permissions, features, or services are added to Google Cloud, your custom roles will not be updated automatically.\n\nWhen you create a custom role, you must choose an organization or project to create it in. You can then grant the custom role on the organization or project, as well as any resources within that organization or project.\n\nhttps://cloud.google.com/iam/docs/understanding-custom-roles#basic_concepts"
      },
      {
        "date": "2020-08-13T01:12:00.000Z",
        "voteCount": 32,
        "content": "\"You want to prevent Google Cloud product changes from broadening their permissions in the future.\" then CUSTOM ROLE"
      },
      {
        "date": "2021-10-16T15:16:00.000Z",
        "voteCount": 3,
        "content": "Great hint, thanks!"
      },
      {
        "date": "2023-12-02T06:47:00.000Z",
        "voteCount": 2,
        "content": "The answer would be B as it will help the DevOps team to work on any resources for others future production project."
      },
      {
        "date": "2024-03-08T03:07:00.000Z",
        "voteCount": 1,
        "content": "But if Google change their roles, they can broaden the rights to those engineers, so that would be a wrong answer IMO. C looks like the correct one from the list."
      },
      {
        "date": "2023-09-26T22:56:00.000Z",
        "voteCount": 3,
        "content": "The giveaway is \"prevent google cloud product changes from broadening their permissions\". Which means that we need to create a custom role. Also they mentioned all production services and not production projects so C"
      },
      {
        "date": "2023-09-06T13:02:00.000Z",
        "voteCount": 1,
        "content": "Custom roles help you enforce the principle of least privilege, because they help to ensure that the principals in your organization have only the permissions that they need.\nCustom roles are user-defined, and allow you to bundle one or more supported permissions to meet your specific needs. When you create a custom role, you must choose an organization or project to create it in. You can then grant the custom role on the organization or project, as well as any resources within that organization or project.\nNote: You cannot define custom roles at the folder level. If you need to use a custom role within a folder, define the custom role at the organization level. \nhttps://cloud.google.com/iam/docs/roles-overview#custom"
      },
      {
        "date": "2023-09-06T13:00:00.000Z",
        "voteCount": 1,
        "content": "Custom roles help you enforce the principle of least privilege, because they help to ensure that the principals in your organization have only the permissions that they need.\n\nCustom roles are user-defined, and allow you to bundle one or more supported permissions to meet your specific needs. When you create a custom role, you must choose an organization or project to create it in. You can then grant the custom role on the organization or project, as well as any resources within that organization or project.\n\nNote: You cannot define custom roles at the folder level. If you need to use a custom role within a folder, define the custom role at the organization level. \n\nhttps://cloud.google.com/iam/docs/roles-overview#custom"
      },
      {
        "date": "2023-09-04T23:23:00.000Z",
        "voteCount": 2,
        "content": "C is the correct answer"
      },
      {
        "date": "2023-04-18T02:34:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2022-12-28T22:44:00.000Z",
        "voteCount": 1,
        "content": "Had this question 2 days ago. C is correct."
      },
      {
        "date": "2022-10-08T21:56:00.000Z",
        "voteCount": 1,
        "content": "C is the correct answer, give the devops team the least privileged role, only the required permissions to access the production services, as the question states 'to prevent product changes' for which editor role is not recommended either at Project or organizational level, organizational level access gives broad scope to all the projects in the organization, this role cannot be given to the devops team. \n\nA. Editor has privilege to change the products, and the scope is broad\nB. Editor has privilege to change the products\nC. Recommended, as this will give only required permission at project level to devops team.\nD. They require only project level access. This gives access to all project in the organization."
      },
      {
        "date": "2022-09-27T00:59:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer is (C):\nCustom roles are user-defined, and allow you to bundle one or more supported permissions to meet your specific needs. Custom roles are not maintained by Google; when new permissions, features, or services are added to Google Cloud, your custom roles will not be updated automatically."
      },
      {
        "date": "2022-08-02T06:47:00.000Z",
        "voteCount": 2,
        "content": "correct answer is A"
      },
      {
        "date": "2022-07-04T01:09:00.000Z",
        "voteCount": 1,
        "content": "There is no doubt, the correct answer is C"
      },
      {
        "date": "2022-06-24T04:06:00.000Z",
        "voteCount": 1,
        "content": "C is right"
      },
      {
        "date": "2022-02-13T18:53:00.000Z",
        "voteCount": 2,
        "content": "C seems to be the popular answer, and it makes sense because the generic roles are not sufficient for these specific requirements. I added this voting comment because the community answers are not currently visible."
      },
      {
        "date": "2022-01-08T22:52:00.000Z",
        "voteCount": 2,
        "content": "I vote for C"
      },
      {
        "date": "2021-12-04T11:07:00.000Z",
        "voteCount": 4,
        "content": "I initially thought C.  But I think this may be a trick question.  \"The DevOps team needs access to ALL of the PRODUCTION services...\"  which are in a \"single\" project.  If \"Project Editor\" is assigned at on the \"production\" project it gives them access to \"ALL\" production services including product changes in the \"production\" project. A custom role would have to be modified to get access to product changes in the production project that required additional permissions, so the DevOps team would not have access to \"ALL\" services until the custom role is modified.\nI am changing my choice to B."
      },
      {
        "date": "2022-09-07T06:00:00.000Z",
        "voteCount": 1,
        "content": "Your choice doesn't follow the rule of least privilege. So correct answer is C."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 159,
    "url": "https://www.examtopics.com/discussions/google/view/28356-exam-associate-cloud-engineer-topic-1-question-159/",
    "body": "You are building an application that processes data files uploaded from thousands of suppliers. Your primary goals for the application are data security and the expiration of aged data. You need to design the application to:<br>* Restrict access so that suppliers can access only their own data.<br>* Give suppliers write access to data only for 30 minutes.<br>* Delete data that is over 45 days old.<br>You have a very short development cycle, and you need to make sure that the application requires minimal maintenance. Which two strategies should you use?<br>(Choose two.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBuild a lifecycle policy to delete Cloud Storage objects after 45 days.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse signed URLs to allow suppliers limited time access to store their objects.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up an SFTP server for your application, and create a separate user for each supplier.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBuild a Cloud function that triggers a timer of 45 days to delete objects that have expired.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDevelop a script that loops through all Cloud Storage buckets and deletes any buckets that are older than 45 days."
    ],
    "answer": "AB",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AB",
        "count": 13,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-08-22T13:55:00.000Z",
        "voteCount": 47,
        "content": "Correct Answers are: (AB):\n\n(A)  Object Lifecycle Management\nDelete\nThe Delete action deletes an object when the object meets all conditions specified in the lifecycle rule.\n\nException: In buckets with Object Versioning enabled, deleting the live version of an object causes it to become a noncurrent version, while deleting a noncurrent version deletes that version permanently.\nhttps://cloud.google.com/storage/docs/lifecycle#delete\n\n\n(B) Signed URLs\nThis page provides an overview of signed URLs, which you use to give time-limited resource access to anyone in possession of the URL, regardless of whether they have a Google account\n\nhttps://cloud.google.com/storage/docs/access-control/signed-urls"
      },
      {
        "date": "2020-08-12T19:00:00.000Z",
        "voteCount": 16,
        "content": "AB is the answer"
      },
      {
        "date": "2023-09-06T13:04:00.000Z",
        "voteCount": 2,
        "content": "Create object lifecycle policy to automatically delete the objects after 45 days. Create signed URLs to temporarily provide the access for specified amount of time."
      },
      {
        "date": "2023-11-29T05:18:00.000Z",
        "voteCount": 1,
        "content": "Did you pass the exam? and pls let me know, The questions that are come from this dumps?"
      },
      {
        "date": "2023-09-04T23:27:00.000Z",
        "voteCount": 2,
        "content": "AB is the correct answer, as A helps to make a lifecycle policy to delete the data after 45 days and b helps the customer to acces their data as per the question requiremnt"
      },
      {
        "date": "2023-01-29T20:56:00.000Z",
        "voteCount": 2,
        "content": "It's a bit obvious. Cloud functions wouldn't really work well with this and would probably require lots of maintenance just as any other option. AB are the correct ones."
      },
      {
        "date": "2022-10-21T14:59:00.000Z",
        "voteCount": 2,
        "content": "AB achieves this objective"
      },
      {
        "date": "2022-08-23T17:12:00.000Z",
        "voteCount": 1,
        "content": "its clearly AB, life cycle and provider private url"
      },
      {
        "date": "2022-08-19T02:22:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer Combo: (AB)"
      },
      {
        "date": "2022-07-29T10:57:00.000Z",
        "voteCount": 1,
        "content": "Correct Answers are: (AB)"
      },
      {
        "date": "2022-07-03T20:01:00.000Z",
        "voteCount": 1,
        "content": "Correct Answers are: A and B"
      },
      {
        "date": "2022-07-01T05:55:00.000Z",
        "voteCount": 1,
        "content": "A and B is the right answer"
      },
      {
        "date": "2022-06-24T04:08:00.000Z",
        "voteCount": 1,
        "content": "A and B right answer"
      },
      {
        "date": "2022-02-13T18:57:00.000Z",
        "voteCount": 1,
        "content": "The website answer says A and E, but these do the same thing. The question also asks about setting up security, so the other part of the answer must be B"
      },
      {
        "date": "2021-04-24T03:20:00.000Z",
        "voteCount": 2,
        "content": "My Answer: A B"
      },
      {
        "date": "2021-03-14T10:31:00.000Z",
        "voteCount": 3,
        "content": "A. Build a lifecycle policy to delete Cloud Storage objects after 45 days.\nB. Use signed URLs to allow suppliers limited time access to store their objects."
      },
      {
        "date": "2020-12-22T01:09:00.000Z",
        "voteCount": 1,
        "content": "A &amp; B helps."
      },
      {
        "date": "2020-12-28T00:31:00.000Z",
        "voteCount": 4,
        "content": "did you pass exam ?"
      },
      {
        "date": "2020-10-05T23:00:00.000Z",
        "voteCount": 2,
        "content": "for me A and B"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 160,
    "url": "https://www.examtopics.com/discussions/google/view/46581-exam-associate-cloud-engineer-topic-1-question-160/",
    "body": "Your company wants to standardize the creation and management of multiple Google Cloud resources using Infrastructure as Code. You want to minimize the amount of repetitive code needed to manage the environment. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDevelop templates for the environment using Cloud Deployment Manager.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse curl in a terminal to send a REST request to the relevant Google API for each individual resource.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Cloud Console interface to provision and manage all related resources.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a bash script that contains all requirement steps as gcloud commands."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-03-11T14:25:00.000Z",
        "voteCount": 33,
        "content": "A\nYou can use Google Cloud Deployment Manager to create a set of Google Cloud resources and manage them as a unit, called a deployment. For example, if your team's development environment needs two virtual machines (VMs) and a BigQuery database, you can define these resources in a configuration file, and use Deployment Manager to create, change, or delete these resources. You can make the configuration file part of your team's code repository, so that anyone can create the same environment with consistent results.\nhttps://cloud.google.com/deployment-manager/docs/quickstart"
      },
      {
        "date": "2021-03-14T10:34:00.000Z",
        "voteCount": 11,
        "content": "A. Develop templates for the environment using Cloud Deployment Manager."
      },
      {
        "date": "2024-05-04T19:52:00.000Z",
        "voteCount": 1,
        "content": "A. \nInfrastructure as Code = Cloud Deployment Manager."
      },
      {
        "date": "2023-09-06T13:08:00.000Z",
        "voteCount": 2,
        "content": "Cloud Deployment Manager is an infrastructure deployment service that automates the creation and management of Google Cloud resources. Write flexible template and configuration files and use them to create deployments that have a variety of Google Cloud services, such as Cloud Storage, Compute Engine, and Cloud SQL, configured to work together.\nYou can use Google Cloud Deployment Manager to create a set of Google Cloud resources and manage them as a unit, called a deployment.\nhttps://cloud.google.com/deployment-manager/docs"
      },
      {
        "date": "2023-09-04T23:33:00.000Z",
        "voteCount": 1,
        "content": "A seems more correct, u can use the deployement manager to create your instances with the same configurstion file"
      },
      {
        "date": "2023-05-10T11:29:00.000Z",
        "voteCount": 1,
        "content": "A. Develop templates for the environment using Cloud Deployment Manager.\n\nAlthough the preferred IaC tool is Terraform. There no mention of Deployment Manager anymore in the Google on-demand courses but there is an entire course on Terraform."
      },
      {
        "date": "2023-03-27T14:02:00.000Z",
        "voteCount": 1,
        "content": "I'm going with A"
      },
      {
        "date": "2022-06-24T04:10:00.000Z",
        "voteCount": 1,
        "content": "A is right"
      },
      {
        "date": "2022-03-21T02:03:00.000Z",
        "voteCount": 1,
        "content": "Develop templates for the environment using Cloud Deployment Manager."
      },
      {
        "date": "2021-12-06T11:11:00.000Z",
        "voteCount": 2,
        "content": "Templates only"
      },
      {
        "date": "2021-11-26T23:05:00.000Z",
        "voteCount": 2,
        "content": "Its A."
      },
      {
        "date": "2021-03-14T08:12:00.000Z",
        "voteCount": 4,
        "content": "A.\nAccording to this document https://cloud.google.com/solutions/infrastructure-as-code\nIaC (Infrastructure as code) tools for Google Cloud:\nDeployment Manager, Terraform, Puppet, Chef ..."
      },
      {
        "date": "2021-03-13T18:51:00.000Z",
        "voteCount": 2,
        "content": "I think A"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 161,
    "url": "https://www.examtopics.com/discussions/google/view/75144-exam-associate-cloud-engineer-topic-1-question-161/",
    "body": "You are performing a monthly security check of your Google Cloud environment and want to know who has access to view data stored in your Google Cloud<br>Project. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Audit Logs for all APIs that are related to data storage.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReview the IAM permissions for any role that allows for data access.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReview the Identity-Aware Proxy settings for each resource.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Data Loss Prevention job."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 35,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-05-27T13:36:00.000Z",
        "voteCount": 16,
        "content": "Only use audit logs to look at history (PAST)\nIf you need current, up-to-date, info regarding permissions always go to IAM"
      },
      {
        "date": "2022-05-10T03:07:00.000Z",
        "voteCount": 10,
        "content": "B is the one: \n\nA. Enable Audit Logs for all APIs that are related to data storage. --&gt; That is not the correct answer, if someone with permissions has not accessed or does not access, it will not be listed.\nB. Review the IAM permissions for any role that allows for data access. --&gt; That's correct\nC. Review the Identity-Aware Proxy settings for each resource. --&gt; Nothing relevant, Proxy? Is configured? The question don't ask or tell something about if it is configured. \nD. Create a Data Loss Prevention job. --&gt; Data Loss Prevention nothing to see here."
      },
      {
        "date": "2024-06-03T23:26:00.000Z",
        "voteCount": 2,
        "content": "\"who has access\"  &gt; We neet to know present, so check IAM &gt; it is B\nIf question says, check who has accessed, yes is past, audit logs"
      },
      {
        "date": "2024-05-14T06:34:00.000Z",
        "voteCount": 1,
        "content": "acces =&gt; IAM\nHistory =&gt; Logs"
      },
      {
        "date": "2022-09-03T04:45:00.000Z",
        "voteCount": 2,
        "content": "B \"WHO HAS ACCESS\""
      },
      {
        "date": "2023-07-05T10:55:00.000Z",
        "voteCount": 1,
        "content": "Yes, that's the catch.. The question here is \"Who has access?\" and not \"Who has accessed?\"\n\nAnswer is \"B\"."
      },
      {
        "date": "2022-06-24T04:12:00.000Z",
        "voteCount": 2,
        "content": "B is correct"
      },
      {
        "date": "2022-05-24T06:15:00.000Z",
        "voteCount": 3,
        "content": "Without any doubt, it's B."
      },
      {
        "date": "2022-05-08T23:20:00.000Z",
        "voteCount": 3,
        "content": "B is the one"
      },
      {
        "date": "2022-05-10T09:23:00.000Z",
        "voteCount": 1,
        "content": "B. 'Audit logs help you answer \"who did what, where, and when?\"'(from https://cloud.google.com/logging/docs/audit). So, not who has access, but rather who accessed."
      },
      {
        "date": "2022-05-04T05:43:00.000Z",
        "voteCount": 1,
        "content": "La r\u00e9ponse A."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 162,
    "url": "https://www.examtopics.com/discussions/google/view/46958-exam-associate-cloud-engineer-topic-1-question-162/",
    "body": "Your company has embraced a hybrid cloud strategy where some of the applications are deployed on Google Cloud. A Virtual Private Network (VPN) tunnel connects your Virtual Private Cloud (VPC) in Google Cloud with your company's on-premises network. Multiple applications in Google Cloud need to connect to an on-premises database server, and you want to avoid having to change the IP configuration in all of your applications when the IP of the database changes.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Cloud NAT for all subnets of your VPC to be used when egressing from the VM instances.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a private zone on Cloud DNS, and configure the applications with the DNS name.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the IP of the database as custom metadata for each instance, and query the metadata server.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tQuery the Compute Engine internal DNS from the applications to retrieve the IP of the database."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-04-17T13:36:00.000Z",
        "voteCount": 34,
        "content": "B, \nForwarding zones\nCloud DNS forwarding zones let you configure target name servers for specific private zones. Using a forwarding zone is one way to implement outbound DNS forwarding from your VPC network.\n\nA Cloud DNS forwarding zone is a special type of Cloud DNS private zone. Instead of creating records within the zone, you specify a set of forwarding targets. Each forwarding target is an IP address of a DNS server, located in your VPC network, or in an on-premises network connected to your VPC network by Cloud VPN or Cloud Interconnect.\n\nA does not apply, that is to provide internet access to resources\nC, does not apply\nD, I don't get it\n\nso B"
      },
      {
        "date": "2021-04-21T00:07:00.000Z",
        "voteCount": 3,
        "content": "Agreed, It's B although I chose A intitally. After some careful consideration and understanding how Cloud NAT works, I'm sticking with B \nhttps://cloud.google.com/nat/docs/overview"
      },
      {
        "date": "2021-04-22T07:40:00.000Z",
        "voteCount": 1,
        "content": "Further clarification:\n''On-premises clients can resolve records in private zones, forwarding zones, and peering zones for which the VPC network has been authorized. On-premises clients use Cloud VPN or Cloud Interconnect to connect to the VPC network.''"
      },
      {
        "date": "2021-06-13T16:43:00.000Z",
        "voteCount": 3,
        "content": "this is talking about On-premises client resolving nodes outside their network . the question is about how would the application tier within the VPC would resolve the database server . you're confusing the resolution direction my friend"
      },
      {
        "date": "2021-06-13T19:43:00.000Z",
        "voteCount": 3,
        "content": "It is still B , but it's rather outbound forward that's needed here : \nDNS outbound Forwarding : \n- Set up outbound forwarding private zones to query on-premises servers (On-prem Authoritative Zone: corp.example.com)\n- In Cloud Router , add a custom route advertisement for GCP DNS proxies range 35.199.192.0/19  to the on-premises environment.\n- Make sure inbound DNS traffic from 35.199.192.0/19 is allowed on on-prem firewall \n- Cloud Router should be learning on-prem network route from On-prem Router\nhttps://youtu.be/OH_Jw8NhEGU?t=1283\nhttps://cloud.google.com/dns/docs/best-practices#use_forwarding_zones_to_query_on-premises_servers"
      },
      {
        "date": "2022-08-31T05:15:00.000Z",
        "voteCount": 1,
        "content": "\"A does not apply, that is to provide internet access to resources\" - do you really think NAT is only being used between public and private? Well...it's not! :)"
      },
      {
        "date": "2021-03-25T22:32:00.000Z",
        "voteCount": 9,
        "content": "https://cloud.google.com/dns/docs/best-practices#best_practices_for_dns_forwarding_zones_and_server_policies\nCloud DNS offers DNS forwarding zones and DNS server policies to allow lookups of DNS names between your on-premises and Google Cloud environment. You have multiple options for configuring DNS forwarding. The following section lists best practices for hybrid DNS setup. These best practices are illustrated in the Reference architectures for hybrid DNS.\nSo I think B is correct"
      },
      {
        "date": "2023-11-23T09:57:00.000Z",
        "voteCount": 1,
        "content": "B\n\nhttps://cloud.google.com/dns/docs/overview"
      },
      {
        "date": "2023-09-06T13:13:00.000Z",
        "voteCount": 3,
        "content": "DNS is a hierarchical distributed database that lets you store IP addresses and other data and look them up by name. Cloud DNS lets you publish your zones and records in DNS without the burden of managing your own DNS servers and software.\nCloud DNS offers both public zones and private managed DNS zones. A public zone is visible to the public internet, while a private zone is visible only from one or more Virtual Private Cloud (VPC) networks that you specify.\nhttps://cloud.google.com/dns/docs/overview"
      },
      {
        "date": "2023-09-05T08:20:00.000Z",
        "voteCount": 1,
        "content": "B is the correct Answer"
      },
      {
        "date": "2023-01-29T21:22:00.000Z",
        "voteCount": 1,
        "content": "Based on this - https://cloud.google.com/dns/docs/overview#dns-forwarding-methods B must be the best option. I don't think there's a \"typo\" (or completely wrongly worded answer) in option D (there's comments saying that instead of Compute Engine it should be on-premise). I believe option D is wrong on purpose to create a confusion."
      },
      {
        "date": "2022-10-08T22:15:00.000Z",
        "voteCount": 1,
        "content": "B is correct answer,\nConfigure Private Google Access for on-premises hosts,\n\nDNS configuration\nYour on-premises network must have DNS zones and records configured so that Google domain names resolve to the set of IP addresses for either private.googleapis.com or restricted.googleapis.com. You can create Cloud DNS managed private zones and use a Cloud DNS inbound server policy, or you can configure on-premises name servers. For example, you can use BIND or Microsoft Active Directory DNS.\n\nhttps://cloud.google.com/vpc/docs/configure-private-google-access-hybrid#config-domain"
      },
      {
        "date": "2022-09-26T18:08:00.000Z",
        "voteCount": 1,
        "content": "Ans is D, looks like there is typo"
      },
      {
        "date": "2022-08-14T02:54:00.000Z",
        "voteCount": 1,
        "content": "B. DNS works best with dynamic IPs."
      },
      {
        "date": "2022-07-03T20:16:00.000Z",
        "voteCount": 1,
        "content": "Correct Ans is B\nRef - https://cloud.google.com/dns/docs/best-practices#best_practices_for_private_zones"
      },
      {
        "date": "2022-07-01T23:34:00.000Z",
        "voteCount": 2,
        "content": "B Cloud DNS"
      },
      {
        "date": "2022-06-24T04:15:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2022-06-18T01:36:00.000Z",
        "voteCount": 1,
        "content": "B: DNS"
      },
      {
        "date": "2022-01-10T16:31:00.000Z",
        "voteCount": 3,
        "content": "ans D\nhttps://cloud.google.com/compute/docs/internal-dns"
      },
      {
        "date": "2021-12-08T04:15:00.000Z",
        "voteCount": 1,
        "content": "option B"
      },
      {
        "date": "2021-11-27T19:41:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/dns/docs/overview#:~:text=Create%20an%20inbound,the%20VPC%20network."
      },
      {
        "date": "2021-07-28T07:30:00.000Z",
        "voteCount": 3,
        "content": "IT's D, because:\nA) Cloud NAT direction will be from the cloud resources to the on prem, an the DB is on prem (It will not work if the IP of the database on prem changes, and you have an VPN you should traffic your data trough it).\nB) If you create a private zone and configure the applications, if your servers doesnt use the dns will not work.\nC) I think is not suitable. I guess you should re-deploy all your Apps with the new conf if the address change.\nD) If you have an A record ip of your DB HOST(wich is on prem) in Engine internal DNS and if it changes, you can update the registry quickly to change it to the new IP address, so it will be the best option for sure."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 163,
    "url": "https://www.examtopics.com/discussions/google/view/46387-exam-associate-cloud-engineer-topic-1-question-163/",
    "body": "You have developed a containerized web application that will serve internal colleagues during business hours. You want to ensure that no costs are incurred outside of the hours the application is used. You have just created a new Google Cloud project and want to deploy the application. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the container on Cloud Run for Anthos, and set the minimum number of instances to zero.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the container on Cloud Run (fully managed), and set the minimum number of instances to zero.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the container on App Engine flexible environment with autoscaling, and set the value min_instances to zero in the app.yaml.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the container on App Engine flexible environment with manual scaling, and set the value instances to zero in the app.yaml."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 26,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-03-11T07:36:00.000Z",
        "voteCount": 39,
        "content": "I think that is B the correct answer, because Cloud Run can scale to 0:\nhttps://cloud.google.com/run/docs/about-instance-autoscaling\nAnd App Engine Flexible can't scale to 0, the minimum instance number is 1:\nhttps://cloud.google.com/appengine/docs/the-appengine-environments#comparing_high-level_features"
      },
      {
        "date": "2022-08-08T02:03:00.000Z",
        "voteCount": 3,
        "content": "No for the App Engine Flexible Environment, but App Engine Standard can also scale to zero."
      },
      {
        "date": "2021-03-28T15:39:00.000Z",
        "voteCount": 16,
        "content": "B:\nnot A because Anthos is an add-on to GKE clusters, 'new project' means we dont have a GKE cluster to work with\nhttps://cloud.google.com/kuberun/docs/architecture-overview#components_in_the_default_installation"
      },
      {
        "date": "2024-05-23T09:31:00.000Z",
        "voteCount": 1,
        "content": "Container = cloud run (not App Engine)\n\nOn AE, app runs as a node process, like booting it up with npm start locally. AE is a traditional hosting platform: it runs continuously and serves requests as they come in. At the end of the month, you pay for the amount of time it was running, which is typically \u201cthe entire month\u201d.\n\nCloud Run runs containers, so for each release you have to build a container and push it to GCP. Unlike App Engine, Cloud Run only runs when requests come in, so you don\u2019t pay for time spent idling."
      },
      {
        "date": "2023-09-06T13:17:00.000Z",
        "voteCount": 3,
        "content": "Cloud Run is a managed compute platform that lets you run containers directly on top of Google's scalable infrastructure.\nCloud Run adds and removes instances automatically to handle all incoming requests. If there are no incoming requests to your service, even the last remaining instance will be removed. This behavior is commonly referred to as scale to zero.\nhttps://cloud.google.com/run/docs/overview/what-is-cloud-run"
      },
      {
        "date": "2023-09-05T08:23:00.000Z",
        "voteCount": 1,
        "content": "B is the  correct answer, in c, d  as App engine flexible environment can't scale to Zero and A in this GKE cluster is used but we have just created a project so it will add extra cost"
      },
      {
        "date": "2023-05-10T12:29:00.000Z",
        "voteCount": 4,
        "content": "C. and D. are wrong answers as only the App Engine standard environment scales down to zero.\nAnswer A. will incur extra cost as Cloud Run for Anthos runs on Kubernetes, so need to have a k8s cluster available.\nB. Is correct, as \"Cloud Run automatically scales up or down from zero to N depending on traffic, leveraging container image streaming for a fast startup time.\" from https://cloud.google.com/run"
      },
      {
        "date": "2022-10-08T22:33:00.000Z",
        "voteCount": 4,
        "content": "B is the correct answer,\nCloud Functions can scale to zero, whereas App Engine will not be able to scale to zero, it should have at least one instance.\n\nAdd-on Info,\nApp-Engine Standard can scale to zero, whereas App-Engine Flexible couldn't scale down to zero."
      },
      {
        "date": "2022-09-16T00:18:00.000Z",
        "voteCount": 2,
        "content": "B is the answer since we can scale to 0 and the other key word is \"containerized\""
      },
      {
        "date": "2022-07-04T01:24:00.000Z",
        "voteCount": 1,
        "content": "B is the right answer"
      },
      {
        "date": "2022-06-24T04:20:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2022-05-13T10:41:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/appengine/docs/standard/python/config/appref\nmin_instances\nWarning: For this feature to function properly, you must make sure that warmup requests are enabled and that your application handles warmup requests.\nNote: This setting applies only if the version of the app defined by this app.yaml file is configured to receive traffic. To learn more about routing traffic to different versions of an app, see Splitting Traffic.\nOptional. The minimum number of instances for App Engine to create for this module version. These instances serve traffic when requests arrive, and continue to serve traffic even when additional instances are started up as required to handle traffic.\nSpecify a value from 0 to 1000. You can set the parameter to the value 0 to allow scaling to 0 instances to lower costs when no requests are being served. Note that you are charged for the number of instances specified whether they are receiving traffic or not.\n\nSo C"
      },
      {
        "date": "2022-05-11T16:43:00.000Z",
        "voteCount": 4,
        "content": "along with the reason that most have stated (only Cloud Run can scale down to 0 instances) another reason is that Cloud Run is pay-per-use. App-engine flexible is paid based on usage of vCPU, memory, and persistent disks,  so you will be racking up cost quickly just because your VM's are created (regardless if they're running or not)"
      },
      {
        "date": "2022-04-29T04:04:00.000Z",
        "voteCount": 1,
        "content": "B is the correct"
      },
      {
        "date": "2022-03-11T13:04:00.000Z",
        "voteCount": 3,
        "content": "Flex caanot scale down to 0. Standard can but that is not relevant here.\nTherefore, Cloud Run is the best answer since it can scale down to 0 when there is no traffic"
      },
      {
        "date": "2022-02-21T22:25:00.000Z",
        "voteCount": 1,
        "content": "Correct anwer is B, Cloud Run is a serverless solution that, same as Cloud Functions, can be activated based on events."
      },
      {
        "date": "2022-02-21T17:59:00.000Z",
        "voteCount": 1,
        "content": "The majority vote here seems to be B so I'm posting this as a voting comment to make that visible. I am not entirely sure, but suspect that the \"min-instances\" issue is a red herring - that is, I understand why some comments raise this as being a reason why B may not be correct bit I think maybe it's not the intention of the question to focus on that rather tricky aspect."
      },
      {
        "date": "2022-01-21T03:33:00.000Z",
        "voteCount": 2,
        "content": "Option B is correct. After Business hours there will be no collogues to work on that application then cloud run can scale to zero instances"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 164,
    "url": "https://www.examtopics.com/discussions/google/view/46495-exam-associate-cloud-engineer-topic-1-question-164/",
    "body": "You have experimented with Google Cloud using your own credit card and expensed the costs to your company. Your company wants to streamline the billing process and charge the costs of your projects to their monthly invoice. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGrant the financial team the IAM role of \u05d2\u20acBilling Account User\u05d2\u20ac on the billing account linked to your credit card.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up BigQuery billing export and grant your financial department IAM access to query the data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a ticket with Google Billing Support to ask them to send the invoice to your company.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the billing account of your projects to the billing account of your company.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-03-11T10:37:00.000Z",
        "voteCount": 28,
        "content": "1000% Ans D"
      },
      {
        "date": "2022-04-28T13:41:00.000Z",
        "voteCount": 11,
        "content": "Please do not overthink the question. The question does not mention anything about finance teams, so A cannot be correct. D is the only one that makes sense out of the remaining options."
      },
      {
        "date": "2023-08-13T01:48:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2023-05-10T12:43:00.000Z",
        "voteCount": 1,
        "content": "With A. the financial team can only link the billing account linked to the credit card.\nB. C. are wrong (no comment).\nD. is the only correct answer even though it doesn't give the exact permission to grant in oder to do this. I guess the financial team already has the Billing Project Manager role at a folder or organization level which would allow them to make the change."
      },
      {
        "date": "2023-01-26T12:26:00.000Z",
        "voteCount": 2,
        "content": "D is the answer"
      },
      {
        "date": "2022-12-29T20:38:00.000Z",
        "voteCount": 1,
        "content": "d seems the answer"
      },
      {
        "date": "2022-09-24T10:40:00.000Z",
        "voteCount": 4,
        "content": "had this one today"
      },
      {
        "date": "2022-07-01T23:37:00.000Z",
        "voteCount": 1,
        "content": "D D D D D"
      },
      {
        "date": "2022-06-24T04:22:00.000Z",
        "voteCount": 1,
        "content": "D is perfect"
      },
      {
        "date": "2022-01-08T23:14:00.000Z",
        "voteCount": 2,
        "content": "D seems correct ans"
      },
      {
        "date": "2021-12-12T07:50:00.000Z",
        "voteCount": 1,
        "content": "Either A or D are incomplete solution.\n\nSoution A\nGrant financial team the IAM role of Project Owner or Project Billing Manager on your project, then let financial team change the billing account.\n\nSolution B\nGrant you IAM role of Billing Account Administrator or Billing Account User on company's project, then you can change the billing account.\n\nI hope this question won't be in the exam, or has a more accurate answer in the exam."
      },
      {
        "date": "2022-09-27T10:10:00.000Z",
        "voteCount": 1,
        "content": "D -- is the answer\ncant be A. you dont want to share your personnel credit card data with company billing account"
      },
      {
        "date": "2023-04-07T19:07:00.000Z",
        "voteCount": 1,
        "content": "the company will let you link their account to you personal project?"
      },
      {
        "date": "2021-09-26T11:03:00.000Z",
        "voteCount": 2,
        "content": "correct answer D- Change the billing account of your projects to the billing account of your company"
      },
      {
        "date": "2021-05-20T07:31:00.000Z",
        "voteCount": 2,
        "content": "D is correct"
      },
      {
        "date": "2021-03-18T06:20:00.000Z",
        "voteCount": 3,
        "content": "Does the user have access to the company billing account? Not sure of D, I think more A. \n\nTo change the Cloud Billing account for a project, you need to be able to move a project from one Cloud Billing account to another. To accomplish this task, you need permissions adequate to unlink the project from the existing Cloud Billing account AND to link the project to the target Cloud Billing account.\nRoles with adequate permissions to perform this task: Project Owner or Project Billing Manager on the project, AND Billing Account Administrator or Billing Account User for the target Cloud Billing account\n\nhttps://cloud.google.com/billing/docs/how-to/modify-project#change_the_billing_account_for_a_project"
      },
      {
        "date": "2021-03-24T17:37:00.000Z",
        "voteCount": 1,
        "content": "A would be ok if assigning Project Billing Manager IAM role to the finance team. \n\nSo the answer will be D."
      },
      {
        "date": "2021-07-07T07:35:00.000Z",
        "voteCount": 1,
        "content": "\"Billing Account User\"  Role Link projects to billing accounts. Since It does not say anything about user permissions , lets assume if user does not have sufficient permission then Granting the financial team the \"\"Billing Account User\" \" role will do the job"
      },
      {
        "date": "2021-12-09T08:49:00.000Z",
        "voteCount": 1,
        "content": "\"Grant the financial team the IAM role of \u05d2\u20acBilling Account User\u05d2\u20ac on the billing account linked to your credit card. \"\nGiving billing user rights to the financial team to the billing account binded to your credit card wont give them any means to change it or maybe direct/export to the company's billing.\nBilling user gives the following permissions on a billing account:\n\" This role has very restricted permissions, so you can grant it broadly, typically in combination with Project Creator. These two roles allow a user to create new projects linked to the billing account on which the role is granted. \""
      },
      {
        "date": "2021-12-09T09:32:00.000Z",
        "voteCount": 1,
        "content": "Still, you do make a valid point regarding the access. How will i add my projects to the company's wihtout access? Its not possible indeed.\nThat said, i would say that the best option is B (and when i say best i mean for the given options), since you can explort billing data in bigquery:\nhttps://cloud.google.com/billing/docs/how-to/export-data-bigquery"
      },
      {
        "date": "2022-09-27T10:13:00.000Z",
        "voteCount": 1,
        "content": "in B option we can get only billing / budget data, but question asks for streamlining of billing process, mean they want to pay bills using company invoice."
      },
      {
        "date": "2021-03-14T13:00:00.000Z",
        "voteCount": 4,
        "content": "D. Change the billing account of your projects to the billing account of your company."
      },
      {
        "date": "2021-03-14T09:01:00.000Z",
        "voteCount": 2,
        "content": "D. Change the billing account of your projects to the billing account of your company."
      },
      {
        "date": "2021-03-13T18:58:00.000Z",
        "voteCount": 3,
        "content": "yes it is D"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 165,
    "url": "https://www.examtopics.com/discussions/google/view/46746-exam-associate-cloud-engineer-topic-1-question-165/",
    "body": "You are running a data warehouse on BigQuery. A partner company is offering a recommendation engine based on the data in your data warehouse. The partner company is also running their application on Google Cloud. They manage the resources in their own project, but they need access to the BigQuery dataset in your project. You want to provide the partner company with access to the dataset. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Service Account in your own project, and grant this Service Account access to BigQuery in your project.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Service Account in your own project, and ask the partner to grant this Service Account access to BigQuery in their project.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAsk the partner to create a Service Account in their project, and have them give the Service Account access to BigQuery in their project.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAsk the partner to create a Service Account in their project, and grant their Service Account access to the BigQuery dataset in your project.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 13,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-03-14T13:04:00.000Z",
        "voteCount": 36,
        "content": "D. Ask the partner to create a Service Account in their project, and grant their Service Account access to the BigQuery dataset in your project."
      },
      {
        "date": "2021-03-26T00:30:00.000Z",
        "voteCount": 14,
        "content": "BigQuery is in our project,so we need to create a service account and grant it access BigQuery role.That can make partner company to use this account to use it to access our project's BigQuery.So I vote A."
      },
      {
        "date": "2022-05-24T06:39:00.000Z",
        "voteCount": 3,
        "content": "Your understanding is bit wrong here, my friend!"
      },
      {
        "date": "2021-04-01T08:35:00.000Z",
        "voteCount": 11,
        "content": "See, the ones who want our access needs to create a service account(in our case it's the partner company), then we give access to the service account with the user permissions. Clearly, D says the same thing!"
      },
      {
        "date": "2024-02-29T09:42:00.000Z",
        "voteCount": 3,
        "content": "D is the best answer:\n- By having the partner create a Service Account and you granting access to it, you maintain ownership and control over your BigQuery data.\n- You can specifically grant the partner's Service Account the necessary BigQuery permissions (e.g., \"BigQuery Data Viewer\"), avoiding overly broad access.\n- Each company manages Service Accounts within their own projects, maintaining a separation of concerns.\n\nWhy Others Aren't as Ideal:\nA &amp; B: Creating a Service Account in your project and sharing it with the partner (or vice versa) introduces potential management complexities and blurs the lines of responsibility for that Service Account.\nC: Giving the partner company full control to grant their own service accounts access to your dataset could open up broader access than intended."
      },
      {
        "date": "2023-09-06T13:24:00.000Z",
        "voteCount": 4,
        "content": "Cross project access. Application in Project A want to access a service in project B.\n1. Create a service account in project A.\n2. Give the required permission to access the services in project B."
      },
      {
        "date": "2023-05-10T12:48:00.000Z",
        "voteCount": 1,
        "content": "A. Useless if the private key of the Service Account is not shared with the partner (this would not be a good practice in terms of security)\nB. Not possible.\nC. Useless as the won't have access to the data in our data warehouse on BigQuery.\nD. Is the correct answer and follow best practices."
      },
      {
        "date": "2022-11-21T05:27:00.000Z",
        "voteCount": 2,
        "content": "Should be D"
      },
      {
        "date": "2022-11-01T05:50:00.000Z",
        "voteCount": 1,
        "content": "\"Service accounts are both identities and resources. Because service accounts are identities, you can let a service account access resources in your project by granting it a role, just like you would for any other principal.\""
      },
      {
        "date": "2022-09-24T10:40:00.000Z",
        "voteCount": 2,
        "content": "had this one today"
      },
      {
        "date": "2022-06-24T04:24:00.000Z",
        "voteCount": 1,
        "content": "D is right"
      },
      {
        "date": "2022-03-25T03:36:00.000Z",
        "voteCount": 2,
        "content": "https://gtseres.medium.com/using-service-accounts-across-projects-in-gcp-cf9473fef8f0#:~:text=Go%20to%20the%20destination%20project,Voila!"
      },
      {
        "date": "2022-03-02T06:09:00.000Z",
        "voteCount": 4,
        "content": "I thought it was A. But when I quickly did some research I found this: \n\"Service accounts are both identities and resources. Because service accounts are identities, you can let a service account access resources in your project by granting it a role, just like you would for any other principal.\"\nThus, the answer is D."
      },
      {
        "date": "2021-12-09T10:09:00.000Z",
        "voteCount": 4,
        "content": "D per my understanding: if the need is to authenticate the application to access your dataset, it's the application's serice account that will be provided during the authentication, so the service account is to be created at their side to run the application, not the other way around."
      },
      {
        "date": "2021-12-09T10:14:00.000Z",
        "voteCount": 2,
        "content": "Another insight:\nA is to broad. The question states: \"provide the partner company with access to the dataset\"\nA states: \"grant this Service Account access to BigQuery in your project\"\nI think D is a more granular option, given that A would give access to all datasets in your bigquery data warehousing."
      },
      {
        "date": "2021-11-26T09:11:00.000Z",
        "voteCount": 4,
        "content": "How is it D? I want to give access to my BigQuery data, so I need to provide the ServiceAccount. I create it, put some decent predefined roles on it, and whenever I stop working with the other company, I either invalidate the JSON key of the SA or I simply delete the SA. For me, it is A."
      },
      {
        "date": "2022-04-18T02:44:00.000Z",
        "voteCount": 2,
        "content": "Righ but how will the other project ever gain access to resources on YOUR project? Key thing here is that your sharing ACROSS different projects so you need a bridge between them. D provides that bridge by connecting THEIR service account with YOUR resource (big query)"
      },
      {
        "date": "2021-06-18T06:23:00.000Z",
        "voteCount": 6,
        "content": "Answer should be D, as the other company project needs access in your project."
      },
      {
        "date": "2021-05-12T05:06:00.000Z",
        "voteCount": 4,
        "content": "It|s A"
      },
      {
        "date": "2021-04-17T13:49:00.000Z",
        "voteCount": 6,
        "content": "D is the answer"
      },
      {
        "date": "2021-03-14T09:11:00.000Z",
        "voteCount": 3,
        "content": "I think it is \"D\" but I have not found such usecase when you share dataset with another organization via service account"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 166,
    "url": "https://www.examtopics.com/discussions/google/view/46417-exam-associate-cloud-engineer-topic-1-question-166/",
    "body": "Your web application has been running successfully on Cloud Run for Anthos. You want to evaluate an updated version of the application with a specific percentage of your production users (canary deployment). What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new service with the new version of the application. Split traffic between this version and the version that is currently running.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new revision with the new version of the application. Split traffic between this version and the version that is currently running.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new service with the new version of the application. Add an HTTP Load Balancer in front of both services.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new revision with the new version of the application. Add an HTTP Load Balancer in front of both revisions."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-03-11T08:18:00.000Z",
        "voteCount": 37,
        "content": "In my opinion correct answer is B:\nhttps://cloud.google.com/run/docs/rollouts-rollbacks-traffic-migration?utm_campaign=CDR_ahm_aap-severless_cloud-run-faq_&amp;utm_source=external&amp;utm_medium=web\n\nCloud Run can split traffic between revisions"
      },
      {
        "date": "2021-03-14T12:58:00.000Z",
        "voteCount": 11,
        "content": "The google doc link is incorrect. You need to specify CloudRun for Anthos\nhttps://cloud.google.com/kuberun/docs/rollouts-rollbacks-traffic-migration\n\nAnyway principles for CloudRun and CloundRun for Anthos are the same. Traffic can be split between multiple revisions.\nThe answer is \"B\""
      },
      {
        "date": "2021-03-14T13:19:00.000Z",
        "voteCount": 10,
        "content": "B. Create a new revision with the new version of the application. Split traffic between this version and the version that is currently running."
      },
      {
        "date": "2023-09-06T13:27:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/anthos/run/docs/rollouts-rollbacks-traffic-migration"
      },
      {
        "date": "2023-05-18T15:47:00.000Z",
        "voteCount": 1,
        "content": "which answer is correct.  the one \"Correct Answer\"  or the Community vote distribution winner ?"
      },
      {
        "date": "2023-08-29T16:55:00.000Z",
        "voteCount": 2,
        "content": "I recommend to you to go with the answer that has the most number of Upvotes"
      },
      {
        "date": "2024-09-09T12:16:00.000Z",
        "voteCount": 1,
        "content": "I recommend that you analyze all options and make an informed decision, rather than just following the upvotes."
      },
      {
        "date": "2023-01-02T22:48:00.000Z",
        "voteCount": 1,
        "content": "Keyword - \"Updated Version\"\nB. Create a new \"revision\""
      },
      {
        "date": "2022-12-23T14:01:00.000Z",
        "voteCount": 1,
        "content": "B is ok"
      },
      {
        "date": "2022-10-27T03:58:00.000Z",
        "voteCount": 1,
        "content": "The latest Document\nhttps://cloud.google.com/anthos/run/docs/rollouts-rollbacks-traffic-migration"
      },
      {
        "date": "2022-10-24T01:57:00.000Z",
        "voteCount": 1,
        "content": "B. Create a new revision"
      },
      {
        "date": "2022-09-16T01:17:00.000Z",
        "voteCount": 1,
        "content": "B is the correct answer"
      },
      {
        "date": "2022-09-02T18:03:00.000Z",
        "voteCount": 1,
        "content": "i think B"
      },
      {
        "date": "2022-06-24T04:27:00.000Z",
        "voteCount": 2,
        "content": "B is correct for this scenario, there is no need to create new services"
      },
      {
        "date": "2022-05-24T17:34:00.000Z",
        "voteCount": 1,
        "content": "Cloud Run for Anthos allows you to specify which revisions should receive traffic and to specify traffic percentages that are received by a revision"
      },
      {
        "date": "2022-02-02T09:31:00.000Z",
        "voteCount": 1,
        "content": "If course B"
      },
      {
        "date": "2021-06-28T21:36:00.000Z",
        "voteCount": 5,
        "content": "Answer should be B."
      },
      {
        "date": "2021-06-14T10:04:00.000Z",
        "voteCount": 5,
        "content": "Canary deployments are a method of releasing software to a subset of users or servers. The plan is to deliver the update to a small selection of servers first, test it, and then roll it out to the other servers.\n\nSo it's a Revision or new update or a new version, not a service."
      },
      {
        "date": "2021-06-05T09:59:00.000Z",
        "voteCount": 2,
        "content": "Its B\nhttps://cloud.google.com/run/docs/managing/revisions"
      },
      {
        "date": "2021-05-23T15:36:00.000Z",
        "voteCount": 3,
        "content": "Its B, revision or version, not service."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 167,
    "url": "https://www.examtopics.com/discussions/google/view/46427-exam-associate-cloud-engineer-topic-1-question-167/",
    "body": "Your company developed a mobile game that is deployed on Google Cloud. Gamers are connecting to the game with their personal phones over the Internet. The game sends UDP packets to update the servers about the gamers' actions while they are playing in multiplayer mode. Your game backend can scale over multiple virtual machines (VMs), and you want to expose the VMs over a single IP address. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an SSL Proxy load balancer in front of the application servers.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an Internal UDP load balancer in front of the application servers.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an External HTTP(s) load balancer in front of the application servers.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an External Network load balancer in front of the application servers.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 14,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-04-17T14:13:00.000Z",
        "voteCount": 32,
        "content": "Answer is D, cell phones are sending UDP packets and the only that can receive that type of traffic is a External Network TCP/UDP\nhttps://cloud.google.com/load-balancing/docs/network"
      },
      {
        "date": "2021-08-16T13:47:00.000Z",
        "voteCount": 2,
        "content": "Google Cloud HTTP(S) Load Balancing is a global, proxy-based Layer 7 load balancer that enables you to run and scale your services worldwide behind a single external IP address. External HTTP(S) Load Balancing distributes HTTP and HTTPS traffic to backends hosted on Compute Engine and Google Kubernetes Engine (GKE).\n\nhttps://cloud.google.com/load-balancing/docs/https"
      },
      {
        "date": "2022-07-12T15:19:00.000Z",
        "voteCount": 3,
        "content": "what you are trying to say ? What is your answer ? A B C D ?"
      },
      {
        "date": "2022-08-06T00:16:00.000Z",
        "voteCount": 3,
        "content": "All the load balancer products in GCP give you a single IP address for the backend servers you registered to it.\n\nAlso, External HTTP(s) load balancer only support the port that used by HTTP which is the port 80 and HTTPS which is the port 443.\n\nAnd Google Cloud external TCP/UDP Network Load Balancing is referred to as \"Network Load Balancing\" which supports UDP packets.\n\n- https://cloud.google.com/load-balancing/docs/load-balancing-overview#about\n- https://cloud.google.com/load-balancing/docs/network\n- https://cloud.google.com/load-balancing/docs/https"
      },
      {
        "date": "2021-06-10T19:25:00.000Z",
        "voteCount": 14,
        "content": "Answer is D. there are so many confusion here, from B,C or D. For myself im eliminating all options except B,D due to the traffic type. which leaves me with B or D. Then next the traffic source either external or internal which in this case is an external traffic from the internet, therefore my final answer is D. \n\nhttps://cloud.google.com/load-balancing/docs/choosing-load-balancer"
      },
      {
        "date": "2021-10-25T15:32:00.000Z",
        "voteCount": 4,
        "content": "Following the diagram, there's no doubt about D. We have external clients connecting to our gaming service on google cloud that works using UDP traffic that results in using External Network Load Balancing. I feel that it's simple as it is. I also go with D."
      },
      {
        "date": "2024-01-01T18:51:00.000Z",
        "voteCount": 4,
        "content": "1.\tUDP Traffic Support:\n\u2022\tAn external Network Load Balancer in Google Cloud supports both TCP and UDP traffic. Since your game uses UDP packets for multiplayer interactions, the Network Load Balancer is appropriate for handling this type of traffic.\n2.\tSingle IP for Multiple VMs:\n\u2022\tNetwork Load Balancers allow you to use a single, anycast IP address that can distribute incoming traffic across multiple VMs in your backend. This aligns with your requirement to expose the backend servers through a single IP address."
      },
      {
        "date": "2023-09-05T08:34:00.000Z",
        "voteCount": 2,
        "content": "for udp external load balancer, D is the correct answer"
      },
      {
        "date": "2023-06-11T05:36:00.000Z",
        "voteCount": 5,
        "content": "By elimination\nA: SSL proxy LB is for TCP traffic not for UDP, eliminated\nB: External LB is required, Eliminated\nC: Http LB works at layer 7, here protocol is UDP, eliminated\nD: Correct answer"
      },
      {
        "date": "2023-05-18T15:48:00.000Z",
        "voteCount": 1,
        "content": "\"Correct Answer\" says A   and community vote says D(100%)  \nwhich one is correct?"
      },
      {
        "date": "2023-03-27T14:18:00.000Z",
        "voteCount": 1,
        "content": "Going with D"
      },
      {
        "date": "2023-02-17T05:01:00.000Z",
        "voteCount": 1,
        "content": "Ans is D"
      },
      {
        "date": "2022-12-01T13:06:00.000Z",
        "voteCount": 2,
        "content": "The question tricked me. I saw UDP and immediately thought it was B.\n\nThe correct answer is D, as the LB needs to be External, and SSL\\HTTPS are not the right load balancers for this application."
      },
      {
        "date": "2022-07-29T11:41:00.000Z",
        "voteCount": 1,
        "content": "External Network LB used for UDP"
      },
      {
        "date": "2022-06-24T04:34:00.000Z",
        "voteCount": 1,
        "content": "D seems correct.."
      },
      {
        "date": "2022-05-24T06:45:00.000Z",
        "voteCount": 1,
        "content": "I'm dead sure, it's D!"
      },
      {
        "date": "2022-03-25T05:59:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/load-balancing/docs/choosing-load-balancer#lb-decision-tree"
      },
      {
        "date": "2022-03-15T12:21:00.000Z",
        "voteCount": 1,
        "content": "D - Check https://cloud.google.com/load-balancing/images/choose-lb.svg"
      },
      {
        "date": "2022-01-24T14:58:00.000Z",
        "voteCount": 2,
        "content": "D, because: \nhttps://cloud.google.com/load-balancing/docs/network#:~:text=Google%20Cloud%20external-,TCP/UDP,-Network%20Load%20Balancing"
      },
      {
        "date": "2022-01-10T20:00:00.000Z",
        "voteCount": 2,
        "content": "Answer is C. Players need to access through internet by HTTP(S) load balancing"
      },
      {
        "date": "2022-06-17T06:02:00.000Z",
        "voteCount": 1,
        "content": "It is not specified whether the app protocol is HTTP(S) or not, only that it is UDP paquets. Internet is not limited to the http protocol.\nAnswer D"
      },
      {
        "date": "2022-01-07T00:49:00.000Z",
        "voteCount": 3,
        "content": "D. Configure an External Network load balancer in front of the application servers.\n\"VM over single (external) IP address -&gt;&gt;&gt; getting UDP packets through External LB\n\""
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 168,
    "url": "https://www.examtopics.com/discussions/google/view/47007-exam-associate-cloud-engineer-topic-1-question-168/",
    "body": "You are working for a hospital that stores its medical images in an on-premises data room. The hospital wants to use Cloud Storage for archival storage of these images. The hospital wants an automated process to upload any new medical images to Cloud Storage. You need to design and implement a solution. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Pub/Sub topic, and enable a Cloud Storage trigger for the Pub/Sub topic. Create an application that sends all medical images to the Pub/Sub topic.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy a Dataflow job from the batch template, \u05d2\u20acDatastore to Cloud Storage.\u05d2\u20ac Schedule the batch job on the desired interval.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a script that uses the gsutil command line interface to synchronize the on-premises storage with Cloud Storage. Schedule the script as a cron job.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the Cloud Console, go to Cloud Storage. Upload the relevant images to the appropriate bucket."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 31,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-03-14T13:08:00.000Z",
        "voteCount": 26,
        "content": "From the question the key point is \"upload ANY NEW medical images to Cloud Storage\". So we are not interested in old images. That's why we need some trigger that will upload images. I think option \"A\" with PubSub is the best"
      },
      {
        "date": "2021-03-15T08:30:00.000Z",
        "voteCount": 1,
        "content": "I am not sure but the question also mentions that \"wants to use Cloud Storage for archival storage of these images\". It can create an application that sends all medical images to storage and no need via PubSub?"
      },
      {
        "date": "2021-03-28T16:03:00.000Z",
        "voteCount": 5,
        "content": "Pub/Sub will be good for all future files in in-prem data-storage.\n\nwe want to sync all + new, so a local on-prem server running a cron job (not GCE CronJob) to run gsutil to transfer files to Cloud Storage would work.\n\nI vote for C"
      },
      {
        "date": "2021-05-03T01:40:00.000Z",
        "voteCount": 6,
        "content": "Sorry you are wrong, the question clearly indicates \"The hospital wants an automated process to upload ANY NEW medical images to Cloud Storage.\" It does not mention the need to upload the original stock of images, only the new ones. Then I think the right answer must be A, as you said \"Pub/sub will be good for all future files in prem data-storage\" which is exactly what the questions is pointing to."
      },
      {
        "date": "2021-07-28T15:02:00.000Z",
        "voteCount": 3,
        "content": "ans is C"
      },
      {
        "date": "2022-09-30T19:20:00.000Z",
        "voteCount": 1,
        "content": "In option C we are using a cron job, not dragging and dropping the images."
      },
      {
        "date": "2021-03-14T13:59:00.000Z",
        "voteCount": 22,
        "content": "C. Create a script that uses the gsutil command line interface to synchronize the on-premises storage with Cloud Storage. Schedule the script as a cron job."
      },
      {
        "date": "2022-08-16T00:54:00.000Z",
        "voteCount": 1,
        "content": "Where does it say that the on-premises images are already digitized? and even if they are, where does it say that we also keep the old images?\nI think the correct answer is \"A\""
      },
      {
        "date": "2022-09-11T17:56:00.000Z",
        "voteCount": 6,
        "content": "Tell yo yourself how the images would end up in the pubsub first of all. Also usually the process is in the other way around for pubsub notifications: Once an object lands in GCS the pubsub is notified of it. \n\nOption A makes totally nonsense. Check the flow again. \n\nFrom the options the only one that \"makes more sense\" is Option C"
      },
      {
        "date": "2024-01-01T18:59:00.000Z",
        "voteCount": 7,
        "content": "A. Pub/Sub Topic with Cloud Storage Trigger: While Cloud Pub/Sub is great for event-driven architectures, it's not directly applicable for file synchronization scenarios. It would also require substantial modification to the existing infrastructure to send images to Pub/Sub.\n\nB. Dataflow Job: Dataflow is a powerful service for stream and batch data processing, but using it solely for file synchronization is overkill. It also requires more setup and maintenance compared to a simple gsutil script.\n\nD. Manual Upload in Cloud Console: This is not feasible for an automated process, as it requires manual intervention and isn\u2019t practical for a large number of files."
      },
      {
        "date": "2023-09-17T22:58:00.000Z",
        "voteCount": 1,
        "content": "I am also for A. Any new data should be send to Cloud Storage. Yes you need to create an application. To send data to pubsub. But for possible migration to cloud you can use the existing setup"
      },
      {
        "date": "2023-09-05T08:38:00.000Z",
        "voteCount": 2,
        "content": "C is the right answer as they want the automated process to upload any new medical image"
      },
      {
        "date": "2023-08-30T08:13:00.000Z",
        "voteCount": 2,
        "content": "C is correct. \nA will not work because pub/sub is meant for service to service communication only: \nhttps://cloud.google.com/pubsub/docs/overview#compare_service-to-service_and_service-to-client_communication\nYes C option will sync any new images from onprem to cloud."
      },
      {
        "date": "2023-07-23T05:00:00.000Z",
        "voteCount": 1,
        "content": "New images can use Pub/Sub"
      },
      {
        "date": "2022-10-09T03:21:00.000Z",
        "voteCount": 8,
        "content": "C is the correct answer.\nKeyword, they require cloud storage for archival and the want to automate the process to upload new medical image to cloud storage, hence we go for gsutil to copy on-prem images to cloud storage and automate the process via cron job. whereas Pub/Sub listens to the changes in the Cloud Storage bucket and triggers the pub/sub topic, which is not required."
      },
      {
        "date": "2023-07-05T11:11:00.000Z",
        "voteCount": 2,
        "content": "I agree. The requirement is for both history images and future images. So I go with Option \"C\"."
      },
      {
        "date": "2022-07-30T04:35:00.000Z",
        "voteCount": 3,
        "content": "The Hospital wants Cloud storage for archival of old images and also sync the new images, for this logic the answer is C"
      },
      {
        "date": "2022-06-24T04:41:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2022-04-29T03:58:00.000Z",
        "voteCount": 1,
        "content": "C is correct."
      },
      {
        "date": "2022-04-24T07:45:00.000Z",
        "voteCount": 4,
        "content": "Discarding (A) because  \"Cloud Storage trigger\". So for option A the triggering event should be making a change in Cloud Storage, while in the real use case, the triggering should be adding a new medical image to the \"on-premises data room\""
      },
      {
        "date": "2022-09-11T17:58:00.000Z",
        "voteCount": 1,
        "content": "Correct, and on top of that, how are they suppose to connect the on premises to the cloud. Nothing is mentioned. But either way, option A does not make sense at all (as you already explained too). Correct option: C"
      },
      {
        "date": "2022-03-25T08:07:00.000Z",
        "voteCount": 3,
        "content": "I would go with C\n\nNot A, don\u2019t think you can send image files to Pub/Sub. Technically you can do so by converting image to some binary text, but then we don\u2019t know the size of the image and there is a limitation on message size. Not recommended.\n\nNot B \u2013 there is only this template \u201cDatastore to Cloud Storage Text\u201d, as the name implies it is for text, https://cloud.google.com/dataflow/docs/guides/templates/provided-batch#datastore-to-cloud-storage-text, and it reads from datastore which is definitely not where the medical images are stored, from the question \u201c\u2026 stores its medical images in an on-premises data room\u201d.\n\nNot D \u2013 it\u2019s not automated"
      },
      {
        "date": "2022-02-11T03:15:00.000Z",
        "voteCount": 1,
        "content": "Pub/Sub could make sense if you ignore the size limit. But the way it is described with the storage trigger would just not be working."
      },
      {
        "date": "2022-01-12T21:07:00.000Z",
        "voteCount": 1,
        "content": "A is correct. The key is automation whenever there is a new image, it needs to upload to cloud storage. Only pub/sub can make the automation work."
      },
      {
        "date": "2022-01-07T00:46:00.000Z",
        "voteCount": 1,
        "content": "I think A(once the hospital receive the new images Cloud pub/sub will act on it) and C(creating a script with rsync command https://stackoverflow.com/questions/37662416/how-to-sync-a-local-folder-with-a-folder-in-a-google-cloud-platform-bucket) must be the Correct options"
      },
      {
        "date": "2021-12-20T02:33:00.000Z",
        "voteCount": 1,
        "content": "\" any new medical images\" so \"A\" using Pub\\Sub"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 169,
    "url": "https://www.examtopics.com/discussions/google/view/47038-exam-associate-cloud-engineer-topic-1-question-169/",
    "body": "Your auditor wants to view your organization's use of data in Google Cloud. The auditor is most interested in auditing who accessed data in Cloud Storage buckets. You need to help the auditor access the data they need. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTurn on Data Access Logs for the buckets they want to audit, and then build a query in the log viewer that filters on Cloud Storage.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAssign the appropriate permissions, and then create a Data Studio report on Admin Activity Audit Logs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAssign the appropriate permissions, and then use Cloud Monitoring to review metrics.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the export logs API to provide the Admin Activity Audit Logs in the format they want."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-03-14T02:52:00.000Z",
        "voteCount": 41,
        "content": "It should be A. \nData access log are not enabled by default due to the fact that it incurs costs. \nSo you need to enable it first. \nAnd then you can filter it in the log viewer"
      },
      {
        "date": "2021-03-14T15:12:00.000Z",
        "voteCount": 11,
        "content": "A. Turn on Data Access Logs for the buckets they want to audit, and then build a query in the log viewer that filters on Cloud Storage."
      },
      {
        "date": "2024-09-24T23:29:00.000Z",
        "voteCount": 1,
        "content": "Once again wrong answer. It should be option A"
      },
      {
        "date": "2024-03-15T13:33:00.000Z",
        "voteCount": 1,
        "content": "A. Turn on Data Access Logs for the buckets they want to audit, and then build a query in the log viewer that filters on Cloud Storage."
      },
      {
        "date": "2023-09-06T02:26:00.000Z",
        "voteCount": 1,
        "content": "IF Data Access Logs had ALREADY been enabled,  then option B would be a good answer\nReason - (1) best practice for cloud auditing - enable Admin Activity audit logs, then set IAM permissions\n(ref: https://cloud.google.com/logging/docs/audit/best-practices)\nand (2) Create a Data Studio (now renamed to Looker) report on Admin Activity Audit Logs\n(ref: https://cloud.google.com/looker/docs/looker-core-audit-logging)\nBut you cannot assume from the question that Data Access Logs are enabled (NB: they are NOT by default)"
      },
      {
        "date": "2023-09-05T08:41:00.000Z",
        "voteCount": 1,
        "content": "A is the right answer as first we need to turn on the data access logs"
      },
      {
        "date": "2022-11-11T04:02:00.000Z",
        "voteCount": 1,
        "content": "I have doubts about the answer A, the auditor wants to see the audit logs, and in this answer it is not explicit if he will be allowed to see it."
      },
      {
        "date": "2022-10-09T03:34:00.000Z",
        "voteCount": 2,
        "content": "A is the correct answer,\nSince the auditor wants to know who accessed the cloud storage data, we need data acces logs for cloud storage.\n\nTypes of audit logs\nCloud Audit Logs provides the following audit logs for each Cloud project, folder, and organization:\n\nAdmin Activity audit logs\nData Access audit logs\nSystem Event audit logs\nPolicy Denied audit logs\n\n***Data Access audit logs contain API calls that read the configuration or metadata of resources, as well as user-driven API calls that create, modify, or read user-provided resource data.\n\nhttps://cloud.google.com/logging/docs/audit#types"
      },
      {
        "date": "2022-06-24T04:43:00.000Z",
        "voteCount": 1,
        "content": "A is right"
      },
      {
        "date": "2022-06-20T22:11:00.000Z",
        "voteCount": 4,
        "content": "question says auditor is most interested in who accessED data in Cloud Storage. im not sure how auditoring is done for those who answered A but this means they want the logs for past users who accessed the data from a sepecified time. Turning on the feature now is kind of too late. poorly written question and answers. No point in an auditor coming in and giving the company all the exact questions they are going to ask and come back and ask them in a few months time. A seems like the better choices though"
      },
      {
        "date": "2022-05-24T06:53:00.000Z",
        "voteCount": 2,
        "content": "If it's A then how will we assign the permission for the auditor to view the logs?\nI had chosen option A on the first place, but later changed it considering that the auditor won't have the access to view the logs."
      },
      {
        "date": "2022-05-03T12:17:00.000Z",
        "voteCount": 1,
        "content": "Based on how I read the question- \nWe want Data Access log, not Admin Activity Audit Logs."
      },
      {
        "date": "2022-03-25T08:09:00.000Z",
        "voteCount": 1,
        "content": "Data access log are not enabled by default due to the fact that it incurs costs.\nSo you need to enable it first.\nAnd then you can filter it in the log viewer"
      },
      {
        "date": "2022-03-15T11:56:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/logging/docs/audit#data-access\n\nCloud Storage: When Cloud Storage usage logs are enabled, Cloud Storage writes usage data to the Cloud Storage bucket, which generates Data Access audit logs for the bucket. The generated Data Access audit log has its caller identity redacted."
      },
      {
        "date": "2022-02-21T18:29:00.000Z",
        "voteCount": 1,
        "content": "The majority vote here is A, despite some confusion around the wording of the question. I tend to agree because it's the solution that most closely reflects the requirements of the question (buckets, cloud storage)."
      },
      {
        "date": "2021-12-09T10:51:00.000Z",
        "voteCount": 3,
        "content": "A. I could not find a way to enable audit logs in specific buckets, only on the whole storage level:\nhttps://cloud.google.com/logging/docs/audit/services\n\nB. Admin activity audit logs cover admin actions, such as metada or config changes:\nhttps://cloud.google.com/logging/docs/audit#admin-activity\n\nC. Cloud monitoring is not for auditing: https://cloud.google.com/monitoring\n\nD. Again, Admin Activity Audit Logs should not be used to audit data access, specially from bukets.\n\nMy conclusion: all these answers are wrong. My assumption: A is badly written. Specific buckets were not to be mentioned. I Vote A, but i think this Q&amp;A is messed up. Maybe a correction? or deletion."
      },
      {
        "date": "2021-12-13T11:09:00.000Z",
        "voteCount": 1,
        "content": "Actually, there is a different service named User Logs that permits to focus on a single bucket.\nRefer to google page:\nhttps://cloud.google.com/storage/docs/access-logs\nUsage logs provide information for all of the requests made on a specified bucket"
      },
      {
        "date": "2022-02-28T00:15:00.000Z",
        "voteCount": 1,
        "content": "The question just says \"buckets\" and hints that the audit should cover all org data, so I don't think there is any need to overanalyse, you are correct in choosing A"
      },
      {
        "date": "2021-12-07T10:23:00.000Z",
        "voteCount": 4,
        "content": "I choose D. reason is here: Cloud Audit Logs generates the following audit logs for operations in Cloud Storage:\n\nAdmin Activity logs: Entries for operations that modify the configuration or metadata of a project, bucket, or object.\n\nData Access logs: Entries for operations that modify objects or read a project, bucket, or object. There are several sub-types of data access logs:\n\nADMIN_READ: Entries for operations that read the configuration or metadata of a project, bucket, or object.\n\nDATA_READ: Entries for operations that read an object.\n\nDATA_WRITE: Entries for operations that create or modify an object."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 170,
    "url": "https://www.examtopics.com/discussions/google/view/46456-exam-associate-cloud-engineer-topic-1-question-170/",
    "body": "You received a JSON file that contained a private key of a Service Account in order to get access to several resources in a Google Cloud project. You downloaded and installed the Cloud SDK and want to use this private key for authentication and authorization when performing gcloud commands. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the command gcloud auth login and point it to the private key.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the command gcloud auth activate-service-account and point it to the private key.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPlace the private key file in the installation directory of the Cloud SDK and rename it to \u05d2\u20accredentials.json\u05d2\u20ac.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPlace the private key file in your home directory and rename it to \u05d2\u20acGOOGLE_APPLICATION_CREDENTIALS\u05d2\u20ac."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 8,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-03-14T16:03:00.000Z",
        "voteCount": 44,
        "content": "B. Use the command gcloud auth activate-service-account and point it to the private key.\n\nAuthorizing with a service account\ngcloud auth activate-service-account authorizes access using a service account. As with gcloud init and gcloud auth login, this command saves the service account credentials to the local system on successful completion and sets the specified account as the active account in your Cloud SDK configuration.\n\nhttps://cloud.google.com/sdk/docs/authorizing#authorizing_with_a_service_account"
      },
      {
        "date": "2021-03-14T13:22:00.000Z",
        "voteCount": 20,
        "content": "B.\ngcloud auth activate-service-account --help\n\nNAME)\n    gcloud auth activate-service-account - authorize access to Google Cloud\n        Platform with a service account\n\nSYNOPSIS\n    gcloud auth activate-service-account [ACCOUNT] --key-file=KEY_FILE\n        [--password-file=PASSWORD_FILE | --prompt-for-password]\n        [GCLOUD_WIDE_FLAG ...]\n\nDESCRIPTION\n    To allow gcloud (and other tools in Cloud SDK) to use service account\n    credentials to make requests, use this command to import these credentials\n    from a file that contains a private authorization key, and activate them\n    for use in gcloud. gcloud auth activate-service-account serves the same\n    function as gcloud auth login but uses a service account rather than Google\n    user credentials."
      },
      {
        "date": "2023-02-18T06:33:00.000Z",
        "voteCount": 1,
        "content": "See below information suggesting that service account can be used to authorize with the command \"gcloud auth login\". Not sure if this is a recent update:\n\n\"The gcloud auth login command authorizes access by using workload identity federation, which provides access to external workloads, or by using a service account key.\"\n\n\"To activate your service account, run gcloud auth login with the --cred-file flag:\n\n\ngcloud auth login --cred-file=CONFIGURATION_OR_KEY_FILE\nReplace CONFIGURATION_OR_KEY_FILE with the path to one of the following:\n\nA credential configuration file for workload identity federation\nA service account key file\"\n\nhttps://cloud.google.com/sdk/docs/authorizing#authorize_with_a_service_account"
      },
      {
        "date": "2023-09-06T10:27:00.000Z",
        "voteCount": 1,
        "content": "As per google - gcloud auth activate-service-account serves the same function as gcloud auth login but uses a service account rather than Google user credentials.\n\nRef: https://cloud.google.com/sdk/gcloud/reference/auth/activate-service-account"
      },
      {
        "date": "2024-10-02T07:42:00.000Z",
        "voteCount": 1,
        "content": "Authorize with a service account\nThe gcloud auth login command can authorize access with a service account by using a credential file stored on your local file system. This credential can be a user credential with permission to impersonate the service account, a credential configuration file for workload identity federation, or a service account key.\n\nhttps://cloud.google.com/sdk/docs/authorizing#auth-login"
      },
      {
        "date": "2024-10-10T02:40:00.000Z",
        "voteCount": 1,
        "content": "Now, I see that while A works, B is a better answer.\n"
      },
      {
        "date": "2024-02-08T09:53:00.000Z",
        "voteCount": 2,
        "content": "The Answer is A. The command to use service account for authentication is precisely gcloud auth activate-service-account where you can point out to the key file using the flag --key-file.\n\nhttps://cloud.google.com/sdk/gcloud/reference/auth/activate-service-account"
      },
      {
        "date": "2024-01-01T19:06:00.000Z",
        "voteCount": 1,
        "content": "\u2022\tThe command syntax is gcloud auth activate-service-account --key-file=PATH_TO_KEY_FILE, where PATH_TO_KEY_FILE is the path to the JSON file containing the service account's private key."
      },
      {
        "date": "2023-09-06T13:33:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/sdk/gcloud/reference/auth/activate-service-account"
      },
      {
        "date": "2023-05-10T22:47:00.000Z",
        "voteCount": 3,
        "content": "D. This method is for application default credentials. See: https://cloud.google.com/docs/authentication/application-default-credentials\nA. This method is to obtain credentials for a user account.\nC. This does nothing. Useless.\nB. Is the correct answer. See: https://cloud.google.com/sdk/gcloud/reference/auth/activate-service-account"
      },
      {
        "date": "2023-04-04T04:17:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/sdk/gcloud/reference/auth/activate-service-account \nB"
      },
      {
        "date": "2022-08-05T18:59:00.000Z",
        "voteCount": 1,
        "content": "Use the command gcloud auth activate-service-account and point it to the private key"
      },
      {
        "date": "2022-07-01T22:09:00.000Z",
        "voteCount": 1,
        "content": "ANswer is B\nTo activate your service account, run gcloud auth activate-service-account:\ngcloud auth activate-service-account [ACCOUNT] --key-file=[KEY_FILE]"
      },
      {
        "date": "2022-06-24T04:45:00.000Z",
        "voteCount": 1,
        "content": "I will go with B"
      },
      {
        "date": "2022-06-13T15:39:00.000Z",
        "voteCount": 1,
        "content": "B is right  Please refer \nhttps://cloud.google.com/storage/docs/authentication"
      },
      {
        "date": "2022-03-21T01:13:00.000Z",
        "voteCount": 1,
        "content": "Ans: B"
      },
      {
        "date": "2021-09-25T05:13:00.000Z",
        "voteCount": 5,
        "content": "B, really straightforward"
      },
      {
        "date": "2021-03-11T09:35:00.000Z",
        "voteCount": 11,
        "content": "Ans : B\ngcloud auth activate-service-account --key-file=/test-service-account.json"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 171,
    "url": "https://www.examtopics.com/discussions/google/view/46865-exam-associate-cloud-engineer-topic-1-question-171/",
    "body": "You are working with a Cloud SQL MySQL database at your company. You need to retain a month-end copy of the database for three years for audit purposes.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up an export job for the first of the month. Write the export file to an Archive class Cloud Storage bucket.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSave the automatic first-of-the-month backup for three years. Store the backup file in an Archive class Cloud Storage bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up an on-demand backup for the first of the month. Write the backup to an Archive class Cloud Storage bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConvert the automatic first-of-the-month backup to an export file. Write the export file to a Coldline class Cloud Storage bucket."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-03-14T13:56:00.000Z",
        "voteCount": 53,
        "content": "https://cloud.google.com/sql/docs/mysql/backup-recovery/backups\nnot B: Automatic backups are made EVERY SINGLE DAY. You can set only the number of backups up to 365. Also you cannot choose your Archival storage as destination\nnot C: You cannot  setup \"on-demand\" backup. User would have to make backups manually every month. Also you cannot choose your Archival storage as destination\nnot D: You cannot conver backup to export file. Also Coldline class is less cost-effective than Archival class.\n\nThe only option left is \"A\" \nYou can set up your job with any date/time schedule. You can export file to any storage with any storage class."
      },
      {
        "date": "2021-06-13T21:00:00.000Z",
        "voteCount": 6,
        "content": "from the same link : \nCan I export a backup?\nNo, you can't export a backup. You can only export instance data. See Exporting data from Cloud SQL to a dump in Cloud storage."
      },
      {
        "date": "2021-06-26T22:03:00.000Z",
        "voteCount": 16,
        "content": "First need to understand backup vs export, two different concepts. - https://cloud.google.com/sql/docs/mysql/backup-recovery/backups\n\nA \u2013 yes, you can export data from Cloud SQL to Cloud Storage- https://cloud.google.com/sql/docs/mysql/import-export/exporting#cloud-sql\n\nNot B, C, D \u2013 be it automatic or on-demand backup, according to the doc \u201cNo, you can't export a backup. You can only export instance data.\u201d"
      },
      {
        "date": "2024-04-26T22:06:00.000Z",
        "voteCount": 1,
        "content": "Should be A:\nBackups are managed by Cloud SQL according to retention policies, and are stored separately from the Cloud SQL instance. Cloud SQL backups differ from an export uploaded to Cloud Storage, where you manage the lifecycle. Backups encompass the entire database. Exports can select specific contents.\n\nhttps://cloud.google.com/sql/docs/mysql/backup-recovery/backups#:~:text=for%20more%20information.-,Backups%20versus%20exports,-Backups%20are%20managed"
      },
      {
        "date": "2023-09-15T21:51:00.000Z",
        "voteCount": 1,
        "content": "https://www.exam-answer.com/retain-month-end-copy-cloud-sql-mysql-database-three-years   B is the correct answer.. I am not sure why people are posting wrong answers here."
      },
      {
        "date": "2023-10-25T01:15:00.000Z",
        "voteCount": 1,
        "content": "cause you're the one wrong:\nhttps://cloud.google.com/sql/docs/mysql/backup-recovery/backups#can_i_export_a_backup"
      },
      {
        "date": "2023-09-05T19:26:00.000Z",
        "voteCount": 1,
        "content": "A is the correct answer, as in a you can export as per your requirement and then moved it to archive class , but in b,c,d you can't do that"
      },
      {
        "date": "2023-05-13T00:55:00.000Z",
        "voteCount": 1,
        "content": "Option B is the best choice because it allows you to leverage the automatic first-of-the-month backup feature that is provided by Cloud SQL. Cloud SQL provides automated backups that can be configured to run at specific times, including the first of the month. By retaining the first-of-the-month backup for three years, you can be sure that you have a complete copy of the database for that month."
      },
      {
        "date": "2023-03-03T22:51:00.000Z",
        "voteCount": 1,
        "content": "Answer A"
      },
      {
        "date": "2023-01-21T10:29:00.000Z",
        "voteCount": 1,
        "content": "Answer A is the correct one according to \"https://cloud.google.com/blog/topics/developers-practitioners/scheduling-cloud-sql-exports-using-cloud-functions-and-cloud-scheduler\" and \"https://cloud.google.com/sql/docs/mysql/backup-recovery/backups#backups_versus_exports\"."
      },
      {
        "date": "2022-11-13T13:55:00.000Z",
        "voteCount": 1,
        "content": "So Answer is A.\nYou can't export back up. Very clear.\nhttps://cloud.google.com/sql/docs/mysql/backup-recovery/backups#can_i_export_a_backup"
      },
      {
        "date": "2022-10-09T03:56:00.000Z",
        "voteCount": 2,
        "content": "A is correct answer,\nExport the SQL month end data as a CSV file to cloud storage bucket, and move the data to Archival Storages for 3 years for audit purpose.\n\nhttps://cloud.google.com/sql/docs/mysql/import-export?authuser=1"
      },
      {
        "date": "2022-09-29T06:32:00.000Z",
        "voteCount": 1,
        "content": "A. Although Cloud SQL doesn't provide a built-in way to automate database exports, you can build your own automation tool using several Google Cloud components.\nhttps://cloud.google.com/sql/docs/mysql/import-export#automating_export_operations"
      },
      {
        "date": "2022-09-24T10:33:00.000Z",
        "voteCount": 4,
        "content": "had this question today"
      },
      {
        "date": "2022-09-22T05:56:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2022-09-03T06:32:00.000Z",
        "voteCount": 2,
        "content": "A seems right to me key word: \"month-end copy\""
      },
      {
        "date": "2022-07-12T16:17:00.000Z",
        "voteCount": 4,
        "content": "Correct Ans - C \nOn demand backup\n\nYou can create a backup at any time (Here we need backup a month-end copy of the database for three years). You can create on-demand backups for any instance, whether the instance has automatic backups enabled or not.\n\nReason : \n1)  you can't export a backup. You can only export instance data so export option A is out from answer.\nBackups encompass the entire database. Exports can select specific contents.\nAs per question You need to retain a month-end copy of the database not specific contents.\n\n2) Automated backups are taken daily, within a 4-hour backup window. Up to seven most recent backups are retained, by default.\n*Cost** to store all backups .. \n\n3) Option D not applicable **Coldline class** *Cost*"
      },
      {
        "date": "2022-02-07T13:54:00.000Z",
        "voteCount": 2,
        "content": "why not D? it is the only one that doesnt store it as Archive class, and since it is for Audit purposes this cant be used\n as Archive allows LESS than one access per year."
      },
      {
        "date": "2021-12-07T10:06:00.000Z",
        "voteCount": 4,
        "content": "I would go with export not backup. Question stated that this is copy. In addition restore of three years old backup might be not possible in newer version. In addition you can retain max 365 backups."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 172,
    "url": "https://www.examtopics.com/discussions/google/view/46607-exam-associate-cloud-engineer-topic-1-question-172/",
    "body": "You are monitoring an application and receive user feedback that a specific error is spiking. You notice that the error is caused by a Service Account having insufficient permissions. You are able to solve the problem but want to be notified if the problem recurs. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the Log Viewer, filter the logs on severity 'Error' and the name of the Service Account.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a sink to BigQuery to export all the logs. Create a Data Studio dashboard on the exported logs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a custom log-based metric for the specific error to be used in an Alerting Policy.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGrant Project Owner access to the Service Account."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 15,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-03-14T16:12:00.000Z",
        "voteCount": 25,
        "content": "C. Create a custom log-based metrics for the specific error to be used in an Alerting Policy."
      },
      {
        "date": "2021-03-11T19:32:00.000Z",
        "voteCount": 11,
        "content": "C seems to be the right answer."
      },
      {
        "date": "2024-03-05T08:06:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/logging/docs/logs-based-metrics"
      },
      {
        "date": "2023-09-05T19:28:00.000Z",
        "voteCount": 4,
        "content": "User wants to check the if problem recurs, that can be only possible by Alert, C is the correct option"
      },
      {
        "date": "2023-04-18T10:17:00.000Z",
        "voteCount": 2,
        "content": "C. The keyword here is \"want to be notified\" that means an alert."
      },
      {
        "date": "2022-10-09T04:01:00.000Z",
        "voteCount": 2,
        "content": "C is the correct answer,\nSince the problem is resolved, We need to monitor if the error recurs, hence we create a custom log based metrics to monitor only the particular service account."
      },
      {
        "date": "2022-09-03T06:34:00.000Z",
        "voteCount": 1,
        "content": "C as Keyword \"want to be notified if the problem recurs\""
      },
      {
        "date": "2022-06-24T04:51:00.000Z",
        "voteCount": 1,
        "content": "C right"
      },
      {
        "date": "2022-06-13T15:02:00.000Z",
        "voteCount": 1,
        "content": "C is correct."
      },
      {
        "date": "2022-04-26T08:15:00.000Z",
        "voteCount": 3,
        "content": "C - the only answer that outputs a notification"
      },
      {
        "date": "2022-01-24T14:11:00.000Z",
        "voteCount": 3,
        "content": "\"C\" is right, the only answer that includes setting up an alert!"
      },
      {
        "date": "2022-01-20T03:06:00.000Z",
        "voteCount": 1,
        "content": "Answer should be C"
      },
      {
        "date": "2021-11-19T15:22:00.000Z",
        "voteCount": 2,
        "content": "Ans: C"
      },
      {
        "date": "2021-06-18T06:17:00.000Z",
        "voteCount": 4,
        "content": "You want to be aerted next time, so only option C meets that criteria."
      },
      {
        "date": "2021-05-31T03:22:00.000Z",
        "voteCount": 4,
        "content": "C is correct : You are able to solve the problem but want to be notified if the problem recurs."
      },
      {
        "date": "2021-05-20T10:23:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2021-04-22T15:47:00.000Z",
        "voteCount": 2,
        "content": "You are managing a project for the Business Intelligence (BI) department in your company. A data pipeline ingests data into BigQuery via streaming. You want the users in the BI department to be able to run the custom SQL queries against the latest data in BigQuery. What should you do?\n\nA. Create a Data Studio dashboard that uses the related BigQuery tables as a source and give the BI team view access to the Data Studio dashboard.\nB. Create a Service Account for the BI team and distribute a new private key to each member of the BI team.\nC. Use Cloud Scheduler to schedule a batch Dataflow job to copy the data from BigQuery to the BI team\u2019s internal data warehouse.\nD. Assign the IAM role of BigQuery User to a Google Group that contains the members of the BI team.\n\nit's A"
      },
      {
        "date": "2021-04-25T12:01:00.000Z",
        "voteCount": 2,
        "content": "I think it's D. Can anyone confirm?"
      },
      {
        "date": "2021-04-22T15:48:00.000Z",
        "voteCount": 1,
        "content": "Question 178"
      },
      {
        "date": "2021-04-28T13:37:00.000Z",
        "voteCount": 2,
        "content": "A.  \"Every time the dashboard is refreshed, it pulls new data from the view, which in turn dynamically reflects the latest data in BigQuery\". Data Science on the Google Cloud Platform: Implementing End-to-End Real-Time (C)"
      },
      {
        "date": "2021-05-09T18:02:00.000Z",
        "voteCount": 1,
        "content": "The question is saying \"want to be notified if the problem recurs\", I don't see how A meets that requirement."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 173,
    "url": "https://www.examtopics.com/discussions/google/view/51203-exam-associate-cloud-engineer-topic-1-question-173/",
    "body": "You are developing a financial trading application that will be used globally. Data is stored and queried using a relational structure, and clients from all over the world should get the exact identical state of the data. The application will be deployed in multiple regions to provide the lowest latency to end users. You need to select a storage option for the application data while minimizing latency. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Bigtable for data storage.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud SQL for data storage.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Spanner for data storage.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Firestore for data storage."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 21,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-06-26T22:10:00.000Z",
        "voteCount": 17,
        "content": "C, Cloud Spanner, keywords are globally, relational structure and lastly \"clients from all over the world should get the exact identical state of the data\" which implies strong consistency is needed."
      },
      {
        "date": "2023-09-05T19:30:00.000Z",
        "voteCount": 4,
        "content": "question demands, exact state of data and minimum latency to users , for this cloud spanner is the only option"
      },
      {
        "date": "2023-04-18T03:48:00.000Z",
        "voteCount": 2,
        "content": "financial trading application\nrelational structure \nmultiple regions"
      },
      {
        "date": "2023-03-10T04:27:00.000Z",
        "voteCount": 4,
        "content": "C, always you need to select BBDD check for data analysis bigquery, something very big or fast bigtable, something with HA cloud sql, and something globally available cloud spanner, the key here is globaly available"
      },
      {
        "date": "2022-10-09T04:09:00.000Z",
        "voteCount": 3,
        "content": "C is the correct answer,\nKeywords, Financial data (large data) used globally, data stored and queried using relational structure (SQL), clients should get exact identical copies(Strong Consistency), Multiple region, low latency to end user, select storage option to minimize latency."
      },
      {
        "date": "2022-10-09T04:12:00.000Z",
        "voteCount": 1,
        "content": "Spanner powers business-critical applications in retail, financial services, gaming, media and entertainment, technology, healthcare and more.\n\nUse cases for Cloud Spanner\nhttps://www.youtube.com/watch?v=1b4flZwAQfM&amp;t=1s"
      },
      {
        "date": "2022-09-29T01:25:00.000Z",
        "voteCount": 4,
        "content": "it's C 100%\nGuys come on, it's a pretty straight forward scenario.\nif you have the keywords \"relational DB\" and the word \"Globally\" in a sentence always go for Cloud Spanner."
      },
      {
        "date": "2022-09-28T10:11:00.000Z",
        "voteCount": 1,
        "content": "C. is the answer"
      },
      {
        "date": "2022-09-26T06:47:00.000Z",
        "voteCount": 1,
        "content": "Why not A. Big table as per keywords relational, global and low latency"
      },
      {
        "date": "2023-01-13T16:58:00.000Z",
        "voteCount": 2,
        "content": "BigTable is not a relational database. Everything else is true for it but it a noSQL non Relational Database."
      },
      {
        "date": "2022-09-14T07:57:00.000Z",
        "voteCount": 1,
        "content": "C is the answer.\n\nCloud Spanner is a global relational database."
      },
      {
        "date": "2022-09-04T06:11:00.000Z",
        "voteCount": 1,
        "content": "Should be the correct answer."
      },
      {
        "date": "2022-08-30T23:46:00.000Z",
        "voteCount": 1,
        "content": "C, Cloud Spanner.  Globally and trading (tend to receive 1000s Records per second) are key here."
      },
      {
        "date": "2022-08-24T09:03:00.000Z",
        "voteCount": 1,
        "content": "Why not CLoud Sql with replicas in multiple region to serve \"global\" ?"
      },
      {
        "date": "2022-06-13T14:59:00.000Z",
        "voteCount": 1,
        "content": "This is straight forward question, Answer is C."
      },
      {
        "date": "2022-04-26T08:13:00.000Z",
        "voteCount": 1,
        "content": "C is correct - Relational and Global"
      },
      {
        "date": "2021-11-19T15:15:00.000Z",
        "voteCount": 2,
        "content": "Choose - C"
      },
      {
        "date": "2021-07-13T09:32:00.000Z",
        "voteCount": 3,
        "content": "Yes. C is right answer"
      },
      {
        "date": "2021-05-02T03:04:00.000Z",
        "voteCount": 4,
        "content": "C) Spanner - Global, low latency, relational"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 174,
    "url": "https://www.examtopics.com/discussions/google/view/51202-exam-associate-cloud-engineer-topic-1-question-174/",
    "body": "You are about to deploy a new Enterprise Resource Planning (ERP) system on Google Cloud. The application holds the full database in-memory for fast data access, and you need to configure the most appropriate resources on Google Cloud for this application. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tProvision preemptible Compute Engine instances.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tProvision Compute Engine instances with GPUs attached.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tProvision Compute Engine instances with local SSDs attached.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tProvision Compute Engine instances with M1 machine type.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 12,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-05-01T08:07:00.000Z",
        "voteCount": 31,
        "content": "Yes D, M1 Machine types for ERP i.e. SAP-HANA:\nhttps://cloud.google.com/compute/docs/machine-types"
      },
      {
        "date": "2024-10-10T02:54:00.000Z",
        "voteCount": 1,
        "content": "D"
      },
      {
        "date": "2023-09-05T19:39:00.000Z",
        "voteCount": 3,
        "content": "D is the right answer as M1 one are best for the databases i.e SAP hana"
      },
      {
        "date": "2023-02-14T14:07:00.000Z",
        "voteCount": 4,
        "content": "C. Provision Compute Engine instances with local SSDs attached.\n\nThe best option for an ERP system that holds the full database in-memory for fast data access is to provision Compute Engine instances with local SSDs attached. Local SSDs offer high input/output operations per second (IOPS) and low latency, which can significantly improve the performance of in-memory databases. Preemptible Compute Engine instances are designed for short-lived and fault-tolerant workloads and are not recommended for a critical system like an ERP. GPUs are typically used for specialized compute-intensive workloads like machine learning and deep learning. M1 machine type is a general-purpose machine type and may not provide enough performance for an in-memory database."
      },
      {
        "date": "2022-10-09T04:20:00.000Z",
        "voteCount": 3,
        "content": "D is the correct answer,\nM1 machine series\t\nMedium in-memory databases such as SAP HANA\nTasks that require intensive use of memory with higher memory-to-vCPU ratios than the general-purpose high-memory machine types.\nIn-memory databases and in-memory analytics, business warehousing (BW) workloads, genomics analysis, SQL analysis services.\nMicrosoft SQL Server and similar databases."
      },
      {
        "date": "2022-09-03T06:36:00.000Z",
        "voteCount": 1,
        "content": "D, keyword \"Full database in-memory \""
      },
      {
        "date": "2022-08-05T23:59:00.000Z",
        "voteCount": 2,
        "content": "Vote for D as the right answer. M1 machine type is the one of two Memory-Optimized machine types in GCP.\n\nhttps://cloud.google.com/compute/docs/machine-types"
      },
      {
        "date": "2022-08-06T00:01:00.000Z",
        "voteCount": 1,
        "content": "Read this also to see the difference of the two.\n\nhttps://cloud.google.com/compute/docs/memory-optimized-machines"
      },
      {
        "date": "2022-07-29T13:29:00.000Z",
        "voteCount": 2,
        "content": "D:  M1 Machine types for ERP i.e. SAP-HANA\n\nMedium-large in-memory databases such as SAP HANA\nIn-memory databases and in-memory analytics\nMicrosoft SQL Server and similar databases"
      },
      {
        "date": "2022-06-13T14:53:00.000Z",
        "voteCount": 1,
        "content": "D is right choice, when answer selected by author is C doesn't make any sense. User also need to understand the services well before attempting exam."
      },
      {
        "date": "2022-05-24T07:11:00.000Z",
        "voteCount": 1,
        "content": "I chose option D, because the first three didn't make any sense :D"
      },
      {
        "date": "2022-01-24T14:08:00.000Z",
        "voteCount": 3,
        "content": "D!!! \nThe \"M1\" VM type is right, it offers between 1.4TB and 3.75TB of RAM."
      },
      {
        "date": "2021-12-23T10:24:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/compute/docs/machine-types#:~:text=databases%20such%20as-,SAP%20HANA,-In%2Dmemory%20databases\n\nhttps://www.sap.com/india/products/hana.html#:~:text=is%20SAP%20HANA-,in%2Dmemory,-database%3F"
      },
      {
        "date": "2021-12-13T08:01:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer is D"
      },
      {
        "date": "2021-11-19T15:15:00.000Z",
        "voteCount": 1,
        "content": "D is the Answer"
      },
      {
        "date": "2021-11-08T21:54:00.000Z",
        "voteCount": 1,
        "content": "\"m1-megamem-96\" can attach local SSD. Correct is D."
      },
      {
        "date": "2021-07-25T09:08:00.000Z",
        "voteCount": 2,
        "content": "Note that VM instances m1-megamem-96 are both from the M1 family AND can have local SSDs attached to them.\nhttps://cloud.google.com/compute/docs/memory-optimized-machines#m1_vms"
      },
      {
        "date": "2021-07-13T09:38:00.000Z",
        "voteCount": 4,
        "content": "Answer is D.\nApplications of Memory optimized VMs are,\n1. Medium-large in-memory databases such as SAP HANA\n2. In-memory databases and in-memory analytics\n3. Microsoft SQL Server and similar databases"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 175,
    "url": "https://www.examtopics.com/discussions/google/view/51200-exam-associate-cloud-engineer-topic-1-question-175/",
    "body": "You have developed an application that consists of multiple microservices, with each microservice packaged in its own Docker container image. You want to deploy the entire application on Google Kubernetes Engine so that each microservice can be scaled individually. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate and deploy a Custom Resource Definition per microservice.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate and deploy a Docker Compose File.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate and deploy a Job per microservice.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate and deploy a Deployment per microservice.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-02-28T23:35:00.000Z",
        "voteCount": 25,
        "content": "I was a little unsure about this question, here's how I understand why D is the best answer\n\nA. Custom Resource Definition... we have docker containers already, which is an established kind of resource for Kubernetes.  We don't need to create a whole new type of resource, so this is wrong.\nB. Docker Compose is a wholly different tool from Kubernetes.   \nC. A Kubernetes job describes a specific \"task\" which involves a bunch of pods and things.  It makes no sense to have one job per microservice, a \"Job\" would be a bunch of different microservices executing together.  \nD. is the leftover, correct answer.  You can add scaling to each Deployment, an important aspect of the question."
      },
      {
        "date": "2022-05-24T07:15:00.000Z",
        "voteCount": 2,
        "content": "Thanks for your insights! Makes sense."
      },
      {
        "date": "2021-05-01T06:20:00.000Z",
        "voteCount": 20,
        "content": "D is the correct answer"
      },
      {
        "date": "2023-10-12T22:22:00.000Z",
        "voteCount": 4,
        "content": "To deploy a microservices-based application on Google Kubernetes Engine (GKE), it's common to create and deploy a Deployment per microservice.\n\nD. Create and deploy a Deployment per microservice.\n\nHere's why:\n\nDeployment: In Kubernetes, a Deployment is a resource that allows you to define, create, and manage the desired number of replicas of your application. Each microservice can be independently managed and scaled using its own Deployment.\nThis approach provides the flexibility to scale individual microservices as needed and manage their lifecycle effectively. Each microservice will have its own set of pods that can be scaled up or down independently, making it suitable for a microservices architecture."
      },
      {
        "date": "2023-09-09T20:19:00.000Z",
        "voteCount": 1,
        "content": "D is the corrrect answer"
      },
      {
        "date": "2023-09-06T21:14:00.000Z",
        "voteCount": 1,
        "content": "D is the correct answer."
      },
      {
        "date": "2023-09-05T19:42:00.000Z",
        "voteCount": 2,
        "content": "D seems more correct , as in A , we already have the own docker container image , so no need to create , \nb is completely diffenret tool,\nc is also of no use"
      },
      {
        "date": "2022-09-03T06:37:00.000Z",
        "voteCount": 1,
        "content": "D, keyword \"each microservice can be scaled individually\"!"
      },
      {
        "date": "2022-07-29T13:28:00.000Z",
        "voteCount": 1,
        "content": "D is the correct answer"
      },
      {
        "date": "2022-06-13T14:49:00.000Z",
        "voteCount": 1,
        "content": "D is the best answer among other choices."
      },
      {
        "date": "2022-01-24T14:04:00.000Z",
        "voteCount": 2,
        "content": "D is right!\nIt's one of Googles main ideas to distribute a complex system into microservices. They do it as well and encourage customers to do the same."
      },
      {
        "date": "2021-11-19T15:16:00.000Z",
        "voteCount": 2,
        "content": "D is the Answer"
      },
      {
        "date": "2021-11-16T05:21:00.000Z",
        "voteCount": 2,
        "content": "Ans: D"
      },
      {
        "date": "2021-07-13T09:45:00.000Z",
        "voteCount": 5,
        "content": "Yes. D is correct. Can deploy each service through\nkubectl apply -f &lt;deployment_config.yaml&gt;"
      },
      {
        "date": "2021-07-12T09:54:00.000Z",
        "voteCount": 1,
        "content": "D is the correct answer"
      },
      {
        "date": "2021-06-16T09:05:00.000Z",
        "voteCount": 2,
        "content": "D is correct"
      },
      {
        "date": "2021-04-30T12:26:00.000Z",
        "voteCount": 6,
        "content": "D is the answer"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 176,
    "url": "https://www.examtopics.com/discussions/google/view/51259-exam-associate-cloud-engineer-topic-1-question-176/",
    "body": "You will have several applications running on different Compute Engine instances in the same project. You want to specify at a more granular level the service account each instance uses when calling Google Cloud APIs. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWhen creating the instances, specify a Service Account for each instance.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWhen creating the instances, assign the name of each Service Account as instance metadata.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAfter starting the instances, use gcloud compute instances update to specify a Service Account for each instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAfter starting the instances, use gcloud compute instances update to assign the name of the relevant Service Account as instance metadata."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 15,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-05-03T19:42:00.000Z",
        "voteCount": 24,
        "content": "A            ."
      },
      {
        "date": "2021-06-26T22:17:00.000Z",
        "voteCount": 20,
        "content": "A, when you create an instance using the gcloud command-line tool or the Google Cloud Console, you can specify which service account the instance uses when calling Google Cloud APIs - https://cloud.google.com/compute/docs/access/service-accounts#associating_a_service_account_to_an_instance"
      },
      {
        "date": "2023-09-05T19:50:00.000Z",
        "voteCount": 2,
        "content": "Option A is correct , when you create the instance , that time itself you can specify the service account of each instance"
      },
      {
        "date": "2023-02-14T15:06:00.000Z",
        "voteCount": 4,
        "content": "A. When creating the instances, specify a Service Account for each instance.\n\nTo specify a more granular level of service account for each Compute Engine instance, you should specify a Service Account for each instance when you create it. This can be done through the Compute Engine API or the Cloud Console. By doing so, the specified Service Account will be used when calling Google Cloud APIs from that instance.\n\nOption B, assigning the name of each Service Account as instance metadata, is not the best solution as metadata can be accessed by anyone with access to the instance, which could potentially lead to security issues.\n\nOptions C and D, using gcloud compute instances update to specify a Service Account or assign the name of a Service Account as instance metadata after starting the instances, can also be done, but it is a less efficient approach as it requires additional steps and can lead to human error if not properly documented."
      },
      {
        "date": "2023-03-09T02:29:00.000Z",
        "voteCount": 3,
        "content": "used chatgpt"
      },
      {
        "date": "2022-08-08T01:56:00.000Z",
        "voteCount": 3,
        "content": "Vote for A, because there is no instance running yet. \"You will have several applications running...\""
      },
      {
        "date": "2022-07-04T02:01:00.000Z",
        "voteCount": 1,
        "content": "A, there is no instance running yet"
      },
      {
        "date": "2022-06-13T14:46:00.000Z",
        "voteCount": 1,
        "content": "A is good option for given scenario."
      },
      {
        "date": "2022-03-26T06:00:00.000Z",
        "voteCount": 3,
        "content": "You can set/update the service account only when the instance is not running"
      },
      {
        "date": "2022-02-23T11:29:00.000Z",
        "voteCount": 2,
        "content": "A - the instances are not running yet"
      },
      {
        "date": "2022-01-24T14:00:00.000Z",
        "voteCount": 1,
        "content": "A: you can define which GCP service account is associated with a Compute Engine instance when creating one. It is still possible to change the service account later.\nLink to the GCP docs: https://cloud.google.com/compute/docs/access/create-enable-service-accounts-for-instances#using"
      },
      {
        "date": "2021-10-22T06:46:00.000Z",
        "voteCount": 3,
        "content": "A is correct. You can change the assigned service account, use gcloud compute instances set-service-account ...., not the update"
      },
      {
        "date": "2021-09-16T11:19:00.000Z",
        "voteCount": 2,
        "content": "Answer is C. The instances are already running. So you need to change the Service account"
      },
      {
        "date": "2021-09-16T11:21:00.000Z",
        "voteCount": 8,
        "content": "My bad - \"you will have\" . Correct asnwer - A"
      },
      {
        "date": "2023-06-16T02:57:00.000Z",
        "voteCount": 1,
        "content": "Even if they would be running, I don't think it's possible to change the service account with the \"update\" command. You need to use \"set-service-account\" appropriately: https://cloud.google.com/sdk/gcloud/reference/compute/instances/set-service-account"
      },
      {
        "date": "2021-06-15T01:42:00.000Z",
        "voteCount": 4,
        "content": "A should be correct"
      },
      {
        "date": "2021-04-30T12:25:00.000Z",
        "voteCount": 4,
        "content": "A is the answer"
      },
      {
        "date": "2021-04-30T07:21:00.000Z",
        "voteCount": 3,
        "content": "It should be A"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 177,
    "url": "https://www.examtopics.com/discussions/google/view/51204-exam-associate-cloud-engineer-topic-1-question-177/",
    "body": "You are creating an application that will run on Google Kubernetes Engine. You have identified MongoDB as the most suitable database system for your application and want to deploy a managed MongoDB environment that provides a support SLA. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Cloud Bigtable cluster, and use the HBase API.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy MongoDB Atlas from the Google Cloud Marketplace.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDownload a MongoDB installation package, and run it on Compute Engine instances.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDownload a MongoDB installation package, and run it on a Managed Instance Group."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 12,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-05-20T10:24:00.000Z",
        "voteCount": 17,
        "content": "Simple it's B"
      },
      {
        "date": "2021-05-09T18:26:00.000Z",
        "voteCount": 14,
        "content": "MongoDB Atlas is actually managed and supported by third-party service providers.\n\nhttps://console.cloud.google.com/marketplace/details/gc-launcher-for-mongodb-atlas/mongodb-atlas"
      },
      {
        "date": "2021-05-11T02:23:00.000Z",
        "voteCount": 7,
        "content": "I think that's it. The answer is B"
      },
      {
        "date": "2023-09-26T21:09:00.000Z",
        "voteCount": 4,
        "content": "The keyword is managed MongoDB environment, both C and D are managed by users, A is irrelevant, So B"
      },
      {
        "date": "2023-09-05T19:51:00.000Z",
        "voteCount": 2,
        "content": "B seems more correct , just deploy it from the Market place."
      },
      {
        "date": "2023-06-19T13:59:00.000Z",
        "voteCount": 1,
        "content": "Answer is B"
      },
      {
        "date": "2022-10-24T04:43:00.000Z",
        "voteCount": 1,
        "content": "the best answer is B"
      },
      {
        "date": "2022-10-24T01:27:00.000Z",
        "voteCount": 1,
        "content": "b. fast and simple"
      },
      {
        "date": "2022-10-23T14:16:00.000Z",
        "voteCount": 1,
        "content": "Answer is B"
      },
      {
        "date": "2022-09-29T06:49:00.000Z",
        "voteCount": 1,
        "content": "B. is the answer"
      },
      {
        "date": "2022-09-03T06:38:00.000Z",
        "voteCount": 1,
        "content": "B, keyword \" support SLA\""
      },
      {
        "date": "2022-06-13T14:43:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/mongodb"
      },
      {
        "date": "2022-06-24T07:28:00.000Z",
        "voteCount": 1,
        "content": "B is right"
      },
      {
        "date": "2022-06-13T14:42:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2022-04-28T13:22:00.000Z",
        "voteCount": 6,
        "content": "Anytime the question mentions a third party software, always use the cloud marketplace. Answer is B."
      },
      {
        "date": "2022-01-16T22:55:00.000Z",
        "voteCount": 1,
        "content": "the question says \"want to deploy a managed MongoDB environment\" which means it should be managed by something, i.e compute engine or MIG.\n\nNear Ans is C or D\nI choose C - becoz no need of Mongo db running on MIG, GKE can easily handle mongoDb on compute engine."
      },
      {
        "date": "2021-12-22T05:40:00.000Z",
        "voteCount": 1,
        "content": "Deploy MongoDB Atlas its free Tier - does free tier provides a Support SLA?"
      },
      {
        "date": "2021-12-14T02:49:00.000Z",
        "voteCount": 2,
        "content": "B is the right option to use managed services."
      },
      {
        "date": "2021-11-19T15:10:00.000Z",
        "voteCount": 1,
        "content": "I Pick B"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 178,
    "url": "https://www.examtopics.com/discussions/google/view/51261-exam-associate-cloud-engineer-topic-1-question-178/",
    "body": "You are managing a project for the Business Intelligence (BI) department in your company. A data pipeline ingests data into BigQuery via streaming. You want the users in the BI department to be able to run the custom SQL queries against the latest data in BigQuery. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Data Studio dashboard that uses the related BigQuery tables as a source and give the BI team view access to the Data Studio dashboard.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Service Account for the BI team and distribute a new private key to each member of the BI team.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Scheduler to schedule a batch Dataflow job to copy the data from BigQuery to the BI team's internal data warehouse.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAssign the IAM role of BigQuery User to a Google Group that contains the members of the BI team.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 13,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-05-08T09:21:00.000Z",
        "voteCount": 31,
        "content": "D is correct\nroles/bigquery.user\nWhen applied to a dataset, this role provides the ability to read the dataset's metadata and list tables in the dataset.\nWhen applied to a project, this role also provides the ability to run jobs, including queries, within the project. A member with this role can enumerate their own jobs, cancel their own jobs, and enumerate datasets within a project. Additionally, allows the creation of new datasets within the project; the creator is granted the BigQuery Data Owner role (roles/bigquery.dataOwner) on these new datasets."
      },
      {
        "date": "2021-08-12T19:20:00.000Z",
        "voteCount": 9,
        "content": "Why on the earth would the answer be C? It has no relevance to the question. The answer is D, hands down"
      },
      {
        "date": "2023-09-05T19:54:00.000Z",
        "voteCount": 3,
        "content": "D is the right answer,  just assign them the role by IAM and they will be able to use BQ"
      },
      {
        "date": "2022-12-06T22:24:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2022-11-02T06:16:00.000Z",
        "voteCount": 1,
        "content": "makes mor sense"
      },
      {
        "date": "2022-09-16T02:05:00.000Z",
        "voteCount": 2,
        "content": "D is correct because google recommendations are always to privilege groups to individual accounts and this is what can make the users query the database unlike the Data Studio"
      },
      {
        "date": "2022-09-03T06:40:00.000Z",
        "voteCount": 1,
        "content": "D is right"
      },
      {
        "date": "2022-09-03T06:39:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2022-07-12T16:28:00.000Z",
        "voteCount": 2,
        "content": "D is the answer\n\nHint - to **run the custom SQL queries*** against the latest data in BigQuery"
      },
      {
        "date": "2022-07-06T10:43:00.000Z",
        "voteCount": 1,
        "content": "C is correct, data pipeline is the key:\n\nhttps://cloud.google.com/dataflow/docs/guides/data-pipelines#create_a_batch_data_pipeline\n\nTo create this sample batch data pipeline, you must have access to the following resources in your project:\n\nA Cloud Storage bucket to store input and output files\nA BigQuery dataset where you will create a table."
      },
      {
        "date": "2022-06-17T07:05:00.000Z",
        "voteCount": 2,
        "content": "Answer: D\nThe simplest.\nIt is not requested to automate the query. The BI team may also need to modify their query or have several different ones to meet the needs."
      },
      {
        "date": "2022-06-13T14:40:00.000Z",
        "voteCount": 1,
        "content": "D is correct\nhttps://cloud.google.com/bigquery/docs/access-control"
      },
      {
        "date": "2022-01-24T13:56:00.000Z",
        "voteCount": 1,
        "content": "D sounds perfect with minimal steps.\nQuote from the GCP docs: \"BigQuery User\n(roles/bigquery.user)\n\nWhen applied to a dataset, this role provides the ability to read the dataset's metadata and list tables in the dataset.\n\nWhen applied to a project, this role also provides the ability to run jobs, including queries, within the project. A principal with this role can enumerate their own jobs, cancel their own jobs, and enumerate datasets within a project. Additionally, allows the creation of new datasets within the project; the creator is granted the BigQuery Data Owner role (roles/bigquery.dataOwner) on these new datasets.\""
      },
      {
        "date": "2022-01-18T01:04:00.000Z",
        "voteCount": 1,
        "content": "Answer is C\nOption C says, there is a probability of an internal BI datawarehouse. Before providing the iam permissions, it is better to copy data to internal BI. \nMy view"
      },
      {
        "date": "2022-01-15T13:08:00.000Z",
        "voteCount": 2,
        "content": "agree With D"
      },
      {
        "date": "2022-01-12T14:50:00.000Z",
        "voteCount": 1,
        "content": "maybe D is not sufficient because, as per documentation:\n\"Note: For a user to be able to query the tables in a dataset, it is not sufficient for the user to have access to the dataset. A user must also have permission to run a query job in a project. If you want to give a user permission to run a query from your project, give the user the bigquery.jobs.create permission for the project. You can do this by assigning the user the roles/bigquery.jobUser role for your project. For more information, see Access control examples\"."
      },
      {
        "date": "2021-11-19T15:11:00.000Z",
        "voteCount": 2,
        "content": "I agree With D"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 179,
    "url": "https://www.examtopics.com/discussions/google/view/51262-exam-associate-cloud-engineer-topic-1-question-179/",
    "body": "Your company is moving its entire workload to Compute Engine. Some servers should be accessible through the Internet, and other servers should only be accessible over the internal network. All servers need to be able to talk to each other over specific ports and protocols. The current on-premises network relies on a demilitarized zone (DMZ) for the public servers and a Local Area Network (LAN) for the private servers. You need to design the networking infrastructure on<br>Google Cloud to match these requirements. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a single VPC with a subnet for the DMZ and a subnet for the LAN. 2. Set up firewall rules to open up relevant traffic between the DMZ and the LAN subnets, and another firewall rule to allow public ingress traffic for the DMZ.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a single VPC with a subnet for the DMZ and a subnet for the LAN. 2. Set up firewall rules to open up relevant traffic between the DMZ and the LAN subnets, and another firewall rule to allow public egress traffic for the DMZ.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a VPC with a subnet for the DMZ and another VPC with a subnet for the LAN. 2. Set up firewall rules to open up relevant traffic between the DMZ and the LAN subnets, and another firewall rule to allow public ingress traffic for the DMZ.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a VPC with a subnet for the DMZ and another VPC with a subnet for the LAN. 2. Set up firewall rules to open up relevant traffic between the DMZ and the LAN subnets, and another firewall rule to allow public egress traffic for the DMZ."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 18,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-07-09T07:32:00.000Z",
        "voteCount": 29,
        "content": "Passed the test today. About 80% of the questions are here."
      },
      {
        "date": "2021-10-11T01:44:00.000Z",
        "voteCount": 2,
        "content": "you got same questions from this examtopics"
      },
      {
        "date": "2021-07-13T10:05:00.000Z",
        "voteCount": 5,
        "content": "Congratulations!"
      },
      {
        "date": "2021-05-03T02:12:00.000Z",
        "voteCount": 28,
        "content": "A is the Right answer. You can discard B and C because they lack the need of creating Network Peering to communicate the DMZ VPC with the LAN VPC (LAN VPC is not exposed to public so they need to communicate via private addresses which cannot be achieved with 2 VPCs without Network Peering). Plus, you can discard B, as you don't need to enable the egress traffic, you always need to enable the ingress traffic as this is never enabled by default."
      },
      {
        "date": "2021-05-24T07:46:00.000Z",
        "voteCount": 12,
        "content": "A is wrong. You don't need to set up firewall rules between subnets of the same VPC. C is the answer"
      },
      {
        "date": "2021-07-28T22:04:00.000Z",
        "voteCount": 1,
        "content": "You need fw rules"
      },
      {
        "date": "2022-01-29T06:27:00.000Z",
        "voteCount": 2,
        "content": "of course you do"
      },
      {
        "date": "2021-05-25T14:13:00.000Z",
        "voteCount": 5,
        "content": "C is Create a VPC with a subnet for the DMZ and another VPC with a subnet for the LAN. 2. Set up firewall rules to open up relevant traffic between the DMZ and the LAN subnets, and another firewall rule to allow public ingress traffic for the DMZ. Without peering 2 VPC's how this this be done ?"
      },
      {
        "date": "2022-09-01T21:00:00.000Z",
        "voteCount": 1,
        "content": "and where do you have the VPC peering to communicate both VPCs?"
      },
      {
        "date": "2023-10-12T22:38:00.000Z",
        "voteCount": 7,
        "content": "The answer is A:\nHere's the explanation:\n\n--&gt;Single VPC: Creating a single Virtual Private Cloud (VPC) is a common practice to manage your resources in Google Cloud.\nSubnet for DMZ and LAN: Creating separate subnets within the same VPC for the DMZ (public-facing) and LAN (private) resources is a recommended approach to segregate your resources.\n--&gt;Firewall Rules: Setting up firewall rules allows you to control traffic between the DMZ and LAN subnets and enables you to define specific access policies. You also need to allow public traffic (ingress) into the DMZ to make the public-facing resources accessible from the internet."
      },
      {
        "date": "2023-09-09T20:21:00.000Z",
        "voteCount": 2,
        "content": "A is the correct answer, as it meet the question requirment"
      },
      {
        "date": "2022-10-20T13:52:00.000Z",
        "voteCount": 1,
        "content": "A is the right choice"
      },
      {
        "date": "2022-09-03T06:41:00.000Z",
        "voteCount": 1,
        "content": "A seems right"
      },
      {
        "date": "2022-08-25T03:35:00.000Z",
        "voteCount": 1,
        "content": "hi All what is the ans"
      },
      {
        "date": "2022-06-24T07:32:00.000Z",
        "voteCount": 1,
        "content": "1 VPC enough for LAN and DMZ , Need to open appropriate firewall rules. A is right."
      },
      {
        "date": "2022-06-17T07:23:00.000Z",
        "voteCount": 2,
        "content": "Vote for A\nBy default traffic between subnets on a VPC network is not allowed (except on the \"default\" network). \n(This blocks traffic between all instances, not just traffic between subnets =&gt; FW rules must be defined to allow communications between all instances, regardless the subnets)\n2 VPC will not work without peering."
      },
      {
        "date": "2022-03-26T06:57:00.000Z",
        "voteCount": 2,
        "content": "You can't explicitly create a FW rule for the subnet, but connections are allowed or denied on a per-instance basis. You can think of the VPC firewall rules as existing not only between your instances and other networks, but also between individual instances within the same network.\nC will not work without peering..."
      },
      {
        "date": "2022-01-29T00:11:00.000Z",
        "voteCount": 6,
        "content": "Guys i cleared my exam last week. This question bank is must. 80% questions were from here."
      },
      {
        "date": "2022-08-09T23:20:00.000Z",
        "voteCount": 1,
        "content": "congratulation, i need to pass exam in end month .Can you give me some advise please?"
      },
      {
        "date": "2022-01-24T06:48:00.000Z",
        "voteCount": 3,
        "content": "A - my vote. Two different vpc need vpc peering."
      },
      {
        "date": "2022-01-12T14:17:00.000Z",
        "voteCount": 1,
        "content": "I don't understand why you say that the answer is A. If you have 2 subnets in the same network you won't have firewall between the 2 subnets. So you can't have a DMZ that can communicate with a private network. So the answer should be C."
      },
      {
        "date": "2021-09-23T06:14:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2021-09-23T20:44:00.000Z",
        "voteCount": 3,
        "content": "Hi, I have an exam today. Are the questions still to some extent valid?"
      },
      {
        "date": "2021-10-14T23:56:00.000Z",
        "voteCount": 1,
        "content": "Yes it is. I passed my exam on 3rd Oct received certificate on 7th Oct. Exam topics and a study course in Udemy which I bought during their discount sale, helped me."
      },
      {
        "date": "2021-08-18T05:50:00.000Z",
        "voteCount": 4,
        "content": "Textbook example of DMZ and private subnet topology, hence answer A. Anyone who thinks C or multiple VPCs or whatever I strongly suggest you do CCNA before coming here."
      },
      {
        "date": "2021-07-13T09:59:00.000Z",
        "voteCount": 2,
        "content": "Yes. Correct answer is A. No need to complicate the setup by creating two different VPC networks."
      },
      {
        "date": "2021-07-04T07:59:00.000Z",
        "voteCount": 5,
        "content": "All questions are still valid. I cleared my paper yesterday (shayan18@live.com)"
      },
      {
        "date": "2021-07-13T10:06:00.000Z",
        "voteCount": 1,
        "content": "Congratulations!"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 180,
    "url": "https://www.examtopics.com/discussions/google/view/74600-exam-associate-cloud-engineer-topic-1-question-180/",
    "body": "You have just created a new project which will be used to deploy a globally distributed application. You will use Cloud Spanner for data storage. You want to create a Cloud Spanner instance. You want to perform the first step in preparation of creating the instance. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable the Cloud Spanner API.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure your Cloud Spanner instance to be multi-regional.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new VPC network with subnetworks in all desired regions.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGrant yourself the IAM role of Cloud Spanner Admin."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 20,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-06-13T14:34:00.000Z",
        "voteCount": 11,
        "content": "A is right\nhttps://cloud.google.com/spanner/docs/getting-started/set-up"
      },
      {
        "date": "2022-05-31T12:13:00.000Z",
        "voteCount": 6,
        "content": "If you click on Create instance, the message is show in bottom: Cloud Spanner API for your project has been enabled."
      },
      {
        "date": "2024-10-02T08:34:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/spanner/docs/getting-started/set-up"
      },
      {
        "date": "2023-12-29T10:32:00.000Z",
        "voteCount": 1,
        "content": "B looks more like it. \nWhen creating a Cloud Spanner instance, you configure the instance details first before enabling the Cloud Spanner API, if its not enabled by default i.e. if this is the first time you are using Cloud Spanner in your project."
      },
      {
        "date": "2023-09-21T05:57:00.000Z",
        "voteCount": 2,
        "content": "Its definitely A. Link: https://cloud.google.com/spanner/docs/quickstart-console"
      },
      {
        "date": "2023-09-05T20:00:00.000Z",
        "voteCount": 1,
        "content": "Answer A is correct before you do anything first you needto enable the API of that particulr service"
      },
      {
        "date": "2023-01-13T07:55:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/spanner/docs/quickstart-console?_ga=2.68426577.-1975890344.1661276010&amp;_gac=1.161955406.1673625078.Cj0KCQiAn4SeBhCwARIsANeF9DJxfolckwcRZqaOS7Rem2pzXWGmlBaLlxK4hHSe3YZ4DtE5oHHKVMQaArPUEALw_wcB#:~:text=see%20Pricing.-,Before%20you%20begin,Enable%20the%20Cloud%20Spanner%20API,-Create%20an%20instance"
      },
      {
        "date": "2023-01-13T07:54:00.000Z",
        "voteCount": 1,
        "content": "A is right\nhttps://cloud.google.com/spanner/docs/quickstart-console?_ga=2.68426577.-1975890344.1661276010&amp;_gac=1.161955406.1673625078.Cj0KCQiAn4SeBhCwARIsANeF9DJxfolckwcRZqaOS7Rem2pzXWGmlBaLlxK4hHSe3YZ4DtE5oHHKVMQaArPUEALw_wcB#:~:text=see%20Pricing.-,Before%20you%20begin,Enable%20the%20Cloud%20Spanner%20API,-Create%20an%20instance"
      },
      {
        "date": "2022-12-15T06:48:00.000Z",
        "voteCount": 1,
        "content": "Try the scenario yourself. Its A"
      },
      {
        "date": "2022-09-03T06:42:00.000Z",
        "voteCount": 1,
        "content": "A seems right"
      },
      {
        "date": "2022-08-06T06:48:00.000Z",
        "voteCount": 3,
        "content": "Answer must be B , here is why?\nI was confused between A and B but I tested this by creating a new project, when you go to spanner and click on create a spanner instance it automatically enables the API for you and you can all see this activity on the notification panel on the top right along with the visibility of this message clearly on that instance page as well, First it will auto-enable the API and then It will give you an option to select multi-region, Now the questions say your first step that has to be multi-region since enabling the API was done by google automatically and none other options makes sense here."
      },
      {
        "date": "2022-08-06T06:59:00.000Z",
        "voteCount": 8,
        "content": "Correction : It's A \nSince it does not specify if we are using command line tool or UI, if you are using command line tool then you will have to enable this."
      },
      {
        "date": "2022-06-24T03:44:00.000Z",
        "voteCount": 2,
        "content": "because Api auto enabled when you click create new instance on cloud spanner UI"
      },
      {
        "date": "2022-08-06T13:31:00.000Z",
        "voteCount": 3,
        "content": "This is TRUE. \nVerified this:\n- Go to your GCP project and verify that \"Cloud Spanner API\" is NOT enabled.\n- Go to Cloud Spanner. Click \"Create Instance\".\n- Check back again the \"Cloud Spanner API\" , you will see that status is \"API Enabled\"."
      },
      {
        "date": "2022-05-24T04:43:00.000Z",
        "voteCount": 4,
        "content": "Answer : A --&gt; Tested"
      },
      {
        "date": "2022-08-06T06:49:00.000Z",
        "voteCount": 1,
        "content": "How ? I tested this as well and It auto enables the API. Unless this is a new feature by google and the question is old then I am not sure."
      },
      {
        "date": "2022-09-20T20:42:00.000Z",
        "voteCount": 1,
        "content": "If you used CLI then the API would have to be enabled manually."
      },
      {
        "date": "2022-05-02T02:14:00.000Z",
        "voteCount": 2,
        "content": "Enabling API is the first step"
      },
      {
        "date": "2022-04-29T10:36:00.000Z",
        "voteCount": 1,
        "content": "A - I tested..."
      },
      {
        "date": "2022-04-29T01:27:00.000Z",
        "voteCount": 1,
        "content": "A:-tested it need to Enable Api first"
      },
      {
        "date": "2022-04-26T08:43:00.000Z",
        "voteCount": 1,
        "content": "shoudnt the ans be enable cloud spanner API.Option A"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 181,
    "url": "https://www.examtopics.com/discussions/google/view/79493-exam-associate-cloud-engineer-topic-1-question-181/",
    "body": "You have created a new project in Google Cloud through the gcloud command line interface (CLI) and linked a billing account. You need to create a new Compute<br>Engine instance using the CLI. You need to perform the prerequisite steps. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Cloud Monitoring Workspace.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a VPC network in the project.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable the compute googleapis.com API.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGrant yourself the IAM role of Computer Admin."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 14,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-26T07:34:00.000Z",
        "voteCount": 2,
        "content": "agree with C, there are few API enabled by default and the Compute engine API is not a part of them. do not be confused, personally, I thought this API should be  part of the basic APIs enabled but it it not."
      },
      {
        "date": "2024-03-01T13:15:00.000Z",
        "voteCount": 3,
        "content": "The compute.googleapis.com API must be enabled in your project before you can utilize Compute Engine features and issue commands to create instances."
      },
      {
        "date": "2023-01-21T06:03:00.000Z",
        "voteCount": 1,
        "content": "Why not B? Can you create Compute Engine instance without assigning it it VPC?"
      },
      {
        "date": "2023-03-27T02:14:00.000Z",
        "voteCount": 2,
        "content": "I believe you can make use of the default vpc"
      },
      {
        "date": "2023-05-14T21:13:00.000Z",
        "voteCount": 2,
        "content": "When you create a new project on the GCP, a default VPC network is automatically created for you."
      },
      {
        "date": "2023-06-05T12:33:00.000Z",
        "voteCount": 1,
        "content": "Yeah, but who uses the default VPC and CIDR ranges?  Technically you could, but its not best practice and RARELY would fit in with a companies existing infrastructure."
      },
      {
        "date": "2022-12-19T11:58:00.000Z",
        "voteCount": 3,
        "content": "api &gt; iam role .i vote for C !!"
      },
      {
        "date": "2022-09-16T03:18:00.000Z",
        "voteCount": 4,
        "content": "nothing can be done before activating the API"
      },
      {
        "date": "2022-09-14T07:55:00.000Z",
        "voteCount": 2,
        "content": "C is the obvious answer."
      },
      {
        "date": "2022-09-06T11:58:00.000Z",
        "voteCount": 3,
        "content": "api first"
      },
      {
        "date": "2022-09-03T06:43:00.000Z",
        "voteCount": 2,
        "content": "C the compute googleapis.com API"
      },
      {
        "date": "2022-09-02T10:58:00.000Z",
        "voteCount": 2,
        "content": "Must be C"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 182,
    "url": "https://www.examtopics.com/discussions/google/view/79790-exam-associate-cloud-engineer-topic-1-question-182/",
    "body": "Your company has developed a new application that consists of multiple microservices. You want to deploy the application to Google Kubernetes Engine (GKE), and you want to ensure that the cluster can scale as more applications are deployed in the future. You want to avoid manual intervention when each new application is deployed. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the application on GKE, and add a HorizontalPodAutoscaler to the deployment.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the application on GKE, and add a VerticalPodAutoscaler to the deployment.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a GKE cluster with autoscaling enabled on the node pool. Set a minimum and maximum for the size of the node pool.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a separate node pool for each application, and deploy each application to its dedicated node pool."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 19,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 14,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-06-09T07:13:00.000Z",
        "voteCount": 9,
        "content": "Answer is C\nThe key point is \"ensure that the CLUSTER can scale\"\nA- HorizontalPodAutoscaler - ensures to scale the number of pods\nwhile \nC- Create a GKE cluster with autoscaling enabled on the node pool. Set a minimum and maximum for the size of the node pool.\nensures to scale the number of nodes in the cluster.\n\nSo the answer is C."
      },
      {
        "date": "2023-06-24T16:26:00.000Z",
        "voteCount": 5,
        "content": "C is the right choice... See this for reference https://cloud.google.com/architecture/best-practices-for-running-cost-effective-kubernetes-applications-on-gke#fine-tune_gke_autoscaling\n\nA- HorizontalPodAutoscaler -  it is best suited for stateless workers that can spin up quickly to react to usage spikes, and shut down gracefully to avoid workload instability."
      },
      {
        "date": "2024-10-03T00:43:00.000Z",
        "voteCount": 1,
        "content": "c"
      },
      {
        "date": "2024-09-21T14:05:00.000Z",
        "voteCount": 1,
        "content": "A is right ans"
      },
      {
        "date": "2024-07-26T07:44:00.000Z",
        "voteCount": 1,
        "content": "Answer is C:\nthe HPA is used in for scaling a deployment (an application), but here, the question is asking to scale the cluster when new applications are being added which have different and independent deployments, we have scaling at cluster level, then at deployment level either horizontally  or vertically."
      },
      {
        "date": "2024-01-01T20:39:00.000Z",
        "voteCount": 2,
        "content": "In the context of deploying a new application and ensuring future scalability with minimal manual intervention, focusing on pod scalability is indeed fundamental. This is accurately addressed by option A (Deploy the application on GKE, and add a HorizontalPodAutoscaler to the deployment).\nHowever, it's also important to have node autoscaling enabled (as mentioned in option C) to ensure that the cluster can accommodate the scaling pods. Both pod and node scaling are important for a fully scalable solution, but the immediate focus when deploying a new application is typically on pod configuration and scaling."
      },
      {
        "date": "2024-03-01T13:30:00.000Z",
        "voteCount": 1,
        "content": "This is incorrect. Horizontal Pod Autoscalers scale based on pod-level metrics such as CPU. While useful, HPAs don't directly address the need to add more nodes if the underlying infrastructure is at capacity. The answer is C which provides the most effective and streamlined way to achieve automatic cluster-level scaling in a GKE environment hosting multiple microservices."
      },
      {
        "date": "2023-06-04T19:49:00.000Z",
        "voteCount": 2,
        "content": "Its A,\nWhen you first deploy your workload to a Kubernetes cluster, you may not be sure about its resource requirements and how those requirements might change depending on usage patterns, external dependencies, or other factors. Horizontal Pod autoscaling helps to ensure that your workload functions consistently in different situations, and allows you to control costs by only paying for extra capacity when you need it."
      },
      {
        "date": "2023-04-20T00:11:00.000Z",
        "voteCount": 2,
        "content": "i think it is A \"you want to ensure that the cluster can scale as more applications are deployed in the future.\""
      },
      {
        "date": "2023-04-18T04:14:00.000Z",
        "voteCount": 2,
        "content": "option C"
      },
      {
        "date": "2023-04-20T00:11:00.000Z",
        "voteCount": 2,
        "content": "option A*"
      },
      {
        "date": "2023-04-05T12:38:00.000Z",
        "voteCount": 1,
        "content": "Not knowing how many pods, and like nooneknows said, chatgpt... A is correct."
      },
      {
        "date": "2023-04-04T17:45:00.000Z",
        "voteCount": 2,
        "content": "Chat GPT said A is the Answer!"
      },
      {
        "date": "2023-03-27T02:17:00.000Z",
        "voteCount": 4,
        "content": "i believe A is the answer, you cant figure out how many nodes you will need in the future...how you gotta set a maximum"
      },
      {
        "date": "2023-02-20T06:44:00.000Z",
        "voteCount": 2,
        "content": "How do you set the maximum number of nodes and you do not know how you app will scale in the future? I think A is more accurate here."
      },
      {
        "date": "2022-12-28T07:36:00.000Z",
        "voteCount": 4,
        "content": "less manual intervention"
      },
      {
        "date": "2022-10-20T14:00:00.000Z",
        "voteCount": 1,
        "content": "C is the right choice... See this for reference https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-autoscaler"
      },
      {
        "date": "2022-10-09T05:39:00.000Z",
        "voteCount": 1,
        "content": "C is the correct answer, you can enable the cluster autoscaling in node pool by specifying min and max node size.\n\nhttps://cloud.google.com/kubernetes-engine/docs/how-to/cluster-autoscaler#adding_a_node_pool_with_autoscaling"
      },
      {
        "date": "2022-09-30T03:55:00.000Z",
        "voteCount": 2,
        "content": "it's mentioning \"the cluster can scale\" the answer is C"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 183,
    "url": "https://www.examtopics.com/discussions/google/view/74599-exam-associate-cloud-engineer-topic-1-question-183/",
    "body": "You need to manage a third-party application that will run on a Compute Engine instance. Other Compute Engine instances are already running with default configuration. Application installation files are hosted on Cloud Storage. You need to access these files from the new instance without allowing other virtual machines (VMs) to access these files. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate the instance with the default Compute Engine service account. Grant the service account permissions on Cloud Storage.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate the instance with the default Compute Engine service account. Add metadata to the objects on Cloud Storage that matches the metadata on the new instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new service account and assign this service account to the new instance. Grant the service account permissions on Cloud Storage.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new service account and assign this service account to the new instance. Add metadata to the objects on Cloud Storage that matches the metadata on the new instance."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 25,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-08-24T09:20:00.000Z",
        "voteCount": 9,
        "content": "\"without allowing other instances\" , the other instances are created with default compute engine service account. So you must create a new independant service account"
      },
      {
        "date": "2023-09-08T00:20:00.000Z",
        "voteCount": 3,
        "content": "C is correct."
      },
      {
        "date": "2022-09-26T13:20:00.000Z",
        "voteCount": 3,
        "content": "C is the clear choice. Want to create a new service account instead of using the default and grant it permissions in cloud storage. Straightforward C."
      },
      {
        "date": "2022-09-16T08:42:00.000Z",
        "voteCount": 1,
        "content": "C is right"
      },
      {
        "date": "2022-09-03T06:45:00.000Z",
        "voteCount": 1,
        "content": "C seems right to me"
      },
      {
        "date": "2022-06-13T14:24:00.000Z",
        "voteCount": 2,
        "content": "C\nhttps://cloud.google.com/iam/docs/best-practices-for-using-and-managing-service-accounts \nIf an application uses third-party or custom identities and needs to access a resource, such as a BigQuery dataset or a Cloud Storage bucket, it must perform a transition between principals. Because Google Cloud APIs don't recognize third-party or custom identities, the application can't propagate the end-user's identity to BigQuery or Cloud Storage. Instead, the application has to perform the access by using a different Google identity."
      },
      {
        "date": "2022-05-23T03:05:00.000Z",
        "voteCount": 1,
        "content": "Although C is the correct answer notice that, as Google recommend, you first need to grant the service account the required permission before attach it to a resource."
      },
      {
        "date": "2022-05-02T14:46:00.000Z",
        "voteCount": 4,
        "content": "C all the way. Restricts access to other VMs since they won\u2019t have the new service account you have associated with your new VM"
      },
      {
        "date": "2022-05-01T12:23:00.000Z",
        "voteCount": 2,
        "content": "C, other VMs will run as default service account."
      },
      {
        "date": "2022-04-26T15:38:00.000Z",
        "voteCount": 2,
        "content": "C is correct as the other vms have default service accounts."
      },
      {
        "date": "2022-04-26T07:50:00.000Z",
        "voteCount": 2,
        "content": "C, using Default account makes the storage visible to other machines"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 184,
    "url": "https://www.examtopics.com/discussions/google/view/74598-exam-associate-cloud-engineer-topic-1-question-184/",
    "body": "You need to configure optimal data storage for files stored in Cloud Storage for minimal cost. The files are used in a mission-critical analytics pipeline that is used continually. The users are in Boston, MA (United States). What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure regional storage for the region closest to the users. Configure a Nearline storage class.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure regional storage for the region closest to the users. Configure a Standard storage class.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure dual-regional storage for the dual region closest to the users. Configure a Nearline storage class.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure dual-regional storage for the dual region closest to the users. Configure a Standard storage class."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 90,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 66,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-05-24T07:44:00.000Z",
        "voteCount": 34,
        "content": "Mission critical is the keyword here which specifies that we need to have a multi-regional backup of the data to survive any regional failures.\nSo option D is the correct choice here."
      },
      {
        "date": "2022-08-07T22:15:00.000Z",
        "voteCount": 2,
        "content": "At the first point in this documentation says that dual-regional storage is used for business continuity and disaster recovery. Disaster can affect to a regional architecture. I think it's make sense to use dual-regional storage for this case. Also, dual-regional storage is cheaper than multi-regional. \n\nhttps://cloud.google.com/storage/docs/dual-regions#use-dual-region-storage"
      },
      {
        "date": "2022-08-06T13:42:00.000Z",
        "voteCount": 4,
        "content": "Keywords: minimal cost and mission-critical\nLooks like people are just looking to be on the cost side. You need to meet both.\n\nIn this case, it needs to be \"dual-region\". This is much cheaper than storage in \"multi-region\" which is obviously not in the choices."
      },
      {
        "date": "2022-10-30T07:16:00.000Z",
        "voteCount": 5,
        "content": "Dual region is expensive than multi-region. (Also mentioned in the documentation: https://cloud.google.com/storage/docs/locations)\nWhen we set objects to be multi-regional, we get to decide/shuffle the data around at will to meet our storage needs. When you take that control away from us, it reduces the flexibility of our systems, making it more expensive to operate."
      },
      {
        "date": "2022-05-02T14:49:00.000Z",
        "voteCount": 23,
        "content": "Continuous access to data means Standard since all of the other options are for infrequently accessed storage (Nearline, Coldline, Archive). Since no other regions are mentioned, single region is best in this case"
      },
      {
        "date": "2022-05-23T03:08:00.000Z",
        "voteCount": 8,
        "content": "And beacuse single region is \"costly-effective\"."
      },
      {
        "date": "2024-10-03T00:50:00.000Z",
        "voteCount": 1,
        "content": "Mission-critcal"
      },
      {
        "date": "2024-09-25T01:10:00.000Z",
        "voteCount": 1,
        "content": "I will go for D"
      },
      {
        "date": "2024-09-10T14:20:00.000Z",
        "voteCount": 2,
        "content": "I need a spaceship, but please make it from the cheapest materials, my mission is very critical"
      },
      {
        "date": "2024-02-18T18:48:00.000Z",
        "voteCount": 3,
        "content": "I vote for B because of the keywords: miniaml-cost. The question even specified the single region that the use in, dual region wouldnt match the minimal cost requirement."
      },
      {
        "date": "2024-03-13T05:54:00.000Z",
        "voteCount": 1,
        "content": "That was my thinking as well"
      },
      {
        "date": "2024-01-13T03:57:00.000Z",
        "voteCount": 2,
        "content": "mission-critical = dual-regional"
      },
      {
        "date": "2024-01-05T00:07:00.000Z",
        "voteCount": 1,
        "content": "SLA with dual-region is higher than single region, which sounds normal :\nhttps://cloud.google.com/storage/sla\nSo, as this platform is mission-critical, we have to maximize the SLA of the storage, so answer D."
      },
      {
        "date": "2024-01-01T20:51:00.000Z",
        "voteCount": 4,
        "content": "Dual-Regional Storage: While dual-regional storage offers higher availability and redundancy by storing data in two geographic locations, it's typically more expensive than regional storage. For mission-critical applications where users are primarily in one geographic area (Boston), the added cost and complexity of dual-regional storage may not be necessary."
      },
      {
        "date": "2023-11-22T01:39:00.000Z",
        "voteCount": 1,
        "content": "i think C"
      },
      {
        "date": "2024-03-13T05:55:00.000Z",
        "voteCount": 1,
        "content": "C is not correct due to using Nearline Storage."
      },
      {
        "date": "2023-11-03T00:02:00.000Z",
        "voteCount": 2,
        "content": "The correct answer is B. Configure regional storage for the region closest to the users. Configure a Standard storage class.\nThe files are used in a mission-critical analytics pipeline that is used continually. This means that the files need to be available quickly and reliably. Regional storage is the best option for this because it ensures that the files are stored in the same region as the users. This will minimize latency and ensure that the files are available quickly.\nThe Standard storage class is the best option for this because it provides a good balance between cost and performance. Nearline storage is a lower-cost option, but it is also slower. Standard storage is a good option for files that are used frequently, but not constantly.\nThe other options are not as good for this scenario. Dual-regional storage is more expensive than regional storage, and it does not provide any performance benefits. Coldline storage is the lowest-cost option, but it is also the slowest."
      },
      {
        "date": "2023-11-02T19:34:00.000Z",
        "voteCount": 2,
        "content": "I will go with B, as both dual-region and regional has 4 9's availability.\nhttps://cloud.google.com/storage/docs/locations"
      },
      {
        "date": "2023-10-03T07:53:00.000Z",
        "voteCount": 3,
        "content": "Correct answer is B\nRequirement is low cost and mission critical. It needs low latency which by using region storage will be achieved. \nDual Storage is more expensive then multi regional. There is no need for dual storage in this case. If there will be a zonal failure it can be handled by regional storage.\nUsing Dual storage here will increase cost."
      },
      {
        "date": "2023-09-08T00:24:00.000Z",
        "voteCount": 4,
        "content": "B is correct answer. Cloud storage provides 11 9's durability which is safe for mission critical files. Plus the question provides the users location to be a single region, and minimal cost should be there."
      },
      {
        "date": "2023-09-06T03:19:00.000Z",
        "voteCount": 4,
        "content": "Because Google Cloud Storage is 11 Nines durable (99.999999999%) its pretty safe for mission critical data (ref: https://cloud.google.com/blog/products/storage-data-transfer/understanding-cloud-storage-11-9s-durability-target).  \nBUT the AVAILABILITY  SLA on single region is 99.9% and for dual region its 99.95% (ONLY 0.05% better)... For the additional cost - dual region offers MINIMAL additional availability (ref: https://cloud.google.com/storage/docs/storage-classes#standard)"
      },
      {
        "date": "2024-03-06T09:05:00.000Z",
        "voteCount": 1,
        "content": "This is the best point brought up by anyone here. The 11 9's makes it both cost effective and great for mission-critical data. Well done."
      },
      {
        "date": "2023-08-23T05:52:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/storage/docs/locations#location_recommendations"
      },
      {
        "date": "2023-05-16T10:43:00.000Z",
        "voteCount": 3,
        "content": "C is not correct because it would be more expensive than B. Dual-regional storage is designed for data that needs to be available in multiple regions. It is more expensive than regional storage, but it offers higher availability.\n\nIn this case, the files are used in a mission-critical analytics pipeline that is used continually. This means that the files need to be available with low latency. However, the users are only in Boston, MA (United States). Therefore, there is no need for the files to be available in multiple regions.\n\nTherefore, the best option is to configure regional storage for the region closest to the users (us-central1) and configure a Standard storage class. This will be less expensive than dual-regional storage and will still provide the required low latency."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 185,
    "url": "https://www.examtopics.com/discussions/google/view/74507-exam-associate-cloud-engineer-topic-1-question-185/",
    "body": "You are developing a new web application that will be deployed on Google Cloud Platform. As part of your release cycle, you want to test updates to your application on a small portion of real user traffic. The majority of the users should still be directed towards a stable version of your application. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the application on App Engine. For each update, create a new version of the same service. Configure traffic splitting to send a small percentage of traffic to the new version.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the application on App Engine. For each update, create a new service. Configure traffic splitting to send a small percentage of traffic to the new service.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the application on Kubernetes Engine. For a new release, update the deployment to use the new version.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the application on Kubernetes Engine. For a new release, create a new deployment for the new version. Update the service to use the new deployment."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 23,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-10-09T05:45:00.000Z",
        "voteCount": 7,
        "content": "A is correct answer,\nKeyword, Version, traffic splitting, App Engine supports traffic splitting for versions before releasing."
      },
      {
        "date": "2022-06-13T14:17:00.000Z",
        "voteCount": 5,
        "content": "It is no brainer questions, It is A."
      },
      {
        "date": "2023-12-07T19:44:00.000Z",
        "voteCount": 2,
        "content": "A is correct.\nStill, D seems also a correct approach. One can create a canary deployment with GKE and just update a service version."
      },
      {
        "date": "2024-02-19T04:22:00.000Z",
        "voteCount": 2,
        "content": "But it's more expensive and Google wants you to think cheap as possible in general."
      },
      {
        "date": "2023-09-08T00:25:00.000Z",
        "voteCount": 2,
        "content": "Answer is A."
      },
      {
        "date": "2023-05-21T00:02:00.000Z",
        "voteCount": 4,
        "content": "Similar questions seem to appear multiple times."
      },
      {
        "date": "2022-12-27T07:20:00.000Z",
        "voteCount": 4,
        "content": "some of these questions, the default by examtopics is completely different why so, why cannot they fix it once a real answer is known."
      },
      {
        "date": "2022-09-03T06:47:00.000Z",
        "voteCount": 2,
        "content": "A obvious choice"
      },
      {
        "date": "2022-08-13T04:47:00.000Z",
        "voteCount": 2,
        "content": "Vote goes to A"
      },
      {
        "date": "2022-06-08T05:55:00.000Z",
        "voteCount": 2,
        "content": "A obviously. No need to create a new service."
      },
      {
        "date": "2022-05-30T06:09:00.000Z",
        "voteCount": 1,
        "content": "Obviously A"
      },
      {
        "date": "2022-05-02T14:51:00.000Z",
        "voteCount": 4,
        "content": "A all the way. When deploying new versions you can split traffic for A/B testing to see how user base reacts to changes."
      },
      {
        "date": "2022-05-01T12:26:00.000Z",
        "voteCount": 1,
        "content": "A is correct, there's no need to create a new service for each update."
      },
      {
        "date": "2022-04-28T06:20:00.000Z",
        "voteCount": 1,
        "content": "ans is A"
      },
      {
        "date": "2022-04-26T08:50:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2022-04-25T11:08:00.000Z",
        "voteCount": 3,
        "content": "A is correct"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 186,
    "url": "https://www.examtopics.com/discussions/google/view/74499-exam-associate-cloud-engineer-topic-1-question-186/",
    "body": "You need to add a group of new users to Cloud Identity. Some of the users already have existing Google accounts. You want to follow one of Google's recommended practices and avoid conflicting accounts. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInvite the user to transfer their existing account.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInvite the user to use an email alias to resolve the conflict.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTell the user that they must delete their existing account.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTell the user to remove all personal email from the existing account."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 22,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-05-11T04:45:00.000Z",
        "voteCount": 10,
        "content": "https://cloud.google.com/architecture/identity/assessing-existing-user-accounts\n\nIf you want to maintain the access rights and some of the data associated with the Gmail account, you can ask the owner to remove Gmail from the user account so that you can then migrate them to Cloud Identity or Google Workspace."
      },
      {
        "date": "2024-10-03T00:59:00.000Z",
        "voteCount": 1,
        "content": "If you want to maintain the access rights and some of the data associated with the Gmail account, you can ask the owner to remove Gmail from the user account so that you can then migrate them to Cloud Identity or Google Workspace.\n\n"
      },
      {
        "date": "2024-10-10T04:01:00.000Z",
        "voteCount": 1,
        "content": "Now, I think A is a better answer."
      },
      {
        "date": "2024-08-20T08:11:00.000Z",
        "voteCount": 2,
        "content": "Explanation:\n\n    Email alias: This approach allows users to maintain their existing Google account while using a different email address for their work-related activities.\n    No account transfer: Avoids the complexities and potential issues associated with transferring accounts.\n    Clear separation: Maintains a clear distinction between personal and work-related activities.\n\nWhy not other options:\n\n    A. Account transfer: This is generally not recommended as it can lead to data loss or complications.\n    C. and D. Deleting or modifying existing accounts: These options are not practical or desirable as they disrupt the user's existing workflow.\n\nBy suggesting an email alias, you provide a user-friendly and secure solution to the account conflict."
      },
      {
        "date": "2023-09-08T00:29:00.000Z",
        "voteCount": 1,
        "content": "Answer is A."
      },
      {
        "date": "2022-09-03T06:48:00.000Z",
        "voteCount": 1,
        "content": "A obvious choice"
      },
      {
        "date": "2022-08-06T09:03:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer: A"
      },
      {
        "date": "2022-08-06T09:07:00.000Z",
        "voteCount": 2,
        "content": "Here is why ?\nQuestion states \"Some of the users already have existing Google accounts.\" Meaning they have personal account or any google account and what Option B is saying is to use aliases, as per google documentation this is only helpful when we want someone to receive emails in one inbox with 2 email names, meaning x@google.com and y@google.com goes to the same inbox BUT what you can't do is to have personal@gogle.com and company@google.com since the company wouldn't add you to their domain as that is not google recommended practice. \nhttps://support.google.com/a/answer/33327?hl=en#when_to_use"
      },
      {
        "date": "2022-08-05T22:17:00.000Z",
        "voteCount": 4,
        "content": "Vote for A as the right answer. The docs in this link:\n\nhttps://cloud.google.com/architecture/identity/migrating-consumer-accounts\n\nas provided by PAUGURU in his comment explains clearly about resolving account conflict. In the doc says nothing about to change email alias to resolve the conflict. So, following the documentation in that link means you are following the Googles Recommended Practices."
      },
      {
        "date": "2022-07-31T03:55:00.000Z",
        "voteCount": 2,
        "content": "A is the answer, for security reasons google best practices recommend transfer the account"
      },
      {
        "date": "2022-07-13T20:48:00.000Z",
        "voteCount": 1,
        "content": "Answer is B\nhttps://support.google.com/cloudidentity/answer/7062710"
      },
      {
        "date": "2022-08-05T22:10:00.000Z",
        "voteCount": 1,
        "content": "I am not sure if the link you provide explains the reason of the reason of your choosen answer. As in the documentation stated the email alias after what happen if you rename the email. Also, the documentation doesn't explain about account conflict.\n\nBetter with the docs link provided by PAUGURU: https://cloud.google.com/architecture/identity/migrating-consumer-accounts\n\nIt's explains clearly about the conflicting email and best practices too."
      },
      {
        "date": "2022-08-05T22:12:00.000Z",
        "voteCount": 1,
        "content": "*explains the reason of your choosen answer.\n\nsorry, messed up sentence"
      },
      {
        "date": "2022-08-06T09:03:00.000Z",
        "voteCount": 1,
        "content": "It should be A, I was confused with this as well but B is not relevant in this use case. Look at my Above comments."
      },
      {
        "date": "2022-07-05T02:27:00.000Z",
        "voteCount": 2,
        "content": "As per my understanding, B is the correct answer."
      },
      {
        "date": "2022-06-13T14:13:00.000Z",
        "voteCount": 2,
        "content": "A is right and followed GCP documentation to get more information."
      },
      {
        "date": "2022-04-25T10:15:00.000Z",
        "voteCount": 3,
        "content": "A looks right\nhttps://cloud.google.com/architecture/identity/migrating-consumer-accounts"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 187,
    "url": "https://www.examtopics.com/discussions/google/view/74440-exam-associate-cloud-engineer-topic-1-question-187/",
    "body": "You need to manage a Cloud Spanner instance for best query performance. Your instance in production runs in a single Google Cloud region. You need to improve performance in the shortest amount of time. You want to follow Google best practices for service configuration. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an alert in Cloud Monitoring to alert when the percentage of high priority CPU utilization reaches 45%. If you exceed this threshold, add nodes to your instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an alert in Cloud Monitoring to alert when the percentage of high priority CPU utilization reaches 45%. Use database query statistics to identify queries that result in high CPU usage, and then rewrite those queries to optimize their resource usage.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an alert in Cloud Monitoring to alert when the percentage of high priority CPU utilization reaches 65%. If you exceed this threshold, add nodes to your instance.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an alert in Cloud Monitoring to alert when the percentage of high priority CPU utilization reaches 65%. Use database query statistics to identify queries that result in high CPU usage, and then rewrite those queries to optimize their resource usage."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 30,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-04-25T10:05:00.000Z",
        "voteCount": 14,
        "content": "C looks correct, increase instances on single region if CPU above 65%\nhttps://cloud.google.com/spanner/docs/cpu-utilization#recommended-max"
      },
      {
        "date": "2024-10-10T04:05:00.000Z",
        "voteCount": 1,
        "content": "C"
      },
      {
        "date": "2023-12-07T19:52:00.000Z",
        "voteCount": 2,
        "content": "I believe it's D.\n\" Create an alert in Cloud Monitoring to alert when the percentage of high priority CPU utilization reaches 65%. Use database query statistics to identify queries that result in high CPU usage, and then rewrite those queries to optimize their resource usage.\"\n\nIs that ever possible to add any node to Cloud Spanner? Come on."
      },
      {
        "date": "2023-12-11T14:13:00.000Z",
        "voteCount": 4,
        "content": "Taking it back. The answer is C.\n\n\"Compute capacity defines amount of server and storage resources that are available to the databases in an instance. When you create an instance, you specify its compute capacity as a number of processing units or as a number of nodes, with 1000 processing units being equal to 1 node.\"\nhttps://cloud.google.com/spanner/docs/instances"
      },
      {
        "date": "2023-08-21T05:30:00.000Z",
        "voteCount": 4,
        "content": "I was keep thinking of A until I get to the link that thanks for @rsuresh27 provided bellow. the 45% is for the multi region."
      },
      {
        "date": "2023-04-20T00:13:00.000Z",
        "voteCount": 3,
        "content": "Metric\tMaximum for single-region instances\tMaximum per region for multi-region instances\nHigh priority total\t65%\t45%\n24-hour smoothed aggregate\t90%\t90%"
      },
      {
        "date": "2023-04-05T12:48:00.000Z",
        "voteCount": 1,
        "content": "C makes sense."
      },
      {
        "date": "2022-10-30T07:34:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/spanner/docs/cpu-utilization"
      },
      {
        "date": "2022-09-03T06:49:00.000Z",
        "voteCount": 1,
        "content": "C looks correct"
      },
      {
        "date": "2022-06-13T14:08:00.000Z",
        "voteCount": 1,
        "content": "shortest timeframe is key here , I am going with C as my answer."
      },
      {
        "date": "2022-06-10T06:35:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2022-05-31T12:38:00.000Z",
        "voteCount": 1,
        "content": "Metric\tMaximum for single-region instances\tMaximum per region for multi-region instances\nHigh priority total&gt;\t65%\t45%\n24-hour smoothed aggregate&gt;\t90%\t90%"
      },
      {
        "date": "2022-04-25T04:58:00.000Z",
        "voteCount": 3,
        "content": "The correct answer is C\nhttps://cloud.google.com/spanner/docs/cpu-utilization#recommended-max"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 188,
    "url": "https://www.examtopics.com/discussions/google/view/74494-exam-associate-cloud-engineer-topic-1-question-188/",
    "body": "Your company has an internal application for managing transactional orders. The application is used exclusively by employees in a single physical location. The application requires strong consistency, fast queries, and ACID guarantees for multi-table transactional updates. The first version of the application is implemented in PostgreSQL, and you want to deploy it to the cloud with minimal code changes. Which database is most appropriate for this application?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBigQuery",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud SQL\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Spanner",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Datastore"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 52,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 28,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-05-16T16:18:00.000Z",
        "voteCount": 18,
        "content": "Read the question :\nThe application is used exclusively by employees in a single physical location."
      },
      {
        "date": "2022-09-24T20:47:00.000Z",
        "voteCount": 4,
        "content": "Correct. That is the key thing. I have no idea why some people ended up thinking cloud spanner is better. Definitely is alternative A."
      },
      {
        "date": "2022-09-24T20:48:00.000Z",
        "voteCount": 7,
        "content": "Sorry I meant B, I had a typo."
      },
      {
        "date": "2022-04-25T09:49:00.000Z",
        "voteCount": 14,
        "content": "B -&gt; minimal code changes"
      },
      {
        "date": "2022-05-15T03:34:00.000Z",
        "voteCount": 6,
        "content": "Cloud SQL for PostgreSQL is a fully-managed database service that helps you set up, maintain, manage, and administer your PostgreSQL relational databases on Google Cloud Platform.\nhttps://cloud.google.com/sql/docs/postgres"
      },
      {
        "date": "2024-10-10T04:06:00.000Z",
        "voteCount": 1,
        "content": "B"
      },
      {
        "date": "2024-08-30T05:30:00.000Z",
        "voteCount": 1,
        "content": "Read the question :\nThe application is used exclusively by employees in a single physical location."
      },
      {
        "date": "2023-10-23T11:50:00.000Z",
        "voteCount": 5,
        "content": "Cloud spanner does everyhting cloud SQL already does. + It offers globality which we don't need and horizontal scaling that is mentionned nowhere"
      },
      {
        "date": "2023-08-07T07:10:00.000Z",
        "voteCount": 1,
        "content": "C. Select Cloud Spanner.\n\nCloud Spanner offers strong consistency, fast queries, and importantly, ACID guarantees for updates in multi-table transactions.\nCloud Spanner is well suited for large transactional databases that require horizontal scaling and offers relational database semantics.\nEven if the first version was PostgreSQL, Cloud Spanner is the best choice for this kind of application with strict requirements for ACID transactions.\nCloud SQL is also a relational database service, and while some database engines offer ACID transactions, it is not designed like Cloud Spanner for the strict requirements of multi-table transactional updates.\n\nReference link: Google Cloud - Cloud Spanner: https://cloud.google.com/spanner\nReference link: Google Cloud - ACID Transactions in Cloud Spanner: https://cloud.google.com/spanner/docs/transactions"
      },
      {
        "date": "2023-05-16T10:55:00.000Z",
        "voteCount": 2,
        "content": "the best choice for this application is Cloud SQL for PostgreSQL. It offers the required strong consistency, fast queries, and ACID guarantees for multi-table transactional updates. It is also a good choice for applications that are implemented in PostgreSQL and that you want to deploy to the cloud with minimal code changes."
      },
      {
        "date": "2023-05-11T12:54:00.000Z",
        "voteCount": 2,
        "content": "B. instead of C. because it's use in a single physical location and is compatible with PostgreSQL."
      },
      {
        "date": "2023-04-05T12:50:00.000Z",
        "voteCount": 1,
        "content": "B makes sense."
      },
      {
        "date": "2023-03-10T04:57:00.000Z",
        "voteCount": 1,
        "content": "Its B because of single location, usually cloud sql is for non global or/and HA sql ddbb, spanner is for global sql ddbb"
      },
      {
        "date": "2023-02-19T19:42:00.000Z",
        "voteCount": 1,
        "content": "key: single physical location"
      },
      {
        "date": "2023-02-10T21:03:00.000Z",
        "voteCount": 1,
        "content": "The answer is B. \nOne of the major advantages of Spanner is that the data is replicated over the Globe, since this data will only be accessed at one location Cloud SQL will be sufficient."
      },
      {
        "date": "2023-02-07T17:10:00.000Z",
        "voteCount": 1,
        "content": "Not only Cloud Spanner has the ACID guarantees. SQL and others can be.......now I go to B (I have changed my vote)"
      },
      {
        "date": "2023-01-31T12:04:00.000Z",
        "voteCount": 2,
        "content": "C is the right answer\nOnly Cloud Spanner has the ACID guarantees and String consistency as mentioned in the question,\nfor those who are saying B because of minimal changes with PostgreSQL check all the feature of CloudSpanner with below link\nhttps://cloud.google.com/spanner#all-features \nhttps://cloud.google.com/spanner/docs/postgresql-interface"
      },
      {
        "date": "2023-01-30T01:32:00.000Z",
        "voteCount": 1,
        "content": "Reference: https://cloud.google.com/blog/topics/developers-practitioners/your-google-cloud-database-options-explained"
      },
      {
        "date": "2023-01-15T19:16:00.000Z",
        "voteCount": 1,
        "content": "Only Cloud Spanner has the ACID guarantees........I go to C"
      },
      {
        "date": "2023-01-31T01:06:00.000Z",
        "voteCount": 4,
        "content": "no, not only spanner has acid. ACID is characteristic of basically any RDBMS, including postgres or mysql."
      },
      {
        "date": "2023-02-07T17:07:00.000Z",
        "voteCount": 1,
        "content": "You are right my friend. I have changed my vote afte your answer. Thanks alot. Now it is B."
      },
      {
        "date": "2023-01-08T01:55:00.000Z",
        "voteCount": 2,
        "content": "Read the test:\n- from a single phisical location\n- with minimal changes: onpremise Postgres -&gt; Cloud SQL (Postgres)"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 189,
    "url": "https://www.examtopics.com/discussions/google/view/75169-exam-associate-cloud-engineer-topic-1-question-189/",
    "body": "You are assigned to maintain a Google Kubernetes Engine (GKE) cluster named 'dev' that was deployed on Google Cloud. You want to manage the GKE configuration using the command line interface (CLI). You have just downloaded and installed the Cloud SDK. You want to ensure that future CLI commands by default address this specific cluster What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the command gcloud config set container/cluster dev.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the command gcloud container clusters update dev.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a file called gke.default in the ~/.gcloud folder that contains the cluster name.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a file called defaults.json in the ~/.gcloud folder that contains the cluster name."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 27,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-08-28T21:19:00.000Z",
        "voteCount": 12,
        "content": "To set a default cluster for gcloud commands, run the following command:\nhttps://cloud.google.com/kubernetes-engine/docs/how-to/managing-clusters"
      },
      {
        "date": "2024-05-22T08:18:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/kubernetes-engine/docs/how-to/managing-clusters#default_cluster_gcloud\n\nTo set a default cluster for gcloud commands, run the following command:\ngcloud config set container/cluster CLUSTER_NAME"
      },
      {
        "date": "2024-05-15T00:37:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/kubernetes-engine/docs/how-to/managing-clusters?hl=fr"
      },
      {
        "date": "2023-09-08T00:34:00.000Z",
        "voteCount": 1,
        "content": "Answer = A"
      },
      {
        "date": "2022-09-03T06:53:00.000Z",
        "voteCount": 1,
        "content": "A looks right to me"
      },
      {
        "date": "2022-06-13T13:36:00.000Z",
        "voteCount": 4,
        "content": "A is right \nTo set a default cluster for gcloud commands, run the following command:\n\n\ngcloud config set container/cluster CLUSTER_NAME"
      },
      {
        "date": "2022-05-11T05:11:00.000Z",
        "voteCount": 3,
        "content": "Set a default cluster forgcloud\nTo set a default cluster for commands gcloud, run the following command:\n\nPer https://cloud.google.com/kubernetes-engine/docs/how-to/managing-clusters?hl=fr\ngcloud config set container/cluster CLUSTER_NAME"
      },
      {
        "date": "2022-05-11T03:53:00.000Z",
        "voteCount": 1,
        "content": "Answer is A, \nTo set a default cluster for gcloud commands, run the following command:\ngcloud config set container/cluster CLUSTER_NAME\nhttps://cloud.google.com/kubernetes-engine/docs/how-to/managing-clusters?hl=en"
      },
      {
        "date": "2022-05-10T20:05:00.000Z",
        "voteCount": 3,
        "content": "To set a default cluster for gcloud commands, run the following command:\n\ngcloud config set container/cluster CLUSTER_NAME"
      },
      {
        "date": "2022-05-15T03:37:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/kubernetes-engine/docs/how-to/managing-clusters#default_cluster_kubectl"
      },
      {
        "date": "2022-05-07T19:03:00.000Z",
        "voteCount": 1,
        "content": "Correct Ans B\nhttps://cloud.google.com/sdk/gcloud/reference/container/clusters/update"
      },
      {
        "date": "2022-05-04T11:44:00.000Z",
        "voteCount": 2,
        "content": "B sounds right."
      },
      {
        "date": "2022-05-04T08:06:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/kubernetes-engine/docs/how-to/managing-clusters?hl=fr"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 190,
    "url": "https://www.examtopics.com/discussions/google/view/74495-exam-associate-cloud-engineer-topic-1-question-190/",
    "body": "The sales team has a project named Sales Data Digest that has the ID acme-data-digest. You need to set up similar Google Cloud resources for the marketing team but their resources must be organized independently of the sales team. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGrant the Project Editor role to the Marketing team for acme-data-digest.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Project Lien on acme-data-digest and then grant the Project Editor role to the Marketing team.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate another project with the ID acme-marketing-data-digest for the Marketing team and deploy the resources there.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new project named Marketing Data Digest and use the ID acme-data-digest. Grant the Project Editor role to the Marketing team."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 32,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-06-30T11:34:00.000Z",
        "voteCount": 11,
        "content": "Answer should be C because the resources for the marketing team should be independent from the Sales team. Resources are tied and separated by projects."
      },
      {
        "date": "2024-04-20T08:47:00.000Z",
        "voteCount": 2,
        "content": "Creating a separate project for the Marketing team allows you to organize their resources independently of the Sales team, which is a best practice for resource management and access control.\nGranting the Project Editor role to the Marketing team for the Sales team's project (acme-data-digest) would give them unnecessary access to Sales team resources (option A).\nCreating a Project Lien (option B) is not relevant in this scenario, as it's used to prevent resource deletion, not to manage access or organization.\nUsing the same ID (acme-data-digest) for a new project (option D) could lead to confusion and conflicts, and is not a recommended practice.\nBy creating a separate project for the Marketing team, you can ensure clear organization, access control, and resource management for both teams."
      },
      {
        "date": "2024-03-02T20:24:00.000Z",
        "voteCount": 1,
        "content": "C is the answer"
      },
      {
        "date": "2024-01-02T09:34:00.000Z",
        "voteCount": 2,
        "content": "c = 100%"
      },
      {
        "date": "2023-11-16T11:02:00.000Z",
        "voteCount": 4,
        "content": "C. Create another project with the ID acme-marketing-data-digest for the Marketing team and deploy the resources there.\n\nExplanation:\nOption C is the correct choice for organizing resources independently for the marketing team. By creating a separate project (acme-marketing-data-digest), you ensure that the marketing team's resources are isolated from the sales team's resources. This approach provides a clean and distinct organizational structure for each team.\n\nOptions A, B, and D involve using the same project (acme-data-digest) for both teams, which could lead to potential conflicts and lack of resource isolation. Option B suggests using a project lien, but liens are typically used to prevent the deletion of projects and don't provide the organizational separation needed for independent teams."
      },
      {
        "date": "2023-10-23T11:53:00.000Z",
        "voteCount": 2,
        "content": "only C makes sense"
      },
      {
        "date": "2023-10-09T05:16:00.000Z",
        "voteCount": 1,
        "content": "Why not D?"
      },
      {
        "date": "2023-09-08T00:49:00.000Z",
        "voteCount": 1,
        "content": "Answer = C"
      },
      {
        "date": "2023-06-24T10:20:00.000Z",
        "voteCount": 1,
        "content": "No more a technical exam ;("
      },
      {
        "date": "2022-09-30T22:14:00.000Z",
        "voteCount": 1,
        "content": "Sales and Marketing resource has to be in separate project so C."
      },
      {
        "date": "2022-09-03T06:54:00.000Z",
        "voteCount": 1,
        "content": "C looks right"
      },
      {
        "date": "2022-08-02T23:47:00.000Z",
        "voteCount": 1,
        "content": "C for sure"
      },
      {
        "date": "2022-06-13T13:37:00.000Z",
        "voteCount": 2,
        "content": "C is right answer based on given scenario.."
      },
      {
        "date": "2022-05-11T17:44:00.000Z",
        "voteCount": 3,
        "content": "C is straight to the point and addresses exactly all of the concerns"
      },
      {
        "date": "2022-05-02T11:02:00.000Z",
        "voteCount": 3,
        "content": "C is the better choice"
      },
      {
        "date": "2022-05-01T12:34:00.000Z",
        "voteCount": 2,
        "content": "C, other options seem obviously wrong"
      },
      {
        "date": "2022-04-29T01:25:00.000Z",
        "voteCount": 3,
        "content": "C seems the right answer"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 191,
    "url": "https://www.examtopics.com/discussions/google/view/74496-exam-associate-cloud-engineer-topic-1-question-191/",
    "body": "You have deployed multiple Linux instances on Compute Engine. You plan on adding more instances in the coming weeks. You want to be able to access all of these instances through your SSH client over the internet without having to configure specific access on the existing and new instances. You do not want the<br>Compute Engine instances to have a public IP. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Cloud Identity-Aware Proxy for HTTPS resources.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Cloud Identity-Aware Proxy for SSH and TCP resources\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an SSH keypair and store the public key as a project-wide SSH Key.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an SSH keypair and store the private key as a project-wide SSH Key."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 12,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 4,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-04-26T15:15:00.000Z",
        "voteCount": 13,
        "content": "B is correct as question say no public IP on the instance."
      },
      {
        "date": "2022-05-10T07:13:00.000Z",
        "voteCount": 16,
        "content": "Use IAP TCP to enable access to VM instances that do not have external IP addresses or do not permit direct access over the internet.\nhttps://cloud.google.com/iap/docs/using-tcp-forwarding"
      },
      {
        "date": "2023-11-09T06:46:00.000Z",
        "voteCount": 2,
        "content": "Answer B\nhttps://cloud.google.com/blog/products/identity-security/cloud-iap-enables-context-aware-access-to-vms-via-ssh-and-rdp-without-bastion-hosts"
      },
      {
        "date": "2023-11-04T01:51:00.000Z",
        "voteCount": 3,
        "content": "B - Cloud Identity-Aware Proxy (IAP) allows you to set up secure access to your VM instances without the need to expose them to the public internet. By using IAP for SSH and TCP resources, you can manage access to the instances through a central point (IAP), which serves as a secure way to access your resources without the need for public IP addresses.\n\nIAP allows you to set up access controls based on user identities and their permissions, rather than relying on specific IP addresses or public keys configured on individual instances. This streamlines access management and enhances security, providing centralized control over SSH access to your Compute Engine instances."
      },
      {
        "date": "2023-04-18T04:37:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2022-10-27T06:55:00.000Z",
        "voteCount": 4,
        "content": "Absolutely B\nhttps://cloud.google.com/iap/docs/using-tcp-forwarding#tunneling_ssh_connections"
      },
      {
        "date": "2022-09-20T03:18:00.000Z",
        "voteCount": 1,
        "content": "b is right"
      },
      {
        "date": "2022-09-03T06:55:00.000Z",
        "voteCount": 1,
        "content": "B looks right"
      },
      {
        "date": "2022-06-13T11:11:00.000Z",
        "voteCount": 4,
        "content": "B is correct, With TCP forwarding, IAP can protect SSH and RDP access to your VMs hosted on Google Cloud. Your VM instances don't even need public IP addresses."
      },
      {
        "date": "2022-05-24T05:11:00.000Z",
        "voteCount": 2,
        "content": "B is correct"
      },
      {
        "date": "2022-05-18T05:23:00.000Z",
        "voteCount": 1,
        "content": "I think it is B \nhttps://medium.com/google-cloud/how-to-ssh-into-your-gce-machine-without-a-public-ip-4d78bd23309e"
      },
      {
        "date": "2022-05-14T10:51:00.000Z",
        "voteCount": 2,
        "content": "B is correct as it uses IAP"
      },
      {
        "date": "2022-05-11T17:53:00.000Z",
        "voteCount": 2,
        "content": "IAP lets you establish a central authorization layer for applications accessed by HTTPS. This statement immediately eliminates A &amp; B since you would need to publicly access the instance. C is also incorrect because it uses a private SSH key. Private keys are only meant for the user themselves as proof of their identity. Public SSH keys are meant to be used for access within applications, so that is the most applicable in this case. D, final answer."
      },
      {
        "date": "2022-05-11T17:52:00.000Z",
        "voteCount": 2,
        "content": "IAP lets you establish a central authorization layer for applications accessed by HTTPS. This statement immediately eliminates A &amp; B since you would need to publicly access the instance. C is also incorrect because it uses a private SSH key. Private keys are only meant for the user themselves as proof of their identity. Public SSH keys are meant to be used for access within applications, so that is the most applicable in this case. D, final answer."
      },
      {
        "date": "2022-07-27T11:03:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/iap/docs/using-tcp-forwarding\nread pls!!"
      },
      {
        "date": "2022-05-11T17:53:00.000Z",
        "voteCount": 2,
        "content": "Scratch this. Made a mistake. C is the correct answer (thought they were flipped)"
      },
      {
        "date": "2022-05-25T18:47:00.000Z",
        "voteCount": 5,
        "content": "The question states \"You do not want the\nCompute Engine instances to have a public IP\", so that knocks out C and D as both options require public access.\n\nIAP however supports port forwarding for your client so the instances are never exposed. That leaves A and B. With the question explicitly stating \"your ssh client\", that means you need to configure the Cloud Identity Aware proxy for ssh (i.e. port forwarding.  Touching the proxies is not \"configure specific access on the existing and new instances\" as all this occurs on IAP and not the compute engine api."
      },
      {
        "date": "2022-04-25T09:50:00.000Z",
        "voteCount": 2,
        "content": "C looks better"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 192,
    "url": "https://www.examtopics.com/discussions/google/view/74626-exam-associate-cloud-engineer-topic-1-question-192/",
    "body": "You have created an application that is packaged into a Docker image. You want to deploy the Docker image as a workload on Google Kubernetes Engine. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpload the image to Cloud Storage and create a Kubernetes Service referencing the image.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpload the image to Cloud Storage and create a Kubernetes Deployment referencing the image.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpload the image to Container Registry and create a Kubernetes Service referencing the image.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpload the image to Container Registry and create a Kubernetes Deployment referencing the image.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 49,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-10-30T07:52:00.000Z",
        "voteCount": 19,
        "content": "A deployment is responsible for keeping a set of pods running. A service is responsible for enabling network access to a set of pods."
      },
      {
        "date": "2023-06-16T06:25:00.000Z",
        "voteCount": 11,
        "content": "Keep in mind, in the new exam, it would be rather \"Artifact Registry\""
      },
      {
        "date": "2022-09-30T22:18:00.000Z",
        "voteCount": 3,
        "content": "Upload your docker image on container registry then give a ref while creating deployment. So D!"
      },
      {
        "date": "2022-09-20T06:25:00.000Z",
        "voteCount": 7,
        "content": "I also vote for D. I passed my exam today and this question was there."
      },
      {
        "date": "2022-12-13T17:04:00.000Z",
        "voteCount": 1,
        "content": "You said passed the exam, How do I know which answer to take? Voting and the actual answer never match. You passed the exam using this question? Should I use community answer or most voted? Pls, help. I have an exam in 3 days.Thanks"
      },
      {
        "date": "2022-12-18T04:03:00.000Z",
        "voteCount": 1,
        "content": "how was your exam? what's your answer to this question that you asked, now that you have given it."
      },
      {
        "date": "2022-09-20T03:08:00.000Z",
        "voteCount": 2,
        "content": "its D , A and B are obviously incorrect"
      },
      {
        "date": "2022-09-11T19:59:00.000Z",
        "voteCount": 1,
        "content": "It\u2019s D"
      },
      {
        "date": "2022-09-03T06:58:00.000Z",
        "voteCount": 1,
        "content": "D is right"
      },
      {
        "date": "2022-08-28T08:40:00.000Z",
        "voteCount": 1,
        "content": "I have hands on expereince to answer this question."
      },
      {
        "date": "2022-08-24T23:23:00.000Z",
        "voteCount": 1,
        "content": "D is the Answer"
      },
      {
        "date": "2022-06-16T07:52:00.000Z",
        "voteCount": 1,
        "content": "D - not even a debate!"
      },
      {
        "date": "2022-06-13T11:13:00.000Z",
        "voteCount": 1,
        "content": "D perfect answer for given scenario."
      },
      {
        "date": "2022-05-24T08:08:00.000Z",
        "voteCount": 1,
        "content": "D!!!!!!"
      },
      {
        "date": "2022-05-03T17:25:00.000Z",
        "voteCount": 4,
        "content": "You can only create GKE instances for docker containers through container registry. D all the way"
      },
      {
        "date": "2022-05-01T12:36:00.000Z",
        "voteCount": 2,
        "content": "D is best for storing and deploying containers to GKE"
      },
      {
        "date": "2022-04-26T15:00:00.000Z",
        "voteCount": 3,
        "content": "Correct answer is D."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 193,
    "url": "https://www.examtopics.com/discussions/google/view/79798-exam-associate-cloud-engineer-topic-1-question-193/",
    "body": "You are using Data Studio to visualize a table from your data warehouse that is built on top of BigQuery. Data is appended to the data warehouse during the day.<br>At night, the daily summary is recalculated by overwriting the table. You just noticed that the charts in Data Studio are broken, and you want to analyze the problem. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReview the Error Reporting page in the Cloud Console to find any errors.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the BigQuery interface to review the nightly job and look for any errors.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Debugger to find out why the data was not refreshed correctly.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn Cloud Logging, create a filter for your Data Studio report."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 30,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 12,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 6,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-01-21T10:53:00.000Z",
        "voteCount": 14,
        "content": "B. Use the BigQuery interface to review the nightly job and look for any errors.\n\nSince the problem is related to the data in the data warehouse, it would be useful to check the status of the nightly job that recalculates the data and overwrites the table. By reviewing the job in the BigQuery interface, you can see if it completed successfully and if there were any errors that may have caused the charts in Data Studio to break. Reviewing the Error Reporting page in the Cloud Console, using Cloud Debugger and creating a filter in Cloud Logging may not be directly related to the problem with the data."
      },
      {
        "date": "2023-12-05T23:15:00.000Z",
        "voteCount": 5,
        "content": "for those who says 'C' is the answer : \nCloud Debugger was deprecated on May 16, 2022 and the service was shut down on May 31, 2023. You can continue to use the open source Snapshot Debugger. Snapshot Debugger was archived on September 7, 2023, so it is not receiving bug fixes or security patches. Snapshot Debugger remains available for use. You can also fork the repository and maintain your own version.\n\nCloud Debugger was deprecated on May 16, 2022 and the service was shut down on May 31, 2023. You can continue to use the open source Snapshot Debugger. Snapshot Debugger was archived on September 7, 2023, so it is not receiving bug fixes or security patches. Snapshot Debugger remains available for use. You can also fork the repository and maintain your own version.\n\nhttps://cloud.google.com/stackdriver/docs/deprecations/debugger-deprecation"
      },
      {
        "date": "2024-09-16T02:50:00.000Z",
        "voteCount": 1,
        "content": "So which on is the correct answer?"
      },
      {
        "date": "2024-09-11T20:31:00.000Z",
        "voteCount": 2,
        "content": "Since the issue involves a table in BigQuery, which is overwritten nightly with recalculated data, it's most likely that something went wrong during this nightly job. Reviewing the job in the BigQuery interface allows you to check for errors, job failures, or any issues during the table overwrite process, which could be causing the charts in Data Studio to break."
      },
      {
        "date": "2023-12-02T19:19:00.000Z",
        "voteCount": 1,
        "content": "Answer: C\nCloud Debugger provides targeted debugging capabilities for Data Studio, allowing you to focus on the specific report or charts that are experiencing issues."
      },
      {
        "date": "2023-11-02T23:17:00.000Z",
        "voteCount": 1,
        "content": "You should use the BigQuery interface to review the nightly job and look for any errors.\nThe reason for this is that the nightly job is responsible for recalculating the daily summary by overwriting the table. If there are any errors in the nightly job, it could cause the charts in Data Studio to be broken. By reviewing the nightly job, you can identify and fix any errors that may be causing the problem.\nThe other options are not as likely to be helpful in this situation. The Error Reporting page in the Cloud Console is not likely to be helpful because it will only show errors that have been reported by other users. The Cloud Debugger is not likely to be helpful because it is used to debug code, not to troubleshoot problems with data. The Cloud Logging filter is not likely to be helpful because it is used to filter logs, not to troubleshoot problems with data."
      },
      {
        "date": "2023-10-23T05:03:00.000Z",
        "voteCount": 1,
        "content": "B. Use the BigQuery interface to review the nightly job and look for any errors.\n\nSince the charts in Data Studio are broken, and the data is being appended to the data warehouse during the day and the daily summary is being recalculated at night by overwriting the table, the most likely cause of the problem is an error in the nightly job."
      },
      {
        "date": "2023-10-17T17:21:00.000Z",
        "voteCount": 1,
        "content": "The other options are less likely to provide the information you need for this specific situation:\n\nA. \"Review the Error Reporting page in the Cloud Console\" is more about application errors in code, rather than issues with BigQuery job executions.\n\nC. \"Use Cloud Debugger\" is not applicable here as it's used for debugging applications written in languages like Java, Python, etc., and doesn't work with BigQuery SQL or data processing tasks.\n\nD. \"In Cloud Logging, create a filter for your Data Studio report\" might not be helpful because, while Cloud Logging does capture a wide variety of logs, it doesn't provide the direct, detailed job execution information that the BigQuery interface does. Furthermore, Data Studio reporting errors may not be related to the underlying data processing job."
      },
      {
        "date": "2023-10-17T17:21:00.000Z",
        "voteCount": 2,
        "content": "B. Use the BigQuery interface to review the nightly job and look for any errors.\n\nHere\u2019s why this approach is appropriate:\n\nJob Information: BigQuery logs information about every job executed, including data loads or query jobs. If the nightly job that refreshes your data warehouse table encountered an error, this would be captured and could be viewed in the job's information in the BigQuery console.\n\nError Details: If the job failed or encountered issues, the BigQuery interface would provide details about the error, which can help you understand if there were problems with the query syntax, the data, or any other aspect of the operation.\n\nImmediate Feedback: Reviewing the job's execution details can give you immediate insights without needing to wait for logs or errors to propagate through other systems, which can be time-consuming and might not provide the direct feedback you need."
      },
      {
        "date": "2023-07-05T01:37:00.000Z",
        "voteCount": 4,
        "content": "It needs to be B because it is the only way to proper investigate the issue."
      },
      {
        "date": "2023-07-04T06:54:00.000Z",
        "voteCount": 2,
        "content": "The correct answer in this scenario would be option B: Use the BigQuery interface to review the nightly job and look for any errors.\n\nWhen the charts in Data Studio are broken after the nightly recalculation of the table, it indicates that there might be an issue with the data refresh or update process in BigQuery. By reviewing the nightly job in the BigQuery interface, you can check for any errors or anomalies that occurred during the recalculation process."
      },
      {
        "date": "2023-05-11T12:09:00.000Z",
        "voteCount": 1,
        "content": "I am sorry to say that there is no filter for Data Studio in Cloud Logging (see: https://cloud.google.com/logging/docs/api/v2/resource-list#resource-indexes). So the correct answer is B."
      },
      {
        "date": "2023-05-12T15:16:00.000Z",
        "voteCount": 1,
        "content": "You are correct. However you can use Cloud Logging to troubleshoot issues with Data Studio by looking at the logs of the data sources that are being used by your report. I think this is what D mean to be."
      },
      {
        "date": "2023-04-08T18:39:00.000Z",
        "voteCount": 1,
        "content": "I'll select D intuitively since I'm not using Data Studio and bigQuery that much"
      },
      {
        "date": "2023-04-05T13:00:00.000Z",
        "voteCount": 1,
        "content": "D makes sense."
      },
      {
        "date": "2023-03-15T08:54:00.000Z",
        "voteCount": 1,
        "content": "i think the answer is D"
      },
      {
        "date": "2023-02-19T20:13:00.000Z",
        "voteCount": 1,
        "content": "My ans: D"
      },
      {
        "date": "2023-01-08T05:26:00.000Z",
        "voteCount": 1,
        "content": "There might be many issues in Google Data Studio, the fastest process might be to check Cloud Logging. Hence option D should be correct."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 194,
    "url": "https://www.examtopics.com/discussions/google/view/79796-exam-associate-cloud-engineer-topic-1-question-194/",
    "body": "You have been asked to set up the billing configuration for a new Google Cloud customer. Your customer wants to group resources that share common IAM policies. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse labels to group resources that share common IAM policies.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse folders to group resources that share common IAM policies.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up a proper billing account structure to group IAM policies.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up a proper project naming structure to group IAM policies."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 30,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-09-09T19:12:00.000Z",
        "voteCount": 14,
        "content": "B for me\n\"Folders are used to group resources that share common IAM policies\"\nhttps://cloud.google.com/resource-manager/docs/creating-managing-folders"
      },
      {
        "date": "2022-10-09T12:35:00.000Z",
        "voteCount": 7,
        "content": "B is correct Answer,\n\nFolders are nodes in the Cloud Platform Resource Hierarchy. A folder can contain projects, other folders, or a combination of both. Organizations can use folders to group projects under the organization node in a hierarchy. For example, your organization might contain multiple departments, each with its own set of Google Cloud resources. Folders allow you to group these resources on a per-department basis. Folders are used to group resources that share common IAM policies. While a folder can contain multiple folders or resources, a given folder or resource can have exactly one parent.\n\nhttps://cloud.google.com/resource-manager/docs/creating-managing-folders"
      },
      {
        "date": "2023-11-04T02:27:00.000Z",
        "voteCount": 2,
        "content": "B: Folders in Google Cloud provide a way to organize resources hierarchically and apply common IAM (Identity and Access Management) policies at the folder level. This structure allows you to group resources together based on organizational needs, such as by department, teams, environments, or projects, and apply IAM policies to the entire group of resources within that folder.\n\nBy utilizing folders, you can effectively manage and enforce consistent access controls, policies, and permissions across multiple resources within the same folder, thereby fulfilling the customer's requirement of grouping resources with common IAM policies."
      },
      {
        "date": "2023-09-13T00:24:00.000Z",
        "voteCount": 1,
        "content": "B is correct Answer,"
      },
      {
        "date": "2022-10-20T19:57:00.000Z",
        "voteCount": 4,
        "content": "\"Folders are used to group resources that share common IAM policies.\""
      },
      {
        "date": "2022-09-18T05:56:00.000Z",
        "voteCount": 3,
        "content": "B is the answer.\n\nhttps://cloud.google.com/resource-manager/docs/access-control-folders#best-practices-folders-iam"
      },
      {
        "date": "2022-09-17T08:04:00.000Z",
        "voteCount": 2,
        "content": "Folders are used to group resources that share common IAM policies"
      },
      {
        "date": "2022-09-03T07:01:00.000Z",
        "voteCount": 1,
        "content": "B seems right to me"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 195,
    "url": "https://www.examtopics.com/discussions/google/view/79797-exam-associate-cloud-engineer-topic-1-question-195/",
    "body": "You have been asked to create robust Virtual Private Network (VPN) connectivity between a new Virtual Private Cloud (VPC) and a remote site. Key requirements include dynamic routing, a shared address space of 10.19.0.1/22, and no overprovisioning of tunnels during a failover event. You want to follow Google- recommended practices to set up a high availability Cloud VPN. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a custom mode VPC network, configure static routes, and use active/passive routing.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an automatic mode VPC network, configure static routes, and use active/active routing.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a custom mode VPC network, use Cloud Router border gateway protocol (BGP) routes, and use active/passive routing.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an automatic mode VPC network, use Cloud Router border gateway protocol (BGP) routes, and configure policy-based routing."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 29,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-09-25T17:57:00.000Z",
        "voteCount": 13,
        "content": "we need  custom mode vpc so subnets are not created automatically (the ip range is mentioned in the question) also we will need active/passive HA VPN (as it is not mentioned we will have to use more than one HA VPN gateway).  \n\nLinks : https://cloud.google.com/network-connectivity/docs/vpn/concepts/best-practices\nhttps://cloud.google.com/network-connectivity/docs/vpn/concepts/overview#active\nhttps://cloud.google.com/vpc/docs/vpc#subnet-ranges"
      },
      {
        "date": "2022-09-25T17:59:00.000Z",
        "voteCount": 5,
        "content": "Also for dynamic routing we need HA VPN\nLink:  https://cloud.google.com/network-connectivity/docs/vpn/concepts/choosing-networks-routing#dynamic-routing"
      },
      {
        "date": "2022-10-09T12:43:00.000Z",
        "voteCount": 5,
        "content": "C . Choose a Cloud VPN gateway that uses dynamic routing and the Border Gateway Protocol (BGP). Google recommends using HA VPN and deploying on-premises devices that support BGP.\n\nChoose the appropriate tunnel configuration\nChoose the appropriate tunnel configuration based on the number of HA VPN gateways:\n\nIf you have a single HA VPN gateway, use an active/passive tunnel configuration.\n\nIf you have more than one HA VPN gateway, use an active/active tunnel configuration.\n\nhttps://cloud.google.com/network-connectivity/docs/vpn/concepts/best-practices"
      },
      {
        "date": "2023-10-23T05:10:00.000Z",
        "voteCount": 2,
        "content": "C is correct."
      },
      {
        "date": "2023-09-09T20:25:00.000Z",
        "voteCount": 1,
        "content": "C is the correct answer as we need to make sure that the subnets are not being created automatically"
      },
      {
        "date": "2022-10-04T03:31:00.000Z",
        "voteCount": 2,
        "content": "c is the correct one"
      },
      {
        "date": "2022-09-21T17:17:00.000Z",
        "voteCount": 4,
        "content": "Google Cloud Router\nOn Google Cloud, dynamic routing can be established using Cloud Router. It exchanges network topology information through Border Gateway Protocol (BGP). Cloud Router advertises subnets from its VPC network to another router or gateway via BGP. This is great for setting up VPN between the cloud and on-prem, as topology changes automatically propagate with no manual intervention and higher redundancy for your systems.\n\n\nYou now have:\n\nDiscovery of remote networks\nMaintaining up-to-date routing information\nChoosing the best path to destination networks\nAbility to find a new best path if the current path is no longer available\nAnd a great side effect can be lower latency because Cloud Router learns routes through BGP which allows for optimal data paths to reach its destination, whether that be another network or a VPN gateway to on-premise. Cloud Router is also how Dedicated Interconnect can give you 10 gbp/s bandwidth between your cloud VPC and your peered on-premise data center."
      },
      {
        "date": "2022-09-16T06:05:00.000Z",
        "voteCount": 1,
        "content": "C might be right"
      },
      {
        "date": "2022-09-03T07:05:00.000Z",
        "voteCount": 1,
        "content": "I think it should be C as there is too much customisation."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 196,
    "url": "https://www.examtopics.com/discussions/google/view/82343-exam-associate-cloud-engineer-topic-1-question-196/",
    "body": "You are running multiple microservices in a Kubernetes Engine cluster. One microservice is rendering images. The microservice responsible for the image rendering requires a large amount of CPU time compared to the memory it requires. The other microservices are workloads that are optimized for n1-standard machine types. You need to optimize your cluster so that all workloads are using resources as efficiently as possible. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAssign the pods of the image rendering microservice a higher pod priority than the other microservices.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a node pool with compute-optimized machine type nodes for the image rendering microservice. Use the node pool with general-purpose machine type nodes for the other microservices.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the node pool with general-purpose machine type nodes for the image rendering microservice. Create a node pool with compute-optimized machine type nodes for the other microservices.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the required amount of CPU and memory in the resource requests specification of the image rendering microservice deployment. Keep the resource requests for the other microservices at the default."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-11-26T17:40:00.000Z",
        "voteCount": 3,
        "content": "B is right"
      },
      {
        "date": "2022-10-20T20:06:00.000Z",
        "voteCount": 1,
        "content": "B looks like  the right choice here"
      },
      {
        "date": "2022-09-29T10:44:00.000Z",
        "voteCount": 1,
        "content": "B. is the Answer"
      },
      {
        "date": "2022-09-22T17:32:00.000Z",
        "voteCount": 3,
        "content": "I agree B is the answer."
      },
      {
        "date": "2022-09-20T10:06:00.000Z",
        "voteCount": 1,
        "content": "B is the answer."
      },
      {
        "date": "2022-09-20T02:16:00.000Z",
        "voteCount": 4,
        "content": "C is not correct coz general purpose machine types will not suffice for image rendering.\nB is the most suitable answer."
      },
      {
        "date": "2022-09-15T12:38:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 197,
    "url": "https://www.examtopics.com/discussions/google/view/80022-exam-associate-cloud-engineer-topic-1-question-197/",
    "body": "Your organization has three existing Google Cloud projects. You need to bill the Marketing department for only their Google Cloud services for a new initiative within their group. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Verify that you are assigned the Billing Administrator IAM role for your organization's Google Cloud Project for the Marketing department. 2. Link the new project to a Marketing Billing Account.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Verify that you are assigned the Billing Administrator IAM role for your organization's Google Cloud account. 2. Create a new Google Cloud Project for the Marketing department. 3. Set the default key-value project labels to department:marketing for all services in this project.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Verify that you are assigned the Organization Administrator IAM role for your organization's Google Cloud account. 2. Create a new Google Cloud Project for the Marketing department. 3. Link the new project to a Marketing Billing Account.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Verify that you are assigned the Organization Administrator IAM role for your organization's Google Cloud account. 2. Create a new Google Cloud Project for the Marketing department. 3. Set the default key-value project labels to department:marketing for all services in this project."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 36,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 19,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 12,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-09-27T06:24:00.000Z",
        "voteCount": 20,
        "content": "I understand that the question implies the creation of a new project, however neither of the roles listed have that functionality. If you chose B you are choosing an answer that has a direct contradiction because the Billing Account Admin does not have the permissions to create a new project. Thus, I think it is better to assume the new initiative/project is already created or being created by someone else and your job is simply to link the project to the account which you do have the appropriate permissions to perform. \nA is my choice."
      },
      {
        "date": "2024-02-01T02:13:00.000Z",
        "voteCount": 3,
        "content": "Worth noting that an Organization Administrator doesn't have permissions to deal with billing, so I think C/D are no good:\nhttps://cloud.google.com/iam/docs/understanding-roles#resourcemanager.organizationAdmin"
      },
      {
        "date": "2022-09-04T03:28:00.000Z",
        "voteCount": 8,
        "content": "Between A&amp; B, Billing Administrator IAM role is either at the organisation level not project level. Hence A is out. C &amp; D doesn't make sense."
      },
      {
        "date": "2022-09-20T02:12:00.000Z",
        "voteCount": 1,
        "content": "The billing account administrator role can also be given at the project level. A is correct. Refer to this doc: https://cloud.google.com/billing/docs/how-to/billing-access"
      },
      {
        "date": "2022-09-22T04:06:00.000Z",
        "voteCount": 1,
        "content": "Correction I meant the billing account admin role can be given at the organisation or the billing account level."
      },
      {
        "date": "2023-11-19T15:08:00.000Z",
        "voteCount": 1,
        "content": "Option A is incorrect because the billing account admin scope is either the \"Organization level\" or the \"Billing account level\""
      },
      {
        "date": "2023-11-19T15:08:00.000Z",
        "voteCount": 1,
        "content": "Option A is incorrect because the billing account admin scope is either the \"Organization level\" or the \"Billing account level\" NOT the project level"
      },
      {
        "date": "2024-02-19T13:28:00.000Z",
        "voteCount": 1,
        "content": "According to gcp docs - https://cloud.google.com/iam/docs/understanding-roles#resourcemanager.organizationAdmin. Org Admin does not have this permission -&gt; resourcemanager.projects.create, necessary to create project. So C and D are out"
      },
      {
        "date": "2024-01-22T20:22:00.000Z",
        "voteCount": 1,
        "content": "By assigning the Organization Administrator IAM role, you will have the necessary permissions to manage the organization's Google Cloud resources. \n\nCreating a new project for the Marketing department ensures that their services are isolated and billed separately. \n\nLinking the new project to a Marketing Billing Account allows you to track and manage the billing specifically for the Marketing department's initiatives. \n\nSetting default key-value project labels to department:marketing for all services in this project (as mentioned in option D) is not necessary for billing purposes, but it can be helpful for organizing and categorizing resources within the project."
      },
      {
        "date": "2024-01-13T04:20:00.000Z",
        "voteCount": 2,
        "content": "it's C."
      },
      {
        "date": "2023-12-29T11:21:00.000Z",
        "voteCount": 2,
        "content": "it's C"
      },
      {
        "date": "2023-12-09T08:14:00.000Z",
        "voteCount": 2,
        "content": "Its C."
      },
      {
        "date": "2023-12-02T19:14:00.000Z",
        "voteCount": 2,
        "content": "Answer is C.\nBilling Administrator IAM: https://cloud.google.com/iam/docs/job-functions/billing\nOrganization Administrator IAM: https://cloud.google.com/iam/docs/understanding-roles\nBilling Administrator IAM\nThe Billing Administrator IAM role allows users to manage billing-related tasks for a specific project. These tasks include:\n-Viewing and managing billing accounts\n-Setting billing alerts\n-Configuring payment methods\n-Monitoring billing activity\n-Organization Administrator IAM\n\nThe Organization Administrator IAM role allows users to manage overall Google Cloud usage and resources within an organization. These tasks include:\n-Creating and deleting projects\n-Managing users and groups\n-Setting up IAM policies\n-Enabling and configuring Cloud services"
      },
      {
        "date": "2023-10-23T05:54:00.000Z",
        "voteCount": 4,
        "content": "C. Verify that you are assigned the Organization Administrator IAM role for your organization's Google Cloud account. 2. Create a new Google Cloud Project for the Marketing department. 3. Link the new project to a Marketing Billing Account.\n\nTo bill the Marketing department for only their Google Cloud services for a new initiative within their group, you need to create a new Google Cloud project for the Marketing department and link it to a Marketing Billing Account."
      },
      {
        "date": "2023-08-03T02:40:00.000Z",
        "voteCount": 2,
        "content": "Option C: This option is accurate. As an Organization Administrator, you have the authority to create new projects within the organization's Google Cloud account. After creating the new Google Cloud Project for the Marketing department, you can link the project to a specific Marketing Billing Account to ensure that only their services are billed under their initiative."
      },
      {
        "date": "2023-06-17T05:57:00.000Z",
        "voteCount": 2,
        "content": "The ans is A: Billing Admin cannot create a new project."
      },
      {
        "date": "2023-05-11T08:39:00.000Z",
        "voteCount": 1,
        "content": "C. doesn't provide the necessary permission to link a billing account to a projet.\nB. and D. don't provide the necessary permission to create a new projet.\nFurthermore, labels are used to filter resources in the billing breakdown. (See: https://cloud.google.com/compute/docs/labeling-resources#what-are-labels). Also, billing is set at a project level (See: https://cloud.google.com/billing/docs/how-to/modify-project#overview): \"A Cloud Billing account is used to define who pays for a given set of resources, and it can be linked to one or more projects. Project usage is charged to the linked Cloud Billing account.\"\nAnswer A. is the only correct option."
      },
      {
        "date": "2023-04-18T04:48:00.000Z",
        "voteCount": 2,
        "content": "Option A: no need to create a new project"
      },
      {
        "date": "2023-04-13T14:10:00.000Z",
        "voteCount": 2,
        "content": "C. 1. Verify that you are assigned the Organization Administrator IAM role for your organization's Google Cloud account. 2. Create a new Google Cloud Project for the Marketing department. 3. Link the new project to a Marketing Billing Account.\n\nThis approach ensures that you have the necessary permissions (Organization Administrator IAM role) to create a new project for the Marketing department. After creating the project, you'll link it to a Marketing Billing Account so that only their Google Cloud services are billed to their department. This way, you can separate costs for the Marketing department's new initiative from other projects in your organization."
      },
      {
        "date": "2023-04-05T13:06:00.000Z",
        "voteCount": 1,
        "content": "A makes sense."
      },
      {
        "date": "2023-01-26T20:44:00.000Z",
        "voteCount": 4,
        "content": "\"Your organization has three existing Google Cloud projects\". \nDon\u00b4t need to create anything....\nI go to A"
      },
      {
        "date": "2023-01-21T17:05:00.000Z",
        "voteCount": 1,
        "content": "D is da way to go, I think.\nAn Organization Administrator can create projects. Owner of a new project is its creator. Owner can add labels to the project, and this is exactly the requirement here: selective billing (filtering) via labels."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 198,
    "url": "https://www.examtopics.com/discussions/google/view/113199-exam-associate-cloud-engineer-topic-1-question-198/",
    "body": "You deployed an application on a managed instance group in Compute Engine. The application accepts Transmission Control Protocol (TCP) traffic on port 389 and requires you to preserve the IP address of the client who is making a request. You want to expose the application to the internet by using a load balancer. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExpose the application by using an external TCP Network Load Balancer.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExpose the application by using a TCP Proxy Load Balancer.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExpose the application by using an SSL Proxy Load Balancer.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExpose the application by using an internal TCP Network Load Balancer."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 23,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-01T23:10:00.000Z",
        "voteCount": 5,
        "content": "TCP Network Load Balancer: This type of load balancer operates at the network layer (Layer 4 of the OSI model). It is designed for routing TCP traffic and is well-suited for scenarios where you need to maintain the original source IP address of the client. This is crucial in your case since the application requires the preservation of the client's IP address."
      },
      {
        "date": "2024-03-02T10:38:00.000Z",
        "voteCount": 4,
        "content": "Those saying B are incorrect: \n- External TCP Network Load Balancers DO preserve the client's IP address. This is a core feature of this type of load balancer in Google Cloud.\n- While TCP Proxy Load Balancers also support client IP preservation, their primary strength lies in additional Layer 7 capabilities.\n- In the absence of requirements for advanced traffic manipulation at the application layer, the External TCP Network Load Balancer remains the best choice."
      },
      {
        "date": "2024-01-05T06:51:00.000Z",
        "voteCount": 2,
        "content": "How to preserve client IP in a Network Load Balancer TCP :\nhttps://cloud.google.com/load-balancing/docs/tcp/setting-up-tcp#proxy-protocol"
      },
      {
        "date": "2023-12-29T11:04:00.000Z",
        "voteCount": 1,
        "content": "A. External TCP Network Load Balancer: While it handles TCP traffic, it doesn't inherently preserve client IP addresses.\nC. SSL Proxy Load Balancer: This is primarily intended for encrypted SSL traffic, not general TCP traffic.\nD. Internal TCP Network Load Balancer: This is for internal traffic within a VPC, not for exposing applications to the internet."
      },
      {
        "date": "2024-03-02T10:39:00.000Z",
        "voteCount": 2,
        "content": "Incorrect. External TCP Network Load Balancers DO preserve the client's IP address."
      },
      {
        "date": "2023-12-09T08:19:00.000Z",
        "voteCount": 2,
        "content": "It is A, \nNote: Proxy-based load balancers send connections to the backends from different GFE or Envoy IP addresses. If you're using a form of authentication that relies on keeping track of the IP address that opened the first connection, and expects that same IP address to open the second connection, you might not want to use a proxy load balancer. Proxy load balancers don't preserve client IP addresses by default. This type of authentication is more compatible with the passthrough load balancers. For proxy load balancers such as the internal and external Application Load Balancers, we recommend that you use Identity-Aware Proxy (IAP) as your authentication method instead.\nhttps://cloud.google.com/load-balancing/docs/choosing-load-balancer#:~:text=Proxy%20load%20balancers%20do%20not%20preserve%20client%20IP"
      },
      {
        "date": "2023-12-05T15:27:00.000Z",
        "voteCount": 1,
        "content": "Answer is: A\nExternal proxy Network Load Balancers let you use a single IP address for all users worldwide. \n\nhttps://cloud.google.com/load-balancing/docs/tcp"
      },
      {
        "date": "2023-09-13T03:21:00.000Z",
        "voteCount": 2,
        "content": "Its A, for sure"
      },
      {
        "date": "2023-07-23T18:49:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/load-balancing/docs/choosing-load-balancer#:~:text=Proxy%20load%20balancers%20do%20not%20preserve%20client%20IP"
      },
      {
        "date": "2023-07-17T14:04:00.000Z",
        "voteCount": 1,
        "content": "I am going with A as the client IP needs to be preserved. Not sure with on2it votes once for A and once for B with the same comment especially because you need a *pass-through* load balancer to preserve the client IP as stated here: https://cloud.google.com/load-balancing/docs/choosing-load-balancer#proxy-pass-through \n\n\"You'd choose a passthrough Network Load Balancer to preserve client source IP addresses\""
      },
      {
        "date": "2023-07-04T06:06:00.000Z",
        "voteCount": 2,
        "content": "The correct answer in this scenario would be option B: Expose the application by using a TCP Proxy Load Balancer.\n\nA TCP Proxy Load Balancer is suitable for preserving the client's IP address when accepting TCP traffic on a specific port, such as port 389 in this case. When a client makes a request to the load balancer, the load balancer maintains the client's source IP address and forwards the traffic to the appropriate backend instances in the managed instance group. This allows the application to see the original client IP address and respond accordingly.\n\nOption A, using an external TCP Network Load Balancer, does not preserve the client's IP address. The load balancer's IP address is seen as the source IP by the application, which may not meet the requirement."
      },
      {
        "date": "2023-07-04T06:05:00.000Z",
        "voteCount": 1,
        "content": "The correct answer in this scenario would be option B: Expose the application by using a TCP Proxy Load Balancer.\n\nA TCP Proxy Load Balancer is suitable for preserving the client's IP address when accepting TCP traffic on a specific port, such as port 389 in this case. When a client makes a request to the load balancer, the load balancer maintains the client's source IP address and forwards the traffic to the appropriate backend instances in the managed instance group. This allows the application to see the original client IP address and respond accordingly.\n\nOption A, using an external TCP Network Load Balancer, does not preserve the client's IP address. The load balancer's IP address is seen as the source IP by the application, which may not meet the requirement."
      },
      {
        "date": "2023-07-02T10:32:00.000Z",
        "voteCount": 4,
        "content": "Anser is A! \nIf you are using a TCP/UDP network load balancer that preserves the client IP address (AWS Network Load Balancer, GCP External Network Load Balancer, Azure Load Balancer) or you are using Round-Robin DNS, then you can use the externalTrafficPolicy: Local setting to also preserve the client IP inside Kubernetes by bypassing kube-proxy and preventing it from sending traffic to other nodes."
      },
      {
        "date": "2023-07-01T10:50:00.000Z",
        "voteCount": 4,
        "content": "The answer has to be A: external TCP Network Load Balancer.\n\nFrom the Google doc \"Choose a load balancer\" (https://cloud.google.com/load-balancing/docs/choosing-load-balancer) it clearly states:\n\"You'd choose a passthrough (passthrough = non-proxy) Network Load Balancer to preserve client source IP addresses (...)\".\n\nI also found a source that shows how to preserves a client\u2019s IP address in the TCP payload in a Proxy TCP load balancer (https://medium.com/google-cloud/preserving-client-ips-through-google-clouds-global-tcp-and-ssl-proxy-load-balancers-3697d76feeb1)... with A LOT of effort... but with such a clear statement from the google doc, I would stick to A.\n\nBy the way, I think that Google made a real mess with the load balancer types. This is the only thing more simple in AWS than in GCP."
      },
      {
        "date": "2023-06-25T06:37:00.000Z",
        "voteCount": 1,
        "content": "In this scenario, using a TCP Proxy Load Balancer would be the most appropriate choice. A TCP Proxy Load Balancer operates at the transport layer (Layer 4) of the OSI model and can preserve the client's IP address while load balancing the TCP traffic to the backend instances.\n\nExternal TCP Network Load Balancer (option A) is designed for network-level load balancing, but it does not have built-in support for preserving the client's IP address. Therefore, it may not be suitable for this specific requirement."
      },
      {
        "date": "2023-06-28T12:51:00.000Z",
        "voteCount": 1,
        "content": "As part of the requirements it requires to perserve the ip address which is incompatible with the TCP proxy. Leaving option A as the only viable option: https://googlecloudarchitect.us/types-of-load-balancers-gcp/"
      },
      {
        "date": "2023-06-24T13:21:00.000Z",
        "voteCount": 1,
        "content": "A we need to perserve the client ip addresses: https://cloud.in28minutes.com/gcp-certification-google-cloud-load-balancers"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 199,
    "url": "https://www.examtopics.com/discussions/google/view/113246-exam-associate-cloud-engineer-topic-1-question-199/",
    "body": "You are building a multi-player gaming application that will store game information in a database. As the popularity of the application increases, you are concerned about delivering consistent performance. You need to ensure an optimal gaming performance for global users, without increasing the management complexity. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud SQL database with cross-region replication to store game statistics in the EU, US, and APAC regions.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Spanner to store user data mapped to the game statistics.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse BigQuery to store game statistics with a Redis on Memorystore instance in the front to provide global consistency.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore game statistics in a Bigtable database partitioned by username."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 5,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-08-22T08:27:00.000Z",
        "voteCount": 5,
        "content": "https://cloud.google.com/solutions/databases/games"
      },
      {
        "date": "2023-08-17T04:26:00.000Z",
        "voteCount": 5,
        "content": "global users = Cloud Spanner\nCorrect Answer is B"
      },
      {
        "date": "2024-03-02T10:42:00.000Z",
        "voteCount": 3,
        "content": "Option B, leveraging Cloud Spanner, provides a powerful solution specifically designed for globally distributed, consistently performant applications while keeping operational complexity low \u2013 making it the ideal choice for the multi-player gaming scenario.\n\nBigtable is well-suited for massive scale, but its NoSQL nature might require more data modeling effort compared to Cloud Spanner for gaming-related data."
      },
      {
        "date": "2024-02-20T15:00:00.000Z",
        "voteCount": 2,
        "content": "Bigtable"
      },
      {
        "date": "2023-12-29T10:11:00.000Z",
        "voteCount": 3,
        "content": "it's D because in this specific case cause Bigtable scales seamlessly to handle massive amounts of data and high read/write throughput,\n ideal for multiplayer gaming applications"
      },
      {
        "date": "2023-08-05T05:01:00.000Z",
        "voteCount": 2,
        "content": "Spanner should meet expectation"
      },
      {
        "date": "2023-06-25T06:41:00.000Z",
        "voteCount": 4,
        "content": "Among the options provided, the better answer for ensuring optimal gaming performance for global users without increasing management complexity would be option B\n\nCloud Spanner is a globally distributed, horizontally scalable database service provided by Google Cloud Platform. It offers strong consistency guarantees, high availability, and automatic scaling. \n\n It offers the necessary features to ensure optimal gaming performance, global scalability, strong consistency, and automatic scaling, making it a suitable choice for storing user data mapped to game statistics."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 200,
    "url": "https://www.examtopics.com/discussions/google/view/113200-exam-associate-cloud-engineer-topic-1-question-200/",
    "body": "You are building an application that stores relational data from users. Users across the globe will use this application. Your CTO is concerned about the scaling requirements because the size of the user base is unknown. You need to implement a database solution that can scale with your user growth with minimum configuration changes. Which storage solution should you use?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud SQL",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFirestore",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Spanner\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBigtable"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-09-18T08:02:00.000Z",
        "voteCount": 2,
        "content": "The key phrase is \"with minimum configurations changes\". Cloud Spanner is the best choice."
      },
      {
        "date": "2023-08-05T05:04:00.000Z",
        "voteCount": 3,
        "content": "and scales horizontally"
      },
      {
        "date": "2023-07-19T02:28:00.000Z",
        "voteCount": 3,
        "content": "Cloud Spanner because Relational database, scale across regions for workloads that have more stringent availability requirements, Handles large amounts of data and for high transactional consistency\n\nhttps://cloud.google.com/blog/topics/developers-practitioners/databases-google-cloud-part-2-options-glance/"
      },
      {
        "date": "2023-07-04T02:54:00.000Z",
        "voteCount": 3,
        "content": "spanner- relational and global"
      },
      {
        "date": "2023-07-01T01:58:00.000Z",
        "voteCount": 1,
        "content": "Spanner"
      },
      {
        "date": "2023-06-24T13:22:00.000Z",
        "voteCount": 1,
        "content": "I vote option C"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 201,
    "url": "https://www.examtopics.com/discussions/google/view/113247-exam-associate-cloud-engineer-topic-1-question-201/",
    "body": "Your company has multiple projects linked to a single billing account in Google Cloud. You need to visualize the costs with specific metrics that should be dynamically calculated based on company-specific criteria. You want to automate the process. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the Google Cloud console, visualize the costs related to the projects in the Reports section.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the Google Cloud console, visualize the costs related to the projects in the Cost breakdown section.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the Google Cloud console, use the export functionality of the Cost table. Create a Looker Studio dashboard on top of the CSV export.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Cloud Billing data export to BigQuery for the billing account. Create a Looker Studio dashboard on top of the BigQuery export.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 19,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-06-25T06:43:00.000Z",
        "voteCount": 6,
        "content": "Option D closely aligns with the requirements mentioned in the question.\n\nBy configuring Cloud Billing data export to BigQuery, you can automate the process of exporting billing data to a BigQuery dataset. You can then use Looker Studio, a data visualization and exploration platform, to create a dashboard on top of the BigQuery export. This allows you to visualize costs with specific metrics that can be dynamically calculated based on company-specific criteria."
      },
      {
        "date": "2023-09-03T18:36:00.000Z",
        "voteCount": 6,
        "content": "Cloud Billing export to BigQuery enables you to export detailed Google Cloud billing data (such as usage, cost estimates, and pricing data) automatically throughout the day to a BigQuery dataset that you specify. Then you can access your Cloud Billing data from BigQuery for detailed analysis, or use a tool like Looker Studio to visualize your data."
      },
      {
        "date": "2023-09-03T19:06:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/billing/docs/how-to/export-data-bigquery"
      },
      {
        "date": "2024-03-02T10:54:00.000Z",
        "voteCount": 2,
        "content": "The key here is that the cost visualization goes beyond the standard metrics, implying a need to manipulate and aggregate the raw data which makes D the best solution"
      },
      {
        "date": "2023-08-26T03:34:00.000Z",
        "voteCount": 3,
        "content": "D!\nKeys here are \"dynamically calculated\" and \"automate\"."
      },
      {
        "date": "2023-08-08T01:21:00.000Z",
        "voteCount": 1,
        "content": "why not A? can someone pls explain"
      },
      {
        "date": "2023-12-06T06:24:00.000Z",
        "voteCount": 1,
        "content": "Answer A is using default report with basic charts and trends, dose not customize as company-specific criteria. Therefore, answer D is correct."
      },
      {
        "date": "2023-07-20T05:29:00.000Z",
        "voteCount": 2,
        "content": "D is the right answer because you can automate process with export billing data to bigquery and then using the bigquery export to create the Looker studio dashboard"
      },
      {
        "date": "2023-07-20T05:31:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/billing/docs/how-to/export-data-bigquery"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 202,
    "url": "https://www.examtopics.com/discussions/google/view/113134-exam-associate-cloud-engineer-topic-1-question-202/",
    "body": "You have an application that runs on Compute Engine VM instances in a custom Virtual Private Cloud (VPC). Your company\u2019s security policies only allow the use of internal IP addresses on VM instances and do not let VM instances connect to the internet. You need to ensure that the application can access a file hosted in a Cloud Storage bucket within your project. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Private Service Access on the Cloud Storage Bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd storage.googleapis.com to the list of restricted services in a VPC Service Controls perimeter and add your project to the list of protected projects.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Private Google Access on the subnet within the custom VPC.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy a Cloud NAT instance and route the traffic to the dedicated IP address of the Cloud Storage bucket."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 22,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-08-03T20:29:00.000Z",
        "voteCount": 5,
        "content": "Private Google Access lets you connect VM instances to GCP services without external IP addresses and only internal. A is wrong because even though Private Services Access lets you also access GCP and other services through internal IPs, it also allows the VMs to have external IPs. \nhttps://cloud.google.com/vpc/docs/private-google-access"
      },
      {
        "date": "2023-09-09T05:19:00.000Z",
        "voteCount": 5,
        "content": "C is the correct Answer as Private Google Access allows you to the connect on the internal networks, A is incorrect becuause Cloud Storage bucket dont have such services to connect to Private Acesss`"
      },
      {
        "date": "2023-09-03T19:05:00.000Z",
        "voteCount": 5,
        "content": "Cloud Storage is not a supported service for Private Service Access. Hence, A cannot be the answer.\nhttps://cloud.google.com/vpc/docs/private-services-access#private-services-supported-services\n\nVM instances that only have internal IP addresses (no external IP addresses) can use Private Google Access. They can reach the external IP addresses of Google APIs and services. If you disable Private Google Access, the VM instances can no longer reach Google APIs and services; they can only send traffic within the VPC network.\nhttps://cloud.google.com/vpc/docs/private-google-access"
      },
      {
        "date": "2023-07-06T00:46:00.000Z",
        "voteCount": 3,
        "content": "Private Google Access is a VPC feature"
      },
      {
        "date": "2023-07-01T12:29:00.000Z",
        "voteCount": 1,
        "content": "C allows access to Google services &amp; API's"
      },
      {
        "date": "2023-06-23T13:08:00.000Z",
        "voteCount": 3,
        "content": "Right answer."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 203,
    "url": "https://www.examtopics.com/discussions/google/view/113112-exam-associate-cloud-engineer-topic-1-question-203/",
    "body": "Your company completed the acquisition of a startup and is now merging the IT systems of both companies. The startup had a production Google Cloud project in their organization. You need to move this project into your organization and ensure that the project is billed to your organization. You want to accomplish this task with minimal effort. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the projects.move method to move the project to your organization. Update the billing account of the project to that of your organization.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure that you have an Organization Administrator Identity and Access Management (IAM) role assigned to you in both organizations. Navigate to the Resource Manager in the startup\u2019s Google Cloud organization, and drag the project to your company's organization.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Private Catalog for the Google Cloud Marketplace, and upload the resources of the startup's production project to the Catalog. Share the Catalog with your organization, and deploy the resources in your company\u2019s project.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an infrastructure-as-code template for all resources in the project by using Terraform, and deploy that template to a new project in your organization. Delete the project from the startup\u2019s Google Cloud organization."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 23,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-06-25T06:30:00.000Z",
        "voteCount": 11,
        "content": "Option A is correct as it suggests using the \"projects.move\" method provided by Google Cloud to move the project from the startup's organization to your organization. This method allows you to transfer the ownership and control of a project to another organization. By moving the project, you can ensure that it is under your organization's management.\nWhile the other options contain elements that may be relevant in certain scenarios, they do not directly address the requirement of moving the project and ensuring billing to your organization."
      },
      {
        "date": "2023-11-07T16:55:00.000Z",
        "voteCount": 4,
        "content": "gcloud beta project move is the command with the flag of --organization= or --folder= with the IDs."
      },
      {
        "date": "2023-11-02T10:43:00.000Z",
        "voteCount": 1,
        "content": "Option A. \nHere is the command - gcloud beta projects move"
      },
      {
        "date": "2023-09-09T05:20:00.000Z",
        "voteCount": 1,
        "content": "A is correct Answer you can use the project move method to move the project to your organization"
      },
      {
        "date": "2023-09-03T19:17:00.000Z",
        "voteCount": 2,
        "content": "Here, A is the only choice because neither B nor C nor D be the answer.\nYou can move a project within an organization. But you need to migrate a project across organizations.\nhttps://cloud.google.com/resource-manager/docs/project-migration\nhttps://medium.com/google-cloud/migrating-a-project-from-one-organization-to-another-gcp-4b37a86dd9e6"
      },
      {
        "date": "2023-06-23T10:20:00.000Z",
        "voteCount": 4,
        "content": "Don't overthink it.\n\nhttps://cloud.google.com/resource-manager/docs/project-migration-checklist"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 204,
    "url": "https://www.examtopics.com/discussions/google/view/113114-exam-associate-cloud-engineer-topic-1-question-204/",
    "body": "All development (dev) teams in your organization are located in the United States. Each dev team has its own Google Cloud project. You want to restrict access so that each dev team can only create cloud resources in the United States (US). What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a folder to contain all the dev projects. Create an organization policy to limit resources in US locations.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an organization to contain all the dev projects. Create an Identity and Access Management (IAM) policy to limit the resources in US regions.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Identity and Access Management (IAM) policy to restrict the resources locations in the US. Apply the policy to all dev projects.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Identity and Access Management (IAM) policy to restrict the resources locations in all dev projects. Apply the policy to all dev roles."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 23,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 5,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-11-09T08:53:00.000Z",
        "voteCount": 6,
        "content": "Answer A\nAn organization policy configures a single constraint that restricts one or more Google Cloud services. The organization policy is set on an organization, folder, or project resource to enforce the constraint on that resource and any child resources.\nhttps://cloud.google.com/resource-manager/docs/organization-policy/overview"
      },
      {
        "date": "2024-10-15T05:39:00.000Z",
        "voteCount": 1,
        "content": "You can limit the physical location of a new resource with the Organization Policy Service resource locations constraint.\nhttps://cloud.google.com/resource-manager/docs/organization-policy/defining-locations"
      },
      {
        "date": "2023-10-24T07:46:00.000Z",
        "voteCount": 4,
        "content": "it's A : IAM is about WHO does WHAT\nOrganization policy is only about WHAT"
      },
      {
        "date": "2023-09-22T01:03:00.000Z",
        "voteCount": 4,
        "content": "Its A.\n\n\"Organization Policy\" does not indicate that it will be ONLY applied to a organization, it can be applied to any resource within a organization to restrict and add conditions. This policy focus on WHAT and not WHO (IAM). So, since in this case we want to restrict to VMs in US, its clearly the option A.\n\nLink: https://cloud.google.com/resource-manager/docs/organization-policy/overview\n\"Identity and Access Management focuses on who, and lets the administrator authorize who can take action on specific resources based on permissions.\"\n\n\"Organization Policy focuses on what, and lets the administrator set restrictions on specific resources to determine how they can be configured.\""
      },
      {
        "date": "2023-09-13T01:17:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/resource-manager/docs/organization-policy/defining-locations?hl=fr#gcloud"
      },
      {
        "date": "2023-09-09T05:24:00.000Z",
        "voteCount": 1,
        "content": "A is the correct answer as you can just add the projects to the same folder and just apply the Organization policy"
      },
      {
        "date": "2023-09-09T03:16:00.000Z",
        "voteCount": 1,
        "content": "The organization policy is set on an organization, folder, or project resource to enforce the constraint on that resource and any child resources. An organization policy contains one or more rules that specify how, and whether, to enforce the constraint.\n\nA"
      },
      {
        "date": "2023-09-03T19:41:00.000Z",
        "voteCount": 3,
        "content": "Restricting resources creation in specific locations can be achieved by Organization policy. The Organization Policy Service gives you centralized and programmatic control over your organization's cloud resources.\nYou can use \"Google Cloud Platform - Resource Location Restriction\" Organization policy constraint for the solution asked in the question.\nhttps://cloud.google.com/resource-manager/docs/organization-policy/overview\nhttps://cloud.google.com/resource-manager/docs/organization-policy/org-policy-constraints"
      },
      {
        "date": "2023-09-03T19:43:00.000Z",
        "voteCount": 1,
        "content": "Option A is mentioning to create a folder that contains all dev projects."
      },
      {
        "date": "2023-08-26T03:54:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/resource-manager/docs/access-control-folders\nA is wrong, The documentation says it needs a folder admin role to apply folder's IAM policy to limit resources access. A said organization level policies."
      },
      {
        "date": "2023-08-26T04:27:00.000Z",
        "voteCount": 2,
        "content": "OK, I changed my mind. A folder contains projects. Then mange the folder's IAM policies maybe can be considered as \"organization level\" .\n\nWTF, I feel I am getting an English certification."
      },
      {
        "date": "2023-08-17T05:12:00.000Z",
        "voteCount": 2,
        "content": "c as it cannot be A as it will apply to all folders"
      },
      {
        "date": "2023-08-15T03:53:00.000Z",
        "voteCount": 2,
        "content": "Organization policies are made up of constraints that allow you to:\n    Limit resource sharing based on domain.\n    Limit the usage of Identity and Access Management service accounts.\n    Restrict the physical location of newly created resources!!!!!!!!!\nhttps://cloud.google.com/resource-manager/docs/organization-policy/overview"
      },
      {
        "date": "2023-08-08T08:31:00.000Z",
        "voteCount": 2,
        "content": "C is the correct Answer,\nIt CANNOT be A as setting an organization policy will restrict every single project in the organization and not only the dev projects in the folder.\nIt CANNOT be B either, because projects can only even be created if an organization already exists."
      },
      {
        "date": "2023-07-23T08:06:00.000Z",
        "voteCount": 1,
        "content": "Option A"
      },
      {
        "date": "2023-06-25T06:27:00.000Z",
        "voteCount": 3,
        "content": "Option A is the most suitable answer among the provided choices. \nBy creating a folder to contain all the dev projects, you can organize them in a logical structure within your organization. Then, you can apply an organization policy to limit the resources in US locations. This policy can be configured to restrict the creation of cloud resources outside the United States. It provides a centralized approach to enforce the restriction across all the dev projects within the folder."
      },
      {
        "date": "2023-06-23T10:25:00.000Z",
        "voteCount": 2,
        "content": "You need to use \"Google Cloud Platform - Resource Location Restriction\"  organization policy.\n\nhttps://cloud.google.com/resource-manager/docs/organization-policy/org-policy-constraints"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 205,
    "url": "https://www.examtopics.com/discussions/google/view/113116-exam-associate-cloud-engineer-topic-1-question-205/",
    "body": "You are configuring Cloud DNS. You want to create DNS records to point home.mydomain.com, mydomain.com, and www.mydomain.com to the IP address of your Google Cloud load balancer. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate one CNAME record to point mydomain.com to the load balancer, and create two A records to point WWW and HOME to mydomain.com respectively.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate one CNAME record to point mydomain.com to the load balancer, and create two AAAA records to point WWW and HOME to mydomain.com respectively.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate one A record to point mydomain.com to the load balancer, and create two CNAME records to point WWW and HOME to mydomain.com respectively.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate one A record to point mydomain.com to the load balancer, and create two NS records to point WWW and HOME to mydomain.com respectively."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 22,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-09-03T19:55:00.000Z",
        "voteCount": 10,
        "content": "Record name               | Type      | Value\nmydomain.com           | A            | load balancer IP\nwww.mydomain.com  | CNAME | mydomain.com\nhome.mydomain.com | CNAME | mydomain.com\nhttps://cloud.google.com/dns/docs/records-overview#supported_dns_record_types"
      },
      {
        "date": "2024-07-28T03:16:00.000Z",
        "voteCount": 1,
        "content": "Selected Answer: C\nSimply, you can't create cname for thr root domain (mydomain.com). When you refe to other GCP services, it is a best practice to use cname/alais for the other records"
      },
      {
        "date": "2024-03-02T11:16:00.000Z",
        "voteCount": 1,
        "content": "Option C strikes the right balance between direct mapping with an A record for the root domain and using CNAME aliases for subdomains. This aligns with DNS best practices and simplifies management."
      },
      {
        "date": "2023-09-09T05:26:00.000Z",
        "voteCount": 3,
        "content": "C is the correct answer, as for this you need to create One A record and two CNAME records"
      },
      {
        "date": "2023-08-05T06:12:00.000Z",
        "voteCount": 1,
        "content": "A record is always for load balancer IP, then CNAME"
      },
      {
        "date": "2023-08-03T20:39:00.000Z",
        "voteCount": 2,
        "content": "C because the A record which points a domain name to an IPv4 address is used for the main domain (mydomain.com), the other two for www and home are aliases (CNAME) that can be pointed to the main domain of mydomain.com."
      },
      {
        "date": "2023-06-25T06:24:00.000Z",
        "voteCount": 4,
        "content": "Option A suggests creating one CNAME record to point mydomain.com to the load balancer, which is incorrect because CNAME records cannot coexist with other record types on the same domain/subdomain. In this case, you need to use an A record instead.\n\nOption B suggests creating two AAAA records, which are used for IPv6 addresses. Unless you specifically have an IPv6 address for your load balancer, using AAAA records would not be appropriate.\n\nOption D suggests creating two NS records, which are used for specifying the authoritative name servers for a domain. NS records are not used to point subdomains to IP addresses or load balancers.\n\nTherefore, option C is the correct answer, as it correctly suggests creating one A record to point mydomain.com to the load balancer, and two CNAME records to point WWW and HOME to mydomain.com respectively."
      },
      {
        "date": "2023-06-23T10:32:00.000Z",
        "voteCount": 1,
        "content": "You can only associate A(IP) record to a domain.\n\nhttps://cloud.google.com/dns/docs/set-up-dns-records-domain-name#create_a_record_to_point_the_domain_to_an_external_ip_address"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 206,
    "url": "https://www.examtopics.com/discussions/google/view/113117-exam-associate-cloud-engineer-topic-1-question-206/",
    "body": "You have two subnets (subnet-a and subnet-b) in the default VPC. Your database servers are running in subnet-a. Your application servers and web servers are running in subnet-b. You want to configure a firewall rule that only allows database traffic from the application servers to the database servers. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\u2022\tCreate service accounts sa-app and sa-db.<br>\u2022\tAssociate service account sa-app with the application servers and the service account sa-db with the database servers.<br>\u2022\tCreate an ingress firewall rule to allow network traffic from source service account sa-app to target service account sa-db.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\u2022\tCreate network tags app-server and db-server.<br>\u2022\tAdd the app-server tag to the application servers and the db-server tag to the database servers.<br>\u2022\tCreate an egress firewall rule to allow network traffic from source network tag app-server to target network tag db-server.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\u2022\tCreate a service account sa-app and a network tag db-server.<br>\u2022\tAssociate the service account sa-app with the application servers and the network tag db-server with the database servers.<br>\u2022\tCreate an ingress firewall rule to allow network traffic from source VPC IP addresses and target the subnet-a IP addresses.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\u2022\tCreate a network tag app-server and service account sa-db.<br>\u2022\tAdd the tag to the application servers and associate the service account with the database servers.<br>\u2022\tCreate an egress firewall rule to allow network traffic from source network tag app-server to target service account sa-db."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 18,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 7,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-09-03T20:54:00.000Z",
        "voteCount": 5,
        "content": "Both service accounts and network tags can be used for creating a Cloud Firewall rule. The prime word is \"to allow network traffic from app server to database server\" which is achievable by inbound/ingress rule and not egress rule.\nhttps://cloud.google.com/firewall/docs/firewalls#rule_assignment"
      },
      {
        "date": "2024-09-25T06:32:00.000Z",
        "voteCount": 1,
        "content": "A IS CORRECT"
      },
      {
        "date": "2024-09-18T09:36:00.000Z",
        "voteCount": 1,
        "content": "By specifying the source and target tags in the firewall rule, you can ensure that only traffic from instances with the app-server tag can reach instances with the db-server tag."
      },
      {
        "date": "2023-12-17T03:29:00.000Z",
        "voteCount": 2,
        "content": "the answer is A"
      },
      {
        "date": "2023-11-21T01:09:00.000Z",
        "voteCount": 3,
        "content": "for \"allow\" rules, restrict them to only allow specific virtual machines by specifying the virtual machine's service account\n\nRefer to https://cloud.google.com/firewall/docs/firewalls"
      },
      {
        "date": "2023-11-21T01:09:00.000Z",
        "voteCount": 2,
        "content": "Answer is A"
      },
      {
        "date": "2023-11-15T20:01:00.000Z",
        "voteCount": 1,
        "content": "Answer is B.\nNetwork tags can be simpler to manage, especially if the access control is based on broader categories (like application servers and database servers) rather than individual instances.\nIf you need fine-grained control and identity-based access, go with service accounts. If you prefer simplicity and broader categorization, network tags may be a suitable choice."
      },
      {
        "date": "2023-11-15T20:13:00.000Z",
        "voteCount": 1,
        "content": "Although what was previously stated is correct, I must correct it. The correct answer would be option A.  Creating an ingress firewall rule on the subnet where your database servers are located is the appropriate approach. This rule would control incoming traffic to the database servers, ensuring that only traffic from the specified application servers (identified by network tags or service accounts) is allowed."
      },
      {
        "date": "2023-09-09T05:28:00.000Z",
        "voteCount": 4,
        "content": "Answer A is correct as question demands traffic to application to database which can be only be achieved by the ingreess rule"
      },
      {
        "date": "2023-08-17T05:04:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer is A"
      },
      {
        "date": "2023-08-06T04:53:00.000Z",
        "voteCount": 1,
        "content": "From the TomFoot link\n'for example, allow my \u201capplication x\u201d servers to access my \u201cdatabase y.\u201d'"
      },
      {
        "date": "2023-08-03T20:59:00.000Z",
        "voteCount": 3,
        "content": "Even though you could use service accounts for firewall rules, why is B wrong? It seems to do what the question requests and is the standard method."
      },
      {
        "date": "2023-08-21T09:09:00.000Z",
        "voteCount": 3,
        "content": "Because we need an ingress firewall."
      },
      {
        "date": "2023-07-01T02:29:00.000Z",
        "voteCount": 1,
        "content": "Service accs can be used for firewall management."
      },
      {
        "date": "2023-06-30T13:42:00.000Z",
        "voteCount": 2,
        "content": "You can use service for firewall rules.\n\nhttps://cloud.google.com/blog/products/gcp/simplify-cloud-vpc-firewall-management-with-service-accounts"
      },
      {
        "date": "2023-06-30T12:24:00.000Z",
        "voteCount": 2,
        "content": "A seems to be the most appropriate: https://cloud.google.com/firewall/docs/firewalls"
      },
      {
        "date": "2023-06-30T12:14:00.000Z",
        "voteCount": 2,
        "content": "A seems to be the most appropriate: https://cloud.google.com/firewall/docs/firewalls"
      },
      {
        "date": "2023-06-23T10:38:00.000Z",
        "voteCount": 3,
        "content": "Service account? It doesn't make any sense. \nIt's clearly a firewall solution."
      },
      {
        "date": "2023-06-30T13:43:00.000Z",
        "voteCount": 2,
        "content": "You can use service for firewall rules.\n\nhttps://cloud.google.com/blog/products/gcp/simplify-cloud-vpc-firewall-management-with-service-accounts"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 207,
    "url": "https://www.examtopics.com/discussions/google/view/113120-exam-associate-cloud-engineer-topic-1-question-207/",
    "body": "Your team wants to deploy a specific content management system (CMS) solution to Google Cloud. You need a quick and easy way to deploy and install the solution. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSearch for the CMS solution in Google Cloud Marketplace. Use gcloud CLI to deploy the solution.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSearch for the CMS solution in Google Cloud Marketplace. Deploy the solution directly from Cloud Marketplace.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSearch for the CMS solution in Google Cloud Marketplace. Use Terraform and the Cloud Marketplace ID to deploy the solution with the appropriate parameters.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the installation guide of the CMS provider. Perform the installation through your configuration management system."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 12,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-08-03T21:00:00.000Z",
        "voteCount": 5,
        "content": "Fastest and easiest way to deploy a solution straight from the marketplace"
      },
      {
        "date": "2023-11-02T11:29:00.000Z",
        "voteCount": 1,
        "content": "Option B, so says all and they are all correct."
      },
      {
        "date": "2023-07-04T05:21:00.000Z",
        "voteCount": 2,
        "content": "Indeed directly from Cloud Marketplace"
      },
      {
        "date": "2023-07-01T02:37:00.000Z",
        "voteCount": 1,
        "content": "We can deploy it directly from Cloud Marketplace."
      },
      {
        "date": "2023-06-30T13:46:00.000Z",
        "voteCount": 1,
        "content": "I think B would be simple and fast."
      },
      {
        "date": "2023-06-23T10:40:00.000Z",
        "voteCount": 2,
        "content": "Key Words: Quick and Easy"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 208,
    "url": "https://www.examtopics.com/discussions/google/view/113121-exam-associate-cloud-engineer-topic-1-question-208/",
    "body": "You are working for a startup that was officially registered as a business 6 months ago. As your customer base grows, your use of Google Cloud increases. You want to allow all engineers to create new projects without asking them for their credit card information. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Billing account, associate a payment method with it, and provide all project creators with permission to associate that billing account with their projects.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGrant all engineers permission to create their own billing accounts for each new project.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tApply for monthly invoiced billing, and have a single invoice for the project paid by the finance team.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a billing account, associate it with a monthly purchase order (PO), and send the PO to Google Cloud."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-09-04T01:52:00.000Z",
        "voteCount": 2,
        "content": "Cloud Billing accounts pay for Google Cloud projects and Google Maps Platform projects. \n\nYou might be eligible to switch your account type to monthly invoiced billing if your business meets certain requirements. These requirements include, but aren't limited to the following:\n- You must be registered as a business for a minimum of one year.\n- You expect to spend a minimum of $40,000 a year on Google Cloud.\n- Invoiced billing must be available in your country.\nhttps://cloud.google.com/billing/docs/how-to/invoiced-billing\n\nSo A is the correct answer here."
      },
      {
        "date": "2023-07-01T02:42:00.000Z",
        "voteCount": 3,
        "content": "A seems the best among the rest."
      },
      {
        "date": "2023-06-25T06:34:00.000Z",
        "voteCount": 4,
        "content": "Option A is the better answer for the given scenario. It allows you to centralize billing and payment management while providing flexibility to project creators. By creating a billing account and associating a payment method with it, you establish a central source for billing and payment for all projects.\n\nGranting project creators permission to associate the billing account with their projects ensures that they can create projects without the need for their individual credit card information. This approach streamlines the process and avoids the hassle of collecting credit card details from each engineer.\n\nAdditionally, this option allows for easy monitoring and management of project costs through a single billing account, making it simpler to track expenses and allocate resources effectively."
      },
      {
        "date": "2023-06-24T13:18:00.000Z",
        "voteCount": 1,
        "content": "It's option A"
      },
      {
        "date": "2023-06-23T10:42:00.000Z",
        "voteCount": 1,
        "content": "Don't overthink."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 209,
    "url": "https://www.examtopics.com/discussions/google/view/116733-exam-associate-cloud-engineer-topic-1-question-209/",
    "body": "Your continuous integration and delivery (CI/CD) server can\u2019t execute Google Cloud actions in a specific project because of permission issues. You need to validate whether the used service account has the appropriate roles in the specific project.<br><br>What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOpen the Google Cloud console, and check the Identity and Access Management (IAM) roles assigned to the service account at the project or inherited from the folder or organization levels.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOpen the Google Cloud console, and check the organization policies.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOpen the Google Cloud console, and run a query to determine which resources this service account can access.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOpen the Google Cloud console, and run a query of the audit logs to find permission denied errors for this service account."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 10,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-11-27T21:51:00.000Z",
        "voteCount": 4,
        "content": "Its not a rocket science. its straightforward question"
      },
      {
        "date": "2023-11-03T18:21:00.000Z",
        "voteCount": 2,
        "content": "A is the answer"
      },
      {
        "date": "2023-09-13T06:10:00.000Z",
        "voteCount": 4,
        "content": "A it's really straightforward"
      },
      {
        "date": "2023-09-09T05:29:00.000Z",
        "voteCount": 1,
        "content": "A is the correct Answer"
      },
      {
        "date": "2023-08-03T08:35:00.000Z",
        "voteCount": 1,
        "content": "It's A from GPT"
      },
      {
        "date": "2023-07-31T14:04:00.000Z",
        "voteCount": 2,
        "content": "I go for A"
      },
      {
        "date": "2023-07-29T08:16:00.000Z",
        "voteCount": 1,
        "content": "C should be the correct answer here"
      },
      {
        "date": "2023-07-31T14:03:00.000Z",
        "voteCount": 1,
        "content": "Why not A?"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 210,
    "url": "https://www.examtopics.com/discussions/google/view/115514-exam-associate-cloud-engineer-topic-1-question-210/",
    "body": "Your team is using Linux instances on Google Cloud. You need to ensure that your team logs in to these instances in the most secure and cost efficient way. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAttach a public IP to the instances and allow incoming connections from the internet on port 22 for SSH.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the gcloud compute ssh command with the --tunnel-through-iap flag. Allow ingress traffic from the IP range 35.235.240.0/20 on port 22.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a third party tool to provide remote access to the instances.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a bastion host with public internet access. Create the SSH tunnel to the instance through the bastion host."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 19,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-09-09T05:30:00.000Z",
        "voteCount": 6,
        "content": "Common sense B is the correct answer , must safer from using the third party apps or the public id addresses"
      },
      {
        "date": "2024-10-12T17:24:00.000Z",
        "voteCount": 1,
        "content": "According to Gemini: In Google Cloud Platform (GCP), Identity-Aware Proxy (IAP) is a more secure alternative to bastion hosts for accessing private resources. IAP encrypts SSH connections end-to-end, so it can't inspect the contents of the session. IAP also provides access controls to reduce the risk of unauthorized access and data breaches. https://cloud.google.com/compute/docs/connect/ssh-best-practices/network-access#use-a-bastion-host"
      },
      {
        "date": "2024-09-19T00:05:00.000Z",
        "voteCount": 2,
        "content": "Why the others are not correct? \nBastion Host: While a bastion host can provide remote access, it introduces additional complexity and potential security risks.\nThird-Party Tools: Using third-party tools may add costs and introduce dependencies."
      },
      {
        "date": "2024-02-27T16:46:00.000Z",
        "voteCount": 1,
        "content": "One General Question:  Most of the cases the Answer provided for each questions in Exam Topic Differs from the  Answer comes as a result as part of discussion.   \nJust worried, since appearing ACE exam-Should we go with Answers what the group of people says (with highest percentage opted answer)?"
      },
      {
        "date": "2024-05-09T07:58:00.000Z",
        "voteCount": 5,
        "content": "Go with majority, the website might have the wrong answer but discussion and majority people mostly know the right answer."
      },
      {
        "date": "2023-11-27T21:54:00.000Z",
        "voteCount": 2,
        "content": "B is correct"
      },
      {
        "date": "2023-12-03T06:57:00.000Z",
        "voteCount": 2,
        "content": "I will be appearing for Ace in the upcoming week is this 255 questions will be enough to pass the exam"
      },
      {
        "date": "2023-12-06T07:07:00.000Z",
        "voteCount": 4,
        "content": "Please give us feedback when you are done"
      },
      {
        "date": "2023-12-11T00:43:00.000Z",
        "voteCount": 8,
        "content": "Yes it is enough to clear, 6-7 questions may come from outside exam topics but you can expect atleast 40 to come out of 246 questions."
      },
      {
        "date": "2023-08-06T05:21:00.000Z",
        "voteCount": 4,
        "content": "You can use Bastion if   \n\"You have a specific use case, like session recording, and you can't use IAP.\"\nhttps://cloud.google.com/compute/docs/connect/ssh-internal-ip"
      },
      {
        "date": "2023-08-27T07:17:00.000Z",
        "voteCount": 1,
        "content": "Thanks for that link but I think it is C, Although totally agree that Bastion comes 2nd in that table, no way all the user would have IP within this range 35.235.240.0/20!"
      },
      {
        "date": "2023-09-06T13:10:00.000Z",
        "voteCount": 2,
        "content": "\"allows ingress traffic from the IP range `35.235.240.0/20`. This range contains all IP addresses that IAP uses for TCP forwarding\"\nhttps://cloud.google.com/iap/docs/using-tcp-forwarding#create-firewall-rule"
      },
      {
        "date": "2023-08-05T11:49:00.000Z",
        "voteCount": 1,
        "content": "But the question states \"You need to ensure that your team logs in to these instances in the most secure and cost efficient way\"\nBastion is more secure than IAP but I'm not sure is more cost effective...\nHard to choose"
      },
      {
        "date": "2023-08-03T21:17:00.000Z",
        "voteCount": 4,
        "content": "Understood about IAP being a secure way to SSH but where did the \"Allow ingress traffic from the IP range 35.235.240.0/20 on port 22.\" come from and how does that fit in? The question had no details about it and the IP range seemed to come out of nowhere."
      },
      {
        "date": "2023-12-06T07:54:00.000Z",
        "voteCount": 2,
        "content": "35.235.240.0/20 is IP range of Cloud IAP for TCP forwarding, we need to allow ingress as the guideline below:\nhttps://cloud.google.com/iap/docs/using-tcp-forwarding#preparing_your_project_for_tcp_forwarding"
      },
      {
        "date": "2023-07-19T02:42:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/compute/docs/connect/ssh-using-iap#gcloud\naccording the documentation the correct answer is B"
      },
      {
        "date": "2023-07-17T06:06:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/compute/docs/connect/ssh-using-iap#gcloud"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 211,
    "url": "https://www.examtopics.com/discussions/google/view/115517-exam-associate-cloud-engineer-topic-1-question-211/",
    "body": "An external member of your team needs list access to compute images and disks in one of your projects. You want to follow Google-recommended practices when you grant the required permissions to this user. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a custom role, and add all the required compute.disks.list and compute.images.list permissions as includedPermissions. Grant the custom role to the user at the project level.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a custom role based on the Compute Image User role. Add the compute.disks.list to the includedPermissions field. Grant the custom role to the user at the project level.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a custom role based on the Compute Storage Admin role. Exclude unnecessary permissions from the custom role. Grant the custom role to the user at the project level.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGrant the Compute Storage Admin role at the project level."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 17,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 4,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-09-26T07:16:00.000Z",
        "voteCount": 3,
        "content": "You can't give B because, Image user will be able to use the Image to create resources. Only give list access"
      },
      {
        "date": "2023-09-13T06:21:00.000Z",
        "voteCount": 2,
        "content": "Its A. Give user ONLY the required permission."
      },
      {
        "date": "2023-09-09T05:32:00.000Z",
        "voteCount": 3,
        "content": "A is the correct answer , just create the custom role add all the required permissoons , give to the user"
      },
      {
        "date": "2023-08-27T01:16:00.000Z",
        "voteCount": 4,
        "content": "I have successfully created a custom role with compute.disks.list and compute.image.list permissions. I have also tried creating it based on the Compute Storage Admin role. However, you still need to select compute.disks.list and compute.image.list individually; all permissions are unchecked by default. So A fits fine."
      },
      {
        "date": "2023-08-06T05:30:00.000Z",
        "voteCount": 3,
        "content": "The key word is \"needs list access\",  so only A meets this requirement"
      },
      {
        "date": "2023-08-03T21:25:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/iam/docs/custom-roles-permissions-support - Both compute.disks.list and compute.images.list are available as permissions for custom roles. Makes more sense to make a new custom role than going off an admin one then adjusting it."
      },
      {
        "date": "2023-07-23T19:00:00.000Z",
        "voteCount": 4,
        "content": "Option B allows you to create a custom role that is based on the existing Compute Image User role, which already includes the necessary permissions for accessing compute images. Then, you add the compute.disks.list permission to the custom role's includedPermissions field to grant the user list access to compute disks as well. This ensures that the user has precisely the permissions needed for their specific tasks and nothing more, following the principle of least privilege."
      },
      {
        "date": "2023-07-23T08:19:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/sdk/gcloud/reference/compute/images/list\n\nhttps://cloud.google.com/compute/docs/reference/rest/v1/disks/list"
      },
      {
        "date": "2023-07-22T05:43:00.000Z",
        "voteCount": 2,
        "content": "Answer is B: Compute image user role provide permission to list and read images without having other permissions on the image. Granting this role at the project level gives users the ability to list all images in the project and create resources, such as instances and persistent disks, based on images in the project. Adding the compute.disks.list then meet all the question requirements"
      },
      {
        "date": "2023-07-18T17:55:00.000Z",
        "voteCount": 3,
        "content": "Tried this, could not find those permissions when I tried to create custom role directly, you need to create from the role"
      },
      {
        "date": "2023-07-21T07:51:00.000Z",
        "voteCount": 2,
        "content": "You're right, changing my answer to C."
      },
      {
        "date": "2023-07-17T06:16:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/compute/docs/access/iam"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 212,
    "url": "https://www.examtopics.com/discussions/google/view/115518-exam-associate-cloud-engineer-topic-1-question-212/",
    "body": "You are running a web application on Cloud Run for a few hundred users. Some of your users complain that the initial web page of the application takes much longer to load than the following pages. You want to follow Google\u2019s recommendations to mitigate the issue. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the minimum number of instances for your Cloud Run service to 3.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the concurrency number to 1 for your Cloud Run service.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the maximum number of instances for your Cloud Run service to 100.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate your web application to use the protocol HTTP/2 instead of HTTP/1.1."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 15,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-10-18T18:12:00.000Z",
        "voteCount": 9,
        "content": "This is a typical cold start problem. Cold starts happen when a serverless platform like Cloud Run needs to start a new instance to handle a request because no suitable instances are available. This startup time can cause a delay, which is noticeable to users, especially on the first page load."
      },
      {
        "date": "2023-11-02T13:15:00.000Z",
        "voteCount": 1,
        "content": "Both A and D looks good. \nBut going with A as it is worded as some users are impacted during startup, http/2 would help resolve more than what issue is worded here."
      },
      {
        "date": "2023-09-13T06:33:00.000Z",
        "voteCount": 3,
        "content": "Its A. Look at this link: https://cloud.google.com/run/docs/tips/general#optimize_performance\n\nYou'll see that one of Google's recommendations for improve performance and reduce \"cold starts\", is to set a minimum of instances."
      },
      {
        "date": "2023-09-07T06:40:00.000Z",
        "voteCount": 1,
        "content": "I checked the google recommendations for cloud run but they never mention HTTP/2. I am going with A"
      },
      {
        "date": "2023-08-09T19:54:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/functions/docs/configuring/min-instances\nEven though the initial # of instances present in the VM is not stated, setting a min amount of instances \"can further help you avoid cold starts and reduce application latency\"."
      },
      {
        "date": "2023-08-06T05:37:00.000Z",
        "voteCount": 1,
        "content": "min. # of instances should solve latency issue"
      },
      {
        "date": "2023-08-05T12:00:00.000Z",
        "voteCount": 2,
        "content": "I chose A too.\nThe question should be related to GCP products and how can you best configure them.\nI shouldn't be forced to change my app to use HTTP/2"
      },
      {
        "date": "2023-08-05T23:20:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/blog/topics/developers-practitioners/3-ways-optimize-cloud-run-response-times"
      },
      {
        "date": "2023-08-02T19:47:00.000Z",
        "voteCount": 1,
        "content": "I chosse A\nnotice ''some of user'' and ''initial web page slowly than other page'' \nbecause the some of user login the web without load and the cloud run scale the instance to 0 so that the users had to waiting the startup new instance sometimes."
      },
      {
        "date": "2023-07-20T08:36:00.000Z",
        "voteCount": 2,
        "content": "https://www.cloudflare.com/learning/performance/http2-vs-http1.1/\n\nI'm going with D too. \nB wouldn't help: https://cloud.google.com/run/docs/about-concurrency#concurrency-1\nC wouldn't help - setting a max won't increase speed ever.\nA would not necessarily help - there's no indication that the initial page is taking much longer just because there are too few instances. However, D would improve how things load, as per the first link I posted."
      },
      {
        "date": "2023-07-17T15:06:00.000Z",
        "voteCount": 1,
        "content": "It should be D. HTTP/2 helps to speed up app performance."
      },
      {
        "date": "2023-07-17T06:24:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/run/docs/configuring/min-instances"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 213,
    "url": "https://www.examtopics.com/discussions/google/view/116245-exam-associate-cloud-engineer-topic-1-question-213/",
    "body": "You are building a data lake on Google Cloud for your Internet of Things (IoT) application. The IoT application has millions of sensors that are constantly streaming structured and unstructured data to your backend in the cloud. You want to build a highly available and resilient architecture based on Google-recommended practices. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStream data to Pub/Sub, and use Dataflow to send data to Cloud Storage.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStream data to Pub/Sub, and use Storage Transfer Service to send data to BigQuery.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStream data to Dataflow, and use Dataprep by Trifacta to send data to Bigtable.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStream data to Dataflow, and use Storage Transfer Service to send data to BigQuery."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 19,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-07-23T19:02:00.000Z",
        "voteCount": 11,
        "content": "A. Streaming data to Pub/Sub allows you to decouple the ingestion of data from the processing and storage, providing a scalable and reliable message queue that can handle the high volume of data coming from millions of sensors.\n\nUsing Dataflow to consume data from Pub/Sub and send it to Cloud Storage allows for real-time data processing and storage. Dataflow is a fully managed service for processing data in real-time or batch mode, making it an ideal choice for handling the constant stream of data from IoT sensors.\n\nStoring data in Cloud Storage offers high durability and availability, providing a robust foundation for building a data lake. Cloud Storage is a scalable object storage service that can handle large volumes of structured and unstructured data, making it well-suited for the IoT application's data requirements."
      },
      {
        "date": "2023-11-02T13:32:00.000Z",
        "voteCount": 5,
        "content": "Pub/Sub , Dataflow and BigTable would have been idle solution, but since Cloud Storage is the only option with that combo, I will go with A."
      },
      {
        "date": "2023-11-04T08:01:00.000Z",
        "voteCount": 2,
        "content": "ideal*"
      },
      {
        "date": "2023-11-02T02:59:00.000Z",
        "voteCount": 1,
        "content": "Accordingly to: \nhttps://cloud.google.com/learn/what-is-a-data-lake\n\"Related products and services\nGoogle Cloud offers a suite of autoscaling services that enable you to build a data lake that integrates with your existing applications, skills, and IT investments. This includes Dataflow and Cloud Data Fusion for data ingestion, [[[Cloud Storage]]] for storage, and Dataproc and BigQuery for data and analytics processing. \"\nGoogle topics only mentions Cloud Storage for such system, so A IS CORRECT."
      },
      {
        "date": "2023-09-13T06:36:00.000Z",
        "voteCount": 1,
        "content": "Its A, for sure."
      },
      {
        "date": "2023-09-09T05:35:00.000Z",
        "voteCount": 3,
        "content": "A is the correct answer a there is both unstuctured and strucutder data"
      },
      {
        "date": "2023-08-06T06:03:00.000Z",
        "voteCount": 2,
        "content": "according to https://cloud.google.com/architecture/optimized-large-scale-analytics-ingestion"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 214,
    "url": "https://www.examtopics.com/discussions/google/view/115341-exam-associate-cloud-engineer-topic-1-question-214/",
    "body": "You are running out of primary internal IP addresses in a subnet for a custom mode VPC. The subnet has the IP range 10.0.0.0/20, and the IP addresses are primarily used by virtual machines in the project. You need to provide more IP addresses for the virtual machines. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a secondary IP range 10.1.0.0/20 to the subnet.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the subnet IP range from 10.0.0.0/20 to 10.0.0.0/18.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the subnet IP range from 10.0.0.0/20 to 10.0.0.0/22.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConvert the subnet IP range from IPv4 to IPv6."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 18,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 8,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-09-22T02:50:00.000Z",
        "voteCount": 5,
        "content": "This one is tricky. First i was going with B, then i did some search. Option A and B can indeed add more IPs. However, i think the option is A because between those 2 options the option A we will add IPs without changing the any ours VMs configurations that we currently have. If we choose B might need to change our current VMs configuration in order to reflect the new IP range expanded. You guys understand what i mean?\n\nLink: https://cloud.google.com/vpc/docs/create-modify-vpc-networks#expand-subnet\n\n\"If you expand the primary IPv4 range of a subnet, you might need to modify other configurations that are assuming this IP address range\""
      },
      {
        "date": "2024-05-21T03:44:00.000Z",
        "voteCount": 1,
        "content": "You are wrong, with new subnet range, we dont have to reflect anything to existing vms."
      },
      {
        "date": "2023-10-30T14:26:00.000Z",
        "voteCount": 7,
        "content": "What are the configurations needed? \nFirst of all the CIDR Range 10.0.0.0/18 Include the CIDR range 10.0.0.0/20 and this is a mandatory step for adjusting the Primary IP CIDR Range so No change needed on the machine level."
      },
      {
        "date": "2023-10-31T07:32:00.000Z",
        "voteCount": 1,
        "content": "you will need to update the gateway IP of all the servers in 10.0.0.0/20, while changing to 10.0.0.0/18. So adding a new subnet makes sense."
      },
      {
        "date": "2023-10-31T07:34:00.000Z",
        "voteCount": 3,
        "content": "i was wrong, GW does not change. B seems correct"
      },
      {
        "date": "2023-12-08T11:29:00.000Z",
        "voteCount": 1,
        "content": "But the subnet mask will change"
      },
      {
        "date": "2024-04-05T08:17:00.000Z",
        "voteCount": 2,
        "content": "Purpose of Secondary IP Range: \"Adding a secondary IP range allows you to assign additional IP addresses to instances in a subnet without changing the subnet's primary IP range. This can be useful when you want to segregate traffic or allocate specific IP addresses to certain types of instances or workloads within the same subnet.\"\nIn the question, no where the logical separation of vm or traffic segregation is mentioned so by expanding th eprimary ip range will increase the available ips so Option B is correct."
      },
      {
        "date": "2024-03-02T11:56:00.000Z",
        "voteCount": 2,
        "content": "Option B offers a practical and scalable solution to address the shortage of IP addresses by enlarging the subnet's range in the most efficient way. \n\nOption A, while technically feasible, managing multiple IP ranges within a subnet adds complexity and can potentially lead to routing issues. Even with a secondary IP range, you'll likely need to either:\n1. Configure VMs with multiple network interfaces, each assigned an IP from a different range. This adds management overhead.\n2. Re-configure existing VMs with IPs from the new secondary range, potentially causing downtime or requiring complex IP address changes."
      },
      {
        "date": "2024-02-17T06:54:00.000Z",
        "voteCount": 1,
        "content": "I think that if we go with option B is OK but we need to configure the new mask on VMs\nIf we go with option A, for me is OK but VMs are not in the same LAN, we need to configure connectivity but nothing related with that requirement in the question...so maybe A is more accurate"
      },
      {
        "date": "2024-02-11T21:33:00.000Z",
        "voteCount": 1,
        "content": "B can not be the answer we can use B if the IP address are primarily used by Interfaces or services with in VM"
      },
      {
        "date": "2024-01-02T09:35:00.000Z",
        "voteCount": 1,
        "content": "A. Add a secondary IP range 10.1.0.0/20 to the subnet:\n\u2022\tThis option involves adding a secondary IP range to the existing subnet. This can provide additional IP addresses without changing the existing primary IP range.\nB. Change the subnet IP range from 10.0.0.0/20 to 10.0.0.0/18:\n\u2022\tThis involves expanding the current subnet's CIDR range to a larger block (from /20 to /18). This expansion will significantly increase the number of available IP addresses.\n\u2022\tHowever, changing the CIDR block of an existing subnet is not straightforward in GCP. It typically requires creating a new subnet with the desired range and migrating resources, which can be complex and disruptive."
      },
      {
        "date": "2023-11-27T12:49:00.000Z",
        "voteCount": 4,
        "content": "All subnets have a primary CIDR range, which is the range of internal IP addresses that define the subnet. Each VM instance gets its primary internal IP address from this range. You can also allocate alias IP ranges from that primary range, or you can add a secondary range to the subnet and allocate alias IP ranges from the secondary range. Use of alias IP ranges does not require secondary subnet ranges. These secondary subnet ranges merely provide an organizational tool.\n\nhttps://cloud.google.com/vpc/docs/alias-ip"
      },
      {
        "date": "2023-11-13T22:28:00.000Z",
        "voteCount": 3,
        "content": "A is the Answer\n(1st) we can use secondary IP range, since we are talking about VMs.\n You can optionally add secondary IP address ranges to a subnet, which are only used by alias IP ranges. However, you can configure alias IP ranges for instances from the primary or secondary range of a subnet.\n\nEach primary or secondary IPv4 range for all subnets in a VPC network must be a unique valid CIDR block.\n(2nd) 10.1.0.0/20 is a valid block and it will not overlap with 10.0.0.0/20 ( The range is 10.0.0-15.0-255).\n\nRemember the keywords from the question \"primary\" and \"ip add for the VM)"
      },
      {
        "date": "2023-09-09T05:35:00.000Z",
        "voteCount": 2,
        "content": "B is the correct answer"
      },
      {
        "date": "2023-09-05T05:47:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/vpc/docs/create-modify-vpc-networks#expand-subnet\nhttps://cloud.google.com/sdk/gcloud/reference/compute/networks/subnets/expand-ip-range"
      },
      {
        "date": "2023-08-16T14:51:00.000Z",
        "voteCount": 1,
        "content": "The ask is to add more IPs, it's not about to keep the VM ins same subnet or to keep communication open between existing and new VMs, so the answer should be A..."
      },
      {
        "date": "2023-09-13T06:40:00.000Z",
        "voteCount": 3,
        "content": "By changing the IP range to ..18 it will add more IPs to the subnet. So the answer B is correct."
      },
      {
        "date": "2023-08-06T06:15:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/vpc/docs/create-modify-vpc-networks#expand-subnet"
      },
      {
        "date": "2023-07-23T19:03:00.000Z",
        "voteCount": 1,
        "content": "Clearly B"
      },
      {
        "date": "2023-07-23T08:23:00.000Z",
        "voteCount": 3,
        "content": "Should be B for subnet expansion"
      },
      {
        "date": "2023-07-20T08:30:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/sdk/gcloud/reference/compute/networks/subnets/expand-ip-range\nhttps://cloud.google.com/vpc/docs/create-modify-vpc-networks#expand-subnet\nhttps://techlibrary.hpe.com/docs/otlink-wo/CIDR-Conversion-Table.html\n\nExpanding the subnet to a smaller prefix will allow more primary internal IP addresses. I don't know why gw2100 \"withdraws\" his vote for B and changes it to A - it's possible to add a secondary IP range as in answer A, but expanding the initial range as in B would work just fine.\n\nC is shrinking the range - cannot work.\n\nNot sure about D"
      },
      {
        "date": "2023-07-15T18:52:00.000Z",
        "voteCount": 1,
        "content": "should be B.  you expand the ip with a bigger network"
      },
      {
        "date": "2023-07-15T18:54:00.000Z",
        "voteCount": 3,
        "content": "I withdraw my vote.  It should be A."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 215,
    "url": "https://www.examtopics.com/discussions/google/view/116246-exam-associate-cloud-engineer-topic-1-question-215/",
    "body": "Your company requires all developers to have the same permissions, regardless of the Google Cloud project they are working on. Your company\u2019s security policy also restricts developer permissions to Compute Engine, Cloud Functions, and Cloud SQL. You want to implement the security policy with minimal effort. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\u2022\tCreate a custom role with Compute Engine, Cloud Functions, and Cloud SQL permissions in one project within the Google Cloud organization.<br>\u2022\tCopy the role across all projects created within the organization with the gcloud iam roles copy command.<br>\u2022\tAssign the role to developers in those projects.<br>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\u2022\tAdd all developers to a Google group in Google Groups for Workspace.<br>\u2022\tAssign the predefined role of Compute Admin to the Google group at the Google Cloud organization level.<br>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\u2022\tAdd all developers to a Google group in Cloud Identity.<br>\u2022\tAssign predefined roles for Compute Engine, Cloud Functions, and Cloud SQL permissions to the Google group for each project in the Google Cloud organization.<br>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\u2022\tAdd all developers to a Google group in Cloud Identity.<br>\u2022\tCreate a custom role with Compute Engine, Cloud Functions, and Cloud SQL permissions at the Google Cloud organization level.<br>\u2022\tAssign the custom role to the Google group.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-03-02T11:59:00.000Z",
        "voteCount": 3,
        "content": "D combines the security of a custom role tailored to the company's policy with the ease of management provided by organization-level assignment to a Cloud Identity group."
      },
      {
        "date": "2024-02-17T07:01:00.000Z",
        "voteCount": 2,
        "content": "Best practise is to use predefined roles but in this case we need to apply some restrictions about our company's security policy so I think D is the valid response."
      },
      {
        "date": "2023-09-13T06:55:00.000Z",
        "voteCount": 4,
        "content": "I vote for D"
      },
      {
        "date": "2023-09-13T06:55:00.000Z",
        "voteCount": 1,
        "content": "I vote for D"
      },
      {
        "date": "2023-09-09T05:37:00.000Z",
        "voteCount": 2,
        "content": "d is the correct answer"
      },
      {
        "date": "2023-09-05T05:50:00.000Z",
        "voteCount": 1,
        "content": "Permissions provided at the Organization level are inherited to the folder level and project level."
      },
      {
        "date": "2023-08-18T08:19:00.000Z",
        "voteCount": 3,
        "content": "Use predefined roles: Use predefined roles, such as \u201cEditor\u201d or \u201cViewer\u201d, instead of creating custom roles. This makes it easier to understand the level of access associated with a role.Use custom roles: Create custom roles when predefined roles do not meet the specific needs of your organization.  \nIn the link below:\n\nhttps://cloud.google.com/iam/docs/roles-overview#custom\n\nWhen to use custom roles\nIn most situations, you should be able to use predefined roles instead of custom roles. Predefined roles are maintained by Google, and are updated automatically when new permissions, features, or services are added to Google Cloud. In contrast, custom roles are not maintained by Google; when Google Cloud adds new permissions, features, or services, your custom roles will not be updated automatically."
      },
      {
        "date": "2023-08-06T06:29:00.000Z",
        "voteCount": 1,
        "content": "only D meets best practices"
      },
      {
        "date": "2023-07-23T19:08:00.000Z",
        "voteCount": 3,
        "content": "https://www.cloudskillsboost.google/focuses/1035?parent=catalog#:~:text=custom%20role%20at%20the%20organization%20level"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 216,
    "url": "https://www.examtopics.com/discussions/google/view/116247-exam-associate-cloud-engineer-topic-1-question-216/",
    "body": "You are working for a hospital that stores its medical images in an on-premises data room. The hospital wants to use Cloud Storage for archival storage of these images. The hospital wants an automated process to upload any new medical images to Cloud Storage. You need to design and implement a solution. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Pub/Sub topic, and enable a Cloud Storage trigger for the Pub/Sub topic. Create an application that sends all medical images to the Pub/Sub topic.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a script that uses the gcloud storage command to synchronize the on-premises storage with Cloud Storage, Schedule the script as a cron job.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Pub/Sub topic, and create a Cloud Function connected to the topic that writes data to Cloud Storage. Create an application that sends all medical images to the Pub/Sub topic.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the Google Cloud console, go to Cloud Storage. Upload the relevant images to the appropriate bucket."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 24,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 7,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-07-23T19:11:00.000Z",
        "voteCount": 7,
        "content": "Option C is more robust and utilises the GCP functionalities correctly."
      },
      {
        "date": "2023-08-05T12:04:00.000Z",
        "voteCount": 4,
        "content": "You can not send images to a Pub/Sub topic."
      },
      {
        "date": "2024-01-14T03:56:00.000Z",
        "voteCount": 5,
        "content": "Same as No.168."
      },
      {
        "date": "2024-03-02T12:11:00.000Z",
        "voteCount": 4,
        "content": "B makes the most sense. Using the gcloud storage command to synchronize offers a straightforward way to mirror the on-premises data with Cloud Storage. Cron jobs are a well-established mechanism for scheduling recurring tasks, ensuring the synchronization process runs according to the hospital's needs. This approach is relatively easy to set up and maintain, especially for on-premises systems where installing additional software might be restricted."
      },
      {
        "date": "2024-01-02T09:47:00.000Z",
        "voteCount": 3,
        "content": "A. Pub/Sub topic with a Cloud Storage trigger:\n\u2022\tGoogle Cloud Pub/Sub is a messaging service for exchanging event data among applications and services. However, creating a Cloud Storage trigger for a Pub/Sub topic isn't feasible as triggers usually work the other way around (Cloud Storage events triggering Pub/Sub messages).\n\u2022\tThis option would not provide a direct or automated way to upload images from on-premises storage to Cloud Storage."
      },
      {
        "date": "2024-01-02T09:47:00.000Z",
        "voteCount": 3,
        "content": ". Script using gcloud storage command and cron job:\n\u2022\tThis approach involves writing a script that synchronizes the on-premises storage with Cloud Storage using the gcloud storage command.\n\u2022\tScheduling this script as a cron job would automate the process, allowing for regular uploads of new images without manual intervention.\n\u2022\tThis is a straightforward approach and aligns well with the requirement of automated archival storage."
      },
      {
        "date": "2024-01-02T09:52:00.000Z",
        "voteCount": 3,
        "content": "gcloud storage Command: there is no gcloud storage command in the Google Cloud SDK. The gcloud tool does not directly provide a functionality for synchronizing files to Google Cloud Storage.\n\nCorrect Tool - gsutil: The correct tool to use for synchronizing files with Google Cloud Storage is gsutil, specifically the gsutil rsync command. gsutil is indeed part of the Google Cloud SDK, but it's a separate tool from gcloud."
      },
      {
        "date": "2024-01-02T09:53:00.000Z",
        "voteCount": 4,
        "content": "I still go with B, because it's the closest answer."
      },
      {
        "date": "2023-12-31T10:47:00.000Z",
        "voteCount": 1,
        "content": "Per ChatGPT, \nOption A is the most appropriate solution for the hospital's requirement as it provides an automated, scalable, and event-driven approach for uploading new medical images from the on-premises data room to Cloud Storage."
      },
      {
        "date": "2024-03-02T12:08:00.000Z",
        "voteCount": 1,
        "content": "You can't use ChatGCP with Google questions! Gemini all the way ;)\n\nThat being said, B is the best solution. It provides a reliable, scheduled, and easy-to-manage solution that aligns perfectly with the hospital's need to automate archival storage of medical images in Cloud Storage. Introducing Pub/Sub and an additional application adds complexity. While Pub/Sub can be useful for event-driven architectures, it's overkill for this basic synchronization."
      },
      {
        "date": "2024-03-02T12:08:00.000Z",
        "voteCount": 1,
        "content": "ChatGPT... I've been writing GCP too many times today lol"
      },
      {
        "date": "2023-12-10T04:49:00.000Z",
        "voteCount": 2,
        "content": "Its B, replicated question."
      },
      {
        "date": "2023-12-06T03:09:00.000Z",
        "voteCount": 4,
        "content": "Hello, guys. the exact same question was Question. 91. \nand the answer was : \n-  Create a script that uses the gsutil command line interface to synchronize the on-premises storage with Cloud Storage. Schedule the script as a cron job.\n\nSo I would vote for B"
      },
      {
        "date": "2023-11-15T21:06:00.000Z",
        "voteCount": 1,
        "content": "The answer is C.  The hint is \"The hospital wants an automated process to upload any new medical images to Cloud Storage\".  By creating a Cloud Function connected to the topic, you can write a serverless function that automatically executes when a new message is published to the topic. The Cloud Function can then write the data to Cloud Storage, which is a durable and cost-effective storage service for archival purposes."
      },
      {
        "date": "2023-09-13T07:34:00.000Z",
        "voteCount": 2,
        "content": "This question already appear before. But before the B option was with \"gsutil\" command, now is with the \"gcloud\" storage command. Despite this difference i believe the answer is still B cause this new command is more or less the same as the \"gsutil\". Look at these link:\nhttps://cloud.google.com/blog/products/storage-data-transfer/new-gcloud-storage-cli-for-your-data-transfers"
      },
      {
        "date": "2023-08-15T05:01:00.000Z",
        "voteCount": 1,
        "content": "B is not correct , as it states using gcloud command. One must use gsutil instead.\nIMO, C is correct."
      },
      {
        "date": "2023-08-16T12:06:00.000Z",
        "voteCount": 1,
        "content": "I believe that you could use \"gcloud storage cp\"  command as alternative to gsutil \n# https://cloud.google.com/sdk/gcloud/reference/storage/cp\n# https://cloud.google.com/blog/products/storage-data-transfer/new-gcloud-storage-cli-for-your-data-transfers"
      },
      {
        "date": "2023-08-06T06:31:00.000Z",
        "voteCount": 2,
        "content": "Repeated question"
      },
      {
        "date": "2023-08-02T06:35:00.000Z",
        "voteCount": 4,
        "content": "THIS IS B"
      },
      {
        "date": "2023-08-01T13:31:00.000Z",
        "voteCount": 3,
        "content": "We are dealing with image files so google cloud storage seems more appropriate"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 217,
    "url": "https://www.examtopics.com/discussions/google/view/117116-exam-associate-cloud-engineer-topic-1-question-217/",
    "body": "Your company has an internal application for managing transactional orders. The application is used exclusively by employees in a single physical location. The application requires strong consistency, fast queries, and ACID guarantees for multi-table transactional updates. The first version of the application is implemented in PostgreSQL, and you want to deploy it to the cloud with minimal code changes. Which database is most appropriate for this application?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBigtable",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBigQuery",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud SQL\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFirestore"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-28T14:34:00.000Z",
        "voteCount": 4,
        "content": "Another duplicate question"
      },
      {
        "date": "2023-11-23T08:41:00.000Z",
        "voteCount": 4,
        "content": "Cloud SQL is very similar to postgreSQL"
      },
      {
        "date": "2023-11-07T19:00:00.000Z",
        "voteCount": 2,
        "content": "Cloud SQL (supports PostgreSQL), so it should require minimal code changes.\nAnswer:\nC. Cloud SQL"
      },
      {
        "date": "2023-09-16T02:48:00.000Z",
        "voteCount": 2,
        "content": "So what about this another question?\n\nYour company has an internal application for managing transactional orders. The application is used exclusively by employees in a single physical location. The application requires strong consistency, fast queries, and ACID guarantees for multi-table transactional updates. The first version of the application is implemented in PostgreSQL, and you want to deploy it to the cloud with minimal code changes. Which database is most appropriate for this application?\nA. BigQuery\nB. Cloud SQL\nC. Cloud Spanner\nD. Cloud Datastore\n\nIt should be Cloud Spanner or Cloud SQL ?"
      },
      {
        "date": "2023-09-22T03:10:00.000Z",
        "voteCount": 4,
        "content": "Cloud SQL"
      },
      {
        "date": "2023-08-06T08:11:00.000Z",
        "voteCount": 2,
        "content": "ACID and strong consistency are in C or D, but Firestore is for documents and in question we have \"multi-table updates\" so there left C"
      },
      {
        "date": "2023-09-26T09:22:00.000Z",
        "voteCount": 1,
        "content": "You missed the main \"postgres\" part but yeah"
      },
      {
        "date": "2023-08-02T06:37:00.000Z",
        "voteCount": 1,
        "content": "Cloud SQL is the most appropriate choice for deploying the application with the required characteristics while minimizing code changes and maintaining strong consistency, fast queries, and ACID guarantees for multi-table transactional updates."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 218,
    "url": "https://www.examtopics.com/discussions/google/view/116249-exam-associate-cloud-engineer-topic-1-question-218/",
    "body": "Your company runs one batch process in an on-premises server that takes around 30 hours to complete. The task runs monthly, can be performed offline, and must be restarted if interrupted. You want to migrate this workload to the cloud while minimizing cost. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Instance Template with Spot VMs On. Create a Managed Instance Group from the template and adjust Target CPU Utilization. Migrate the workload.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the workload to a Compute Engine VM. Start and stop the instance as needed.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the workload to a Google Kubernetes Engine cluster with Spot nodes.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the workload to a Compute Engine Spot VM."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 23,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 7,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 7,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-07-23T19:13:00.000Z",
        "voteCount": 10,
        "content": "B. Migrating the workload to a Compute Engine VM and starting and stopping the instance as needed allows you to control when the task runs. This approach provides flexibility in terms of when to initiate the batch process, and it can be easily scheduled to run monthly. By stopping the instance when the task is not running, you can save on compute costs."
      },
      {
        "date": "2024-09-25T07:24:00.000Z",
        "voteCount": 2,
        "content": "D is correct"
      },
      {
        "date": "2024-08-26T04:44:00.000Z",
        "voteCount": 1,
        "content": "B. is correct. shreykul provided you the correct answer, take it or leave it."
      },
      {
        "date": "2024-07-17T12:44:00.000Z",
        "voteCount": 2,
        "content": "It seems to me that answer is A.\nScale to zero - After completed batch processes, minimize cost.\nSpot VM -  minimize cost.\nfailure resistant - do not need restart instance.\nYou already have a template - delete and deploy your Compute instances once a month easily and fast. \n\nCan you explain me unless I understand some thing."
      },
      {
        "date": "2024-04-16T07:50:00.000Z",
        "voteCount": 1,
        "content": "A is the most robust and cost-effective for your scenario. This setup leverages the cost savings of Spot VMs while also providing the ability to handle interruptions through a Managed Instance Group, ensuring the process is completed even if individual instances are preempted. Additionally, the automatic scaling based on CPU utilization helps manage resources efficiently, further reducing costs.\n\nThus, A not only minimizes costs by using Spot VMs but also enhances reliability and scalability through a Managed Instance Group, making it the most suitable choice for migrating your batch process to the cloud."
      },
      {
        "date": "2024-05-20T03:57:00.000Z",
        "voteCount": 2,
        "content": "\"runs one batch process\"\n\nSo, why would you like a instange group? it is only one batch process. And if it is interrupted, it has to be restarted...for me it is B."
      },
      {
        "date": "2024-07-17T12:44:00.000Z",
        "voteCount": 1,
        "content": "It seems to me that answer is A.\nScale to zero - After completed batch processes, minimize cost.\nSpot VM -  minimize cost.\nfailure resistant - do not need restart instance.\nYou already have a template - delete and deploy your Compute instances once a month easily and fast. \n\nCan you explain me unless I understand some thing."
      },
      {
        "date": "2024-04-13T19:09:00.000Z",
        "voteCount": 1,
        "content": "I don\u00b4t  like this kind of questions... but I think is A..\nAn Instance Template with Spot VMs (not preemptible)..."
      },
      {
        "date": "2024-10-08T00:15:00.000Z",
        "voteCount": 1,
        "content": "But will this 30hour job have to restart every time spot VM is replaced since they can preempt with 30 sec notice."
      },
      {
        "date": "2024-03-08T20:25:00.000Z",
        "voteCount": 2,
        "content": "Same as 136"
      },
      {
        "date": "2024-07-06T10:05:00.000Z",
        "voteCount": 2,
        "content": "no, this question has different answer choices with the inclusion of spot vms"
      },
      {
        "date": "2024-03-02T14:21:00.000Z",
        "voteCount": 2,
        "content": "I don't believe Spot VMs are the best solution here since they can be preempted by Google at any time. Option B prioritizes predictable execution and cost optimization for this scheduled batch process. It balances cost-savings with the need for guaranteed completion, aligning well with the provided scenario."
      },
      {
        "date": "2024-03-29T00:47:00.000Z",
        "voteCount": 1,
        "content": "wonderfully said."
      },
      {
        "date": "2024-02-17T11:45:00.000Z",
        "voteCount": 1,
        "content": "Spot VM best practices"
      },
      {
        "date": "2024-02-17T11:44:00.000Z",
        "voteCount": 1,
        "content": "Spot VM best practices"
      },
      {
        "date": "2024-01-13T23:15:00.000Z",
        "voteCount": 1,
        "content": "Spot VM best practices"
      },
      {
        "date": "2024-01-02T10:03:00.000Z",
        "voteCount": 2,
        "content": "An Instance Template with Spot VMs in a Managed Instance Group (MIG) has no 24-hour limitation, can restart via the MIG if interrupted, and offers low costs, making it a perfect match."
      },
      {
        "date": "2024-05-20T03:59:00.000Z",
        "voteCount": 1,
        "content": "ok, but the questions says that the process can NOT be interruputed, if does, you must restarted from the begining..."
      },
      {
        "date": "2023-12-31T10:48:00.000Z",
        "voteCount": 1,
        "content": "Per ChatGPT,\nOption A is the most appropriate solution as it leverages the cost benefits of Spot VMs, provides scalability through Managed Instance Groups, and aligns with the characteristics of the workload, resulting in minimized cost for running the batch process in the cloud."
      },
      {
        "date": "2023-12-16T12:54:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/compute/docs/instances/create-use-spot#best-practices\n\nSpot VM best practices create an instance template and Spot VM does not have a minimum or maximum runtime to there is no 24 hour limit as suggested by some."
      },
      {
        "date": "2024-02-17T10:25:00.000Z",
        "voteCount": 3,
        "content": "Read the link you shared, it says: \"Spot VMs are virtual machine (VM) instances with the spot provisioning model. Spot VMs are available at a 60-91% discount compared to the price of standard VMs. However, Compute Engine might reclaim the resources by preempting Spot VMs at any time. Spot VMs are recommended only for fault-tolerant applications that can withstand VM preemption.\"\n\nThe last part literally says that Spot VMs are recommended for fault-tolerant appliactions, by the questions, this application is not fault tolerant at all mate."
      },
      {
        "date": "2023-11-23T10:48:00.000Z",
        "voteCount": 3,
        "content": "D\n\nhttps://cloud.google.com/compute/docs/instances/spot#limitations\nSpot VMs are unlike Preemptible VMs (https://cloud.google.com/compute/docs/instances/preemptible#limitations) , which do not stop after 24 hours. And we can re-start the batch job in case if it gets stopped in between as per the question. So D is the best cost optimal solution."
      },
      {
        "date": "2023-11-09T18:50:00.000Z",
        "voteCount": 3,
        "content": "Answer B.  Spot VMs are not suitable for batch processes that take an extended period of time to complete, such as a 30-hour batch process. Spot VMs typically have a maximum runtime of 24 hours."
      },
      {
        "date": "2023-12-01T04:55:00.000Z",
        "voteCount": 3,
        "content": "Spot VM runs until it is stopped (by google), Unlike preemptible, which has a lifetime 24 hr"
      },
      {
        "date": "2023-11-07T14:01:00.000Z",
        "voteCount": 4,
        "content": "I had this question in my exam. There was Spot VM option available, but I still picked manual starting/stopping as job is not fault tolerant."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 219,
    "url": "https://www.examtopics.com/discussions/google/view/116899-exam-associate-cloud-engineer-topic-1-question-219/",
    "body": "You are planning to migrate the following on-premises data management solutions to Google Cloud:<br><br>\u2022\tOne MySQL cluster for your main database<br>\u2022\tApache Kafka for your event streaming platform<br>\u2022\tOne Cloud SQL for PostgreSQL database for your analytical and reporting needs<br><br>You want to implement Google-recommended solutions for the migration. You need to ensure that the new solutions provide global scalability and require minimal operational and infrastructure management. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate from MySQL to Cloud SQL, from Kafka to Pub/Sub, and from Cloud SQL for PostgreSQL to BigQuery.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate from MySQL to Cloud Spanner, from Kafka to Pub/Sub, and from Cloud SQL for PostgreSQL to BigQuery.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate from MySQL to Cloud Spanner, from Kafka to Memorystore, and from Cloud SQL for PostgreSQL to Cloud SQL.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate from MySQL to Cloud SQL, from Kafka to Memorystore, and from Cloud SQL for PostgreSQL to Cloud SQL."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 21,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-11-02T14:25:00.000Z",
        "voteCount": 11,
        "content": "Needs Global scalability ---- Spanner is in, CloudSQL is out.\nKafka --&gt; Pub/Sub &amp; not memory store.\nPostgres --&gt; BigQuery as it needs scalability and for analytics."
      },
      {
        "date": "2024-05-28T21:12:00.000Z",
        "voteCount": 1,
        "content": "B is correct, even M$ copilot chose it :D"
      },
      {
        "date": "2023-09-20T04:40:00.000Z",
        "voteCount": 2,
        "content": "B is the correct answer."
      },
      {
        "date": "2023-09-13T08:36:00.000Z",
        "voteCount": 2,
        "content": "Its B, makes sense"
      },
      {
        "date": "2023-09-09T05:41:00.000Z",
        "voteCount": 2,
        "content": "B is the correrct answer as question demands global scalibity  for it cloud spanner , for PostgreSQL to BIG query"
      },
      {
        "date": "2023-09-05T20:08:00.000Z",
        "voteCount": 2,
        "content": "For event streaming -&gt; Cloud Pub/Sub is a good choice.\nhttps://cloud.google.com/pubsub/docs/overview\n\nDatabase for analytics and reporting -&gt; BigQuery is a good choice.\nhttps://cloud.google.com/bigquery/docs/introduction\n\nFor MySQL, we have two options Cloud SQL or Cloud Spanner. Cloud SQL has MySQL flavored database, while Cloud Spanner provide Google Standard SQL (aka GoogleSQL) which supports standard SQL queries. In addition, the question is asking about \"global scalability and require minimal operational and infrastructure management\", where Cloud Spanner wins a score.\nHence, B is the correct answer.\nhttps://cloud.google.com/spanner\nhttps://cloud.google.com/spanner/docs/create-query-database-console#create-instance\nhttps://cloud.google.com/spanner/docs/create-query-database-console#create-database"
      },
      {
        "date": "2023-08-06T08:16:00.000Z",
        "voteCount": 1,
        "content": "agree B"
      },
      {
        "date": "2023-08-01T08:21:00.000Z",
        "voteCount": 1,
        "content": "B should be the answer. Cloud Spanner - Global support."
      },
      {
        "date": "2023-08-01T04:42:00.000Z",
        "voteCount": 3,
        "content": "B should be right"
      },
      {
        "date": "2023-07-31T06:37:00.000Z",
        "voteCount": 4,
        "content": "B should be the answer  as cloud spanner provides scalability"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 220,
    "url": "https://www.examtopics.com/discussions/google/view/117138-exam-associate-cloud-engineer-topic-1-question-220/",
    "body": "During a recent audit of your existing Google Cloud resources, you discovered several users with email addresses outside of your Google Workspace domain. You want to ensure that your resources are only shared with users whose email addresses match your domain. You need to remove any mismatched users, and you want to avoid having to audit your resources to identify mismatched users. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Cloud Scheduler task to regularly scan your projects and delete mismatched users.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Cloud Scheduler task to regularly scan your resources and delete mismatched users.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet an organizational policy constraint to limit identities by domain to automatically remove mismatched users.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet an organizational policy constraint to limit identities by domain, and then retroactively remove the existing mismatched users\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 18,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-09-13T08:44:00.000Z",
        "voteCount": 6,
        "content": "Its D. \"The domain restriction constraint is not retroactive. Once a domain restriction is set, this limitation will apply to IAM policy changes made from that point forward, and not to any previous changes.\". Link: https://cloud.google.com/resource-manager/docs/organization-policy/restricting-domains"
      },
      {
        "date": "2024-10-03T03:58:00.000Z",
        "voteCount": 1,
        "content": "Organization policies are not retroactive. If you need to force a change to your resource hierarchy that would violate an enforced constraint, you can disable the organization policy, make the change, and then enable the organization policy again.\n\n"
      },
      {
        "date": "2024-08-26T04:49:00.000Z",
        "voteCount": 1,
        "content": "joao_01 provided the appropriate answer, take it or leave it."
      },
      {
        "date": "2023-09-05T21:36:00.000Z",
        "voteCount": 4,
        "content": "D seems to be most appropriate. You can use organization policy constraint to limit the identities by domain. Once the organization policy is set, you can remove the leftover users that mismatched the conditions.\nhttps://cloud.google.com/resource-manager/docs/organization-policy/restricting-domains"
      },
      {
        "date": "2023-08-09T20:04:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/resource-manager/docs/organization-policy/org-policy-constraints\n\nThis list constraint defines the set of domains that email addresses added to Essential Contacts can have.\nBy default, email addresses with any domain can be added to Essential Contacts.\nThe allowed/denied list must specify one or more domains of the form @example.com. If this constraint is active and configured with allowed values, only email addresses with a suffix matching one of the entries from the list of allowed domains can be added in Essential Contacts.\nThis constraint has no effect on updating or removing existing contacts.\nconstraints/essentialcontacts.allowedContactDomains"
      },
      {
        "date": "2023-08-06T08:24:00.000Z",
        "voteCount": 2,
        "content": "In order to define an organization policy, you choose a constraint, which is a particular type of restriction"
      },
      {
        "date": "2023-08-04T07:00:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/resource-manager/docs/organization-policy/org-policy-constraints - Domain restricted sharing\n\nIf this constraint is active, only principals that belong to the allowed customer IDs can be added to IAM policies. It doesn't specifically say, but I think it doesn't get rid of existing principals."
      },
      {
        "date": "2023-08-02T08:49:00.000Z",
        "voteCount": 2,
        "content": "Should be D. Organization policy does not remove users automatically."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 221,
    "url": "https://www.examtopics.com/discussions/google/view/117106-exam-associate-cloud-engineer-topic-1-question-221/",
    "body": "Your application is running on Google Cloud in a managed instance group (MIG). You see errors in Cloud Logging for one VM that one of the processes is not responsive. You want to replace this VM in the MIG quickly. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the gcloud compute instances update command with a REFRESH action for the VM.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the gcloud compute instance-groups managed recreate-instances command to recreate the VM.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSelect the MIG from the Compute Engine console and, in the menu, select Replace VMs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate and apply the instance template of the MIG."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-10-15T06:06:00.000Z",
        "voteCount": 1,
        "content": "gcloud compute instance-groups managed recreate-instances"
      },
      {
        "date": "2024-09-25T08:02:00.000Z",
        "voteCount": 2,
        "content": "This allows you to quickly recreate the specific VM"
      },
      {
        "date": "2024-07-17T13:15:00.000Z",
        "voteCount": 1,
        "content": "Answer is B.\nB:      You can recreate one or more VMs\nC:      Only option to replace each VMs in MIG\n\nCompare it:\nhttps://cloud.google.com/sdk/gcloud/reference/compute/instance-groups/managed/recreate-instances\n\nhttps://cloud.google.com/sdk/gcloud/reference/compute/instance-groups/managed/rolling-action/replace"
      },
      {
        "date": "2024-07-12T23:48:00.000Z",
        "voteCount": 1,
        "content": "I think option C is correct, since gcloud CLI might take some extra time to reinitiate the instance, option B looks wrong"
      },
      {
        "date": "2024-02-17T08:17:00.000Z",
        "voteCount": 1,
        "content": "The underlying virtual machine instances are deleted and recreated based on the latest instance template configured for the managed instance group."
      },
      {
        "date": "2023-10-15T06:26:00.000Z",
        "voteCount": 2,
        "content": "B hoga common sense"
      },
      {
        "date": "2023-09-24T12:36:00.000Z",
        "voteCount": 2,
        "content": "I think its C, because the question talk of \"Replace the VM\". \nIn B you recreate the same VM, so i think the C is more sense."
      },
      {
        "date": "2023-09-09T05:43:00.000Z",
        "voteCount": 2,
        "content": "B is the correct answer , as the question demands in MIG managed instance group. B"
      },
      {
        "date": "2023-09-05T21:44:00.000Z",
        "voteCount": 2,
        "content": "You can recreate specified instance(s) in a managed instance group.\nhttps://cloud.google.com/sdk/gcloud/reference/compute/instance-groups/managed/recreate-instances"
      },
      {
        "date": "2023-08-05T23:15:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/sdk/gcloud/reference/compute/instance-groups/managed/recreate-instances"
      },
      {
        "date": "2023-08-02T08:57:00.000Z",
        "voteCount": 1,
        "content": "should be B."
      },
      {
        "date": "2023-08-02T05:58:00.000Z",
        "voteCount": 2,
        "content": "Following the document below is B:https://cloud.google.com/sdk/gcloud/reference/compute/instance-groups/managed/recreate-instances"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 222,
    "url": "https://www.examtopics.com/discussions/google/view/117104-exam-associate-cloud-engineer-topic-1-question-222/",
    "body": "You want to permanently delete a Pub/Sub topic managed by Config Connector in your Google Cloud project. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse kubectl to create the label deleted-by-cnrm and to change its value to true for the topic resource.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse kubectl to delete the topic resource.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse gcloud CLI to delete the topic.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse gcloud CLI to update the topic label managed-by-cnrm to false."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-11-04T07:27:00.000Z",
        "voteCount": 32,
        "content": "i hate these questions so much"
      },
      {
        "date": "2023-09-05T21:52:00.000Z",
        "voteCount": 5,
        "content": "If a resource is managed by the Config Connector, you can update/delete it through kubectl command.\nhttps://cloud.google.com/config-connector/docs/how-to/getting-started#before_you_begin"
      },
      {
        "date": "2024-10-03T04:05:00.000Z",
        "voteCount": 1,
        "content": "Deleting a resource\nUse kubectl delete to delete resources. For example, to delete the PubSubTopic you created earlier, run kubectl delete with your pubsub-topic.yaml file:\nkubectl delete -f pubsub-topic.yaml\n\n"
      },
      {
        "date": "2023-09-09T05:45:00.000Z",
        "voteCount": 2,
        "content": "B is the  right answer, if the resource is managed by the config connector you can just use the kubectl to use detele or update the topic resource"
      },
      {
        "date": "2023-08-06T08:52:00.000Z",
        "voteCount": 3,
        "content": "created by kubectl should be removed by it"
      },
      {
        "date": "2023-08-05T12:19:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/config-connector/docs/how-to/getting-started#deleting_a_resource"
      },
      {
        "date": "2023-08-02T08:57:00.000Z",
        "voteCount": 1,
        "content": "should be C."
      },
      {
        "date": "2023-08-02T06:37:00.000Z",
        "voteCount": 1,
        "content": "THIS IS B"
      },
      {
        "date": "2023-08-02T05:49:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/sdk/gcloud/reference/pubsub/topics/delete"
      },
      {
        "date": "2023-08-05T12:18:00.000Z",
        "voteCount": 2,
        "content": "Read the question. It's  a Pub/Sub topic managed by Config Connector. \nhttps://cloud.google.com/config-connector/docs/how-to/getting-started#deleting_a_resource"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 223,
    "url": "https://www.examtopics.com/discussions/google/view/117110-exam-associate-cloud-engineer-topic-1-question-223/",
    "body": "Your company is using Google Workspace to manage employee accounts. Anticipated growth will increase the number of personnel from 100 employees to 1,000 employees within 2 years. Most employees will need access to your company\u2019s Google Cloud account. The systems and processes will need to support 10x growth without performance degradation, unnecessary complexity, or security issues. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the users to Active Directory. Connect the Human Resources system to Active Directory. Turn on Google Cloud Directory Sync (GCDS) for Cloud Identity. Turn on Identity Federation from Cloud Identity to Active Directory.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOrganize the users in Cloud Identity into groups. Enforce multi-factor authentication in Cloud Identity.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTurn on identity federation between Cloud Identity and Google Workspace. Enforce multi-factor authentication for domain wide delegation.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a third-party identity provider service through federation. Synchronize the users from Google Workplace to the third-party provider in real time."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 22,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 12,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-11-07T18:39:00.000Z",
        "voteCount": 12,
        "content": "Why would you need to federate the identities for Cloud ID and Workspace accounts? It's the same thing! I have a dev gcp org doing exactly this..."
      },
      {
        "date": "2023-08-02T06:13:00.000Z",
        "voteCount": 5,
        "content": "I think C is the best"
      },
      {
        "date": "2024-05-20T03:37:00.000Z",
        "voteCount": 1,
        "content": "Google Cloud Identity is Google\u2019s identity provider (idP) that is used by both Workspace and Google Cloud.\nFirst, I thought it was C, but there is not identity federation between Cloud Identity and Workspace, because Cloud identity goes first and it is used by GCP, Workspace or other Google services.\nMoreover, By default, Cloud Identity Free includes 50 free licenses"
      },
      {
        "date": "2024-03-07T01:07:00.000Z",
        "voteCount": 3,
        "content": "According to the following article, Google Workspace already allows access to Google Cloud platform, so there should be no need for \"identity federation between Cloud Identity and Google Workspace\" (which I cannot find documentation about). C is out.\n\nhttps://cloud.google.com/iam/docs/user-identities\n\nA and D are out because Google is not going to suggest you use some other identity provider, lol"
      },
      {
        "date": "2024-03-02T15:04:00.000Z",
        "voteCount": 1,
        "content": "I see everyone is split here, but I like C best. Option C provides a streamlined and secure way to accommodate growth within the Google ecosystem. It prioritizes both ease of management and security.\n\nYou already use Workspace for user management... Adding Active Directory adds complexity and is unnecessary; A is out. While improving security and organization, it doesn't directly solve the issue of seamless access to GCP resources or address user management as you scale; B is out\u2026. While some scenarios might justify it, you're already using Google Workspace for identity. This option adds complexity, cost, and an extra system to manage; D is out\u2026."
      },
      {
        "date": "2024-02-18T11:07:00.000Z",
        "voteCount": 1,
        "content": "I think B helps to manage but the answer in this scenario is C due to allows for seamless integration between user accounts in both services"
      },
      {
        "date": "2024-02-17T10:37:00.000Z",
        "voteCount": 1,
        "content": "I choose B."
      },
      {
        "date": "2023-12-31T10:50:00.000Z",
        "voteCount": 1,
        "content": "Per ChatGPT,\nOption A is the most suitable choice as it leverages the scalability and centralization capabilities of Active Directory, integrates with existing systems, and ensures seamless user management and authentication across Google Workspace and Google Cloud, aligning with the requirements for anticipated growth without introducing unnecessary complexity or security issues."
      },
      {
        "date": "2024-03-02T14:56:00.000Z",
        "voteCount": 2,
        "content": "Please stop using ChatGPT. Active Directory is Azure. This adds significant overhead in setup and management of a new directory system. Integrating it with both Workspace and Google Cloud increases complexity."
      },
      {
        "date": "2023-12-04T07:40:00.000Z",
        "voteCount": 1,
        "content": "Answer is A.   It is talking about long term.\nThis option can support 10x growth without performance degradation, unnecessary complexity, or security issues, as it leverages the scalability, reliability, and security of Active Directory and Cloud Identity."
      },
      {
        "date": "2023-10-13T07:39:00.000Z",
        "voteCount": 2,
        "content": "\u2022 Option C involves setting up a connection between Cloud Identity (which is used for Google Workspace) and Google Cloud. This connection simplifies managing user accounts for Google Cloud services.\n\t\u2022 It enforces an extra layer of security by requiring multi-factor authentication, which is essential for protecting user accounts and data.\n\t\u2022 This solution aligns with your company's growth plans, as it accommodates an increase in users from 100 to 1,000 without adding unnecessary complexity or performance issues.\n\tSo, by selecting option C, you're ensuring a scalable, secure, and straightforward way to manage user access as your company expands."
      },
      {
        "date": "2023-11-10T09:02:00.000Z",
        "voteCount": 6,
        "content": "ChatGPT is not always right, you know :)"
      },
      {
        "date": "2023-11-25T12:11:00.000Z",
        "voteCount": 1,
        "content": "What would you choose, Sir ?"
      },
      {
        "date": "2023-09-24T12:44:00.000Z",
        "voteCount": 3,
        "content": "I think its A"
      },
      {
        "date": "2023-09-10T04:58:00.000Z",
        "voteCount": 3,
        "content": "There is nothing like \"identity federation between Cloud Identity and Google Workspace\". You can only add Cloud Identity Free to your existing account.\nYou can manage users using Cloud Identity without adding all of them to Google Workspace.\nGroups will help to manage the increased number of users.\nTherefore B answer is correct."
      },
      {
        "date": "2023-11-24T05:58:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/workforce-identity-federation?hl=en"
      },
      {
        "date": "2023-09-09T05:47:00.000Z",
        "voteCount": 1,
        "content": "c is right"
      },
      {
        "date": "2023-09-06T06:08:00.000Z",
        "voteCount": 2,
        "content": "(a) If you already administer a Google Workspace account and want to enable more users to use Google Cloud, you might not want to assign all users a Google Workspace license. In this case, add Cloud Identity Free to your existing account. You can then onboard more users without additional charge and decide which users should have access to Google Workspace by assigning them a Google Workspace license..  \n(b) To let your users collaborate by using Google Workspace, and to minimize administrative overhead, it's best to manage all users through a single Cloud Identity or Google Workspace account and provide a single user account to each individual. This approach helps ensure that settings such as password policies, single sign-on, and two-step verification are consistently applied to all users.\nref: https://cloud.google.com/architecture/identity/best-practices-for-planning"
      },
      {
        "date": "2023-08-27T21:59:00.000Z",
        "voteCount": 1,
        "content": "Why not A?"
      },
      {
        "date": "2023-09-26T06:34:00.000Z",
        "voteCount": 1,
        "content": "They need reduced complexity, while this works, we skip one step (active directory) by linking cloud identity with workspace"
      },
      {
        "date": "2023-08-16T07:16:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/architecture/identity/best-practices-for-planning"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 224,
    "url": "https://www.examtopics.com/discussions/google/view/117111-exam-associate-cloud-engineer-topic-1-question-224/",
    "body": "You want to host your video encoding software on Compute Engine. Your user base is growing rapidly, and users need to be able to encode their videos at any time without interruption or CPU limitations. You must ensure that your encoding solution is highly available, and you want to follow Google-recommended practices to automate operations. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy your solution on multiple standalone Compute Engine instances, and increase the number of existing instances when CPU utilization on Cloud Monitoring reaches a certain threshold.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy your solution on multiple standalone Compute Engine instances, and replace existing instances with high-CPU instances when CPU utilization on Cloud Monitoring reaches a certain threshold.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy your solution to an instance group, and increase the number of available instances whenever you see high CPU utilization in Cloud Monitoring.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy your solution to an instance group, and set the autoscaling based on CPU utilization.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 17,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-08-26T01:49:00.000Z",
        "voteCount": 6,
        "content": "The answer is D.\nYou can create a managed instance group with autoscaling enabled based on CPU utilization. This way appropriate number of instances can be added or removed based on the CPU metrics.\nhttps://cloud.google.com/compute/docs/instance-groups/create-mig-with-basic-autoscaling\nhttps://cloud.google.com/sdk/gcloud/reference/compute/instance-groups/managed/set-autoscaling"
      },
      {
        "date": "2024-01-27T06:31:00.000Z",
        "voteCount": 2,
        "content": "D is the correct"
      },
      {
        "date": "2023-12-21T10:01:00.000Z",
        "voteCount": 1,
        "content": "D is the correct answer!"
      },
      {
        "date": "2023-11-02T15:13:00.000Z",
        "voteCount": 2,
        "content": "Answer is D.\nMIG with scaling using CPU utilization."
      },
      {
        "date": "2023-09-09T05:48:00.000Z",
        "voteCount": 2,
        "content": "D seems more correct,  just set the autoscaling based on the CPU ulilization"
      },
      {
        "date": "2023-08-17T07:01:00.000Z",
        "voteCount": 1,
        "content": "D seems most appropriate"
      },
      {
        "date": "2023-08-06T09:10:00.000Z",
        "voteCount": 2,
        "content": "definitely D"
      },
      {
        "date": "2023-08-02T06:16:00.000Z",
        "voteCount": 4,
        "content": "https://cloud.google.com/sdk/gcloud/reference/compute/instance-groups/managed/set-autoscaling"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 225,
    "url": "https://www.examtopics.com/discussions/google/view/116982-exam-associate-cloud-engineer-topic-1-question-225/",
    "body": "Your managed instance group raised an alert stating that new instance creation has failed to create new instances. You need to solve the instance creation problem. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an instance template that contains valid syntax which will be used by the instance group. Delete any persistent disks with the same name as instance names.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an instance template that contains valid syntax that will be used by the instance group. Verify that the instance name and persistent disk name values are not the same in the template.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tVerify that the instance template being used by the instance group contains valid syntax. Delete any persistent disks with the same name as instance names. Set the disks.autoDelete property to true in the instance template.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDelete the current instance template and replace it with a new instance template. Verify that the instance name and persistent disk name values are not the same in the template. Set the disks.autoDelete property to true in the instance template."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 22,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-08-26T02:32:00.000Z",
        "voteCount": 16,
        "content": "C is eliminated because you cannot update or modify an existing instance template. It is immutable.\nhttps://cloud.google.com/compute/docs/instance-templates#how_to_update_instance_templates\n\nD is eliminated because you cannot delete an instance template if a managed instance group references it.\nhttps://cloud.google.com/compute/docs/instance-templates/get-list-delete-instance-templates#delete_an_instance_template\n\nB is eliminated because you cannot set different/custom name for persistent disk in an instance template.\nhttps://cloud.google.com/sdk/gcloud/reference/compute/instance-templates/create#--create-disk\n\nSo we left with A. Therefore A is the correct answer. Although, it should set the disks.autoDelete property to true for completeness."
      },
      {
        "date": "2024-09-25T08:49:00.000Z",
        "voteCount": 1,
        "content": "C is corrct answer"
      },
      {
        "date": "2023-11-27T21:17:00.000Z",
        "voteCount": 2,
        "content": "instance templates are immutable so can not modify or update"
      },
      {
        "date": "2023-10-27T07:14:00.000Z",
        "voteCount": 2,
        "content": "When a managed instance group raises an alert indicating that new instance creation has failed, it's important to investigate the issue. This issue is typically related to the configuration of the instance template used by the group. Therefore, the first step is to verify that the instance template being utilized by the instance group contains valid syntax. This includes checking all settings such as machine type, boot disk, and any custom configurations. Valid syntax ensures that the instances created adhere to the specified configurations."
      },
      {
        "date": "2023-10-12T09:00:00.000Z",
        "voteCount": 2,
        "content": "repeated question"
      },
      {
        "date": "2023-08-16T06:59:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/compute/docs/troubleshooting/troubleshooting-migs"
      },
      {
        "date": "2023-08-06T09:15:00.000Z",
        "voteCount": 1,
        "content": "Existing instance group has already template so no requirement to create new one. Problem is with persistent disk so delete it and configure autodelete solves problem now and in future"
      },
      {
        "date": "2023-08-13T03:27:00.000Z",
        "voteCount": 2,
        "content": "i was wrong, templates are immutable so you cannot update them, so option A is valid"
      },
      {
        "date": "2023-08-16T07:01:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/compute/docs/instance-templates#how_to_update_instance_templates\nInstance templates are designed to create instances with identical configurations. So you cannot update an existing instance template or change an instance template after you create it.\n\nIf you need to make changes to the configuration, create a new instance template."
      },
      {
        "date": "2023-08-01T06:01:00.000Z",
        "voteCount": 2,
        "content": "THIS IS A"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 226,
    "url": "https://www.examtopics.com/discussions/google/view/117109-exam-associate-cloud-engineer-topic-1-question-226/",
    "body": "You have created an application that is packaged into a Docker image. You want to deploy the Docker image as a workload on Google Kubernetes Engine. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpload the image to Cloud Storage and create a Kubernetes Service referencing the image.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpload the image to Cloud Storage and create a Kubernetes Deployment referencing the image.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpload the image to Artifact Registry and create a Kubernetes Service referencing the image.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpload the image to Artifact Registry and create a Kubernetes Deployment referencing the image.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 13,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-08-26T02:55:00.000Z",
        "voteCount": 6,
        "content": "A and B are eliminated because Cloud Storage is not a preferred place for storing docker images.\nhttps://cloud.google.com/artifact-registry/docs/docker/store-docker-container-images\n\nC is eliminated because Kubernetes Service is responsible for networking and connectivity between pods and external entities.\n\nKubernetes Deployment is responsible for deploying and managing an application (running in pods) on your GKE cluster.\nhttps://cloud.google.com/kubernetes-engine/docs/how-to/deploying-workloads-overview#stateless_applications"
      },
      {
        "date": "2024-10-11T00:45:00.000Z",
        "voteCount": 1,
        "content": "D is the best answer."
      },
      {
        "date": "2023-08-07T01:45:00.000Z",
        "voteCount": 2,
        "content": "best choice"
      },
      {
        "date": "2023-08-02T06:07:00.000Z",
        "voteCount": 4,
        "content": "Artifact Registry is a fully managed container registry that integrates seamlessly with Google Kubernetes Engine and other Google Cloud services. By uploading the Docker image to Artifact Registry, you can create a Kubernetes Deployment that references the image stored in Artifact Registry. This ensures that Kubernetes can pull the image from a trusted and managed source, while the Deployment manages the deployment and scaling of the application pods based on the image."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 227,
    "url": "https://www.examtopics.com/discussions/google/view/117250-exam-associate-cloud-engineer-topic-1-question-227/",
    "body": "You are using Looker Studio to visualize a table from your data warehouse that is built on top of BigQuery. Data is appended to the data warehouse during the day. At night, the daily summary is recalculated by overwriting the table. You just noticed that the charts in Looker Studio are broken, and you want to analyze the problem. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn Cloud Logging, create a filter for your Looker Studio report.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the open source CLI tool, Snapshot Debugger, to find out why the data was not refreshed correctly.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReview the Error Reporting page in the Google Cloud console to find any errors.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the BigQuery interface to review the nightly job and look for any errors.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 16,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-09-14T08:05:00.000Z",
        "voteCount": 9,
        "content": "This question appear before. I would go with D, altough the question before had opinions between the Cloud Logging and Bigquery options. Despite this i think that D is most appropriate for this case."
      },
      {
        "date": "2024-10-11T00:46:00.000Z",
        "voteCount": 1,
        "content": "D is the best answer."
      },
      {
        "date": "2023-09-22T12:20:00.000Z",
        "voteCount": 3,
        "content": "D is the answer"
      },
      {
        "date": "2023-08-12T05:53:00.000Z",
        "voteCount": 3,
        "content": "D is right"
      },
      {
        "date": "2023-08-03T07:38:00.000Z",
        "voteCount": 3,
        "content": "D is correct"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 228,
    "url": "https://www.examtopics.com/discussions/google/view/117121-exam-associate-cloud-engineer-topic-1-question-228/",
    "body": "You have a batch workload that runs every night and uses a large number of virtual machines (VMs). It is fault-tolerant and can tolerate some of the VMs being terminated. The current cost of VMs is too high. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun a test using simulated maintenance events. If the test is successful, use Spot N2 Standard VMs when running future jobs.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun a test using simulated maintenance events. If the test is successful, use N2 Standard VMs when running future jobs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun a test using a managed instance group. If the test is successful, use N2 Standard VMs in the managed instance group when running future jobs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun a test using N1 standard VMs instead of N2. If the test is successful, use N1 Standard VMs when running future jobs."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 20,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-10-11T00:48:00.000Z",
        "voteCount": 1,
        "content": "You can use Spot VMs in your clusters and node pools to run stateless, batch, or fault-tolerant workloads that can tolerate disruptions caused by the ephemeral nature of Spot VMs.\n\n"
      },
      {
        "date": "2024-02-12T13:39:00.000Z",
        "voteCount": 1,
        "content": "A is correct."
      },
      {
        "date": "2023-10-12T09:03:00.000Z",
        "voteCount": 4,
        "content": "A - spot VMs for fault tolerant workloads"
      },
      {
        "date": "2023-09-14T08:09:00.000Z",
        "voteCount": 3,
        "content": "Its A, makes sense"
      },
      {
        "date": "2023-09-13T05:21:00.000Z",
        "voteCount": 1,
        "content": "A is the correct answer"
      },
      {
        "date": "2023-09-09T05:51:00.000Z",
        "voteCount": 1,
        "content": "A is the correct answer as Spot VMs are highly affordable"
      },
      {
        "date": "2023-08-26T03:25:00.000Z",
        "voteCount": 2,
        "content": "Here, the keywords are batch workloads, large number of VMs, tolerate some VMs being terminated, and addressing cost of VMs. Spot VMs have significant discounts than on-demand VMs, therefore A is the correct answer.\nhttps://cloud.google.com/compute/docs/instances/spot"
      },
      {
        "date": "2023-08-07T01:57:00.000Z",
        "voteCount": 4,
        "content": "Spot VMs are highly affordable compute instances suitable for batch jobs and fault-tolerant workloads. Spot VMs offer the same machine types, options, and performance as regular compute instances. If your applications are fault tolerant and can withstand possible instance preemptions, then Spot instances can reduce your Compute Engine costs by up to 91%!"
      },
      {
        "date": "2023-08-02T06:47:00.000Z",
        "voteCount": 3,
        "content": "A definitely"
      },
      {
        "date": "2023-08-02T21:43:00.000Z",
        "voteCount": 1,
        "content": "Please , put the reference, thanks."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 229,
    "url": "https://www.examtopics.com/discussions/google/view/117783-exam-associate-cloud-engineer-topic-1-question-229/",
    "body": "You created several resources in multiple Google Cloud projects. All projects are linked to different billing accounts. To better estimate future charges, you want to have a single visual representation of all costs incurred. You want to include new cost data as soon as possible. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFill all resources in the Pricing Calculator to get an estimate of the monthly cost.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Reports view in the Cloud Billing Console to view the desired cost information.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tVisit the Cost Table page to get a CSV export and visualize it using Looker Studio.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Billing Data Export to BigQuery and visualize the data in Looker Studio.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 16,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-10-11T00:53:00.000Z",
        "voteCount": 1,
        "content": "Cloud Billing export to BigQuery enables you to export detailed Google Cloud billing data (such as usage, cost estimates, and pricing data) automatically throughout the day to a BigQuery dataset that you specify. Then you can access your Cloud Billing data from BigQuery for detailed analysis, or use a tool like Looker Studio to visualize your data. You can also use this export method to export data to a JSON file.\n"
      },
      {
        "date": "2024-10-08T04:16:00.000Z",
        "voteCount": 2,
        "content": "Answer D\nSame answer is for Q-201 but different question parameters (multiple projects, same billing account, dynamically calculated cost visualizations)"
      },
      {
        "date": "2024-03-03T09:08:00.000Z",
        "voteCount": 2,
        "content": "Visualize costs = BQ and Looker"
      },
      {
        "date": "2024-02-12T13:42:00.000Z",
        "voteCount": 1,
        "content": "D is correct."
      },
      {
        "date": "2023-09-14T08:09:00.000Z",
        "voteCount": 3,
        "content": "Its D, most appropriate"
      },
      {
        "date": "2023-09-13T05:23:00.000Z",
        "voteCount": 4,
        "content": "D is the correct answer\n\"single visual representation of all costs incurred\""
      },
      {
        "date": "2023-08-26T03:58:00.000Z",
        "voteCount": 4,
        "content": "A and C are straightaway eliminated.\nCloud Billing Reports displays a chart that plots usage costs for all projects linked to a Cloud Billing account, but projects should be linked to a single Cloud Billing account. So, C is also eliminated.\nhttps://cloud.google.com/billing/docs/how-to/reports\n\nYou can combine Cloud Billing data export to BigQuery with Looker Studio to stay up to date on your Google Cloud costs.\nhttps://cloud.google.com/billing/docs/how-to/visualize-data"
      },
      {
        "date": "2023-08-10T04:23:00.000Z",
        "voteCount": 1,
        "content": "We want to aggregate the costs for multiple billing accounts"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 230,
    "url": "https://www.examtopics.com/discussions/google/view/117609-exam-associate-cloud-engineer-topic-1-question-230/",
    "body": "Your company has a large quantity of unstructured data in different file formats. You want to perform ETL transformations on the data. You need to make the data accessible on Google Cloud so it can be processed by a Dataflow job. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpload the data to BigQuery using the bq command line tool.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpload the data to Cloud Storage using the gcloud storage command.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpload the data into Cloud SQL using the import function in the Google Cloud console.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpload the data into Cloud Spanner using the import function in the Google Cloud console."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 15,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-09-14T08:10:00.000Z",
        "voteCount": 3,
        "content": "Its B, non structure data"
      },
      {
        "date": "2023-09-13T05:24:00.000Z",
        "voteCount": 3,
        "content": "\"unstructured data in different file formats\""
      },
      {
        "date": "2023-09-09T05:53:00.000Z",
        "voteCount": 3,
        "content": "For unstrucutrred data cloud striooage  is the right answer"
      },
      {
        "date": "2023-08-26T04:22:00.000Z",
        "voteCount": 2,
        "content": "The question is asking about unstructured data. So, options A, C &amp; E are eliminated.\nB is the correct answer."
      },
      {
        "date": "2023-08-08T12:50:00.000Z",
        "voteCount": 2,
        "content": "Unstructured is the keyword in this questions. All possible answers are structured, but Cloud Storage."
      },
      {
        "date": "2023-08-08T05:45:00.000Z",
        "voteCount": 2,
        "content": "only B is for unstructured data"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 231,
    "url": "https://www.examtopics.com/discussions/google/view/117117-exam-associate-cloud-engineer-topic-1-question-231/",
    "body": "You have deployed an application on a single Compute Engine instance. The application writes logs to disk. Users start reporting errors with the application. You want to diagnose the problem. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNavigate to Cloud Logging and view the application logs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a health check on the instance and set a \u201cconsecutive successes\u201d Healthy threshold value of 1.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConnect to the instance\u2019s serial console and read the application logs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall and configure the Ops agent and view the logs from Cloud Logging.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-09-15T00:39:00.000Z",
        "voteCount": 5,
        "content": "We need to install the agent. Its D"
      },
      {
        "date": "2023-08-05T23:09:00.000Z",
        "voteCount": 5,
        "content": "I would go with D.\nBy default there is no logs agent installed on a compute instance.\nSo first you will have to install the Ops Agent and after a few minutes the logs will be visible in Cloud logging"
      },
      {
        "date": "2024-05-20T02:54:00.000Z",
        "voteCount": 1,
        "content": "Telemetry is not application logs. Even if you install ops agent, you will not be able to consult the logs that the application writes to the instance disk. You have to enter the instance to inspect those logs, because those logs are not saved by the ops agent in gcp"
      },
      {
        "date": "2024-05-20T02:57:00.000Z",
        "voteCount": 2,
        "content": "The question is tricky, since the application starts to work poorly, that does not mean that we find the reason in the application logs, we have to look at the logs (telemetry) of the instance to determine if it is a problem of the GCP infrastructure, so first have the operations agent installed and sending metrics"
      },
      {
        "date": "2023-08-26T04:35:00.000Z",
        "voteCount": 5,
        "content": "The Ops Agent is the primary agent for collecting telemetry from your Compute Engine instances.\nAnswer is D\nhttps://cloud.google.com/logging/docs/agent/ops-agent\nhttps://cloud.google.com/logging/docs/agent/ops-agent/configuration"
      },
      {
        "date": "2023-08-08T06:27:00.000Z",
        "voteCount": 4,
        "content": "https://cloud.google.com/logging/docs/logging-gce-quickstart"
      },
      {
        "date": "2023-08-02T06:42:00.000Z",
        "voteCount": 2,
        "content": "Chat Gpt says it's A"
      },
      {
        "date": "2023-11-02T15:38:00.000Z",
        "voteCount": 18,
        "content": "Chat GPT need to get Google certified before qualifying to answer :-)"
      },
      {
        "date": "2023-11-10T09:24:00.000Z",
        "voteCount": 3,
        "content": ":)))))"
      },
      {
        "date": "2024-01-02T14:20:00.000Z",
        "voteCount": 8,
        "content": "after 5 months, Chat GPT chooses D. it evolves. lol."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 232,
    "url": "https://www.examtopics.com/discussions/google/view/117615-exam-associate-cloud-engineer-topic-1-question-232/",
    "body": "You recently received a new Google Cloud project with an attached billing account where you will work. You need to create instances, set firewalls, and store data in Cloud Storage. You want to follow Google-recommended practices. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the gcloud CLI services enable cloudresourcemanager.googleapis.com command to enable all resources.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the gcloud services enable compute.googleapis.com command to enable Compute Engine and the gcloud services enable storage-api.googleapis.com command to enable the Cloud Storage APIs.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOpen the Google Cloud console and enable all Google Cloud APIs from the API dashboard.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOpen the Google Cloud console and run gcloud init --project <project-id> in a Cloud Shell.\n\t\t\t\t\t\t\t\t\t\t</project-id>"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 15,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-10-08T08:06:00.000Z",
        "voteCount": 2,
        "content": "Answer B. \nAs for Firewall, after enabling the compute engine api, firewall can be setup in CLI. The Compute Engine API encompasses the functionality for configuring network resources, including firewalls."
      },
      {
        "date": "2024-07-17T14:08:00.000Z",
        "voteCount": 1,
        "content": "But gcloud services enable storage-api.googleapis.com enabled by default. Maybe it another answer than B?"
      },
      {
        "date": "2023-11-02T15:41:00.000Z",
        "voteCount": 4,
        "content": "B is correct, need to enable API first thing before you can use any services.\nCan be done using Google Console or using CLI."
      },
      {
        "date": "2023-09-15T00:43:00.000Z",
        "voteCount": 4,
        "content": "Enable correspondent APIs. Its B"
      },
      {
        "date": "2023-09-09T05:55:00.000Z",
        "voteCount": 3,
        "content": "B is the correct answer"
      },
      {
        "date": "2023-08-15T04:21:00.000Z",
        "voteCount": 4,
        "content": "At first you need to enable API"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 233,
    "url": "https://www.examtopics.com/discussions/google/view/117122-exam-associate-cloud-engineer-topic-1-question-233/",
    "body": "Your application development team has created Docker images for an application that will be deployed on Google Cloud. Your team does not want to manage the infrastructure associated with this application. You need to ensure that the application can scale automatically as it gains popularity. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an instance template with the container image, and deploy a Managed Instance Group with Autoscaling.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpload Docker images to Artifact Registry, and deploy the application on Google Kubernetes Engine using Standard mode.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpload Docker images to the Cloud Storage, and deploy the application on Google Kubernetes Engine using Standard mode.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpload Docker images to Artifact Registry, and deploy the application on Cloud Run.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 24,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-09-09T05:57:00.000Z",
        "voteCount": 5,
        "content": "D is the correct answer , as question says your team dont want to manage the infrastrucutre associated withthe application this is offered by the cloudrun"
      },
      {
        "date": "2023-09-15T00:55:00.000Z",
        "voteCount": 5,
        "content": "Its D. GKE standard the nodes are managed by user. So D is correct."
      },
      {
        "date": "2023-09-14T06:38:00.000Z",
        "voteCount": 3,
        "content": "D. Upload Docker images to Artifact Registry and deploy the application on Cloud Run."
      },
      {
        "date": "2023-08-27T03:46:00.000Z",
        "voteCount": 3,
        "content": "Key Hint : Your team does not want to manage the infrastructure associated with this application\nThis is offered by Cloud Run, hence option D."
      },
      {
        "date": "2023-08-26T05:04:00.000Z",
        "voteCount": 2,
        "content": "Answer is D.\nCloud Run is container-as-a-service offering from Google Cloud. You can deploy the containerized application directly on top of Google's infrastructure, and only when a request comes.\nhttps://cloud.google.com/run/docs/overview/what-is-cloud-run"
      },
      {
        "date": "2023-08-26T05:05:00.000Z",
        "voteCount": 2,
        "content": "You don't need to worry about underlying infrastructure.\nA, C and E requires resources, virtual instance, GKE cluster provisioning and maintaining. so these are eliminated."
      },
      {
        "date": "2023-08-08T07:10:00.000Z",
        "voteCount": 2,
        "content": "GKE Standard mode: You manage the underlying infrastructure, including configuring the individual nodes.\nInstance group - you manage the infrastructure as well\nso after elimination A,B,C stays D"
      },
      {
        "date": "2023-08-05T05:56:00.000Z",
        "voteCount": 3,
        "content": "IT'S D"
      },
      {
        "date": "2023-08-02T06:53:00.000Z",
        "voteCount": 4,
        "content": "Option D"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 234,
    "url": "https://www.examtopics.com/discussions/google/view/116894-exam-associate-cloud-engineer-topic-1-question-234/",
    "body": "You are migrating a business critical application from your local data center into Google Cloud. As part of your high-availability strategy, you want to ensure that any data used by the application will be immediately available if a zonal failure occurs. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the application data on a zonal persistent disk. Create a snapshot schedule for the disk. If an outage occurs, create a new disk from the most recent snapshot and attach it to a new VM in another zone.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the application data on a zonal persistent disk. If an outage occurs, create an instance in another zone with this disk attached.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the application data on a regional persistent disk. Create a snapshot schedule for the disk. If an outage occurs, create a new disk from the most recent snapshot and attach it to a new VM in another zone.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the application data on a regional persistent disk. If an outage occurs, create an instance in another zone with this disk attached.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 17,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-11-11T20:04:00.000Z",
        "voteCount": 12,
        "content": "Answer is D.   When your are  using regional persistent disks, the data is automatically replicated to two replicas without the requirement of maintaining application replication.  There is no need to use a snapshot."
      },
      {
        "date": "2023-11-02T16:59:00.000Z",
        "voteCount": 1,
        "content": "Answer is C and not D.\nUse regional PD with regular snapshot for data redundancy. \nC works, but data will be stale, as there is no latest snapshot.\nA&amp;B are out, as they are using regular PD and not regional PD."
      },
      {
        "date": "2023-11-06T16:30:00.000Z",
        "voteCount": 8,
        "content": "I come back to correct his. Answer is D. Regional PD will be accessible even when zone fails and app can be brought up using same disk in different zone."
      },
      {
        "date": "2023-10-27T07:19:00.000Z",
        "voteCount": 1,
        "content": "Option C provides the best combination of data redundancy and recovery speed, making it the ideal choice for high-availability scenarios. While other options like option A (zonal persistent disk with snapshots) or option D (regional disk with instances in other zones) can also work, they may not offer the same level of efficiency and data protection as option C. Option B (zonal disk without replication) is less desirable because it lacks data redundancy and necessitates manual intervention to restore data in case of a zonal failure."
      },
      {
        "date": "2023-09-15T01:02:00.000Z",
        "voteCount": 3,
        "content": "I thought first it was C. Then i saw Google documentation and for sure the answer is D."
      },
      {
        "date": "2023-09-09T05:59:00.000Z",
        "voteCount": 3,
        "content": "D is the correct answer"
      },
      {
        "date": "2023-09-08T11:46:00.000Z",
        "voteCount": 2,
        "content": "C,\nIf you are designing robust systems or high availability services on Compute Engine, use regional Persistent Disk combined with other best practices such as backing up your data using snapshots."
      },
      {
        "date": "2023-08-17T15:20:00.000Z",
        "voteCount": 1,
        "content": "It should be D."
      },
      {
        "date": "2023-08-16T05:27:00.000Z",
        "voteCount": 3,
        "content": "D, C doesn't make sense because article explicitly says that: 'During the failover, the regional persistent disk that is synchronously replicated to the secondary zone is force attached to the standby VM by the application control plane, and all traffic is directed to that VM based on health check signals.'\nhttps://cloud.google.com/compute/docs/disks/high-availability-regional-persistent-disk#failover"
      },
      {
        "date": "2023-08-14T02:04:00.000Z",
        "voteCount": 4,
        "content": "The benefit of regional persistent disks is that in the event of a zonal outage, where your virtual machine (VM) instance might become unavailable, you can usually force attach a regional persistent disk to a VM instance in a secondary zone in the same region."
      },
      {
        "date": "2023-08-08T07:42:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/compute/docs/disks/high-availability-regional-persistent-disk"
      },
      {
        "date": "2023-08-04T06:21:00.000Z",
        "voteCount": 2,
        "content": "CHATGPT says C"
      },
      {
        "date": "2024-03-03T13:30:00.000Z",
        "voteCount": 1,
        "content": "For the love of everything holy, stop using Chat GPT.  The snapshot process is unnecessary and introduces delays during the restore phase, impacting how quickly data can be made available. Regional disks replicate data across multiple zones within a region. This means your data is always available even if a single zone goes down.  In case of a zonal failure, you can seamlessly spin up a new instance in another zone within the same region and directly attach the existing regional disk to it.\n\nD is a much better solution."
      },
      {
        "date": "2023-08-04T05:42:00.000Z",
        "voteCount": 2,
        "content": "correct: C"
      },
      {
        "date": "2023-08-01T14:43:00.000Z",
        "voteCount": 2,
        "content": "It should be C."
      },
      {
        "date": "2023-08-01T07:12:00.000Z",
        "voteCount": 2,
        "content": "I think c, for high availability use regional instead of zonal, and you can create snapshot to protect against data loss\nhttps://cloud.google.com/compute/docs/disks#reliability_2:~:text=Similar%20to%20zonal%20Persistent%20Disk%2C%20you%20can%20create%20snapshots%20of%20Persistent%20Disk%20to%20protect%20against%20data%20loss%20due%20to%20user%20error.%20Snapshots%20are%20incremental%2C%20and%20take%20only%20minutes%20to%20create%20even%20if%20you%20snapshot%20disks%20that%20are%20attached%20to%20running%20instances."
      },
      {
        "date": "2023-07-31T05:52:00.000Z",
        "voteCount": 2,
        "content": "Chat Gpt sayd \"D\""
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 235,
    "url": "https://www.examtopics.com/discussions/google/view/117119-exam-associate-cloud-engineer-topic-1-question-235/",
    "body": "The DevOps group in your organization needs full control of Compute Engine resources in your development project. However, they should not have permission to create or update any other resources in the project. You want to follow Google\u2019s recommendations for setting permissions for the DevOps group. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGrant the basic role roles/viewer and the predefined role roles/compute.admin to the DevOps group.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an IAM policy and grant all compute.instanceAdmin.* permissions to the policy. Attach the policy to the DevOps group.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a custom role at the folder level and grant all compute.instanceAdmin.* permissions to the role. Grant the custom role to the DevOps group.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGrant the basic role roles/editor to the DevOps group."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 26,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 16,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-11-02T17:03:00.000Z",
        "voteCount": 8,
        "content": "Answer is A. \nroles/viewer gives read only access on Project, so it does not create/update any resources.\nroles/compute.admin gives full access to Compute Engine resources."
      },
      {
        "date": "2023-11-16T18:06:00.000Z",
        "voteCount": 7,
        "content": "Answer is C.\n1. The DevOps group needs full control of Compute Engine resources in your development project.  --&gt; So, we grants permissions to create and update Compute Engine instances and their related resources, such as disks, images, and snapshots. \n\nA// Create a custom role at the folder level and grant all compute.instanceAdmin.* permissions to the role. \n\n\n2. They should not have permission to create or update any other resources in the project.  --&gt; We do not grant permissions to create or update any other resources in the project, such as Cloud Storage buckets, Cloud Functions, or BigQuery datasets.\n\nA// Grant the custom role to the DevOps group."
      },
      {
        "date": "2024-01-09T17:00:00.000Z",
        "voteCount": 2,
        "content": "We can only grant a custom role within the project or organization in which we created it. We cannot grant custom roles on other projects or organizations, or on resources within other projects or organizations. Note: We cannot define custom roles at the folder level. So, C cannot be the answer."
      },
      {
        "date": "2024-05-30T09:07:00.000Z",
        "voteCount": 1,
        "content": "ok, yes, we can not create a custom role at folder level, but we can create the custom role at organization level, and then, go to IAM at folder level, and use that custom role that give permissions at folder level. I have just try it and works.\n\nMoreover, it is not possible A, because question says that Dev group has to have permissions in development project. I think question is not correctly written. Becuase A answer allow Dev Grop to create resources in any project in the organization. \n\nBut finally, knowing the question is not writteng correctly, in the exam, I think I will bet for A."
      },
      {
        "date": "2024-05-30T09:10:00.000Z",
        "voteCount": 1,
        "content": "Yes, I have just read another time answer C. C is not possible because says that creation of the custom role is at folder level. That is not possible.\n\nIn real life, we would create the custom role at organization level, and the use it at folder level, so Dev group only have the permissions in their dev projecto.\n\nFor this question, in an exam, we have to pick A. \n\nThank you and good luck"
      },
      {
        "date": "2023-11-16T18:23:00.000Z",
        "voteCount": 1,
        "content": "Furthermore, Google recommends using custom roles to grant the minimum set of permissions that users need to perform their tasks."
      },
      {
        "date": "2023-11-25T14:52:00.000Z",
        "voteCount": 1,
        "content": "completely incorrect\n\n Compute Admin \n(roles/compute.admin)\n\nFull control of all Compute Engine resources.\n\nIf the user will be managing virtual machine instances that are configured to run as a service account, you must also grant the roles/iam.serviceAccountUser role."
      },
      {
        "date": "2024-08-26T09:58:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is A. Take it or leave it"
      },
      {
        "date": "2024-02-26T08:20:00.000Z",
        "voteCount": 2,
        "content": "\"roles/compute.admin\" - Full control of all Compute Engine resources.\n\"roles/compute.instanceAdmin\" - If the user will be managing virtual machine instances that are configured to run as a service account, you must also grant the roles/iam.serviceAccountUser role.\n\nCorrect answer is definitely A"
      },
      {
        "date": "2024-01-14T00:22:00.000Z",
        "voteCount": 1,
        "content": "Google recommends using custom roles"
      },
      {
        "date": "2024-05-20T02:31:00.000Z",
        "voteCount": 3,
        "content": "IAM policy is not for a project, is for organization, it is not B"
      },
      {
        "date": "2024-01-02T14:40:00.000Z",
        "voteCount": 2,
        "content": "A. Grant roles/viewer and roles/compute.admin:\n\u2022\tThe roles/viewer role provides read-only access to most Google Cloud services\n\u2022\tThe roles/compute.admin role gives full control over Compute Engine resources, which is appropriate for the DevOps group's needs."
      },
      {
        "date": "2023-12-16T16:33:00.000Z",
        "voteCount": 3,
        "content": "This one is very tricky, by my opinion correct answer is B.\n\nThis wildcard at the end is important \"grant all compute.instanceAdmin.*\" that means that you need to assign two policies that are already there:\n\n- roles/compute.instanceAdmin.v1\n- roles/compute.instanceAdmin (beta)\n\nSo if the user has compute.instanceAdmin.v1 he will have full compute access without adding the additional one \"roles/iam.serviceAccountUser\". Also another argument against answer A is the Google recommendations to use the basic roles only when there is no predefined roles, and this is valid for all kind of environments not just production."
      },
      {
        "date": "2024-05-20T02:32:00.000Z",
        "voteCount": 1,
        "content": "iam policy is for organization, this question is for a project. So it is not B"
      },
      {
        "date": "2024-03-01T02:24:00.000Z",
        "voteCount": 1,
        "content": "I selected B as well due to the basic roles being mentioned in A, which Google says it's a no no as they are too broad."
      },
      {
        "date": "2023-12-10T07:27:00.000Z",
        "voteCount": 2,
        "content": "Its B, 100%"
      },
      {
        "date": "2023-10-26T03:54:00.000Z",
        "voteCount": 3,
        "content": "it's A, No doubt.\n\n- \"they should not have permission to create or update any other resources in the project\" \n\nthat sentence doesn't state that they don't want give acess to other resources, just not create or update.\nbasic roles/viewer gives permissions for read-only actions:\nhttps://cloud.google.com/iam/docs/understanding-roles\n\n- \"Full control of all Compute Engine resources\"\nCompute Admin (roles/compute.admin) gives full control of all Compute Engine resources.\nhttps://cloud.google.com/iam/docs/understanding-roles#compute.admin\n\ncompute.instanceAdmin.*  does not."
      },
      {
        "date": "2023-10-23T01:31:00.000Z",
        "voteCount": 1,
        "content": "C is definitely wrong. You cant create custom roles at folder level, you can create it at project or organization level"
      },
      {
        "date": "2023-10-12T07:22:00.000Z",
        "voteCount": 3,
        "content": "I think its B. since they want full access to compute engine, so compute.instanceAdmin role but to restrict access to other resources, so no folder level access(C) is needed.\n\nAccording to the web search results, one possible role that can give full access to Compute Engine but no access to other resources is the Compute Instance Admin role. This role allows a user to create and manage instances, disks, images, and snapshots, but not other resources like networks, firewalls, or load balancers."
      },
      {
        "date": "2023-10-11T11:59:00.000Z",
        "voteCount": 4,
        "content": "Explanation:\n\nThe compute.instanceAdmin.* permissions provide full control over Compute Engine resources, which aligns with the requirement for the DevOps group to have complete control over Compute Engine resources.\nCreating an IAM policy and granting these specific permissions ensures the permissions are scoped to the project level, meeting the requirement to grant permissions only within the project and not beyond.\nThis option grants the necessary permissions for Compute Engine management at the project level while limiting the scope to the specified project."
      },
      {
        "date": "2023-09-15T01:25:00.000Z",
        "voteCount": 3,
        "content": "For me its B, until anyone says the contrary and why. It give ONLY the permissions required. No more or less."
      },
      {
        "date": "2024-09-12T11:52:00.000Z",
        "voteCount": 1,
        "content": "iam policy is for organization, this question is for a project, so actually - MORE"
      },
      {
        "date": "2023-09-11T06:14:00.000Z",
        "voteCount": 2,
        "content": "Answer A\nCompute Admin\n\n(roles/compute.admin)\nFull control of all Compute Engine resources."
      },
      {
        "date": "2023-09-09T06:02:00.000Z",
        "voteCount": 2,
        "content": "A is th correct answer as it provied all the required access"
      },
      {
        "date": "2023-09-06T06:40:00.000Z",
        "voteCount": 3,
        "content": "Compute Admin (roles/compute.admin) = Full control of all Compute Engine resources.\nThe only permission to have full control of Computer Engine Resources (as required in question)\nref:  https://cloud.google.com/iam/docs/understanding-roles#compute.admin\n\nCompute.instanceAdmin does NOT allow FULL control of Compute Engine, only \nPermissions to create, modify, and delete virtual machine instances. This includes permissions to create, modify, and delete disks, and also to configure Shielded VM settings."
      },
      {
        "date": "2023-09-04T15:58:00.000Z",
        "voteCount": 1,
        "content": "C meets the requirement of permission with least privilege"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 236,
    "url": "https://www.examtopics.com/discussions/google/view/117120-exam-associate-cloud-engineer-topic-1-question-236/",
    "body": "Your team is running an on-premises ecommerce application. The application contains a complex set of microservices written in Python, and each microservice is running on Docker containers. Configurations are injected by using environment variables. You need to deploy your current application to a serverless Google Cloud cloud solution. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse your existing CI/CD pipeline. Use the generated Docker images and deploy them to Cloud Run. Update the configurations and the required endpoints.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse your existing continuous integration and delivery (CI/CD) pipeline. Use the generated Docker images and deploy them to Cloud Function. Use the same configuration as on-premises.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the existing codebase and deploy each service as a separate Cloud Function. Update the configurations and the required endpoints.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse your existing codebase and deploy each service as a separate Cloud Run. Use the same configurations as on-premises."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 17,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-09-09T06:06:00.000Z",
        "voteCount": 5,
        "content": "A is the correct answer , use your existing CI/cd pipeline and update the configuratons and the required endpoints"
      },
      {
        "date": "2024-10-11T01:17:00.000Z",
        "voteCount": 1,
        "content": "Cloud Run is better than Cloud Functions and we have to update the configurations."
      },
      {
        "date": "2023-11-04T03:46:00.000Z",
        "voteCount": 3,
        "content": "A seems the most effecient"
      },
      {
        "date": "2023-09-15T02:17:00.000Z",
        "voteCount": 2,
        "content": "I think its A"
      },
      {
        "date": "2023-09-14T06:52:00.000Z",
        "voteCount": 1,
        "content": "I vote D"
      },
      {
        "date": "2023-08-08T13:22:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/run/docs/configuring/services/environment-variables\n\"The environment variables defined in the container runtime contract are reserved and cannot be set. In particular, the PORT environment variable is injected inside your container by Cloud Run. You should not set it yourself.\"\n\nHence, by \"using the same configurations of on-premise\" you are just using the environment variables already present on the container."
      },
      {
        "date": "2023-08-09T20:21:00.000Z",
        "voteCount": 8,
        "content": "I was wrong. The right answer is A. The current approach to loadi Docker images into Artifact Registry (formerly Container Registry), is by using CI/CD Pipelines."
      },
      {
        "date": "2023-08-08T09:01:00.000Z",
        "voteCount": 1,
        "content": "app was written for docker image, it likely should be rewritten for cloud run"
      },
      {
        "date": "2023-08-05T23:05:00.000Z",
        "voteCount": 4,
        "content": "I think it's A. \nIt can't be D because you can not use the same configuration as on-premise."
      },
      {
        "date": "2023-08-05T04:40:00.000Z",
        "voteCount": 1,
        "content": "I vote for D"
      },
      {
        "date": "2023-08-05T04:39:00.000Z",
        "voteCount": 1,
        "content": "I vote for D"
      },
      {
        "date": "2023-08-02T06:45:00.000Z",
        "voteCount": 1,
        "content": "I THINK IT'S A"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 237,
    "url": "https://www.examtopics.com/discussions/google/view/117450-exam-associate-cloud-engineer-topic-1-question-237/",
    "body": "You are running multiple microservices in a Kubernetes Engine cluster. One microservice is rendering images. The microservice responsible for the image rendering requires a large amount of CPU time compared to the memory it requires. The other microservices are workloads that are optimized for n2-standard machine types. You need to optimize your cluster so that all workloads are using resources as efficiently as possible. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAssign the pods of the image rendering microservice a higher pod priority than the other microservices.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a node pool with compute-optimized machine type nodes for the image rendering microservice. Use the node pool with general-purpose machine type nodes for the other microservices.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the node pool with general-purpose machine type nodes for the image rendering microservice. Create a node pool with compute-optimized machine type nodes for the other microservices.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the required amount of CPU and memory in the resource requests specification of the image rendering microservice deployment. Keep the resource requests for the other microservices at the default."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 13,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-09-15T02:18:00.000Z",
        "voteCount": 5,
        "content": "Its B, question appear before"
      },
      {
        "date": "2024-03-18T19:04:00.000Z",
        "voteCount": 3,
        "content": "repeated question, the answer is B."
      },
      {
        "date": "2024-01-14T00:27:00.000Z",
        "voteCount": 3,
        "content": "Same as No.196."
      },
      {
        "date": "2023-08-08T09:05:00.000Z",
        "voteCount": 2,
        "content": "repeated question"
      },
      {
        "date": "2023-08-05T23:03:00.000Z",
        "voteCount": 3,
        "content": "B has logic"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 238,
    "url": "https://www.examtopics.com/discussions/google/view/117449-exam-associate-cloud-engineer-topic-1-question-238/",
    "body": "You are working in a team that has developed a new application that needs to be deployed on Kubernetes. The production application is business critical and should be optimized for reliability. You need to provision a Kubernetes cluster and want to follow Google-recommended practices. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a GKE Autopilot cluster. Enroll the cluster in the rapid release channel.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a GKE Autopilot cluster. Enroll the cluster in the stable release channel.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a zonal GKE standard cluster. Enroll the cluster in the stable release channel.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a regional GKE standard cluster. Enroll the cluster in the rapid release channel."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 12,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-10-31T04:43:00.000Z",
        "voteCount": 12,
        "content": "\"recommended practices\" --&gt; Autopilot\n\"optimized for reliability\" --&gt; Stable release"
      },
      {
        "date": "2023-09-09T06:10:00.000Z",
        "voteCount": 5,
        "content": "Autopilot cluster is more relaible and gives more time to fix"
      },
      {
        "date": "2023-09-15T02:23:00.000Z",
        "voteCount": 2,
        "content": "I selected B"
      },
      {
        "date": "2023-08-08T09:17:00.000Z",
        "voteCount": 3,
        "content": "Autopilot is more reliable and stable release gives more time to fix issues in new version of GKE"
      },
      {
        "date": "2023-08-05T23:02:00.000Z",
        "voteCount": 2,
        "content": "Autopilot cluster is recommended by Google"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 239,
    "url": "https://www.examtopics.com/discussions/google/view/117153-exam-associate-cloud-engineer-topic-1-question-239/",
    "body": "You are responsible for a web application on Compute Engine. You want your support team to be notified automatically if users experience high latency for at least 5 minutes. You need a Google-recommended solution with no development cost. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExport Cloud Monitoring metrics to BigQuery and use a Looker Studio dashboard to monitor your web application\u2019s latency.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an alert policy to send a notification when the HTTP response latency exceeds the specified threshold.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement an App Engine service which invokes the Cloud Monitoring API and sends a notification in case of anomalies.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Cloud Monitoring dashboard to observe latency and take the necessary actions when the response latency exceeds the specified threshold."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 18,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-09-26T06:05:00.000Z",
        "voteCount": 6,
        "content": "\"No development cost\" : BigQuery and GAE out\n\"Automatically\" : Option D out (\"take necessary actions\")\nLeft with option B"
      },
      {
        "date": "2024-03-03T15:45:00.000Z",
        "voteCount": 3,
        "content": "B. Cloud Monitoring's alerting policies are designed specifically for this scenario"
      },
      {
        "date": "2023-09-13T07:28:00.000Z",
        "voteCount": 3,
        "content": "\"You need a Google-recommended solution with no development cost\""
      },
      {
        "date": "2023-08-08T09:26:00.000Z",
        "voteCount": 4,
        "content": "https://cloud.google.com/monitoring/alerts#alerting-example"
      },
      {
        "date": "2023-08-02T12:47:00.000Z",
        "voteCount": 2,
        "content": "B seems to be the best answer"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 240,
    "url": "https://www.examtopics.com/discussions/google/view/117154-exam-associate-cloud-engineer-topic-1-question-240/",
    "body": "You have an on-premises data analytics set of binaries that processes data files in memory for about 45 minutes every midnight. The sizes of those data files range from 1 gigabyte to 16 gigabytes. You want to migrate this application to Google Cloud with minimal effort and cost. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a container for the set of binaries. Use Cloud Scheduler to start a Cloud Run job for the container.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a container for the set of binaries. Deploy the container to Google Kubernetes Engine (GKE) and use the Kubernetes scheduler to start the application.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpload the code to Cloud Functions. Use Cloud Scheduler to start the application.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLift and shift to a VM on Compute Engine. Use an instance schedule to start and stop the instance.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 31,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 6,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-10-06T19:48:00.000Z",
        "voteCount": 8,
        "content": "Here's why option D is the most appropriate:\n\n-&gt;Compute Engine: Compute Engine provides virtual machines (VMs) that closely resemble traditional on-premises servers. It allows you to migrate your existing application as-is to the Google Cloud platform.\n\n-&gt;Instance Scheduling: You can schedule the VM instance to start and stop at specific times, such as midnight, to align with your existing processing schedule. This ensures that the application runs at the required time, similar to the on-premises setup.\n\n-&gt;Minimal Effort and Cost: The \"lift and shift\" approach minimizes the need for code modifications or containerization, reducing migration complexity. It also allows you to use the same binaries and configurations as your on-premises setup, saving development effort. You only pay for the VM's compute resources when it's running, making it cost-effective."
      },
      {
        "date": "2023-09-15T02:42:00.000Z",
        "voteCount": 5,
        "content": "This one is a tough one. Ill consider this in my answers:\nCost --&gt; Both are more the same (the process will run at the same frequency)\nEffort --&gt; Create the image in A takes more effort then to option D.\n\nWith this in mind ill choose D. (before i was choosing A)."
      },
      {
        "date": "2023-11-02T17:32:00.000Z",
        "voteCount": 1,
        "content": "Cost will reduce as D option is starting VM at midnight for the job and stop after completion."
      },
      {
        "date": "2024-03-01T02:54:00.000Z",
        "voteCount": 1,
        "content": "For my 2c I think I would have selected A, but D is a possibility.\n\nA because in the long run Cloud Run should cost less than D I believe, as it would take less CPU time through a period of time if data is from 1-16GB.\n\nD could be because it potentially requires \"less\" effort compared to CR."
      },
      {
        "date": "2023-12-05T08:36:00.000Z",
        "voteCount": 1,
        "content": "I think the answer is C.\n\nCloud Run is ideal for applications that have short running times and variable workloads, like this data analytics application. Also, Lifting and shifting the application to a VM on Compute Engine would require to manage the VM by a person, including tasks like patching the operating system, managing security updates, and scaling the VM. This is more effort than using Cloud Run."
      },
      {
        "date": "2023-12-05T16:42:00.000Z",
        "voteCount": 2,
        "content": "Sorry, I must correct, I think it's A."
      },
      {
        "date": "2023-11-02T17:32:00.000Z",
        "voteCount": 3,
        "content": "I vote for D."
      },
      {
        "date": "2023-10-03T01:15:00.000Z",
        "voteCount": 4,
        "content": "D is the correct answer."
      },
      {
        "date": "2023-09-09T06:14:00.000Z",
        "voteCount": 3,
        "content": "D is the correct answer as it requires the minimoal effort"
      },
      {
        "date": "2023-09-08T00:00:00.000Z",
        "voteCount": 1,
        "content": "A is correct answer."
      },
      {
        "date": "2023-08-17T17:44:00.000Z",
        "voteCount": 4,
        "content": "minimal effort is to lift and shift to VM\nminimal cost is to use schedule to start and stop the instance"
      },
      {
        "date": "2023-08-11T02:20:00.000Z",
        "voteCount": 1,
        "content": "Things to consider:\n1. can cloud functions run binary file?\n2. Data file size varies from 1 to 16 GB. so how to determine the VM spec?"
      },
      {
        "date": "2023-08-08T09:38:00.000Z",
        "voteCount": 3,
        "content": "i've made mistake, should be D"
      },
      {
        "date": "2023-08-08T09:38:00.000Z",
        "voteCount": 2,
        "content": "lift and shift is less effort and cost, you don't have to pay for refactoring or creating image."
      },
      {
        "date": "2023-08-05T12:46:00.000Z",
        "voteCount": 4,
        "content": "I would choose D because I want to migrate this application to Google Cloud with minimal effort and cost. \nCloud Run requires to create a container image and this means some kind of development and testing."
      },
      {
        "date": "2023-08-02T12:53:00.000Z",
        "voteCount": 2,
        "content": "A seems to be the less disruptive solution with lower costs"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 241,
    "url": "https://www.examtopics.com/discussions/google/view/117006-exam-associate-cloud-engineer-topic-1-question-241/",
    "body": "You used the gcloud container clusters command to create two Google Cloud Kubernetes (GKE) clusters: prod-cluster and dev-cluster.<br><br>\u2022\tprod-cluster is a standard cluster.<br>\u2022\tdev-cluster is an auto-pilot cluster.<br><br>When you run the kubectl get nodes command, you only see the nodes from prod-cluster. Which commands should you run to check the node status for dev-cluster?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tgcloud container clusters get-credentials dev-cluster<br>kubectl get nodes<br>\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tgcloud container clusters update -generate-password dev-cluster kubectl get nodes<br>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tkubectl config set-context dev-cluster<br>kubectl cluster-info<br>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tkubectl config set-credentials dev-cluster<br>kubectl cluster-info"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 12,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-10-11T02:13:00.000Z",
        "voteCount": 1,
        "content": "gcloud container clusters get-credentials updates a kubeconfig file with appropriate credentials and endpoint information to point kubectl at a specific cluster in Google Kubernetes Engine.\nhttps://cloud.google.com/sdk/gcloud/reference/container/clusters/get-credentials"
      },
      {
        "date": "2023-09-15T07:57:00.000Z",
        "voteCount": 1,
        "content": "Its the A"
      },
      {
        "date": "2023-09-09T06:16:00.000Z",
        "voteCount": 1,
        "content": "A is the correct answer as it , updated the config file"
      },
      {
        "date": "2023-08-24T03:14:00.000Z",
        "voteCount": 4,
        "content": "gcloud container clusters get-credentials updates a kubeconfig file with appropriate credentials and endpoint information to point kubectl at a specific cluster in Google Kubernetes Engine"
      },
      {
        "date": "2023-08-09T00:58:00.000Z",
        "voteCount": 3,
        "content": "gcloud container clusters get-credentials updates a kubeconfig file with appropriate credentials and endpoint information to point kubectl at a specific cluster in Google Kubernetes Engine"
      },
      {
        "date": "2023-08-05T12:49:00.000Z",
        "voteCount": 2,
        "content": "The gcloud container clusters get-credentials command sets the Kubernetes context to the specified cluster (in this case, dev-cluster). This ensures that the subsequent kubectl commands will be executed against the dev-cluster.\nAfter setting the context, the kubectl get nodes command is used to retrieve the node status for the dev-cluster, showing the list of nodes in the cluster."
      },
      {
        "date": "2023-08-01T13:31:00.000Z",
        "voteCount": 2,
        "content": "It should be A"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 242,
    "url": "https://www.examtopics.com/discussions/google/view/117267-exam-associate-cloud-engineer-topic-1-question-242/",
    "body": "You recently discovered that your developers are using many service account keys during their development process. While you work on a long term improvement, you need to quickly implement a process to enforce short-lived service account credentials in your company. You have the following requirements:<br><br>\u2022\tAll service accounts that require a key should be created in a centralized project called pj-sa.<br>\u2022\tService account keys should only be valid for one day.<br><br>You need a Google-recommended solution that minimizes cost. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement a Cloud Run job to rotate all service account keys periodically in pj-sa. Enforce an org policy to deny service account key creation with an exception to pj-sa.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement a Kubernetes CronJob to rotate all service account keys periodically. Disable attachment of service accounts to resources in all projects with an exception to pj-sa.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnforce an org policy constraint allowing the lifetime of service account keys to be 24 hours. Enforce an org policy constraint denying service account key creation with an exception on pj-sa.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnforce a DENY org policy constraint over the lifetime of service account keys for 24 hours. Disable attachment of service accounts to resources in all projects with an exception to pj-sa."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 13,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-09-15T08:04:00.000Z",
        "voteCount": 3,
        "content": "Its C, makes sense"
      },
      {
        "date": "2023-09-09T06:17:00.000Z",
        "voteCount": 3,
        "content": "c is the coorect answer"
      },
      {
        "date": "2023-09-08T00:09:00.000Z",
        "voteCount": 3,
        "content": "C is correct.\nhttps://cloud.google.com/resource-manager/docs/organization-policy/restricting-service-accounts#limit_key_expiry\nhttps://cloud.google.com/resource-manager/docs/organization-policy/restricting-service-accounts#disable_service_account_key_creation"
      },
      {
        "date": "2023-08-09T01:02:00.000Z",
        "voteCount": 3,
        "content": "it should be C"
      },
      {
        "date": "2023-08-05T12:51:00.000Z",
        "voteCount": 4,
        "content": "You can use an org policy to enforce a 24-hour lifetime for service account keys.\nYou can use an org policy to deny service account key creation, with an exception for the pj-sa project.\nThis is a Google-recommended solution and it is relatively inexpensive."
      },
      {
        "date": "2023-08-03T10:55:00.000Z",
        "voteCount": 3,
        "content": "Answer is C. Constraint: constraints/iam.serviceAccountKeyExpiryHours does not accept DENY values so D can not be correct."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 243,
    "url": "https://www.examtopics.com/discussions/google/view/117253-exam-associate-cloud-engineer-topic-1-question-243/",
    "body": "Your company is running a three-tier web application on virtual machines that use a MySQL database. You need to create an estimated total cost of cloud infrastructure to run this application on Google Cloud instances and Cloud SQL. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Google spreadsheet with multiple Google Cloud resource combinations. On a separate sheet, import the current Google Cloud prices and use these prices for the calculations within formulas.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Google Cloud Pricing Calculator and select the Cloud Operations template to define your web application with as much detail as possible.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement a similar architecture on Google Cloud, and run a reasonable load test on a smaller scale. Check the billing information, and calculate the estimated costs based on the real load your system usually handles.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Google Cloud Pricing Calculator to determine the cost of every Google Cloud resource you expect to use. Use similar size instances for the web server, and use your current on-premises machines as a comparison for Cloud SQL.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 23,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-08-11T02:42:00.000Z",
        "voteCount": 9,
        "content": "There is no such thing called \"Cloud Operations template\""
      },
      {
        "date": "2024-08-10T05:41:00.000Z",
        "voteCount": 1,
        "content": "Where is Raaad for such a tough questions no comment . D"
      },
      {
        "date": "2024-03-05T17:40:00.000Z",
        "voteCount": 3,
        "content": "Note to all: there is no such thing as a \"Cloud Operations Template\" =&gt; B is out."
      },
      {
        "date": "2023-10-31T01:46:00.000Z",
        "voteCount": 3,
        "content": "Google Cloud Pricing Calculator is the recommended approach for cost estimation and you provide resources similar to what you see in on-premises for Web servers and add Cloud SQL as a managed service."
      },
      {
        "date": "2023-10-22T02:41:00.000Z",
        "voteCount": 3,
        "content": "D is correct. It can give u a more accurate figure."
      },
      {
        "date": "2023-09-15T08:09:00.000Z",
        "voteCount": 2,
        "content": "Its D, try to simulate using what we have"
      },
      {
        "date": "2023-09-08T00:10:00.000Z",
        "voteCount": 2,
        "content": "D is correct."
      },
      {
        "date": "2023-08-20T23:52:00.000Z",
        "voteCount": 2,
        "content": "es la D"
      },
      {
        "date": "2023-08-09T01:07:00.000Z",
        "voteCount": 2,
        "content": "it's D"
      },
      {
        "date": "2023-08-05T12:56:00.000Z",
        "voteCount": 4,
        "content": "Google Cloud Pricing Calculator, is the recommended approach for creating an estimated total cost of cloud infrastructure. By selecting the relevant Google Cloud resources (such as instances for web servers and Cloud SQL for the database), and specifying similar sizes and configurations, you can obtain a more accurate estimation of the costs."
      },
      {
        "date": "2023-08-03T07:44:00.000Z",
        "voteCount": 1,
        "content": "Use calculation and perations template // it's B"
      },
      {
        "date": "2023-09-22T06:01:00.000Z",
        "voteCount": 4,
        "content": "Dafuq, that doesnt make any sense"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 244,
    "url": "https://www.examtopics.com/discussions/google/view/117440-exam-associate-cloud-engineer-topic-1-question-244/",
    "body": "You have a Bigtable instance that consists of three nodes that store personally identifiable information (PII) data. You need to log all read or write operations, including any metadata or configuration reads of this database table, in your company\u2019s Security Information and Event Management (SIEM) system. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\u2022\tNavigate to Cloud Monitoring in the Google Cloud console, and create a custom monitoring job for the Bigtable instance to track all changes.<br>\u2022\tCreate an alert by using webhook endpoints, with the SIEM endpoint as a receiver.<br>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\u2022\tNavigate to the Audit Logs page in the Google Cloud console, and enable Admin Write logs for the Bigtable instance.<br>\u2022\tCreate a Cloud Functions instance to export logs from Cloud Logging to your SIEM.<br>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\u2022\tNavigate to the Audit Logs page in the Google Cloud console, and enable Data Read, Data Write and Admin Read logs for the Bigtable instance.<br>\u2022\tCreate a Pub/Sub topic as a Cloud Logging sink destination, and add your SIEM as a subscriber to the topic.<br>\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\u2022\tInstall the Ops Agent on the Bigtable instance during configuration.<br>\u2022\tCreate a service account with read permissions for the Bigtable instance.<br>\u2022\tCreate a custom Dataflow job with this service account to export logs to the company\u2019s SIEM system."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 17,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-10-06T20:07:00.000Z",
        "voteCount": 7,
        "content": "Option C is the most appropriate choice for capturing audit and data access logs from a Bigtable instance and sending them to your SIEM system.\n\n1) Enabling Data Read, Data Write, and Admin Read logs for the Bigtable instance ensures that you capture the relevant operations, including read and write operations, as well as administrative reads, in the audit logs.\n2) Creating a Pub/Sub topic as a Cloud Logging sink destination allows you to export the logs from Cloud Logging to Pub/Sub. This is a common approach for sending logs to external systems, including SIEMs.\n3) Adding your SIEM as a subscriber to the Pub/Sub topic ensures that the logs are forwarded to your SIEM system, allowing you to monitor and analyze them for security and compliance purposes.\n\nNB:A Cloud Logging sink destination is a configuration that specifies where logs collected by Google Cloud's Cloud Logging service should be sent or exported. It allows you to control the destination of logs generated by various Google Cloud services, such as Compute Engine, Cloud Storage, BigQuery, and more."
      },
      {
        "date": "2024-10-11T02:26:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/bigtable/docs/audit-logging#permission-type"
      },
      {
        "date": "2024-01-15T02:57:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/bigtable/docs/audit-logging"
      },
      {
        "date": "2023-09-15T08:16:00.000Z",
        "voteCount": 1,
        "content": "Its C!"
      },
      {
        "date": "2023-09-09T06:20:00.000Z",
        "voteCount": 2,
        "content": "C is the correct answer, as it helps you to read and write"
      },
      {
        "date": "2023-09-08T00:11:00.000Z",
        "voteCount": 1,
        "content": "C is correct."
      },
      {
        "date": "2023-08-16T06:12:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/bigtable/docs/audit-logging#available-logs\nB: Admin write logs are already enabled by default"
      },
      {
        "date": "2023-08-09T01:28:00.000Z",
        "voteCount": 2,
        "content": "Data Access audit logs\u2014except for BigQuery\u2014are disabled by default and you need to enable them"
      },
      {
        "date": "2023-08-05T13:00:00.000Z",
        "voteCount": 2,
        "content": "Enabling Admin Write logs for the Bigtable instance in Cloud Logging will capture administrative write actions on the Bigtable instance. This includes any configuration changes and metadata reads related to the Bigtable instance.\nCreating a Cloud Functions instance and configuring it to export logs from Cloud Logging to your SIEM allows you to take the captured logs and route them to your SIEM system in a format that your SIEM can understand. Cloud Functions can act as a serverless function to process and forward the logs to your SIEM using an appropriate method, such as sending them via an API or message queue."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 245,
    "url": "https://www.examtopics.com/discussions/google/view/117163-exam-associate-cloud-engineer-topic-1-question-245/",
    "body": "You want to set up a Google Kubernetes Engine cluster. Verifiable node identity and integrity are required for the cluster, and nodes cannot be accessed from the internet. You want to reduce the operational cost of managing your cluster, and you want to follow Google-recommended practices. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy a private autopilot cluster.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy a public autopilot cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy a standard public cluster and enable shielded nodes.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy a standard private cluster and enable shielded nodes."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 19,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 7,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-09-04T21:14:00.000Z",
        "voteCount": 9,
        "content": "In a private cluster, nodes only have internal IP addresses, which means that nodes and Pods are isolated from the internet by default.\nhttps://cloud.google.com/kubernetes-engine/docs/how-to/private-clusters\n\nShielded GKE Nodes provide strong, verifiable node identity and integrity to increase the security of Google Kubernetes Engine (GKE) nodes.\nNote: For GKE Autopilot clusters, the Shielded GKE Nodes feature is enabled by default and cannot be overridden.\nhttps://cloud.google.com/kubernetes-engine/docs/how-to/shielded-gke-nodes"
      },
      {
        "date": "2024-08-26T10:40:00.000Z",
        "voteCount": 1,
        "content": "scanner2 provided the correct answer."
      },
      {
        "date": "2024-08-13T06:37:00.000Z",
        "voteCount": 1,
        "content": "as per chatgpt\n\nOption A. Deploy a private autopilot cluster is a good choice because it combines:\n\nReduced Operational Costs: Google manages the infrastructure, scaling, and maintenance, minimizing your management overhead.\nEnhanced Security: Private Autopilot clusters use shielded nodes, ensuring verifiable node identity and integrity, and are not accessible from the internet.\nGoogle-Recommended Practices: Autopilot clusters follow best practices for performance and security with minimal configuration required from you."
      },
      {
        "date": "2024-04-08T06:50:00.000Z",
        "voteCount": 1,
        "content": "Deploying a standard private cluster and enabling shielded nodes would meet all the requirements. In a private cluster, nodes are not accessible from the internet by default, ensuring enhanced security. Enabling shielded nodes provides verifiable node identity and integrity, further strengthening the security measures. Additionally, following Google-recommended practices, such as using standard clusters instead of autopilot clusters, offers more control and helps reduce operational costs."
      },
      {
        "date": "2024-07-18T11:41:00.000Z",
        "voteCount": 2,
        "content": "Shielded GKE Nodes feature is enabled by default."
      },
      {
        "date": "2024-07-18T11:41:00.000Z",
        "voteCount": 2,
        "content": "For GKE Autopilot clusters."
      },
      {
        "date": "2024-03-05T17:55:00.000Z",
        "voteCount": 1,
        "content": "Reposting this subcomment because I believe most people are reading this incorrectly, and I want to contribute to the answers ratio:\n\nWhy is everyone so sure that \"operational cost\" refers to work-hours and not money? (i.e. \"operating costs\")\n\nFrom Wikipedia: Operating costs or operational costs, are the expenses which are related to the operation of a business, or to the operation of a device, component, piece of equipment or facility.\n\nThis question is asking to reduce the MONETARY cost. Standard costs less than Autopilot. Accordingly, the answer should be D."
      },
      {
        "date": "2024-03-05T18:09:00.000Z",
        "voteCount": 1,
        "content": "FYI to all, the phrase \"operational cost\" is only found in two GCP documents (both blog articles, not official product documentation), and they use competing definitions... So this is a poorly worded question.\n\nThat said, since this was phrased as \"operational cost of *managing your cluster*\", I think I may have been incorrect. It seems perhaps this is indeed referring to the reduction of work-hours and manual effort needed to manage the cluster."
      },
      {
        "date": "2024-02-04T22:50:00.000Z",
        "voteCount": 1,
        "content": "Since A and D both seem to provide the identity/integrity and internet inaccessibility, it seems the critical distinction is based on \"reduce the operational cost of managing your cluster\". \"Operational cost\" doesn't seem to be a commonly used term (from a quick google search), but \"operating costs\" seem to refer specifically to monetary expenses, not work-hours. Wouldn't a standard cluster be cheaper than autopilot? Thus the answer is D, not A?"
      },
      {
        "date": "2023-12-31T10:56:00.000Z",
        "voteCount": 2,
        "content": "ChatGPT says Option D,\nBy following this approach, you can meet your requirements for node security and access control while also benefitting from the operational cost savings associated with managed GKE clusters and Google's best practices for security."
      },
      {
        "date": "2024-03-03T16:35:00.000Z",
        "voteCount": 1,
        "content": "Stop. Using. Chat GPT.\n\nD is viable for security, but with the standard GKE mode, you'd be responsible for managing the control plane and node-level operations, increasing operational complexity. \"You want to reduce the operational cost of managing your cluster\"\n\nOption A leverages the managed experience of Autopilot with the security of private nodes and shielded GKE for node identity/integrity. The answer is A."
      },
      {
        "date": "2024-03-05T17:54:00.000Z",
        "voteCount": 1,
        "content": "Why is everyone so sure that \"operational cost\" refers to work-hours and not money? (i.e. \"operating costs\")\n\nFrom Wikipedia: Operating costs or operational costs, are the expenses which are related to the operation of a business, or to the operation of a device, component, piece of equipment or facility.\n\nThis question is asking to reduce the MONETARY cost. Standard costs less than Autopilot. Accordingly, the answer should be D."
      },
      {
        "date": "2023-12-28T01:00:00.000Z",
        "voteCount": 1,
        "content": "Autopilot clusters are fully managed and do not have the option to restrict internet access.\nIn a private cluster, nodes are not accessible from the internet by default. Enabling shielded nodes provides verifiable node identity and integrity."
      },
      {
        "date": "2024-03-03T16:39:00.000Z",
        "voteCount": 2,
        "content": "This is incorrect. By default, Autopilot clusters create nodes within a private VPC network. This inherently restricts internet access to the nodes themselves. The answer is A."
      },
      {
        "date": "2023-10-22T02:35:00.000Z",
        "voteCount": 2,
        "content": "A is correct. \u201creduce the operational cost of managing your cluster\u201d, means you need to choose an autopilot cluster. Google will manage your cluster configuration. And about the \u201ccannot be accessed from the internet\u201d you should use shielded nodes."
      },
      {
        "date": "2023-08-11T02:51:00.000Z",
        "voteCount": 4,
        "content": "Note: For GKE Autopilot clusters, the Shielded GKE Nodes feature is enabled by default and cannot be overridden."
      },
      {
        "date": "2023-08-09T20:31:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/kubernetes-engine/docs/how-to/shielded-gke-nodes\n\n\"For GKE Autopilot clusters, the Shielded GKE Nodes feature is enabled by default and cannot be overridden\""
      },
      {
        "date": "2023-08-09T01:48:00.000Z",
        "voteCount": 2,
        "content": "The Shielded GKE node feature is enabled by default for all Autopilot clusters and is impossible to disable manually. \nhttps://www.googlecloudcommunity.com/gc/Architecture-Framework-Community/Manage-GKE-Cluster-Security-with-Autopilot-Mode/ba-p/396435"
      },
      {
        "date": "2023-08-05T04:59:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/kubernetes-engine/docs/how-to/shielded-gke-nodes"
      },
      {
        "date": "2023-08-02T13:31:00.000Z",
        "voteCount": 1,
        "content": "Shielded GKE Nodes provide strong, verifiable node identity and integrity to increase the security of GKE nodes and should be enabled on all GKE clusters.: https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster"
      },
      {
        "date": "2023-10-26T05:51:00.000Z",
        "voteCount": 1,
        "content": "For GKE Autopilot clusters, the Shielded GKE Nodes feature is enabled by default and cannot be overridden."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 246,
    "url": "https://www.examtopics.com/discussions/google/view/117164-exam-associate-cloud-engineer-topic-1-question-246/",
    "body": "Your company wants to migrate their on-premises workloads to Google Cloud. The current on-premises workloads consist of:<br><br>\u2022\tA Flask web application<br>\u2022\tA backend API<br>\u2022\tA scheduled long-running background job for ETL and reporting<br><br>You need to keep operational costs low. You want to follow Google-recommended practices to migrate these workloads to serverless solutions on Google Cloud. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the web application to App Engine and the backend API to Cloud Run. Use Cloud Tasks to run your background job on Compute Engine.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the web application to App Engine and the backend API to Cloud Run. Use Cloud Tasks to run your background job on Cloud Run.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun the web application on a Cloud Storage bucket and the backend API on Cloud Run. Use Cloud Tasks to run your background job on Cloud Run.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun the web application on a Cloud Storage bucket and the backend API on Cloud Run. Use Cloud Tasks to run your background job on Compute Engine."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 24,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-15T10:16:00.000Z",
        "voteCount": 8,
        "content": "it's asking for a serverless solution so A and D are automatically out due to the inclusion of Compute Engine (server-based solution). you would use App engine to run a web app, not cloud storage. That's why it's B"
      },
      {
        "date": "2023-09-18T00:39:00.000Z",
        "voteCount": 3,
        "content": "Its B, it's serveless and low cost"
      },
      {
        "date": "2023-09-04T21:01:00.000Z",
        "voteCount": 3,
        "content": "Since the question is asking about serverless solutions. Here, B is the correct answer.\nMigrate web application to the App Engine.\nMigrate backend API to Cloud Run.\nMigrate scheduled long-running job to Cloud Task that will run the background job using Cloud Run."
      },
      {
        "date": "2023-08-16T06:03:00.000Z",
        "voteCount": 3,
        "content": "Serverless"
      },
      {
        "date": "2023-08-09T01:53:00.000Z",
        "voteCount": 2,
        "content": "B is most reasonable"
      },
      {
        "date": "2023-08-05T05:02:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/architecture/migration-to-gcp-deploying-your-workloads"
      },
      {
        "date": "2023-08-02T13:41:00.000Z",
        "voteCount": 2,
        "content": "B seems the best option"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 247,
    "url": "https://www.examtopics.com/discussions/google/view/130019-exam-associate-cloud-engineer-topic-1-question-247/",
    "body": "Your company is moving its continuous integration and delivery (CI/CD) pipeline to Compute Engine instances. The pipeline will manage the entire cloud infrastructure through code. How can you ensure that the pipeline has appropriate permissions while your system is following security best practices?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\u2022\tAttach a single service account to the compute instances.<br>\u2022\tAdd minimal rights to the service account.<br>\u2022\tAllow the service account to impersonate a Cloud Identity user with elevated permissions to create, update, or delete resources.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\u2022\tAdd a step for human approval to the CI/CD pipeline before the execution of the infrastructure provisioning.<br>\u2022\tUse the human approvals IAM account for the provisioning.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\u2022\tAttach a single service account to the compute instances.<br>\u2022\tAdd all required Identity and Access Management (IAM) permissions to this service account to create, update, or delete resources.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\u2022\tCreate multiple service accounts, one for each pipeline with the appropriate minimal Identity and Access Management (IAM) permissions.<br>\u2022\tUse a secret manager service to store the key files of the service accounts.<br>\u2022\tAllow the CI/CD pipeline to request the appropriate secrets during the execution of the pipeline.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-09-14T08:38:00.000Z",
        "voteCount": 1,
        "content": "It seems, you all just use chat gpt to get the answer. But did you even notice it says one they need to move only one pipeline?"
      },
      {
        "date": "2024-09-14T08:39:00.000Z",
        "voteCount": 2,
        "content": "By the way, chat gpt o1-preview says: that A is the answer\n\nPrinciple of Least Privilege: By assigning minimal rights to the service account, you limit access to only what's necessary for regular operations.\nImpersonation for Elevated Actions: Allowing the service account to impersonate a Cloud Identity user with elevated permissions ensures that higher-level permissions are used only when needed and are tightly controlled.\nSecurity Best Practices: This approach avoids the use of long-lived credentials or storing service account keys, reducing potential security risks."
      },
      {
        "date": "2024-03-03T16:51:00.000Z",
        "voteCount": 3,
        "content": "Option D combines the principle of least privilege with granular permissions, secure credential management, and controlled access during pipeline execution."
      },
      {
        "date": "2024-02-14T13:21:00.000Z",
        "voteCount": 3,
        "content": "Options A and C both involve attaching a single service account to the compute instances, which goes against the principle of least privilege and increases the risk if that single account is compromised. Option B introduces human approval into the CI/CD pipeline, which could slow down the deployment process and might not be feasible for fully automated deployments. Therefore, option D is the most suitable choice for ensuring both security and efficiency in the CI/CD pipeline setup."
      },
      {
        "date": "2024-01-02T16:45:00.000Z",
        "voteCount": 2,
        "content": "Principle of Least Privilege: Creating separate service accounts for different aspects of your CI/CD pipeline allows you to adhere to the principle of least privilege. This means each service account is granted only the permissions necessary for its specific role in the pipeline.\n\nSecurity and Organization: Using multiple service accounts makes it easier to manage permissions, track activities, and audit usage for specific tasks or components of your CI/CD process.\n\nSecret Management: Storing the service account key files in a secret manager service (like Google Cloud Secret Manager) enhances security. This approach securely manages and accesses these keys, reducing the risk of unauthorized access or exposure.\n\nDynamic Access: Allowing the CI/CD pipeline to request the appropriate secrets during execution ensures that credentials are provided only when needed and aren't unnecessarily exposed or stored in less secure environments."
      },
      {
        "date": "2024-01-02T16:46:00.000Z",
        "voteCount": 2,
        "content": "A. Single Service Account with Impersonation: While using a single service account with minimal rights and impersonation can work, it introduces complexity and might not offer the same level of granularity and security as multiple service accounts. Impersonation also adds an additional layer that needs to be securely managed."
      },
      {
        "date": "2023-12-31T10:54:00.000Z",
        "voteCount": 2,
        "content": "ChatGPT says Option D,\nBy following this approach, you can ensure that your CI/CD pipeline has appropriate permissions while adhering to security best practices, including the principle of least privilege and secure management of credentials."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 248,
    "url": "https://www.examtopics.com/discussions/google/view/129835-exam-associate-cloud-engineer-topic-1-question-248/",
    "body": "Your application stores files on Cloud Storage by using the Standard Storage class. The application only requires access to files created in the last 30 days. You want to automatically save costs on files that are no longer accessed by the application. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an object lifecycle on the storage bucket to change the storage class to Archive Storage for objects with an age over 30 days.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a cron job in Cloud Scheduler to call a Cloud Functions instance every day to delete files older than 30 days.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a retention policy on the storage bucket of 30 days, and lock the bucket by using a retention policy lock.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable object versioning on the storage bucket and add lifecycle rules to expire non-current versions after 30 days."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 8,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-10-03T06:48:00.000Z",
        "voteCount": 1,
        "content": "It is cheaper to delete the files as there is no requirement to keep them."
      },
      {
        "date": "2024-10-01T00:10:00.000Z",
        "voteCount": 1,
        "content": "This is the same logic as Microsoft:\nwhen you have 2 options: one needs to pay and the other is free, choose the one with fee.\nThat is the right answer."
      },
      {
        "date": "2024-09-12T18:33:00.000Z",
        "voteCount": 1,
        "content": "Cloud Storage lifecycle management, you can automatically transition objects between storage classes based on certain conditions, such as age. Since your application only requires access to files created in the last 30 days, you can set a lifecycle rule to move files that are older than 30 days to Archive Storage, which offers the lowest storage costs but is designed for infrequent access."
      },
      {
        "date": "2024-09-05T00:27:00.000Z",
        "voteCount": 1,
        "content": "This option utilizes Cloud Storage's built-in object lifecycle management feature, which can automatically transition files older than 30 days to Archive Storage, thereby saving storage costs without requiring manual management. In comparison, option B is feasible but more complex and does not align with best practices."
      },
      {
        "date": "2024-08-22T20:11:00.000Z",
        "voteCount": 2,
        "content": "B is correct"
      },
      {
        "date": "2024-08-17T19:45:00.000Z",
        "voteCount": 1,
        "content": "A\nA is Correct because it suggests changing the storage class to Archive Storage for objects with an age of over 30 days through a lifecycle rule on the storage bucket. This is a cost-effective solution because Google Cloud Storage offers different storage classes with varying costs. The \"Archive Storage\" class is designed for infrequently accessed data and comes at a lower cost compared to the standard storage class. Using a lifecycle rule to transition objects older than 30 days to the Archive Storage class helps save costs by utilizing a more cost-efficient storage class for older data."
      },
      {
        "date": "2024-07-23T08:51:00.000Z",
        "voteCount": 1,
        "content": "Another classic annoyingly vague question, but I would have to go with A because 'normally' you would keep files for longer than 30 days. If it is ok to delete, then B"
      },
      {
        "date": "2024-07-18T12:44:00.000Z",
        "voteCount": 2,
        "content": "I would say A even though it doesn't mention that the files are still needed. As keeping archived version/ backups is best practise in IT generally. If the question explicitly mentioned that backups were taken using some other method or that old versions were no longer required then B would be the correct answer."
      },
      {
        "date": "2024-07-18T12:13:00.000Z",
        "voteCount": 1,
        "content": "At question not to talk about any analytics or critical data for audit and so on. You need only save cost, so B is answer."
      },
      {
        "date": "2024-07-18T12:08:00.000Z",
        "voteCount": 2,
        "content": "Ridiculous question."
      },
      {
        "date": "2024-07-17T16:06:00.000Z",
        "voteCount": 1,
        "content": "B- Deleting the files means you no longer have to pay for storing them"
      },
      {
        "date": "2024-04-05T07:28:00.000Z",
        "voteCount": 1,
        "content": "There is no requirement listed to keep the files.  Deletion is the best option."
      },
      {
        "date": "2024-03-26T03:19:00.000Z",
        "voteCount": 2,
        "content": "B or D are correct in my opinion as A does not delete unused files (archive storage is not as cheap as deleting). C only sets a retention policy (https://cloud.google.com/storage/docs/bucket-lock) which means you can only delete files over 30 days but it does not enable automatic deletion of old files. Object Versioning preserves deleted objects as versioned, noncurrent objects that remain accessible in your bucket until explicitly removed (https://cloud.google.com/storage/docs/object-versioning)."
      },
      {
        "date": "2024-03-22T20:50:00.000Z",
        "voteCount": 1,
        "content": "Best choice is A"
      },
      {
        "date": "2024-03-20T02:35:00.000Z",
        "voteCount": 1,
        "content": "Your application stores files on Cloud Storage by using the Standard Storage class. The application only requires access to files created in the last 30 days. You want to automatically save costs on files that are no longer accessed by the application. What should you do?\n\nA. Create an object lifecycle on the storage bucket to change the storage class to Archive Storage for objects with an age over 30 days.\nB. Create a cron job in Cloud Scheduler to call a Cloud Functions instance every day to delete files older than 30 days.\nC. Create a retention policy on the storage bucket of 30 days, and lock the bucket by using a retention policy lock.\nD. Enable object versioning on the storage bucket and add lifecycle rules to expire non-current versions after 30 days.\n\nA provides a simple, automated, and cost-effective solution for your scenario."
      },
      {
        "date": "2024-05-19T07:59:00.000Z",
        "voteCount": 2,
        "content": "A is not cost-effective solution, so no longer accessed files persist instead of eliminate them"
      },
      {
        "date": "2024-01-15T03:10:00.000Z",
        "voteCount": 4,
        "content": "Shouldn't we delete files that are over 30 days old because they are unnecessary?"
      },
      {
        "date": "2024-02-08T03:44:00.000Z",
        "voteCount": 2,
        "content": "Options B and C involve deleting files, which may not be suitable if you need to retain the files for compliance or historical purposes."
      },
      {
        "date": "2024-03-04T01:11:00.000Z",
        "voteCount": 2,
        "content": "I don't know the answer TBH, but the Q doesn't say anything about retaining files. It does ask to lower cost as much as possible, so deleting them would be cheaper, right?"
      },
      {
        "date": "2024-01-14T11:08:00.000Z",
        "voteCount": 1,
        "content": "Option A, use of object lifecycle"
      },
      {
        "date": "2024-05-19T07:58:00.000Z",
        "voteCount": 1,
        "content": "save costs on files that are no longer accessed by the application; DELETION"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 249,
    "url": "https://www.examtopics.com/discussions/google/view/130018-exam-associate-cloud-engineer-topic-1-question-249/",
    "body": "Your manager asks you to deploy a workload to a Kubernetes cluster. You are not sure of the workload's resource requirements or how the requirements might vary depending on usage patterns, external dependencies, or other factors. You need a solution that makes cost-effective recommendations regarding CPU and memory requirements, and allows the workload to function consistently in any situation. You want to follow Google-recommended practices. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the Horizontal Pod Autoscaler for availability, and configure the cluster autoscaler for suggestions.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the Horizontal Pod Autoscaler for availability, and configure the Vertical Pod Autoscaler recommendations for suggestions.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the Vertical Pod Autoscaler recommendations for availability, and configure the Cluster autoscaler for suggestions.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the Vertical Pod Autoscaler recommendations for availability, and configure the Horizontal Pod Autoscaler for suggestions."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-10T14:58:00.000Z",
        "voteCount": 7,
        "content": "Horizontal Pod Autoscaler (HPA): It automatically scales the number of pods in a deployment, replica set, or stateful set based on observed CPU utilization (or, with custom metrics support, on some other application-provided metrics). This helps maintain availability by ensuring that your application has the necessary number of pods to handle the workload.\n\nVertical Pod Autoscaler (VPA): It automatically adjusts the CPU and memory reservations for your pods to help \"right size\" your applications. This is particularly useful when you're unsure of the resource requirements. VPA makes recommendations for the appropriate CPU and memory settings based on usage patterns, which can be very effective for cost optimization.\n\nThis combination ensures that your workload is both horizontally scalable (to handle changes in demand) and vertically optimized (to use resources efficiently), following Google-recommended practices for Kubernetes workloads."
      },
      {
        "date": "2024-03-03T17:12:00.000Z",
        "voteCount": 2,
        "content": "I believe B is the best choice: HPA ensures availability by scaling the number of pods based on metrics (like CPU utilization). The VPA analyzes resource utilization and provides recommendations for CPU and memory requests and limits. This is key for right-sizing your pods for optimal cost efficiency."
      },
      {
        "date": "2024-01-07T03:12:00.000Z",
        "voteCount": 1,
        "content": "D is the correct answer"
      },
      {
        "date": "2024-03-04T01:25:00.000Z",
        "voteCount": 1,
        "content": "VPA is not recommended as per Google requirements, so that answer must be wrong."
      },
      {
        "date": "2024-01-02T16:55:00.000Z",
        "voteCount": 3,
        "content": "Horizontal Pod Autoscaler (HPA): HPA automatically adjusts the number of pods in a deployment based on observed CPU utilization (or other select metrics). This is crucial for maintaining the availability of your workload, especially if the workload experiences varying levels of traffic or load. HPA ensures that there are enough pods to handle the load, scaling out (adding more pods) when demand is high and scaling in (removing pods) when demand is low.\n\n\nVertical Pod Autoscaler (VPA) Recommendations: VPA automatically adjusts the CPU and memory reservations for pods in a deployment. It can operate in a mode where it only provides recommendations (without automatically applying them), which is useful for understanding the resource needs of your workload. VPA recommendations can guide you in setting appropriate CPU and memory limits based on the observed usage of your workload."
      },
      {
        "date": "2024-01-02T16:56:00.000Z",
        "voteCount": 1,
        "content": "A. Cluster Autoscaler for Suggestions: While the Cluster Autoscaler is useful for scaling the number of nodes in the cluster, it doesn\u2019t provide recommendations on the CPU and memory requirements for individual pods.\n\nC. VPA for Availability, Cluster Autoscaler for Suggestions: VPA can automatically adjust pod sizes, but using it for ensuring availability might lead to frequent and potentially disruptive pod restarts. The Cluster Autoscaler is again more about node-level scaling rather than providing pod resource recommendations.\n\nD. VPA for Availability, HPA for Suggestions: This configuration isn't ideal as VPA's primary function isn't about maintaining high availability but rather about optimizing resource allocation. HPA, on the other hand, is specifically designed for scaling the number of pods based on load, which is directly related to availability."
      },
      {
        "date": "2024-01-01T16:44:00.000Z",
        "voteCount": 2,
        "content": "Ans is B\nB. Configure the Horizontal Pod Autoscaler for availability, and configure the Vertical Pod Autoscaler recommendations for suggestions.\n\nThis approach allows you to manage the number of pods based on the workload (HPA) and get optimal CPU and memory settings for each pod (VPA), which is in line with Google-recommended practices for managing Kubernetes workloads with uncertain resource requirements. This combination ensures that your workload can function consistently in varying situations by automatically adjusting both the quantity of pods and the resources each pod is allocated."
      },
      {
        "date": "2023-12-31T10:52:00.000Z",
        "voteCount": 1,
        "content": "ChatGPT says option D,\nBy configuring VPA for resource recommendations based on actual usage patterns and HPA for scaling pod instances based on demand, you can ensure that your workload is both cost-effective and capable of adapting to varying resource requirements, all while following Google-recommended practices for Kubernetes workloads."
      },
      {
        "date": "2024-03-03T17:07:00.000Z",
        "voteCount": 1,
        "content": "Chat strikes again. Option B provides a more tailored and Google-recommended approach given the uncertainty about the workload's resource needs. It prioritizes establishing an efficient baseline with VPA before relying on HPA for scaling."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 250,
    "url": "https://www.examtopics.com/discussions/google/view/129836-exam-associate-cloud-engineer-topic-1-question-250/",
    "body": "You need to migrate invoice documents stored on-premises to Cloud Storage. The documents have the following storage requirements:<br><br>\u2022\tDocuments must be kept for five years.<br>\u2022\tUp to five revisions of the same invoice document must be stored, to allow for corrections.<br>\u2022\tDocuments older than 365 days should be moved to lower cost storage tiers.<br><br>You want to follow Google-recommended practices to minimize your operational and development costs. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable retention policies on the bucket, and use Cloud Scheduler to invoke a Cloud Function to move or delete your documents based on their metadata.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable retention policies on the bucket, use lifecycle rules to change the storage classes of the objects, set the number of versions, and delete old files.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable object versioning on the bucket, and use Cloud Scheduler to invoke a Cloud Functions instance to move or delete your documents based on their metadata.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable object versioning on the bucket, use lifecycle conditions to change the storage class of the objects, set the number of versions, and delete old files.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 6,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-03-22T20:56:00.000Z",
        "voteCount": 1,
        "content": "Follow D"
      },
      {
        "date": "2024-03-05T18:46:00.000Z",
        "voteCount": 2,
        "content": "I believe the answer is actually B, and D won't cut it.\n\nD does not address the need \"Documents must be kept for five years.\" A retention policy is required, otherwise someone can just delete a document.\n\nSomeone else suggested that you can't set object versioning with life cycle rules, but that's not quite true. https://cloud.google.com/storage/docs/lifecycle\n\nYou can, but it does require object versioning to be enabled... So none of these answers are ideal, but I think the omission of setting a retention policy explicitly misses the first requirement stated."
      },
      {
        "date": "2024-03-07T17:41:00.000Z",
        "voteCount": 4,
        "content": "Upon further reading, it seems Retention Policies and Object Versioning are mutually exclusive, meaning B cannot cover the second requirement. https://cloud.google.com/storage/docs/object-versioning\n\nIt's not explicitly stated, but it is implied that Object Versioning can prevent total deletion (i.e. deleting a live version of an object moves it to a non-current version).\n\nI guess the answer will have to be D"
      },
      {
        "date": "2024-01-14T11:15:00.000Z",
        "voteCount": 3,
        "content": "The correct answer is **D. Enable object versioning on the bucket, use lifecycle conditions to change the storage class of the objects, set the number of versions, and delete old files**.\n\nHere's why:\n- **Object versioning** allows you to keep up to five revisions of the same invoice document.\n- **Lifecycle conditions** can be used to automatically change the storage class of objects older than 365 days to a lower-cost storage tier.\n- You can also set the number of versions to keep and automatically delete old files, which helps to manage storage costs effectively. \n\nThis approach aligns with Google-recommended practices and helps to minimize operational and development costs."
      },
      {
        "date": "2024-01-05T04:52:00.000Z",
        "voteCount": 1,
        "content": "Obvious answer is   D"
      },
      {
        "date": "2024-01-02T17:10:00.000Z",
        "voteCount": 4,
        "content": "- Object Versioning: Enabling object versioning on the Cloud Storage bucket allows you to store up to five revisions of the same invoice document. This satisfies the requirement for keeping multiple versions of each document for corrections.\n\n- Lifecycle Conditions: Google Cloud Storage allows you to define lifecycle conditions for objects within a bucket. These conditions can automatically change the storage class of objects when they meet certain criteria, such as age. After 365 days, you can automatically move documents to lower-cost storage classes like Nearline, Coldline, or Archive, which reduces storage costs while still retaining the data.\n\n- Version Management and Deletion: The lifecycle rules can also be configured to manage the number of object versions retained and to delete old versions or objects, ensuring compliance with the five-year retention requirement."
      },
      {
        "date": "2024-01-02T17:11:00.000Z",
        "voteCount": 4,
        "content": "Why B is not correct:\nLifecycle rules in Google Cloud Storage can be used to manage the deletion of old versions of objects. However, it's important to note that lifecycle rules alone do not set the number of versions to keep; they can only delete versions based on age or other criteria.\nLifecycle rules in Google Cloud Storage do not have a direct setting to limit the number of object versions (like keeping only the last five versions). Object versioning in Google Cloud Storage keeps all versions of an object until they are explicitly deleted (either manually or through lifecycle rules)."
      },
      {
        "date": "2023-12-31T10:42:00.000Z",
        "voteCount": 1,
        "content": "Per ChatGPT, Option D aligns with Google-recommended practices for managing objects in Cloud Storage, including versioning, lifecycle management, and cost optimization, making it the best choice for the given scenario."
      },
      {
        "date": "2023-12-29T22:53:00.000Z",
        "voteCount": 1,
        "content": "D. Enable object versioning on the bucket, use lifecycle conditions to change the storage class of the objects, set the number of versions, and delete old files."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 251,
    "url": "https://www.examtopics.com/discussions/google/view/129837-exam-associate-cloud-engineer-topic-1-question-251/",
    "body": "You installed the Google Cloud CLI on your workstation and set the proxy configuration. However, you are worried that your proxy credentials will be recorded in the gcloud CLI logs. You want to prevent your proxy credential from being logged. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure username and password by using gcloud config set proxy/username and gcloud config set proxy/password commands.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEncode username and password in sha256 encoding, and save in to a text file. Use filename as a value in the gcloud config set core/custom_ca_certs_file command.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tProvide values for CLOUDSDK_PROXY_USERNAME and CLOUDSDK_PROXY_PASSWORD in the gcloud CLI tool configuration file.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the CLOUDSDK_PROXY_USERNAME and CLOUDSDK_PROXY_PASSWORD properties by using environment variables in your command line tool.<br>\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 9,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-10-04T03:29:00.000Z",
        "voteCount": 1,
        "content": "Alternatively, to avoid having the proxy credentials recorded in any logs (such as shell history or gcloud CLI logs) or in the gcloud CLI configuration file, you can set the properties using environment variables\n\nhttps://cloud.google.com/sdk/docs/proxy-settings"
      },
      {
        "date": "2024-10-11T02:49:00.000Z",
        "voteCount": 1,
        "content": "export CLOUDSDK_PROXY_USERNAME [USERNAME]\nexport CLOUDSDK_PROXY_PASSWORD [PASSWORD]"
      },
      {
        "date": "2024-03-03T17:25:00.000Z",
        "voteCount": 1,
        "content": "Option D is the best answer."
      },
      {
        "date": "2024-02-28T08:00:00.000Z",
        "voteCount": 4,
        "content": "Answer is D.  See Google Docs: https://cloud.google.com/sdk/docs/proxy-settings\n\nAlternatively, to avoid having the proxy credentials recorded in any logs (such as shell history or gcloud CLI logs) or in the gcloud CLI configuration file, you can set the properties using environment variables.\n\nexport CLOUDSDK_PROXY_USERNAME [USERNAME]\n\nexport CLOUDSDK_PROXY_PASSWORD [PASSWORD]"
      },
      {
        "date": "2024-02-17T09:21:00.000Z",
        "voteCount": 1,
        "content": "To avoid shell CLI history, logs or conf. file. Google recommends to set it on env. variables related with no storing of those values. Option D."
      },
      {
        "date": "2024-01-04T04:19:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/sdk/docs/proxy-settings"
      },
      {
        "date": "2024-01-02T17:17:00.000Z",
        "voteCount": 4,
        "content": "- Using Environment Variables: By setting the proxy credentials as environment variables (CLOUDSDK_PROXY_USERNAME and CLOUDSDK_PROXY_PASSWORD), you avoid having to enter them directly into the CLI tool where they might be logged. Environment variables are a common way to securely pass sensitive information like credentials.\n\n- No Logging of Credentials: The gcloud CLI typically does not log environment variables, so your credentials should be safe from being recorded in the CLI logs.\n\n- Ease of Use: Setting environment variables is straightforward and does not require modifying configuration files or encoding credentials."
      },
      {
        "date": "2024-01-02T17:17:00.000Z",
        "voteCount": 2,
        "content": "A. Using gcloud config set for Username and Password: This approach directly enters the credentials into the gcloud CLI configuration, which could potentially be logged or exposed in the configuration file.\nB. Encoding Credentials and Custom CA Certs File: This option suggests a method that isn't directly related to setting proxy credentials. The core/custom_ca_certs_file is used for specifying a custom CA (Certificate Authority) certificate file, not for proxy credentials.\nC. Using the Configuration File: Modifying the configuration file to include proxy credentials might expose them in plain text within the file, which could be a security risk. It's generally safer to use environment variables for this purpose."
      },
      {
        "date": "2024-01-01T16:43:00.000Z",
        "voteCount": 1,
        "content": "Ans is D"
      },
      {
        "date": "2023-12-31T10:41:00.000Z",
        "voteCount": 1,
        "content": "Per ChatGPT, Option C is the most appropriate choice for securely providing proxy credentials to the gcloud CLI tool without risking exposure in logs or other outputs."
      },
      {
        "date": "2024-03-03T17:24:00.000Z",
        "voteCount": 1,
        "content": "I think this test may have rocked you due to ChatGPT. Storing credentials in the config file increases the risk of exposure if the file is compromised. It's the same problem as A. The answer is D."
      },
      {
        "date": "2023-12-29T22:53:00.000Z",
        "voteCount": 1,
        "content": "D. Set the CLOUDSDK_PROXY_USERNAME and CLOUDSDK_PROXY_PASSWORD properties by using environment variables in your command line tool."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 252,
    "url": "https://www.examtopics.com/discussions/google/view/129838-exam-associate-cloud-engineer-topic-1-question-252/",
    "body": "Your company developed an application to deploy on Google Kubernetes Engine. Certain parts of the application are not fault-tolerant and are allowed to have downtime. Other parts of the application are critical and must always be available. You need to configure a Google Kubernetes Engine cluster while optimizing for cost. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a cluster with a single node-pool by using standard VMs. Label he fault-tolerant Deployments as spot_true.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a cluster with a single node-pool by using Spot VMs. Label the critical Deployments as spot_false.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a cluster with both a Spot VM node pool and a node pool by using standard VMs. Deploy the critical deployments on the Spot VM node pool and the fault-tolerant deployments on the node pool by using standard VMs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a cluster with both a Spot VM node pool and a nods pool by using standard VMs. Deploy the critical deployments on the node pool by using standard VMs and the fault-tolerant deployments on the Spot VM node pool.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 11,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-03-03T17:46:00.000Z",
        "voteCount": 3,
        "content": "Correct answer is D."
      },
      {
        "date": "2024-02-20T18:32:00.000Z",
        "voteCount": 3,
        "content": "D is the right answer here. Spot Vms are fault-tolerant."
      },
      {
        "date": "2024-01-14T12:54:00.000Z",
        "voteCount": 2,
        "content": "The correct answer is D. Create a cluster with both a Spot VM node pool and a node pool by using standard VMs. Deploy the critical deployments on the node pool by using standard VMs and the fault-tolerant deployments on the Spot VM node pool.\n\nHere\u2019s why:\n\nSpot VMs are cost-effective but can be preempted at any time, making them suitable for fault-tolerant parts of the application that can afford downtime.\nStandard VMs, while more expensive, provide consistent availability and are ideal for critical parts of the application that must always be available.\nBy creating a cluster with both types of node pools, you can optimize for cost while ensuring the availability of critical application components."
      },
      {
        "date": "2024-01-02T17:22:00.000Z",
        "voteCount": 3,
        "content": "Spot VM Node Pool for Fault-Tolerant Parts: Spot VMs in GKE are cost-effective but can be preempted (terminated) by Google Cloud with little notice if their resources are needed elsewhere. They are suitable for workloads that can handle interruptions, like the fault-tolerant parts of your application.\n\nStandard VM Node Pool for Critical Parts: Standard VMs offer more reliability and are not subject to preemption like Spot VMs. Using a standard VM node pool for the critical parts of your application ensures they remain available and are not disrupted by potential preemptions."
      },
      {
        "date": "2024-01-01T16:45:00.000Z",
        "voteCount": 2,
        "content": "Ans is D"
      },
      {
        "date": "2023-12-31T10:40:00.000Z",
        "voteCount": 1,
        "content": "Per ChatGPT, Option C aligns with the requirements of optimizing cost, ensuring fault tolerance for certain parts of the application, and maintaining high availability for critical parts by using a combination of Spot VM node pools and standard VM node pools in the same GKE cluster."
      },
      {
        "date": "2024-02-01T03:40:00.000Z",
        "voteCount": 2,
        "content": "@KelvinToo  chatgpt answers are not always correct."
      },
      {
        "date": "2024-01-01T08:12:00.000Z",
        "voteCount": 1,
        "content": "Critical deployments is not fault-tolerant. Spot VM can restart after 24 hours."
      },
      {
        "date": "2023-12-29T22:53:00.000Z",
        "voteCount": 3,
        "content": "D. Create a cluster with both a Spot VM node pool and a nods pool by using standard VMs. Deploy the critical deployments on the node pool by using standard VMs and the fault-tolerant deployments on the Spot VM node pool."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 253,
    "url": "https://www.examtopics.com/discussions/google/view/129839-exam-associate-cloud-engineer-topic-1-question-253/",
    "body": "You need to deploy an application in Google Cloud using serverless technology. You want to test a new version of the application with a small percentage of production traffic. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the application to Cloud Run. Use gradual rollouts for traffic splitting.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the application to Google Kubernetes Engine. Use Anthos Service Mash for traffic splitting.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the application to Cloud Functions. Specify the version number in the functions name.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the application to App Engine. For each new version, create a new service."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 16,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-18T09:09:00.000Z",
        "voteCount": 10,
        "content": "I would go for A. Here is why:\n\nCore requirements: \n1. Serverless technology\n2. Gradual deployment\n\nA: Cloud Run is serverless and supports gradual deployments with the help of tags and gcloud run services update-traffic.\nB: GKE is not considered serverless. Anthos Service Mash does support gradual deployments.\nC: Cloud Functions is serverless but specifying the version number in the functions name doesn't help to achieve gradual deployments. Instead using revisions or tags in combination with gcloud run services update-traffic is required.\nD: App Engine is serverless. For gradual deployments, you should create a new version of your service and then use gcloud app services set-traffic to split your traffic. Don't create a new service for each new version."
      },
      {
        "date": "2024-10-09T03:00:00.000Z",
        "voteCount": 1,
        "content": "Answer A\nD. also comes close with App Engine (since serverless and can split traffic in versions), but here is says create new service for each version where it's not needed in App engine."
      },
      {
        "date": "2024-03-03T17:53:00.000Z",
        "voteCount": 2,
        "content": "A is the correct answer."
      },
      {
        "date": "2024-01-14T12:56:00.000Z",
        "voteCount": 4,
        "content": "The correct answer is **A. Deploy the application to Cloud Run. Use gradual rollouts for traffic splitting**.\n\nHere's why:\n- **Cloud Run** is a serverless platform that allows you to deploy and run your applications without worrying about infrastructure management. It supports deploying new versions of an application and gradually rolling out updates using traffic splitting. This makes it ideal for testing a new version of an application with a small percentage of production traffic.\n- The other options do not provide the same level of support for serverless deployment and traffic splitting for testing new versions of an application."
      },
      {
        "date": "2023-12-31T10:39:00.000Z",
        "voteCount": 3,
        "content": "Per ChatGPT, Option A, deploying the application to Cloud Run and using gradual rollouts for traffic splitting, is the best choice for testing a new version of the application with a small percentage of production traffic while leveraging serverless technology in Google Cloud."
      },
      {
        "date": "2023-12-29T22:54:00.000Z",
        "voteCount": 1,
        "content": "A. Deploy the application to Cloud Run. Use gradual rollouts for traffic splitting."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 254,
    "url": "https://www.examtopics.com/discussions/google/view/129840-exam-associate-cloud-engineer-topic-1-question-254/",
    "body": "Your company's security vulnerability management policy wants a member of the security team to have visibility into vulnerabilities and other OS metadata for a specific Compute Engine instance. This Compute Engine instance hosts a critical application in your Google Cloud project. You need to implement your company's security vulnerability management policy. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\u2022\tEnsure that the Ops Agent is installed on the Compute Engine instance.<br>\u2022\tCreate a custom metric in the Cloud Monitoring dashboard.<br>\u2022\tProvide the security team member with access to this dashboard.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\u2022\tEnsure that the Ops Agent is installed on the Compute Engine instance.<br>\u2022\tProvide the security team member roles/osconfig.inventoryViewer permission.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\u2022\tEnsure that the OS Config agent is installed on the Compute Engine instance.<br>\u2022\tProvide the security team member roles/osconfig.vulnerabilityReportViewer permission.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\u2022\tEnsure that the OS Config agent is installed on the Compute Engine instance.<br>\u2022\tCreate a log sink to BigQuery dataset.<br>\u2022\tProvide the security team member with access to this dataset."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 13,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-02T17:36:00.000Z",
        "voteCount": 10,
        "content": "Ops Agent: The Ops Agent is primarily used for collecting system and application metrics, as well as logs in Google Cloud. It is adept at monitoring the performance and health of applications and virtual machines but does not specialize in vulnerability assessment or OS-level inventory management.\n\nOS Config Agent: This agent is specifically designed for OS configuration, inventory management, and vulnerability reporting in Google Cloud. It can gather and report on system-level inventory information (like installed packages) and OS vulnerabilities."
      },
      {
        "date": "2024-01-02T17:36:00.000Z",
        "voteCount": 4,
        "content": "Visibility into Vulnerabilities and Other OS Metadata:\n\nTo access vulnerability data specifically, the roles/osconfig.vulnerabilityReportViewer role is more appropriate. This role is designed to provide access to vulnerability reports generated by the OS Config agent.\nTo access general OS metadata, the osconfig.inventoryViewer role is suitable, as it allows the user to view inventory information collected by the OS Config agent."
      },
      {
        "date": "2024-01-02T17:36:00.000Z",
        "voteCount": 5,
        "content": "In the context of the provided options, none of them specifically combines both the osconfig.inventoryViewer and roles/osconfig.vulnerabilityReportViewer roles with the OS Config agent. However, Option C (ensuring the OS Config agent is installed and providing the roles/osconfig.vulnerabilityReportViewer permission) comes closest to fulfilling the requirement, particularly for the critical aspect of vulnerability visibility."
      },
      {
        "date": "2024-03-03T18:03:00.000Z",
        "voteCount": 2,
        "content": "C is correct here.  It leverages the OS Config agent and a well-defined IAM role."
      },
      {
        "date": "2023-12-31T10:38:00.000Z",
        "voteCount": 1,
        "content": "Per ChatGPT, Option C aligns with the requirement of providing visibility into vulnerabilities and other OS metadata for the specific Compute Engine instance while following the principle of least privilege by granting only the necessary permissions to the security team member."
      },
      {
        "date": "2023-12-29T22:54:00.000Z",
        "voteCount": 2,
        "content": "C. \u2022 Ensure that the OS Config agent is installed on the Compute Engine instance.\n\u2022 Provide the security team member roles/osconfig.vulnerabilityReportViewer permission."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 255,
    "url": "https://www.examtopics.com/discussions/google/view/129841-exam-associate-cloud-engineer-topic-1-question-255/",
    "body": "You want to enable your development team to deploy new features to an existing Cloud Run service in production. To minimize the risk associated with a new revision, you want to reduce the number of customers who might be affected by an outage without introducing any development or operational costs to your customers. You want to follow Google-recommended practices for managing revisions to a service. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAsk your customers to retry access to your service with exponential backoff to mitigate any potential problems after the new revision is deployed.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGradually roll out the new revision and split customer traffic between the revisions to allow rollback in case a problem occurs.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSend all customer traffic to the new revision, and roll back to a previous revision if you witness any problems in production.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy your application to a second Cloud Run service, and ask your customers to use the second Cloud Run service."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 12,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-18T09:27:00.000Z",
        "voteCount": 6,
        "content": "I would go with B. Here is why:\n\nRequirements:\n1. Reduce the number of customers who might be affected by an outage (caused by a new version release)\n2. Don't introduce any development or operational costs to the customers.\n(3. Follow Google-recommended practices)\n\nA: Might require the customer to setup exponential backoff on their end -&gt; Requirement 2 not met.\nB: Fulfills all requirements.\nC: Does not meet requirement 1, as switching 100% traffic at once to the new version would affect all of the customers, if there would be any issue in this version.\nD: Does not meet requirements 1, 2 and 3, as an issue would affect all customers (once they have switched to the new service), switching to the second cloud run service causes dev/ops costs on the customer side and it doesn't follow Google-recommended practices, as a new version of an existing service should not be released as a new service."
      },
      {
        "date": "2023-12-29T22:55:00.000Z",
        "voteCount": 6,
        "content": "B. Gradually roll out the new revision and split customer traffic between the revisions to allow rollback in case a problem occurs."
      },
      {
        "date": "2024-03-03T18:33:00.000Z",
        "voteCount": 2,
        "content": "B for the win! Deploy new features = split traffic. Every time."
      },
      {
        "date": "2023-12-31T10:38:00.000Z",
        "voteCount": 4,
        "content": "Per ChatGPT, Option B aligns with the recommended practice of gradually rolling out new revisions, allowing for controlled monitoring and risk mitigation, without introducing any development or operational costs to customers."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 256,
    "url": "https://www.examtopics.com/discussions/google/view/129842-exam-associate-cloud-engineer-topic-1-question-256/",
    "body": "You have deployed an application on a Compute Engine instance. An external consultant needs to access the Linux-based instance. The consultant is connected to your corporate network through a VPN connection, but the consultant has no Google account. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstruct the external consultant to use the gcloud compute ssh command line tool by using Identity-Aware Proxy to access the instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstruct the external consultant to use the gcloud compute ssh command line tool by using the public IP address of the instance to access it.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstruct the external consultant to generate an SSH key pair, and request the public key from the consultant. Add the public key to the instance yourself, and have the consultant access the instance through SSH with their private key.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstruct the external consultant to generate an SSH key pair, and request the private key from the consultant. Add the private key to the instance yourself, and have the consultant access the instance through SSH with their public key."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 13,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 11,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-02T17:44:00.000Z",
        "voteCount": 7,
        "content": "A. Using Identity-Aware Proxy (IAP): While IAP is a secure method of accessing Compute Engine instances, it typically requires a Google account for authentication, which the consultant does not have."
      },
      {
        "date": "2024-01-14T13:03:00.000Z",
        "voteCount": 6,
        "content": "The correct answer is **C**.\n\nTo allow an external consultant to access a Linux-based Compute Engine instance, you should:\n- Instruct the external consultant to generate an **SSH key pair**. This will result in a public key and a private key.\n- Request the **public key** from the consultant. The public key can be shared without compromising security.\n- Add the public key to the instance yourself. This will allow the consultant to authenticate with the Compute Engine instance.\n- Have the consultant access the instance through SSH with their **private key**. The private key should be kept secret and not shared.\n\nThe other options (A, B, and D) are not correct because they either require the consultant to have a Google account, expose the instance to the public internet, or involve sharing the private key, which is a security risk."
      },
      {
        "date": "2024-09-27T19:30:00.000Z",
        "voteCount": 2,
        "content": "A. IAP can work with no google account.https://cloud.google.com/iap/docs/external-identities"
      },
      {
        "date": "2024-09-23T05:50:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/iap/docs/external-identities."
      },
      {
        "date": "2024-09-05T11:48:00.000Z",
        "voteCount": 1,
        "content": "Although its confusing and contradicting with Question 152, I would proceed to choose C for this. Question states no google account and the consultant is connected through VPN to the corporate network also its \"an instant\", not many.."
      },
      {
        "date": "2024-08-30T11:17:00.000Z",
        "voteCount": 1,
        "content": "This is correct. For IAP you need a google account. Read the damn question, it clearly states that he doesn't have a google account goshdarnit!"
      },
      {
        "date": "2024-07-21T06:49:00.000Z",
        "voteCount": 1,
        "content": "IMO answer is \"A\".\nIAP does not require Google account. There are other authentication methods supported by IAP too. Ref: https://cloud.google.com/iap/docs/authenticate-users-external-identities"
      },
      {
        "date": "2024-05-19T04:35:00.000Z",
        "voteCount": 1,
        "content": "A requires a Google account and the consultant has not. So it is C."
      },
      {
        "date": "2024-04-28T23:59:00.000Z",
        "voteCount": 3,
        "content": "Should be A based on the previous questions"
      },
      {
        "date": "2024-02-01T02:32:00.000Z",
        "voteCount": 5,
        "content": "See responses on question #152\nAnd https://cloud.google.com/iap/docs/external-identities.\n\nRE: IAP \"This is useful if your application is already using an external authentication system, and migrating your users to Google accounts is impractical\""
      },
      {
        "date": "2024-02-17T09:28:00.000Z",
        "voteCount": 2,
        "content": "but the consultant is already in the corporate network through VPN, no need an external access"
      },
      {
        "date": "2024-01-28T06:04:00.000Z",
        "voteCount": 3,
        "content": "A  (IAP) is the solution"
      },
      {
        "date": "2023-12-31T10:36:00.000Z",
        "voteCount": 4,
        "content": "Per ChatGPT, Option C provides a secure and recommended method for granting the external consultant access to the Compute Engine instance using SSH key authentication without the need for a Google account."
      },
      {
        "date": "2023-12-29T22:55:00.000Z",
        "voteCount": 3,
        "content": "C. Instruct the external consultant to generate an SSH key pair, and request the public key from the consultant. Add the public key to the instance yourself, and have the consultant access the instance through SSH with their private key."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 257,
    "url": "https://www.examtopics.com/discussions/google/view/130017-exam-associate-cloud-engineer-topic-1-question-257/",
    "body": "After a recent security incident, your startup company wants better insight into what is happening in the Google Cloud environment. You need to monitor unexpected firewall changes and instance creation. Your company prefers simple solutions. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a log sink to forward Cloud Audit Logs filtered for firewalls and compute instances to Cloud Storage. Use BigQuery to periodically analyze log events in the storage bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Logging filters to create log-based metrics for firewall and instance actions. Monitor the changes and set up reasonable alerts.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall Kibana on a compute instance. Create a log sink to forward Cloud Audit Logs filtered for firewalls and compute instances to Pub/Sub. Target the Pub/Sub topic to push messages to the Kibana instance. Analyze the logs on Kibana in real time.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTurn on Google Cloud firewall rules logging, and set up alerts for any insert, update, or delete events."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 10,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-08-19T07:28:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/firewall/docs/firewall-rules-logging"
      },
      {
        "date": "2024-07-18T14:12:00.000Z",
        "voteCount": 2,
        "content": "I think that key-words is \"After a recent security incident\", you need be notified if something happening with so important thing like secure."
      },
      {
        "date": "2024-02-20T18:38:00.000Z",
        "voteCount": 2,
        "content": "B is a simple solution."
      },
      {
        "date": "2024-01-30T07:21:00.000Z",
        "voteCount": 4,
        "content": "log sink is advanced and it is used for routing logs to specific destinations. \nso answer B"
      },
      {
        "date": "2024-01-15T10:04:00.000Z",
        "voteCount": 1,
        "content": "this is simple."
      },
      {
        "date": "2023-12-31T10:35:00.000Z",
        "voteCount": 2,
        "content": "Per ChatGPT, Option B provides a simple and effective solution using native Google Cloud services (Cloud Logging and log-based metrics) to monitor unexpected firewall changes and instance creation, while also allowing for the setup of reasonable alerts to ensure timely response to any security incidents."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 258,
    "url": "https://www.examtopics.com/discussions/google/view/129843-exam-associate-cloud-engineer-topic-1-question-258/",
    "body": "You are configuring service accounts for an application that spans multiple projects. Virtual machines (VMs) running in the web-applications project need access to BigQuery datasets in the crm-databases project. You want to follow Google-recommended practices to grant access to the service account in the web-applications project. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGrant \"project owner\" for web-applications appropriate roles to crm-databases.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGrant \"project owner\" role to crm-databases and the web-applications project.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGrant \"project owner\" role to crm-databases and roles/bigquery.dataViewer role to web-applications.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGrant roles/bigquery.dataViewer role to crm-databases and appropriate roles to web-applications.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 9,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-07T20:56:00.000Z",
        "voteCount": 5,
        "content": "D is the least privilege and Google's recommended practices."
      },
      {
        "date": "2024-10-09T07:21:00.000Z",
        "voteCount": 1,
        "content": "The question does not describe any project requiring \"owner\" role access, hence granting that role to any of the project would violate least privilege. \nCan argue that crm-databases should have full access hence need owner role, but question does not mention specifically, and we only assume that."
      },
      {
        "date": "2024-04-24T13:18:00.000Z",
        "voteCount": 1,
        "content": "I had my exam today and select A. I did only because of these sentence \"service accounts for an application that spans multiple projects .\" not 100% sure if it's correct but service account for web apps needs permissions to span projects. Maybe I got it wrong but A makes sense.\nIt's tricky cause you don't know if web-apps will also do some updates on BigQuery or not."
      },
      {
        "date": "2024-03-04T10:09:00.000Z",
        "voteCount": 2,
        "content": "D is the best answer and, for me, it was a process of elimination. The Project Owner role grants far-reaching permissions beyond what's needed for reading BQ datasets, violating the principle of least privilege."
      },
      {
        "date": "2024-01-02T18:16:00.000Z",
        "voteCount": 1,
        "content": "Interpreting 'Project Owner' as the responsible entity, and not as the 'Project Owner' IAM role in Google Cloud: In this case, the instruction directs the person or entity managing the 'web-applications' project to grant appropriate roles for accessing the 'crm-databases' project. If this interpretation aligns with the intent of Option A, then it would indeed be a correct approach. Otherwise, none of the provided options would be correct."
      },
      {
        "date": "2024-09-16T19:11:00.000Z",
        "voteCount": 1,
        "content": "We need to assign roles to the service account. It should have read access on the crm project.\n\nD is correct."
      },
      {
        "date": "2024-02-06T07:57:00.000Z",
        "voteCount": 4,
        "content": "You're managing the service accounts, why would you grant any role to 'web-applications' project owner? The most appropiate should be D, because you are granting a wrong role to the service accounts in 'crm-databases' project, but then the option says that appropiate roles will be granted to service accounts in 'web-applications' project."
      },
      {
        "date": "2024-01-01T08:32:00.000Z",
        "voteCount": 4,
        "content": "It is 116 question. The answer is D."
      },
      {
        "date": "2023-12-31T10:34:00.000Z",
        "voteCount": 1,
        "content": "Per ChatGPT, Option D aligns with the principle of least privilege, provides separation of concerns between projects, and allows for granular access control, making it the best choice for granting access to the service account in the web-applications project to access BigQuery datasets in the crm-databases project while following Google-recommended practices."
      },
      {
        "date": "2024-01-02T18:10:00.000Z",
        "voteCount": 2,
        "content": "why give the role to the project crm-databases, it makes no sense."
      },
      {
        "date": "2023-12-29T22:56:00.000Z",
        "voteCount": 2,
        "content": "D. Grant roles/bigquery.dataViewer role to crm-databases and appropriate roles to web-applications."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 259,
    "url": "https://www.examtopics.com/discussions/google/view/129844-exam-associate-cloud-engineer-topic-1-question-259/",
    "body": "Your Dataproc cluster runs in a single Virtual Private Cloud (VPC) network in a single subnetwork with range 172.16.20.128/25. There are no private IP addresses available in the subnetwork. You want to add new VMs to communicate with your cluster using the minimum number of steps. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the existing subnet range to 172.16.20.0/24.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new Secondary IP Range in the VPC and configure the VMs to use that range.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new VPC network for the VMs. Enable VPC Peering between the VMs'VPC network and the Dataproc cluster VPC network.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new VPC network for the VMs with a subnet of 172.32.0.0/16. Enable VPC network Peering between the Dataproc VPC network and the VMs VPC network. Configure a custom Route exchange."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 10,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-03T06:56:00.000Z",
        "voteCount": 9,
        "content": "A. Same as question 129. Option A involves modifying the subnet range of the existing VPC network to increase the number of available IP addresses. By changing the subnet range to 172.16.20.0/24, you will have a larger IP address range to allocate to new VMs, allowing them to communicate with the Dataproc cluster.\n\nTo expand the IP range of a Compute Engine subnetwork, you can use:\ngcloud compute networks subnets expand-ip-range NAME"
      },
      {
        "date": "2024-01-28T01:35:00.000Z",
        "voteCount": 1,
        "content": "Modify is not equals to Expanding"
      },
      {
        "date": "2024-02-01T18:30:00.000Z",
        "voteCount": 3,
        "content": "not the same question. \nQuestion #: 259 : \"There are no private IP addresses available in the subnetwork\"\nQuestion #: 129 : \"There are no private IP addresses available in the VPC network.\""
      },
      {
        "date": "2024-10-11T03:42:00.000Z",
        "voteCount": 1,
        "content": "gcloud compute networks subnets expand-ip-range - expand the IP range of a Compute Engine subnetwork"
      },
      {
        "date": "2024-05-17T12:21:00.000Z",
        "voteCount": 2,
        "content": "The question state \"using the minimum number of steps\" , then it should be A."
      },
      {
        "date": "2024-03-04T03:19:00.000Z",
        "voteCount": 1,
        "content": "I would say A is the answer, but I have no idea what the Q means when specifying \"You want to add new VMs to communicate with your cluster using the minimum number of steps.\" \n\nDoes it mean that you want to add VMs and use the same subnet or add new VMs and use another subnet and then want those VMs communicating with the VMs in the other subnet?"
      },
      {
        "date": "2024-02-01T05:08:00.000Z",
        "voteCount": 1,
        "content": "The reason A isn't correct is because you can only expand a subnet by \"setting the prefix length to a smaller number\"\nSee: https://cloud.google.com/vpc/docs/create-modify-vpc-networks#expand-subnet\n\nThe reason B isn't correct is because you can only use a secondary (aka 'alias') IP address when there is a primary already in place. In this scenario this isn't possible to do for the new VMs because there are no primary IP addresses available.\n\nTherefore C seems like a feasible approach, with fewer  steps than D (even if D is possible, which I don't know)."
      },
      {
        "date": "2024-01-14T13:09:00.000Z",
        "voteCount": 3,
        "content": "The correct answer is **A**.\n\nTo add new VMs to communicate with your Dataproc cluster using the minimum number of steps, you should:\n- **Modify the existing subnet range** to 172.16.20.0/24. This will expand the range of available IP addresses in the subnet, allowing you to add new VMs.\n\nThe other options (B, C, and D) are not correct because they involve more steps than necessary (such as creating a new Secondary IP Range, a new VPC network, or enabling VPC Peering), which is not aligned with the requirement of using the minimum number of steps."
      },
      {
        "date": "2024-01-02T08:57:00.000Z",
        "voteCount": 3,
        "content": "Answer is A"
      },
      {
        "date": "2024-01-01T16:55:00.000Z",
        "voteCount": 2,
        "content": "Option C is right one as we dont have additional private ips left\nFor option D, This option is viable but is more complex than simply creating a new VPC and establishing VPC Peering."
      },
      {
        "date": "2023-12-31T10:33:00.000Z",
        "voteCount": 1,
        "content": "Per ChatGPT, Option B aligns with the requirement of adding new VMs to communicate with the Dataproc cluster using the minimum number of steps while addressing the constraint of no available private IP addresses in the existing subnetwork."
      },
      {
        "date": "2023-12-29T22:56:00.000Z",
        "voteCount": 1,
        "content": "B. Create a new Secondary IP Range in the VPC and configure the VMs to use that range."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 260,
    "url": "https://www.examtopics.com/discussions/google/view/129845-exam-associate-cloud-engineer-topic-1-question-260/",
    "body": "You are building a backend service for an ecommerce platform that will persist transaction data from mobile and web clients. After the platform is launched, you expect a large volume of global transactions. Your business team wants to run SQL queries to analyze the data. You need to build a highly available and scalable data store for the platform. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a multi-region Cloud Spanner instance with an optimized schema.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a multi-region Firestore database with aggregation query enabled.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a multi-region Cloud SQL for PostgreSQL database with optimized indexes.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a multi-region BigQuery dataset with optimized tables."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 17,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-03T07:00:00.000Z",
        "voteCount": 6,
        "content": "A. Key is \u201clarge volume of global transactions\u201d, so Cloud Spanner would be a good choice."
      },
      {
        "date": "2024-08-27T10:29:00.000Z",
        "voteCount": 3,
        "content": "PiperMe perfectly summed it up. Rember: Global + SQL = Cloud Spanner"
      },
      {
        "date": "2024-03-04T11:52:00.000Z",
        "voteCount": 4,
        "content": "A. Global + SQL = Cloud Spanner"
      },
      {
        "date": "2023-12-31T10:32:00.000Z",
        "voteCount": 4,
        "content": "Per ChatGPT, Option A, creating a multi-region Cloud Spanner instance with an optimized schema, is the best choice for building a highly available and scalable data store that can efficiently handle global transactions and support SQL queries for analysis."
      },
      {
        "date": "2023-12-29T22:56:00.000Z",
        "voteCount": 4,
        "content": "A. Create a multi-region Cloud Spanner instance with an optimized schema."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 261,
    "url": "https://www.examtopics.com/discussions/google/view/129846-exam-associate-cloud-engineer-topic-1-question-261/",
    "body": "You are in charge of provisioning access for all Google Cloud users in your organization. Your company recently acquired a startup company that has their own Google Cloud organization. You need to ensure that your Site Reliability Engineers (SREs) have the same project permissions in the startup company's organization as in your own organization. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the Google Cloud console for your organization, select Create role from selection, and choose destination as the startup company's organization.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the Google Cloud console for the startup company, select Create role from selection and choose source as the startup company's Google Cloud organization.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the gcloud iam roles copy command, and provide the Organization ID of the startup company's Google Cloud Organization as the destination.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the gcloud iam roles copy command, and provide the project IDs of all projects in the startup company's organization as the destination."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-10-10T05:06:00.000Z",
        "voteCount": 1,
        "content": "I'm confused between C and D, both use \"gcloud iam roles copy\" which is required for this scenario, but most voted answer is C while I'm unsure why giving project level permission is incorrect. According to lease privilege SRE should be okay to get permission project by project which is a hassle than C but it is not incorrect and can be done.\nAnyone pls correct me or give a link to an official document."
      },
      {
        "date": "2024-10-10T05:31:00.000Z",
        "voteCount": 2,
        "content": "According to Gemini (Couldn't find official source) below is correct which means D is also possible answer. in the question is does not say fewest steps or best practice method.\n\ngcloud iam roles copy --source-organization=organizations/123456789 --destination-projects=projects/project1,projects/project2,projects/project3 --role-ids=roles/compute.admin,roles/storage.objectViewer"
      },
      {
        "date": "2024-10-11T03:51:00.000Z",
        "voteCount": 1,
        "content": "We could have a large number of projects, so copying by project may not be practical. Therefore copying by organisation is fewest steps.\n\nAlso just copying to all projects gives the same result as copying by organisation, but involves more steps."
      },
      {
        "date": "2024-05-23T13:08:00.000Z",
        "voteCount": 1,
        "content": "The keys in question are SRE and project permissions. Choosing C will copy all your organization IAM permissions and not the only ones defined for SRE at project level.\nSo, the correct answer is D."
      },
      {
        "date": "2024-09-13T12:13:00.000Z",
        "voteCount": 1,
        "content": "--dest-project=DEST_PROJECT - the project of the destination role.\nWe cannot specify all projects. So, I think that's the trick here."
      },
      {
        "date": "2024-05-03T15:57:00.000Z",
        "voteCount": 1,
        "content": "The correct one is C"
      },
      {
        "date": "2024-03-04T12:06:00.000Z",
        "voteCount": 3,
        "content": "C. The gcloud iam roles copy command is designed specifically for copying IAM roles between different resources, including organizations."
      },
      {
        "date": "2023-12-31T10:31:00.000Z",
        "voteCount": 2,
        "content": "Per ChatGPT, Option C is the correct choice for achieving consistency in project permissions across both organizations by leveraging the gcloud iam roles copy command and providing the Organization ID of the startup company's Google Cloud Organization as the destination."
      },
      {
        "date": "2023-12-29T22:58:00.000Z",
        "voteCount": 2,
        "content": "C. Use the gcloud iam roles copy command, and provide the Organization ID of the startup company's Google Cloud Organization as the destination."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 262,
    "url": "https://www.examtopics.com/discussions/google/view/129847-exam-associate-cloud-engineer-topic-1-question-262/",
    "body": "You need to extract text from audio files by using the Speech-to-Text API. The audio files are pushed to a Cloud Storage bucket. You need to implement a fully managed, serverless compute solution that requires authentication and aligns with Google-recommended practices. You want to automate the call to the API by submitting each file to the API as the audio file arrives in the bucket. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an App Engine standard environment triggered by Cloud Storage bucket events to submit the file URI to the Google Speech-to-TextAPI.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun a Kubernetes job to scan the bucket regularly for incoming files, and call the Speech-to-Text API for each unprocessed file.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun a Python script by using a Linux cron job in Compute Engine to scan the bucket regularly for incoming files, and call the Speech-to-Text API for each unprocessed file.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Cloud Function triggered by Cloud Storage bucket events to submit the file URI to the Google Speech-to-Text API.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-10-13T14:24:00.000Z",
        "voteCount": 1,
        "content": "Why not App Engine? since it's also serverless."
      },
      {
        "date": "2024-10-10T05:45:00.000Z",
        "voteCount": 2,
        "content": "Reasons-\nServerless: Cloud Functions provide a serverless environment, eliminating the need for managing infrastructure.\nTriggered by Cloud Storage events: The Cloud Function can be triggered automatically as new audio files are added to the bucket, automating the process of submitting them to the API.\nAuthentication: Cloud Functions can use service accounts to authenticate with the Speech-to-Text API, ensuring secure access.\nFully managed: Cloud Functions handle the execution environment, scaling, and other operational aspects, reducing your management overhead.\nGoogle-recommended practices: Using Cloud Functions and Cloud Storage aligns with Google's recommended practices for building serverless applications."
      },
      {
        "date": "2024-03-20T18:50:00.000Z",
        "voteCount": 1,
        "content": "D, as PiperMe said."
      },
      {
        "date": "2024-03-06T13:21:00.000Z",
        "voteCount": 3,
        "content": "- Cloud Functions is serverless\n- It can be directly triggered by Cloud Storage events\n- Can integrate with the Speech-to-Text API"
      },
      {
        "date": "2023-12-31T10:31:00.000Z",
        "voteCount": 3,
        "content": "Per ChatGPT, Option D provides a serverless, scalable, and fully managed solution that aligns with Google-recommended practices for automating the extraction of text from audio files using the Speech-to-Text API in response to Cloud Storage bucket events."
      },
      {
        "date": "2023-12-29T22:58:00.000Z",
        "voteCount": 4,
        "content": "D. Create a Cloud Function triggered by Cloud Storage bucket events to submit the file URI to the Google Speech-to-Text API."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 263,
    "url": "https://www.examtopics.com/discussions/google/view/130016-exam-associate-cloud-engineer-topic-1-question-263/",
    "body": "Your customer wants you to create a secure website with autoscaling based on the compute instance CPU load. You want to enhance performance by storing static content in Cloud Storage. Which resources are needed to distribute the user traffic?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAn external HTTP(S) load balancer with a managed SSL certificate to distribute the load and a URL map to target the requests for the static content to the Cloud Storage backend.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAn external network load balancer pointing to the backend instances to distribute the load evenly. The web servers will forward the request to the Cloud Storage as needed.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAn internal HTTP(S) load balancer together with Identity-Aware Proxy to allow only HTTPS traffic.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAn external HTTP(S) load balancer to distribute the load and a URL map to target the requests for the static content to the Cloud Storage backend. Install the HTTPS certificates on the instance."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-03T14:03:00.000Z",
        "voteCount": 7,
        "content": "Why D is not correct: \nInstalling HTTPS Certificates on the Instance: While this approach can work, it's generally more efficient and secure to use a managed SSL certificate with the load balancer itself, rather than installing and managing certificates on each individual instance. Managing SSL certificates on the load balancer simplifies certificate management, especially in an autoscaling environment where new instances are frequently created and removed."
      },
      {
        "date": "2024-10-10T06:04:00.000Z",
        "voteCount": 1,
        "content": "Option A: preferred.\nUses an external HTTP(S) load balancer with a managed SSL certificate. This means that the SSL certificate is managed by Google Cloud, and you don't need to install or renew it on your compute instances. This simplifies the management of your website and ensures that it is always accessible over HTTPS.\n\nOption D:\nUses an external HTTP(S) load balancer but requires you to install the HTTPS certificates on the instance. This means that you are responsible for managing and renewing the certificates, which can be more complex and time-consuming."
      },
      {
        "date": "2024-03-04T05:56:00.000Z",
        "voteCount": 3,
        "content": "I'd say because the answer contains \"Install the HTTPS certificates on the instance.\", which is not a good way to go about things. You;d want the managed SSL cert (ans A) so that you don't have to worry about it. So A makes more sense."
      },
      {
        "date": "2024-03-22T21:41:00.000Z",
        "voteCount": 1,
        "content": "Choose option A"
      },
      {
        "date": "2023-12-31T10:28:00.000Z",
        "voteCount": 1,
        "content": "ChatGPT says the answer is A."
      },
      {
        "date": "2023-12-31T10:30:00.000Z",
        "voteCount": 1,
        "content": "Option A aligns with the requirements of security, autoscaling, and performance optimization by leveraging Cloud Storage for static content while efficiently distributing user traffic."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 264,
    "url": "https://www.examtopics.com/discussions/google/view/129848-exam-associate-cloud-engineer-topic-1-question-264/",
    "body": "The core business of your company is to rent out construction equipment at large scale. All the equipment that is being rented out has been equipped with multiple sensors that send event information every few seconds. These signals can vary from engine status, distance traveled, fuel level, and more. Customers are billed based on the consumption monitored by these sensors. You expect high throughput \u2013 up to thousands of events per hour per device \u2013 and need to retrieve consistent data based on the time of the event. Storing and retrieving individual signals should be atomic. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate files in Cloud Storage as data comes in.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a file in Filestore per device, and append new data to that file.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIngest the data into Cloud SQL. Use multiple read replicas to match the throughput.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIngest the data into Bigtable. Create a row key based on the event timestamp.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-10-11T04:01:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/bigtable/docs/overview#what-its-good-for"
      },
      {
        "date": "2024-03-22T21:43:00.000Z",
        "voteCount": 1,
        "content": "I saw this Q from somewhere before :3"
      },
      {
        "date": "2024-01-15T03:34:00.000Z",
        "voteCount": 3,
        "content": "This is the same question as No. 139."
      },
      {
        "date": "2024-01-03T14:09:00.000Z",
        "voteCount": 2,
        "content": "it asks to retrieve consistent data based on the time of the event. D is the only option matching."
      },
      {
        "date": "2024-01-03T07:22:00.000Z",
        "voteCount": 2,
        "content": "D. Explanation:\n- Bigtable is a highly scalable, NoSQL database designed for high throughput and low-latency applications, making it suitable for scenarios with high ingest rates and rapid data retrieval.\n- Creating a row key based on the event timestamp would facilitate efficient retrieval of time-based data, ensuring consistency and atomicity for individual signals.\n- Bigtable's design allows for fast access to data using row keys, providing optimal performance when retrieving specific signals or events based on timestamps.\n- It also offers the scalability needed for handling thousands of events per hour per device."
      },
      {
        "date": "2024-01-03T07:22:00.000Z",
        "voteCount": 2,
        "content": "Options A, B, and C might not efficiently handle the high throughput and atomic retrieval requirements:\n- Cloud Storage (Option A) might not offer the necessary atomicity for individual signal retrieval.\n- Filestore (Option B) could struggle with scaling and might not provide the atomic access needed for individual signal retrieval.\n- Cloud SQL (Option C) could face challenges in scaling to handle the high throughput effectively and might not match the required performance and atomicity for individual signal retrieval compared to Bigtable."
      },
      {
        "date": "2023-12-31T10:28:00.000Z",
        "voteCount": 1,
        "content": "ChatGPT says the answer is D."
      },
      {
        "date": "2023-12-29T22:59:00.000Z",
        "voteCount": 2,
        "content": "D. Ingest the data into Bigtable. Create a row key based on the event timestamp."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 265,
    "url": "https://www.examtopics.com/discussions/google/view/129849-exam-associate-cloud-engineer-topic-1-question-265/",
    "body": "You just installed the Google Cloud CLI on your new corporate laptop. You need to list the existing instances of your company on Google Cloud. What must you do before you run the gcloud compute instances list command? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun gcloud auth login, enter your login credentials in the dialog window, and paste the received login token to gcloud CLI.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Google Cloud service account, and download the service account key. Place the key file in a folder on your machine where gcloud CLI can find it.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDownload your Cloud Identity user account key. Place the key file in a folder on your machine where gcloud CLI can find it.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun gcloud config set compute/zone $my_zone to set the default zone for gcloud CLI.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun gcloud config set project $my_project to set the default project for gcloud CLI."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 15,
        "isMostVoted": true
      },
      {
        "answer": "E",
        "count": 6,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-03T07:32:00.000Z",
        "voteCount": 7,
        "content": "AE (I couldn't select both). \nIn addition to authenticating your account (Option A) to ensure the necessary permissions, setting the default project using `gcloud config set project [Your_Project_ID]` is crucial. It guarantees that subsequent commands are executed within the intended project's context. Both steps are essential before listing instances in Google Cloud Platform: proper authentication for permissions and setting the default project to ensure the commands run in the correct project context."
      },
      {
        "date": "2024-01-03T07:33:00.000Z",
        "voteCount": 1,
        "content": "D is not correct, as it\u2019s optional to set the zone."
      },
      {
        "date": "2024-03-22T21:45:00.000Z",
        "voteCount": 6,
        "content": "I go with option A &amp; E for basic actions"
      },
      {
        "date": "2024-09-30T15:53:00.000Z",
        "voteCount": 2,
        "content": "A &amp; E is the right one"
      },
      {
        "date": "2024-09-16T19:33:00.000Z",
        "voteCount": 2,
        "content": "AE correct options"
      },
      {
        "date": "2024-08-27T10:54:00.000Z",
        "voteCount": 2,
        "content": "It's A + E. Also thanks you so much for everyone else verifying. It's a huge help"
      },
      {
        "date": "2024-02-17T09:43:00.000Z",
        "voteCount": 2,
        "content": "In my honest opinion A &amp; E"
      },
      {
        "date": "2024-01-03T00:45:00.000Z",
        "voteCount": 1,
        "content": "It can be E. So its A"
      },
      {
        "date": "2024-01-02T13:42:00.000Z",
        "voteCount": 3,
        "content": "AE, auth and set project"
      },
      {
        "date": "2023-12-31T09:55:00.000Z",
        "voteCount": 4,
        "content": "ChatGPT says the answer is A and E."
      },
      {
        "date": "2023-12-31T10:26:00.000Z",
        "voteCount": 1,
        "content": "Voting comment will only allow me to select one answer not both."
      },
      {
        "date": "2023-12-29T22:59:00.000Z",
        "voteCount": 2,
        "content": "A and E"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 266,
    "url": "https://www.examtopics.com/discussions/google/view/129850-exam-associate-cloud-engineer-topic-1-question-266/",
    "body": "You are planning to migrate your on-premises data to Google Cloud. The data includes:<br><br>\u2022\t200 TB of video files in SAN storage<br>\u2022\tData warehouse data stored on Amazon Redshift<br>\u2022\t20 GB of PNG files stored on an S3 bucket<br><br>You need to load the video files into a Cloud Storage bucket, transfer the data warehouse data into BigQuery, and load the PNG files into a second Cloud Storage bucket. You want to follow Google-recommended practices and avoid writing any code for the migration. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse gcloud storage for the video files, Dataflow for the data warehouse data, and Storage Transfer Service for the PNG files.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Transfer Appliance for the videos, BigQuery Data Transfer Service for the data warehouse data, and Storage Transfer Service for the PNG files.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Storage Transfer Service for the video files, BigQuery Data Transfer Service for the data warehouse data, and Storage Transfer Service for the PNG files.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Data Fusion for the video files, Dataflow for the data warehouse data, and Storage Transfer Service for the PNG files."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-03T14:21:00.000Z",
        "voteCount": 8,
        "content": "\u2022\tTransfer Appliance for Video Files: Given the large size of the video files (200 TB) in SAN storage, using Google's Transfer Appliance is a practical solution. Transfer Appliance is a hardware solution provided by Google for transferring large amounts of data. It is well-suited for scenarios where uploading data over the internet is too slow or not feasible.\n\n\u2022\tBigQuery Data Transfer Service for Data Warehouse Data: To migrate data from Amazon Redshift to BigQuery, the BigQuery Data Transfer Service is the appropriate tool. It automates the migration of data from several sources, including Amazon Redshift, into BigQuery.\n\n\u2022\tStorage Transfer Service for PNG Files: The Storage Transfer Service is ideal for moving data from Amazon S3 to Google Cloud Storage. It simplifies the process of importing your PNG files from S3 to a Cloud Storage bucket."
      },
      {
        "date": "2024-10-11T04:05:00.000Z",
        "voteCount": 1,
        "content": "We need the Transfer Appliance for the video."
      },
      {
        "date": "2024-06-08T19:31:00.000Z",
        "voteCount": 2,
        "content": "I think the answer is B. The controversy is whether the answer is B or C, i.e. whether one should use the transfer appliance or storage transfer service. From my knowledge, storage transfer service is not applicable for SAN, so the answer should be B."
      },
      {
        "date": "2024-01-30T06:41:00.000Z",
        "voteCount": 3,
        "content": "If it is on-prem to cloud, transfer appliance. \n\nStorage transfer service is for transferring data between buckets."
      },
      {
        "date": "2024-02-01T06:27:00.000Z",
        "voteCount": 3,
        "content": "Hi bro, I need to talk with you about GCP Associate Cloud Cngineer"
      },
      {
        "date": "2024-01-03T07:46:00.000Z",
        "voteCount": 1,
        "content": "C. Explanation:\n- Storage Transfer Service: It provides a straightforward way to transfer large amounts of data from an on-premises data source or cloud storage provider to Cloud Storage without writing code. This service can efficiently handle the migration of video files and PNG files.\n- BigQuery Data Transfer Service: This service allows the transfer of data from various sources, including Amazon Redshift, directly into BigQuery. It simplifies and automates the migration of data warehouse data without coding requirements."
      },
      {
        "date": "2024-01-03T07:46:00.000Z",
        "voteCount": 1,
        "content": "While options A and D involve using Dataflow and Cloud Data Fusion, respectively, these services may require additional configuration, development, or transformation logic, which is contrary to the requirement of avoiding code for migration.\n\nOption B suggests using Transfer Appliance for the videos, which might be applicable for large-scale physical data transfers, but it's not the most suitable option for this scenario involving Cloud migration without coding."
      },
      {
        "date": "2024-02-20T18:56:00.000Z",
        "voteCount": 1,
        "content": "But it is said that the data of the videos is on a SAN storage, is it san storage supported by google cloud storage transfer service? I cant see it here on this documentation: https://cloud.google.com/storage-transfer/docs/sources-and-sinks\n\nIt is not public accessible and I dont think it is a file system either. So option B should be the most suitable here. Dont you think ?"
      },
      {
        "date": "2024-01-03T00:40:00.000Z",
        "voteCount": 1,
        "content": "C makes sense. Use transfer appliance for vid"
      },
      {
        "date": "2024-01-02T13:37:00.000Z",
        "voteCount": 1,
        "content": "C makes more sense"
      },
      {
        "date": "2024-01-01T17:03:00.000Z",
        "voteCount": 1,
        "content": "B. Use Transfer Appliance for the videos, BigQuery Data Transfer Service for the data warehouse data, and Storage Transfer Service for the PNG files: Transfer Appliance is designed for moving large amounts of data (like 200 TB of videos) into Google Cloud Storage. The BigQuery Data Transfer Service automates data movement from several sources, including Amazon Redshift, into BigQuery. Storage Transfer Service is appropriate for moving data from Amazon S3 to Google Cloud Storage."
      },
      {
        "date": "2023-12-31T09:52:00.000Z",
        "voteCount": 2,
        "content": "ChatGPT says the answer is C."
      },
      {
        "date": "2023-12-29T22:59:00.000Z",
        "voteCount": 1,
        "content": "B. Use Transfer Appliance for the videos, BigQuery Data Transfer Service for the data warehouse data, and Storage Transfer Service for the PNG files."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 267,
    "url": "https://www.examtopics.com/discussions/google/view/144064-exam-associate-cloud-engineer-topic-1-question-267/",
    "body": "You want to deploy a new containerized application into Google Cloud by using a Kubernetes manifest. You want to have full control over the Kubernetes deployment, and at the same time, you want to minimize configuring infrastructure. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the application on GKE Autopilot.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the application on Cloud Run.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the application on GKE Standard.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the application on Cloud Functions."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 10,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-09-13T11:41:00.000Z",
        "voteCount": 2,
        "content": "Autopilot mode is designed for those who prefer a hands-off approach. It\u2019s best suited for businesses that want to leverage the power of Kubernetes without diving deep into its intricacies. It offers a simplified, managed experience with Google taking care of the operational overhead.\n\nOn the other hand, Standard mode is for those who want granular control over their Kubernetes environment. It\u2019s ideal for businesses with specific requirements and those who have the expertise to manage and optimize their Kubernetes clusters."
      },
      {
        "date": "2024-08-27T11:08:00.000Z",
        "voteCount": 3,
        "content": "Despite the managed nature of the infrastructure of a GKE Autopilot Cluster, you still have full control over your Kubernetes workloads, configurations, and deployments. This allows you to use Kubernetes manifests and customize your deployment as needed."
      },
      {
        "date": "2024-08-24T17:08:00.000Z",
        "voteCount": 1,
        "content": "C is the answer"
      },
      {
        "date": "2024-09-13T11:41:00.000Z",
        "voteCount": 1,
        "content": "but not to this question"
      },
      {
        "date": "2024-08-01T19:54:00.000Z",
        "voteCount": 1,
        "content": "Key requirements \"full control\" and \"minimize configuring infrastructure\" - GKE Standard Supports both"
      },
      {
        "date": "2024-07-26T21:26:00.000Z",
        "voteCount": 1,
        "content": "C is the answe"
      },
      {
        "date": "2024-07-24T02:42:00.000Z",
        "voteCount": 3,
        "content": "Minimise effort. And GKE is a managed k8s service for deploying container-ised apps using k8s"
      },
      {
        "date": "2024-07-17T16:28:00.000Z",
        "voteCount": 2,
        "content": "A- minimize effort"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 268,
    "url": "https://www.examtopics.com/discussions/google/view/144167-exam-associate-cloud-engineer-topic-1-question-268/",
    "body": "Your team is building a website that handles votes from a large user population. The incoming votes will arrive at various rates. You want to optimize the storage and processing of the votes. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSave the incoming votes to Firestore. Use Cloud Scheduler to trigger a Cloud Functions instance to periodically process the votes.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a dedicated instance to process the incoming votes. Send the votes directly to this instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSave the incoming votes to a JSON file on Cloud Storage. Process the votes in a batch at the end of the day.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSave the incoming votes to Pub/Sub. Use the Pub/Sub topic to trigger a Cloud Functions instance to process the votes.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-10-04T05:41:00.000Z",
        "voteCount": 1,
        "content": "Use Pub/Sub."
      },
      {
        "date": "2024-08-30T11:57:00.000Z",
        "voteCount": 2,
        "content": "D. is the correct answer"
      },
      {
        "date": "2024-07-24T02:50:00.000Z",
        "voteCount": 3,
        "content": "Pub/Sub (a messaging service) triggering a Cloud Function (the glue between services that are otherwise independent) to trigger something else to process the votes"
      },
      {
        "date": "2024-07-19T01:41:00.000Z",
        "voteCount": 4,
        "content": "Pub/Sub: Google Cloud Pub/Sub is a messaging service which directly uses for this purpose.\n\nCloud Functions: By triggering a Cloud Function with a Pub/Sub topic, you can process the votes as they arrive, ensuring low-latency handling and efficient scaling based on demand. This approach provides real-time processing and can handle bursts of traffic effectively."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 269,
    "url": "https://www.examtopics.com/discussions/google/view/144168-exam-associate-cloud-engineer-topic-1-question-269/",
    "body": "You are deploying an application on Google Cloud that requires a relational database for storage. To satisfy your company\u2019s security policies, your application must connect to your database through an encrypted and authenticated connection that requires minimal management and integrates with Identity and Access Management (IAM). What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy a Cloud SQL database with the SSL mode set to encrypted only, configure SSL/TLS client certificates, and configure a database user and password.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy a Cloud SQL database with the SSL mode set to encrypted only, configure SSL/TLS client certificates, and configure IAM database authentication.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy a Cloud SQL database and configure IAM database authentication. Access the database through the Cloud SQL Auth Proxy.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy a Cloud SQL database and configure a database user and password. Access the database through the Cloud SQL Auth Proxy."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-19T01:47:00.000Z",
        "voteCount": 6,
        "content": "Cloud SQL Auth Proxy: This proxy ensures secure connections to your Cloud SQL database by automatically handling encryption (SSL/TLS) and IAM-based authentication. It simplifies the management of secure connections without needing to manage SSL/TLS certificates manually.\n\nIAM Database Authentication: This allows you to use IAM credentials to authenticate to the database, providing a unified and secure authentication mechanism that integrates seamlessly with Google Cloud IAM."
      },
      {
        "date": "2024-07-19T01:50:00.000Z",
        "voteCount": 1,
        "content": "A,B: You must managing SSL certification and database credentials.\n\nD: Relies on database-specific credentials rather than IAM, which doesn't fully leverage the benefits of IAM integration."
      },
      {
        "date": "2024-07-26T21:33:00.000Z",
        "voteCount": 1,
        "content": "As per Google Gemini answwer is B."
      },
      {
        "date": "2024-09-27T10:10:00.000Z",
        "voteCount": 2,
        "content": "Google Gemini is not relaible"
      },
      {
        "date": "2024-07-24T02:53:00.000Z",
        "voteCount": 2,
        "content": "Initially chose B but then read BuenaCloudDE's comment and agreed that managing SSL certs is more complicated than using Cloud SQL Auth Proxy"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 270,
    "url": "https://www.examtopics.com/discussions/google/view/143927-exam-associate-cloud-engineer-topic-1-question-270/",
    "body": "You have two Google Cloud projects: project-a with VPC vpc-a (10.0.0.0/16) and project-b with VPC vpc-b (10.8.0.0/16). Your frontend application resides in vpc-a and the backend API services are deployed in vpc-b. You need to efficiently and cost-effectively enable communication between these Google Cloud projects. You also want to follow Google-recommended practices. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an OpenVPN connection between vpc-a and vpc-b.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate VPC Network Peering between vpc-a and vpc-b.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a Cloud Router in vpc-a and another Cloud Router in vpc-b.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a Cloud Interconnect connection between vpc-a and vpc-b."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-10-11T04:15:00.000Z",
        "voteCount": 2,
        "content": "Google Cloud VPC Network Peering connects two Virtual Private Cloud (VPC) networks so that resources in each network can communicate with each other. Peered VPC networks can be in the same project, different projects of the same organization, or different projects of different organizations.\nhttps://cloud.google.com/vpc/docs/vpc-peering"
      },
      {
        "date": "2024-07-24T02:55:00.000Z",
        "voteCount": 2,
        "content": "VPC Network Peering is the most efficient and cost-effective compared to the other options"
      },
      {
        "date": "2024-07-19T02:04:00.000Z",
        "voteCount": 3,
        "content": "VPC Network Peering: This allows private and secure communication between VPCs in different Google Cloud projects without using public IP addresses or VPN connections. It is cost-effective because it only incurs network egress charges within the same region and provides high-bandwidth, low-latency connectivity."
      },
      {
        "date": "2024-07-15T09:12:00.000Z",
        "voteCount": 2,
        "content": "Why other options are not as suitable:\n\nA. OpenVPN connection: OpenVPN requires setting up and managing a VPN gateway, adding complexity and potential overhead.\nC. Cloud Routers: While Cloud Routers are powerful tools for managing dynamic routing, they are unnecessary for simple communication between two VPCs.\nD. Cloud Interconnect: Cloud Interconnect is a high-speed, dedicated connection for hybrid cloud environments. It's overkill for connecting two VPCs within GCP and would be much more expensive than VPC Network Peering."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 271,
    "url": "https://www.examtopics.com/discussions/google/view/143948-exam-associate-cloud-engineer-topic-1-question-271/",
    "body": "Your company is running a critical workload on a single Compute Engine VM instance. Your company's disaster recovery policies require you to back up the entire instance\u2019s disk data every day. The backups must be retained for 7 days. You must configure a backup solution that complies with your company\u2019s security policies and requires minimal setup and configuration. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the instance to use persistent disk asynchronous replication.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure daily scheduled persistent disk snapshots with a retention period of 7 days.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Cloud Scheduler to trigger a Cloud Function each day that creates a new machine image and deletes machine images that are older than 7 days.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a bash script using gsutil to run daily through a cron job. Copy the disk\u2019s files to a Cloud Storage bucket with archive storage class and an object lifecycle rule to delete the objects after 7 days."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-10-04T06:37:00.000Z",
        "voteCount": 1,
        "content": "B is the answer."
      },
      {
        "date": "2024-07-24T02:58:00.000Z",
        "voteCount": 2,
        "content": "You can create snapshots from persistent disks. B is the simplest option"
      },
      {
        "date": "2024-07-19T03:30:00.000Z",
        "voteCount": 3,
        "content": "B is answer."
      },
      {
        "date": "2024-07-16T03:52:00.000Z",
        "voteCount": 3,
        "content": "Compute Engine snapshots provide a fast and efficient way to back up the entire disk of a VM instance, including the operating system, applications, and data. They are incremental backups, meaning they only store the changes made since the last snapshot, which helps save storage costs."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 272,
    "url": "https://www.examtopics.com/discussions/google/view/143949-exam-associate-cloud-engineer-topic-1-question-272/",
    "body": "Your company requires that Google Cloud products are created with a specific configuration to comply with your company\u2019s security policies. You need to implement a mechanism that will allow software engineers at your company to deploy and update Google Cloud products in a preconfigured and approved manner. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate Java packages that utilize the Google Cloud Client Libraries for Java to configure Google Cloud products. Store and share the packages in a source code repository.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate bash scripts that utilize the Google Cloud CLI to configure Google Cloud products. Store and share the bash scripts in a source code repository.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Google Cloud APIs by using curl to configure Google Cloud products. Store and share the curl commands in a source code repository.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate Terraform modules that utilize the Google Cloud Terraform Provider to configure Google Cloud products. Store and share the modules in a source code repository.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-16T03:54:00.000Z",
        "voteCount": 6,
        "content": "Here's why this is the most suitable solution:\n\nInfrastructure as Code (IaC): Terraform is an IaC tool that allows you to define and provision infrastructure resources using declarative configuration files. This approach ensures consistency in resource configuration across different deployments, making it easier to enforce security policies and compliance requirements.\nModularity: Terraform modules promote reusability and maintainability. You can create modules for specific GCP products or configurations and share them within your organization. This reduces duplication of effort and ensures that all deployments adhere to the same standards."
      },
      {
        "date": "2024-10-04T06:41:00.000Z",
        "voteCount": 1,
        "content": "Terraform is the most commonly used tool to provision and automate Google Cloud infrastructure.\n\nhttps://cloud.google.com/docs/terraform/terraform-overview"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 273,
    "url": "https://www.examtopics.com/discussions/google/view/143951-exam-associate-cloud-engineer-topic-1-question-273/",
    "body": "You are a Google Cloud organization administrator. You need to configure organization policies and log sinks on Google Cloud projects that cannot be removed by project users to comply with your company's security policies. The security policies are different for each company department. Each company department has a user with the Project Owner role assigned to their projects. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a standard naming convention for projects that includes the department name. Configure organization policies on the organization and log sinks on the projects.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a standard naming convention for projects that includes the department name. Configure both organization policies and log sinks on the projects.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOrganize projects under folders for each department. Configure both organization policies and log sinks on the folders.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOrganize projects under folders for each department. Configure organization policies on the organization and log sinks on the folders."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-16T04:08:00.000Z",
        "voteCount": 7,
        "content": "As per question, the security policies are different for each company department. Hence, organizing each department in folders, Organizational Policies on Folders, and Log Sinks on Folders will work. Project owners cannot modify or remove organization policies applied at the folder or organization level."
      },
      {
        "date": "2024-07-19T03:46:00.000Z",
        "voteCount": 2,
        "content": "RuchiMishra is right."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 274,
    "url": "https://www.examtopics.com/discussions/google/view/143952-exam-associate-cloud-engineer-topic-1-question-274/",
    "body": "You are deploying a web application using Compute Engine. You created a managed instance group (MIG) to host the application. You want to follow Google-recommended practices to implement a secure and highly available solution. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse SSL proxy load balancing for the MIG and an A record in your DNS private zone with the load balancer's IP address.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse SSL proxy load balancing for the MIG and a CNAME record in your DNS public zone with the load balancer\u2019s IP address.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse HTTP(S) load balancing for the MIG and a CNAME record in your DNS private zone with the load balancer\u2019s IP address.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse HTTP(S) load balancing for the MIG and an A record in your DNS public zone with the load balancer\u2019s IP address.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-08-11T11:05:00.000Z",
        "voteCount": 2,
        "content": "D IS CORRECT"
      },
      {
        "date": "2024-07-24T03:04:00.000Z",
        "voteCount": 2,
        "content": "D is correct - web application would use HTTP/S and you would need an A record which is public to access it"
      },
      {
        "date": "2024-07-19T03:58:00.000Z",
        "voteCount": 2,
        "content": "D is correct!"
      },
      {
        "date": "2024-07-19T03:57:00.000Z",
        "voteCount": 2,
        "content": "Compliance question was already two times before. And I stay with HTTP(S) load balancer because it recommend practices  in Associate Cloud Engineer Path.\n\nHTTP(S) Load Balancing: This is a globally distributed, managed service for HTTP and HTTPS traffic that provides high availability, automatic scaling, and support for SSL termination. It ensures your web application is secure and can handle varying traffic loads efficiently.\n\nA Record in DNS Public Zone: An A record maps your domain name to the IP address of the load balancer, making your application accessible to users over the internet. Using a public DNS zone ensures that your application is reachable globally."
      },
      {
        "date": "2024-07-19T04:01:00.000Z",
        "voteCount": 2,
        "content": "SSL Proxy Load Balancing with DNS Private Zone and A record: SSL Proxy Load Balancing is suitable for non-HTTP(S) traffic and not recommended for web applications serving HTTP/HTTPS content. Using a private DNS zone would restrict access to internal networks, not the internet."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 275,
    "url": "https://www.examtopics.com/discussions/google/view/143953-exam-associate-cloud-engineer-topic-1-question-275/",
    "body": "You have several hundred microservice applications running in a Google Kubernetes Engine (GKE) cluster. Each microservice is a deployment with resource limits configured for each container in the deployment. You've observed that the resource limits for memory and CPU are not appropriately set for many of the microservices. You want to ensure that each microservice has right sized limits for memory and CPU. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a Vertical Pod Autoscaler for each microservice.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the cluster's node pool machine type and choose a machine type with more memory and CPU.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a Horizontal Pod Autoscaler for each microservice.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure GKE cluster autoscaling."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 8,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-07-28T05:30:00.000Z",
        "voteCount": 2,
        "content": "As mentioned \"resource limits configured for each container in the deployment\", so can not vertically increase resources/memory\nCluster scale out is not relevant here.\nWith Horizontal scaling, adding more pods ultimately makes sure each service have \"right sized memory and CPU\""
      },
      {
        "date": "2024-07-24T03:09:00.000Z",
        "voteCount": 2,
        "content": "Vertical is for more specific resources of the individual pods. Horizontal is for creating more copies of the instances (adding more pods)."
      },
      {
        "date": "2024-07-21T07:12:00.000Z",
        "voteCount": 2,
        "content": "A seems better"
      },
      {
        "date": "2024-07-16T04:12:00.000Z",
        "voteCount": 4,
        "content": "Here's why a Vertical Pod Autoscaler (VPA) is the most suitable solution for this scenario:\n\nRight-Sizing Resources: VPA is designed to automatically adjust the resource requests and limits (CPU and memory) for pods based on their actual usage. This ensures that pods have enough resources to run efficiently without being over-provisioned, which can lead to wasted resources and higher costs.\n\nAutomated Optimization: VPA continuously monitors the resource usage of your pods and recommends optimal settings. You can choose to apply these recommendations automatically or manually, giving you flexibility and control over the process.\n\nMicroservice-Specific Tuning:  By configuring a VPA for each microservice, you can fine-tune the resource allocation for each individual service based on its specific needs and usage patterns. This is more efficient than making blanket changes to the entire cluster or node pool."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 276,
    "url": "https://www.examtopics.com/discussions/google/view/143954-exam-associate-cloud-engineer-topic-1-question-276/",
    "body": "Your company uses BigQuery to store and analyze data. Upon submitting your query in BigQuery, the query fails with a quotaExceeded error. You need to diagnose the issue causing the error. What should you do? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse BigQuery BI Engine to analyze the issue.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the INFORMATION_SCHEMA views to analyze the underlying issue.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Cloud Trace to analyze the issue.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSearch errors in Cloud Audit Logs to analyze the issue.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tView errors in Cloud Monitoring to analyze the issue."
    ],
    "answer": "BD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BD",
        "count": 13,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-16T04:15:00.000Z",
        "voteCount": 6,
        "content": "B &amp; D.\nHere's why these two methods are crucial for diagnosing quotaExceeded errors in BigQuery:\n\nB. INFORMATION_SCHEMA Views: BigQuery's INFORMATION_SCHEMA provides metadata about datasets, tables, and jobs. Relevant views like JOBS_BY_PROJECT and JOBS_BY_USER can help you analyze recent queries, their resource consumption (bytes processed, slots used), and any errors encountered. This can reveal which queries are exceeding quotas and what type of quota (e.g., query size, daily limit) is being exceeded.\n\nD. Cloud Audit Logs: Audit logs record all API calls and administrative actions within your GCP projects. By searching for quotaExceeded errors in the audit logs, you can see the exact error messages, timestamps, and potentially the queries that triggered the error. This helps pinpoint the specific resources and actions causing the issue."
      },
      {
        "date": "2024-10-04T06:52:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/bigquery/docs/troubleshoot-quotas#diagnosis"
      },
      {
        "date": "2024-09-27T19:35:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/bigquery/docs/troubleshoot-quotas"
      },
      {
        "date": "2024-07-21T07:14:00.000Z",
        "voteCount": 1,
        "content": "BD is correct"
      },
      {
        "date": "2024-07-19T04:53:00.000Z",
        "voteCount": 4,
        "content": "a. Query INFORMATION_SCHEMA to identify requests that exceed quotas.\n\nb. See the Cloud Audit Logs for more details on 'quotaExceeded' errors and the context in which they occur."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 277,
    "url": "https://www.examtopics.com/discussions/google/view/143955-exam-associate-cloud-engineer-topic-1-question-277/",
    "body": "Your team has developed a stateless application which requires it to be run directly on virtual machines. The application is expected to receive a fluctuating amount of traffic and needs to scale automatically. You need to deploy the application. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the application on a managed instance group and configure autoscaling.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the application on a Kubernetes Engine cluster and configure node pool autoscaling.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the application on Cloud Functions and configure the maximum number instances.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the application on Cloud Run and configure autoscaling."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-24T03:17:00.000Z",
        "voteCount": 2,
        "content": "If the application did not have to be run on VMs I would have chosen D as Cloud Run would be easier and can scale to zero which reduces idle-time cost. But the app does, so MIGs is the choice."
      },
      {
        "date": "2024-07-21T07:16:00.000Z",
        "voteCount": 2,
        "content": "MIG supports stateless applications."
      },
      {
        "date": "2024-07-19T04:58:00.000Z",
        "voteCount": 2,
        "content": "Vote for A"
      },
      {
        "date": "2024-07-16T04:18:00.000Z",
        "voteCount": 4,
        "content": "Here's why A is the most suitable solution:\n\nManaged Instance Groups (MIGs): MIGs are designed to manage groups of identical VMs, making them ideal for running stateless applications. They provide features like auto-scaling, auto-healing, and load balancing, which are crucial for handling fluctuating traffic.\n\nAutoscaling:  You can configure autoscaling policies to automatically add or remove VM instances based on metrics like CPU utilization, HTTP load balancing traffic, or Stackdriver Monitoring metrics. This ensures that your application can scale up to handle peak traffic and scale down during periods of low demand."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 278,
    "url": "https://www.examtopics.com/discussions/google/view/143956-exam-associate-cloud-engineer-topic-1-question-278/",
    "body": "Your web application is hosted on Cloud Run and needs to query a Cloud SQL database. Every morning during a traffic spike, you notice API quota errors in Cloud SQL logs. The project has already reached the maximum API quota. You want to make a configuration change to mitigate the issue. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the minimum number of Cloud Run instances.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse traffic splitting.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the maximum number of Cloud Run instances.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet a minimum concurrent requests environment variable for the application."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 17,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-16T04:21:00.000Z",
        "voteCount": 8,
        "content": "Here's why A is the most effective solution to mitigate API quota errors during traffic spikes:\n\nCold Starts and API Calls: Cloud Run services scale to zero when not in use. When a new request arrives, a new instance is spun up, leading to a cold start. During this cold start, multiple API calls might be made to initialize the application and connect to the Cloud SQL database. If there's a sudden spike in traffic, a large number of cold starts can occur simultaneously, exceeding the Cloud SQL API quota.\n\nMinimum Instances: By setting a minimum number of Cloud Run instances, you can ensure that a few instances are always running, even during periods of low traffic. This eliminates cold starts during traffic spikes and reduces the number of concurrent API calls made to Cloud SQL, helping you stay within the quota limits."
      },
      {
        "date": "2024-08-10T02:17:00.000Z",
        "voteCount": 2,
        "content": "A Make sense"
      },
      {
        "date": "2024-07-24T03:19:00.000Z",
        "voteCount": 4,
        "content": "There has been a previous question relating to this issue which is caused by Cold Starts (RuchiMishra explains this). Solving the issue would be by configuring a minimum number of instances always running"
      },
      {
        "date": "2024-07-21T07:20:00.000Z",
        "voteCount": 3,
        "content": "As explained by RuchiMishra, we need to keep a minimum number of instances always running."
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 279,
    "url": "https://www.examtopics.com/discussions/google/view/143957-exam-associate-cloud-engineer-topic-1-question-279/",
    "body": "You need to deploy a single stateless web application with a web interface and multiple endpoints. For security reasons, the web application must be reachable from an internal IP address from your company's private VPC and on-premises network. You also need to update the web application multiple times per day with minimal effort and want to manage a minimal amount of cloud infrastructure. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the web application on Google Kubernetes Engine standard edition with an internal ingress.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the web application on Cloud Run with Private Google Access configured.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the web application on Cloud Run with Private Service Connect configured.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the web application to GKE Autopilot with Private Google Access configured."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 10,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-09-02T14:47:00.000Z",
        "voteCount": 3,
        "content": "We need to connect to the on-premise network. Private google connect can enable that"
      },
      {
        "date": "2024-08-27T12:14:00.000Z",
        "voteCount": 3,
        "content": "Option B: Private Google Access allows internal Google Cloud resource access but does not make Cloud Run services accessible from on-premises networks.\nOption C: Private Service Connect creates private endpoints that are accessible from your VPC, and with the proper network configuration (e.g., VPN or Interconnect), allows access from on-premises networks."
      },
      {
        "date": "2024-07-25T08:06:00.000Z",
        "voteCount": 4,
        "content": "We need to connect to the on-premise network. Private google access does not enable this."
      },
      {
        "date": "2024-07-24T03:21:00.000Z",
        "voteCount": 2,
        "content": "The sentence \"you also need to update the web application multiple times per day with minimal effort and want to manage a minimal amount of cloud infrastructure\" makes Cloud Run favourable over GKE."
      },
      {
        "date": "2024-07-19T05:24:00.000Z",
        "voteCount": 1,
        "content": "The most important thing that told me to choose B is the easier upgrade that Cloud Run provides."
      },
      {
        "date": "2024-07-16T04:27:00.000Z",
        "voteCount": 2,
        "content": "Here's why B is the most suitable for the given requirements:\n\nCloud Run: Cloud Run is a fully managed serverless platform for containerized applications. It eliminates the need to manage infrastructure, making it easy to deploy and update your web application multiple times a day with minimal effort. It also scales automatically based on traffic.\n\nPrivate Google Access (PGA): PGA allows resources in a private VPC network (without public IP addresses) to access Google APIs and services, including Cloud Run. This enables you to keep your web application private while still making it accessible from your internal network and on-premises environment."
      },
      {
        "date": "2024-08-27T12:15:00.000Z",
        "voteCount": 2,
        "content": "It doesn't create private endpoints"
      }
    ],
    "examNameCode": "associate-cloud-engineer",
    "topicNumber": "1"
  }
]