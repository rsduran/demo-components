[
  {
    "topic": 1,
    "index": 1,
    "url": "https://www.examtopics.com/discussions/google/view/92022-exam-professional-cloud-database-engineer-topic-1-question-1/",
    "body": "You are developing a new application on a VM that is on your corporate network. The application will use Java Database Connectivity (JDBC) to connect to Cloud SQL for PostgreSQL. Your Cloud SQL instance is configured with IP address 192.168.3.48, and SSL is disabled. You want to ensure that your application can access your database instance without requiring configuration changes to your database. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDefine a connection string using your Google username and password to point to the external (public) IP address of your Cloud SQL instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDefine a connection string using a database username and password to point to the internal (private) IP address of your Cloud SQL instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDefine a connection string using Cloud SQL Auth proxy configured with a service account to point to the internal (private) IP address of your Cloud SQL instance.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDefine a connection string using Cloud SQL Auth proxy configured with a service account to point to the external (public) IP address of your Cloud SQL instance."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 16,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-02-28T02:20:00.000Z",
        "voteCount": 7,
        "content": "Option B is the best choice.\n\nBy using the internal (private) IP address of the Cloud SQL instance, the traffic will stay within the corporate network and will not traverse the public internet. This will help to ensure that the traffic is secure and cannot be intercepted by unauthorized parties. Additionally, using the internal IP address does not require any additional configuration changes to the database instance.\n\nOption A is not recommended as it requires exposing the database instance's external (public) IP address, which can be less secure and may require additional firewall rules.\n\nOption C is a valid option if SSL is enabled on the Cloud SQL instance, but since SSL is disabled in this scenario, this option is not suitable.\n\nOption D is not recommended as it requires exposing the database instance's external (public) IP address, which can be less secure and may require additional firewall rules."
      },
      {
        "date": "2023-03-26T03:33:00.000Z",
        "voteCount": 1,
        "content": "The Cloud SQL Proxy wraps your connection in an SSL/TLS layer, resolving the concern about compatibility:\nhttps://cloud.google.com/sql/docs/postgres/connect-auth-proxy"
      },
      {
        "date": "2023-11-20T15:25:00.000Z",
        "voteCount": 1,
        "content": "B can be the correct answer, bt the most secure and best solution is C, because the Auth Proxy will enable ssl for you without enabling it on the Cloud SQL instance."
      },
      {
        "date": "2023-03-02T19:36:00.000Z",
        "voteCount": 5,
        "content": "C.\nThe IP address given is a private IP address and not routable via the internet. Therefore any answer which references a public IP is wrong by definition (A, D). That leaves B and C. B cannot be correct because the app is on a corporate network and thus not on a Google VPC network. Good security practices dictate using Cloud SQL Auth Proxy and a service account which access the Cloud SQL instance via its private IP address."
      },
      {
        "date": "2024-09-28T01:09:00.000Z",
        "voteCount": 4,
        "content": "The Cloud SQL connectors are libraries that provide encryption and IAM-based authorization when connecting to a Cloud SQL instance. They can't provide a network path to a Cloud SQL instance if one is not already present.\n\n\nOther ways to connect to a Cloud SQL instance include using a database client or the Cloud SQL Auth proxy.\n"
      },
      {
        "date": "2024-05-08T19:58:00.000Z",
        "voteCount": 1,
        "content": "because the most secure way is using Cloud SQL Proxy"
      },
      {
        "date": "2023-10-30T08:12:00.000Z",
        "voteCount": 3,
        "content": "C is the valid"
      },
      {
        "date": "2023-09-17T23:21:00.000Z",
        "voteCount": 2,
        "content": "Vote for C"
      },
      {
        "date": "2023-03-02T14:03:00.000Z",
        "voteCount": 1,
        "content": "Service account is a must."
      },
      {
        "date": "2023-02-24T11:30:00.000Z",
        "voteCount": 2,
        "content": "Vote for C"
      },
      {
        "date": "2023-01-04T13:43:00.000Z",
        "voteCount": 4,
        "content": "Vote for C"
      },
      {
        "date": "2022-12-26T09:55:00.000Z",
        "voteCount": 3,
        "content": "Vote for C"
      },
      {
        "date": "2022-12-25T08:33:00.000Z",
        "voteCount": 3,
        "content": "C: Define a connection string using Cloud SQL Auth proxy *** configured with a service account to point to the internal (private) IP address of your Cloud SQL instance."
      },
      {
        "date": "2022-12-24T14:11:00.000Z",
        "voteCount": 2,
        "content": "Database Migration Service\nSimplify migrations to the cloud. Available now for MySQL and PostgreSQL, with SQL Server and Oracle migrations in preview.\n\u2022\tMigrate to Cloud SQL and AlloyDB for PostgreSQL from on-premises, Google Cloud, or other clouds\n\u2022\tReplicate data continuously for minimal downtime migrations\n\u2022\tServerless and easy to set up"
      },
      {
        "date": "2022-12-22T21:55:00.000Z",
        "voteCount": 2,
        "content": "C is the correct  answer"
      },
      {
        "date": "2022-12-19T07:58:00.000Z",
        "voteCount": 4,
        "content": "C is correct answer. First of all SSL is disabled and it is not secure to get it exposed to Internet. https://cloud.google.com/sql/docs/postgres/connect-overview#authentication_options"
      },
      {
        "date": "2022-12-18T13:25:00.000Z",
        "voteCount": 2,
        "content": "C is correct, must be private ip because the ip starts with 192... and cloud sql require a proxy to connect because exist on a tenant project"
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 2,
    "url": "https://www.examtopics.com/discussions/google/view/92032-exam-professional-cloud-database-engineer-topic-1-question-2/",
    "body": "Your digital-native business runs its database workloads on Cloud SQL. Your website must be globally accessible 24/7. You need to prepare your Cloud SQL instance for high availability (HA). You want to follow Google-recommended practices. What should you do? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up manual backups.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a PostgreSQL database on-premises as the HA option.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure single zone availability for automated backups.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable point-in-time recovery.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSchedule automated backups.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "DE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "DE",
        "count": 16,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-02-04T14:05:00.000Z",
        "voteCount": 6,
        "content": "D,E.\nA is wrong because why bother configuring manual backups when Cloud SQL will automate that for you.\nB seems attractive, but why bother replicating back to on-prem when you can configure a Cloud SQL for HA.\nC is wrong because a single zone failure would not give you HA.\nThat leaves D &amp; E."
      },
      {
        "date": "2024-05-14T03:14:00.000Z",
        "voteCount": 1,
        "content": "Agree with D, E"
      },
      {
        "date": "2024-05-08T19:57:00.000Z",
        "voteCount": 1,
        "content": "D and E because point in time recovery and schedule backup are HA efforts and CloudSQL already have these features"
      },
      {
        "date": "2023-09-17T23:23:00.000Z",
        "voteCount": 1,
        "content": "Vote for D, E."
      },
      {
        "date": "2023-03-02T14:05:00.000Z",
        "voteCount": 2,
        "content": "Only valid options."
      },
      {
        "date": "2022-12-26T09:57:00.000Z",
        "voteCount": 3,
        "content": "Vote for DE, seems only reasonable options to me"
      },
      {
        "date": "2022-12-26T01:53:00.000Z",
        "voteCount": 2,
        "content": "To prepare your Cloud SQL instance for high availability, you should do the following:\n\nD. Enable point-in-time recovery - This feature allows you to restore your database to a specific point in time. It helps protect against data loss and can be used in the event of data corruption or accidental data deletion.\n\nE. Schedule automated backups - Automated backups allow you to take regular backups of your database without manual intervention. You can use these backups to restore your database in the event of data loss or corruption.\n\nNote that options A and C are not recommended practices for high availability. Option B is not related to Cloud SQL."
      },
      {
        "date": "2022-12-22T21:56:00.000Z",
        "voteCount": 2,
        "content": "D,E is the correct  answer"
      },
      {
        "date": "2022-12-22T16:47:00.000Z",
        "voteCount": 2,
        "content": "Enable point-in-time recovery.\nSchedule automated backups."
      },
      {
        "date": "2022-12-22T05:44:00.000Z",
        "voteCount": 1,
        "content": "D. Enable point-in-time recovery.\nE. Schedule automated backups"
      },
      {
        "date": "2022-12-21T07:26:00.000Z",
        "voteCount": 2,
        "content": "D. Enable point-in-time recovery.\nE. Schedule automated backups.\n\nTo prepare your Cloud SQL instance for high availability, Google recommends enabling point-in-time recovery and scheduling automated backups.\n\nPoint-in-time recovery allows you to restore your database to a specific point in time, helping you to recover from data loss or corruption.\n\nScheduling automated backups ensures that you have a recent copy of your database available for recovery in case of an outage or other issue."
      },
      {
        "date": "2022-12-21T07:26:00.000Z",
        "voteCount": 1,
        "content": "Option A, setting up manual backups, would not be a recommended practice because manual backups are prone to errors and can be time-consuming to create and maintain. Automated backups are a more reliable and efficient way to ensure that you have a recent copy of your database available for recovery.\n\nOption B, creating a PostgreSQL database on-premises as the HA option, would not be a recommended practice because it would not take advantage of the high availability features provided by Cloud SQL.\n\nOption C, configuring single zone availability for automated backups, would not be a recommended practice because it would not provide sufficient protection against outages or other issues. To ensure high availability, it is recommended to use a multi-zone configuration for your Cloud SQL instance."
      },
      {
        "date": "2022-12-19T00:06:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/sql/docs/mysql/high-availability#backups-and-restores"
      },
      {
        "date": "2022-12-19T00:06:00.000Z",
        "voteCount": 1,
        "content": "Answer is D and E. https://cloud.google.com/sql/docs/mysql/high-availability#backups-and-restores"
      },
      {
        "date": "2022-12-18T19:50:00.000Z",
        "voteCount": 2,
        "content": "Automated backups and point-in-time recovery must be enabled for high availability (point-in-time recovery uses binary logs).\nFor more information check here -&gt; https://cloud.google.com/sql/docs/mysql/high-availability#backups-and-restores"
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 3,
    "url": "https://www.examtopics.com/discussions/google/view/92023-exam-professional-cloud-database-engineer-topic-1-question-3/",
    "body": "Your company wants to move to Google Cloud. Your current data center is closing in six months. You are running a large, highly transactional Oracle application footprint on VMWare. You need to design a solution with minimal disruption to the current architecture and provide ease of migration to Google Cloud. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate applications and Oracle databases to Google Cloud VMware Engine (VMware Engine).\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate applications and Oracle databases to Compute Engine.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate applications to Cloud SQL.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate applications and Oracle databases to Google Kubernetes Engine (GKE)."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 12,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-05-08T19:59:00.000Z",
        "voteCount": 1,
        "content": "obviously A"
      },
      {
        "date": "2023-09-17T23:25:00.000Z",
        "voteCount": 2,
        "content": "Lift , shift. Correct andswer is A."
      },
      {
        "date": "2023-03-02T14:08:00.000Z",
        "voteCount": 2,
        "content": "Classic lift and lift. Everything keeps the same structure. Therefore minimizing impact to zero."
      },
      {
        "date": "2023-02-04T14:10:00.000Z",
        "voteCount": 3,
        "content": "A.\nThe key here is the current architecture and minimal disruption to it. The simplest way to keep the current architecture is a live migrate using VMware. That can only mean one thing, use  Oracle running in GCVE. \nYou could do B. There's nothing stopping you creating a VM in GCE, copying the Oracle binaries to it and spinning up an Oracle database or several. However, the licensing costs would not be attractive (if even supported), plus the migration would likely be disruptive. C is wrong because Cloud SQL doesn't support Oracle. D is wrong because that represents an architecture change."
      },
      {
        "date": "2022-12-26T09:59:00.000Z",
        "voteCount": 4,
        "content": "Since there is no Bare Metal for Oracle option and VMware mentioned -&gt; Choose VMware"
      },
      {
        "date": "2022-12-19T00:10:00.000Z",
        "voteCount": 2,
        "content": "Here is the explanation: https://cloud.google.com/blog/products/databases/migrate-databases-to-google-cloud-vmware-engine-gcve"
      },
      {
        "date": "2022-12-18T20:01:00.000Z",
        "voteCount": 1,
        "content": "A GCVE VMware environment runs natively on Google Cloud bare metal infrastructure in some Google Cloud locations, and the GCVE service includes all the features required to help consume the VMware platforms efficiently and securely\n.\nhttps://cloud.google.com/blog/products/databases/migrate-databases-to-google-cloud-vmware-engine-gcve"
      },
      {
        "date": "2022-12-18T14:00:00.000Z",
        "voteCount": 1,
        "content": "Oracle databases can only be migrated to bare metal solutions"
      },
      {
        "date": "2022-12-22T22:01:00.000Z",
        "voteCount": 1,
        "content": "Yes .GCP recommended to use Bare Metal solution for Oracle but option is missing in answers"
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 4,
    "url": "https://www.examtopics.com/discussions/google/view/92034-exam-professional-cloud-database-engineer-topic-1-question-4/",
    "body": "Your customer has a global chat application that uses a multi-regional Cloud Spanner instance. The application has recently experienced degraded performance after a new version of the application was launched. Your customer asked you for assistance. During initial troubleshooting, you observed high read latency. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse query parameters to speed up frequently executed queries.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the Cloud Spanner configuration from multi-region to single region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse SQL statements to analyze SPANNER_SYS.READ_STATS* tables.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse SQL statements to analyze SPANNER_SYS.QUERY_STATS* tables."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 19,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-12-12T13:53:00.000Z",
        "voteCount": 3,
        "content": "Read statistics provide insight into how an application is using the database, and are useful when investigating performance issues.\n\nhttps://cloud.google.com/spanner/docs/introspection/read-statistics"
      },
      {
        "date": "2023-09-25T12:48:00.000Z",
        "voteCount": 2,
        "content": "C! You should analyze the situation before changing the architecture so drastically."
      },
      {
        "date": "2023-09-20T19:36:00.000Z",
        "voteCount": 2,
        "content": "C is the correct answer"
      },
      {
        "date": "2023-09-17T23:28:00.000Z",
        "voteCount": 1,
        "content": "C is definitely the correct answer here. SPANNER_SYS.READ_STATS* contains statistics about reads."
      },
      {
        "date": "2023-08-04T22:12:00.000Z",
        "voteCount": 1,
        "content": "C is the correct answer"
      },
      {
        "date": "2023-06-20T13:43:00.000Z",
        "voteCount": 2,
        "content": "C. Read stats"
      },
      {
        "date": "2023-03-11T05:41:00.000Z",
        "voteCount": 2,
        "content": "C.\nA Query parameters is vague at best. B would not achieve anything. C and D look interesting, but as others have stated, querying the READ_STATS* tables would give you information about what is causing read issues. So C is the best answer."
      },
      {
        "date": "2023-03-05T09:43:00.000Z",
        "voteCount": 3,
        "content": "C. Use SQL statements to analyze SPANNER_SYS.READ_STATS* tables"
      },
      {
        "date": "2023-03-05T06:59:00.000Z",
        "voteCount": 2,
        "content": "C. Use SQL statements to analyze SPANNER_SYS.READ_STATS* tables"
      },
      {
        "date": "2022-12-26T01:51:00.000Z",
        "voteCount": 4,
        "content": "C. Use SQL statements to analyze SPANNER_SYS.READ_STATS* tables.\n\nTo troubleshoot high read latency, you can use SQL statements to analyze the SPANNER_SYS.READ_STATS* tables. These tables contain statistics about read operations in Cloud Spanner, including the number of reads, read latency, and the number of read errors. By analyzing these tables, you can identify the cause of the high read latency and take appropriate action to resolve the issue. Other options, such as using query parameters to speed up frequently executed queries or changing the Cloud Spanner configuration from multi-region to single region, may not be directly related to the issue of high read latency. Similarly, analyzing the SPANNER_SYS.QUERY_STATS* tables, which contain statistics about query operations, may not be relevant to the issue of high read latency."
      },
      {
        "date": "2022-12-25T08:30:00.000Z",
        "voteCount": 3,
        "content": "C: Use SQL statements to analyze ***** SPANNER_SYS.READ_STATS* tables."
      },
      {
        "date": "2022-12-22T22:04:00.000Z",
        "voteCount": 4,
        "content": "C is the correct  answer"
      },
      {
        "date": "2022-12-19T06:49:00.000Z",
        "voteCount": 1,
        "content": "B - Is correct"
      },
      {
        "date": "2022-12-18T20:12:00.000Z",
        "voteCount": 3,
        "content": "Read statistics provide insight into how an application is using the database, and are useful when investigating performance issues\n.\nhttps://cloud.google.com/spanner/docs/introspection/read-statistics#when_to_use_read_statistics"
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 5,
    "url": "https://www.examtopics.com/discussions/google/view/92040-exam-professional-cloud-database-engineer-topic-1-question-5/",
    "body": "Your company has PostgreSQL databases on-premises and on Amazon Web Services (AWS). You are planning multiple database migrations to Cloud SQL in an effort to reduce costs and downtime. You want to follow Google-recommended practices and use Google native data migration tools. You also want to closely monitor the migrations as part of the cutover strategy. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Database Migration Service to migrate all databases to Cloud SQL.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Database Migration Service for one-time migrations, and use third-party or partner tools for change data capture (CDC) style migrations.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse data replication tools and CDC tools to enable migration.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a combination of Database Migration Service and partner tools to support the data migration strategy."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 25,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-05-31T11:40:00.000Z",
        "voteCount": 1,
        "content": "database migration service can be used to migrate from on-prem and other clouds (AWS)"
      },
      {
        "date": "2024-03-19T00:55:00.000Z",
        "voteCount": 1,
        "content": "A is a correct answer"
      },
      {
        "date": "2023-12-21T02:33:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/database-migration/docs/overview"
      },
      {
        "date": "2023-12-12T14:07:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/database-migration/docs/overview"
      },
      {
        "date": "2023-09-17T23:29:00.000Z",
        "voteCount": 3,
        "content": "A is the correct answer here."
      },
      {
        "date": "2023-05-03T14:21:00.000Z",
        "voteCount": 3,
        "content": "The question says to use Google native data migration tools"
      },
      {
        "date": "2024-03-31T10:16:00.000Z",
        "voteCount": 1,
        "content": "good point, thank you for pointing it out"
      },
      {
        "date": "2023-03-12T21:54:00.000Z",
        "voteCount": 4,
        "content": "DMS will do the CDC too."
      },
      {
        "date": "2023-03-11T05:50:00.000Z",
        "voteCount": 3,
        "content": "A.\nThe question says to use Google native data migration tools. That eliminates B and D. C doesn't specify the data replication tool in question so it's a reasonable assumption its referring to database native replication which wouldn't be a Google native solution. That eliminates C. That leave A."
      },
      {
        "date": "2023-03-02T14:16:00.000Z",
        "voteCount": 1,
        "content": "DMS  will do the job. For the init time and the CDC phase"
      },
      {
        "date": "2023-03-02T05:53:00.000Z",
        "voteCount": 1,
        "content": "Option A is the most straightforward and recommended solution for migrating PostgreSQL databases to Cloud SQL while following Google-recommended practices and using native data migration tools."
      },
      {
        "date": "2023-02-02T14:38:00.000Z",
        "voteCount": 1,
        "content": "A\nhttps://cloud.google.com/blog/products/databases/tips-for-migrating-across-compatible-database-engines"
      },
      {
        "date": "2022-12-26T10:02:00.000Z",
        "voteCount": 4,
        "content": "A is enough"
      },
      {
        "date": "2022-12-25T08:29:00.000Z",
        "voteCount": 1,
        "content": "A: Use Database Migration Service to migrate all databases to Cloud SQL."
      },
      {
        "date": "2022-12-22T22:09:00.000Z",
        "voteCount": 1,
        "content": "C is the correct  answer, we can use GCP migration tool for onetime or CDC"
      },
      {
        "date": "2023-01-06T07:47:00.000Z",
        "voteCount": 2,
        "content": "My bad ,A is correct answer"
      },
      {
        "date": "2022-12-22T11:31:00.000Z",
        "voteCount": 1,
        "content": "A for live magrations"
      },
      {
        "date": "2022-12-18T21:51:00.000Z",
        "voteCount": 2,
        "content": "Migrate to Cloud SQL and AlloyDB for PostgreSQL from on-premises, Google Cloud, or other clouds"
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 6,
    "url": "https://www.examtopics.com/discussions/google/view/92039-exam-professional-cloud-database-engineer-topic-1-question-6/",
    "body": "You are setting up a Bare Metal Solution environment. You need to update the operating system to the latest version. You need to connect the Bare Metal Solution environment to the internet so you can receive software updates. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSetup a static external IP address in your VPC network.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up bring your own IP (BYOIP) in your VPC.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up a Cloud NAT gateway on the Compute Engine VM.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up Cloud NAT service."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 19,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 7,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-08-01T09:32:00.000Z",
        "voteCount": 5,
        "content": "https://cloud.google.com/bare-metal/docs/bms-setup?hl=en#bms-access-options\nIt can't be D. This doc mentions NAT gateway, not Cloud NAT."
      },
      {
        "date": "2024-04-29T04:56:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/bare-metal/docs/bms-setup#bms-access-options\n\nNote: Cloud NAT feature doesn't support transitive endpoints thus it can not be used standalone to provide the internet access to the BMS server. Compute Engine VM must be used along with Cloud NAT."
      },
      {
        "date": "2024-02-18T23:14:00.000Z",
        "voteCount": 1,
        "content": "Knows the difference between Cloud NAT Services and Cloud NAT Gateway,\nCloud NAT services  - lets your VMs and container pods create outbound connections to the internet or to other Virtual Private Cloud (VPC) networks.\nCloud NAT Gateway - Cloud NAT uses NAT gateway to manage the connections.\n\nAlso, Cloud NAT gateway is region and VPC network specific, we can use cloud NAT mapping to a VM instance , not the gateway itself."
      },
      {
        "date": "2024-02-20T20:49:00.000Z",
        "voteCount": 1,
        "content": "Looks like we can map NAT gateway on GCE, will change to C."
      },
      {
        "date": "2024-02-01T04:36:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/bare-metal/docs/bms-setup?hl=en#bms-access-internet-vm-nat\noffers 3 options all of them involve Compute Engine VM and CloudNAT.  so C"
      },
      {
        "date": "2023-12-21T02:34:00.000Z",
        "voteCount": 1,
        "content": "C - The following instructions set up a NAT gateway on a Compute Engine VM to connect the servers in a Bare Metal Solution environment to the internet for purposes such as receiving software updates"
      },
      {
        "date": "2023-12-11T01:37:00.000Z",
        "voteCount": 1,
        "content": "C - The following instructions set up a NAT gateway on a Compute Engine VM to connect the servers in a Bare Metal Solution environment to the internet for purposes such as receiving software updates"
      },
      {
        "date": "2023-09-28T20:19:00.000Z",
        "voteCount": 3,
        "content": "C - https://cloud.google.com/bare-metal/docs/bms-setup?hl=en#bms-access-internet-vm-nat\n\nThe docs specifically says \"Setting up a NAT gateway on a Compute Engine VM\" is the way to give BMS internet access."
      },
      {
        "date": "2023-09-17T23:31:00.000Z",
        "voteCount": 1,
        "content": "Voting for C."
      },
      {
        "date": "2023-09-14T08:53:00.000Z",
        "voteCount": 2,
        "content": "Correct answer is D. Option C (setting up a Cloud NAT gateway on a Compute Engine VM) is not a recommended approach for providing internet access to your Bare Metal Solution environment"
      },
      {
        "date": "2023-08-04T02:00:00.000Z",
        "voteCount": 1,
        "content": "D:\nCloud NAT is a network address translation (NAT) service that allows you to connect your Bare Metal Solution environment to the internet without having to assign a public IP address to each machine. This is the best option for you because it is the most secure and easiest way to connect your Bare Metal Solution environment to the internet.\n\nhttps://cloud.google.com/bare-metal/docs/bms-setup#bms-access-options"
      },
      {
        "date": "2023-08-08T05:59:00.000Z",
        "voteCount": 1,
        "content": "In this link, there is no mention to Cloud NAT"
      },
      {
        "date": "2023-08-02T11:15:00.000Z",
        "voteCount": 1,
        "content": "The BMS environment traffic originates outside the VPC, so Cloud NAT can not work. C is the correct one"
      },
      {
        "date": "2023-07-28T21:13:00.000Z",
        "voteCount": 1,
        "content": "I agree D is the simplest option."
      },
      {
        "date": "2023-03-26T04:11:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/bare-metal/docs/bms-setup#bms-access-internet\nThe BMS documentation mentions the Cloud NAT service as an option but the provided example involves manually deploying a NAT gateway on a GCE machine, without explaining why you would need this option as opposed to the managed NAT service. However there are no limitations mentioned, so I take it both options are valid.\n\nIn this question, there is no mention of an existing GCE machine, therefore a managed NAT service is the simplest option, which avoids additional infrastructure - hence D is my choice."
      },
      {
        "date": "2023-03-09T01:27:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/bare-metal/docs/bms-setup#bms-access-internet \nD is the simplest option"
      },
      {
        "date": "2023-03-02T20:03:00.000Z",
        "voteCount": 2,
        "content": "C.\nA could directly expose your BMS solution to the internet. Not good. B doesn't make sense. D (setup Cloud NAT service) is incorrect. Cloud NAT is intended for GCE VMs which do not have an external (public) IP to connect to the internet. Since this is a BMS solution, it's not a GCE VM by definition, so D cannot be correct. That leaves C. Using a GCE VM configured with Cloud NAT is the way to solve this. The BMS server talks to the GCE VM via private IP and the GCE VM talks to the internet on the BMS server's behalf. The link provided by range9005 is spot on."
      },
      {
        "date": "2022-12-25T08:28:00.000Z",
        "voteCount": 2,
        "content": "C: Set up a Cloud NAT *** gateway on the Compute Engine VM."
      },
      {
        "date": "2022-12-22T22:10:00.000Z",
        "voteCount": 1,
        "content": "C is the correct  answer"
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 7,
    "url": "https://www.examtopics.com/discussions/google/view/92468-exam-professional-cloud-database-engineer-topic-1-question-7/",
    "body": "Your organization is running a MySQL workload in Cloud SQL. Suddenly you see a degradation in database performance. You need to identify the root cause of the performance degradation. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Logs Explorer to analyze log data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Monitoring to monitor CPU, memory, and storage utilization metrics.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Error Reporting to count, analyze, and aggregate the data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Debugger to inspect the state of an application."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-03-11T06:03:00.000Z",
        "voteCount": 2,
        "content": "B.\nNo actual errors are mentioned so using Error reporting would be irrelevant. That eliminates C. Inspecting the state of an application is also irrelevant since so mention of any application changes is made. Eliminate D. That leave A and B and B is the best answer. In Cloud SQL you get monitoring built right in (which you don't by default with GCE VMs). Cloud SQL monitoring metrics include CPU utilization, storage usage, memory usage, r/w operations and egress/ingress bytes. Has to be B."
      },
      {
        "date": "2022-12-25T08:28:00.000Z",
        "voteCount": 2,
        "content": "B: Use Cloud Monitoring ***** to monitor CPU, memory, and storage utilization metrics."
      },
      {
        "date": "2022-12-23T17:19:00.000Z",
        "voteCount": 1,
        "content": "B is the correct  answer"
      },
      {
        "date": "2022-12-22T15:43:00.000Z",
        "voteCount": 2,
        "content": "If your instance stops responding to connections or performance is degraded, make sure it conforms to the Operational Guidelines \nhttps://cloud.google.com/sql/docs/mysql/diagnose-issues#:~:text=If%20your%20instance%20stops%20responding%20to%20connections%20or%20performance%20is%20degraded%2C%20make%20sure%20it%20conforms%20to%20the%20Operational%20Guidelines\n\nAnd then checking resource constraints:\nStorage full\nCPU overloaded\nToo many database tables\nhttps://cloud.google.com/sql/docs/mysql/operational-guidelines#resource_constraints\n\nCloud Monitoring seems like the only way to check 2/3 of those, so for me answer is B"
      },
      {
        "date": "2022-12-22T11:33:00.000Z",
        "voteCount": 1,
        "content": "B. Use Cloud Monitoring to monitor CPU, memory, and storage utilization metrics."
      },
      {
        "date": "2022-12-22T09:00:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer - B"
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 8,
    "url": "https://www.examtopics.com/discussions/google/view/92505-exam-professional-cloud-database-engineer-topic-1-question-8/",
    "body": "You work for a large retail and ecommerce company that is starting to extend their business globally. Your company plans to migrate to Google Cloud. You want to use platforms that will scale easily, handle transactions with the least amount of latency, and provide a reliable customer experience. You need a storage layer for sales transactions and current inventory levels. You want to retain the same relational schema that your existing platform uses. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore your data in Firestore in a multi-region location, and place your compute resources in one of the constituent regions.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy Cloud Spanner using a multi-region instance, and place your compute resources close to the default leader region.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBuild an in-memory cache in Memorystore, and deploy to the specific geographic regions where your application resides.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy a Bigtable instance with a cluster in one region and a replica cluster in another geographic region."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-05-08T22:06:00.000Z",
        "voteCount": 1,
        "content": "Obviously B"
      },
      {
        "date": "2023-09-28T20:37:00.000Z",
        "voteCount": 1,
        "content": "B and Spanner as soon as it says Global and relational"
      },
      {
        "date": "2023-09-17T23:32:00.000Z",
        "voteCount": 1,
        "content": "B, spanner."
      },
      {
        "date": "2023-05-03T14:27:00.000Z",
        "voteCount": 2,
        "content": "It's B. Spanner"
      },
      {
        "date": "2023-04-26T05:23:00.000Z",
        "voteCount": 1,
        "content": "Global, scale easily and keeping the relation schema. It's B. Spanner. There is no other option."
      },
      {
        "date": "2023-03-11T06:07:00.000Z",
        "voteCount": 4,
        "content": "B.\nThe clues are \"globally\" and \"relational schema\". Relational rules out Firestore (A) and Bigtable (D). Cloud Spanner is both global in scale and relational, so it fits. So B."
      },
      {
        "date": "2023-03-02T21:00:00.000Z",
        "voteCount": 2,
        "content": "Spanner is the right answer"
      },
      {
        "date": "2022-12-25T08:27:00.000Z",
        "voteCount": 1,
        "content": "B: Deploy Cloud Spanner *** using a multi-region instance, and place your compute resources close to the default leader region."
      },
      {
        "date": "2022-12-23T17:24:00.000Z",
        "voteCount": 1,
        "content": "B is the correct  answer"
      },
      {
        "date": "2022-12-22T15:53:00.000Z",
        "voteCount": 2,
        "content": "Spanner seems to be the only option, since it's the only relational DB. Plus \"scale easily\", which is another clear indication of Spanner."
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 9,
    "url": "https://www.examtopics.com/discussions/google/view/92469-exam-professional-cloud-database-engineer-topic-1-question-9/",
    "body": "You host an application in Google Cloud. The application is located in a single region and uses Cloud SQL for transactional data. Most of your users are located in the same time zone and expect the application to be available 7 days a week, from 6 AM to 10 PM. You want to ensure regular maintenance updates to your Cloud SQL instance without creating downtime for your users. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a maintenance window during a period when no users will be on the system. Control the order of update by setting non-production instances to earlier and production instances to later.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate your database with one primary node and one read replica in the region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable maintenance notifications for users, and reschedule maintenance activities to a specific time after notifications have been sent.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure your Cloud SQL instance with high availability enabled."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 16,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-26T10:10:00.000Z",
        "voteCount": 9,
        "content": "Since we don't really need HA and we have a window that users are not need our app - A is fine, and D looks like an overkill"
      },
      {
        "date": "2024-05-08T23:27:00.000Z",
        "voteCount": 1,
        "content": "A because maintenance should be on the time with minimal impact"
      },
      {
        "date": "2023-09-17T23:34:00.000Z",
        "voteCount": 2,
        "content": "A is the correct answer."
      },
      {
        "date": "2023-03-11T06:13:00.000Z",
        "voteCount": 4,
        "content": "A.\nGoogle controls maintenance which could cause some downtime. Hence D would be irrelevant. C seems like a lot of work. B is also irrelevant. That leaves A as the best answer since you can choose your maintenance window to be after users will not be using the system. The addition of the earlier and later  information is fluff and is not relevant to the question."
      },
      {
        "date": "2023-03-02T21:17:00.000Z",
        "voteCount": 1,
        "content": "A is right."
      },
      {
        "date": "2023-01-04T13:47:00.000Z",
        "voteCount": 4,
        "content": "Vote for A. Configure time slot for maintainance\nHA for fail over but also has maintainance window"
      },
      {
        "date": "2022-12-25T08:25:00.000Z",
        "voteCount": 4,
        "content": "A: Configure a maintenance window ***** during a period when no users will be on the system. Control the order of update by setting non-production instances to earlier and production instances to later."
      },
      {
        "date": "2022-12-23T17:28:00.000Z",
        "voteCount": 1,
        "content": "6AM to 10PM"
      },
      {
        "date": "2022-12-23T17:27:00.000Z",
        "voteCount": 3,
        "content": "A is the correct  answer because application is used between"
      },
      {
        "date": "2022-12-22T09:14:00.000Z",
        "voteCount": 2,
        "content": "Correct answer - D\nhttps://cloud.google.com/sql/docs/mysql/high-availability#HA-configuration"
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 10,
    "url": "https://www.examtopics.com/discussions/google/view/92471-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "Your team recently released a new version of a highly consumed application to accommodate additional user traffic. Shortly after the release, you received an alert from your production monitoring team that there is consistently high replication lag between your primary instance and the read replicas of your Cloud SQL for MySQL instances. You need to resolve the replication lag. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIdentify and optimize slow running queries, or set parallel replication flags.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStop all running queries, and re-create the replicas.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEdit the primary instance to upgrade to a larger disk, and increase vCPU count.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEdit the primary instance to add additional memory."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 20,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-05-08T23:29:00.000Z",
        "voteCount": 1,
        "content": "obviously A"
      },
      {
        "date": "2024-03-17T21:07:00.000Z",
        "voteCount": 1,
        "content": "Other options don't directly resolve the issue. B is the worst answer since it disrupts the whole read setup."
      },
      {
        "date": "2023-12-21T02:34:00.000Z",
        "voteCount": 3,
        "content": "Definitely A."
      },
      {
        "date": "2023-09-17T23:35:00.000Z",
        "voteCount": 3,
        "content": "Definitely A."
      },
      {
        "date": "2023-04-26T05:31:00.000Z",
        "voteCount": 3,
        "content": "A. Optimize query for resolve replication lag. Docs: https://cloud.google.com/sql/docs/mysql/replication/replication-lag#optimize_queries_and_schema"
      },
      {
        "date": "2023-03-26T04:26:00.000Z",
        "voteCount": 1,
        "content": "I would have thought that recreating your replicas should be a standard action as part of a major client software release - especially one that potentially makes structural changes to the DB, as implied by the description here. \n\nOption B seems to me like the most effective solution in this scenario, as well as the simplest."
      },
      {
        "date": "2023-03-11T06:20:00.000Z",
        "voteCount": 4,
        "content": "A.\nHigh replication lag is caused when the write load is too high for the replica to handle. Other causes include slow running queries on the replica, tables not having PKs thus forcing FTS, queries like DELETE\u2026WHERE. Possible solutions are configure parallel replications, increase the size of the replica, send read traffic to the read replica, index the tables, identify and fix slow write queries, recreate the replica. To increase the throughput of replication increase the flag slave_parallel_workers. B is possible but should not be the first option. C and D add resource but don\u2019t fix the issue. As others have said, the issue could be network related and additional traffic is mentioned in the question. A is still the best answer."
      },
      {
        "date": "2023-03-02T21:27:00.000Z",
        "voteCount": 2,
        "content": "A. But in reality, none.  You need to analyze the root cause. Network connection latency or bandwidth might be relevant too."
      },
      {
        "date": "2023-04-26T05:31:00.000Z",
        "voteCount": 1,
        "content": "It's true, none options seems to be right here because you need to analyze everything first."
      },
      {
        "date": "2023-01-04T13:47:00.000Z",
        "voteCount": 1,
        "content": "Vote for A"
      },
      {
        "date": "2022-12-26T10:13:00.000Z",
        "voteCount": 2,
        "content": "Vote for A"
      },
      {
        "date": "2022-12-25T08:24:00.000Z",
        "voteCount": 2,
        "content": "A: Identify and optimize slow running queries, or set parallel ***** replication flags."
      },
      {
        "date": "2022-12-23T19:43:00.000Z",
        "voteCount": 2,
        "content": "A &amp; C is correct but A is the best answer"
      },
      {
        "date": "2022-12-22T09:24:00.000Z",
        "voteCount": 3,
        "content": "correct answer - A\nhttps://cloud.google.com/sql/docs/mysql/replication/manage-replicas#:~:text=Replication%20lag%20is%20consistently,Find%20and%20fix%20them."
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 11,
    "url": "https://www.examtopics.com/discussions/google/view/92472-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "Your organization operates in a highly regulated industry. Separation of concerns (SoC) and security principle of least privilege (PoLP) are critical. The operations team consists of:<br>Person A is a database administrator.<br>Person B is an analyst who generates metric reports.<br>Application C is responsible for automatic backups.<br>You need to assign roles to team members for Cloud Spanner. Which roles should you assign?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\troles/spanner.databaseAdmin for Person A<br>roles/spanner.databaseReader for Person B<br>roles/spanner.backupWriter for Application C\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\troles/spanner.databaseAdmin for Person A<br>roles/spanner.databaseReader for Person B<br>roles/spanner.backupAdmin for Application C",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\troles/spanner.databaseAdmin for Person A<br>roles/spanner.databaseUser for Person B<br>roles/spanner databaseReader for Application C",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\troles/spanner.databaseAdmin for Person A<br>roles/spanner.databaseUser for Person B<br>roles/spanner.backupWriter for Application C"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 16,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-03-11T06:38:00.000Z",
        "voteCount": 7,
        "content": "A.\nC is wrong because databaseUser (Person B) would allow database writes and the question says generate metric reports, which would be read access only. databaseReader (Application C) doesn't allow backups.\nD is wrong because databaseUser (Person B) would allow database writes. That leaves A and B. Based upon Google's own documentation, it must be A. B would work, but backupAdmin for Application C would allow backup deletion as well as creation. backupWriter is described in the docs as \"is intended to be used by scripts that automate backup creation\". \nhttps://cloud.google.com/spanner/docs/iam"
      },
      {
        "date": "2024-03-17T20:59:00.000Z",
        "voteCount": 1,
        "content": "We need an Admin for A, A reader for B and a Writer for C. Therefore A is the correct answer."
      },
      {
        "date": "2023-09-25T12:53:00.000Z",
        "voteCount": 1,
        "content": "A is the one. \nYou don't need the backupAdmin."
      },
      {
        "date": "2023-09-17T23:36:00.000Z",
        "voteCount": 2,
        "content": "Answer is A."
      },
      {
        "date": "2023-04-16T23:40:00.000Z",
        "voteCount": 2,
        "content": "It should be A as per the documentation. \nhttps://cloud.google.com/spanner/docs/iam#spanner.backupWriter"
      },
      {
        "date": "2023-03-02T21:31:00.000Z",
        "voteCount": 2,
        "content": "A is the best answer"
      },
      {
        "date": "2022-12-25T08:23:00.000Z",
        "voteCount": 3,
        "content": "A: roles/spanner.databaseAdmin for Person A\nroles/spanner.databaseReader for Person B\nroles/spanner.backupWriter for Application C"
      },
      {
        "date": "2022-12-24T08:54:00.000Z",
        "voteCount": 4,
        "content": "B and C are obviously wrong because application only needs backupWriter permissions.\nD is wrong because roles/spanner.databaseUser contains write permissions, and we don't need that."
      },
      {
        "date": "2022-12-23T19:47:00.000Z",
        "voteCount": 3,
        "content": "A is the correct answer.\nCloud Spanner Backup Writer\nThis role is intended to be used by scripts that automate backup creation. A principal with this role can create backups, but cannot update or delete them. Lowest-level resource"
      },
      {
        "date": "2022-12-22T09:31:00.000Z",
        "voteCount": 2,
        "content": "Correct answer - A"
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 12,
    "url": "https://www.examtopics.com/discussions/google/view/92474-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "You are designing an augmented reality game for iOS and Android devices. You plan to use Cloud Spanner as the primary backend database for game state storage and player authentication. You want to track in-game rewards that players unlock at every stage of the game. During the testing phase, you discovered that costs are much higher than anticipated, but the query response times are within the SLA. You want to follow Google-recommended practices. You need the database to be performant and highly available while you keep costs low. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tManually scale down the number of nodes after the peak period has passed.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse interleaving to co-locate parent and child rows.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Cloud Spanner query optimizer to determine the most efficient way to execute the SQL query.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse granular instance sizing in Cloud Spanner and Autoscaler.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 15,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-03-11T06:49:00.000Z",
        "voteCount": 7,
        "content": "D.\nA is nonsense. Using interleaved tables can help speed up queries, but the question says query response times are OK. So B is wrong. C is wrong for the same reason. That leaves D. The question is about which factors determine the cost of running Spanner. They include region vs. multi-region, compute unit (nodes or processing units), how much storage and how much backup space. From the Google docs, it says \u201cWhen you create a Cloud Spanner instance, you choose the number of compute capacity nodes or processing units to serve your data. However, if the workload of an instance changes, Cloud Spanner doesn't automatically adjust the size of the instance. This document introduces the Autoscaler tool for Cloud Spanner (Autoscaler), an open source tool that you can use as a companion tool to Cloud Spanner. This tool lets you automatically increase or reduce the number of nodes or processing units in one or more Spanner instances based on how their capacity is being used.\u201d\nhttps://cloud.google.com/spanner/docs/autoscaling-overview"
      },
      {
        "date": "2023-09-26T11:18:00.000Z",
        "voteCount": 1,
        "content": "It\u2019s D"
      },
      {
        "date": "2023-09-17T23:37:00.000Z",
        "voteCount": 3,
        "content": "Autoscaling is the way to go here. D."
      },
      {
        "date": "2023-06-26T01:30:00.000Z",
        "voteCount": 3,
        "content": "Granular instance is available in Public Preview. With this feature, you can run workloads on Spanner at as low as 1/10th the cost of regular instances, \nhttps://cloud.google.com/blog/products/databases/get-more-out-of-spanner-with-granular-instance-sizing"
      },
      {
        "date": "2023-03-26T05:59:00.000Z",
        "voteCount": 3,
        "content": "As the others say - use autoscaling to rightsize the cluster"
      },
      {
        "date": "2022-12-25T08:22:00.000Z",
        "voteCount": 4,
        "content": "D: Use granular instance sizing in Cloud Spanner and Autoscaler."
      },
      {
        "date": "2022-12-23T20:55:00.000Z",
        "voteCount": 4,
        "content": "D is the correct answer, https://cloud.google.com/architecture/autoscaling-cloud-spanner"
      },
      {
        "date": "2022-12-22T09:45:00.000Z",
        "voteCount": 2,
        "content": "Correct answer - D"
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 13,
    "url": "https://www.examtopics.com/discussions/google/view/92622-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "You recently launched a new product to the US market. You currently have two Bigtable clusters in one US region to serve all the traffic. Your marketing team is planning an immediate expansion to APAC. You need to roll out the regional expansion while implementing high availability according to Google-recommended practices. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMaintain a target of 23% CPU utilization by locating:<br>cluster-a in zone us-central1-a<br>cluster-b in zone europe-west1-d<br>cluster-c in zone asia-east1-b",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMaintain a target of 23% CPU utilization by locating:<br>cluster-a in zone us-central1-a<br>cluster-b in zone us-central1-b<br>cluster-c in zone us-east1-a",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMaintain a target of 35% CPU utilization by locating:<br>cluster-a in zone us-central1-a<br>cluster-b in zone australia-southeast1-a<br>cluster-c in zone europe-west1-d<br>cluster-d in zone asia-east1-b",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMaintain a target of 35% CPU utilization by locating:<br>cluster-a in zone us-central1-a<br>cluster-b in zone us-central2-a<br>cluster-c in zone asia-northeast1-b<br>cluster-d in zone asia-east1-b\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-03-11T09:33:00.000Z",
        "voteCount": 10,
        "content": "D.\nThe question HA for US and APAC. Any answer which mentions Europe must be wrong. That eliminates A and C. HA requires &gt; 1 cluster, so B must be wrong, leaving D. D shows 2 clusters in US and 2 in APAC."
      },
      {
        "date": "2024-05-09T07:12:00.000Z",
        "voteCount": 1,
        "content": "obviously D"
      },
      {
        "date": "2023-09-17T23:39:00.000Z",
        "voteCount": 1,
        "content": "D seems right."
      },
      {
        "date": "2023-07-30T05:46:00.000Z",
        "voteCount": 1,
        "content": "D is correct."
      },
      {
        "date": "2022-12-25T08:21:00.000Z",
        "voteCount": 2,
        "content": "D: Maintain a target of 35% CPU utilization by locating:\ncluster-a in zone us-central1-a\ncluster-b in zone us-central2-a\ncluster-c in zone asia-northeast1-b\ncluster-d in zone asia-east1-b"
      },
      {
        "date": "2022-12-24T09:48:00.000Z",
        "voteCount": 4,
        "content": "Forgot to vote"
      },
      {
        "date": "2022-12-24T09:48:00.000Z",
        "voteCount": 4,
        "content": "Looks like D to me - it's the only one with 2 US and 2 Asia regions. Also https://cloud.google.com/bigtable/docs/replication-settings#regional-failover"
      },
      {
        "date": "2022-12-23T20:58:00.000Z",
        "voteCount": 2,
        "content": "D is the correct answer"
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 14,
    "url": "https://www.examtopics.com/discussions/google/view/92020-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "Your ecommerce website captures user clickstream data to analyze customer traffic patterns in real time and support personalization features on your website. You plan to analyze this data using big data tools. You need a low-latency solution that can store 8 TB of data and can scale to millions of read and write requests per second. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite your data into Bigtable and use Dataproc and the Apache Hbase libraries for analysis.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy a Cloud SQL environment with read replicas for improved performance. Use Datastream to export data to Cloud Storage and analyze with Dataproc and the Cloud Storage connector.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Memorystore to handle your low-latency requirements and for real-time analytics.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStream your data into BigQuery and use Dataproc and the BigQuery Storage API to analyze large volumes of data."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 19,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 5,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-03-11T09:54:00.000Z",
        "voteCount": 6,
        "content": "A.\nCloud SQL could not handle the load, so B is wrong. Memorystore can scale up to 300 GB. The question mentions needing 8 TB, so C must be wrong. BigQuery could not handle the latency requirements of the question, which leaves A. Bigtable could handle the volume of writes at the speeds required."
      },
      {
        "date": "2022-12-25T08:21:00.000Z",
        "voteCount": 5,
        "content": "A: Write your data into Bigtable ***** and use Dataproc and the Apache Hbase libraries for analysis."
      },
      {
        "date": "2024-02-20T22:19:00.000Z",
        "voteCount": 3,
        "content": "Bigtable is ideal for clickstream and IOT use cases, also it can process high performance read and writes globally."
      },
      {
        "date": "2024-02-15T03:48:00.000Z",
        "voteCount": 1,
        "content": "This option uses BigQuery, that has a low latency and is a big data"
      },
      {
        "date": "2023-11-06T02:48:00.000Z",
        "voteCount": 4,
        "content": "A is not correct because Bigtable is not designed for real-time analytics. It is a good choice for storing and retrieving small amounts of data quickly, but it is not as efficient for analyzing large volumes of data.\nB is not correct because it cannot support Million of Read and Write\nC is not correct because of storage limitation\nD is correct"
      },
      {
        "date": "2023-09-17T23:41:00.000Z",
        "voteCount": 2,
        "content": "I would opt for A."
      },
      {
        "date": "2023-12-11T02:20:00.000Z",
        "voteCount": 1,
        "content": "Why opt A? It is not real-time, and the question mentions that they want to analysis, why not use bigquery, only 8 TB"
      },
      {
        "date": "2023-09-16T22:13:00.000Z",
        "voteCount": 2,
        "content": "The correct answer is D. Stream your data into BigQuery and use Dataproc and the BigQuery Storage API to analyze large volumes of data.. A is used for NOSQL"
      },
      {
        "date": "2023-06-26T01:59:00.000Z",
        "voteCount": 2,
        "content": "At a high level, Bigtable is a NoSQL wide-column database. It's optimized for low latency, large numbers of reads and writes, and maintaining performance at scale. Bigtable use cases are of a certain scale or throughput with strict latency requirements, such as IoT, AdTech, FinTech, and so on. If high throughput and low latency at scale are not priorities for you, then another NoSQL database like Firestore might be a better fit."
      },
      {
        "date": "2023-03-07T20:59:00.000Z",
        "voteCount": 1,
        "content": "B. Normalize the data model.\nD. Promote high-cardinality attributes in multi-attribute primary keys.\n\nWhen designing a schema for Cloud Spanner, it is important to follow best practices to avoid hotspots and ensure optimal performance. Hotspots occur when too many requests are targeted at a single node or group of nodes, causing them to become overloaded and potentially impacting performance."
      },
      {
        "date": "2023-03-07T21:00:00.000Z",
        "voteCount": 1,
        "content": "Normalization is a recommended best practice in database schema design, including in Cloud Spanner. It involves breaking down large tables into smaller, more manageable tables that are linked together by relationships. This can help reduce duplication of data and improve performance by reducing the amount of data that needs to be read or written to the database.\n\nPromoting high-cardinality attributes in multi-attribute primary keys is also recommended in Cloud Spanner schema design. High-cardinality attributes are those that have a large number of distinct values, such as product IDs or customer IDs. Including these attributes in the primary key can help distribute data more evenly across nodes, reducing the likelihood of hotspots.\n\nUsing an auto-incrementing value as the primary key or a bit-reverse sequential value as the primary key can result in hotspots, particularly if new data is being added at a high rate. These approaches can cause all new data to be inserted into a single node, leading to performance issues."
      },
      {
        "date": "2023-03-07T21:05:00.000Z",
        "voteCount": 1,
        "content": "The above answer is for Question #15, my mistake I put the comments here"
      },
      {
        "date": "2023-03-07T20:54:00.000Z",
        "voteCount": 1,
        "content": "D. Stream your data into BigQuery and use Dataproc and the BigQuery Storage API to analyze large volumes of data.\n\nBigQuery is a fully managed, serverless data warehouse that allows you to store and analyze large datasets using SQL-like queries. It is designed to handle petabyte-scale data and is optimized for fast query performance. By streaming your clickstream data into BigQuery, you can store and process large amounts of data in real-time.\n\nDataproc, on the other hand, is a fully-managed cloud service for running Apache Hadoop and Spark clusters. It provides a managed, easy-to-use environment for data processing, which can be used to analyze the data stored in BigQuery.\n\nThe BigQuery Storage API allows you to directly access data stored in BigQuery from external applications, including Dataproc, which enables you to run advanced analytics on large volumes of data with low latency.\n\nThis approach provides a scalable, low-latency solution for storing and analyzing large volumes of data, making it a good fit for your requirements."
      },
      {
        "date": "2023-03-05T11:56:00.000Z",
        "voteCount": 2,
        "content": "A. Write your data into Bigtable and use Dataproc and the Apache Hbase libraries for analysis. Most Voted"
      },
      {
        "date": "2023-03-03T13:04:00.000Z",
        "voteCount": 2,
        "content": "A looks like best option."
      },
      {
        "date": "2022-12-23T22:25:00.000Z",
        "voteCount": 3,
        "content": "A is correct answer, C wouldn't be handled 8TB data\nScalable: Start with the lowest tier and smallest size and then grow your instance as needed. Memorystore provides automated scaling using APIs, and optimized node placement across zones for redundancy. Memorystore for Memcached can support clusters as large as 5 TB, enabling millions of QPS at very low latency"
      },
      {
        "date": "2022-12-19T08:44:00.000Z",
        "voteCount": 5,
        "content": "Answer is A. Click stream and time series data and the size is 8TB. Read low latency with reads and writes. Correct answer is A to use BigTable for storage and use either CBT or Hbase API to interact with data."
      },
      {
        "date": "2022-12-19T08:00:00.000Z",
        "voteCount": 3,
        "content": "B couldn't handle this volume of writes and read, D wouldn't be able to handle the writing and C wouldn't be suited for this."
      },
      {
        "date": "2022-12-18T13:21:00.000Z",
        "voteCount": 2,
        "content": "must be A"
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 15,
    "url": "https://www.examtopics.com/discussions/google/view/92092-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "Your company uses Cloud Spanner for a mission-critical inventory management system that is globally available. You recently loaded stock keeping unit (SKU) and product catalog data from a company acquisition and observed hotspots in the Cloud Spanner database. You want to follow Google-recommended schema design practices to avoid performance degradation. What should you do? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an auto-incrementing value as the primary key.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNormalize the data model.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPromote low-cardinality attributes in multi-attribute primary keys.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPromote high-cardinality attributes in multi-attribute primary keys.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse bit-reverse sequential value as the primary key.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "DE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "DE",
        "count": 24,
        "isMostVoted": true
      },
      {
        "answer": "CE",
        "count": 6,
        "isMostVoted": false
      },
      {
        "answer": "BD",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "BE",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-02-28T09:23:00.000Z",
        "voteCount": 7,
        "content": "Spanner needs high cardinality primary key to avoid hotspotting."
      },
      {
        "date": "2024-01-19T04:52:00.000Z",
        "voteCount": 3,
        "content": "I would go with D and E"
      },
      {
        "date": "2023-10-01T19:23:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/spanner/docs/schema-design\n\nD because high cardinality means you have more unique values in the collumn. That's a good thing for a hot-spotting issue. \nE because Spanner specifically has this feature to reduce hot spotting. Basically, it generates unique values https://cloud.google.com/spanner/docs/schema-design#bit_reverse_primary_key"
      },
      {
        "date": "2023-07-28T22:32:00.000Z",
        "voteCount": 2,
        "content": "I agree with Hilab's comment below."
      },
      {
        "date": "2023-04-26T06:08:00.000Z",
        "voteCount": 4,
        "content": "D and E, the docs are below.\nD: https://cloud.google.com/bigtable/docs/schema-design#row-keys-avoid\nE: https://cloud.google.com/spanner/docs/schema-design#bit_reverse_primary_key"
      },
      {
        "date": "2023-04-25T15:29:00.000Z",
        "voteCount": 1,
        "content": "high-cardinality"
      },
      {
        "date": "2023-04-10T11:09:00.000Z",
        "voteCount": 3,
        "content": "Correct answers are D,E.\nRefer to the link which is self explanatory.\nhttps://cloud.google.com/spanner/docs/schema-design"
      },
      {
        "date": "2023-03-26T07:03:00.000Z",
        "voteCount": 3,
        "content": "A - incrementing values are an explicitly documented antipattern\nB - normalising the schema does not specifically address hotspotting\nC - low cardinality values in the primary key will also cause hotspotting\nD - promoting high cardinality values in the primary key (i.e. moving them nearer the front of the value) is a recommended approach to reduce hotspotting\nE - bit-reversed keys are an explicitly recommended best practice"
      },
      {
        "date": "2023-03-11T10:10:00.000Z",
        "voteCount": 2,
        "content": "D, E.\nA is wrong because that will promote hotspots. C is wrong because low cardinality attributes being part of the key (particularly at the front multi-attribute keys) will also promote hotspots. That makes D correct by definition. This leave B or D at the other correct answer. The fact the new data has already been added to the database suggests the data model is already properly normalized. In addition, one of the techniques to reduce or eliminate hotspots is to bit reverse sequential values. It\u2019s in Google\u2019s docs here:\nhttps://cloud.google.com/spanner/docs/schema-design"
      },
      {
        "date": "2023-03-07T21:05:00.000Z",
        "voteCount": 2,
        "content": "B. Normalize the data model.\nD. Promote high-cardinality attributes in multi-attribute primary keys.\n\nWhen designing a schema for Cloud Spanner, it is important to follow best practices to avoid hotspots and ensure optimal performance. Hotspots occur when too many requests are targeted at a single node or group of nodes, causing them to become overloaded and potentially impacting performance."
      },
      {
        "date": "2023-03-07T21:06:00.000Z",
        "voteCount": 2,
        "content": "Normalization is a recommended best practice in database schema design, including in Cloud Spanner. It involves breaking down large tables into smaller, more manageable tables that are linked together by relationships. This can help reduce duplication of data and improve performance by reducing the amount of data that needs to be read or written to the database.\n\nPromoting high-cardinality attributes in multi-attribute primary keys is also recommended in Cloud Spanner schema design. High-cardinality attributes are those that have a large number of distinct values, such as product IDs or customer IDs. Including these attributes in the primary key can help distribute data more evenly across nodes, reducing the likelihood of hotspots.\n\nUsing an auto-incrementing value as the primary key or a bit-reverse sequential value as the primary key can result in hotspots, particularly if new data is being added at a high rate. These approaches can cause all new data to be inserted into a single node, leading to performance issues."
      },
      {
        "date": "2023-03-04T12:41:00.000Z",
        "voteCount": 4,
        "content": "\"hotspots\" in a database means that many IOPS (usually writes/updates) are happening on the same data-block; usually due to calling the same DATA.  \nlow cardinality =&gt; same value in the column ==&gt; hotspots. \nHigh  cardinality  =&gt; different values in the column  ==&gt; avoiding hotspotting."
      },
      {
        "date": "2023-02-12T04:46:00.000Z",
        "voteCount": 3,
        "content": "CE\nA and D: WRONG. Anti-pattern\nSince the question specifically stated the hotspots cause by new SKUs and product catalog data added, so the goal would be:\n1. The old data keeps distributed without any extra work needed.\n2. Resolving the new data hot spots problem.\nIt seems to me that SKU and product catalog are already normalized, so further normalize might touch the old data. This means B is out. If the new data already normalized, then it must have some high-cardinality attributes, e.g. SKU_id, and some low-cardinality attributes, e.g. category_id. So I picked low-cardinality attibutes in multi-attribute primary keys as C. I agreed with E as already Google recommended practice.\nReference:\nhttps://cloud.google.com/spanner/docs/schema-design"
      },
      {
        "date": "2023-01-02T16:11:00.000Z",
        "voteCount": 1,
        "content": "CE. Normalizing the data is not generally recommended if interleaving can suffice."
      },
      {
        "date": "2022-12-25T08:20:00.000Z",
        "voteCount": 2,
        "content": "B: Normalize the data model.\n E: Use bit-reverse sequential value as the primary key."
      },
      {
        "date": "2023-02-18T20:48:00.000Z",
        "voteCount": 1,
        "content": "Wrong B"
      },
      {
        "date": "2022-12-23T23:08:00.000Z",
        "voteCount": 2,
        "content": "Looks  CE is correct  for me"
      },
      {
        "date": "2022-12-22T10:09:00.000Z",
        "voteCount": 1,
        "content": "Correct answer - CE"
      },
      {
        "date": "2022-12-19T08:51:00.000Z",
        "voteCount": 1,
        "content": "Answer is B &amp; E for schema design. https://cloud.google.com/spanner/docs/schema-design . B&amp;E are correct answers"
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 16,
    "url": "https://www.examtopics.com/discussions/google/view/92052-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "You are managing multiple applications connecting to a database on Cloud SQL for PostgreSQL. You need to be able to monitor database performance to easily identify applications with long-running and resource-intensive queries. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse log messages produced by Cloud SQL.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Query Insights for Cloud SQL.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Cloud Monitoring dashboard with available metrics from Cloud SQL.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud SQL instance monitoring in the Google Cloud Console."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 10,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-03-11T10:20:00.000Z",
        "voteCount": 6,
        "content": "B.\nQuery Insights helps identify performance and load issues at the database application layer. None of the other options do that. So the answer is B."
      },
      {
        "date": "2024-05-09T07:20:00.000Z",
        "voteCount": 1,
        "content": "B. Query Insights is built in feature to identify long running query"
      },
      {
        "date": "2023-03-05T12:09:00.000Z",
        "voteCount": 2,
        "content": "B. Query insights: for identifying long running SQLs https://cloud.google.com/sql/docs/mysql/using-query-insights#introduction"
      },
      {
        "date": "2023-03-04T12:59:00.000Z",
        "voteCount": 2,
        "content": "B. Query insights: for identifying long running SQLs  https://cloud.google.com/sql/docs/mysql/using-query-insights#introduction"
      },
      {
        "date": "2023-02-11T00:38:00.000Z",
        "voteCount": 3,
        "content": "Answer should be B: As long-running query and resource intensive queries are available in query insight providing details about which queries are taking how much time and resource utilization at what stage."
      },
      {
        "date": "2022-12-26T10:27:00.000Z",
        "voteCount": 4,
        "content": "Vote for B"
      },
      {
        "date": "2022-12-25T13:47:00.000Z",
        "voteCount": 2,
        "content": "Option B\n\nhttps://cloud.google.com/sql/docs/postgres/using-query-insights"
      },
      {
        "date": "2022-12-25T08:19:00.000Z",
        "voteCount": 1,
        "content": "B: Use Query Insights for Cloud SQL.\n\nging multiple applications connecting"
      },
      {
        "date": "2022-12-23T23:14:00.000Z",
        "voteCount": 1,
        "content": "B is the correct answer, agree with jitu028"
      },
      {
        "date": "2022-12-22T10:12:00.000Z",
        "voteCount": 2,
        "content": "Correct answer - B\nhttps://www.youtube.com/watch?v=qN7x3ngwz1o"
      },
      {
        "date": "2022-12-21T06:02:00.000Z",
        "voteCount": 1,
        "content": "For Database performance, Cloud SQL System insights dashboard is preferable"
      },
      {
        "date": "2022-12-19T09:13:00.000Z",
        "voteCount": 1,
        "content": "C is correct answer. We can use Cloud SQL Instance monitoring as well, but need to build the custom metrics for the metrics needed. Instead with Cloud Monitoring these metrics are already available.  https://cloud.google.com/sql/docs/mysql/monitor-instance#cloud-monitoring https://cloud.google.com/sql/docs/sqlserver/admin-api/metrics"
      },
      {
        "date": "2022-12-19T01:19:00.000Z",
        "voteCount": 1,
        "content": "The Cloud SQL System insights dashboard helps you detect and analyze system performance problems.\n.\nhttps://cloud.google.com/sql/docs/postgres/monitor-instance#sql-system-insights"
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 17,
    "url": "https://www.examtopics.com/discussions/google/view/92212-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "You are building an application that allows users to customize their website and mobile experiences. The application will capture user information and preferences. User profiles have a dynamic schema, and users can add or delete information from their profile. You need to ensure that user changes automatically trigger updates to your downstream BigQuery data warehouse. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore your data in Bigtable, and use the user identifier as the key. Use one column family to store user profile data, and use another column family to store user preferences.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud SQL, and create different tables for user profile data and user preferences from your recommendations model. Use SQL to join the user profile data and preferences",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Firestore in Native mode, and store user profile data as a document. Update the user profile with preferences specific to that user and use the user identifier to query.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Firestore in Datastore mode, and store user profile data as a document. Update the user profile with preferences specific to that user and use the user identifier to query."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-03-26T07:33:00.000Z",
        "voteCount": 5,
        "content": "Dynamic schema indicates this is a NoSQL solution (ruling out Cloud SQL) and the application use case specifically suits Firestore (the question even refers to storing data in documents) as opposed to BigTable.\n\nFirestore in Native supports realtime client updates, which is needed for the analytics requirement: https://cloud.google.com/firestore/docs/firestore-or-datastore#feature_comparison"
      },
      {
        "date": "2024-01-16T16:07:00.000Z",
        "voteCount": 1,
        "content": "Firestore in native mode and Bigtable both seems to be correct answer but weightage can be given to Bigtable on the basis of replication to BigQuery via change streams is easier."
      },
      {
        "date": "2023-04-29T08:36:00.000Z",
        "voteCount": 2,
        "content": "Use Firestore in Datastore mode for new server projects.\n\nFirestore in Datastore mode allows you to use established Datastore server architectures while removing fundamental Datastore limitations. Datastore mode can automatically scale to millions of writes per second.\n\nUse Firestore in Native mode for new mobile and web apps.\n\nFirestore offers mobile and web client libraries with real-time and offline features. Native mode can automatically scale to millions of concurrent clients."
      },
      {
        "date": "2023-03-11T10:38:00.000Z",
        "voteCount": 4,
        "content": "C.\nA dynamic schema means the database backend cannot be relational. That eliminates B. No criteria is mentioned that would justify Bigtable (low latency or massive data volume), so eliminate A. That leaves Firestore options which make sense since it\u2019s a NoSQL database. Since \u201cwebsite\u201d and \u201cmobile\u201d are both mentioned in the question, Firestore in Native mode must be the correct answer."
      },
      {
        "date": "2022-12-25T08:18:00.000Z",
        "voteCount": 2,
        "content": "C: Use Firestore in Native mode, and store user profile data as a document. Update the user profile with preferences specific to that user and use the user identifier to query."
      },
      {
        "date": "2022-12-23T23:31:00.000Z",
        "voteCount": 2,
        "content": "C is the correct Answer"
      },
      {
        "date": "2022-12-21T06:04:00.000Z",
        "voteCount": 3,
        "content": "Firestore introduces new features such as:\n\nA new, strongly consistent storage layer\nA collection and document data model\nReal-time updates\nMobile and Web client libraries\n.\nhttps://cloud.google.com/datastore/docs/firestore-or-datastore#in_native_mode"
      },
      {
        "date": "2022-12-20T12:02:00.000Z",
        "voteCount": 2,
        "content": "Seems like a better use for firestore and since it needs to reflect changes downstream in real time the native one would be better."
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 18,
    "url": "https://www.examtopics.com/discussions/google/view/92213-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "Your application uses Cloud SQL for MySQL. Your users run reports on data that relies on near-real time; however, the additional analytics caused excessive load on the primary database. You created a read replica for the analytics workloads, but now your users are complaining about the lag in data changes and that their reports are still slow. You need to improve the report performance and shorten the lag in data replication without making changes to the current reports. Which two approaches should you implement? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate secondary indexes on the replica.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate additional read replicas, and partition your analytics users to use different read replicas.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDisable replication on the read replica, and set the flag for parallel replication on the read replica. Re-enable replication and optimize performance by setting flags on the primary instance.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDisable replication on the primary instance, and set the flag for parallel replication on the primary instance. Re-enable replication and optimize performance by setting flags on the read replica.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMove your analytics workloads to BigQuery, and set up a streaming pipeline to move data and update BigQuery."
    ],
    "answer": "BC",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BC",
        "count": 12,
        "isMostVoted": true
      },
      {
        "answer": "AC",
        "count": 4,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "DE",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "CE",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-01-04T14:04:00.000Z",
        "voteCount": 7,
        "content": "Vote for AC\nA https://cloud.google.com/sql/docs/mysql/replication/read-replica-indexes  increase performance on read operation\nC https://cloud.google.com/sql/docs/mysql/replication/manage-replicas#basic-steps-to-change-parallel-replication-flags"
      },
      {
        "date": "2023-03-11T11:59:00.000Z",
        "voteCount": 6,
        "content": "B, C.\nYou have 2 problems. Replication lag and slow report performance. E is eliminated because using BigQuery would mean changes to the current reports. Report slowness could be the result of poor indexing or just too much read load (or both!). Since excessive load is mentioned in the question, creating additional read replicas and spreading the analytics workload around makes B correct and eliminates A as a way to speed up reporting. That leaves the replication problem. Cloud SQL enables single threaded replication by default, so it stands to reason enabling parallel replication would help the lag. To do that you disable replication on the replica (not the primary), set flags on the replica and optionally set flags on the primary instance to optimize performance for parallel replication. That makes C correct and D incorrect.\nhttps://cloud.google.com/sql/docs/mysql/replication/manage-replicas#configuring-parallel-replication"
      },
      {
        "date": "2023-09-15T11:10:00.000Z",
        "voteCount": 1,
        "content": "B isn't correct ==&gt; \"without making changes to the current reports\". If you choose B, reports will need changes to point to the new instances."
      },
      {
        "date": "2024-07-09T01:01:00.000Z",
        "voteCount": 1,
        "content": "The two main issues at hand are replication lag and slow report performance. While option B, creating additional read replicas, could address the load issue, it would require changes to the current reports, which is not desirable. Instead, option A, implementing secondary indexes in Cloud SQL, could enhance report speed without altering the reports. As for the replication lag, option C suggests enabling parallel replication in Cloud SQL, which is a plausible solution. To do that you disable replication on the replica (not the primary), set flags on the replica and optionally set flags on the primary instance to optimize performance for parallel replication. Therefore, considering the constraints and the context, options A and C are the most suitable choices."
      },
      {
        "date": "2024-06-04T12:36:00.000Z",
        "voteCount": 1,
        "content": "B - Addresses performance improvements: Reduce the burden on the primary instance by offloading replication work to multiple read replicas.\nhttps://cloud.google.com/sql/docs/mysql/replication#cascading-replicas\nC- Addresses lag time because Parallel replication reduces replication lag by increasing the number of SQL threads that work to execute these transactions.\nhttps://cloud.google.com/sql/docs/mysql/replication/manage-replicas#configuring-parallel-replication"
      },
      {
        "date": "2024-02-12T08:35:00.000Z",
        "voteCount": 3,
        "content": "Just got this question and there is no A, and it is not 'choose two' but one answer only."
      },
      {
        "date": "2023-09-03T13:59:00.000Z",
        "voteCount": 4,
        "content": "There's no discussion about C.\nA &amp; B both sounds reasonable. Why I would choose A instead of B? Due to keywords \"without making changes to the current reports\" and \"MySQL\". Option B would require to point to new IP addresses (the new read replicas) and split which group of users which run X reports and which group of user which run Y reports connect to which read replica. Option A (secondary indexes) is only available for Cloud SQL (the question's use case is about MySQL) and explicitly mentions \"for reporting purposes\"."
      },
      {
        "date": "2023-05-26T07:44:00.000Z",
        "voteCount": 3,
        "content": "B. By creating additional read replicas, you can distribute the load of analytics workloads across multiple instances. Partitioning your analytics users to use different read replicas allows you to further distribute the workload and improve performance. This helps to alleviate the excessive load on the primary database and enhances the reporting experience for users.\nC. Disabling replication on the read replica can help reduce the data replication lag. By setting the flag for parallel replication on the read replica, you allow parallel execution of replication threads, which can expedite data replication. Additionally, optimizing performance by setting flags on the primary instance can help improve the overall performance of the replication process and reduce the lag experienced by the read replica."
      },
      {
        "date": "2023-09-15T11:10:00.000Z",
        "voteCount": 1,
        "content": "B isn't correct ==&gt; \"without making changes to the current reports\". If you choose B, reports will need changes to point to the new instances."
      },
      {
        "date": "2023-03-07T21:38:00.000Z",
        "voteCount": 1,
        "content": "B. Create additional read replicas, and partition your analytics users to use different read replicas.\nD. Disable replication on the primary instance, and set the flag for parallel replication on the primary instance. Re-enable replication and optimize performance by setting flags on the read replica.\n\nCreating additional read replicas can distribute the analytics workload and reduce the lag in data replication. By partitioning your analytics users to use different read replicas, you can further reduce the load on each replica and improve performance."
      },
      {
        "date": "2023-03-07T21:38:00.000Z",
        "voteCount": 1,
        "content": "Disabling replication on the primary instance and setting the flag for parallel replication can improve the replication speed and reduce the lag in data replication. Once you have optimized performance on the primary instance, you can re-enable replication and optimize performance on the read replica.\n\nCreating secondary indexes on the replica may improve query performance but will not reduce the lag in data replication. Moving your analytics workloads to BigQuery and setting up a streaming pipeline to move data can provide near-real-time data but will require significant changes to your current reports."
      },
      {
        "date": "2023-03-05T12:19:00.000Z",
        "voteCount": 1,
        "content": "B. Create additional read replicas, and partition your analytics users to use different read replicas. Most Voted\nC. Disable replication on the read replica, and set the flag for parallel replication on the read replica. Re-enable replication and optimize performance by setting flags on the primary instance."
      },
      {
        "date": "2023-09-15T11:10:00.000Z",
        "voteCount": 1,
        "content": "B isn't correct ==&gt; \"without making changes to the current reports\". If you choose B, reports will need changes to point to the new instances."
      },
      {
        "date": "2023-03-04T13:18:00.000Z",
        "voteCount": 2,
        "content": "A. Create secondary indexes on the replica. - No indication that the reports will benefit from indexes. \nB. Create additional read replicas, and partition your analytics users to use different read replicas. --&gt; might rebalance the load.  \nC. Disable replication on the read replica, and set the flag for parallel replication on the read replica. Re-enable replication and optimize performance by setting flags on the primary instance. --&gt; might add parallelism to the replication lag.  \nD. Disable replication on the primary instance, and set the flag for parallel replication on the primary instance. Re-enable replication and optimize performance by setting flags on the read replica. --&gt; na\nE. Move your analytics workloads to BigQuery, and set up a streaming pipeline to move data and update BigQuery.--&gt; according to question statement , no SQL rewrite is possible."
      },
      {
        "date": "2023-02-19T20:52:00.000Z",
        "voteCount": 2,
        "content": "A &amp; C looks fine."
      },
      {
        "date": "2023-01-26T07:26:00.000Z",
        "voteCount": 3,
        "content": "Ans is AC"
      },
      {
        "date": "2023-01-25T22:20:00.000Z",
        "voteCount": 1,
        "content": "Creating secondary indexes on the replica can help improve the performance of the reports by allowing the read replica to quickly locate the data it needs without having to scan the entire table. This can help speed up the queries"
      },
      {
        "date": "2023-01-25T11:19:00.000Z",
        "voteCount": 1,
        "content": "Ans is AC"
      },
      {
        "date": "2023-01-10T21:32:00.000Z",
        "voteCount": 4,
        "content": "The question has 2 issues - replication lag and reports running slow.\nB - will address reports running snow since fewer uses will be on the replica server\nC - will address replication lag."
      },
      {
        "date": "2023-01-24T12:03:00.000Z",
        "voteCount": 1,
        "content": "AC must correct choice in the situation."
      },
      {
        "date": "2023-01-02T16:29:00.000Z",
        "voteCount": 1,
        "content": "Moving workload to BQ is not an option. That, at a minimum, would require connection changes in the reports and the question specifically states that report changes are unacceptable. Aside from that, we do not know if the reports are being generated by a tool and whether that tool supports BQ."
      },
      {
        "date": "2022-12-29T17:51:00.000Z",
        "voteCount": 2,
        "content": "AC - Not understanding votes for E, can't be done without some changes to reports."
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 19,
    "url": "https://www.examtopics.com/discussions/google/view/92340-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "You are evaluating Cloud SQL for PostgreSQL as a possible destination for your on-premises PostgreSQL instances. Geography is becoming increasingly relevant to customer privacy worldwide. Your solution must support data residency requirements and include a strategy to: configure where data is stored control where the encryption keys are stored govern the access to data<br>What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReplicate Cloud SQL databases across different zones.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Cloud SQL for PostgreSQL instance on Google Cloud for the data that does not need to adhere to data residency requirements. Keep the data that must adhere to data residency requirements on-premises. Make application changes to support both databases.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAllow application access to data only if the users are in the same region as the Google Cloud region for the Cloud SQL for PostgreSQL database.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse features like customer-managed encryption keys (CMEK), VPC Service Controls, and Identity and Access Management (IAM) policies.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-10-01T20:16:00.000Z",
        "voteCount": 3,
        "content": "D because \nCMEK where the encryption keys are stored \nIAM govern the access to data\nVPC Service Controls configure where data is stored control"
      },
      {
        "date": "2023-09-25T13:04:00.000Z",
        "voteCount": 1,
        "content": "D. \nC might seem ok, but you'd need some kind of tracking to localize users, and there is no mention of it."
      },
      {
        "date": "2023-05-25T13:02:00.000Z",
        "voteCount": 1,
        "content": "should be CSEK and not CMEK. Then 'D'."
      },
      {
        "date": "2023-03-11T12:10:00.000Z",
        "voteCount": 1,
        "content": "D.\nUsing IAM policies, VPC Service Controls and CMEK is the best answer. A doesn\u2019t make sense since Geography would be a factor at the Region level, not zone level. B is a lot of work and GCP is all about making things easier. C address part of the issue, but D addresses more. The link provided by sp57 is spot on."
      },
      {
        "date": "2023-01-05T04:16:00.000Z",
        "voteCount": 1,
        "content": "C - it is about location of the data"
      },
      {
        "date": "2023-01-04T14:05:00.000Z",
        "voteCount": 1,
        "content": "My vote D"
      },
      {
        "date": "2022-12-29T18:06:00.000Z",
        "voteCount": 3,
        "content": "D, https://cloud.google.com/blog/products/identity-security/meet-data-residency-requirements-with-google-cloud"
      },
      {
        "date": "2022-12-25T08:26:00.000Z",
        "voteCount": 2,
        "content": "D. Use features like customer-managed encryption keys (CMEK), VPC Service Controls, and Identity and Access Management (IAM) policies."
      },
      {
        "date": "2022-12-24T00:00:00.000Z",
        "voteCount": 1,
        "content": "D is the correct answer"
      },
      {
        "date": "2022-12-21T06:13:00.000Z",
        "voteCount": 2,
        "content": "data residency requirements can be achiy with CMEK, VPC and IAM"
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 20,
    "url": "https://www.examtopics.com/discussions/google/view/92343-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "Your customer is running a MySQL database on-premises with read replicas. The nightly incremental backups are expensive and add maintenance overhead. You want to follow Google-recommended practices to migrate the database to Google Cloud, and you need to ensure minimal downtime. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Google Kubernetes Engine (GKE) cluster, install MySQL on the cluster, and then import the dump file.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the mysqldump utility to take a backup of the existing on-premises database, and then import it into Cloud SQL.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Compute Engine VM, install MySQL on the VM, and then import the dump file.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an external replica, and use Cloud SQL to synchronize the data to the replica.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 15,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-03-11T12:42:00.000Z",
        "voteCount": 7,
        "content": "D.\nThe question says backups and maintenance are an issue, so moving to a managed service (Cloud SQL) would be the right thing to do. That eliminates C and A. Option B could (depending upon the DB size) require a lot of downtime to export, copy the dump file to Cloud Storage, then import into Cloud SQL. Therefore, the least amount of downtime would be D.\nhttps://cloud.google.com/sql/docs/mysql/replication/configure-replication-from-external"
      },
      {
        "date": "2024-01-18T12:48:00.000Z",
        "voteCount": 3,
        "content": "The only issue that I have with the D answer is that an external replica is an explicit Cloud SQL concept not a MySQL concept; external replica is when you have a primary instance already in Cloud SQL, however, if you have your MySQL on premises then you are dealing with replication from an external server. I have read documentation for CloudSQL for MySQL before and I am more convinced with B to be frank"
      },
      {
        "date": "2023-09-10T08:13:00.000Z",
        "voteCount": 2,
        "content": "External replica is a setup with primary on cloud an the replica external to the cloud .eg.e on prem.. which is the reverse of what the question is looking for"
      },
      {
        "date": "2023-12-11T02:48:00.000Z",
        "voteCount": 1,
        "content": "Yep, D sounds wired, the question said it should import from the on-premise. So my opinion is B"
      },
      {
        "date": "2023-05-26T07:50:00.000Z",
        "voteCount": 2,
        "content": "This approach provides a seamless migration process with minimal impact on the application's availability."
      },
      {
        "date": "2023-03-26T07:55:00.000Z",
        "voteCount": 2,
        "content": "Another good explanation from dynamic_dba."
      },
      {
        "date": "2023-03-23T15:35:00.000Z",
        "voteCount": 2,
        "content": "I agree with D"
      },
      {
        "date": "2023-03-05T12:23:00.000Z",
        "voteCount": 3,
        "content": "D is the better option"
      },
      {
        "date": "2023-03-04T13:37:00.000Z",
        "voteCount": 4,
        "content": "minimal downtime!!!  And the customer has read replicas + backups are expensive. \nD is the better option"
      },
      {
        "date": "2023-02-28T19:57:00.000Z",
        "voteCount": 2,
        "content": "D : External read replica will help achive minimal down time."
      },
      {
        "date": "2023-02-11T01:16:00.000Z",
        "voteCount": 4,
        "content": "Correct answer is D: Create cloudsql replica of on-prem server and promote with almost-no downtime by pointing app to cloudsql. mysqldump is heavy and time-consuming operation (even though if you run it on on-prem read-replica, it will be identical to migrating to cloudsql using managed database migration service."
      },
      {
        "date": "2023-01-06T01:42:00.000Z",
        "voteCount": 3,
        "content": "D create a read replica on cloud then promote it"
      },
      {
        "date": "2022-12-24T00:06:00.000Z",
        "voteCount": 2,
        "content": "B is correct answer"
      },
      {
        "date": "2022-12-21T06:22:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/database-migration/docs/mysql/mysql-dump"
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 21,
    "url": "https://www.examtopics.com/discussions/google/view/92127-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "Your team uses thousands of connected IoT devices to collect device maintenance data for your oil and gas customers in real time. You want to design inspection routines, device repair, and replacement schedules based on insights gathered from the data produced by these devices. You need a managed solution that is highly scalable, supports a multi-cloud strategy, and offers low latency for these IoT devices. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Firestore with Looker.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Spanner with Data Studio.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse MongoD8 Atlas with Charts.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Bigtable with Looker."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 19,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 17,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-05-26T07:52:00.000Z",
        "voteCount": 7,
        "content": "By combining Google Cloud IoT Core for device management and data ingestion, Google Cloud Bigtable for storing and processing IoT data with low latency, and Looker for advanced analytics and visualization, you can build a highly scalable, multi-cloud compatible, and low-latency solution to address your IoT device maintenance requirements effectively."
      },
      {
        "date": "2023-03-11T12:54:00.000Z",
        "voteCount": 5,
        "content": "D.\nThe question says a managed solution, so that eliminates C. Firestore and Spanner do not have the scalability or low latency required. This leaves D. Bigtable by itself is a GCP thing, but Looker allows data visualization across multiple cloud environments. \nhttps://www.looker.com/google-cloud/"
      },
      {
        "date": "2024-05-19T00:54:00.000Z",
        "voteCount": 1,
        "content": "Agree with D as You need a managed solution"
      },
      {
        "date": "2024-04-29T05:22:00.000Z",
        "voteCount": 1,
        "content": "Bigtable can't be integrated with looker"
      },
      {
        "date": "2024-04-15T10:30:00.000Z",
        "voteCount": 1,
        "content": "C . As its says to avoid refactoring."
      },
      {
        "date": "2024-04-04T00:44:00.000Z",
        "voteCount": 1,
        "content": "Google will only pitch its own products"
      },
      {
        "date": "2024-03-28T04:21:00.000Z",
        "voteCount": 1,
        "content": "multi-cloud"
      },
      {
        "date": "2024-02-22T02:13:00.000Z",
        "voteCount": 1,
        "content": "I think Looker is multi-cloud as some of the colleagues said in this post. So I choose D.\n\nhttps://services.google.com/fh/files/misc/042420-ppm-multi-cloud-one-sheet-8-5x11-en-web-gc.pdf"
      },
      {
        "date": "2024-02-22T02:13:00.000Z",
        "voteCount": 2,
        "content": "I think Looker is multi-cloud as some of the colleagues said in this post. So I choose D.\n\nhttps://services.google.com/fh/files/misc/042420-ppm-multi-cloud-one-sheet-8-5x11-en-web-gc.pdf"
      },
      {
        "date": "2024-01-30T04:45:00.000Z",
        "voteCount": 1,
        "content": "As others said, MongoDB is the only one that support multi-cloud (otherwise D is also a candidate)."
      },
      {
        "date": "2024-04-25T10:12:00.000Z",
        "voteCount": 1,
        "content": "BigTable is nothing but hbase."
      },
      {
        "date": "2023-11-02T20:51:00.000Z",
        "voteCount": 4,
        "content": "The answer is D. Use Bigtable with Looker.\n\nBigtable is a fully managed, petabyte-scale NoSQL wide column database service for storing large amounts of data. It is designed for low latency, high throughput, and scalability. Bigtable is a good choice for storing IoT data because it can handle the high volume of data generated by IoT devices and provide low latency for real-time analysis.\n\nLooker is a business intelligence and data analytics platform that can be used to visualize and analyze data stored in Bigtable. Looker provides a variety of features that can be used to design inspection routines, device repair, and replacement schedules based on insights gathered from the data produced by IoT devices. Looker supports hosting on public clouds like AWS and GCP, and in multi-cloud and hybrid environments."
      },
      {
        "date": "2023-09-25T13:05:00.000Z",
        "voteCount": 4,
        "content": "\"Multi-cloud\" is the key that rules out everything except C."
      },
      {
        "date": "2023-07-10T05:33:00.000Z",
        "voteCount": 3,
        "content": "Every thing in this question points to D except for one scenario \"multi-cloud\". So just becuase multi cloud is specified, I choose 'C'"
      },
      {
        "date": "2023-06-14T02:38:00.000Z",
        "voteCount": 1,
        "content": "for everyone saying managed solution is the key, MongoDb is also a managed solution"
      },
      {
        "date": "2023-05-03T15:14:00.000Z",
        "voteCount": 2,
        "content": "Bigtable for IOT stuff"
      },
      {
        "date": "2023-04-26T07:03:00.000Z",
        "voteCount": 2,
        "content": "C. Supports Multi-cloud strategy and managed solution. This makes MongoDB Atlas the only option."
      },
      {
        "date": "2023-03-26T08:33:00.000Z",
        "voteCount": 3,
        "content": "This is a curious question!\n\nThis scenario has BigTable written all over it - large amounts of data from many devices to be analysed in realtime. I would even argue it could qualify as a multicloud solution, given the links to HBASE.\n\nBUT it does not support SQL queries and is not therefore compatible (on its own) with Looker.\n\nFirestore + Looker has the same problem.\n\nSpanner + Data Studio is at least a compatible pairing, but I agree with others that it doesn't fit this use-case - not least because it's Google-native.\n\nBy contrast, MongoDB Atlas is a managed solution (just not by Google) which is compatible with the proposed reporting tool (Mongo's own Charts), it's specifically designed for this type of solution and of course it can run on any cloud.\n\nTherefore the only possible answer is C, even though it isn't a Google product!"
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 22,
    "url": "https://www.examtopics.com/discussions/google/view/92128-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "Your application follows a microservices architecture and uses a single large Cloud SQL instance, which is starting to have performance issues as your application grows. in the Cloud Monitoring dashboard, the CPU utilization looks normal You want to follow Google-recommended practices to resolve and prevent these performance issues while avoiding any major refactoring. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Spanner instead of Cloud SQL.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the number of CPUs for your instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the storage size for the instance.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse many smaller Cloud SQL instances."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 8,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 6,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-19T13:06:00.000Z",
        "voteCount": 8,
        "content": "Correct answer is D. https://cloud.google.com/sql/docs/mysql/best-practices#data-arch - Split your large instances into smaller instances, where possible."
      },
      {
        "date": "2024-01-25T08:06:00.000Z",
        "voteCount": 7,
        "content": "The solution D is better if we can execute a massive refactor. So, the best solution is C because if CPU is normal, the issue is the capacity of the I/O. For increase it, it's enough increase the disk storage. \n\nSource: https://cloud.google.com/sql/docs/sqlserver/best-practices"
      },
      {
        "date": "2024-05-25T08:21:00.000Z",
        "voteCount": 1,
        "content": "Agree with C as the CPU Looks normal."
      },
      {
        "date": "2023-12-25T13:48:00.000Z",
        "voteCount": 2,
        "content": "Splitting smaller databases require major effort.\nAnswer should be C"
      },
      {
        "date": "2023-10-02T06:15:00.000Z",
        "voteCount": 2,
        "content": "It's a microservices architecture and CPU utilization is normal. This means that having multiple Cloud SQL instances will help for each microservice."
      },
      {
        "date": "2023-09-25T13:06:00.000Z",
        "voteCount": 2,
        "content": "Wouldn't D be a big refactoring, as well as switching to Spanner, especially if MySQL is considered?\nThis question is bad."
      },
      {
        "date": "2023-03-11T13:15:00.000Z",
        "voteCount": 5,
        "content": "D.\nNeeding to avoid any major refactoring eliminates A. The question states CPU is not an issue, so that eliminates B. Adding more storage would increase IOPS, but there\u2019s no indication network throughput is an issue, so that eliminates C. That leaves D. A microservice architecture is supposed to use a separate database for each microservice, rather than one big database for all the microservices. So D it is. The link provided by Kloudgeek is spot on."
      },
      {
        "date": "2023-07-10T05:37:00.000Z",
        "voteCount": 1,
        "content": "Will splitting single instance to multiple smaller instance amount to re-factoring ?"
      },
      {
        "date": "2023-03-05T12:31:00.000Z",
        "voteCount": 1,
        "content": "D: Use many smaller Cloud SQL instances."
      },
      {
        "date": "2023-03-05T12:31:00.000Z",
        "voteCount": 2,
        "content": "D: Use many smaller  Cloud SQL instances."
      },
      {
        "date": "2022-12-25T08:14:00.000Z",
        "voteCount": 3,
        "content": "D: Use many smaller ***** Cloud SQL instances."
      },
      {
        "date": "2022-12-24T00:19:00.000Z",
        "voteCount": 1,
        "content": "D is the correct answer"
      },
      {
        "date": "2022-12-21T06:25:00.000Z",
        "voteCount": 2,
        "content": "Split CloudSql instance into many small instances to support Microservices"
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 23,
    "url": "https://www.examtopics.com/discussions/google/view/92624-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "You need to perform a one-time migration of data from a running Cloud SQL for MySQL instance in the us-central1 region to a new Cloud SQL for MySQL instance in the us-east1 region. You want to follow Google-recommended practices to minimize performance impact on the currently running instance. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate and run a Dataflow job that uses JdbcIO to copy data from one Cloud SQL instance to another.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate two Datastream connection profiles, and use them to create a stream from one Cloud SQL instance to another.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a SQL dump file in Cloud Storage using a temporary instance, and then use that file to import into a new instance.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a CSV file by running the SQL statement SELECT...INTO OUTFILE, copy the file to a Cloud Storage bucket, and import it into a new instance."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-19T05:07:00.000Z",
        "voteCount": 1,
        "content": "C is simple and works"
      },
      {
        "date": "2023-03-11T13:22:00.000Z",
        "voteCount": 3,
        "content": "C.\nThe only way to minimize performance impact of running an export on a Cloud SQL instance is to use a serverless export. The fact that no data synchronization is needed since it\u2019s a one off eliminates every option apart from C."
      },
      {
        "date": "2023-03-04T14:07:00.000Z",
        "voteCount": 1,
        "content": "C looks simple and ok."
      },
      {
        "date": "2022-12-27T01:28:00.000Z",
        "voteCount": 1,
        "content": "C - serverless export https://cloud.google.com/sql/docs/mysql/import-export#serverless"
      },
      {
        "date": "2022-12-25T08:13:00.000Z",
        "voteCount": 1,
        "content": "C: Create a SQL dump file in Cloud Storage using a temporary instance, and then use that file to import into a new instance."
      },
      {
        "date": "2022-12-24T00:23:00.000Z",
        "voteCount": 1,
        "content": "C is the correct answer"
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 24,
    "url": "https://www.examtopics.com/discussions/google/view/92625-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "You are running a mission-critical application on a Cloud SQL for PostgreSQL database with a multi-zonal setup. The primary and read replica instances are in the same region but in different zones. You need to ensure that you split the application load between both instances. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Load Balancing for load balancing between the Cloud SQL primary and read replica instances.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse PgBouncer to set up database connection pooling between the Cloud SQL primary and read replica instances.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse HTTP(S) Load Balancing for database connection pooling between the Cloud SQL primary and read replica instances.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Cloud SQL Auth proxy for database connection pooling between the Cloud SQL primary and read replica instances."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 12,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 9,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-03-11T13:48:00.000Z",
        "voteCount": 9,
        "content": "A.\nEliminate D because Cloud SQL Auth Proxy by itself does not provide connection pooling. There\u2019s nothing in the question about needing to load balance HTTP traffic specifically, so ignore C. B is eliminated on the basis Pgbouncer does not have multi-host configuration, failover, or detection and the question specifically says \u201cmission critical\u201d. That leaves A which makes sense since Google Cloud Load Balancer is a regional service and the question specifically mentions a single region."
      },
      {
        "date": "2024-02-11T14:15:00.000Z",
        "voteCount": 2,
        "content": "This doesn't seem correct:\n\nhttps://www.googlecloudcommunity.com/gc/Databases/Load-Balancer-for-Postgres/m-p/647563#:~:text=To%20clarify%2C%20Google%20Cloud%20Platform%27s,in%20Cloud%20SQL%20for%20PostgreSQL."
      },
      {
        "date": "2024-06-04T13:35:00.000Z",
        "voteCount": 2,
        "content": "Cloud SQL doesn't provide load balancing between replicas. You can choose to implement load balancing for your Cloud SQL instance. You can also use connection pooling to distribute queries across replicas with your load balancing setup for better performance.\n\nhttps://cloud.google.com/sql/docs/postgres/replication#:~:text=Cloud%20SQL%20doesn't%20provide,balancing%20setup%20for%20better%20performance.&amp;text=You%20can't%20configure%20maintenance%20windows%20on%20a%20read%20replica."
      },
      {
        "date": "2024-03-23T20:09:00.000Z",
        "voteCount": 1,
        "content": "Should be A. Pgbouncer needs a load balancer or DNS roundrobin in front of it to operate. It can't route the traffic to multiple hosts without it. C, D are wrong."
      },
      {
        "date": "2024-02-21T02:43:00.000Z",
        "voteCount": 1,
        "content": "PgBouncer is especially used to manage connection pools to the PostgreSQL database."
      },
      {
        "date": "2024-01-30T04:56:00.000Z",
        "voteCount": 2,
        "content": "Cloud Load Balancing can't be used to LB cloud SQL (it's mostly for VM), so you can choose to use HAProxy or PGBouncer as  Google recommend connection pooling (https://cloud.google.com/sql/docs/postgres/replication#rr-info)"
      },
      {
        "date": "2023-12-21T02:56:00.000Z",
        "voteCount": 1,
        "content": "By using PgBouncer, you can configure it to distribute the application load between the Cloud SQL primary and read replica instances. PgBouncer will handle connection pooling and load balancing, ensuring efficient utilization of resources and improving performance for your mission-critical application."
      },
      {
        "date": "2023-11-27T10:39:00.000Z",
        "voteCount": 2,
        "content": "It should be (A).  Specifically, you'd (most likely) use a TCP Load balancer \n\nhttps://www.pgbouncer.org/faq.html#how-to-load-balance-queries-between-several-servers\n\n\"PgBouncer does not have an internal multi-host configuration. It is possible via external tools.\"\n\nyou need to frontend stateless pgBouncer instances with a TCP load balancer."
      },
      {
        "date": "2023-11-13T10:14:00.000Z",
        "voteCount": 1,
        "content": "PgBouncer (Option B): PgBouncer is a lightweight connection pooler for PostgreSQL that can efficiently manage and distribute database connections between the primary and read replica instances. It helps in load balancing the application traffic between the instances."
      },
      {
        "date": "2023-05-26T07:58:00.000Z",
        "voteCount": 2,
        "content": "By using PgBouncer, you can configure it to distribute the application load between the Cloud SQL primary and read replica instances. PgBouncer will handle connection pooling and load balancing, ensuring efficient utilization of resources and improving performance for your mission-critical application."
      },
      {
        "date": "2023-03-26T09:24:00.000Z",
        "voteCount": 3,
        "content": "As others have said, A is the only option which could achieve the desired effect, providing TCP load balancing across multiple servers."
      },
      {
        "date": "2023-03-05T02:45:00.000Z",
        "voteCount": 2,
        "content": "A, I think is better.  \nHAProxy is not same as Cloud Load balancing."
      },
      {
        "date": "2023-03-01T19:42:00.000Z",
        "voteCount": 2,
        "content": "Connection pooling !"
      },
      {
        "date": "2023-02-08T06:03:00.000Z",
        "voteCount": 4,
        "content": "https://cloud.google.com/blog/products/databases/using-haproxy-to-scale-read-only-workloads-on-cloud-sql-for-postgresql"
      },
      {
        "date": "2023-03-01T19:41:00.000Z",
        "voteCount": 1,
        "content": "HAProxy  is not same as Cloud Load balancing."
      },
      {
        "date": "2023-04-26T07:14:00.000Z",
        "voteCount": 1,
        "content": "I think in the same way, it's not the same thing. However pgBouncer is not recommended for Load Balancing, just for connection pooling \ud83d\ude4f"
      },
      {
        "date": "2023-01-27T16:34:00.000Z",
        "voteCount": 2,
        "content": "A is the best answer, PgBouncer does not have multi-host"
      },
      {
        "date": "2022-12-30T08:51:00.000Z",
        "voteCount": 3,
        "content": "Per GPC72's referenced link, you need PgBouncer only does connection pooling, need Load balancing coupled.  Since Load Balancing not referenced in B.,  is not A the best answer?\n\n PgBouncer is a popular connection pooler designed for PostgreSQL, but it is not enough to achieve PostgreSQL High Availability by itself as it doesn\u2019t have multi-host configuration, failover, or detection.\n\nUsing a Load Balancer is a way to have High Availability in your database topology. It could be useful for redirecting traffic to healthy database nodes, distribute the"
      },
      {
        "date": "2022-12-30T09:07:00.000Z",
        "voteCount": 1,
        "content": "And C  &amp; D are wrong because they don't pool connections.  ref for refuting C...The load balancer doesn\u2019t store database credentials (except for the health check user), and it doesn\u2019t pool or decrypt/re-encrypt database connections. A single client connection in HAProxy translates to a single client connection in Postgres. \n\nThis approach is suitable when the workload is constrained by the database\u2019s processing capacity, and not by the number of client connections. You may require an additional connection pooling component (e.g. PgBouncer) if the number of clients becomes an issue, for example, when the database instances exhibit performance or stability issues due to the sheer number of simultaneous database connections."
      },
      {
        "date": "2022-12-30T09:09:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/blog/products/databases/using-haproxy-to-scale-read-only-workloads-on-cloud-sql-for-postgresql"
      },
      {
        "date": "2022-12-25T08:12:00.000Z",
        "voteCount": 1,
        "content": "PgBouncer is a light-weight connection pool manager for Greenplum and PostgreSQL. PgBouncer maintains a pool for connections for each database and user combination. PgBouncer either creates a new database connection for a client or reuses an existing connection for the same user and database."
      },
      {
        "date": "2022-12-25T08:12:00.000Z",
        "voteCount": 1,
        "content": "B: Use PgBouncer to set up database connection pooling between the Cloud SQL primary and read replica instances."
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 25,
    "url": "https://www.examtopics.com/discussions/google/view/92205-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "Your organization deployed a new version of a critical application that uses Cloud SQL for MySQL with high availability (HA) and binary logging enabled to store transactional information. The latest release of the application had an error that caused massive data corruption in your Cloud SQL for MySQL database. You need to minimize data loss. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOpen the Google Cloud Console, navigate to SQL &gt; Backups, and select the last version of the automated backup before the corruption.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReload the Cloud SQL for MySQL database using the LOAD DATA command to load data from CSV files that were used to initialize the instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPerform a point-in-time recovery of your Cloud SQL for MySQL database, selecting a date and time before the data was corrupted.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFail over to the Cloud SQL for MySQL HA instance. Use that instance to recover the transactions that occurred before the corruption."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 7,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-03-11T14:07:00.000Z",
        "voteCount": 7,
        "content": "C.\nThe question specifically mentions binary logging and the binary logs are used by point-in-time recovery. D doesn\u2019t buy you anything since the corrupt data would also be on the HA replica you fail over to. B looks like a lot of work and if the Cloud SQL instance were instantiated a while ago, option B could take a long time. A would work but the backup could have been taken a while before the corruption began. In which case restoring using that backup would wipe all the good data up to the point of corruption. The question asks for minimal data loss and the only way to ensure that is to restore to a point-in-time just before the corruption began."
      },
      {
        "date": "2024-01-19T05:09:00.000Z",
        "voteCount": 1,
        "content": "unrelated but this was also a question from PCA.  and the same answer C"
      },
      {
        "date": "2023-03-09T18:21:00.000Z",
        "voteCount": 2,
        "content": "C. Perform a point-in-time recovery of your Cloud SQL for MySQL database, selecting a date and time before the data was corrupted.\n\nPerforming a point-in-time recovery is the best option to minimize data loss in case of data corruption. Point-in-time recovery restores the database to a specific point in time before the data was corrupted, by replaying the binary logs that were generated since the selected time. This option is available when binary logging is enabled on Cloud SQL for MySQL with high availability.\n\nOption A, restoring from an automated backup, can lead to data loss because it might not contain all the changes made to the database after the backup was taken. Option B, reloading the database from CSV files, can be time-consuming and may lead to data loss if the files used for initialization are not up to date. Option D, failing over to the Cloud SQL for MySQL HA instance, may not help in this scenario as the data corruption is replicated to the HA instance, and it is intended to be used for high availability and not for disaster recovery."
      },
      {
        "date": "2023-02-18T01:22:00.000Z",
        "voteCount": 1,
        "content": "A. Originally I thought it was C, but after reading mysql best practices as well as Kloudgeek link carefully I changed my answer. In Cloud SQL best-practice:\nhttps://cloud.google.com/sql/docs/mysql/best-practices\n\"A point-in-time recovery always creates a new instance; you cannot perform a point-in-time recovery to an existing instance.\"\nKloudgeek's link (why gcloud command use clone?):\nhttps://cloud.google.com/sql/docs/mysql/backup-recovery/pitr#perform-pitr-binlog\n\"gcloud sql instances clone instance1 \\\ninstance1-clone \\\n--bin-log-file-name=mysql-bin.0000031 \\\n--bin-log-position=107\"\nIt seems to me that the question does not expect Cloud SQL instance switched just because of data corruption."
      },
      {
        "date": "2022-12-25T08:11:00.000Z",
        "voteCount": 1,
        "content": "C: Perform a point-in-time recovery of your Cloud SQL for MySQL database, selecting a date and time ***** before the data was corrupted."
      },
      {
        "date": "2022-12-24T01:41:00.000Z",
        "voteCount": 1,
        "content": "C is correct answer"
      },
      {
        "date": "2022-12-21T07:54:00.000Z",
        "voteCount": 3,
        "content": "Binary logging --&gt; Point in Recovery"
      },
      {
        "date": "2022-12-20T13:45:00.000Z",
        "voteCount": 2,
        "content": "Since it is retaining transaction log, point in time recovery is enabled and that would be the best option"
      },
      {
        "date": "2022-12-20T09:46:00.000Z",
        "voteCount": 3,
        "content": "Correct Answer C: Binary Logging enabled, with that you can identify the point of time the data was good and recover from that point time.\nhttps://cloud.google.com/sql/docs/mysql/backup-recovery/pitr#perform_the_point-in-time_recovery_using_binary_log_positions"
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 26,
    "url": "https://www.examtopics.com/discussions/google/view/92356-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "You plan to use Database Migration Service to migrate data from a PostgreSQL on-premises instance to Cloud SQL. You need to identify the prerequisites for creating and automating the task. What should you do? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDrop or disable all users except database administration users.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDisable all foreign key constraints on the source PostgreSQL database.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure that all PostgreSQL tables have a primary key.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tShut down the database before the Data Migration Service task is started.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure that pglogical is installed on the source PostgreSQL database.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "CE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "CE",
        "count": 8,
        "isMostVoted": true
      },
      {
        "answer": "BE",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-03-11T14:25:00.000Z",
        "voteCount": 5,
        "content": "C, E.\nA is wrong because you want user accounts migrated as well as the data. B is nonsense. D is also nonsense since the DMS is an online migration. That leave C and E, both of which are mentioned in the Google doc:\nhttps://cloud.google.com/database-migration/docs/postgres/configure-source-database"
      },
      {
        "date": "2023-09-22T06:51:00.000Z",
        "voteCount": 4,
        "content": "You are sure to know the answer is CE if you did the labs\ud83d\ude02"
      },
      {
        "date": "2024-01-18T13:28:00.000Z",
        "voteCount": 3,
        "content": "so true!! I remember doing the challenge on my own for like 2 days to make certain I've got it all right; so the number of times I actually had to install pglogical and create primary keys on tables were ridiculous! :D"
      },
      {
        "date": "2023-04-04T08:31:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/database-migration/docs/postgres/faq"
      },
      {
        "date": "2023-03-09T18:33:00.000Z",
        "voteCount": 3,
        "content": "C. Ensure that all PostgreSQL tables have a primary key.\nE. Ensure that pglogical is installed on the source PostgreSQL database.\n\nWhen using Database Migration Service to migrate data from a PostgreSQL on-premises instance to Cloud SQL, it is important to ensure that all PostgreSQL tables have a primary key. This is because Cloud SQL requires tables to have a primary key to enable replication and ensure data consistency.\n\nIt is also important to ensure that pglogical is installed on the source PostgreSQL database. This is because pglogical is used by Database Migration Service to replicate data changes from the source database to the target Cloud SQL instance.\n\nOptions A and D are not prerequisites for creating and automating the task. Option B is not recommended as it can cause data inconsistencies during the migration. Disabling foreign key constraints may result in data being migrated with foreign key constraint violations."
      },
      {
        "date": "2023-03-05T03:01:00.000Z",
        "voteCount": 1,
        "content": "Run the CREATE EXTENSION IF NOT EXISTS pglogical command on every database on your source instance. This installs the pglogical extension into the database.\nFor tables that don't have primary keys, Database Migration Service supports migration of the initial snapshot and INSERT statements during the change data capture (CDC) phase. You should migrate UPDATE and DELETE statements manually."
      },
      {
        "date": "2023-01-02T16:39:00.000Z",
        "voteCount": 2,
        "content": "CE are correct."
      },
      {
        "date": "2022-12-30T09:28:00.000Z",
        "voteCount": 1,
        "content": "E for sure; presume C is required for \"automation\" of task, because intervention required for tables without primary key.  \nFor tables that don't have primary keys, Database Migration Service supports migration of the initial snapshot and INSERT statements during the change data capture (CDC) phase. You should migrate UPDATE and DELETE statements manually.\n\nhttps://cloud.google.com/database-migration/docs/postgres/configure-source-database"
      },
      {
        "date": "2022-12-30T09:35:00.000Z",
        "voteCount": 2,
        "content": "Found this confirming C - was not option prior to 6/2022 - Tables without primary keys on the source PostgreSQL database are not migrated. For those tables, DMS migrated only the schema. This is no longer a limitation after the June 2022 product update.\n\nAdditional good content beyond scope of question in source link...https://cloud.google.com/blog/products/databases/reduce-downtime-for-postgresql-migration-to-google-cloud-sql"
      },
      {
        "date": "2022-12-26T10:47:00.000Z",
        "voteCount": 2,
        "content": "Vote for CE"
      },
      {
        "date": "2022-12-25T08:10:00.000Z",
        "voteCount": 1,
        "content": "C: Ensure that all PostgreSQL ***** tables have a primary key.\n E: Ensure that pglogical ***** is installed on the source PostgreSQL database."
      },
      {
        "date": "2022-12-24T02:23:00.000Z",
        "voteCount": 1,
        "content": "CE\nhttps://cloud.google.com/database-migration/docs/postgres/faq"
      },
      {
        "date": "2022-12-21T21:15:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/database-migration/docs/postgres/configure-source-database"
      },
      {
        "date": "2022-12-21T21:14:00.000Z",
        "voteCount": 1,
        "content": "CE\nhttps://cloud.google.com/database-migration/docs/postgres/configure-source-database"
      },
      {
        "date": "2022-12-21T07:56:00.000Z",
        "voteCount": 2,
        "content": "Remove all the foreign key constraints\nCome up with necessary pglogical"
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 27,
    "url": "https://www.examtopics.com/discussions/google/view/92206-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "You are using Compute Engine on Google Cloud and your data center to manage a set of MySQL databases in a hybrid configuration. You need to create replicas to scale reads and to offload part of the management operation. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse external server replication.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Data Migration Service.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud SQL for MySQL external replica.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the mysqldump utility and binary logs."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 12,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 12,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-03-11T15:06:00.000Z",
        "voteCount": 15,
        "content": "C.\nD is nonsense, so can be eliminated. The question tells us we\u2019re already managing BOTH sets of MySQL instances, one on prem and the other in GCE. The question also says an objective is to offload part of the management. That can only mean leverage a managed service. The Data(base) Migration Service is a managed service used to instantiate a new migrated DB in Cloud SQL (or AlloyDB for PostgreSQL but that\u2019s not in scope here). The question isn\u2019t asking about database migration, so we can eliminate B. A could be used to create replicas, but doesn\u2019t help with offloading management operations. That leaves C which does use a managed service which could be leveraged to create replicas."
      },
      {
        "date": "2024-07-26T11:47:00.000Z",
        "voteCount": 1,
        "content": "External replica is for scaling read"
      },
      {
        "date": "2024-05-02T06:04:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/sql/docs/mysql/replication/external-"
      },
      {
        "date": "2024-02-21T02:55:00.000Z",
        "voteCount": 1,
        "content": "C. MySQL external replica are best for create replicas to sclae reads and offload"
      },
      {
        "date": "2023-11-25T07:59:00.000Z",
        "voteCount": 3,
        "content": "C is the answer as Question mentions  offload part of the management operation"
      },
      {
        "date": "2023-09-27T07:08:00.000Z",
        "voteCount": 1,
        "content": "it's c"
      },
      {
        "date": "2023-09-18T14:45:00.000Z",
        "voteCount": 2,
        "content": "The ans is C"
      },
      {
        "date": "2023-09-05T15:13:00.000Z",
        "voteCount": 2,
        "content": "A is correct - se external server replication -==&gt; This option allows you to set up replication between your on-premises MySQL server (in your data center) and a MySQL server running on GOOGLE CLOUD COMPUTE ENGINE."
      },
      {
        "date": "2023-05-26T08:05:00.000Z",
        "voteCount": 3,
        "content": "By using external server replication, you can set up and manage replication between your on-premises MySQL database and a replica instance in Google Cloud. This enables you to scale reads, offload management operations, and distribute the workload between your data center and Google Cloud, providing the desired benefits in a hybrid configuration."
      },
      {
        "date": "2023-03-08T16:56:00.000Z",
        "voteCount": 2,
        "content": "Agree with B. \n- Multi-cloud continuous replication\n\nMuch like the read replicas across regions, if data exists in another cloud provider, a migration job can be set up which continuously replicates the database &lt;&lt;into Google Cloud for multi-cloud read-availability&gt;&gt;. Database Migration Service doesn't support a dual-write scenario, that is writing to and reading from both the source and destination.\n\nhttps://cloud.google.com/database-migration/docs/overview#use_cases"
      },
      {
        "date": "2023-03-05T12:49:00.000Z",
        "voteCount": 1,
        "content": "offload part of the management operation =&gt; cloud sql =&gt; C"
      },
      {
        "date": "2023-03-05T12:52:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/sql/docs/mysql/replication/external-server"
      },
      {
        "date": "2023-03-05T03:11:00.000Z",
        "voteCount": 1,
        "content": "A. Use external server replication.  \nhttps://cloud.google.com/sql/docs/mysql/replication/external-server#config-description"
      },
      {
        "date": "2023-02-11T23:43:00.000Z",
        "voteCount": 4,
        "content": "A\nA: CORRECT. External server replication meant to serve the case as the question describe: DC -&gt; GSQL.\nB: WRONG. DMS intends for one time migration + CDC but does not mean to serve as primary + replica fashion. The replica is not ready to serve the traffic.\nC: WRONG. Cloud SQL for MySQL external replica meant to serve the replica is outside of Google cloud, which means GSQL -&gt; GSQL or GSQL -&gt; DC.\nD: WRONG. This option is for one time only.\nReference:\nhttps://cloud.google.com/sql/docs/mysql/replication/external-server\nhttps://cloud.google.com/sql/docs/mysql/replication\nhttps://cloud.google.com/database-migration/docs/overview#use_cases"
      },
      {
        "date": "2023-02-11T23:50:00.000Z",
        "voteCount": 1,
        "content": "Correct the reference links:\nhttps://cloud.google.com/sql/docs/mysql/replication/configure-replication-from-external#curl\nhttps://cloud.google.com/sql/docs/mysql/replication/configure-external-replica\nhttps://cloud.google.com/database-migration/docs/overview#use_cases"
      },
      {
        "date": "2023-01-21T22:19:00.000Z",
        "voteCount": 3,
        "content": "I strongly believe it is A. Since the database is already in your datacenter. So you need to replicate your datacenter database to cloud to offload management. External Server Replication will replicate your database on to cloud sql. \nhttps://cloud.google.com/sql/docs/mysql/replication/external-server"
      },
      {
        "date": "2023-01-04T01:02:00.000Z",
        "voteCount": 1,
        "content": "C: https://cloud.google.com/sql/docs/mysql/replication"
      },
      {
        "date": "2022-12-25T08:09:00.000Z",
        "voteCount": 2,
        "content": "C: Use Cloud SQL for MySQL external replica."
      },
      {
        "date": "2022-12-24T04:25:00.000Z",
        "voteCount": 2,
        "content": "C is the Correct Answer"
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 28,
    "url": "https://www.examtopics.com/discussions/google/view/92227-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "Your company is shutting down their data center and migrating several MySQL and PostgreSQL databases to Google Cloud. Your database operations team is severely constrained by ongoing production releases and the lack of capacity for additional on-premises backups. You want to ensure that the scheduled migrations happen with minimal downtime and that the Google Cloud databases stay in sync with the on-premises data changes until the applications can cut over. What should you do? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Database Migration Service to migrate the databases to Cloud SQL.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a cross-region read replica to migrate the databases to Cloud SQL.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse replication from an external server to migrate the databases to Cloud SQL.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an external read replica to migrate the databases to Cloud SQL.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a read replica to migrate the databases to Cloud SQL."
    ],
    "answer": "AC",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AC",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-26T11:48:00.000Z",
        "voteCount": 1,
        "content": "replication from an external server is for db migration."
      },
      {
        "date": "2023-10-02T15:08:00.000Z",
        "voteCount": 1,
        "content": "A because Database migration service is the managed offering\nC because the external server is used for migration.\nThe others aren't approaches aren't actual methods for migration."
      },
      {
        "date": "2023-04-04T08:35:00.000Z",
        "voteCount": 1,
        "content": "no other choices are correct"
      },
      {
        "date": "2023-03-11T15:16:00.000Z",
        "voteCount": 1,
        "content": "A, C.\nB doesn\u2019t make sense since the DBs aren\u2019t in Cloud SQL yet. D and E don\u2019t makes sense because no method is attached to either answer and you wouldn\u2019t use a read replica as a source anyway. That leaves A and C. A makes sense since it can be scheduled and is online hence little/no downtime. Native DB replication of an external (on prem) server is basically what the Database Migration Service is doing. Which makes C correct as well."
      },
      {
        "date": "2023-03-05T03:21:00.000Z",
        "voteCount": 1,
        "content": "A &amp; C is the correct Answer"
      },
      {
        "date": "2022-12-25T08:07:00.000Z",
        "voteCount": 2,
        "content": "A: Use Database Migration Service ***** to migrate the databases to Cloud SQL.\n C: Use replication ***** from an external server to migrate the databases to Cloud SQL."
      },
      {
        "date": "2022-12-24T04:30:00.000Z",
        "voteCount": 2,
        "content": "A &amp; C is the correct Answer"
      },
      {
        "date": "2022-12-23T12:57:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/sql/docs/mysql/replication/manage-replicas#basic-steps-to-change-parallel-replication-flags"
      },
      {
        "date": "2022-12-21T08:12:00.000Z",
        "voteCount": 1,
        "content": "A is Database Migration Service to migrate the databases with CDC\nC is Replication from an external server is used to Migration Database to Cloud SQL"
      },
      {
        "date": "2022-12-20T14:04:00.000Z",
        "voteCount": 1,
        "content": "Using A for CDC makes sense to me and C is an option as well"
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 29,
    "url": "https://www.examtopics.com/discussions/google/view/92222-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "Your company is migrating the existing infrastructure for a highly transactional application to Google Cloud. You have several databases in a MySQL database instance and need to decide how to transfer the data to Cloud SQL. You need to minimize the downtime for the migration of your 500 GB instance. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a Cloud SQL for MySQL instance for your databases, and configure Datastream to stream your database changes to Cloud SQL.<br>2. Select the Backfill historical data check box on your stream configuration to initiate Datastream to backfill any data that is out of sync between the source and destination.<br>3. Delete your stream when all changes are moved to Cloud SQL for MySQL, and update your application to use the new instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create migration job using Database Migration Service.<br>2. Set the migration job type to Continuous, and allow the databases to complete the full dump phase and start sending data in change data capture (CDC) mode.<br>3. Wait for the replication delay to minimize, initiate a promotion of the new Cloud SQL instance, and wait for the migration job to complete.<br>4. Update your application connections to the new instance.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create migration job using Database Migration Service.<br>2. Set the migration job type to One-time, and perform this migration during a maintenance window.<br>3. Stop all write workloads to the source database and initiate the dump. Wait for the dump to be loaded into the Cloud SQL destination database and the destination database to be promoted to the primary database.<br>4. Update your application connections to the new instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Use the mysqldump utility to manually initiate a backup of MySQL during the application maintenance window.<br>2. Move the files to Cloud Storage, and import each database into your Cloud SQL instance.<br>3. Continue to dump each database until all the databases are migrated.<br>4. Update your application connections to the new instance."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-03-11T15:28:00.000Z",
        "voteCount": 6,
        "content": "B.\nA is wrong because Datastream is a CDC and replication service for data synchronization across heterogeneous databases. It\u2019s reasonable to assume you\u2019ll be using Cloud SQL for MySQL, so you\u2019ll be performing a homogeneous migration. Plus, while the a Datastream source can be MySQL, a Datastream target is either BigQuery or Cloud Storage and not Cloud SQL. See https://cloud.google.com/datastream/docs/overview.\nC is wrong because a one time migration wouldn\u2019t capture all the data changes once the maintenance window ended and the apps were fired back up. Furthermore, stopping all writes during the dump would constitute downtime which the question wants minimized. D would take forever in a rapidly changing source system. B is the cleanest and simplest solution especially since the question puts no time constraint on making the migration happen."
      },
      {
        "date": "2024-05-19T09:36:00.000Z",
        "voteCount": 1,
        "content": "Agree with B"
      },
      {
        "date": "2023-04-04T08:38:00.000Z",
        "voteCount": 2,
        "content": "C isn't correct since it will not minimize the downtime"
      },
      {
        "date": "2023-03-05T03:24:00.000Z",
        "voteCount": 1,
        "content": "B is the correct answer"
      },
      {
        "date": "2022-12-25T08:06:00.000Z",
        "voteCount": 1,
        "content": "B: Create migration job using Database ***** Migration Service.\nSet the migration job type to Continuous, and allow the databases to complete the full dump phase and start sending data in change data capture (CDC) mode.\nWait ***** for the replication delay to minimize, initiate a promotion of the new Cloud SQL instance, and wait for the migration job to complete.\nUpdate your application connections to the new instance."
      },
      {
        "date": "2022-12-24T04:35:00.000Z",
        "voteCount": 2,
        "content": "B is the correct answer"
      },
      {
        "date": "2022-12-21T08:14:00.000Z",
        "voteCount": 2,
        "content": "Continuous Migration with CDC"
      },
      {
        "date": "2022-12-20T14:06:00.000Z",
        "voteCount": 2,
        "content": "C is not minimizing the downtime"
      },
      {
        "date": "2022-12-20T13:37:00.000Z",
        "voteCount": 4,
        "content": "Correct option is B. You need to minimize the downtime of the application but option C refers to stop the app while migration to complete."
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 30,
    "url": "https://www.examtopics.com/discussions/google/view/92223-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "Your company uses the Cloud SQL out-of-disk recommender to analyze the storage utilization trends of production databases over the last 30 days. Your database operations team uses these recommendations to proactively monitor storage utilization and implement corrective actions. You receive a recommendation that the instance is likely to run out of disk space. What should you do to address this storage alert?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNormalize the database to the third normal form.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCompress the data using a different compression algorithm.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tManually or automatically increase the storage capacity.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate another schema to load older data."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-12-20T13:43:00.000Z",
        "voteCount": 6,
        "content": "Correct answer is C: https://cloud.google.com/sql/docs/mysql/using-ood-recommender#apply_recommendations"
      },
      {
        "date": "2023-03-11T15:40:00.000Z",
        "voteCount": 6,
        "content": "C.\nA is wrong since modifying the schemas to 3NF would use more disk. D is nonsense. B sounds vague at best and probably not supported at the database level. C is the best answer. The link provided by Kloudgeek is spot on."
      },
      {
        "date": "2023-04-04T08:40:00.000Z",
        "voteCount": 1,
        "content": "out of disk means need for more space"
      },
      {
        "date": "2022-12-25T08:04:00.000Z",
        "voteCount": 1,
        "content": "C: Manually or automatically increase the storage capacity."
      },
      {
        "date": "2022-12-24T06:39:00.000Z",
        "voteCount": 2,
        "content": "C is the correct answer"
      },
      {
        "date": "2022-12-23T13:03:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/sql/docs/mysql/instance-settings#storage-capacity-2ndgen"
      },
      {
        "date": "2022-12-21T08:34:00.000Z",
        "voteCount": 2,
        "content": "Manually or automatically increase the storage capacity."
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 31,
    "url": "https://www.examtopics.com/discussions/google/view/92229-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "You are managing a mission-critical Cloud SQL for PostgreSQL instance. Your application team is running important transactions on the database when another DBA starts an on-demand backup. You want to verify the status of the backup. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCheck the cloudsql.googleapis.com/postgres.log instance log.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPerform the gcloud sql operations list command.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Audit Logs to verify the status.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Google Cloud Console."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 11,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-03-12T09:34:00.000Z",
        "voteCount": 6,
        "content": "B.\nA is wrong. The cloudsql.googleapis.com/postgres.log log file could be used to find out who started the backup operation, not the status of the operation. C is wrong for a similar reason. D is partially right. In the console there\u2019s an Operations option in the menu on the left. Click that and it shows \u201cCreating backup\u201d together a start time. That\u2019s not a million miles different from the gcloud sql operations list command which shows similar output except there\u2019s a STATUS: line showing the word RUNNING while the backup is in progress. Given the question specifically mentions \u201cstatus\u201d, B would be the better answer. Just."
      },
      {
        "date": "2024-04-05T23:16:00.000Z",
        "voteCount": 1,
        "content": "gcloud sql backup list --instance= instance-id will give the status of backup for that particular instance. So B is good."
      },
      {
        "date": "2023-10-02T17:39:00.000Z",
        "voteCount": 2,
        "content": "B to get the status using gcloud. D is possible but as mentioned, it doesn't specifically mention where in the console so B is a better answer."
      },
      {
        "date": "2023-09-25T13:12:00.000Z",
        "voteCount": 1,
        "content": "If you want the status of the backup, then gcloud is the only viable option, as the Audit Logs will just tell you who started it."
      },
      {
        "date": "2023-05-26T08:11:00.000Z",
        "voteCount": 1,
        "content": "Using the Google Cloud Console is the most straightforward and convenient method for verifying the status of an on-demand backup for your mission-critical Cloud SQL for PostgreSQL instance. It provides a graphical interface that displays the backup status and any relevant details, enabling you to quickly assess the situation and ensure the integrity of your important transactions."
      },
      {
        "date": "2023-04-04T08:43:00.000Z",
        "voteCount": 1,
        "content": "option C is to find out who started the backup, for the status B is correct"
      },
      {
        "date": "2023-03-06T03:13:00.000Z",
        "voteCount": 1,
        "content": "B should the right answer: via  \ngcloud sql operations list --instance=&lt;instance_name&gt;\ngcloud alpha sql operations list --instance=&lt;instance_name&gt;"
      },
      {
        "date": "2023-01-18T05:32:00.000Z",
        "voteCount": 4,
        "content": "Should be B\nPerform the gcloud sql operations list command\n\nhttps://cloud.google.com/sql/docs/postgres/backup-recovery/backups#troubleshooting-backups\n\nUnder Troubleshooting:\nIssue: \"You can't see the current operation's status.\"\n\nThe Google Cloud console reports only success or failure when the operation is done. It isn't designed to show warnings or other updates.\nRun the gcloud sql operations list command to list all operations for the given Cloud SQL instance.\n\nA and C iis wrong because there are used for WHO issued the Backup, but not the current status of the backup\nD is wrong only shows success or failure in the Cloud Console but not the current status of the backup"
      },
      {
        "date": "2022-12-26T10:55:00.000Z",
        "voteCount": 1,
        "content": "Vote for B"
      },
      {
        "date": "2022-12-25T08:04:00.000Z",
        "voteCount": 1,
        "content": "B: Perform the gcloud sql operations list ***** command."
      },
      {
        "date": "2022-12-24T06:45:00.000Z",
        "voteCount": 1,
        "content": "C is the correct answer.log gives more information"
      },
      {
        "date": "2022-12-21T08:35:00.000Z",
        "voteCount": 1,
        "content": "Perform the gcloud sql operations list command."
      },
      {
        "date": "2022-12-20T14:11:00.000Z",
        "voteCount": 2,
        "content": "Correct answer is B. https://cloud.google.com/sql/docs/postgres/backup-recovery/backups#troubleshooting-backups"
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 32,
    "url": "https://www.examtopics.com/discussions/google/view/92363-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "You support a consumer inventory application that runs on a multi-region instance of Cloud Spanner. A customer opened a support ticket to complain about slow response times. You notice a Cloud Monitoring alert about high CPU utilization. You want to follow Google-recommended practices to address the CPU performance issue. What should you do first?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the number of processing units.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the database schema, and add additional indexes.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tShard data required by the application into multiple instances.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDecrease the number of processing units."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 7,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-04-05T23:18:00.000Z",
        "voteCount": 2,
        "content": "A seems to be the first go to choice, if that does not resolve we can move to other options, But first A."
      },
      {
        "date": "2023-09-14T05:53:00.000Z",
        "voteCount": 3,
        "content": "In case of high CPU utilization like, mentioned in question, refer: https://cloud.google.com/spanner/docs/identify-latency-point#:~:text=Check%20the%20CPU%20utilization%20of%20the%20instance.%20If%20the%20CPU%20utilization%20of%20the%20instance%20is%20above%20the%20recommended%20level%2C%20you%20should%20manually%20add%20more%20nodes%2C%20or%20set%20up%20auto%20scaling.\n\"Check the CPU utilization of the instance. If the CPU utilization of the instance is above the recommended level, you should manually add more nodes, or set up auto scaling.\"\n\nIndexes and schema are reviewed post identifying query with slow performance. Refer : https://cloud.google.com/spanner/docs/troubleshooting-performance-regressions#review-schema"
      },
      {
        "date": "2023-09-05T15:55:00.000Z",
        "voteCount": 2,
        "content": "I surprised with the chosen answer. The correct answer is B. When addressing high CPU utilization in a Google Cloud Spanner instance, you should first consider B. Modify the database schema, and add additional indexes. High CPU utilization in a database often occurs due to inefficient queries or lack of appropriate indexes."
      },
      {
        "date": "2023-12-16T07:32:00.000Z",
        "voteCount": 1,
        "content": "So, any words mentioned the schema is not right? or it lacks index?"
      },
      {
        "date": "2023-05-26T08:16:00.000Z",
        "voteCount": 1,
        "content": "By modifying the database schema and adding additional indexes, you can optimize query performance and potentially reduce the CPU utilization in Cloud Spanner. This approach focuses on improving the efficiency of the database and aligning it with the specific requirements of the consumer inventory application. It is important to monitor the impact of these changes and make further optimizations as needed."
      },
      {
        "date": "2023-03-12T09:45:00.000Z",
        "voteCount": 3,
        "content": "A.\nB is wrong since that would increase CPU utilization even further and the question does not mention anything being wrong with index design. D is wrong since that would reduce CPU capacity and thus increase the load on the remaining CPUs. Cloud Spanner does not autoscale. It\u2019s up to you to allocate the number of nodes or processing units to keep CPU utilization under 65%. So add more processing units."
      },
      {
        "date": "2022-12-25T08:03:00.000Z",
        "voteCount": 2,
        "content": "Compute capacity defines amount of server and storage resources that are available to the databases in an instance. When you create an instance, you specify its compute capacity as a number of processing ***** units or as a number of nodes, with 1000 processing units being equal to 1 node. \nrange9005"
      },
      {
        "date": "2022-12-25T08:02:00.000Z",
        "voteCount": 1,
        "content": "A: Increase the number of processing units."
      },
      {
        "date": "2022-12-24T18:30:00.000Z",
        "voteCount": 2,
        "content": "A is correct answer\nB is not correct because modifying schema  is  not a correct option"
      },
      {
        "date": "2022-12-21T08:41:00.000Z",
        "voteCount": 2,
        "content": "Increase the number of processing units."
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 33,
    "url": "https://www.examtopics.com/discussions/google/view/92231-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "Your company uses Bigtable for a user-facing application that displays a low-latency real-time dashboard. You need to recommend the optimal storage type for this read-intensive database. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRecommend solid-state drives (SSD).\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRecommend splitting the Bigtable instance into two instances in order to load balance the concurrent reads.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRecommend hard disk drives (HDD).",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRecommend mixed storage types."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-10-02T17:57:00.000Z",
        "voteCount": 1,
        "content": "SSD because this is the more highly performant PD type"
      },
      {
        "date": "2023-09-25T13:13:00.000Z",
        "voteCount": 1,
        "content": "A.\nData is split correctly on nodes if the row-key is well designed."
      },
      {
        "date": "2023-06-26T21:33:00.000Z",
        "voteCount": 1,
        "content": "if you plan to store extensive historical data for a large number of remote-sensing devices and then use the data to generate daily reports, the cost savings for HDD storage might justify the performance tradeoff. On the other hand, if you plan to use the data to display a real-time dashboard, it probably would not make sense to use HDD storage\u2014reads would be much more frequent in this case, and reads that are not scans are much slower with HDD storage."
      },
      {
        "date": "2023-03-12T09:50:00.000Z",
        "voteCount": 2,
        "content": "A.\nWhen you create a Bigtable instance you have to choose either SSD or HDD. The SSD options says, \u201cLower latency and more rows read per second. Typically used for real-time serving use cases, such as ad serving and mobile app recommendations\u201d. User facing plus low latency plus read intensive equals SSD."
      },
      {
        "date": "2023-03-06T03:18:00.000Z",
        "voteCount": 2,
        "content": "A is correct answer ,Question is about storage type (hardware)"
      },
      {
        "date": "2022-12-25T08:02:00.000Z",
        "voteCount": 2,
        "content": "A: Recommend solid-state drives ***** (SSD)."
      },
      {
        "date": "2022-12-24T19:43:00.000Z",
        "voteCount": 2,
        "content": "A is correct answer ,Question is about storage type so B is not a correct answer"
      },
      {
        "date": "2022-12-23T13:15:00.000Z",
        "voteCount": 1,
        "content": "SSD is significantly faster and has more predictable performance than HDD."
      },
      {
        "date": "2022-12-21T08:42:00.000Z",
        "voteCount": 2,
        "content": "Recommend solid-state drives (SSD)"
      },
      {
        "date": "2022-12-20T14:27:00.000Z",
        "voteCount": 1,
        "content": "B is a right answer but it is not a storage type"
      },
      {
        "date": "2022-12-20T14:14:00.000Z",
        "voteCount": 3,
        "content": "Correct answer is A. https://cloud.google.com/bigtable/docs/choosing-ssd-hdd"
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 34,
    "url": "https://www.examtopics.com/discussions/google/view/92238-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "Your organization has a critical business app that is running with a Cloud SQL for MySQL backend database. Your company wants to build the most fault-tolerant and highly available solution possible. You need to ensure that the application database can survive a zonal and regional failure with a primary region of us-central1 and the backup region of us-east1. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Provision a Cloud SQL for MySQL instance in us-central1-a.<br>2. Create a multiple-zone instance in us-west1-b.<br>3. Create a read replica in us-east1-c.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Provision a Cloud SQL for MySQL instance in us-central1-a.<br>2. Create a multiple-zone instance in us-central1-b.<br>3. Create a read replica in us-east1-b.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Provision a Cloud SQL for MySQL instance in us-central1-a.<br>2. Create a multiple-zone instance in us-east-b.<br>3. Create a read replica in us-east1-c.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Provision a Cloud SQL for MySQL instance in us-central1-a.<br>2. Create a multiple-zone instance in us-east1-b.<br>3. Create a read replica in us-central1-b."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-03-12T10:19:00.000Z",
        "voteCount": 4,
        "content": "B.\nCloud SQL is a regional service with read replicas allowed in other regions. So the answer must reference 2 different zones in the us-central1 region, one for the primary and one for the HA replica. A read replica needs to be in a zone within us-east1. The only options which provides that is B."
      },
      {
        "date": "2022-12-30T13:26:00.000Z",
        "voteCount": 2,
        "content": "B is correct. DR write-up helps... https://cloud.google.com/sql/docs/sqlserver/intro-to-cloud-sql-disaster-recovery"
      },
      {
        "date": "2022-12-25T08:01:00.000Z",
        "voteCount": 1,
        "content": "D: Provision a Cloud SQL for MySQL instance in us-central1-a.\nCreate a multiple-zone instance in ***** us-east1-b.\nCreate a read replica in us-central1-b."
      },
      {
        "date": "2022-12-24T20:28:00.000Z",
        "voteCount": 4,
        "content": "B is the correct answer"
      },
      {
        "date": "2022-12-24T14:15:00.000Z",
        "voteCount": 2,
        "content": "Very confusing description. My only guess is that steps 1 and 2 describe the same action - creating primary instance with multiple zones HA. This eliminates all answers but B, because you can only have HA setup within the same region."
      },
      {
        "date": "2022-12-21T08:49:00.000Z",
        "voteCount": 1,
        "content": "I guess D\nPrimary Instance Us-Central\nFor HA, Multi-Region us-east\nReplica on the top of primary i.e Us-central for low latency"
      },
      {
        "date": "2022-12-20T14:47:00.000Z",
        "voteCount": 1,
        "content": "I got a little confused about the text, for B to be the correct answer the multiple-zone instance would be the stand-by instance and the read replica would be the cross-region read replica."
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 35,
    "url": "https://www.examtopics.com/discussions/google/view/92240-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "You are building an Android game that needs to store data on a Google Cloud serverless database. The database will log user activity, store user preferences, and receive in-game updates. The target audience resides in developing countries that have intermittent internet connectivity. You need to ensure that the game can synchronize game data to the backend database whenever an internet network is available. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Firestore.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud SQL with an external (public) IP address.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an in-app embedded database.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Spanner."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-03-12T10:30:00.000Z",
        "voteCount": 8,
        "content": "A.\nB is wrong since that\u2019s not secure and doesn\u2019t make sense. C is bizarre and doesn\u2019t leverage a GCP serverless database. The key is intermittent internet coverage, meaning real-time syncing is not needed and can be supported. That rules out Spanner, which leaves Firestore. Probably Datastore mode, not that the question mentions that.  The link provided by GCP72 is spot on."
      },
      {
        "date": "2022-12-25T08:00:00.000Z",
        "voteCount": 1,
        "content": "A: Use Firestore."
      },
      {
        "date": "2022-12-24T20:34:00.000Z",
        "voteCount": 3,
        "content": "A is a correct answer , Cloud Firestone https://firebase.google.com/docs/firestore"
      },
      {
        "date": "2022-12-21T08:51:00.000Z",
        "voteCount": 4,
        "content": "Android App --&gt;&gt; Cloud Firestone"
      },
      {
        "date": "2022-12-20T14:53:00.000Z",
        "voteCount": 2,
        "content": "A supports offline sync"
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 36,
    "url": "https://www.examtopics.com/discussions/google/view/92369-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "You released a popular mobile game and are using a 50 TB Cloud Spanner instance to store game data in a PITR-enabled production environment. When you analyzed the game statistics, you realized that some players are exploiting a loophole to gather more points to get on the leaderboard. Another DBA accidentally ran an emergency bugfix script that corrupted some of the data in the production environment. You need to determine the extent of the data corruption and restore the production environment. What should you do? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIf the corruption is significant, use backup and restore, and specify a recovery timestamp.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIf the corruption is significant, perform a stale read and specify a recovery timestamp. Write the results back.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIf the corruption is significant, use import and export.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIf the corruption is insignificant, use backup and restore, and specify a recovery timestamp.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIf the corruption is insignificant, perform a stale read and specify a recovery timestamp. Write the results back.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "AE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AE",
        "count": 10,
        "isMostVoted": true
      },
      {
        "answer": "BD",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "BC",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-24T13:56:00.000Z",
        "voteCount": 10,
        "content": "https://cloud.google.com/spanner/docs/pitr#ways-to-recover\nTo recover the entire database, backup or export the database specifying a timestamp in the past and then restore or import it to a new database. This is typically used to recover from data corruption issues when you have to revert the entire database to a point-in-time before the corruption occurred.\nThis part describes significant corruption - A\n\nTo recover a portion of the database, perform a stale read specifying a query-condition and timestamp in the past, and then write the results back into the live database. This is typically used for surgical operations on a live database. For example, if you accidentally delete a particular row or incorrectly update a subset of data, you can recover it with this method. \nThis describes insignificant corruption case - E"
      },
      {
        "date": "2023-03-12T10:57:00.000Z",
        "voteCount": 3,
        "content": "A, E.\nThe answers are split between significant and insignificant. For insignificant, the simplest form of recovery would be E. That eliminates D. For significant, let\u2019s assume that means a lot of data of the the 50 TB total. A stale read and write back would probably been too onerous, so that eliminates B.  That leaves A and C. The question doesn\u2019t mention anything about logical backups (export) which suggests a restore from a backup would be appropriate of a large amount of data that needed to be recovered. \nhttps://cloud.google.com/spanner/docs/pitr\nhttps://cloud.google.com/spanner/docs/backup/restore-backup"
      },
      {
        "date": "2023-01-02T17:07:00.000Z",
        "voteCount": 3,
        "content": "AE are correct."
      },
      {
        "date": "2022-12-25T07:59:00.000Z",
        "voteCount": 1,
        "content": "B: If the corruption is significant, perform a stale ***** read and specify a recovery timestamp. Write the results back.\nD: If the corruption is insignificant, use backup and ***** restore, and specify a recovery timestamp."
      },
      {
        "date": "2022-12-21T08:53:00.000Z",
        "voteCount": 1,
        "content": "B. If the corruption is significant, perform a stale read and specify a recovery timestamp. Write the results back.\nD. If the corruption is insignificant, use backup and restore, and specify a recovery timestamp."
      },
      {
        "date": "2022-12-21T08:52:00.000Z",
        "voteCount": 1,
        "content": "B. If the corruption is significant, perform a stale read and specify a recovery timestamp. Write the results back.\nC. If the corruption is significant, use import and export."
      },
      {
        "date": "2022-12-21T08:53:00.000Z",
        "voteCount": 1,
        "content": "By mistake"
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 37,
    "url": "https://www.examtopics.com/discussions/google/view/92241-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "You are starting a large CSV import into a Cloud SQL for MySQL instance that has many open connections. You checked memory and CPU usage, and sufficient resources are available. You want to follow Google-recommended practices to ensure that the import will not time out. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tClose idle connections or restart the instance before beginning the import operation.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the amount of memory allocated to your instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure that the service account has the Storage Admin role.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the number of CPUs for the instance to ensure that it can handle the additional import operation."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-03-12T11:06:00.000Z",
        "voteCount": 2,
        "content": "A.\nCPU and memory are OK so that elimiates B and D. C is nonsense which leaves A. This is supported by Google\u2019s own documention (read recommended practices) which says close unused operations and re-start the instance. This is the best way to ensure maximum resources for the import operation.\nhttps://cloud.google.com/sql/docs/mysql/import-export#troubleshooting"
      },
      {
        "date": "2022-12-29T11:27:00.000Z",
        "voteCount": 1,
        "content": "C. for import service account needs storage.buckets.get &amp; storage.objects.get"
      },
      {
        "date": "2022-12-25T07:57:00.000Z",
        "voteCount": 2,
        "content": "A: Close idle connections or restart the instance before beginning the import operation."
      },
      {
        "date": "2022-12-25T01:41:00.000Z",
        "voteCount": 3,
        "content": "The import operation is taking too long.\tToo many active connections can interfere with import operations.\nClose unused operations. Check the CPU and memory usage of your Cloud SQL instance to make sure there are plenty of resources available. The best way to ensure maximum resources for the import is to restart the instance before beginning the operation.\n\nA restart:\n\nCloses all connections.\nEnds any tasks that may be consuming resources\nhttps://cloud.google.com/sql/docs/mysql/import-export"
      },
      {
        "date": "2022-12-24T14:22:00.000Z",
        "voteCount": 1,
        "content": "Eliminate B and D because 'You checked memory and CPU usage, and sufficient resources are available.' \nEliminate C because it makes no sense."
      },
      {
        "date": "2022-12-24T14:24:00.000Z",
        "voteCount": 2,
        "content": "To elaborate on C - it's required for the export into Cloud Storage, which is not the case\nhttps://cloud.google.com/sql/docs/postgres/import-export/import-export-csv#required_roles_and_permissions_for_exporting"
      },
      {
        "date": "2022-12-21T16:13:00.000Z",
        "voteCount": 3,
        "content": "Close idle connections or restart the instance before beginning the import operation."
      },
      {
        "date": "2022-12-20T15:04:00.000Z",
        "voteCount": 3,
        "content": "It should be A given the amount of opened connections"
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 38,
    "url": "https://www.examtopics.com/discussions/google/view/92400-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "You are migrating your data center to Google Cloud. You plan to migrate your applications to Compute Engine and your Oracle databases to Bare Metal Solution for Oracle. You must ensure that the applications in different projects can communicate securely and efficiently with the Oracle databases. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up a Shared VPC, configure multiple service projects, and create firewall rules.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up Serverless VPC Access.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up Private Service Connect.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up Traffic Director."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-03-12T11:31:00.000Z",
        "voteCount": 2,
        "content": "A.\nB is wrong since Serverless VPC Access is for connecting to your VPC network from serverless environments (Cloud Run, App Engine, Cloud Functions). C is wrong as this concerns  private consumption of services across VPC networks that belong to different groups, teams, projects, or organizations. D is wrong because it concerns application networking for services. Nothing in its documentation mentions BMS. That leaves A. I would prefer to have seen something about VPC network peering, but the clincher is firewall rules which you would use to limit IP traffic sources to the backend Oracle DBs residing in their own Google managed VPC on BMS."
      },
      {
        "date": "2023-03-05T13:45:00.000Z",
        "voteCount": 2,
        "content": "The answer is for sure A\nread the following; https://medium.com/google-cloud/shared-vpc-in-google-cloud-64527e0a409e#:~:text=Unlike%20VPC%20peering%2C%20Shared%20VPC%20connects%20projects%20within%20the%20same%20organization.&amp;text=There%20are%20a%20lot%20of,between%20VPCs%20in%20different%20projects."
      },
      {
        "date": "2022-12-25T01:45:00.000Z",
        "voteCount": 3,
        "content": "A is the correct answer"
      },
      {
        "date": "2022-12-24T14:34:00.000Z",
        "voteCount": 3,
        "content": "B is not applicable here.\nC is also not the case - don't confuse it with Private Google Access https://cloud.google.com/bare-metal/docs/bms-security#enforce-a-secure-perimeter-with-private-google-access\nWe don't have a Service Mesh here, so D is also not an option.\n\nI go for A - https://cloud.google.com/bare-metal/docs/bms-security#:~:text=As%20shown%20in%20Figure%206%2C%20use%20a%20shared%20VPC%20architecture%20to%20allow%20resources%20from%20different%20projects%20to%20access%20the%20Bare%20Metal%20Solution%20servers"
      },
      {
        "date": "2022-12-21T16:17:00.000Z",
        "voteCount": 3,
        "content": "When you use Shared VPC, you designate a project as a host project and attach one or more other service projects to it."
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 39,
    "url": "https://www.examtopics.com/discussions/google/view/92401-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "You are running an instance of Cloud Spanner as the backend of your ecommerce website. You learn that the quality assurance (QA) team has doubled the number of their test cases. You need to create a copy of your Cloud Spanner database in a new test environment to accommodate the additional test cases. You want to follow Google-recommended practices. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Functions to run the export in Avro format.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Functions to run the export in text format.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Dataflow to run the export in Avro format.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Dataflow to run the export in text format."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-03-12T11:52:00.000Z",
        "voteCount": 5,
        "content": "C.\nA and B are wrong because Spanner exports are run as Dataflow jobs. The question says you need a copy of your entire database which means all the tables. You cannot export an entire database using the CSV (text) format, but you can using the Avro format. So that would make it the better option.\nhttps://cloud.google.com/spanner/docs/import-export-overview#file-format"
      },
      {
        "date": "2023-03-05T13:47:00.000Z",
        "voteCount": 1,
        "content": "Although I agree with C, I don't know why not D"
      },
      {
        "date": "2022-12-25T07:56:00.000Z",
        "voteCount": 2,
        "content": "C: Use Dataflow to run the export in Avro format."
      },
      {
        "date": "2022-12-25T01:54:00.000Z",
        "voteCount": 3,
        "content": "Answer is C, Dataflow and Avro format.\nCloud functions has timeout Gen-1 6mins Gen-2  1hr"
      },
      {
        "date": "2022-12-21T16:19:00.000Z",
        "voteCount": 4,
        "content": "Use Dataflow to run the export in Avro format.\n.\nhttps://cloud.google.com/spanner/docs/export"
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 40,
    "url": "https://www.examtopics.com/discussions/google/view/92699-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "You need to redesign the architecture of an application that currently uses Cloud SQL for PostgreSQL. The users of the application complain about slow query response times. You want to enhance your application architecture to offer sub-millisecond query latency. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Firestore, and modify your application to offload queries.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Bigtable, and modify your application to offload queries.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Cloud SQL for PostgreSQL read replicas to offload queries.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Memorystore, and modify your application to offload queries.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-25T07:55:00.000Z",
        "voteCount": 8,
        "content": "D: Configure Memorystore and modify your application to offload queries."
      },
      {
        "date": "2023-01-10T22:34:00.000Z",
        "voteCount": 6,
        "content": "The question says \"redesign the architecture of an application\" - Caching data using MemoryStore will trigger this. Read replica will require only updating connection details which would not be considered an application redesign."
      },
      {
        "date": "2023-09-26T02:46:00.000Z",
        "voteCount": 3,
        "content": "\"sub-millisecond latency\" always involves Memorystore. \nFurthermore, as we are talking about a relational DB (Cloud SQL), BigTable is not a solution to be considered."
      },
      {
        "date": "2024-01-20T01:42:00.000Z",
        "voteCount": 2,
        "content": "\"sub-milisecond latency\" always involves Bigtable if anything"
      },
      {
        "date": "2023-09-27T16:26:00.000Z",
        "voteCount": 1,
        "content": "MEMORY STORE IS ALSO A NO RELATIONAL DB LIKE  BIGTABLE"
      },
      {
        "date": "2023-09-05T18:11:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is ==&gt; B. Configure Bigtable, and modify your application to offload queries."
      },
      {
        "date": "2023-09-05T18:13:00.000Z",
        "voteCount": 1,
        "content": "Memorystore is an in-memory data store that can provide low-latency access to cached data, but it may not be suitable for all types of queries, and achieving sub-millisecond latency depends on factors such as data size and query complexity."
      },
      {
        "date": "2023-05-26T08:45:00.000Z",
        "voteCount": 2,
        "content": "The recommended approach is to configure Memorystore (Redis) and modify your application to offload queries. This will allow you to take advantage of the sub-millisecond query latency provided by in-memory caching and significantly improve the performance of your application."
      },
      {
        "date": "2023-04-26T10:34:00.000Z",
        "voteCount": 2,
        "content": "D. The only thing that achieves submilli seconds of response is Redis: https://blog.bytebytego.com/p/ep22-latency-numbers-you-should-know"
      },
      {
        "date": "2023-04-12T12:10:00.000Z",
        "voteCount": 1,
        "content": "D.\nTo meet demands of low latency at increased scale and reduced cost you need an in-memory datastore. Redis and Memchaced are among the most popular. Memorystore is a fully managed in-memory data store service for Redis and Memcached at Google Cloud."
      },
      {
        "date": "2023-03-12T12:01:00.000Z",
        "voteCount": 4,
        "content": "D.\nA is wrong since Firestore would not offer sub-millisecond response. C would help, but sub-millisecond would still be hard to achieve. The question gives no justification for Bigtable and sub-millisecond response does strongly suggest reads from memory rather than disk. That leaves D as the best answer."
      },
      {
        "date": "2023-01-26T07:14:00.000Z",
        "voteCount": 1,
        "content": "Ans is D"
      },
      {
        "date": "2023-01-04T01:08:00.000Z",
        "voteCount": 2,
        "content": "C: read replica"
      },
      {
        "date": "2023-01-03T07:51:00.000Z",
        "voteCount": 1,
        "content": "sub-millisecs -&gt; BigTable"
      },
      {
        "date": "2023-01-04T01:08:00.000Z",
        "voteCount": 1,
        "content": "it requires additional efforts to integrate BT"
      },
      {
        "date": "2022-12-25T02:52:00.000Z",
        "voteCount": 2,
        "content": "C is the correct answer"
      },
      {
        "date": "2022-12-25T10:00:00.000Z",
        "voteCount": 7,
        "content": "Sorry D is the correct answer\nhttps://cloud.google.com/blog/topics/developers-practitioners/what-memorystore/"
      },
      {
        "date": "2022-12-24T14:54:00.000Z",
        "voteCount": 2,
        "content": "Can't find any proper docs for this case, so let's try elimination. I'm not so sure though.\nA and C - don't give us sub-millisecond query latency.\nI don't see how Memorystore can help with queries, since it's cache, so eliminate D as well."
      },
      {
        "date": "2023-01-05T00:38:00.000Z",
        "voteCount": 1,
        "content": "C read replica with additional indexes give benefits. PG =&gt; BT dev efforts to migrate"
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 41,
    "url": "https://www.examtopics.com/discussions/google/view/92167-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "You need to migrate existing databases from Microsoft SQL Server 2016 Standard Edition on a single Windows Server 2019 Datacenter Edition to a single Cloud SQL for SQL Server instance. During the discovery phase of your project, you notice that your on-premises server peaks at around 25,000 read IOPS. You need to ensure that your Cloud SQL instance is sized appropriately to maximize read performance. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a SQL Server 2019 Standard on Standard machine type with 4 vCPUs, 15 GB of RAM, and 800 GB of solid-state drive (SSD).",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a SQL Server 2019 Standard on High Memory machine type with at least 16 vCPUs, 104 GB of RAM, and 200 GB of SSD.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a SQL Server 2019 Standard on High Memory machine type with 16 vCPUs, 104 GB of RAM, and 4 TB of SSD.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a SQL Server 2019 Enterprise on High Memory machine type with 16 vCPUs, 104 GB of RAM, and 500 GB of SSD."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 14,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-12-20T15:20:00.000Z",
        "voteCount": 9,
        "content": "Given that Google SSD performance is related to the size of the disk in an order of 30 IOPS for each GB, ti would require at least 833 GB to handle 25000 IOPS, the only answer that exceeds this value is C.\nhttps://cloud.google.com/compute/docs/disks/performance"
      },
      {
        "date": "2023-09-05T18:18:00.000Z",
        "voteCount": 2,
        "content": "The correct answer is ==&gt; B. Create a SQL Server 2019 Standard on High Memory machine type with at least 16 vCPUs, 104 GB of RAM, and 200 GB of SSD\n\nfocus on \"at least\" keyword"
      },
      {
        "date": "2023-03-12T12:16:00.000Z",
        "voteCount": 2,
        "content": "C.\nD is wrong since the IOPS would not improve based upon the edition of SQL Server. IOPS increases with the amount of storage, so the most amount of storage is C with 4 TB. I checked this using the GCP console and C is correct."
      },
      {
        "date": "2022-12-25T10:04:00.000Z",
        "voteCount": 2,
        "content": "Agree C is the correct answer"
      },
      {
        "date": "2022-12-25T07:54:00.000Z",
        "voteCount": 1,
        "content": "C: Create a SQL Server 2019 Standard on High Memory machine type with 16 vCPUs, ***** 104 GB of RAM, and 4 TB of SSD."
      },
      {
        "date": "2022-12-20T03:58:00.000Z",
        "voteCount": 3,
        "content": "A disk size of 4TB or greater provides more throughput and IOPS. Storage: &gt;= 4TB for the best IOPS https://cloud.google.com/sql/docs/sqlserver/best-practices#admin"
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 42,
    "url": "https://www.examtopics.com/discussions/google/view/92243-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "You are managing a small Cloud SQL instance for developers to do testing. The instance is not critical and has a recovery point objective (RPO) of several days. You want to minimize ongoing costs for this instance. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTake no backups, and turn off transaction log retention.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTake one manual backup per day, and turn off transaction log retention.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTurn on automated backup, and turn off transaction log retention.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTurn on automated backup, and turn on transaction log retention."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-06-17T10:03:00.000Z",
        "voteCount": 1,
        "content": "Why not A. its a testing environment. why we need to have backup when we are low on budget."
      },
      {
        "date": "2023-10-02T21:55:00.000Z",
        "voteCount": 1,
        "content": "C and not B because as per: https://cloud.google.com/sql/docs/mysql/backup-recovery/backups\nOn-demand backups are not automatically deleted the way automated backups are. They persist until you delete them or until their instance is deleted. Because they are not automatically deleted, on-demand backups can have a long-term effect on your billing charges."
      },
      {
        "date": "2023-09-26T02:47:00.000Z",
        "voteCount": 1,
        "content": "C is the one. \nManual backups are always discouraged, and transaction log can be removed for a cheap, dev DB."
      },
      {
        "date": "2023-04-27T02:37:00.000Z",
        "voteCount": 1,
        "content": "I think that is C but I didn't find any link that corroborates with my opinion \ud83d\ude22"
      },
      {
        "date": "2023-04-04T15:21:00.000Z",
        "voteCount": 1,
        "content": "B is manual process and can't be the right approach for automation"
      },
      {
        "date": "2023-03-12T12:21:00.000Z",
        "voteCount": 3,
        "content": "C.\nA is wrong since there is an RPO. B requires manual intervention which partly defeats the object of using a managed service like Cloud SQL. D is wrong since retaining transaction logs would permit point-in-time recovery which is not required. That leaves C."
      },
      {
        "date": "2023-01-10T22:43:00.000Z",
        "voteCount": 1,
        "content": "Automatic backups are incremental where as manual backups are full. Other than compute time for manual backup, storage costs will also increase."
      },
      {
        "date": "2022-12-25T10:06:00.000Z",
        "voteCount": 3,
        "content": "C. Turn on automated backup, and turn off transaction log retention."
      },
      {
        "date": "2022-12-25T07:54:00.000Z",
        "voteCount": 1,
        "content": "C: Turn on automated backup, and turn off transaction log retention."
      },
      {
        "date": "2022-12-20T15:22:00.000Z",
        "voteCount": 2,
        "content": "There is no need to have the overload of using a manual backup, you could schedule an automatic one once a day"
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 43,
    "url": "https://www.examtopics.com/discussions/google/view/92244-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "You manage a meeting booking application that uses Cloud SQL. During an important launch, the Cloud SQL instance went through a maintenance event that resulted in a downtime of more than 5 minutes and adversely affected your production application. You need to immediately address the maintenance issue to prevent any unplanned events in the future. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet your production instance's maintenance window to non-business hours.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the Cloud SQL instance to Cloud Spanner to avoid any future disruptions due to maintenance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tContact Support to understand why your Cloud SQL instance had a downtime of more than 5 minutes.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Scheduler to schedule a maintenance window of no longer than 5 minutes."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 13,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-12-20T15:37:00.000Z",
        "voteCount": 5,
        "content": "A makes more sense than changing products"
      },
      {
        "date": "2024-01-18T09:26:00.000Z",
        "voteCount": 1,
        "content": "Selected Answer: A"
      },
      {
        "date": "2023-12-26T05:31:00.000Z",
        "voteCount": 1,
        "content": "Changing service is too risky. ==&gt; A"
      },
      {
        "date": "2023-03-12T15:00:00.000Z",
        "voteCount": 1,
        "content": "A.\nMigrating to a different platform is a bit extreme, so eliminate B. C would be a good idea in any event, but is not the best answer here. D is not how you manage maintenance windows on Cloud SQL. A is the best answer."
      },
      {
        "date": "2022-12-25T10:18:00.000Z",
        "voteCount": 4,
        "content": "A is the correct answer"
      },
      {
        "date": "2022-12-25T07:53:00.000Z",
        "voteCount": 2,
        "content": "A: Set your production instance's maintenance window to ***** non-business hours."
      },
      {
        "date": "2022-12-23T13:39:00.000Z",
        "voteCount": 3,
        "content": "maintenance window"
      },
      {
        "date": "2022-12-21T11:53:00.000Z",
        "voteCount": 2,
        "content": "Correct Answer is A"
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 44,
    "url": "https://www.examtopics.com/discussions/google/view/92703-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "You are designing a highly available (HA) Cloud SQL for PostgreSQL instance that will be used by 100 databases. Each database contains 80 tables that were migrated from your on-premises environment to Google Cloud. The applications that use these databases are located in multiple regions in the US, and you need to ensure that read and write operations have low latency. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy 2 Cloud SQL instances in the us-central1 region with HA enabled, and create read replicas in us-east1 and us-west1.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy 2 Cloud SQL instances in the us-central1 region, and create read replicas in us-east1 and us-west1.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy 4 Cloud SQL instances in the us-central1 region with HA enabled, and create read replicas in us-central1, us-east1, and us-west1.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy 4 Cloud SQL instances in the us-central1 region, and create read replicas in us-central1, us-east1 and us-west1."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 12,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 6,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-01-06T03:23:00.000Z",
        "voteCount": 8,
        "content": "A is correct. We only have 8000 tables. More than 1 HA env is only required beyond 50000 tables\nIf you have 50,000 or more database tables on a single instance, it could result in the instance becoming unresponsive or unable to perform maintenance operations, and the instance is not covered by the SLA."
      },
      {
        "date": "2023-03-05T14:16:00.000Z",
        "voteCount": 1,
        "content": "although I agree with your answe, table limit is for mysql and not postgresql\nhttps://cloud.google.com/sql/docs/mysql/quotas#table_limit"
      },
      {
        "date": "2023-03-12T15:13:00.000Z",
        "voteCount": 7,
        "content": "A.\nB and D do not mention HA, so eliminate those. That leaves A and C. C talks about 4 instances with HA which presumably means 2 primaries each with an HA standby. Oddly, there are 4 zones in us-central1. The killer is having a read replica also in us-central1 which would mean the same zone would have a read replica and either a primary or HA standby. Not a good idea. Option A is the best choice. A primary and an HA standby in us-central1 (different zones) and then read replicas in us-east1 and us-west1."
      },
      {
        "date": "2024-01-19T05:32:00.000Z",
        "voteCount": 2,
        "content": "as I focus on low latency"
      },
      {
        "date": "2023-11-25T08:39:00.000Z",
        "voteCount": 1,
        "content": "The answer is C. you need a instance for a read replica not 2 per read replica."
      },
      {
        "date": "2023-10-23T07:17:00.000Z",
        "voteCount": 4,
        "content": "Answer C is wrong because the question suggests low-latency and replication for 4 region INCREASE latency.\nAnswer A is correct because attend the two requirements high availability and low latency.\nThe Key for this question is low latency."
      },
      {
        "date": "2024-05-20T08:35:00.000Z",
        "voteCount": 1,
        "content": "Thanks for the clarification."
      },
      {
        "date": "2023-09-05T18:30:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is == C. Deploy 4 Cloud SQL instances in the us-central1 region with HA enabled, and create read replicas in us-central1, us-east1, and us-west1."
      },
      {
        "date": "2024-02-21T21:09:00.000Z",
        "voteCount": 1,
        "content": "we can't have both primary and read replica in same region, it will not be considered as HA\ni will stick with A."
      },
      {
        "date": "2023-05-27T20:08:00.000Z",
        "voteCount": 3,
        "content": "This option provides high availability with four instances in the primary region, ensuring redundancy and fault tolerance. By creating read replicas in us-central1, us-east1, and us-west1, read operations can be distributed across multiple regions, reducing latency for applications in those regions. This design allows for efficient and low-latency read operations while maintaining high availability.\nOption C is the recommended choice as it combines HA with multiple read replicas in different regions, providing both high availability and low-latency read operations for your multi-region application setup."
      },
      {
        "date": "2022-12-28T00:13:00.000Z",
        "voteCount": 3,
        "content": "Why not \" C\" is an answer"
      },
      {
        "date": "2022-12-25T07:52:00.000Z",
        "voteCount": 4,
        "content": "A: Deploy 2 Cloud SQL instances in the us-central1 region ***** with HA enabled, and create read replicas in us-east1 and us-west1."
      },
      {
        "date": "2022-12-24T15:22:00.000Z",
        "voteCount": 6,
        "content": "We only need options with HA enabled and 4 databases with 3 read replicas (each?) seems overkill to me."
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 45,
    "url": "https://www.examtopics.com/discussions/google/view/92707-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "You work in the logistics department. Your data analysis team needs daily extracts from Cloud SQL for MySQL to train a machine learning model. The model will be used to optimize next-day routes. You need to export the data in CSV format. You want to follow Google-recommended practices. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Scheduler to trigger a Cloud Function that will run a select * from table(s) query to call the cloudsql.instances.export API.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Scheduler to trigger a Cloud Function through Pub/Sub to call the cloudsql.instances.export API.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Composer to orchestrate an export by calling the cloudsql.instances.export API.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Composer to execute a select * from table(s) query and export results."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 12,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 5,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-24T15:26:00.000Z",
        "voteCount": 8,
        "content": "https://cloud.google.com/blog/topics/developers-practitioners/scheduling-cloud-sql-exports-using-cloud-functions-and-cloud-scheduler"
      },
      {
        "date": "2024-04-29T10:39:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/sql/docs/mysql/backup-recovery/scheduling-backups#creating_a_pub_sub_topic_a_cloud_function_and_a_cloud_scheduler_job"
      },
      {
        "date": "2023-11-04T17:12:00.000Z",
        "voteCount": 2,
        "content": "C is correct."
      },
      {
        "date": "2023-09-24T23:33:00.000Z",
        "voteCount": 1,
        "content": "The most accurate answer based on Google-recommended practices would be C"
      },
      {
        "date": "2023-05-27T20:32:00.000Z",
        "voteCount": 2,
        "content": "C is the recommended choice as it leverages Cloud Composer's workflow orchestration capabilities to schedule and automate the export process. By calling the cloudsql.instances.export API within the workflow, you can ensure that the data is exported from Cloud SQL for MySQL in CSV format as needed by your data analysis team."
      },
      {
        "date": "2023-05-27T20:17:00.000Z",
        "voteCount": 1,
        "content": "Cloud Composer is a fully managed workflow orchestration service. It allows you to define and manage complex workflows using Apache Airflow. By using Cloud Composer, you can create a workflow that includes a task to export data from Cloud SQL for MySQL using the cloudsql.instances.export API. You can specify the export format as CSV to meet the requirement of your data analysis team. This approach provides a scalable and manageable solution for regular data exports.\nTherefore, option C is the recommended choice as it leverages Cloud Composer's workflow orchestration capabilities to schedule and automate the export process. By calling the cloudsql.instances.export API within the workflow, you can ensure that the data is exported from Cloud SQL for MySQL in CSV format as needed by your data analysis team."
      },
      {
        "date": "2023-03-12T15:27:00.000Z",
        "voteCount": 4,
        "content": "B.\nPerforming a \u201cSELECT * FROM TABLE\u201d wouldn\u2019t give you CSV output and that alone wouldn\u2019t call an API. Eliminate A. There\u2019s no need for Cloud Composer in this scenario, especially when the solution is a known combination of Cloud Scheduler, Cloud Function and a Pub/Sub Topic, which is B."
      },
      {
        "date": "2023-09-12T03:19:00.000Z",
        "voteCount": 3,
        "content": "why will someone use 3 services when it can be done using 1 service only, in this case composer. Secondly export can be for multiple tables, and ,cloud functions has a execution time limit.?\nC still looks most appropriate solution."
      },
      {
        "date": "2023-03-06T07:36:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/blog/topics/developers-practitioners/scheduling-cloud-sql-exports-using-cloud-functions-and-cloud-scheduler"
      },
      {
        "date": "2023-01-13T01:42:00.000Z",
        "voteCount": 2,
        "content": "what if the query is large, Cloud Function has limit? shouldn't it be C?"
      },
      {
        "date": "2022-12-30T16:26:00.000Z",
        "voteCount": 3,
        "content": "As noted in text from this link, Cloud Function can be triggered with/without pub/sub.  I think distinction between A &amp; B is suggestion in A the API s called from query, which is not correct - the 'select query' is passed as parameter in call, so I think B is correct.\n\n\"Note that we could also have a Pub/Sub Trigger configured Cloud Function get triggered by the Scheduler by using the same steps given above to create a job. Except that in the target, you will select Pub/Sub and provide the Pub/Sub Topic name. At the scheduler trigger time, the Cloud Scheduler will publish a message to the topic with the message body that you specify.\"\n\nhttps://rominirani.com/google-cloud-functions-tutorial-using-the-cloud-scheduler-to-trigger-your-functions-756160a95c43"
      },
      {
        "date": "2022-12-28T00:20:00.000Z",
        "voteCount": 3,
        "content": "A is the correct answer, PUSUB is not required for this task"
      },
      {
        "date": "2023-09-15T11:55:00.000Z",
        "voteCount": 1,
        "content": "Doing a SELECT * won't call the API. Pub/Sub is not needed, you are correct, the problem is the SELECT * (nothing to do there)."
      },
      {
        "date": "2022-12-25T07:51:00.000Z",
        "voteCount": 2,
        "content": "A: Use Cloud Scheduler to trigger a Cloud Function that will run a select * from table(s) query to call the cloudsql.instances.export *** API."
      },
      {
        "date": "2023-02-21T16:01:00.000Z",
        "voteCount": 2,
        "content": "The answer is letter B. Google recomends this pattern to perform this task: https://cloud.google.com/sql/docs/mysql/backup-recovery/scheduling-backups"
      },
      {
        "date": "2023-09-15T11:55:00.000Z",
        "voteCount": 1,
        "content": "Doing a SELECT * won't call the API. Pub/Sub is not needed, you are correct, the problem is the SELECT * (nothing to do there)."
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 46,
    "url": "https://www.examtopics.com/discussions/google/view/92248-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "You are choosing a database backend for a new application. The application will ingest data points from IoT sensors. You need to ensure that the application can scale up to millions of requests per second with sub-10ms latency and store up to 100 TB of history. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud SQL with read replicas for throughput.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Firestore, and rely on automatic serverless scaling.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Memorystore for Memcached, and add nodes as necessary to achieve the required throughput.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Bigtable, and add nodes as necessary to achieve the required throughput.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 12,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-20T15:57:00.000Z",
        "voteCount": 7,
        "content": "This is a bigtable use case"
      },
      {
        "date": "2024-06-17T09:13:00.000Z",
        "voteCount": 1,
        "content": "point to consider is :\nIOT data,\nscaling for accepting millions of request.\nstoring historic data of 100TB.\nBig Table is used for low latency but its for operational workload and \nwe need Fully managed Redis and Memcached for sub-millisecond data access.\n\nthat's the only argument here.\n\ni think option c is correct."
      },
      {
        "date": "2024-06-17T09:14:00.000Z",
        "voteCount": 1,
        "content": "https://medium.com/google-cloud/which-database-should-i-choose-44be039179ea"
      },
      {
        "date": "2023-09-26T02:53:00.000Z",
        "voteCount": 1,
        "content": "Key is 100TB of historical data.\nBigTable is the only service (among the mentioned) that is able to handle that load within the required latency."
      },
      {
        "date": "2023-06-18T12:58:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is C"
      },
      {
        "date": "2023-03-12T14:53:00.000Z",
        "voteCount": 2,
        "content": "D.\nSimply a classic use case for Bigtable. Neither Cloud SQL, Firestore nor Memorystore have either the capacity or latency to provide the solution."
      },
      {
        "date": "2023-01-06T11:49:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/memorystore/docs/redis/redis-overview#what_its_good_for\nWhat it's good for:\n\nStream Processing: Whether processing a Twitter feed or stream of data from IoT devices, Memorystore for Redis is a perfect fit for streaming solutions.\nCorrect answer is C"
      },
      {
        "date": "2023-04-27T03:19:00.000Z",
        "voteCount": 1,
        "content": "You just forgot the 100TB of data that needs to be stored \ud83d\ude4f"
      },
      {
        "date": "2022-12-28T00:46:00.000Z",
        "voteCount": 2,
        "content": "C is not a correct answer, D- Bigtable is correct choice for this user case.\nhttps://cloud.google.com/memorystore/docs/redis/redis-overview"
      },
      {
        "date": "2022-12-21T16:24:00.000Z",
        "voteCount": 2,
        "content": "IOT --&gt;&gt; Bigtable"
      },
      {
        "date": "2022-12-21T11:57:00.000Z",
        "voteCount": 3,
        "content": "Answer is D"
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 47,
    "url": "https://www.examtopics.com/discussions/google/view/92249-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "You are designing a payments processing application on Google Cloud. The application must continue to serve requests and avoid any user disruption if a regional failure occurs. You need to use AES-256 to encrypt data in the database, and you want to control where you store the encryption key. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Spanner with a customer-managed encryption key (CMEK).\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Spanner with default encryption.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud SQL with a customer-managed encryption key (CMEK).",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Bigtable with default encryption."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 8,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-07-31T07:58:00.000Z",
        "voteCount": 1,
        "content": "A, failt region"
      },
      {
        "date": "2023-03-12T13:25:00.000Z",
        "voteCount": 4,
        "content": "A.\nAvoiding user disruption if a regional failure occurs means you need to pick a multi-region service. That rules out C and D. Having more control over the EKs means CMEK. That eliminates B."
      },
      {
        "date": "2022-12-28T00:49:00.000Z",
        "voteCount": 4,
        "content": "A is the correct answer because \"  you want to control where you store the encryption key\""
      },
      {
        "date": "2022-12-25T07:49:00.000Z",
        "voteCount": 1,
        "content": "C: Use Cloud SQL with a customer-managed encryption key (CMEK)."
      },
      {
        "date": "2022-12-24T15:40:00.000Z",
        "voteCount": 2,
        "content": "A for me: it's A or C because we want to control keys, but C would cause downtime, since we would need to manually failover to another region if a regional failure occurs, and we don't want that."
      },
      {
        "date": "2022-12-22T20:41:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer - A"
      },
      {
        "date": "2022-12-21T16:28:00.000Z",
        "voteCount": 1,
        "content": "I guess B\nSince Google cloud default encryption comes with AES-256 encryption"
      },
      {
        "date": "2023-01-21T20:58:00.000Z",
        "voteCount": 1,
        "content": "Yes default encryption comes with AES-256 but the question states that you need to control where you store the encryption keys. that can be achieved by CMEK."
      },
      {
        "date": "2022-12-20T15:58:00.000Z",
        "voteCount": 2,
        "content": "A and C would work for this scenario"
      },
      {
        "date": "2022-12-24T15:38:00.000Z",
        "voteCount": 2,
        "content": "Right, but C would cause downtime, since you would need to manually failover to another region"
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 48,
    "url": "https://www.examtopics.com/discussions/google/view/92403-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "You are managing a Cloud SQL for MySQL environment in Google Cloud. You have deployed a primary instance in Zone A and a read replica instance in Zone B, both in the same region. You are notified that the replica instance in Zone B was unavailable for 10 minutes. You need to ensure that the read replica instance is still working. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Google Cloud Console or gcloud CLI to manually create a new clone database.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Google Cloud Console or gcloud CLI to manually create a new failover replica from backup.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tVerify that the new replica is created automatically.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStart the original primary instance and resume replication."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 8,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 6,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-05-20T11:12:00.000Z",
        "voteCount": 1,
        "content": "Agree with C"
      },
      {
        "date": "2024-03-29T11:37:00.000Z",
        "voteCount": 1,
        "content": "For MySQL, this should be C"
      },
      {
        "date": "2024-02-01T05:31:00.000Z",
        "voteCount": 1,
        "content": "For MySQL, this should be C"
      },
      {
        "date": "2024-01-28T18:10:00.000Z",
        "voteCount": 2,
        "content": "When a Cloud SQL read replica becomes unavailable, Cloud SQL automatically tries to create a new replica to replace the failed one. This process is part of the automated failover mechanism in Cloud SQL.\nB. Manually creating a new failover replica from backup might involve additional steps and may not be necessary for a temporary unavailability of the read replica."
      },
      {
        "date": "2023-12-20T04:49:00.000Z",
        "voteCount": 1,
        "content": "\"During a zonal outage, traffic stops to read replicas in that zone. After the zone becomes available again, any read replicas in the zone resume replication from the primary instance.\"\nhttps://cloud.google.com/sql/docs/mysql/high-availability#read_replicas"
      },
      {
        "date": "2023-11-26T05:48:00.000Z",
        "voteCount": 1,
        "content": "C is the correct answer.\n\"During a zonal outage, traffic stops to read replicas in that zone. After the zone becomes available again, any read replicas in the zone resume replication from the primary instance.\"\nhttps://cloud.google.com/sql/docs/mysql/high-availability#read_replicas"
      },
      {
        "date": "2023-12-16T08:12:00.000Z",
        "voteCount": 1,
        "content": "I think is D, if sth wrong it didn;t creat automatically. So B is correct"
      },
      {
        "date": "2023-12-17T10:18:00.000Z",
        "voteCount": 1,
        "content": "srr   it is B, I type it by mistake"
      },
      {
        "date": "2023-09-05T19:11:00.000Z",
        "voteCount": 1,
        "content": "B is correct answer. Creating a new failover replica from a backup is a reliable way to restore replication and ensure that the read replica is up-to-date."
      },
      {
        "date": "2023-07-24T13:09:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2023-06-23T22:20:00.000Z",
        "voteCount": 2,
        "content": "C\n\nRecovery Process: Once Zone-B becomes available again, Cloud SQL will initiate the recovery process for the impacted read replica. The recovery process involves the following steps:\n\n1. Synchronization: Cloud SQL will compare the data in the recovered read replica with the primary instance in Zone-A. If there is any data divergence due to the unavailability period, Cloud SQL will synchronize the read replica with the primary instance to ensure data consistency.\n\n2. Catch-up Replication: The recovered read replica will start catching up on the changes that occurred on the primary instance during its unavailability. It will apply the necessary updates from the primary instance's binary logs (binlogs) to bring the replica up to date.\n\n3. Resuming Read Traffic: Once the synchronization and catch-up replication processes are complete, the read replica in Zone-B will resume its normal operation. It will be able to serve read traffic and stay updated with subsequent changes from the primary instance."
      },
      {
        "date": "2023-05-27T20:40:00.000Z",
        "voteCount": 2,
        "content": "By verifying that the new replica is created automatically, you can ensure that the read replica instance is functioning and replication is maintained even after the temporary unavailability of the replica in Zone B."
      },
      {
        "date": "2023-06-16T08:10:00.000Z",
        "voteCount": 1,
        "content": "A read replica is not recreated automatically."
      },
      {
        "date": "2023-04-04T15:43:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/sql/docs/mysql/replication#:~:text=If%20replication%20is,a%20new%20one."
      },
      {
        "date": "2023-04-04T15:41:00.000Z",
        "voteCount": 1,
        "content": "Resume replication"
      },
      {
        "date": "2023-01-10T09:12:00.000Z",
        "voteCount": 2,
        "content": "B is wrong, failover replica is NOT a read replica\nA and C makes no sense"
      },
      {
        "date": "2022-12-30T17:08:00.000Z",
        "voteCount": 1,
        "content": "How is B correct?  You don't create a replica from backup, and there's no mention of HA that would point to \"failover\" distinction. But don't like other answers either.  Will go with B unless enlightened by subsequent contributor,"
      },
      {
        "date": "2022-12-28T01:24:00.000Z",
        "voteCount": 3,
        "content": "B is the correct answer"
      },
      {
        "date": "2022-12-27T02:31:00.000Z",
        "voteCount": 2,
        "content": "Vote for B"
      },
      {
        "date": "2022-12-27T02:21:00.000Z",
        "voteCount": 4,
        "content": "C - makes no sense\nD - nobody said primary instance was offline, plus you can't stop/resture replication on the primary instance, only on read replica\nA makes no sense: you can't create clone database from read replica, and if it means to create one from the primary instance - how would that help to insure that replica is still working?\nI'll go for B"
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 49,
    "url": "https://www.examtopics.com/discussions/google/view/92754-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "You are migrating an on-premises application to Google Cloud. The application requires a high availability (HA) PostgreSQL database to support business-critical functions. Your company's disaster recovery strategy requires a recovery time objective (RTO) and recovery point objective (RPO) within 30 minutes of failure. You plan to use a Google Cloud managed service. What should you do to maximize uptime for your application?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy Cloud SQL for PostgreSQL in a regional configuration. Create a read replica in a different zone in the same region and a read replica in another region for disaster recovery.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy Cloud SQL for PostgreSQL in a regional configuration with HA enabled. Take periodic backups, and use this backup to restore to a new Cloud SQL for PostgreSQL instance in another region during a disaster recovery event.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy Cloud SQL for PostgreSQL in a regional configuration with HA enabled. Create a cross-region read replica, and promote the read replica as the primary node for disaster recovery.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the PostgreSQL database to multi-regional Cloud Spanner so that a single region outage will not affect your application. Update the schema to support Cloud Spanner data types, and refactor the application."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-03-12T15:57:00.000Z",
        "voteCount": 4,
        "content": "C.\nD might be possible but it\u2019s a lot of effort to migrate to a different platform. Eliminate D. A does not mention HA. Eliminate A. B says to take periodic backups which doesn\u2019t support an RTO/RPO of 30 minutes. The best answer is deploy an HA configuration and have a read replica you could promote to the primary in a different region. C is the best answer."
      },
      {
        "date": "2022-12-28T01:36:00.000Z",
        "voteCount": 3,
        "content": "C is the correct answer"
      },
      {
        "date": "2022-12-25T07:48:00.000Z",
        "voteCount": 1,
        "content": "C: Deploy Cloud SQL for PostgreSQL in a regional configuration with HA enabled. Create a cross-region read replica, and promote ***** the read replica as the primary node for disaster recovery."
      },
      {
        "date": "2022-12-25T02:48:00.000Z",
        "voteCount": 1,
        "content": "I'll go with C"
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 50,
    "url": "https://www.examtopics.com/discussions/google/view/92405-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "Your team is running a Cloud SQL for MySQL instance with a 5 TB database that must be available 24/7. You need to save database backups on object storage with minimal operational overhead or risk to your production workloads. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud SQL serverless exports.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a read replica, and then use the mysqldump utility to export each table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tClone the Cloud SQL instance, and then use the mysqldump utlity to export the data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the mysqldump utility on the primary database instance to export the backup."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 9,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-28T02:35:00.000Z",
        "voteCount": 5,
        "content": "A is the correct answer\nhttps://cloud.google.com/blog/products/databases/introducing-cloud-sql-serverless-exports"
      },
      {
        "date": "2024-05-02T05:10:00.000Z",
        "voteCount": 1,
        "content": "A: https://cloud.google.com/blog/products/databases/introducing-cloud-sql-serverless-exports\n\nServerless exports enables you to export data from your MySQL and PostgreSQL database instances without any impact on performance or risk to your production workloads."
      },
      {
        "date": "2023-03-12T12:35:00.000Z",
        "voteCount": 4,
        "content": "A.\nMinimal operational overhead eliminates B and C. Minimal risk to production workloads eliminates D. That leaves A. Least amount of work and doesn't impact the primary instance."
      },
      {
        "date": "2023-03-06T08:02:00.000Z",
        "voteCount": 1,
        "content": "Cloud SQL backups are incremental. They contain only data that changed after the previous backup was taken. Your oldest backup is a similar size to your database, but the sizes of subsequent backups depend on the rate of change of your data. When the oldest backup is deleted, the size of the next oldest backup increases so that a full backup still exists."
      },
      {
        "date": "2023-03-06T08:00:00.000Z",
        "voteCount": 1,
        "content": "With serverless export, Cloud SQL creates a separate, temporary instance to offload the export operation. Offloading the export operation allows databases on the primary instance to continue to serve queries and perform operations at the usual performance rate. BUT is is export (logical backup) and will never be incremental. and the recovery is slow. for 5TB server is it not an option. (only for mini databases). I believe The better option is C"
      },
      {
        "date": "2022-12-25T07:47:00.000Z",
        "voteCount": 1,
        "content": "A: Use Cloud SQL serverless exports.\nServerless exports enables you to export data from your MySQL and PostgreSQL database instances without any impact on performance or risk to your production workloads. Cloud SQL exports, which offer portable data formats (SQL, CSV), can be triggered anytime and are written to Cloud Storage buckets that you control."
      },
      {
        "date": "2022-12-25T02:24:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/sql/docs/mysql/import-export#:~:text=Use%20serverless%20export,is%20deleted%20automatically."
      },
      {
        "date": "2022-12-22T20:59:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer - A"
      },
      {
        "date": "2022-12-21T16:34:00.000Z",
        "voteCount": 1,
        "content": "Use Cloud SQL serverless exports."
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 51,
    "url": "https://www.examtopics.com/discussions/google/view/92750-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "You are deploying a new Cloud SQL instance on Google Cloud using the Cloud SQL Auth proxy. You have identified snippets of application code that need to access the new Cloud SQL instance. The snippets reside and execute on an application server running on a Compute Engine machine. You want to follow Google-recommended practices to set up Identity and Access Management (IAM) as quickly and securely as possible. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFor each application code, set up a common shared user account.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFor each application code, set up a dedicated user account.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFor the application server, set up a service account.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFor the application server, set up a common shared user account."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-08-15T03:37:00.000Z",
        "voteCount": 1,
        "content": "the best answer would be B . Need granular control for each of the code snippets . The advantage of using a service account for this purpose is that you can create a credential file specifically for the Cloud SQL Auth Proxy, and it is explicitly and permanently linked to the Cloud SQL Auth Proxy as long as it is running. For this reason, using a service account is the recommended method for production instances not running on a Compute Engine instance."
      },
      {
        "date": "2023-04-27T04:05:00.000Z",
        "voteCount": 1,
        "content": "C. The docs proving this are here: Create and configure a Google Cloud service account that has the Cloud SQL Client role with permissions to connect to Cloud SQL."
      },
      {
        "date": "2023-03-12T15:59:00.000Z",
        "voteCount": 2,
        "content": "C.\nThe Google recommendation would be for an application to use a service account. None of the other options make sense in light of this."
      },
      {
        "date": "2022-12-28T02:39:00.000Z",
        "voteCount": 3,
        "content": "Correct answer is C, service account is the GCP recommended option"
      },
      {
        "date": "2022-12-25T07:45:00.000Z",
        "voteCount": 1,
        "content": "B: For each application code, set up a dedicated ***** user account."
      },
      {
        "date": "2022-12-25T02:28:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/sql/docs/mysql/sql-proxy#using-a-service-account"
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 52,
    "url": "https://www.examtopics.com/discussions/google/view/92406-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "Your organization is running a low-latency reporting application on Microsoft SQL Server. In addition to the database engine, you are using SQL Server Analysis Services (SSAS), SQL Server Reporting Services (SSRS), and SQL Server Integration Services (SSIS) in your on-premises environment. You want to migrate your Microsoft SQL Server database instances to Google Cloud. You need to ensure minimal disruption to the existing architecture during migration. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate to Cloud SQL for SQL Server.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate to Cloud SQL for PostgreSQL.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate to Compute Engine.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate to Google Kubernetes Engine (GKE)."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-03-12T16:03:00.000Z",
        "voteCount": 5,
        "content": "C.\nCloud SQL doesn\u2019t support SSAS, SSRS or SSIS. Eliminate A. Migrating to a different database platform doesn\u2019t make sense. Eliminate B. Minimal disruption to the existing architecture rules out GKE. Eliminate D. Leaves C. Migrate to a GCE VM."
      },
      {
        "date": "2024-05-02T05:14:00.000Z",
        "voteCount": 1,
        "content": "Still C: https://cloud.google.com/sql/docs/sqlserver/features\n\neven though SSRS and SSIS are available (running runs a separate host and connects to Cloud SQL) SSAS is not supported."
      },
      {
        "date": "2023-09-26T02:56:00.000Z",
        "voteCount": 1,
        "content": "You'd need GCE, as Cloud SQL does not support SSAS, SRS and SSIS, as of September 26th, 2023.\n\nhttps://cloud.google.com/sql/docs/sqlserver/features"
      },
      {
        "date": "2023-04-04T15:53:00.000Z",
        "voteCount": 3,
        "content": "I always wonder why the answers provided are wrong! so wrong and not even close to the reality! why examtopics.com are doing this?"
      },
      {
        "date": "2023-10-02T20:46:00.000Z",
        "voteCount": 2,
        "content": "They really should just get rid of the answers and just have the discussion unless they plan on making the answers or at least more accurate."
      },
      {
        "date": "2023-12-16T11:11:00.000Z",
        "voteCount": 2,
        "content": "If they provide the answer as well, you will never see the exam topic any more"
      },
      {
        "date": "2023-04-21T05:48:00.000Z",
        "voteCount": 6,
        "content": "I think they do this on purpose.\nThey simply give a random answer on each question and let us fight in the comment section to find the truth answer. The good thing is that we have to work out ass to find the correct answer and we might learn something new along the way (or at least in my case) :D"
      },
      {
        "date": "2023-04-27T04:23:00.000Z",
        "voteCount": 3,
        "content": "I have to agree 100% witg absero1609 kkkkkkkkkkkkk, researching in the comments and reading the docs that actually let me learn fast for the exam."
      },
      {
        "date": "2023-03-07T04:10:00.000Z",
        "voteCount": 1,
        "content": "C. Migrate to Compute Engine is the minimal disruption method. customer will keep his SA logins and other system/dba/admin capabilities.  and the SSAS SRS SSIS solutions will be migrated as well as \"lift &amp; shift\" method."
      },
      {
        "date": "2022-12-28T02:44:00.000Z",
        "voteCount": 3,
        "content": "C is the correct answer\nhttps://cloud.google.com/sql/docs/sqlserver/features#general_features_unavailable_for"
      },
      {
        "date": "2022-12-25T07:45:00.000Z",
        "voteCount": 2,
        "content": "The following features are unavailable in Cloud SQL and not supported by Google Cloud support.\nGeneral features unavailable for Cloud SQL\n\u2022\tSQL Server Reporting Services (SSRS)\n\u2022\tSQL Server Analysis Services (SSAS)\n\u2022\tSQL Server Integration Services (SSIS)"
      },
      {
        "date": "2022-12-25T07:44:00.000Z",
        "voteCount": 1,
        "content": "C: Migrate to Compute Engine"
      },
      {
        "date": "2022-12-25T02:31:00.000Z",
        "voteCount": 1,
        "content": "SSAS, SSRS, SSIS are unavailable for CloudSQL https://cloud.google.com/sql/docs/sqlserver/features"
      },
      {
        "date": "2022-12-21T16:35:00.000Z",
        "voteCount": 1,
        "content": "I guess C\nMigrate to Compute Engine\nBecause Cloud SQL doesn't support SSAS...."
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 53,
    "url": "https://www.examtopics.com/discussions/google/view/92751-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "An analytics team needs to read data out of Cloud SQL for SQL Server and update a table in Cloud Spanner. You need to create a service account and grant least privilege access using predefined roles. What roles should you assign to the service account?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\troles/cloudsql.viewer and roles/spanner.databaseUser\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\troles/cloudsql.editor and roles/spanner.admin",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\troles/cloudsql.client and roles/spanner.databaseReader",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\troles/cloudsql.instanceUser and roles/spanner.databaseUser"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 13,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 5,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-04-23T01:33:00.000Z",
        "voteCount": 1,
        "content": "Should be A. Because roles/cloudsql.instanceUser has the cloudsql.instances.get role, which has the following roles included:\nCloud SQL Admin\nCloud SQL Client\nCloud SQL Editor\nCloud SQL Viewer\n\nThis compromise \u201cleast privilege\u201d requirements"
      },
      {
        "date": "2024-01-30T15:20:00.000Z",
        "voteCount": 1,
        "content": "I think it should be A because the roles/cloudsql.instanceUser role only has:\ncloudsql.instances.get\ncloudsql.instances.login\n\nYou won't even be able to view anything with that role.\n\nhttps://cloud.google.com/sql/docs/mysql/iam-roles"
      },
      {
        "date": "2024-01-21T07:13:00.000Z",
        "voteCount": 1,
        "content": "To me, it is also \"D\", InstanceUser only has 2 permissions and Viewer has like 50 of them"
      },
      {
        "date": "2023-09-05T21:18:00.000Z",
        "voteCount": 2,
        "content": "The current answer is =&gt; D. roles/cloudsql.instanceUser and roles/spanner.databaseUser. roles/cloudsql.instanceUser: This role allows the service account to connect to Cloud SQL instances"
      },
      {
        "date": "2023-09-22T07:49:00.000Z",
        "voteCount": 1,
        "content": "I think you are missing out the \u201cleast privilege\u201d part"
      },
      {
        "date": "2023-09-25T21:19:00.000Z",
        "voteCount": 1,
        "content": "No I am not. pay attention to \"read data out of Cloud SQL\"."
      },
      {
        "date": "2023-09-03T06:57:00.000Z",
        "voteCount": 3,
        "content": "Will go by A"
      },
      {
        "date": "2023-07-03T09:19:00.000Z",
        "voteCount": 4,
        "content": "Correct Ans = A\nExplanation: To read data out of Cloud SQL for SQL Server, you need to use a service account with the roles/cloudsql.viewer role on the Cloud SQL instance. \nThis role grants the service account permission to read data from the instance. Whereas roles/cloudsql.instanceUser will only allow to login to cloud SQL instance. No resource will be allowed to view.\n\t\t\t \nTo update a table in Cloud Spanner, you need to use a service account with the roles/spanner.databaseUser role on the Cloud Spanner instance. \nThis role grants the service account permission to read and write data in the Spanner database."
      },
      {
        "date": "2023-05-27T22:30:00.000Z",
        "voteCount": 1,
        "content": "between A or D. But roles/cloudsql.viewer to broad, so i choose D"
      },
      {
        "date": "2023-04-27T04:40:00.000Z",
        "voteCount": 1,
        "content": "D. I think that instanceUser had the necessary permissions to read data: https://cloud.google.com/sql/docs/sqlserver/iam-roles#roles:~:text=roles/cloudsql.instanceUser"
      },
      {
        "date": "2023-04-21T05:58:00.000Z",
        "voteCount": 3,
        "content": "We want to apply least privilege and need to read data out of Cloud SQL for SQL Server only, `roles/cloudsql.viewer` is good enough to statisfy the those requirement, that filters out B, C, and D already\nhttps://cloud.google.com/sql/docs/sqlserver/iam-roles#roles"
      },
      {
        "date": "2023-03-23T19:27:00.000Z",
        "voteCount": 2,
        "content": "To read data out of Cloud SQL for SQL Server, you need to use a service account with the roles/cloudsql.instanceUser role on the Cloud SQL instance. This role grants the service account permission to read data from the instance.\nTo update a table in Cloud Spanner, you need to use a service account with the roles/spanner.databaseUser role on the Cloud Spanner instance. This role grants the service account permission to read and write data in the Spanner database.\nTherefore, to grant least privilege access, you should assign the service account only the required roles, which are roles/cloudsql.instanceUser and roles/spanner.databaseUser."
      },
      {
        "date": "2023-03-12T14:49:00.000Z",
        "voteCount": 1,
        "content": "A.\nYou need read access in Cloud SQL for SQL Server and read/write access in Cloud Spanner. Admin permissions are not required so eliminate B. roles/spanner.database Reader would not provide write access, so eliminate C. roles/cloudsql.viewer provides read only access to Cloud SQL resources. That eliminates D, leaving A."
      },
      {
        "date": "2023-02-11T15:23:00.000Z",
        "voteCount": 1,
        "content": "Ans should be D for minimum access \nroles/cloudsql.instanceUser : Allowing to login and get\nroles/spanner.databaseUser: read and write"
      },
      {
        "date": "2022-12-28T03:32:00.000Z",
        "voteCount": 3,
        "content": "A is the correct answer"
      },
      {
        "date": "2022-12-25T07:43:00.000Z",
        "voteCount": 2,
        "content": "A: roles/cloudsql.viewer ***** and roles/spanner.databaseUser *****"
      },
      {
        "date": "2022-12-25T02:36:00.000Z",
        "voteCount": 4,
        "content": "https://cloud.google.com/spanner/docs/iam#:~:text=roles/spanner.databaseUser%20contains%20the%20permissions%20spanner.databases.read%20and%20spanner.databases.write\n\nhttps://cloud.google.com/sql/docs/mysql/iam-roles#:~:text=roles/cloudsql.viewer,to%20all%20Cloud%20SQL%20resources."
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 54,
    "url": "https://www.examtopics.com/discussions/google/view/92752-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "You are responsible for designing a new database for an airline ticketing application in Google Cloud. This application must be able to:<br>Work with transactions and offer strong consistency.<br>Work with structured and semi-structured (JSON) data.<br>Scale transparently to multiple regions globally as the operation grows.<br>You need a Google Cloud database that meets all the requirements of the application. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud SQL for PostgreSQL with both cross-region read replicas.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Spanner in a multi-region configuration.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Firestore in Datastore mode.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a Bigtable instance with clusters in multiple regions."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 18,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 5,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-04-21T06:26:00.000Z",
        "voteCount": 7,
        "content": "I'd choose B.\nTo me, there is no totally true option.\nI agree with dynamic_dba that A and D are wrong.\n\nB. Spanner supports JSON, but not that good (as Pilot50 pointed out)\nC. Firestore Firestore in Datastore mode does provide some degree of consistency guarantees, specifically \"eventual consistency\" for global queries and strong consistency for ancestor queries. However, it does not provide strong consistency for non-ancestor queries.\n\nIn contrast, Cloud Spanner provides strong consistency for all transactions, which is especially important for applications that require ACID transactions across multiple regions.\n\nSo between B and C, I'll pick Spanner for strong consistency"
      },
      {
        "date": "2024-08-29T14:31:00.000Z",
        "voteCount": 1,
        "content": "I choose option C. Spanner could work, but it would be very tight. Firestore is the best fit for the use case since it auto-scales transparently and works perfectly with semi-structured data"
      },
      {
        "date": "2024-04-30T02:17:00.000Z",
        "voteCount": 1,
        "content": "b: https://cloud.google.com/spanner/docs/working-with-json#spanner_query_with_json_parameter-java"
      },
      {
        "date": "2023-11-29T05:57:00.000Z",
        "voteCount": 2,
        "content": "A. Use Cloud SQL for PostgreSQL with both cross-region read replicas: This won't scale transparently or globally\nB. Use Cloud Spanner in a multi-region configuration:  supports JSON, designed for mission critical workloads, can be scaled transparently with an autoscaler across the globe.  \nC. Use Firestore in Datastore mode: Firestore can scale transparently in a multi-region; but it can't be scaled GLOBALLY.\nD. Use a Bigtable instance with clusters in multiple regions: This doesn't necesarily fit the supplied workload."
      },
      {
        "date": "2023-04-04T16:00:00.000Z",
        "voteCount": 2,
        "content": "Spanner can't support JSON and not scaling automatically"
      },
      {
        "date": "2023-04-04T16:02:00.000Z",
        "voteCount": 2,
        "content": "Correction: Spanner supports JSON but not scaling automatically \nhttps://cloud.google.com/blog/products/databases/manage-semi-structured-data-in-cloud-spanner-with-json"
      },
      {
        "date": "2023-04-04T16:36:00.000Z",
        "voteCount": 3,
        "content": "Correction: the correct answer is B, \nFirestore in Datastore mode dose not provided strong consistency"
      },
      {
        "date": "2023-03-12T16:15:00.000Z",
        "voteCount": 2,
        "content": "C.\nCloud SQL is a regional service so cannot scale to multiple regions. Eliminate A. Spanner doesn\u2019t support semi-structured data and doesn\u2019t scale transparently. Eliminate B. Bigtable doesn\u2019t work with JSON data. Eliminate D. That leaves C, Firestore in Datastore mode which satisfies all the requirements."
      },
      {
        "date": "2024-02-13T06:59:00.000Z",
        "voteCount": 1,
        "content": "Spanner definitely supports semi-structured data:\n\nhttps://console.cloud.google.com/marketplace/product/google-cloud-platform/cloud-spanner\n\nCloud Spanner is ideal for relational, structured, and semi-structured data that requires high availability, strong consistency, and transactional reads and writes. It offers all the traditional benefits of a relational database \u2013 such as ACID transactions and SQL semantics"
      },
      {
        "date": "2023-03-05T15:01:00.000Z",
        "voteCount": 3,
        "content": "Hello Team, \n\nI hope this comment helps \n\n\nWork with transactions and offer strong consistency\nWork with structured and semi-structured (JSON) data =&gt; NoSQL (cloud SQL + spanner are not suitable here )\nScale transparently to multiple regions globally as the operation grows =&gt; scale transparently =&gt; you don't manage anything\nBig Table is for time series, millisecond access and for JSON \n\n\nReferrence ; https://cloud.google.com/firestore#all-features"
      },
      {
        "date": "2023-03-05T15:02:00.000Z",
        "voteCount": 2,
        "content": "more reference ; https://cloud.google.com/firestore/docs/firestore-or-datastore#in_datastore_mode"
      },
      {
        "date": "2023-02-19T22:40:00.000Z",
        "voteCount": 1,
        "content": "Why not bigtable ?"
      },
      {
        "date": "2023-01-23T01:56:00.000Z",
        "voteCount": 1,
        "content": "Spanner is multiple region and can Scale transparently"
      },
      {
        "date": "2023-03-05T14:58:00.000Z",
        "voteCount": 1,
        "content": "no you scale spanner, it's not transparently, \nFirestore =&gt;With automatic multi-region replication and strong consistency, your data is safe and has a 99.999% availability guarantee, even when disasters strike.\n\nhttps://cloud.google.com/firestore#all-features"
      },
      {
        "date": "2022-12-28T03:45:00.000Z",
        "voteCount": 3,
        "content": "A and B are correct answer but B looks best answer because of \"Scale transparently to multiple regions globally as the operation grows.\""
      },
      {
        "date": "2023-03-05T14:58:00.000Z",
        "voteCount": 1,
        "content": "what about JSON"
      },
      {
        "date": "2022-12-25T07:37:00.000Z",
        "voteCount": 1,
        "content": "A: Use Cloud SQL for PostgreSQL ***** with both cross-region read replicas."
      },
      {
        "date": "2023-02-17T11:39:00.000Z",
        "voteCount": 1,
        "content": "Why Cloud SQL? You comment the worng answer in multiple question around\n here"
      },
      {
        "date": "2022-12-25T02:39:00.000Z",
        "voteCount": 4,
        "content": "I'll go for Spanner because of 'Scale transparently to multiple regions globally as the operation grows'"
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 55,
    "url": "https://www.examtopics.com/discussions/google/view/92753-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "You are writing an application that will run on Cloud Run and require a database running in the Cloud SQL managed service. You want to secure this instance so that it only receives connections from applications running in your VPC environment in Google Cloud. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create your instance with a specified external (public) IP address.<br>2. Choose the VPC and create firewall rules to allow only connections from Cloud Run into your instance.<br>3. Use Cloud SQL Auth proxy to connect to the instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create your instance with a specified external (public) IP address.<br>2. Choose the VPC and create firewall rules to allow only connections from Cloud Run into your instance.<br>3. Connect to the instance using a connection pool to best manage connections to the instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create your instance with a specified internal (private) IP address.<br>2. Choose the VPC with private service connection configured.<br>3. Configure the Serverless VPC Access connector in the same VPC network as your Cloud SQL instance.<br>4. Use Cloud SQL Auth proxy to connect to the instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create your instance with a specified internal (private) IP address.<br>2. Choose the VPC with private service connection configured.<br>3. Configure the Serverless VPC Access connector in the same VPC network as your Cloud SQL instance.<br>4. Connect to the instance using a connection pool to best manage connections to the instance.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 20,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 11,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-25T02:45:00.000Z",
        "voteCount": 11,
        "content": "It's D, CloudSQL Auth proxy is not used when connecting to Private IP \nhttps://cloud.google.com/sql/docs/mysql/connect-run#configure\nhttps://cloud.google.com/sql/docs/mysql/connect-run#connection-pools"
      },
      {
        "date": "2023-02-21T14:44:00.000Z",
        "voteCount": 5,
        "content": "The Cloud SQL Auth proxy works with both public and private IP endpoints: https://cloud.google.com/sql/docs/mysql/connect-auth-proxy"
      },
      {
        "date": "2023-03-12T16:26:00.000Z",
        "voteCount": 5,
        "content": "D.\nCloud Run to Cloud SQL connectivity can be done using private IPs. Eliminate A and B. C would be right except you wouldn\u2019t use Cloud SQL Auth Proxy because Serverless VPC Access would connect directly to the Cloud SQL instance. The connection pool reference in D puts you off, but it is the right answer. The link provided by SVGoogle89 is spot on."
      },
      {
        "date": "2024-04-30T02:20:00.000Z",
        "voteCount": 2,
        "content": "D: https://cloud.google.com/sql/docs/mysql/connect-run#connect"
      },
      {
        "date": "2024-01-22T04:47:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/sql/docs/mysql/connect-run#connect_to - said clearly that there is no need for Cloud SQL Auth Proxy when using with Cloud Run."
      },
      {
        "date": "2024-01-16T04:43:00.000Z",
        "voteCount": 1,
        "content": "API Quota Limits\nCloud Run provides a mechanism that connects using the Cloud SQL Auth Proxy, which uses the Cloud SQL Admin API. API quota limits apply to the Cloud SQL Auth Proxy. The Cloud SQL Admin API quota used is approximately two times the number of Cloud SQL instances configured by the number of Cloud Run instances of a particular service deployed at any one time. You can cap or increase the number of Cloud Run instances to modify the expected API quota consumed."
      },
      {
        "date": "2024-01-16T04:43:00.000Z",
        "voteCount": 2,
        "content": "Selected Answer: C"
      },
      {
        "date": "2023-12-13T06:39:00.000Z",
        "voteCount": 3,
        "content": "Option D.\n\nThis link explicitly indicates that \"For private IP paths, your application will connect directly to your instance through Serverless VPC Access. This method uses TCP to connect directly to the Cloud SQL instance without using the Cloud SQL Auth Proxy.\"\n\nhttps://cloud.google.com/sql/docs/mysql/connect-run#connect_to"
      },
      {
        "date": "2023-11-29T06:00:00.000Z",
        "voteCount": 1,
        "content": "D is the correct answer.  \n\nA common misconception."
      },
      {
        "date": "2023-11-29T06:05:00.000Z",
        "voteCount": 1,
        "content": "When using Cloud Run to connect to Cloud SQL Private IP addresses, it is unnecessary to use the SQL Auth Proxy in Private IP mode.  \n\nThe Serverless VPC Access Connector (which has been superceded by Cloud Run's direct VPC Egress) should connect directly and leverage a connection pooler (potentially in your application via client library, probably better as a separate instance) for more consistent connections to the Cloud SQL DB.  \n\nYou would create a specific \"user\" for this purpose in your database."
      },
      {
        "date": "2023-11-04T18:09:00.000Z",
        "voteCount": 1,
        "content": "C is correct."
      },
      {
        "date": "2023-10-03T09:09:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/sql/docs/mysql/connect-auth-proxy\n\"Works with both public and private IP endpoints\""
      },
      {
        "date": "2023-09-27T07:24:00.000Z",
        "voteCount": 2,
        "content": "it's d, Auth proxy is not used when connecting to Private IP"
      },
      {
        "date": "2023-09-06T00:10:00.000Z",
        "voteCount": 1,
        "content": "Vote C"
      },
      {
        "date": "2023-09-05T21:28:00.000Z",
        "voteCount": 1,
        "content": "correct answer is C - The Cloud SQL Auth proxy acts as a secure intermediary between your Cloud Run application and the Cloud SQL instance, allowing for secure and authenticated database connections while keeping the database inaccessible from the public internet."
      },
      {
        "date": "2023-06-28T01:58:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/sql/docs/mysql/connect-overview\nConfiguring your instance with a private IP is preferred when connecting from a client on a resource with access to a VPC. For more information about what resources can use private IP, see Requirements for Private IP.\n\nFor private IP paths, the following services and applications connect directly to your instance through Serverless VPC Access:\n\nApp Engine standard environment\nApp Engine flexible environment\nCloud Functions\nCloud Run"
      },
      {
        "date": "2023-06-17T22:03:00.000Z",
        "voteCount": 1,
        "content": "C\nCloud SQL Auth Proxy can connect Cloud SQL instance with private ip by specifying --private-ip argument in same VPC. Cloud Run can run a container that gets the auth proxy installable files and run the auth proxy in cloud Run in same VPC."
      },
      {
        "date": "2023-06-13T22:40:00.000Z",
        "voteCount": 2,
        "content": "auth proxy isn't required with private serverless access, the connection pool increases reliability of the connection"
      },
      {
        "date": "2023-05-27T21:36:00.000Z",
        "voteCount": 1,
        "content": "The Cloud SQL Auth proxy provides a secure connection between your application running on Cloud Run and the Cloud SQL instance. It handles authentication and encrypts traffic."
      },
      {
        "date": "2023-04-04T16:42:00.000Z",
        "voteCount": 1,
        "content": "not a great question, bot C and D are acceptable based on this google doc\nhttps://cloud.google.com/sql/docs/mysql/connect-run#best-practices"
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 56,
    "url": "https://www.examtopics.com/discussions/google/view/92075-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "You are troubleshooting a connection issue with a newly deployed Cloud SQL instance on Google Cloud. While investigating the Cloud SQL Proxy logs, you see the message Error 403: Access Not Configured. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCheck the app.yaml value cloud_sql_instances for a misspelled or incorrect instance connection name.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCheck whether your service account has cloudsql.instances.connect permission.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable the Cloud SQL Admin API.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure that you are using an external (public) IP address interface."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 13,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-04-30T02:21:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/sql/docs/mysql/connect-auth-proxy#troubleshooting"
      },
      {
        "date": "2023-11-29T06:11:00.000Z",
        "voteCount": 3,
        "content": "The answer is C.  \n\nIf you're missing cloudsql.instances.connect on the attached service account, the error is 403 FORBIDDEN.  \n\nIf you haven't enabled the Cloud SQL Admin API, the error is 403 ACCESS NOT CONFIGURED."
      },
      {
        "date": "2023-10-03T09:15:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/sql/docs/mysql/connect-auth-proxy#troubleshooting\nC because in docs it says \"Make sure to enable the Cloud SQL Admin API. If it is not, you see output like Error 403: Access Not Configured in your Cloud SQL Auth Proxy logs.\"\n\nB is not correct because this is for a different error: \"If you are getting a 403 notAuthorized error, and you are using a service account to authenticate the Cloud SQL Auth Proxy, make sure the service account has the correct permissions.\""
      },
      {
        "date": "2023-09-27T07:24:00.000Z",
        "voteCount": 1,
        "content": "it's b"
      },
      {
        "date": "2023-09-05T21:31:00.000Z",
        "voteCount": 1,
        "content": "The \"Error 403: Access Not Configured\" message typically indicates a permission issue related to the Cloud SQL instance. The correct answer is B\nB. Check whether your service account has cloudsql.instances.connect permission."
      },
      {
        "date": "2023-07-06T14:38:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/sql/docs/mysql/connect-auth-proxy#troubleshooting"
      },
      {
        "date": "2023-04-04T16:57:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/sql/docs/mysql/connect-auth-proxy#troubleshooting:~:text=Make%20sure%20to,Auth%20proxy%20logs."
      },
      {
        "date": "2023-03-12T14:32:00.000Z",
        "voteCount": 2,
        "content": "C.\nD is not a recommended way to access a Cloud SQL instance and you wouldn\u2019t get a 403 error connecting via a public IP anyway. A and B could trigger a 403 error, but the text would be \u201cnot authorized\u201d. The answer is in Google\u2019s documentation. The link provided by sp57 is spot on."
      },
      {
        "date": "2022-12-29T14:36:00.000Z",
        "voteCount": 1,
        "content": "typo.. its C\nMake sure to enable the Cloud SQL Admin API.\n\nIf it is not, you see output like Error 403: Access Not Configured in your Cloud SQL Auth proxy logs."
      },
      {
        "date": "2022-12-29T14:35:00.000Z",
        "voteCount": 1,
        "content": "B\n403 notAuthorized error, and you are using a service account to authenticate the Cloud SQL Auth proxy, make sure the service account has the correct permissions."
      },
      {
        "date": "2022-12-30T17:43:00.000Z",
        "voteCount": 1,
        "content": "wrong - error is 403 Access not Configured - see 4 bullet pts down at link   https://cloud.google.com/sql/docs/mysql/connect-admin-proxy#troubleshooting\nC is correct"
      },
      {
        "date": "2022-12-28T09:09:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is c\nhttps://groups.google.com/g/google-cloud-sql-discuss/c/yoGQzTRXaOk?pli=1"
      },
      {
        "date": "2022-12-25T07:34:00.000Z",
        "voteCount": 1,
        "content": "C: Enable the Cloud SQL Admin API."
      },
      {
        "date": "2022-12-25T02:54:00.000Z",
        "voteCount": 1,
        "content": "Make sure to enable the Cloud SQL Admin API.\n\nIf it is not, you see output like Error 403: Access Not Configured in your Cloud SQL Auth proxy logs."
      },
      {
        "date": "2022-12-22T21:45:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer - B \nhttps://cloud.google.com/sql/docs/mysql/connect-admin-proxy#troubleshooting:~:text=If%20you%20are%20getting,have%20this%20permission."
      },
      {
        "date": "2022-12-21T16:37:00.000Z",
        "voteCount": 1,
        "content": "Enable the Cloud SQL Admin API"
      },
      {
        "date": "2022-12-19T05:11:00.000Z",
        "voteCount": 2,
        "content": "Answer C, https://cloud.google.com/sql/docs/mysql/connect-admin-proxy#troubleshooting"
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 57,
    "url": "https://www.examtopics.com/discussions/google/view/92773-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "You are working on a new centralized inventory management system to track items available in 200 stores, which each have 500 GB of data. You are planning a gradual rollout of the system to a few stores each week. You need to design an SQL database architecture that minimizes costs and user disruption during each regional rollout and can scale up or down on nights and holidays. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Oracle Real Application Cluster (RAC) databases on Bare Metal Solution for Oracle.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse sharded Cloud SQL instances with one or more stores per database instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a Biglable cluster with autoscaling.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Spanner with a custom autoscaling solution.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 7,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 6,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-03-12T14:19:00.000Z",
        "voteCount": 11,
        "content": "D.\nA SQL database architecture rules out Bigtable. Minimizing costs rules out Oracle RAC. That leaves B and D. B would work with each Cloud SQL instance being dedicated to a few stores which would not impact other Cloud SQL instances already running. The downside is the scaling up/down. To change the vCPU on a Cloud SQL instance requires it to be re-started and that\u2019s disruption. Spanner also doesn\u2019t autoscale by itself, but there\u2019s a tool available for Spanner called Autoscaler which can automate scaling up/down. So on balance, D is the best answer.\nhttps://cloud.google.com/spanner/docs/autoscaling-overview"
      },
      {
        "date": "2024-02-22T03:37:00.000Z",
        "voteCount": 1,
        "content": "i agree with you on ruling out A and C, however cloud spanner is much costlier than cloud SQL , so B woudlnt be the best answer ?"
      },
      {
        "date": "2024-01-21T07:59:00.000Z",
        "voteCount": 2,
        "content": "To me, it is \"B\" here. A couple of reasons; the statement \"minimizes costs and user disruption during each regional rollout\" explicitly emphasises that the disruption relates to the rollout not the scale-up or scale-down, and considering it's only a few stores per db, I would presume it would take like a few minutes tops. Lastly, the cost is a bit of a give away as well since Cloud SQL is like twice as cheap (I did some rough estimates recently using europe-west2 as my benchmark)."
      },
      {
        "date": "2023-09-05T21:43:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is ==&gt;  B. Use sharded Cloud SQL instances with one or more stores per database instance."
      },
      {
        "date": "2023-07-17T17:08:00.000Z",
        "voteCount": 2,
        "content": "B\nGuys, either B or D. The keyword is :Minimize cost\" Although D is the best solution, B is less costly"
      },
      {
        "date": "2023-03-07T04:40:00.000Z",
        "voteCount": 3,
        "content": "1. CloudSQL max out at 64TB, so unable to told 100TB of data. https://cloud.google.com/sql/docs/quotas#metrics_collection_limit\n2. Scale is done manually on SQL Cloud."
      },
      {
        "date": "2023-02-11T16:28:00.000Z",
        "voteCount": 2,
        "content": "CloudSQL max out at 64TB, so unable to told 200 * 500 GB of data.\nA: No. Oracle RAC cannot scale up or down\nB: No. Cloud SQL cannot scale up or down manually and the sharded Cloud SQL sound weird, and doesn't meet the \"minimize costs and user disruption during each REGIONAL rollout\". Also can't break storage limits 64TB.\nC: No. BigTable handles rational db poorly\nhttps://cloud.google.com/sql/docs/quotas#:~:text=Cloud%20SQL%20storage%20limits,core%3A%20Up%20to%203%20TB."
      },
      {
        "date": "2023-01-28T07:39:00.000Z",
        "voteCount": 2,
        "content": "CloudSQL max out at 64TB, so unable to told 200 * 500 GB of data.\nD: choose Spanner"
      },
      {
        "date": "2023-06-20T00:58:00.000Z",
        "voteCount": 1,
        "content": "it's written 500 GB of data per store and cloud sql instance per 1 or couple of stores, what's so hard to understand?"
      },
      {
        "date": "2023-01-09T08:05:00.000Z",
        "voteCount": 1,
        "content": "A - No- oracle RAC cannot scale up or down\nC- No, Big Table is for nonSQL\nB or D\nB - Cloud SQL cannot scale up or down manually and the sharded Cloud SQL sound weird, and doesn't meet the \"minimize costs and user disruption during each REGIONAL rollout\" \n\nFor my, D is the best option as it"
      },
      {
        "date": "2022-12-26T12:00:00.000Z",
        "voteCount": 4,
        "content": "Cloud SQL sharding looks like a good option since we need to minimize costs  \n and we don't need global scaling https://cloud.google.com/community/tutorials/horizontally-scale-mysql-database-backend-with-google-cloud-sql-and-proxysql"
      },
      {
        "date": "2022-12-25T07:33:00.000Z",
        "voteCount": 1,
        "content": "B: Use sharded ***** Cloud SQL instances with one or more stores per database instance.\nSharding makes horizontal scaling possible by partitioning the database into smaller, more manageable parts (shards), then deploying the parts across a cluster of machines. Data queries are routed to the corresponding server automatically, usually with rules embedded in application logic or a query router."
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 58,
    "url": "https://www.examtopics.com/discussions/google/view/92772-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "Your organization has strict policies on tracking rollouts to production and periodically shares this information with external auditors to meet compliance requirements. You need to enable auditing on several Cloud Spanner databases. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse replication to roll out changes to higher environments.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse backup and restore to roll out changes to higher environments.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Liquibase to roll out changes to higher environments.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tManually capture detailed DBA audit logs when changes are rolled out to higher environments."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 12,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-03-12T14:00:00.000Z",
        "voteCount": 5,
        "content": "C.\nTo satisfy audit reporting you would need a way to record what was changed and when. The best answer is one which uses some kind of source code control system (SCCS). That rules out A and B. Any mention of anything manual in a cloud environment should look suspicious, which leave option C. As it happens, Liquibase is an SCCS and can be integrated with Spanner.\nhttps://cloud.google.com/spanner/docs/use-liquibase"
      },
      {
        "date": "2023-04-04T17:13:00.000Z",
        "voteCount": 5,
        "content": "https://cloud.google.com/spanner/docs/use-liquibase"
      },
      {
        "date": "2024-05-15T22:32:00.000Z",
        "voteCount": 1,
        "content": "Use Liquibase to roll out changes to higher environments."
      },
      {
        "date": "2023-09-05T21:49:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is =&gt; D. Manually capturing audit logs provides the level of granularity and control required for compliance auditing."
      },
      {
        "date": "2023-07-17T17:12:00.000Z",
        "voteCount": 2,
        "content": "D.\nAuditors wants to see what changed. The only way to showcase that is to capture the audit logs, who changed what... Hence D"
      },
      {
        "date": "2023-03-07T07:15:00.000Z",
        "voteCount": 2,
        "content": "C, liquidbase for db source control"
      },
      {
        "date": "2023-02-11T17:44:00.000Z",
        "voteCount": 4,
        "content": "I assume production environment shall be a standalone project. Based on this,\nA: No. Replication can't be done cross projects.\nB: No. Backup / restore only get a whole set drop / load statements from audit logs. External auditors would focus on \"what changes\".\nC: Yes. Liguibase could export change.yaml to hand over the external auditors.\nD: No. No way to capture audit logs and ship to the target project. Spanner is a managed service.\nReference:\nhttps://cloud.google.com/spanner/docs/backup/gcloud\nhttps://cloud.google.com/spanner/docs/use-liquibase"
      },
      {
        "date": "2022-12-27T02:30:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/spanner/docs/audit-logging#audited_operations"
      },
      {
        "date": "2022-12-26T18:30:00.000Z",
        "voteCount": 3,
        "content": "C, liquidbase for db source control"
      },
      {
        "date": "2022-12-25T07:32:00.000Z",
        "voteCount": 1,
        "content": "B: Use backup and restore ***** to roll out changes to higher environments.\n[1] Even though restoring a database requires authorization on two resources (the backup ***** and restored database, which might reside in different instances), the RestoreDatabase event is logged only once as a single entry in the instance of the restored database. Within this entry, there will be two authorizationInfo entries: one for the database, checking the spanner.databases.create permission, and one for the backup, checking the spanner.backups.restoreDatabase permission."
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 59,
    "url": "https://www.examtopics.com/discussions/google/view/92742-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "Your organization has a production Cloud SQL for MySQL instance. Your instance is configured with 16 vCPUs and 104 GB of RAM that is running between 90% and 100% CPU utilization for most of the day. You need to scale up the database and add vCPUs with minimal interruption and effort. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIssue a gcloud sql instances patch command to increase the number of vCPUs.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate a MySQL database flag to increase the number of vCPUs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIssue a gcloud compute instances update command to increase the number of vCPUs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBack up the database, create an instance with additional vCPUs, and restore the database."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-30T02:28:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/sdk/gcloud/reference/sql/instances/patch"
      },
      {
        "date": "2023-10-03T09:42:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/sdk/gcloud/reference/sql/instances/patch"
      },
      {
        "date": "2023-03-12T13:49:00.000Z",
        "voteCount": 4,
        "content": "A.\nD would not represent minimal interruption or effort. B would not change the number of vCPUs available to the instance as a whole. Option C is for GCE VMs, not Cloud SQL."
      },
      {
        "date": "2023-03-07T07:20:00.000Z",
        "voteCount": 1,
        "content": "\"gcloud sql instances patch\" --&gt; updates the settings of a Cloud SQL instance"
      },
      {
        "date": "2022-12-25T03:24:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/sdk/gcloud/reference/sql/instances/patch#:~:text=existing%20authorized%20networks.-,%2D%2Dcpu,-%3DCPU"
      },
      {
        "date": "2022-12-24T16:11:00.000Z",
        "voteCount": 2,
        "content": "A: Issue a gcloud sql instances *** patch command to increase the number of vCPUs.\nCores\nY\t1 to 96 (must be either 1 or an even number)\nPartial for shared vCPU\ngcloud sql instances patch INSTANCE [--activation-policy=ACTIVATION_POLICY] [--active-directory-domain=ACTIVE_DIRECTORY_DOMAIN] [--[no-]assign-ip] [--async] [--audit-bucket-path=AUDIT_BUCKET_PATH] [--audit-retention-interval=AUDIT_RETENTION_INTERVAL] [--audit-upload-interval=AUDIT_UPLOAD_INTERVAL] [--availability-type=AVAILABILITY_TYPE] [--clear-password-policy] [--connector-enforcement=CONNECTOR_ENFORCEMENT] [--cpu=CPU] [--database-version"
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 60,
    "url": "https://www.examtopics.com/discussions/google/view/92741-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "You are configuring a brand new Cloud SQL for PostgreSQL database instance in Google Cloud. Your application team wants you to deploy one primary instance, one standby instance, and one read replica instance. You need to ensure that you are following Google-recommended practices for high availability. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the primary instance in zone A, the standby instance in zone C, and the read replica in zone B, all in the same region.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the primary and standby instances in zone A and the read replica in zone B, all in the same region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the primary instance in one region, the standby instance in a second region, and the read replica in a third region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the primary, standby, and read replica instances in zone A, all in the same region."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 3,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-03-12T13:35:00.000Z",
        "voteCount": 9,
        "content": "A.\nCloud SQL is a regional service so the primary and standby instances must be in the same region. A recommended practice would have them in different zones. A read replica could be in a different region, but that option isn\u2019t offered probably because it would be too far away from the user base. Having it in a different zone from the primary and standby instances is therefore the best and only answer."
      },
      {
        "date": "2023-09-06T13:12:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is ==&gt; B. Configure the primary and standby instances in zone A and the read replica in zone B, all in the same region."
      },
      {
        "date": "2022-12-25T06:05:00.000Z",
        "voteCount": 3,
        "content": "HA standby instance can't be located in different region, and Google recommends to use different zones for all three, so answer is A \nhttps://cloud.google.com/sql/docs/postgres/high-availability#failover-overview"
      },
      {
        "date": "2022-12-24T16:10:00.000Z",
        "voteCount": 3,
        "content": "The HA configuration provides data redundancy. A Cloud SQL instance configured for HA is also called a regional instance and has a primary and secondary zone within the configured region. Within a regional instance, the configuration is made up of a primary instance and a standby instance. Through synchronous replication to each zone's persistent disk, all writes made to the primary instance are replicated to disks in both zones before a transaction is reported as committed. In the event of an instance or zone failure, the standby instance becomes the new primary instance. Users are then rerouted to the new primary instance. This process is called a failover."
      },
      {
        "date": "2022-12-24T16:09:00.000Z",
        "voteCount": 3,
        "content": "A: Configure the primary instance in zone A, the standby instance in zone C, and the read replica in zone B, all in the same region."
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 61,
    "url": "https://www.examtopics.com/discussions/google/view/92740-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "You are running a transactional application on Cloud SQL for PostgreSQL in Google Cloud. The database is running in a high availability configuration within one region. You have encountered issues with data and want to restore to the last known pristine version of the database. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a clone database from a read replica database, and restore the clone in the same region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a clone database from a read replica database, and restore the clone into a different zone.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Cloud SQL point-in-time recovery (PITR) feature. Restore the copy from two hours ago to a new database instance.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Cloud SQL database import feature. Import last week's dump file from Cloud Storage."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 7,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-25T07:33:00.000Z",
        "voteCount": 6,
        "content": "I'll go for C"
      },
      {
        "date": "2023-03-13T08:41:00.000Z",
        "voteCount": 5,
        "content": "C.\nAn HA configuration means a primary instance and a failover replica instance. There is no read replica unless you specifically create one. Hence, A and B are wrong. D is wrong since the question doesn\u2019t mention logical backups, whereas an HA configuration would have automatic (physical) backups enabled. A PIT restore is the only answer here."
      },
      {
        "date": "2023-03-07T13:12:00.000Z",
        "voteCount": 1,
        "content": "Logical corruption like deleted records will be pushed to the read replica.   \nUsing import/export from last week is slow for large scale databases and will restore database from last week. \nSo C - restoring using point in time option - is ok."
      },
      {
        "date": "2023-02-20T05:19:00.000Z",
        "voteCount": 1,
        "content": "D. PITR always restore to a **NEW** db instance, but the question does not ask to change the application configuration."
      },
      {
        "date": "2022-12-24T16:09:00.000Z",
        "voteCount": 1,
        "content": "C: Use the Cloud SQL point-in-time recovery *** (PITR) feature. Restore the copy from two hours ago to a new database instance.\nPoint-in-time recovery allows you to recover an instance to a specific point in time. For example, if an operator 'fat finger' error causes a loss of data you can recover a database to the state it was just before the error occurred."
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 62,
    "url": "https://www.examtopics.com/discussions/google/view/92739-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "Your organization has a security policy to ensure that all Cloud SQL for PostgreSQL databases are secure. You want to protect sensitive data by using a key that meets specific locality or residency requirements. Your organization needs to control the key's lifecycle activities. You need to ensure that data is encrypted at rest and in transit. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate the database with Google-managed encryption keys.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate the database with customer-managed encryption keys.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate the database persistent disk with Google-managed encryption keys.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate the database persistent disk with customer-managed encryption keys."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-07-07T05:10:00.000Z",
        "voteCount": 1,
        "content": "This is better option \"database with customer-managed encryption keys\""
      },
      {
        "date": "2023-06-19T03:27:00.000Z",
        "voteCount": 1,
        "content": "B. Create the database with customer-managed encryption keys."
      },
      {
        "date": "2023-03-13T08:42:00.000Z",
        "voteCount": 4,
        "content": "B.\nHaving greater control over EK means use CMEK. That eliminates A and C. When creating a Cloud SQL instance you get to choose the encryption method at the instance level, which would include databases. That makes D not make sense. So it\u2019s B."
      },
      {
        "date": "2023-03-05T15:24:00.000Z",
        "voteCount": 1,
        "content": "B. Create the database with customer-managed encryption keys."
      },
      {
        "date": "2022-12-25T08:06:00.000Z",
        "voteCount": 2,
        "content": "Despite that you select CMEK in the Storage section, it says: This instance is encrypted with a Google-managed key by default. If you need to manage your encryption, you can use a customer-managed key instead.\nAlso, you don't need to create persistent disk, google does that."
      },
      {
        "date": "2022-12-25T08:06:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/sql/docs/postgres/configure-cmek#createcmekinstance"
      },
      {
        "date": "2022-12-24T16:08:00.000Z",
        "voteCount": 3,
        "content": "B: Create the database with customer-managed encryption keys.\nHow do you create a customer managed key? In the navigation pane, choose Customer managed keys. Choose Create key. To create a symmetric encryption KMS key, for Key type choose Symmetric. For information about how to create an asymmetric KMS key in the AWS KMS console, see Creating asymmetric KMS keys (console)."
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 63,
    "url": "https://www.examtopics.com/discussions/google/view/92738-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "Your organization has an existing app that just went viral. The app uses a Cloud SQL for MySQL backend database that is experiencing slow disk performance while using hard disk drives (HDDs). You need to improve performance and reduce disk I/O wait times. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExport the data from the existing instance, and import the data into a new instance with solid-state drives (SSDs).\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEdit the instance to change the storage type from HDD to SSD.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a high availability (HA) failover instance with SSDs, and perform a failover to the new instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a read replica of the instance with SSDs, and perform a failover to the new instance"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 14,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-12-25T08:23:00.000Z",
        "voteCount": 9,
        "content": "You can't edit disk time, this setting is permanent. https://stackoverflow.com/questions/72034607/can-i-change-storage-type-from-hdd-to-ssd-on-cloud-sql-after-creating-an-instanc\nYou also can't choose disk type for read replica."
      },
      {
        "date": "2023-03-13T09:05:00.000Z",
        "voteCount": 5,
        "content": "A.\nB would not work. You cannot change the disk type of the instance after it\u2019s created. C is wrong for the same reason. Adding HA would create a failover replica with the same disk type as the primary. The same is true for a read replica, so D is wrong as well. Leaves A."
      },
      {
        "date": "2024-04-30T02:33:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/sql/docs/mysql/choosing-ssd-hdd#switching"
      },
      {
        "date": "2023-03-07T07:39:00.000Z",
        "voteCount": 2,
        "content": "The choice of SSD or HDD storage for the instance is not available.\nIt should be  is done manually."
      },
      {
        "date": "2023-03-01T17:07:00.000Z",
        "voteCount": 2,
        "content": "Switch between SSD and HDD storage\nWhen you create a Cloud SQL instance, your choice of SSD or HDD storage for the instance is permanent.\n\nIf you need to convert an existing HDD instance to SSD, or conversely, you can export the data from the existing instance and import the data into a new instance. Keep in mind that migrating an entire instance takes time.\nhttps://cloud.google.com/sql/docs/mysql/choosing-ssd-hdd"
      },
      {
        "date": "2023-01-04T14:48:00.000Z",
        "voteCount": 1,
        "content": "Why not D?"
      },
      {
        "date": "2023-01-11T11:48:00.000Z",
        "voteCount": 2,
        "content": "When a read replica is created you do not configure the disk type and it is used as it is on primary to avoid replication errors. That is the reason A is the correct answer. See link --&gt; https://cloud.google.com/sql/docs/mysql/replication/create-replica"
      },
      {
        "date": "2022-12-24T16:06:00.000Z",
        "voteCount": 3,
        "content": "C: Create a high availability (HA) failover instance with SSDs, *** and perform a failover to the new instance."
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 64,
    "url": "https://www.examtopics.com/discussions/google/view/92737-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "You are configuring a new application that has access to an existing Cloud Spanner database. The new application reads from this database to gather statistics for a dashboard. You want to follow Google-recommended practices when granting Identity and Access Management (IAM) permissions. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReuse the existing service account that populates this database.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new service account, and grant it the Cloud Spanner Database Admin role.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new service account, and grant it the Cloud Spanner Database Reader role.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new service account, and grant it the spanner.databases.select permission."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 10,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-25T08:26:00.000Z",
        "voteCount": 7,
        "content": "spanner.database.select is not enough, you also need a spanner.database.read which is included in Cloud Spanner Database Reader role"
      },
      {
        "date": "2023-12-19T23:46:00.000Z",
        "voteCount": 1,
        "content": "Option C is correct. \nHere is why:\nspanner.admin\tGod-tier control over everything\tFull CRUD &amp; DDL for instances, databases, users, policies\tManage entire Cloud Spanner environment\n\nspanner.databaseAdmin\tDatabase master control\tFull CRUD &amp; DDL for databases, users, policies\tCreate, manage, and secure databases\n\nspanner.databaseUser\tPower user with data rights\tFull CRUD &amp; DDL for specified databases\tRead, write, modify data, manage database schema\n\nspanner.databaseReader\tCurious but hands-off observer\tRead-only access and query execution\tAnalyze data without manipulation\n\nspanner.viewer\tPeeping Tom (metaphorically speaking)\tView instances, databases, and users\tMonitor Cloud Spanner metadata, no data access\n\nspanner.backupAdmin\tBackup whiz\tCreate, view, update, delete backups\tManage backups without accessing data"
      },
      {
        "date": "2023-09-27T06:37:00.000Z",
        "voteCount": 2,
        "content": "A is nonsense.\nAdmin is too much, select is not enough. \nC is the one."
      },
      {
        "date": "2023-03-13T08:57:00.000Z",
        "voteCount": 4,
        "content": "C.\nA new application should have its own service account. Eliminate A. The Admin role is too broad. Eliminate B. D grants an individual permission not a role. Granting permissions in IAM is done via a role. From Google\u2019s documentation: \u201cIn IAM, permission to access a resource isn't granted directly to the end user. Instead, permissions are grouped into roles, and roles are granted to authenticated principals.\u201d\nhttps://cloud.google.com/iam/docs/overview"
      },
      {
        "date": "2023-02-20T05:28:00.000Z",
        "voteCount": 1,
        "content": "Why not D?"
      },
      {
        "date": "2023-02-20T05:32:00.000Z",
        "voteCount": 1,
        "content": "Service account task\tRequired permissions\nRead data\tspanner.databases.select\nspanner.sessions.create\nspanner.sessions.delete"
      },
      {
        "date": "2023-01-21T09:44:00.000Z",
        "voteCount": 2,
        "content": "Google Recommended practice is to assign roles but not permissions directly. SO, C"
      },
      {
        "date": "2023-01-21T09:45:00.000Z",
        "voteCount": 1,
        "content": "Sorry Selected Answer is C"
      },
      {
        "date": "2022-12-24T16:05:00.000Z",
        "voteCount": 1,
        "content": "B: Create a new service account, and grant it the Cloud Spanner Database Admin ***** role."
      },
      {
        "date": "2023-02-17T09:47:00.000Z",
        "voteCount": 1,
        "content": "This role does not satisfy the principle of least privilege. The answer is letter C."
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 65,
    "url": "https://www.examtopics.com/discussions/google/view/92736-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "Your retail organization is preparing for the holiday season. Use of catalog services is increasing, and your DevOps team is supporting the Cloud SQL databases that power a microservices-based application. The DevOps team has added instrumentation through Sqlcommenter. You need to identify the root cause of why certain microservice calls are failing. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWatch Query Insights for long running queries.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWatch the Cloud SQL instance monitor for CPU utilization metrics.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWatch the Cloud SQL recommenders for overprovisioned instances.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWatch Cloud Trace for application requests that are failing."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 8,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 7,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-03-13T09:18:00.000Z",
        "voteCount": 8,
        "content": "A.\nCloud Trace doesn\u2019t support Cloud SQL. Eliminate D. Cloud SQL recommenders for overprovisioned instances would tell you about Cloud SQL instances which are too large for their workload. Eliminate C. Monitoring CPU utilization wouldn\u2019t tell you why microservice calls are failing. Eliminate B. SQLcommenter integrates with Query Insights. So A is the best answer.\nhttps://cloud.google.com/blog/topics/developers-practitioners/introducing-sqlcommenter-open-source-orm-auto-instrumentation-library"
      },
      {
        "date": "2023-07-17T06:54:00.000Z",
        "voteCount": 5,
        "content": "https://cloud.google.com/blog/topics/developers-practitioners/introducing-sqlcommenter-open-source-orm-auto-instrumentation-library"
      },
      {
        "date": "2024-07-31T09:14:00.000Z",
        "voteCount": 1,
        "content": "The answer should be D.\nA looks not right because Query Insight is for internal Cloud SQL performance and it's not about calls are failing."
      },
      {
        "date": "2024-06-19T07:02:00.000Z",
        "voteCount": 1,
        "content": "Cloud Trace does not automatically support Cloud SQL, meaning it won't natively track latency or errors within Cloud SQL queries without additional instrumentation.\nCloud Trace can provide comprehensive tracing of application requests, but for Cloud SQL queries to be included, explicit instrumentation is needed, typically via tools like Sqlcommenter. Cloud Trace should be able to include latency data for SQL queries. This setup helps team see the impact of database performance on the overall application performance. This helps in identifying the root cause of microservice call failures."
      },
      {
        "date": "2024-06-19T06:51:00.000Z",
        "voteCount": 1,
        "content": "answer : D\nWhile Query Insights is a powerful tool for optimizing database performance and identifying query-related issues, it does not provide the necessary breadth of information required to diagnose failures in a microservices-based application. Cloud Trace, on the other hand, offers a comprehensive view of application performance, including but not limited to database interactions, making it a more suitable choice for identifying the root cause of microservice call failures."
      },
      {
        "date": "2024-04-30T02:51:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/blog/topics/developers-practitioners/introducing-sqlcommenter-open-source-orm-auto-instrumentation-library"
      },
      {
        "date": "2023-09-08T07:04:00.000Z",
        "voteCount": 3,
        "content": "D - To diagnose and address application-level issues, monitoring at the application request level (Option D) is the appropriate choice."
      },
      {
        "date": "2023-07-24T13:23:00.000Z",
        "voteCount": 3,
        "content": "A. Query insights is always the correct answer"
      },
      {
        "date": "2022-12-25T09:13:00.000Z",
        "voteCount": 4,
        "content": "I'll go for A"
      },
      {
        "date": "2022-12-24T16:04:00.000Z",
        "voteCount": 3,
        "content": "A: Watch Query Insights for long running queries.\nSqlcommenter is an open source library that enables ORMs to augment SQL statements before execution, with comments containing information about the code that caused its execution. This helps in easily correlating slow queries with source code and giving insights into backend database performance. In short, it provides observability into the state of client-side applications and their impact on database performance. Application developers need to do very little application code change to enable Sqlcommenter for their applications using ORMs. Observability information from Sqlcommenter can be used by application developers directly using slow query logs, or it can be integrated into other products or tools, such as ***** Cloud SQL Insights, to provide application-centric monitoring."
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 66,
    "url": "https://www.examtopics.com/discussions/google/view/92735-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "You are designing a database architecture for a global application that stores information about public parks worldwide. The application uses the database for read-only purposes, and a centralized batch job updates the database nightly. You want to select an open source, SQL-compliant database. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Bigtable with multi-region clusters.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Memorystore for Redis with multi-zones within a region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud SQL for PostgreSQL with cross-region replicas.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Spanner with multi-region configuration."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-03-13T09:25:00.000Z",
        "voteCount": 5,
        "content": "C.\nBigtable doesn\u2019t support SQL. Eliminate A. Even if Memorystore could scale, B says single region. Eliminate B. Cloud Spanner supports Google Standard SQL and PostgreSQL dialects. That\u2019s not the same thing as a natively open source database. Eliminate D. That leaves C."
      },
      {
        "date": "2024-05-22T04:56:00.000Z",
        "voteCount": 2,
        "content": "D.\nIt says global application, open source and sql compliant, why not Cloud Spanner? Cloud SQL for PostgreSQL is the best if the question did not mention \"global application\" .. With Cloud SQL for PostgreSQL, you cannot reach the global audience. the latency will be terrible."
      },
      {
        "date": "2024-05-28T00:15:00.000Z",
        "voteCount": 2,
        "content": "According to gemini: While Cloud Spanner is a powerful globally distributed database, it might be overkill for a read-only application with nightly updates. It's also generally more complex to manage compared to Cloud SQL."
      },
      {
        "date": "2024-05-22T04:57:00.000Z",
        "voteCount": 2,
        "content": "Plus Cloud Spanner offers Postgres - opensource version. So it checks that box too."
      },
      {
        "date": "2023-03-07T23:51:00.000Z",
        "voteCount": 2,
        "content": "C. PostgreSQL is a real open source RDBMS"
      },
      {
        "date": "2022-12-25T10:04:00.000Z",
        "voteCount": 3,
        "content": "I'll go for C"
      },
      {
        "date": "2022-12-24T16:03:00.000Z",
        "voteCount": 1,
        "content": "C: Use Cloud SQL for PostgreSQL *** with cross-region replicas."
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 67,
    "url": "https://www.examtopics.com/discussions/google/view/92734-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "Your company is migrating their MySQL database to Cloud SQL and cannot afford any planned downtime during the month of December. The company is also concerned with cost, so you need the most cost-effective solution. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOpen a support ticket in Google Cloud to prevent any maintenance in that MySQL instance during the month of December.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud SQL maintenance settings to prevent any maintenance during the month of December.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate MySQL read replicas in different zones so that, if any downtime occurs, the read replicas will act as the primary instance during the month of December.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a MySQL regional instance so that, if any downtime occurs, the standby instance will act as the primary instance during the month of December."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 3,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-03-13T09:32:00.000Z",
        "voteCount": 7,
        "content": "B.\nYou don\u2019t open a support ticket to prevent maintenance on a managed Cloud SQL instance. Eliminate A. C and D both involve additional costs which the question says is a concern. That leaves B. In Maintenance - Advanced Options you can define a maintenance deny period which can be up to 90 days. So B is the right answer."
      },
      {
        "date": "2023-05-22T10:19:00.000Z",
        "voteCount": 1,
        "content": "Answer B\nhttps://cloud.google.com/sql/docs/mysql/maintenance?hl=fr"
      },
      {
        "date": "2022-12-25T10:08:00.000Z",
        "voteCount": 3,
        "content": "CloudSQL - Edit instance - Maintenance:\nDeny maintenance period\nYou can deny upcoming maintenance by creating a deny period, which can last up to 90 days. You can only have one deny period in effect at a time."
      },
      {
        "date": "2022-12-24T16:02:00.000Z",
        "voteCount": 1,
        "content": "B: Use Cloud SQL maintenance settings to prevent *** any maintenance during the month of December."
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 68,
    "url": "https://www.examtopics.com/discussions/google/view/92733-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "Your online delivery business that primarily serves retail customers uses Cloud SQL for MySQL for its inventory and scheduling application. The required recovery time objective (RTO) and recovery point objective (RPO) must be in minutes rather than hours as a part of your high availability and disaster recovery design. You need a high availability configuration that can recover without data loss during a zonal or a regional failure. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up all read replicas in a different region using asynchronous replication.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up all read replicas in the same region as the primary instance with synchronous replication.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up read replicas in different zones of the same region as the primary instance with synchronous replication, and set up read replicas in different regions with asynchronous replication.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up read replicas in different zones of the same region as the primary instance with asynchronous replication, and set up read replicas in different regions with synchronous replication."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 8,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 7,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-03-13T14:48:00.000Z",
        "voteCount": 11,
        "content": "A.\nGoogle\u2019s documentation (https://cloud.google.com/sql/faq) states MySQL read replicas use asynchronous replication. That eliminates B, C and D because they all say to use read replicas with synchronous replication. That leaves A. A states read replicas (plural) in a different region which suggests more than one read replica, each in a different zone. With the primary instance in region A (zone 1) and presumably a failover replica also in region A (zone 2), read replicas in region B (zone 1 and 2) would provide HA in the event of a region or zone failure."
      },
      {
        "date": "2023-11-13T12:03:00.000Z",
        "voteCount": 1,
        "content": "No you're wrong. D doesn't say synchronous.  And I think option A with having HA in different regions (rather than zones) would increase latency. Usually for HA it's assigned within the same region but different zone. I think the option would be D"
      },
      {
        "date": "2024-07-22T11:44:00.000Z",
        "voteCount": 1,
        "content": "Yes, i was wondering about latency, however, I have seen dynamic DBA is very consistent with good answers, probably he is missing that point.. and you're right it says asynchronous.."
      },
      {
        "date": "2024-07-22T12:51:00.000Z",
        "voteCount": 1,
        "content": "Nevermind, after re-reading, it says synchronous at the end. Ill go for A."
      },
      {
        "date": "2022-12-24T16:01:00.000Z",
        "voteCount": 9,
        "content": "C: Set up read replicas in different zones *** of the same region as the primary instance with *** synchronous replication, and set up read replicas in different regions with asynchronous replication."
      },
      {
        "date": "2024-05-27T02:40:00.000Z",
        "voteCount": 1,
        "content": "Cloud SQL for Postgres is using streaming replication which is asynchronous by default"
      },
      {
        "date": "2024-05-27T02:40:00.000Z",
        "voteCount": 1,
        "content": "Cloud SQL for Postgres is using streaming replication which is asynchronous by default"
      },
      {
        "date": "2024-05-13T17:02:00.000Z",
        "voteCount": 1,
        "content": "A. Because replication is asynchronous, when a regional outage occurs and a failover is attempted, some recent transactions that were committed to the primary may be lost (not replicated to the replica) - (https://cloud.google.com/sql/docs/postgres/replication/cross-region-replicas)"
      },
      {
        "date": "2024-04-30T02:59:00.000Z",
        "voteCount": 1,
        "content": "How is my data replicated?\nMySQL instances: MySQL instances provide a high availability configuration and MySQL read replicas. MySQL read replicas use asynchronous replication.\n\nPostgreSQL instances provide a high availability configuration and read replicas.\n\nSQL Server instances provide a high availability configuration and read replicas.\n\nhttps://cloud.google.com/sql/faq"
      },
      {
        "date": "2023-12-19T08:20:00.000Z",
        "voteCount": 1,
        "content": "Cloud SQL for MySQL doesn't directly support synchronous replication for read replicas, but third-party tools can be used for this purpose. Thus, C provides a better strategy for meeting the objectives."
      },
      {
        "date": "2023-11-13T12:05:00.000Z",
        "voteCount": 1,
        "content": "Set up read replicas in different zones of the same region as the primary instance with asynchronous replication, and set up read replicas in different regions with synchronous replication. Synchronous is for data consistency and Asynchronous is for speed. So option D covers all aspects discussed."
      },
      {
        "date": "2023-10-28T12:52:00.000Z",
        "voteCount": 1,
        "content": "The answer is A. Please see this link - https://cloud.google.com/sql/faq\nWhen it comes to read replica, there is no synchronous replication. \nHow is my data replicated?\nMySQL instances: MySQL instances provide a high availability configuration and MySQL read replicas. MySQL read replicas use asynchronous replication."
      },
      {
        "date": "2023-10-03T12:24:00.000Z",
        "voteCount": 1,
        "content": "If they meant for C to be a standby instance with synchronous replication and then a read replica in other regions with asynchronous replication then it would make sense. However, since it says read replicas only and read replicas do not get async replicated, then it should be A. Hard to say though because A isn't going to protect from data loss either.\nNo really good answer here and badly worded question."
      },
      {
        "date": "2023-09-08T21:55:00.000Z",
        "voteCount": 2,
        "content": "Vote C"
      },
      {
        "date": "2023-07-17T07:52:00.000Z",
        "voteCount": 3,
        "content": "I'm hung up on the phrase \"without data loss\". There is always a chance for some loss of data with asynchronous replication. However, Cloud SQL doesn't support synchronous replication on read replicas (as others have stated). There are NO VALID ANSWERS provided, and the authors of the question clearly have no idea what they're talking about."
      },
      {
        "date": "2023-06-24T05:32:00.000Z",
        "voteCount": 2,
        "content": "Cloud SQL read replicas do not use synchronous replication. Instead, they use asynchronous replication. This means that there is a delay between when a change is made to the primary instance and when it is replicated to the read replica. The delay is typically a few seconds, but it can be longer depending on the network latency between the two instances.\n\nThese leaves A at the end!"
      },
      {
        "date": "2023-03-08T00:41:00.000Z",
        "voteCount": 1,
        "content": "\"without data loss\"  = \"sync replication\""
      },
      {
        "date": "2023-03-05T15:53:00.000Z",
        "voteCount": 1,
        "content": "Non doubt"
      },
      {
        "date": "2023-02-12T23:46:00.000Z",
        "voteCount": 3,
        "content": "C is correct answer"
      },
      {
        "date": "2022-12-31T09:29:00.000Z",
        "voteCount": 4,
        "content": "C is correct answer - see cross-region replica - uses asynch vs synch for replicas in same region."
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 69,
    "url": "https://www.examtopics.com/discussions/google/view/92732-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "Your hotel booking company is expanding into Country A, where personally identifiable information (PII) must comply with regional data residency requirements and audits. You need to isolate customer data in Country A from the rest of the customer data. You want to design a multi-tenancy strategy to efficiently manage costs and operations. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tApply a schema data management pattern.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tApply an instance data management pattern.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tApply a table data management pattern.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tApply a database data management pattern."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 10,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-01-06T12:15:00.000Z",
        "voteCount": 6,
        "content": "I think B is the correct one.https://cloud.google.com/solutions/implementing-multi-tenancy-cloud-spanner#multi-tenancy-data-management-patterns"
      },
      {
        "date": "2024-05-02T02:32:00.000Z",
        "voteCount": 1,
        "content": "to complete that it should be B: https://cloud.google.com/solutions/implementing-multi-tenancy-cloud-spanner#data_management_patterns_and_tenant_lifecycle_management"
      },
      {
        "date": "2024-04-30T03:04:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/solutions/implementing-multi-tenancy-cloud-spanner#multi-tenancy-data-management-patterns"
      },
      {
        "date": "2023-10-03T12:41:00.000Z",
        "voteCount": 3,
        "content": "B because when you deploy a Cloud SQL or Spanner instance you can't specifically target a region at the schema, table, or database levels. You select the region at the instance level."
      },
      {
        "date": "2023-09-09T21:07:00.000Z",
        "voteCount": 1,
        "content": "D is correct answer!"
      },
      {
        "date": "2023-03-13T09:55:00.000Z",
        "voteCount": 4,
        "content": "B.\nFrom the question, it appears the multi-tenancy solution needs to be multi-regional. That suggests Spanner as the database platform. Within a multi-region instance, you cannot place a database into a specific region (D). The same is also true for a table (C) and a schema (A). That leaves B. Makes sense that you would have an instance specific to a given region to comply with the data residency requirements. Even if you did this with Cloud SQL - using the same database/schema/table design everywhere, you would still need to deploy into a specific region to satisfy the data regulations. So B still makes sense."
      },
      {
        "date": "2023-03-08T00:55:00.000Z",
        "voteCount": 2,
        "content": "What is the difference between database and instance? \nInstance is where the database processing is done (memory, shorting, calculating)  and database is where the data is stored.  The task here is to isolate the data and keep the logic as simple as possible.  So D looks like the best option to me. using one system with 1 instance with 1 set of logic ( views, procedures). But, separate , repeated set of base tables (for each PII) stored physically in different regions (data sets, databases, tablespaces)."
      },
      {
        "date": "2022-12-31T09:49:00.000Z",
        "voteCount": 2,
        "content": "I think it's B.  Summary table at link shows only d. database and b. instance options as having high rating for reg &amp; compliance.  Option d. database does not have location flexibility - i.e. can't change location for different tenants, so then 'b.'"
      },
      {
        "date": "2022-12-31T09:50:00.000Z",
        "voteCount": 1,
        "content": "link being... https://cloud.google.com/solutions/implementing-multi-tenancy-cloud-spanner#multi-tenancy-data-management-patterns"
      },
      {
        "date": "2022-12-26T20:24:00.000Z",
        "voteCount": 3,
        "content": "B\nhttps://cloud.google.com/solutions/implementing-multi-tenancy-cloud-spanner"
      },
      {
        "date": "2022-12-25T10:27:00.000Z",
        "voteCount": 1,
        "content": "I think it's schema: A schema is a collection of database objects like tables, triggers, stored procedures, etc."
      },
      {
        "date": "2022-12-24T16:00:00.000Z",
        "voteCount": 2,
        "content": "A: Apply a schema data management pattern.\n\u2022\tSchema: A tenant resides in exclusive tables within a database, and several tenants can be located in the same database."
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 70,
    "url": "https://www.examtopics.com/discussions/google/view/92731-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "You work for a financial services company that wants to use fully managed database services. Traffic volume for your consumer services products has increased annually at a constant rate with occasional spikes around holidays. You frequently need to upgrade the capacity of your database. You want to use Cloud Spanner and include an automated method to increase your hardware capacity to support a higher level of concurrency. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse linear scaling to implement the Autoscaler-based architecture\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse direct scaling to implement the Autoscaler-based architecture.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpgrade the Cloud Spanner instance on a periodic basis during the scheduled maintenance window.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up alerts that are triggered when Cloud Spanner utilization metrics breach the threshold, and then schedule an upgrade during the scheduled maintenance window."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-03-13T10:03:00.000Z",
        "voteCount": 9,
        "content": "A.\nAn automated method eliminates C and D. Autoscaler is the way to go. Autoscaler has 3 scaling methods. Stepwise, linear and direct. From Google\u2019s documentation:\nStepwise - Stepwise scaling is useful for workloads that have small or multiple peaks. \nLinear - Linear scaling is best used with load patterns that change more gradually or have a few large peaks. \nDirect - Direct scaling provides an immediate increase in capacity.\nA is the obvious answer. https://cloud.google.com/spanner/docs/autoscaling-overview"
      },
      {
        "date": "2024-05-02T02:35:00.000Z",
        "voteCount": 1,
        "content": "A: https://cloud.google.com/spanner/docs/autoscaler-tool-overview#linear\n\nLinear scaling is best used with load patterns that change more gradually or have a few large peaks. The method calculates the minimum number of nodes or processing units required to keep utilization below the scaling threshold. The number of nodes or processing units added or removed in each scaling event is not limited to a fixed step amount."
      },
      {
        "date": "2023-03-08T02:02:00.000Z",
        "voteCount": 3,
        "content": "Linear\nLinear scaling is best used with load patterns that change more gradually or have a few large peaks. The method calculates the minimum number of nodes or processing units required to keep utilization below the scaling threshold. The number of nodes or processing units added or removed in each scaling event is not limited to a fixed step amount.\n\nhttps://cloud.google.com/spanner/docs/autoscaling-overview#linear"
      },
      {
        "date": "2022-12-25T10:41:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/architecture/autoscaling-cloud-spanner#linear"
      },
      {
        "date": "2022-12-24T15:59:00.000Z",
        "voteCount": 3,
        "content": "A: Use linear scaling to implement the Autoscaler-based architecture\nLinear scaling is best used with load patterns that change more gradually or have a few ***** large peaks. The method calculates the minimum number of nodes or processing units required to keep utilization below the scaling threshold. The number of nodes or processing units added or removed in each scaling event is not limited to a fixed step amount."
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 71,
    "url": "https://www.examtopics.com/discussions/google/view/92730-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "Your organization has a busy transactional Cloud SQL for MySQL instance. Your analytics team needs access to the data so they can build monthly sales reports. You need to provide data access to the analytics team without adversely affecting performance. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a read replica of the database, provide the database IP address, username, and password to the analytics team, and grant read access to required tables to the team.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a read replica of the database, enable the cloudsql.iam_authentication flag on the replica, and grant read access to required tables to the analytics team.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable the cloudsql.iam_authentication flag on the primary database instance, and grant read access to required tables to the analytics team.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tProvide the database IP address, username, and password of the primary database instance to the analytics, team, and grant read access to required tables to the team."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 15,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 5,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-08-05T23:14:00.000Z",
        "voteCount": 1,
        "content": "I'll go with A, since in IAM I can't grant access on specific table \nIt's Role access (admin,client,editor,..etc)"
      },
      {
        "date": "2023-10-03T13:24:00.000Z",
        "voteCount": 4,
        "content": "B because while both A and B work, B seems more native and secure since you aren't passing around database authentication passwords. You can also better manage the authentication through Google groups since you're using IAM."
      },
      {
        "date": "2023-09-26T14:28:00.000Z",
        "voteCount": 1,
        "content": "B is the correct answer. Option A could work in some scenarios but has some drawbacks."
      },
      {
        "date": "2023-07-17T08:14:00.000Z",
        "voteCount": 1,
        "content": "IAM auth is useful if the consuming analytics solution also integrates with IAM, but nothing in the question stated or suggested that. Therefore I vote for A."
      },
      {
        "date": "2023-06-29T06:08:00.000Z",
        "voteCount": 2,
        "content": "B offers a balance between providing data access to the analytics team and maintaining the performance of the busy transactional Cloud SQL instance."
      },
      {
        "date": "2023-03-08T02:21:00.000Z",
        "voteCount": 2,
        "content": "You can enable IAM database authentication on an instance using the cloudsql.iam_authentication flag. \nOnce you enable this flag, the instance enables logins from accounts that are configured for IAM database authentication."
      },
      {
        "date": "2023-03-05T12:11:00.000Z",
        "voteCount": 3,
        "content": "\"Read replicas do not have the cloudsql.iam_authentication flag enabled automatically when it is enabled on the primary instance.\" https://cloud.google.com/sql/docs/postgres/replication/create-replica#configure_iam_replicas"
      },
      {
        "date": "2023-03-05T12:11:00.000Z",
        "voteCount": 1,
        "content": "Here the documentation for MySQL https://cloud.google.com/sql/docs/mysql/replication/create-replica"
      },
      {
        "date": "2023-02-23T04:48:00.000Z",
        "voteCount": 2,
        "content": "I think its B"
      },
      {
        "date": "2023-01-28T08:58:00.000Z",
        "voteCount": 4,
        "content": "B will still need grant database privileges to the IAM user\nwhere A we can assume they use built-in database authentication and database privileges are already granted"
      },
      {
        "date": "2022-12-31T10:25:00.000Z",
        "voteCount": 1,
        "content": "I think B a trick answer and we're being directed to Cloud SQL built-in database authentication .  per link\n\"When using IAM authentication, permission to access a resource (a Cloud SQL instance) isn't granted directly to the end user. Instead, permissions are grouped into roles, and roles are granted to principals.\"\nand\n\"Roles. For IAM database authentication, a user requires the cloudsql.instances.login permission to log in to an instance. To get this permission, you bind the user or service account to either the predefined Cloud SQL Instance User role or a custom role that bundles the permission. \""
      },
      {
        "date": "2022-12-31T10:35:00.000Z",
        "voteCount": 1,
        "content": "from Cloud Sql Built In Authentication page...\n\n\"Although IAM database authentication is more secure and reliable, you might prefer to use built-in authentication or a hybrid authentication model that includes both authentication types.\n\nYou might create and manage local database users locally within a database to allow specific persons or applications to access a database. Such database users own the objects they create in the database. Cloud SQL offers strong built-in password enforcement. You can define and enable such enforcement through password policies.\"\n\nhttps://cloud.google.com/sql/docs/mysql/built-in-authentication\n\nHence, vote for A"
      },
      {
        "date": "2022-12-31T11:03:00.000Z",
        "voteCount": 1,
        "content": "There are other steps required for iam authentication that are left out of B, supporting contention this is wrong (1) iam user has to be added to instance, (2) iam user granted access  seel link https://cloud.google.com/sql/docs/mysql/add-manage-iam-users#creating-a-database-user"
      },
      {
        "date": "2022-12-25T10:47:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/sql/docs/mysql/authentication#instance_configuration_for"
      },
      {
        "date": "2022-12-24T15:58:00.000Z",
        "voteCount": 1,
        "content": "You can enable IAM database authentication on an instance using the cloudsql.iam_authentication flag. Once you enable this flag, the instance enables logins from accounts that are configured for IAM database authentication.\nCloud SQL IAM database authentication for different instance scenarios\nRead replicas\tIAM database authentication is not enabled in a read replica automatically, even when it is enabled on the primary instance. After you create a read replica, you need to add IAM database authentication. For more information, see Configuring read replica logins for IAM database authentication."
      },
      {
        "date": "2022-12-24T15:57:00.000Z",
        "voteCount": 1,
        "content": "B: Create a ***** read replica of the database, enable the ***** cloudsql.iam_authentication flag on the replica, and grant read access to required tables to the analytics team."
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 72,
    "url": "https://www.examtopics.com/discussions/google/view/92729-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "Your organization stores marketing data such as customer preferences and purchase history on Bigtable. The consumers of this database are predominantly data analysts and operations users. You receive a service ticket from the database operations department citing poor database performance between 9 AM-10 AM every day. The application team has confirmed no latency from their logs. A new cohort of pilot users that is testing a dataset loaded from a third-party data provider is experiencing poor database performance. Other users are not affected. You need to troubleshoot the issue. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIsolate the data analysts and operations user groups to use different Bigtable instances.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCheck the Cloud Monitoring table/bytes_used metric from Bigtable.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Key Visualizer for Bigtable.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd more nodes to the Bigtable cluster."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-12-27T02:51:00.000Z",
        "voteCount": 8,
        "content": "First step of performance troubleshooting is to use Key Visualizer https://cloud.google.com/bigtable/docs/performance#troubleshooting"
      },
      {
        "date": "2023-03-13T13:33:00.000Z",
        "voteCount": 7,
        "content": "C.\nThe test data set is likely to have poor row key design. When loaded into the database, that data is probably hitting the same node and causing a hot spot. Addng more nodes will not solve a hot spot. Eliminate D. Neither will creating a new instance (A). B might be of interest, but is not the primary tool to diagnose Bigtable performance problems. For that, use Key Visualizer. Best answer is C.\nhttps://cloud.google.com/bigtable/docs/keyvis-overview"
      },
      {
        "date": "2024-05-21T23:52:00.000Z",
        "voteCount": 1,
        "content": "Agree with Key Visualizer"
      },
      {
        "date": "2022-12-24T15:57:00.000Z",
        "voteCount": 1,
        "content": "B: Check the Cloud Monitoring table/bytes_used *** metric from Bigtable.\nRead throughput\tThe number of bytes per second of response data sent. This metric refers to the full amount of data that is returned after filters are applied.\n\nWrite throughput\tThe number of bytes per second that were received when data was written."
      },
      {
        "date": "2022-12-25T10:56:00.000Z",
        "voteCount": 1,
        "content": "Where did you get that from? table/bytes_used - Amount of compressed data stored in a table. Sampled every 60 seconds. After sampling, data is not visible for up to 120 seconds.\nhttps://cloud.google.com/bigtable/docs/metrics#:~:text=Bigtable%20application%20profile.-,table/bytes_used,-GA%0AData%20stored"
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 73,
    "url": "https://www.examtopics.com/discussions/google/view/92728-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "Your company is developing a new global transactional application that must be ACID-compliant and have 99.999% availability. You are responsible for selecting the appropriate Google Cloud database to serve as a datastore for this new application. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Firestore.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Spanner.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud SQL.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Bigtable."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-12-25T11:03:00.000Z",
        "voteCount": 8,
        "content": "99,999% availability and global - it's either Spanner or Firestore, but 'transactional' eliminates Firestore"
      },
      {
        "date": "2024-08-30T10:35:00.000Z",
        "voteCount": 1,
        "content": "Cloud SQL Enterprise Plus edition with high availability (HA)&gt;=99.99%\nCloud SQL Enterprise edition with high availability (HA)&gt;=99.95%\nCloud Spanner - Multi-Regional Instance\t&gt;= 99.999%\nCloud Spanner - Regional Instance\t&gt;= 99.99%\nFirestore Multi-Region\t&gt;= 99.999%\nFirestore Regional\t&gt;= 99.99%\nCloud Bigtable - Zonal instance (single cluster)\t&gt;= 99.9%\nThe key is transactional because spanner and firestore covers global and 99.999 , just spanner covers transactional"
      },
      {
        "date": "2024-04-30T03:09:00.000Z",
        "voteCount": 1,
        "content": "99.999% spanner"
      },
      {
        "date": "2023-07-23T08:29:00.000Z",
        "voteCount": 1,
        "content": "B: Cloud Spanner"
      },
      {
        "date": "2022-12-24T15:55:00.000Z",
        "voteCount": 2,
        "content": "B: Use Cloud Spanner."
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 74,
    "url": "https://www.examtopics.com/discussions/google/view/92727-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "You want to migrate your PostgreSQL database from another cloud provider to Cloud SQL. You plan on using Database Migration Service and need to assess the impact of any known limitations. What should you do? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIdentify whether the database has over 512 tables.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIdentify all tables that do not have a primary key.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIdentity all tables that do not have at least one foreign key.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIdentify whether the source database is encrypted using pgcrypto extension.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIdentify whether the source database uses customer-managed encryption keys (CMEK).\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "BE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BE",
        "count": 8,
        "isMostVoted": true
      },
      {
        "answer": "BD",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-25T11:10:00.000Z",
        "voteCount": 7,
        "content": "https://cloud.google.com/database-migration/docs/postgres/known-limitations"
      },
      {
        "date": "2024-04-30T03:12:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/database-migration/docs/postgres/known-limitations:\nIf encrypted databases require customer-managed encryption keys to decrypt the databases, and if Database Migration Service doesn't have access to the keys, then the databases can't be migrated.\n\nFor tables that don't have primary keys, Database Migration Service supports migration of the initial snapshot and INSERT statements during the change data capture (CDC) phase. You should migrate UPDATE and DELETE statements manually."
      },
      {
        "date": "2024-02-08T15:01:00.000Z",
        "voteCount": 2,
        "content": "PKs (needed) and pgcrypto extension (confirm if enabled) to assess limitations during migration"
      },
      {
        "date": "2023-12-15T04:09:00.000Z",
        "voteCount": 4,
        "content": "From this link (https://cloud.google.com/database-migration/docs/postgres/known-limitations), correct answers should be B and D.\n\nExcerpts supporting these choices:\n\n\"For tables that don't have primary keys, Database Migration Service supports migration of the initial snapshot and INSERT statements during the change data capture (CDC) phase. You should migrate UPDATE and DELETE statements manually.\"\n\n\"If encrypted databases require customer-managed encryption keys to decrypt the databases, and if Database Migration Service doesn't have access to the keys, then the databases can't be migrated.\nHowever, if customer data is encrypted by the pgcrypto extension, then the data can be migrated with Database Migration Service (because Cloud SQL supports the extension).\"\n\nTherefore, for the migration to be executable:\n\n* Only tables with a primary key can be migrated;\n* Only data encrypted by the pgcrypto extension can be migrated."
      },
      {
        "date": "2023-09-26T15:00:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is B &amp; D. option E (customer-managed encryption keys) is not directly related to the limitations of Database Migration Service for PostgreSQL to Cloud SQL."
      },
      {
        "date": "2023-03-13T14:20:00.000Z",
        "voteCount": 3,
        "content": "B, E.\nA is irrelevant. So is C. Cloud SQL supports the pgcrypto extension so that\u2019s also irrelevant. That leaves B and E. The link provided by chelbsik is spot on."
      },
      {
        "date": "2022-12-24T15:54:00.000Z",
        "voteCount": 3,
        "content": "B: Identify all tables that do not have a ***** primary key. Tables in databases that don't have a primary key aren't migrated automatically by Database Migration Service. You must manually migrate them. The Migrating tables without a primary key section outlines different strategies.\n\n E: Identify whether the source database uses customer-managed encryption ***** keys (CMEK)."
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 75,
    "url": "https://www.examtopics.com/discussions/google/view/92726-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "Your organization is running a Firestore-backed Firebase app that serves the same top ten news stories on a daily basis to a large global audience. You want to optimize content delivery while decreasing cost and latency. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable serializable isolation in the Firebase app.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy a US multi-region Firestore location.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBuild a Firestore bundle, and deploy bundles to Cloud CDN.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Firestore index on the news story date."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-03-13T14:26:00.000Z",
        "voteCount": 6,
        "content": "C.\nMust decrease cost and latency. Creating or deploying anything would likely cost more, so eliminate B and D. That leaves A and C. A global audience strongly suggests serving content via Google\u2019s Content Delivery Network. Changing the isolation level won\u2019t decrease cost or latency, so C is the best answer."
      },
      {
        "date": "2024-04-30T03:13:00.000Z",
        "voteCount": 1,
        "content": "https://firebase.blog/posts/2021/04/firestore-supports-data-bundles/"
      },
      {
        "date": "2023-09-10T12:46:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is B ==&gt;B. Deploy a US multi-region Firestore location."
      },
      {
        "date": "2022-12-25T11:12:00.000Z",
        "voteCount": 3,
        "content": "I'll go for C"
      },
      {
        "date": "2022-12-24T15:52:00.000Z",
        "voteCount": 1,
        "content": "C: Build a Firestore bundle, and deploy bundles to Cloud *** CDN."
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 76,
    "url": "https://www.examtopics.com/discussions/google/view/92725-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "You need to migrate a 1 TB PostgreSQL database from a Compute Engine VM to Cloud SQL for PostgreSQL. You want to ensure that there is minimal downtime during the migration. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExport the data from the existing database, and load the data into a new Cloud SQL database.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Migrate for Compute Engine to complete the migration.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Datastream to complete the migration.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Database Migration Service to complete the migration.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-12-25T11:24:00.000Z",
        "voteCount": 8,
        "content": "Database Migration Service should hadle this."
      },
      {
        "date": "2024-01-22T05:14:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/blog/products/databases/database-migration-service-now-available-for-cloud-sql-and-more"
      },
      {
        "date": "2023-06-29T03:09:00.000Z",
        "voteCount": 1,
        "content": "https://www.cloudskillsboost.google/focuses/22792?parent=catalog"
      },
      {
        "date": "2023-03-13T14:59:00.000Z",
        "voteCount": 4,
        "content": "D.\nThe key is minimal downtime. That eliminates A. Migrate for Compute Engine operates at the VM level. That eliminates B. Migrating from a GCE VM to Cloud SQL is a classic use case for the Database Migration Service. D is the best answer."
      },
      {
        "date": "2022-12-24T15:51:00.000Z",
        "voteCount": 1,
        "content": "A: Export the data from the existing database, and load the data into a new Cloud SQL database.\nFor a standard export from Cloud SQL, the export is run while the database is online. When the data being exported is smaller, the impact is likely to be minimal. However, when there are large databases, or large objects, such as BLOBs in the database, there's the possibility that the export might degrade database performance. This might impact the time it takes to perform database queries and operations against the database. After you start an export, it's not possible to stop it if your database starts to respond slowly.\nThere are three main approaches to database migration: big bang data migration, trickle data migration, and zero downtime migration."
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 77,
    "url": "https://www.examtopics.com/discussions/google/view/92724-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "You have a large Cloud SQL for PostgreSQL instance. The database instance is not mission-critical, and you want to minimize operational costs. What should you do to lower the cost of backups in this environment?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the automated backups to occur every other day to lower the frequency of backups.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the storage tier of the automated backups from solid-state drive (SSD) to hard disk drive (HDD).",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSelect a different region to store your backups.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReduce the number of automated backups that are retained to two (2).\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-03-13T18:00:00.000Z",
        "voteCount": 7,
        "content": "D.\nA is wrong - you can\u2019t do that. Automated backups are either on or off. You only get to choose the 4 hour window and if the backups are stored in a single or multi-region. B is wrong for the same reason. C wouldn\u2019t lower the cost, but reducing the number of backups retained would. The default is 7. This can be verified using the Google Pricing Calculator."
      },
      {
        "date": "2023-03-01T16:24:00.000Z",
        "voteCount": 6,
        "content": "By default, for each instance, Cloud SQL retains seven automated backups, in addition to on-demand backups. You can configure how many automated backups to retain (from 1 to 365). We charge a lower rate for backup storage than for other types of instances.\nhttps://cloud.google.com/sql/docs/mysql/backup-recovery/backups"
      },
      {
        "date": "2022-12-25T12:08:00.000Z",
        "voteCount": 3,
        "content": "I'll go for D, seems like the best option"
      },
      {
        "date": "2022-12-24T15:50:00.000Z",
        "voteCount": 2,
        "content": "D: Reduce the number of automated backups that are retained to two ***** (2)."
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 78,
    "url": "https://www.examtopics.com/discussions/google/view/92723-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "You are the primary DBA of a Cloud SQL for PostgreSQL database that supports 6 enterprise applications in production. You used Cloud SQL Insights to identify inefficient queries and now need to identify the application that is originating the inefficient queries. You want to follow Google-recommended practices. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tShut down and restart each application.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite a utility to scan database query logs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite a utility to scan application logs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse query tags to add application-centric database monitoring.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-03-13T17:39:00.000Z",
        "voteCount": 6,
        "content": "D.\nA is just wrong on so many levels. B and C represent work and unless you know which SQL belongs to which application won\u2019t actually help. First you need to add query tags to help identify application SQL. D is therefore correct.\nhttps://cloud.google.com/sql/docs/postgres/using-query-insights#filter_by_query_tags"
      },
      {
        "date": "2024-04-30T03:17:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/sql/docs/postgres/using-query-insights#filter_by_query_tags\n\nTo troubleshoot an application, you must first add tags to your SQL queries. Query load tags provide a breakdown of the query load of the selected tag over time."
      },
      {
        "date": "2023-07-25T23:08:00.000Z",
        "voteCount": 2,
        "content": "D: Use query tags to add application-centric database monitoring."
      },
      {
        "date": "2022-12-25T12:11:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/sql/docs/postgres/using-query-insights#specific-query:~:text=Store%20application%20tags,with%20application%20monitoring."
      },
      {
        "date": "2022-12-24T15:49:00.000Z",
        "voteCount": 3,
        "content": "D: Use query tags to add application-centric database monitoring.\nStore application tags\nStores application tags that help you determine the APIs and model-view-controller (MVC) routes that are making requests and group the data to run metrics against it. This option requires you to comment queries with a specific set of tags using the sqlcommenter open source object-relational mapping (ORM) auto-instrumentation library. This information helps Query insights identify the source of a problem and the MVC the problem is coming from. Application paths help you with application monitoring."
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 79,
    "url": "https://www.examtopics.com/discussions/google/view/92686-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "You are designing a database strategy for a new web application. You plan to start with a small pilot in one country and eventually expand to millions of users in a global audience. You need to ensure that the application can run 24/7 with minimal downtime for maintenance. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Spanner in a regional configuration.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Spanner in a multi-region configuration.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud SQL with cross-region replicas.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse highly available Cloud SQL with multiple zones."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 17,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-27T02:55:00.000Z",
        "voteCount": 6,
        "content": "Changing my vote to A"
      },
      {
        "date": "2024-06-20T01:22:00.000Z",
        "voteCount": 1,
        "content": "multi-regional"
      },
      {
        "date": "2024-05-22T09:59:00.000Z",
        "voteCount": 1,
        "content": "Voting for A"
      },
      {
        "date": "2024-04-30T03:18:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/spanner/docs/instance-configurations#move-instance\n\nYou can move your Spanner instance from any instance configuration to any other instance configuration, including between regional and multi-region configurations. Moving your instance does not cause downtime, and Spanner continues to provide the usual transaction guarantees, including strong consistency, during the move."
      },
      {
        "date": "2023-11-05T04:00:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/spanner/docs/instance-configurations#move-instance\nYou can move your Spanner instance from any instance configuration to any other instance configuration, including between regional and multi-region configurations. Moving your instance does not cause downtime, and Spanner continues to provide the usual transaction guarantees, including strong consistency, during the move.\n\nhttps://cloud.google.com/spanner/docs/move-instance\nhttps://cloud.google.com/spanner/docs/move-instance#how-instance-move"
      },
      {
        "date": "2023-10-03T21:04:00.000Z",
        "voteCount": 2,
        "content": "Going with B because \n- Small pilot in one \"country\". A country would be multiple regions\n- Don't have to change to multi-region spanner in the future. \n- Q doesn't say anything about being cost-efficient so might as well start with multi-region."
      },
      {
        "date": "2023-09-11T11:58:00.000Z",
        "voteCount": 1,
        "content": "correct answer is B. Use Cloud Spanner in a multi-region configuration."
      },
      {
        "date": "2023-03-13T17:51:00.000Z",
        "voteCount": 4,
        "content": "A.\nInitially it\u2019s a choice between Cloud SQL and Cloud Spanner. Given the final objective is a massive user base on a global scale, it suggests Spanner. Therefore, starting with Cloud SQL then migrating to Cloud Spanner doesn\u2019t seem like a smart move. Eliminate C and D. You can start small with Spanner (single region, 100 processing units) and convert to multi-region by contacting Google. The wording to that effect is on the Edit Spanner screen in the GCP console. So, given those details it eliminates B. Best answer is A."
      },
      {
        "date": "2023-03-08T04:50:00.000Z",
        "voteCount": 4,
        "content": "\"You plan to start with a small pilot in one country\" == small spanner ==&gt; regional."
      },
      {
        "date": "2023-03-06T06:04:00.000Z",
        "voteCount": 2,
        "content": "You plan to start with a small pilot in one country. so country is multi region right?\nso B ?"
      },
      {
        "date": "2022-12-25T12:21:00.000Z",
        "voteCount": 2,
        "content": "I'll go for B"
      },
      {
        "date": "2022-12-26T12:42:00.000Z",
        "voteCount": 1,
        "content": "I ran a lab, and you can actually convert regional configuration to multi-region with a zero downtime, but you need to contact google for that. Changing my answer to A https://docs.google.com/forms/d/e/1FAIpQLSfZ77ZnuUL0NpU-bOtO5QUkC0cnRCe5YKMiubLXwfV3abBqkg/viewform"
      },
      {
        "date": "2022-12-24T14:25:00.000Z",
        "voteCount": 2,
        "content": "B: Use high availability (HA) *** Cloud SQL with multiple zones."
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 80,
    "url": "https://www.examtopics.com/discussions/google/view/92722-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "Your company is shutting down their on-premises data center and migrating their Oracle databases using Oracle Real Application Clusters (RAC) to Google Cloud. You want minimal to no changes to the applications during the database migration. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the Oracle databases to Cloud Spanner.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the Oracle databases to Compute Engine.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the Oracle databases to Cloud SQL.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the Oracle databases to Bare Metal Solution for Oracle.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-11-05T04:01:00.000Z",
        "voteCount": 2,
        "content": "D\n\nhttps://cloud.google.com/bare-metal?hl=en"
      },
      {
        "date": "2023-03-13T18:04:00.000Z",
        "voteCount": 2,
        "content": "D.\nIn theory you could migrate Oracle to Spanner for PostgreSQL, but that would entail application changes which the questions says can\u2019t happen. Eliminate A. Oracle is not licensed or supported in GCE. Eliminate B. Cloud SQL doesn\u2019t support Oracle. Eliminate C. That leaves D on which you can configure and run RAC."
      },
      {
        "date": "2023-03-08T05:04:00.000Z",
        "voteCount": 2,
        "content": "D. Oracle databases to Bare Metal Solution for Oracle will do the job."
      },
      {
        "date": "2022-12-25T12:22:00.000Z",
        "voteCount": 4,
        "content": "D is the Google-recommended solution"
      },
      {
        "date": "2022-12-24T15:47:00.000Z",
        "voteCount": 2,
        "content": "D: Migrate the Oracle databases to Bare Metal Solution for Oracle."
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 81,
    "url": "https://www.examtopics.com/discussions/google/view/92721-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "Your company wants you to migrate their Oracle, MySQL, Microsoft SQL Server, and PostgreSQL relational databases to Google Cloud. You need a fully managed, flexible database solution when possible. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate all the databases to Cloud SQL.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the Oracle, MySQL, and Microsoft SQL Server databases to Cloud SQL, and migrate the PostgreSQL databases to Compute Engine.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the MySQL, Microsoft SQL Server, and PostgreSQL databases to Compute Engine, and migrate the Oracle databases to Bare Metal Solution for Oracle.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the MySQL, Microsoft SQL Server, and PostgreSQL databases to Cloud SQL, and migrate the Oracle databases to Bare Metal Solution for Oracle.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-07-25T23:11:00.000Z",
        "voteCount": 2,
        "content": "D. Migrate the MySQL, Microsoft SQL Server, and PostgreSQL databases to Cloud SQL, and migrate the Oracle databases to Bare Metal Solution for Oracle."
      },
      {
        "date": "2023-03-13T18:08:00.000Z",
        "voteCount": 3,
        "content": "D.\nCloud SQL doesn\u2019t support Oracle. Eliminate A and B. Migrating 3 of the platforms to GCE would not be a fully managed solution. Eliminate C. D is the only reasonable option, even though it\u2019s not \u201cfully managed\u201d since you have to manage Oracle on BMS."
      },
      {
        "date": "2022-12-25T12:30:00.000Z",
        "voteCount": 4,
        "content": "Oracle should go to the Bare Metal Solution for Oracle only, others - to CloudSQL"
      },
      {
        "date": "2022-12-24T15:47:00.000Z",
        "voteCount": 1,
        "content": "D: Migrate the MySQL, Microsoft SQL Server, and PostgreSQL databases to Cloud SQL, and migrate the Oracle databases to Bare Metal Solution for Oracle."
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 82,
    "url": "https://www.examtopics.com/discussions/google/view/92720-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "You are managing a Cloud SQL for PostgreSQL instance in Google Cloud. You need to test the high availability of your Cloud SQL instance by performing a failover. You want to use the cloud command. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse gcloud sql instances failover &lt;PrimaryInstanceName&gt;.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse gcloud sql instances failover &lt;ReplicaInstanceName&gt;.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse gcloud sql instances promote-replica &lt;PrimaryInstanceName&gt;.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse gcloud sql instances promote-replica &lt;ReplicaInstanceName&gt;."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-03-14T08:52:00.000Z",
        "voteCount": 7,
        "content": "A.\nWe are not promoting a read replica to be the primary instance. Eliminate C and D. When HA is enabled you only deal with the primary instance name, so B doesn\u2019t make sense. That leaves A."
      },
      {
        "date": "2022-12-25T12:32:00.000Z",
        "voteCount": 6,
        "content": "Since primary instance is HA, it can failover between it's two intances"
      },
      {
        "date": "2023-11-05T04:02:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/sdk/gcloud/reference/sql/instances/failover"
      },
      {
        "date": "2023-09-11T12:17:00.000Z",
        "voteCount": 1,
        "content": "correct answer is B! When you initiate the failover on the replica instance, it will be promoted to the primary instance, simulating a high-availability failover event. option A is not the recommended!"
      },
      {
        "date": "2022-12-24T15:46:00.000Z",
        "voteCount": 2,
        "content": "A: Use gcloud sql instances failover ***** &lt;PrimaryInstanceName&gt;."
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 83,
    "url": "https://www.examtopics.com/discussions/google/view/92719-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "You use Python scripts to generate weekly SQL reports to assess the state of your databases and determine whether you need to reorganize tables or run statistics. You want to automate this report but need to minimize operational costs and overhead. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a VM in Compute Engine, and run a cron job.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Cloud Composer instance, and create a directed acyclic graph (DAG).",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Cloud Function, and call the Cloud Function using Cloud Scheduler.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Cloud Function, and call the Cloud Function from a Cloud Tasks queue."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-11-05T04:04:00.000Z",
        "voteCount": 1,
        "content": "Scheduler + CF"
      },
      {
        "date": "2023-06-24T06:19:00.000Z",
        "voteCount": 4,
        "content": "Minimal Cost Approach:\n1. Put the python script on the Cloud Function\n2. Schedule weekly trigger using Cloud Scheduler\n\nCompute Engine and Cloud Composer would increase the cost and making it complex"
      },
      {
        "date": "2023-03-14T09:03:00.000Z",
        "voteCount": 3,
        "content": "C.\nNeed to automate and minimize costs/overhead. Eliminate A. From Google\u2019s documentation, \u201cCloud Composer is a fully managed workflow orchestration service, enabling you to create, schedule, monitor, and manage workflow pipelines that span across clouds and on-premises data centers.\u201d. We don\u2019t need Cloud Composer. Eliminate B. To have something run at a specific time on a specific schedule, you need to use Cloud Scheduler (which has cron like scheduling). Cloud Tasks doesn\u2019t do that. Eliminate D. That leaves C."
      },
      {
        "date": "2023-02-16T23:55:00.000Z",
        "voteCount": 1,
        "content": "C\nCloud Scheduler triggers actions at regular fixed intervals, whereas Cloud Tasks triggers actions based on how the individual task object is configured.\nReference:\nhttps://cloud.google.com/tasks/docs/comp-tasks-sched"
      },
      {
        "date": "2022-12-25T12:38:00.000Z",
        "voteCount": 2,
        "content": "I'll go for C"
      },
      {
        "date": "2022-12-24T15:45:00.000Z",
        "voteCount": 1,
        "content": "C: Create a Cloud Function, and call the Cloud Function using Cloud *** Scheduler.\nCompare Cloud Tasks vs. Cloud Scheduler\nBoth Cloud Tasks and Cloud Scheduler can be used to initiate actions outside of the immediate context. But they have significant differences in functionality and usage. This page helps you understand the differences between them."
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 84,
    "url": "https://www.examtopics.com/discussions/google/view/92718-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "Your company is using Cloud SQL for MySQL with an internal (private) IP address and wants to replicate some tables into BigQuery in near-real time for analytics and machine learning. You need to ensure that replication is fast and reliable and uses Google-managed services. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDevelop a custom data replication service to send data into BigQuery.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud SQL federated queries.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Database Migration Service to replicate tables into BigQuery.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Datastream to capture changes, and use Dataflow to write those changes to BigQuery.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-03-14T09:13:00.000Z",
        "voteCount": 12,
        "content": "D.\nAnytime you see words like \u201cdevelop\u201d or \u201cmanually\u201d be suspicious given this is cloud and everything is supposed to be automated and point-and-click easy. Eliminate A. Federated queries are SQL queries initiated FROM BigQuery to Cloud Spanner or Cloud SQL databases. So B doesn\u2019t make sense. The Database Migration Service does not support BigQuery as a destination database engine. Eliminate C. That leaves D. From Google\u2019s documentation, \u201cDatastream is a serverless and easy-to-use Change Data Capture (CDC) and replication service that allows you to synchronize data across heterogeneous databases, storage systems, and applications reliably and with minimal latency. Datastream supports change data streaming from Oracle and MySQL databases to Google Cloud Storage (GCS). The service offers streamlined integration with Dataflow templates to power up to date materialized views in BigQuery for analytics, replicate their databases into Cloud SQL or Cloud Spanner for database synchronization, or leverage the event stream directly from GCS to realize event-driven architectures.\u201d"
      },
      {
        "date": "2024-05-28T02:44:00.000Z",
        "voteCount": 1,
        "content": "Agree with D"
      },
      {
        "date": "2023-11-05T04:07:00.000Z",
        "voteCount": 1,
        "content": "D because we need replication: \nAs a data analyst, you can query data in Cloud SQL from BigQuery using federated queries (...) Alternatively, to replicate data into BigQuery, you can also use Cloud Data Fusion or Datastream.\nDatastream is a serverless and easy-to-use change data capture (CDC) and replication service that lets you synchronize data reliably, and with minimal latency.\n\nDatastream provides seamless replication of data from operational databases into BigQuery.\n\nreference: https://cloud.google.com/bigquery/docs/cloud-sql-federated-queries https://cloud.google.com/datastream/docs/overview"
      },
      {
        "date": "2023-08-15T07:10:00.000Z",
        "voteCount": 2,
        "content": "You can not connect Datastream directly to Cloud SQL with an internal IP without using A compute instance where a SQL Proxy is deployed to bridge the traffic between Datastream and Cloud SQL. Because connecting to Cloud SQL from Datastream is not possible\n\nhttps://github.com/rocketechgroup/mysql-to-bq-datastream"
      },
      {
        "date": "2023-08-01T07:46:00.000Z",
        "voteCount": 1,
        "content": "B\n\nAs a data analyst, you can query data in Cloud SQL from BigQuery using federated queries.\n\nBigQuery Cloud SQL federation enables BigQuery to query data residing in Cloud SQL in real time, without copying or moving data. Query federation supports both MySQL (2nd generation) and PostgreSQL instances in Cloud SQL.\n\nAlternatively, to replicate data into BigQuery, you can also use Cloud Data Fusion or Datastream. For more about using Cloud Data Fusion, see Replicating data from MySQL to BigQuery.\n\nhttps://cloud.google.com/bigquery/docs/cloud-sql-federated-queries"
      },
      {
        "date": "2022-12-25T12:46:00.000Z",
        "voteCount": 2,
        "content": "I'll go for D"
      },
      {
        "date": "2022-12-24T15:43:00.000Z",
        "voteCount": 3,
        "content": "D: Use Datastream to capture *** changes, and use Dataflow to write those changes to BigQuery.\nDataflow is a fully managed streaming analytics service that minimizes latency, processing time, and cost through autoscaling and batch processing. Dataflow is a managed service for executing a wide variety of data processing patterns. The documentation on this site shows you how to deploy your batch and streaming data processing pipelines using Dataflow, including directions for using service features."
      },
      {
        "date": "2022-12-31T12:47:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/datastream-for-bigquery"
      },
      {
        "date": "2022-12-31T12:50:00.000Z",
        "voteCount": 1,
        "content": "Linked article confirms datastream + dataflow is a \"thing\".  Provides additional customization vs just datastream."
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 85,
    "url": "https://www.examtopics.com/discussions/google/view/92717-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "You are designing a physician portal app in Node.js. This application will be used in hospitals and clinics that might have intermittent internet connectivity. If a connectivity failure occurs, the app should be able to query the cached data. You need to ensure that the application has scalability, strong consistency, and multi-region replication. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Firestore and ensure that the PersistenceEnabled option is set to true.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Memorystore for Memcached.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Pub/Sub to synchronize the changes from the application to Cloud Spanner.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Table.read with the exactStaleness option to perform a read of rows in Cloud Spanner."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-12-25T12:47:00.000Z",
        "voteCount": 5,
        "content": "Intermittment connectivity -&gt; Firestore"
      },
      {
        "date": "2022-12-24T15:42:00.000Z",
        "voteCount": 5,
        "content": "A: Use Firestore and ensure that the PersistenceEnabled ***** option is set to true.\nCloud Firestore is a NoSQL document database that lets you easily store, sync, and query data for your mobile and web apps - at global scale.\nTo use offline persistence, you don't need to make any changes to the code that you use to access Cloud Firestore data. With offline persistence enabled, the Cloud Firestore client library automatically manages online and offline data access and synchronizes local data when the device is back online."
      },
      {
        "date": "2024-05-09T11:59:00.000Z",
        "voteCount": 1,
        "content": "The answer is: C since the use case is for Strong consistency and multi-regional replication."
      },
      {
        "date": "2023-11-05T04:09:00.000Z",
        "voteCount": 1,
        "content": "A:\n\nFirestore supports offline data persistence. This feature caches a copy of the Firestore data that your app is actively using, so your app can access the data when the device is offline. You can write, read, listen to, and query the cached data. When the device comes back online, Firestore synchronizes any local changes made by your app to the Firestore backend.\nhttps://cloud.google.com/firestore/docs/manage-data/enable-offline"
      },
      {
        "date": "2023-09-11T12:37:00.000Z",
        "voteCount": 1,
        "content": "correct answer is C! Firestore  may not provide the same level of strong consistency and multi-region replication as Cloud Spanner. :)"
      },
      {
        "date": "2023-03-14T09:25:00.000Z",
        "voteCount": 4,
        "content": "A.\nAn app with intermitted internet access meaning it does not HAVE to sync with the live data source must mean Firestore in Datastore mode. The only option that mentions Firestore is A.\nhttps://firebase.google.com/docs/firestore/manage-data/enable-offline"
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 86,
    "url": "https://www.examtopics.com/discussions/google/view/92716-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "You manage a production MySQL database running on Cloud SQL at a retail company. You perform routine maintenance on Sunday at midnight when traffic is slow, but you want to skip routine maintenance during the year-end holiday shopping season. You need to ensure that your production system is available 24/7 during the holidays. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDefine a maintenance window on Sundays between 12 AM and 1 AM, and deny maintenance periods between November 1 and January 15.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDefine a maintenance window on Sundays between 12 AM and 5 AM, and deny maintenance periods between November 1 and February 15.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBuild a Cloud Composer job to start a maintenance window on Sundays between 12 AM and 1AM, and deny maintenance periods between November 1 and January 15.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Cloud Scheduler job to start maintenance at 12 AM on Sundays. Pause the Cloud Scheduler job between November 1 and January 15."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 12,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-25T12:51:00.000Z",
        "voteCount": 9,
        "content": "A loogs good. B is impossible because you can't hold maintenance for more than a 90 days period"
      },
      {
        "date": "2023-02-06T12:34:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer: A\n\"Deny maintenance period. A block of days in which Cloud SQL does not schedule maintenance. Deny maintenance periods can be up to 90 days long. \"\n\nhttps://cloud.google.com/sql/docs/mysql/maintenance"
      },
      {
        "date": "2023-03-14T09:31:00.000Z",
        "voteCount": 5,
        "content": "A.\nThis is a maintenance deny question, so eliminate C and D. Maintenance deny periods can be set that last up to 90 days. That eliminates B since the period mentioned would be 107 days. B is also eliminated because the maintenance windows are an hour. You can't set a maintenance window in Cloud SQL to be 5 hours. A is the answer."
      },
      {
        "date": "2024-04-30T03:28:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/sql/docs/mysql/maintenance\nYou can have a deny maintenance period even if you don't have maintenance windows configured for your instance. Deny maintenance periods can span from 1 to 90 days."
      },
      {
        "date": "2023-11-14T04:42:00.000Z",
        "voteCount": 1,
        "content": "Option D allows you to create a Cloud Scheduler job to start maintenance at 12 AM on Sundays and pause the Cloud Scheduler job between November 1 and January 15. This will allow you to perform routine maintenance on Sundays, when traffic is slow, and avoid maintenance during the holiday shopping season, when traffic is highest. This option is the most complete and secure, as it allows you to ensure that your production system is available 24/7 during the holidays."
      },
      {
        "date": "2023-11-05T04:12:00.000Z",
        "voteCount": 2,
        "content": "A:\n\nhttps://cloud.google.com/sql/docs/mysql/maintenance\nhttps://cloud.google.com/sql/docs/mysql/maintenance#management"
      },
      {
        "date": "2023-09-11T12:40:00.000Z",
        "voteCount": 1,
        "content": "correct answer is D. Create a Cloud Scheduler job to start maintenance at 12 AM on Sundays. Pause the Cloud Scheduler job between November 1 and January 15."
      },
      {
        "date": "2022-12-24T15:41:00.000Z",
        "voteCount": 2,
        "content": "A: Define a maintenance window on Sundays between 12 AM and *****1 AM, and deny maintenance periods between November 1 and ***** January 15.\nCloud SQL offers you the ability to configure maintenance updates through a set of maintenance settings.\nYou can configure maintenance to be scheduled at times when brief downtime causes the lowest impact to your applications. For each Cloud SQL instance, you can configure the following:\n\u2022\tMaintenance window. The day of the week and the hour in which Cloud SQL schedules maintenance. Maintenance windows last for ***** one hour. Learn how to configure a maintenance window."
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 87,
    "url": "https://www.examtopics.com/discussions/google/view/92715-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "You want to migrate an on-premises 100 TB Microsoft SQL Server database to Google Cloud over a 1 Gbps network link. You have 48 hours allowed downtime to migrate this database. What should you do? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a change data capture (CDC) migration strategy.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMove the physical database servers from on-premises to Google Cloud.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tKeep the network bandwidth at 1 Gbps, and then perform an offline data migration.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the network bandwidth to 2 Gbps, and then perform an offline data migration.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the network bandwidth to 10 Gbps, and then perform an offline data migration."
    ],
    "answer": "AC",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AC",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "AE",
        "count": 11,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-24T15:39:00.000Z",
        "voteCount": 6,
        "content": "AUse a change data capture ***** (CDC) migration strategy.\n\n BMove the physical database servers from on-premises to Google Cloud.\n\n CKeep the network bandwidth at 1 Gbps, and then perform an offline data migration.\n1 Gbps = 1*24*60*60/8 = 10,800 GB = 10 TB per 24 hrs\tSo, 20 TB per 48 hrs\n DIncrease the network bandwidth to 2 Gbps, and then perform an offline data migration.\n\t\t\t\t\t\t\t\t\tSo, 40 TB per 48 hrs\n EIncrease the network bandwidth to 10 ***** Gbps, and then perform an offline data migration.\n\t\t\t\t\t\t\t\t\tSo, 200 TB per 48 hrs\nAnswer: \t\tA E"
      },
      {
        "date": "2023-09-20T02:54:00.000Z",
        "voteCount": 1,
        "content": "what is 8 in formal?"
      },
      {
        "date": "2024-04-19T04:58:00.000Z",
        "voteCount": 1,
        "content": "it's number of bits in 1 byte"
      },
      {
        "date": "2023-03-14T09:51:00.000Z",
        "voteCount": 5,
        "content": "A, E.\nAccording to Google\u2019s data transfer chart, 100 TB across a 1 Gbps link would take 12 days. I don\u2019t think you can physically move your own equipment into a Google DC. Eliminate B. Increasing the bandwidth to 2 Gbps and doing an offline migration anyway wouldn\u2019t help. The Google Transfer Appliance comes in 7 TB, 40 TB and 300 TB sizes. However, the turnaround time (Google ships the appliance to you, you load it with data, you ship it back, Google unloads it to a Cloud Storage bucket) is approximately 3 weeks. Eliminate C and D. That leaves A and E. A 10 Gbps link would transfer 100 TB in 30 hours. That leaves 18 hours for a CDC solution to sync the data.\nhttps://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets#online_versus_offline_transfer"
      },
      {
        "date": "2023-07-17T10:09:00.000Z",
        "voteCount": 2,
        "content": "E doesn't allow for an online transfer - It's still offline. Doesn't make sense to me to add bandwidth if you're not going to use it."
      },
      {
        "date": "2024-09-27T18:33:00.000Z",
        "voteCount": 1,
        "content": "At 1 Gbps, transferring 100 TB of data could take much longer than 48 hours, so this is not a viable option."
      },
      {
        "date": "2024-04-23T02:04:00.000Z",
        "voteCount": 2,
        "content": "A and C"
      },
      {
        "date": "2024-04-23T02:04:00.000Z",
        "voteCount": 2,
        "content": "C. As per data transfer documentation, 1G should be sufficient to transfer 10TB in 30 hours (required is 48 hours) https://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets#online_versus_offline_transfer\nNo need to upgrade bandwidth."
      },
      {
        "date": "2024-03-19T06:02:00.000Z",
        "voteCount": 2,
        "content": "1 minutes = 60 seconds\n1 hour = 60 minutes = 60 * 60 = 3_600 seconds\n1 day = 24 hours = 24 * 3_600 = 86_400 seconds\n48 hours = 2 days = 86_400 * 2 = 172_800 seconds\n\nNetwork link 1 Gbps, 48 hours transfer 172_800 Gb = 172_800/8 GB = 21_600 GB &lt; 100 TB\n\nNetwork link 2 Gbps, 48 hours transfer 43_200 GB &lt; 100 TB\n\nNetwork link 10 Gbps, 48 hours transfer 210_600 GB = 205.66 TB &gt; 100 TB."
      },
      {
        "date": "2024-01-23T05:34:00.000Z",
        "voteCount": 2,
        "content": "I think the question tricks you into thinking that you need more bandwidth.  For offline, you don't need bandwidth.  So answers is A and C."
      },
      {
        "date": "2023-12-19T18:11:00.000Z",
        "voteCount": 2,
        "content": "Option A &amp; E is correct:\nThe questions and the option tell us two scenarios:\nFirst Scenario: Option A: CDC\nSetup CDC and replication. Within 48 hours this can be completed using 1 GB network bandwidth. Database is required restart to change configuration and the rest is online during replication  \nSecond scenario: offline migration (offline database migration)\nDatabase need to be shutdown and take a full backup and transfer it to GCP.\nAt this points  we can eliminate option C and D because option C transfer 100 TB over 1 GB network bandwidth required 12 days (above allowed downtime 48 hours) and option D transfer 100 TB over network bandwidth 4,74 days (113,78 hours) which is above allowed downtime 48 hours"
      },
      {
        "date": "2023-11-05T04:14:00.000Z",
        "voteCount": 5,
        "content": "A and C for me"
      },
      {
        "date": "2023-09-11T12:50:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is A &amp; D."
      },
      {
        "date": "2023-07-17T10:11:00.000Z",
        "voteCount": 5,
        "content": "A, C\nThe question doesn't tell you that you have to move the database within 48 hours from right now - it says you're allow 48 hours of downtime. i.e. You can schedule it. C, D, and E all require an offline transfer, so increasing bandwidth wouldn't help. 1 Gbps should be enough to handle CDC updates within 48 hours after the initial offline transfer is complete, and it minimizes cost."
      },
      {
        "date": "2023-03-01T16:09:00.000Z",
        "voteCount": 4,
        "content": "Ideally, you can transfer 1 GB in eight seconds over a 1 Gbps network. \nhttps://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets"
      },
      {
        "date": "2023-03-08T05:46:00.000Z",
        "voteCount": 1,
        "content": "Can A+E go together?"
      },
      {
        "date": "2022-12-25T13:09:00.000Z",
        "voteCount": 2,
        "content": "I confirm pk349 calculations"
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 88,
    "url": "https://www.examtopics.com/discussions/google/view/92714-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "You need to provision several hundred Cloud SQL for MySQL instances for multiple project teams over a one-week period. You must ensure that all instances adhere to company standards such as instance naming conventions, database flags, and tags. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAutomate instance creation by writing a Dataflow job.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAutomate instance creation by setting up Terraform scripts.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate the instances using the Google Cloud Console UI.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate clones from a template Cloud SQL instance."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-11-05T04:15:00.000Z",
        "voteCount": 1,
        "content": "B use terraform"
      },
      {
        "date": "2023-03-14T09:55:00.000Z",
        "voteCount": 2,
        "content": "B.\nThe scale of the problem suggests automation. Eliminate C and D. Dataflow concerns data, not instances. The only answer which makes sense is to use Terraform."
      },
      {
        "date": "2023-02-22T08:37:00.000Z",
        "voteCount": 2,
        "content": "B will work"
      },
      {
        "date": "2022-12-25T13:10:00.000Z",
        "voteCount": 3,
        "content": "Use Terraform, it will allow you to template same resource"
      },
      {
        "date": "2022-12-24T15:37:00.000Z",
        "voteCount": 1,
        "content": "B: Automate instance creation by setting up ***** Terraform scripts."
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 89,
    "url": "https://www.examtopics.com/discussions/google/view/92713-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "Your organization is migrating 50 TB Oracle databases to Bare Metal Solution for Oracle. Database backups must be available for quick restore. You also need to have backups available for 5 years. You need to design a cost-effective architecture that meets a recovery time objective (RTO) of 2 hours and recovery point objective (RPO) of 15 minutes. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1 Create the database on a Bare Metal Solution server with the database running on flash storage.<br>2. Keep a local backup copy on all flash storage.<br>3. Keep backups older than one day stored in Actifio OnVault storage.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1 Create the database on a Bare Metal Solution server with the database running on flash storage.<br>2. Keep a local backup copy on standard storage.<br>3. Keep backups older than one day stored in Actifio OnVault storage.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create the database on a Bare Metal Solution server with the database running on flash storage.<br>2. Keep a local backup copy on standard storage.<br>3. Use the Oracle Recovery Manager (RMAN) backup utility to move backups older than one day to a Coldline Storage bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create the database on a Bare Metal Solution server with the database running on flash storage.<br>2. Keep a local backup copy on all flash storage.<br>3. Use the Oracle Recovery Manager (RMAN) backup utility to move backups older than one day to an Archive Storage bucket.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 13,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 7,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-03-19T06:08:00.000Z",
        "voteCount": 2,
        "content": "Bare Metal --&gt; RMAN"
      },
      {
        "date": "2023-12-19T17:08:00.000Z",
        "voteCount": 1,
        "content": "Option B is correct\nThere are two activities: Manage incremental backup (transaction log) and manage obsolete backup (older than retention time to another backup storage (archival)). Option B make as actifio as tool to organize this two activities while Option D make RMAN as tools to organize this to activities. actifio have feature like \u201cIncremental-forever\u201d backups\" and store in  OnVault storage which is official standard for actifio and actifio in gcp which is very fast compare to option D (RMAN). RMAN will need more than 2 hours to do recovery for 50 TB."
      },
      {
        "date": "2023-11-13T19:58:00.000Z",
        "voteCount": 4,
        "content": "Keeping local storage on flash drive would meet the requirement for RTO. And moving older backups to Archival storage is the cost-effective option."
      },
      {
        "date": "2023-11-05T04:20:00.000Z",
        "voteCount": 4,
        "content": "https://cloud.google.com/backup-disaster-recovery/docs/concepts/backupdr-for-oracle#oracle_backup_apis"
      },
      {
        "date": "2023-09-11T15:03:00.000Z",
        "voteCount": 3,
        "content": "correct answer is  C. Option C aligns with the requirements for performance, cost-effectiveness, and long-term data retention while meeting the RTO and RPO objectives."
      },
      {
        "date": "2023-08-01T08:02:00.000Z",
        "voteCount": 1,
        "content": "C\n\nCreating the database on a Bare Metal Solution server with the database running on flash storage will ensure high performance and low latency, which is critical for achieving the RTO and RPO objectives.\n\nKeeping a local backup copy on standard storage will provide a quick restore option in case of any issues or failures within the Bare Metal Solution environment. This allows for faster recovery when needed.\n\nUsing the Oracle Recovery Manager (RMAN) backup utility to move backups older than one day to a Coldline Storage bucket is a cost-effective solution for long-term data retention. Coldline Storage is designed for data archival and offers a lower cost compared to other storage options. By using RMAN, the backup process can be automated and scheduled, ensuring that the backups are regularly moved to the Coldline Storage bucket."
      },
      {
        "date": "2023-06-14T02:27:00.000Z",
        "voteCount": 2,
        "content": "HDD (hard drive) for Local backups or archival workloads"
      },
      {
        "date": "2023-03-08T06:06:00.000Z",
        "voteCount": 3,
        "content": "GCP  wants us to choose B"
      },
      {
        "date": "2023-03-01T16:01:00.000Z",
        "voteCount": 1,
        "content": "RMAN cannot move backups to storage buckets.\nB"
      },
      {
        "date": "2023-03-08T06:04:00.000Z",
        "voteCount": 1,
        "content": "0\n\n\nYou can use gcsfuse to mount GCS bucket as file systems on your machine and use RMAN to create backups there."
      },
      {
        "date": "2023-03-08T06:05:00.000Z",
        "voteCount": 1,
        "content": "but I think you are right. and this is what GCP aiming for."
      },
      {
        "date": "2023-01-07T11:24:00.000Z",
        "voteCount": 3,
        "content": "D is an answer"
      },
      {
        "date": "2022-12-30T01:44:00.000Z",
        "voteCount": 4,
        "content": "D is an answer"
      },
      {
        "date": "2022-12-24T15:36:00.000Z",
        "voteCount": 4,
        "content": "D:\nCreate the database on a Bare Metal Solution server with the database running on flash storage.\nArchive storage\tARCHIVE\t365 days\tYes\t\u2022\t99.95% in multi-regions and dual-regions\n\u2022\t99.9% in regions\n\nKeep a local backup copy on all flash storage.\nUse the Oracle Recovery Manager (RMAN) backup utility to move backups older than one day to an ***** Archive Storage bucket."
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 90,
    "url": "https://www.examtopics.com/discussions/google/view/92712-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "You are a DBA on a Cloud Spanner instance with multiple databases. You need to assign these privileges to all members of the application development team on a specific database:<br><br>Can read tables, views, and DDL -<br><br>Can write rows to the tables -<br><br>Can add columns and indexes -<br><br>Cannot drop the database -<br>What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAssign the Cloud Spanner Database Reader and Cloud Spanner Backup Writer roles.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAssign the Cloud Spanner Database Admin role.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAssign the Cloud Spanner Database User role.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAssign the Cloud Spanner Admin role."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 6,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-12-19T16:22:00.000Z",
        "voteCount": 1,
        "content": "Option C is corect\nspanner.admin\tGod-tier control over everything\tFull CRUD &amp; DDL for instances, databases, users, policies\tManage entire Cloud Spanner environment\n\nspanner.databaseAdmin\tDatabase master control\tFull CRUD &amp; DDL for databases, users, policies\tCreate, manage, and secure databases\n\nspanner.databaseUser\tPower user with data rights\tFull CRUD &amp; DDL for specified databases\tRead, write, modify data, manage database schema\n\nspanner.databaseReader\tCurious but hands-off observer\tRead-only access and query execution\tAnalyze data without manipulation\n\nspanner.viewer\tPeeping Tom (metaphorically speaking)\tView instances, databases, and users\tMonitor Cloud Spanner metadata, no data access\n\nspanner.backupAdmin\tBackup whiz\tCreate, view, update, delete backups\tManage backups without accessing data"
      },
      {
        "date": "2023-09-11T15:18:00.000Z",
        "voteCount": 1,
        "content": "The answer list is not correct. we need to have a custom role for this task. BTW, Option C (assigning the Cloud Spanner Database User role) does not provide the necessary permissions to add columns and indexes or perform DDL operations. that said, it not correct either :)"
      },
      {
        "date": "2023-03-14T10:02:00.000Z",
        "voteCount": 3,
        "content": "C.\nThis has nothing to do with backups. Eliminate A. Any admin role would be too wide. Eliminate B and D. That leaves C. roles/spanner.databaseUser provides r/w. view and update schemas.\nhttps://cloud.google.com/spanner/docs/iam#spanner.databaseUser"
      },
      {
        "date": "2023-02-11T10:27:00.000Z",
        "voteCount": 1,
        "content": "C\nA: WRONG. Neither roles can add columns and indexes (missing spanner.databases.updateDdl permission)\nB: WRONG. Exceeded permissions needed.\nC: CORRECT.\nD: WRONG. Exceeded permissions needed.\nReference:\nhttps://gcp.permissions.cloud/predefinedroles/spanner.admin\nhttps://gcp.permissions.cloud/predefinedroles/spanner.databaseReader\nhttps://gcp.permissions.cloud/predefinedroles/spanner.backupWriter\nhttps://gcp.permissions.cloud/predefinedroles/spanner.databaseAdmin\nhttps://gcp.permissions.cloud/predefinedroles/spanner.databaseUser"
      },
      {
        "date": "2023-01-25T02:30:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/spanner/docs/iam"
      },
      {
        "date": "2022-12-25T13:15:00.000Z",
        "voteCount": 4,
        "content": "Vote for C"
      },
      {
        "date": "2022-12-24T15:34:00.000Z",
        "voteCount": 1,
        "content": "C: Assign the Cloud Spanner Database User *** role."
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 91,
    "url": "https://www.examtopics.com/discussions/google/view/92711-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "Your project is using Bigtable to store data that should not be accessed from the public internet under any circumstances, even if the requestor has a valid service account key. You need to secure access to this data. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Identity and Access Management (IAM) for Bigtable access control.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse VPC Service Controls to create a trusted network for the Bigtable service.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse customer-managed encryption keys (CMEK).",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Google Cloud Armor to add IP addresses to an allowlist."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-03-14T14:33:00.000Z",
        "voteCount": 3,
        "content": "B.\nA is wrong because you might have the right credentials but still access Bigtable across the internet. Same is true for C. Cloud Armor could help, but VPC Service Controls is a classic use case of ensuring access is only from within certain VPC networks. From Google\u2019s documentation, \u201cUsers can define a security perimeter around Google Cloud resources such as Cloud Storage buckets, Bigtable instances, and BigQuery datasets to constrain data within a VPC and control the flow of data.\u201d\nhttps://cloud.google.com/vpc-service-controls"
      },
      {
        "date": "2022-12-25T13:16:00.000Z",
        "voteCount": 4,
        "content": "I'll go for B"
      },
      {
        "date": "2022-12-24T15:34:00.000Z",
        "voteCount": 1,
        "content": "B: Use VPC Service Controls to create a trusted network for the Bigtable service."
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 92,
    "url": "https://www.examtopics.com/discussions/google/view/92710-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "Your organization has a ticketing system that needs an online marketing analytics and reporting application. You need to select a relational database that can manage hundreds of terabytes of data to support this new application. Which database should you use?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud SQL",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBigQuery\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Spanner",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBigtable"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 11,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 9,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-27T16:58:00.000Z",
        "voteCount": 12,
        "content": "bigquery - Analytics and reporting"
      },
      {
        "date": "2023-10-04T18:21:00.000Z",
        "voteCount": 5,
        "content": "This question is poorly worded. I interpret it as: you have a ticketing system that needs an analytics and reporting app. Which database for this analytics and reporting app (not the ticketing system)? It should be BQ. However, it could also be read as what database for the ticketing system which also happens to need analytics and reporting. Which means the answer should be Spanner. Very confusing, but I'm leaning towards B for BQ based on my first interpretation."
      },
      {
        "date": "2024-05-17T01:57:00.000Z",
        "voteCount": 1,
        "content": "Indeed!"
      },
      {
        "date": "2024-03-19T06:30:00.000Z",
        "voteCount": 1,
        "content": "Marketing analytics https://cloud.google.com/bigquery?hl=en#marketing-analytics"
      },
      {
        "date": "2024-02-23T01:57:00.000Z",
        "voteCount": 1,
        "content": "C because OLTP supported in spanner not in BQ"
      },
      {
        "date": "2024-01-31T05:10:00.000Z",
        "voteCount": 1,
        "content": "the question ask about the database for the new app which is analytic and reporting so this should BQ."
      },
      {
        "date": "2023-09-26T17:09:00.000Z",
        "voteCount": 1,
        "content": "option B - managing hundreds of terabytes of data and supporting an online marketing analytics and reporting application, BigQuery (Option B) is the most suitable choice"
      },
      {
        "date": "2023-09-21T04:25:00.000Z",
        "voteCount": 3,
        "content": "The answer is C \nBigquery is not a Database  \" You need to select a relational database \"  Bigquery is a Datawarehouse"
      },
      {
        "date": "2023-07-25T23:24:00.000Z",
        "voteCount": 2,
        "content": "It might be both B and C. B- for analytic purpose but I would be inclining to C as we need OLTP database for ticketing systems."
      },
      {
        "date": "2023-07-22T01:19:00.000Z",
        "voteCount": 2,
        "content": "Bigquery is relational and recomanded for analytics: https://cloud.google.com/products/databases"
      },
      {
        "date": "2023-07-17T10:27:00.000Z",
        "voteCount": 2,
        "content": "This is an analytics problem suited for BigQuery. Cloud Spanner isn't a good solution for analytics queries that scan a full table to do aggregations of the data because the query would span multiple (or all) shards)."
      },
      {
        "date": "2023-03-14T14:35:00.000Z",
        "voteCount": 3,
        "content": "C.\nCloud SQL could not scale to 100s of TBs. Eliminate A. Neither Big Query nor Bigtable are relational (although BigQuery does support SQL). Eliminate B and D. That leaves C."
      },
      {
        "date": "2023-07-21T07:03:00.000Z",
        "voteCount": 3,
        "content": "Bigquery is relational: https://cloud.google.com/products/databases\nMoreover, BQ is designed for \"Multicloud analytics\""
      },
      {
        "date": "2023-01-28T02:54:00.000Z",
        "voteCount": 1,
        "content": "Cloud SQL has a double-digit TB storage limit so it's Cloud Spanner."
      },
      {
        "date": "2023-01-26T06:26:00.000Z",
        "voteCount": 1,
        "content": "Big query is data warehouse not a relational database"
      },
      {
        "date": "2022-12-30T04:23:00.000Z",
        "voteCount": 3,
        "content": "Analytical Database with no transactions my vote B"
      },
      {
        "date": "2022-12-28T06:16:00.000Z",
        "voteCount": 4,
        "content": "B - This is a new application used for analytics, it does not need to take care of the DML required by the ticketing system"
      },
      {
        "date": "2022-12-25T13:17:00.000Z",
        "voteCount": 3,
        "content": "Spanner"
      },
      {
        "date": "2022-12-24T15:33:00.000Z",
        "voteCount": 3,
        "content": "C: Cloud Spanner\nOn the other hand, Google Cloud Spanner supports ***** OLTP along with scalability and high availability. Hence, Cloud Spanner is more suited for E-commerce systems, Core Banking, Gaming, Telecom, etc.\nIt allows updating to existing records and appending data to existing tables."
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 93,
    "url": "https://www.examtopics.com/discussions/google/view/92709-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "You are designing for a write-heavy application. During testing, you discover that the write workloads are performant in a regional Cloud Spanner instance but slow down by an order of magnitude in a multi-regional instance. You want to make the write workloads faster in a multi-regional instance. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPlace the bulk of the read and write workloads closer to the default leader region.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse staleness of at least 15 seconds.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd more read-write replicas.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tKeep the total CPU utilization under 45% in each region."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 12,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-31T13:31:00.000Z",
        "voteCount": 10,
        "content": "https://cloud.google.com/spanner/docs/instance-configurations#multi-region-best-practices\nBest practices\nFor optimal performance, follow these best practices:\n\nDesign a schema that prevents hotspots and other performance issues.\nFor optimal write latency, place compute resources for write-heavy workloads within or close to the default leader region.\nFor optimal read performance outside of the default leader region, use staleness of at least 15 seconds.\nTo avoid single-region dependency for your workloads, place critical compute resources in at least two regions. A good option is to place them next to the two different read-write regions so that any single region outage will not impact all of your application.\nProvision enough compute capacity to keep high priority total CPU utilization under 45% in each region."
      },
      {
        "date": "2023-11-05T04:25:00.000Z",
        "voteCount": 2,
        "content": "For optimal performance, follow these best practices:\n\nDesign a schema that prevents hotspots and other performance issues.\nFor optimal write latency, place compute resources for write-heavy workloads within or close to the default leader region.\nFor optimal read performance outside of the default leader region, use staleness of at least 15 seconds.\nTo avoid single-region dependency for your workloads, place critical compute resources in at least two regions. A good option is to place them next to the two different read-write regions so that any single region outage will not impact all of your application.\nProvision enough compute capacity to keep high priority total CPU utilization under 45% in each region.\nFor the amount of throughput per Spanner node, see performance for multi-region configurations."
      },
      {
        "date": "2023-03-14T14:54:00.000Z",
        "voteCount": 2,
        "content": "A.\nFor optimal read (not write) performance outside of the default leader region, use staleness of at least 15 seconds. Eliminate B. CPU capacity is not mentioned as an issue. Eliminate D. When you create a Spanner instance, u can choose to add additional read-only replicas. Once the instance is created, you can change the instance name and add processing units. You can\u2019t add more read-write replicas. Eliminate C. That leaves A. The link provided by sp57 is spot on."
      },
      {
        "date": "2022-12-28T06:29:00.000Z",
        "voteCount": 2,
        "content": "A. For optimal write latency, place compute resources for write-heavy workloads within or close to the default leader region\nB is applicable to reads\nC will add load/latency\nD is only applicable to \"high priority\" tasks"
      },
      {
        "date": "2022-12-26T01:53:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/spanner/docs/instance-configurations#multi-region-best-practices"
      },
      {
        "date": "2022-12-24T15:31:00.000Z",
        "voteCount": 3,
        "content": "D: Keep the total CPU utilization under 45% in each region.\nFor optimal performance, follow these best practices:\n\u2022\tDesign a schema that prevents hotspots and other performance issues.\n\u2022\tPlace critical compute resources within the same region as your Spanner instance.\n\u2022\tProvision enough compute capacity to keep high priority total CPU utilization ***** under 65%."
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 94,
    "url": "https://www.examtopics.com/discussions/google/view/92708-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "Your company wants to migrate an Oracle-based application to Google Cloud. The application team currently uses Oracle Recovery Manager (RMAN) to back up the database to tape for long-term retention (LTR). You need a cost-effective backup and restore solution that meets a 2-hour recovery time objective (RTO) and a 15-minute recovery point objective (RPO). What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the Oracle databases to Bare Metal Solution for Oracle, and store backups on tapes on-premises.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the Oracle databases to Bare Metal Solution for Oracle, and use Actifio to store backup files on Cloud Storage using the Nearline Storage class.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the Oracle databases to Bare Metal Solution for Oracle, and back up the Oracle databases to Cloud Storage using the Standard Storage class.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the Oracle databases to Compute Engine, and store backups on tapes on-premises."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 3,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-08-16T02:21:00.000Z",
        "voteCount": 4,
        "content": "Currently, instead of Actifio, the correct name is: Google Cloud Backup and DR for Oracle on Bare Metal Solution"
      },
      {
        "date": "2023-03-14T15:00:00.000Z",
        "voteCount": 4,
        "content": "B.\nA doesn\u2019t make sense. Oracle is neither licensed nor supported in GCE. Eliminate D. Standard storage is more expensive that Nearline storage. Eliminate C. That leaves B as the most cost effective solution."
      },
      {
        "date": "2022-12-30T02:19:00.000Z",
        "voteCount": 2,
        "content": "B\nActifio provides a single platform for backup, recovery, cloning and analytics for Cloud and on-premises VMs &amp; Databases, delivering low RTO, RPO, and costs. Enterprises use Actifio to backup data from data centers and Public cloud platforms onto Google Cloud."
      },
      {
        "date": "2022-12-30T02:20:00.000Z",
        "voteCount": 1,
        "content": "https://www.actifio.com/solutions/cloud/google/"
      },
      {
        "date": "2022-12-26T01:58:00.000Z",
        "voteCount": 3,
        "content": "Vote for B"
      },
      {
        "date": "2022-12-24T15:30:00.000Z",
        "voteCount": 2,
        "content": "B: Migrate the Oracle databases to Bare Metal Solution for Oracle, and use Actifio to store backup files on Cloud Storage using the Nearline Storage class.\n BMigrate the Oracle databases to Bare Metal Solution for Oracle, and use Actifio to store backup files on Cloud Storage using the Nearline *** Storage class."
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 95,
    "url": "https://www.examtopics.com/discussions/google/view/92706-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "You are migrating a telehealth care company's on-premises data center to Google Cloud. The migration plan specifies:<br>PostgreSQL databases must be migrated to a multi-region backup configuration with cross-region replicas to allow restore and failover in multiple scenarios.<br>MySQL databases handle personally identifiable information (PII) and require data residency compliance at the regional level.<br>You want to set up the environment with minimal administrative effort. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up Cloud Logging and Cloud Monitoring with Cloud Functions to send an alert every time a new database instance is created, and manually validate the region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up different organizations for each database type, and apply policy constraints at the organization level.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up Pub/Sub to ingest data from Cloud Logging, send an alert every time a new database instance is created, and manually validate the region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up different projects for PostgreSQL and MySQL databases, and apply organizational policy constraints at a project level.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-12-26T02:03:00.000Z",
        "voteCount": 5,
        "content": "A and C don't prevent anything, and between B and D I choose D because there is no sense to maintain a separate org just because of the compliance - a separate project is more than enough."
      },
      {
        "date": "2023-03-14T15:05:00.000Z",
        "voteCount": 4,
        "content": "D.\nA doesn\u2019t make sense. B is complete overkill. C does something manually and is convoluted. That leaves D. Managing policy at the project level is cleaner than having separate Organizations."
      },
      {
        "date": "2022-12-27T18:13:00.000Z",
        "voteCount": 1,
        "content": "one Org with multiple folders and projects under folders"
      },
      {
        "date": "2022-12-24T15:26:00.000Z",
        "voteCount": 1,
        "content": "B: Set up different organizations for each *** database type, and apply policy constraints at the organization level."
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 96,
    "url": "https://www.examtopics.com/discussions/google/view/92705-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "You have a Cloud SQL instance (DB-1) with two cross-region read replicas (DB-2 and DB-3). During a business continuity test, the primary instance (DB-1) was taken offline and a replica (DB-2) was promoted. The test has concluded and you want to return to the pre-test configuration. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBring DB-1 back online.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDelete DB-1, and re-create DB-1 as a read replica in the same region as DB-1.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDelete DB-2 so that DB-1 automatically reverts to the primary instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate DB-4 as a read replica in the same region as DB-1, and promote DB-4 to primary.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 16,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 6,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 5,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-04-30T03:46:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/sql/docs/mysql/high-availability#failback"
      },
      {
        "date": "2023-12-19T15:24:00.000Z",
        "voteCount": 1,
        "content": "Option D is correct.\nOption A is false because\nAfter a failover, the instance that received the failover continues to be the primary instance, even after the original instance comes back online. After the zone or instance that experienced an outage becomes available again, the original primary instance is destroyed and recreated. Then it becomes the new standby instance. If a failover occurs in the future, the new primary will fail over to the original instance in the original zone.\nReference:\nhttps://cloud.google.com/sql/docs/mysql/high-availability#failback"
      },
      {
        "date": "2023-12-19T15:18:00.000Z",
        "voteCount": 1,
        "content": "Option D is correct. \nOption A is false because\nAfter a failover, the instance that received the failover continues to be the primary instance, even after the original instance comes back online. After the zone or instance that experienced an outage becomes available again, the original primary instance is destroyed and recreated. Then it becomes the new standby instance. If a failover occurs in the future, the new primary will fail over to the original instance in the original zone. \nReference:\nhttps://cloud.google.com/sql/docs/mysql/high-availability#failback"
      },
      {
        "date": "2023-11-26T09:10:00.000Z",
        "voteCount": 4,
        "content": "The Option D describes the correct answer (even though it is only half part as it doesn't prevent split brain situations but It is better answer than B, A or C), The issue with answer B is It doesn't return to pretest configuration. Same Goes for A as DB-2 still the primary in this case. Option C is not recommended in case region is not available and still you end up having only Primary (DB-1) available and Not Replica (DB-2) we want pre test config which is both DB-1 and DB-2 available. PLEASE UPVOTE SO OTHERS CAN BENEFIT."
      },
      {
        "date": "2023-11-26T07:08:00.000Z",
        "voteCount": 1,
        "content": "The Option D describes the correct answer (even though it is only half part as it doesn't prevent split brain situations but It is better answer than B, A or C), The issue with answer B is It doesn't return to pretest configuration. Same Goes for A as DB-2 still the primary in this case. Option C is not recommended in case region is not available and still you end up having only Primary (DB-1) available and Not Replica (DB-2) we want pre test config which is both DB-1 and DB-2 available. PLEASE UPVOTE SO OTHERS CAN BENEFIT."
      },
      {
        "date": "2023-11-25T02:46:00.000Z",
        "voteCount": 2,
        "content": "According to this: https://cloud.google.com/solutions/cloud-sql-mysql-disaster-recovery-complete-failover-fallback#phase_1_setting_up_an_ha_database_instance_for_dr\n\nOption D is the correct one. Once you promote a read replica it destroys the old replica flow and you need to create a new instance to promote as primary."
      },
      {
        "date": "2023-11-16T14:35:00.000Z",
        "voteCount": 4,
        "content": "D is correct. Read the other comments"
      },
      {
        "date": "2023-11-04T20:26:00.000Z",
        "voteCount": 3,
        "content": "D is correct."
      },
      {
        "date": "2023-10-19T06:01:00.000Z",
        "voteCount": 3,
        "content": "D.\nIf you need to have the primary instance in the zone that had the outage, you can do a failback. A failback performs the same steps as the failover, only in the opposite direction, to reroute traffic back to the original instance. To perform a failback, use the procedure in Initiating failover.\nhttps://cloud.google.com/sql/docs/mysql/high-availability#failback"
      },
      {
        "date": "2023-09-26T17:20:00.000Z",
        "voteCount": 1,
        "content": "option A- Bring DB-1 back online."
      },
      {
        "date": "2023-09-20T04:47:00.000Z",
        "voteCount": 3,
        "content": "As stated in the documentation (https://cloud.google.com/sql/docs/mysql/replication/cross-region-replicas) the cross region read replica promotion is a manual activity and it destroys all the previous replica flow. The pre-test configuration is having a primary instance on region 1 with 2 read replicas on region 2 and 3; the only way to do that is to create a replica of DB2 in region 1 (DB4), promote it as a primary instance and then create DB4 read replicas on region 2 and 3. \n- Answer D is the only one in that direction\n- Answer A is wrong because the replica flow between DB1 and DB2 has been destroyed with the promotion\n- Answer B is partially correct but not enough\n- Answer C is wrong because we are talking about read replica, not high availability, and the replica flow between DB1 and DB2 has been destroyed by the promotion"
      },
      {
        "date": "2023-09-13T21:37:00.000Z",
        "voteCount": 1,
        "content": "A. Bring back original primary.\nTabs at link below shows process for failover and then failback\nhttps://cloud.google.com/sql/docs/mysql/high-availability#failback"
      },
      {
        "date": "2023-09-11T16:09:00.000Z",
        "voteCount": 1,
        "content": "correct answer is A. Bring DB-1 back online. :)"
      },
      {
        "date": "2023-08-16T02:36:00.000Z",
        "voteCount": 1,
        "content": "A\n\nyou just need to bring it back again as you can see in the Google documentation: https://cloud.google.com/sql/docs/mysql/high-availability#failback"
      },
      {
        "date": "2023-06-30T01:42:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/sql/docs/mysql/replication/cross-region-replicas\nIf the primary instance (db-1) becomes unavailable, you can promote the replica in region B to become the primary. To again have additional replicas in regions A and C, delete the old instances (the former primary instance in A, and the replica in C), and create new read replicas from the new primary instance in B."
      },
      {
        "date": "2023-06-29T06:27:00.000Z",
        "voteCount": 1,
        "content": "A  is the correct choice because it allows you to restore the original primary instance. By bringing DB-1 back online, it will automatically synchronize with the promoted replica (DB-2) and resume its role as the primary instance. This ensures that the system returns to the pre-test configuration."
      },
      {
        "date": "2023-06-24T09:14:00.000Z",
        "voteCount": 3,
        "content": "The answer is A.\nTo return to the pre-test configuration, you need to bring DB-1 back online. This will automatically demote DB-2 and make DB-1 the primary instance again.\n\nOption B is incorrect because it would re-create DB-1 as a read replica, which is not the pre-test configuration.\n\nOption C is incorrect because deleting DB-2 would not automatically revert DB-1 to the primary instance. You would need to manually promote DB-1 to primary.\n\nOption D is incorrect because creating DB-4 as a read replica and promoting it to primary would create a new primary instance, but it would not revert DB-1 to its pre-test configuration."
      },
      {
        "date": "2023-06-24T09:16:00.000Z",
        "voteCount": 1,
        "content": "Bringing DB-1 back online: This option involves reactivating the original primary instance (DB-1). By bringing it back online, it will resume its role as the primary instance, and any changes made to the promoted replica (DB-2) during the test will be discarded. This action restores the pre-test configuration."
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 97,
    "url": "https://www.examtopics.com/discussions/google/view/92704-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "Your team is building a new inventory management application that will require read and write database instances in multiple Google Cloud regions around the globe. Your database solution requires 99.99% availability and global transactional consistency. You need a fully managed backend relational database to store inventory changes. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Bigtable.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Firestore.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud SQL for MySQL",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Spanner.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-12-26T02:34:00.000Z",
        "voteCount": 5,
        "content": "Spanner covers the SLA (99,999), it's global and transactional"
      },
      {
        "date": "2024-03-10T17:45:00.000Z",
        "voteCount": 1,
        "content": "Spanner Use Case: Order &amp; Inventory Management"
      },
      {
        "date": "2024-03-10T17:44:00.000Z",
        "voteCount": 1,
        "content": "D. Spanner Usa Case: Order &amp; Inventory Management"
      },
      {
        "date": "2023-03-14T15:23:00.000Z",
        "voteCount": 3,
        "content": "D.\nRelational rules out A and B. Global rules out C. That leaves D."
      },
      {
        "date": "2022-12-24T15:24:00.000Z",
        "voteCount": 2,
        "content": "D: Use Cloud  Spanner. \tSLA 99.999"
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 98,
    "url": "https://www.examtopics.com/discussions/google/view/92702-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "You are the database administrator of a Cloud SQL for PostgreSQL instance that has pgaudit disabled. Users are complaining that their queries are taking longer to execute and performance has degraded over the past few months. You need to collect and analyze query performance data to help identity slow-running queries. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tView Cloud SQL operations to view historical query information.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWhite a Logs Explorer query to identify database queries with high execution times.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReview application logs to identify database calls.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Query Insights dashboard to identify high execution times.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-12-24T15:22:00.000Z",
        "voteCount": 10,
        "content": "D: Use the Query Insights *** dashboard to identify high execution times."
      },
      {
        "date": "2024-05-23T08:50:00.000Z",
        "voteCount": 1,
        "content": "Query Insights"
      },
      {
        "date": "2024-05-02T02:02:00.000Z",
        "voteCount": 1,
        "content": "D: Use the Query Insights"
      },
      {
        "date": "2023-03-08T09:03:00.000Z",
        "voteCount": 3,
        "content": "D. Use the Query Insights dashboard to identify high execution times."
      },
      {
        "date": "2022-12-24T15:20:00.000Z",
        "voteCount": 1,
        "content": "A Cloud SQL instance configured for HA is also called a regional instance and has a primary and secondary zone within the configured region. Within a regional instance, the configuration is made up of a primary instance and a standby instance. Through synchronous replication to each zone's persistent disk, all writes made to the primary instance are replicated to disks in both zones before a transaction is reported as committed. In the event of an instance or zone failure, the standby instance becomes the new primary instance. Users are then rerouted to the new primary instance. This process is called a failover."
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 99,
    "url": "https://www.examtopics.com/discussions/google/view/92701-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "You are configuring a brand new PostgreSQL database instance in Cloud SQL. Your application team wants to have an optimal and highly available environment with automatic failover to avoid any unplanned outage. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate one regional Cloud SQL instance with a read replica in another region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate one regional Cloud SQL instance in one zone with a standby instance in another zone in the same region.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate two read-write Cloud SQL instances in two different zones with a standby instance in another region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate two read-write Cloud SQL instances in two different regions with a standby instance in another zone."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 7,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-26T02:37:00.000Z",
        "voteCount": 7,
        "content": "In HA configuration standby instance can't be in another region"
      },
      {
        "date": "2023-08-16T05:51:00.000Z",
        "voteCount": 1,
        "content": "A\n\nFrom: https://cloud.google.com/sql/docs/mysql/high-availability\n\nThe HA configuration provides data redundancy. A Cloud SQL instance configured for HA is also called a regional instance and has a primary and secondary zone within the configured region.\n\nWhen we say: regional Cloud SQL that already means that the instance has the configuration made up of a primary instance and a standby instance.\n\nIf we want to have the maximum availability If a replica is a cross-region replica, you can perform a failover to another region; specifically, you can promote a replica to a standalone instance"
      },
      {
        "date": "2023-03-15T11:13:00.000Z",
        "voteCount": 3,
        "content": "B.\nA is not HA. B would be HA. C doesn\u2019t make sense and neither does D. Cloud SQL is a regional service which means the failover replica is in the same region as the primary - hopefully in a different zone. Read replicas can be in different regions, but they\u2019re read replicas not failover replicas."
      },
      {
        "date": "2022-12-24T15:20:00.000Z",
        "voteCount": 1,
        "content": "B: Create one regional Cloud SQL instance in one zone with a standby ***** instance in another zone in the same region."
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 100,
    "url": "https://www.examtopics.com/discussions/google/view/92700-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "During an internal audit, you realized that one of your Cloud SQL for MySQL instances does not have high availability (HA) enabled. You want to follow Google-recommended practices to enable HA on your existing instance. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new Cloud SQL for MySQL instance, enable HA, and use the export and import option to migrate your data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new Cloud SQL for MySQL instance, enable HA, and use Cloud Data Fusion to migrate your data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the gcloud instances patch command to update your existing Cloud SQL for MySQL instance.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tShut down your existing Cloud SQL for MySQL instance, and enable HA."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-12-26T02:51:00.000Z",
        "voteCount": 8,
        "content": "A and B are obviously wrong, but it's still a bit confusing with the last two:\nC - `gcloud instances patch` is not complete, it should be `gcloud sql instances patch`, so I would consider it to be wrong, if not the D - you can't edit your instance (i.e. enable HA) when it's shut down, you can only do it when instance is running, and then it will restart.\nSo I'll go for C"
      },
      {
        "date": "2022-12-27T05:28:00.000Z",
        "voteCount": 14,
        "content": "So I took exam, and command was written correctly in the exam question: `gcloud sql instances patch`, so C is indeed correct here"
      },
      {
        "date": "2023-07-22T12:10:00.000Z",
        "voteCount": 1,
        "content": "Congratulations \ud83c\udf8a"
      },
      {
        "date": "2024-05-23T08:56:00.000Z",
        "voteCount": 1,
        "content": "Agree with C"
      },
      {
        "date": "2023-11-25T02:54:00.000Z",
        "voteCount": 1,
        "content": "C is correct: https://cloud.google.com/sql/docs/mysql/configure-ha#ha-existing"
      },
      {
        "date": "2023-03-15T11:26:00.000Z",
        "voteCount": 3,
        "content": "C.\nYou can add a failover replica to an already created and running Cloud SQL instance. This can be done from the console or by using gcloud. The gcloud command is:\n\ngcloud sql instances patch INSTANCE_NAME \\\n--availability-type REGIONAL \\\n--enable-bin-log \\\n--backup-start-time=HH:MM\n\nNo need to create a new Cloud SQL instance. Eliminate A and B. Adding HA requires you to edit the configuration which you can\u2019t do if the instance is down. So D is wrong."
      },
      {
        "date": "2023-02-28T00:24:00.000Z",
        "voteCount": 1,
        "content": "The correct option is C. Use the gcloud instances patch command to update your existing Cloud SQL for MySQL instance.\n\nExplanation:\n\nCreating a new instance and migrating data can be time-consuming and disruptive to your application's availability. Shutting down the existing instance is not a recommended approach, as it will cause downtime for your application.\n\nThe recommended approach is to use the gcloud instances patch command to enable high availability on your existing Cloud SQL for MySQL instance. This command updates the instance's configuration to enable the failover replica, configure it, and enable automatic failover.\n\nBy following this approach, you can ensure minimal downtime, and your application can continue to operate during the process."
      },
      {
        "date": "2022-12-24T14:58:00.000Z",
        "voteCount": 1,
        "content": "D: Shut down your existing Cloud SQL for MySQL instance, and  enable HA."
      },
      {
        "date": "2023-01-05T02:37:00.000Z",
        "voteCount": 2,
        "content": "you can enbale HA without shutdown https://cloud.google.com/sql/docs/postgres/configure-ha"
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 101,
    "url": "https://www.examtopics.com/discussions/google/view/92698-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "You are managing a set of Cloud SQL databases in Google Cloud. Regulations require that database backups reside in the region where the database is created. You want to minimize operational costs and administrative effort. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the automated backups to use a regional Cloud Storage bucket as a custom location.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the default configuration for the automated backups location.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDisable automated backups, and create an on-demand backup routine to a regional Cloud Storage bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDisable automated backups, and configure serverless exports to a regional Cloud Storage bucket."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-26T02:55:00.000Z",
        "voteCount": 8,
        "content": "Let's eliminate:\nB - defaul is multi-regional even if your instance is regional\nC and D - you don't need to disable automated backups, this will increase administrative efforts\nAll you need to do is to edit your backups and choose to store them in the region that you want instead of multi-regional. A is the answer."
      },
      {
        "date": "2024-04-30T04:46:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/sql/docs/mysql/backup-recovery/backing-up#locationbackups"
      },
      {
        "date": "2023-09-23T04:42:00.000Z",
        "voteCount": 1,
        "content": "Cloud SQL lets you select a custom location for your backup data. This is useful if your organization needs to comply with data residency regulations that require you to keep your backups within a specific geographic boundary. If your organization has this type of requirement, it probably uses a Resource Location Restriction organizational policy."
      },
      {
        "date": "2023-08-16T06:02:00.000Z",
        "voteCount": 2,
        "content": "A\n\nhttps://cloud.google.com/sql/docs/mysql/backup-recovery/backing-up#locationbackups\n\nYou can use a custom location for on-demand and automatic backups. For a complete list of valid location values, see the Instance locations."
      },
      {
        "date": "2023-04-25T04:31:00.000Z",
        "voteCount": 2,
        "content": "A\n\"\"\"\nWhere backups are stored\nBackups locations include:\n\nDefault locations that Cloud SQL selects, based on the location of the original instance.\nCustom locations that you choose when you do not want to use the default location.\n\"\"\"\nhttps://cloud.google.com/sql/docs/mysql/backup-recovery/backups\n\n\"\"\"\nSet a custom location for backups\nOnly use a custom backup location if required by regulation. If not required, use the default multi-region backup location.\n\nYou can use a custom location for on-demand and automatic backups.\n\"\"\"\nhttps://cloud.google.com/sql/docs/mysql/backup-recovery/backing-up#locationbackups"
      },
      {
        "date": "2023-03-15T11:41:00.000Z",
        "voteCount": 2,
        "content": "C.\nYou cannot configure a custom location for automatic backups. A is wrong. The default option for automatic backups is multi-region. B is wrong. The question specifically mentions backups not exports, so eliminate D. That leaves C which involves manual effort, but it does keep the data in the correct region."
      },
      {
        "date": "2023-03-05T18:09:00.000Z",
        "voteCount": 1,
        "content": "I just tested it mates it is for sure c\nyou can't do automatic backup to cloud storage"
      },
      {
        "date": "2023-03-05T18:08:00.000Z",
        "voteCount": 1,
        "content": "C. Disable automated backups, and create an on-demand backup routine to a regional Cloud Storage bucket."
      },
      {
        "date": "2023-01-10T03:03:00.000Z",
        "voteCount": 4,
        "content": "A - https://cloud.google.com/sql/docs/mysql/backup-recovery/backups#default-backup-location\n\"If you do not specify a storage location, your backups are stored in the multiregion that is geographically closest to the location of your Cloud SQL instance.\""
      },
      {
        "date": "2022-12-24T14:50:00.000Z",
        "voteCount": 1,
        "content": "C: Disable automated backups, and create an on-demand *** backup routine to a regional Cloud Storage bucket."
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 102,
    "url": "https://www.examtopics.com/discussions/google/view/92697-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "Your ecommerce application connecting to your Cloud SQL for SQL Server is expected to have additional traffic due to the holiday weekend. You want to follow Google-recommended practices to set up alerts for CPU and memory metrics so you can be notified by text message at the first sign of potential issues. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a Cloud Function to pull CPU and memory metrics from your Cloud SQL instance and to call a custom service to send alerts.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Error Reporting to monitor CPU and memory metrics and to configure SMS notification channels.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Logging to set up a log sink for CPU and memory metrics and to configure a sink destination to send a message to Pub/Sub.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Monitoring to set up an alerting policy for CPU and memory metrics and to configure SMS notification channels.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-03-15T11:40:00.000Z",
        "voteCount": 5,
        "content": "D.\nNo custom service needed. Eliminate A. There are no errors to report, only usage metrics. Eliminate B. Sending a message to Pub/Sub doesn\u2019t notify you by text message. Eliminate C. That leaves D."
      },
      {
        "date": "2022-12-26T02:42:00.000Z",
        "voteCount": 5,
        "content": "D is correct"
      },
      {
        "date": "2023-09-01T09:09:00.000Z",
        "voteCount": 1,
        "content": "This one only talks about SMS"
      },
      {
        "date": "2022-12-24T14:49:00.000Z",
        "voteCount": 1,
        "content": "Cloud Monitoring collects metrics, events, and metadata from Google Cloud, Amazon Web Services (AWS), hosted uptime probes, and application instrumentation. Using the BindPlane service, you can also collect this data from over 150 common application components, on-premise systems, and hybrid cloud systems."
      },
      {
        "date": "2022-12-24T14:48:00.000Z",
        "voteCount": 1,
        "content": "D: Use Cloud Monitoring to set up an alerting policy for CPU and memory metrics and to configure SMS notification channels."
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 103,
    "url": "https://www.examtopics.com/discussions/google/view/92696-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "You finished migrating an on-premises MySQL database to Cloud SQL. You want to ensure that the daily export of a table, which was previously a cron job running on the database server, continues. You want the solution to minimize cost and operations overhead. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Scheduler and Cloud Functions to run the daily export.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a streaming Datatlow job to export the table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up Cloud Composer, and create a task to export the table daily.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun the cron job on a Compute Engine instance to continue the export."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 10,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-26T02:43:00.000Z",
        "voteCount": 9,
        "content": "A is Google recommended couple"
      },
      {
        "date": "2023-09-01T09:16:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/blog/topics/developers-practitioners/scheduling-cloud-sql-exports-using-cloud-functions-and-cloud-scheduler"
      },
      {
        "date": "2023-03-15T11:49:00.000Z",
        "voteCount": 3,
        "content": "A.\nMinimize cost and operational overhead eliminates D. Cloud Composer is a workflow orchestration service for managing workflow pipelines that span across clouds and on premises data centers. Eliminate C. Dataflow processes data. Eliminate B. A is the best answer. \nhttps://cloud.google.com/blog/topics/developers-practitioners/scheduling-cloud-sql-exports-using-cloud-functions-and-cloud-scheduler"
      },
      {
        "date": "2023-02-11T06:57:00.000Z",
        "voteCount": 1,
        "content": "C\nA: Cloud Scheduler and Cloud Functions can't run on the on-premises MySQL database.\nB: Dataflow is mostly good for real time job. Again, need special tweaking to access the on-premises DB.\nC: Best answer. Composer deals with various kinds of inputs. Also handle retry / HA for free.\nD: Run cron job on a GCE instance then we have to maintain GCE instance stability."
      },
      {
        "date": "2023-01-25T05:34:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is \"Create a streaming Datatlow job to export the table\" because it is the most cost-effective and efficient solution. A streaming Datatlow job will allow you to export the table in real time, without having to manually run a cron job. This will save you time and money, and it will also ensure that the data is always up to date."
      },
      {
        "date": "2022-12-24T14:47:00.000Z",
        "voteCount": 3,
        "content": "Please ignore last two submits:\n A: Use Cloud Scheduler *** and Cloud Functions to run the daily export."
      },
      {
        "date": "2022-12-24T14:43:00.000Z",
        "voteCount": 2,
        "content": "A: Use Database Migration Service"
      },
      {
        "date": "2022-12-24T14:43:00.000Z",
        "voteCount": 1,
        "content": "AUse Database Migration Service *** to connect to your on-premises database, and choose continuous replication.\nAfter the on-premises database is migrated, promote the Cloud SQL for MySQL instance, and connect applications to your Cloud SQL instance.\nThe mysqldump client utility performs logical backups, producing a set of SQL statements that can be executed to reproduce the original database object definitions and table data. It dumps one or more MySQL databases for backup or transfer to another SQL server. The mysqldump command can also generate output in CSV, other delimited text, or XML format."
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 104,
    "url": "https://www.examtopics.com/discussions/google/view/92856-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "Your organization needs to migrate a critical, on-premises MySQL database to Cloud SQL for MySQL. The on-premises database is on a version of MySQL that is supported by Cloud SQL and uses the InnoDB storage engine. You need to migrate the database while preserving transactions and minimizing downtime. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Use Database Migration Service to connect to your on-premises database, and choose continuous replication.<br>2. After the on-premises database is migrated, promote the Cloud SQL for MySQL instance, and connect applications to your Cloud SQL instance.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Build a Cloud Data Fusion pipeline for each table to migrate data from the on-premises MySQL database to Cloud SQL for MySQL.<br>2. Schedule downtime to run each Cloud Data Fusion pipeline.<br>3. Verify that the migration was successful.<br>4. Re-point the applications to the Cloud SQL for MySQL instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Pause the on-premises applications.<br>2. Use the mysqldump utility to dump the database content in compressed format.<br>3. Run gsutil \u2013m to move the dump file to Cloud Storage.<br>4. Use the Cloud SQL for MySQL import option.<br>5. After the import operation is complete, re-point the applications to the Cloud SQL for MySQL instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1 Pause the on-premises applications.<br>2. Use the mysqldump utility to dump the database content in CSV format.<br>3. Run gsutil \u2013m to move the dump file to Cloud Storage.<br>4. Use the Cloud SQL for MySQL import option.<br>5. After the import operation is complete, re-point the applications to the Cloud SQL for MySQL instance."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-09-01T09:33:00.000Z",
        "voteCount": 1,
        "content": "Database Migration Service supports one-time and continuous migrations from source databases to Cloud SQL destination databases."
      },
      {
        "date": "2023-03-15T11:53:00.000Z",
        "voteCount": 2,
        "content": "A.\nPreserve transactions and minimize downtime. That eliminates B, C and D. A is the best answer."
      },
      {
        "date": "2023-03-08T11:45:00.000Z",
        "voteCount": 2,
        "content": "A is the right one for me."
      },
      {
        "date": "2023-01-25T05:36:00.000Z",
        "voteCount": 2,
        "content": "To migrate the database while preserving transactions and minimizing downtime, you should use Database Migration Service. This service will allow you to migrate the database in a way that is transparent to your users and applications. It will also allow you to test the migration before you make it live, so that you can be sure that everything will work as expected."
      },
      {
        "date": "2022-12-27T18:55:00.000Z",
        "voteCount": 1,
        "content": "A\nhttps://cloud.google.com/database-migration/docs/mysql/configure-source-database"
      },
      {
        "date": "2022-12-26T02:45:00.000Z",
        "voteCount": 3,
        "content": "A looks like the best option to me"
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 105,
    "url": "https://www.examtopics.com/discussions/google/view/92695-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "Your company is developing a global ecommerce website on Google Cloud. Your development team is working on a shopping cart service that is durable and elastically scalable with live traffic. Business disruptions from unplanned downtime are expected to be less than 5 minutes per month. In addition, the application needs to have very low latency writes. You need a data storage solution that has high write throughput and provides 99.99% uptime. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud SQL for data storage.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Spanner for data storage.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Memorystore for data storage.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Bigtable for data storage."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-05T23:32:00.000Z",
        "voteCount": 1,
        "content": "if the data from shooping kart is transactional, then Yes B, else it should be D"
      },
      {
        "date": "2023-11-16T22:47:00.000Z",
        "voteCount": 1,
        "content": "very low latency = Bigtable"
      },
      {
        "date": "2023-09-01T09:40:00.000Z",
        "voteCount": 3,
        "content": "Spanner  because \"Global\" eliminates others"
      },
      {
        "date": "2023-03-15T15:34:00.000Z",
        "voteCount": 2,
        "content": "B.\nGlobal eliminates Cloud SQL (A). Memorystore is a cache not a data storage solution. Eliminate C. Spanner has multi-region but doesn\u2019t autoscale (unless you use Autoscaler). Bigtable instances can be in different regions and does autoscale. Both Bigtable and Spanner can handle the writes and uptime. The word \u201cdurable\u201d and no mention of huge quantities of data suggests the best answer is B, Cloud Spanner."
      },
      {
        "date": "2023-02-10T03:36:00.000Z",
        "voteCount": 4,
        "content": "B\nNote that what BT differ with Spanner is BT can't do very high throughput (insert single row one-by-one) and low latency at the same time. The official document suggested we should do that in batch; that's a rare case in the shopping cart and no customer will like to wait till other customers finishing their transaction so BT could handle together. Plus, it's hard for BT handling join, e.g. complex upsale, different type of products, etc.\nUptime 99.9% =&gt; BT, Spanner\nLow latency write =&gt; BT, Spanner\nHigh throughput low latency =&gt; Spanner\nECommerce &amp; Shopping cart =&gt; SQL =&gt; Spanner\nhttps://cloud.google.com/bigtable/docs/writes"
      },
      {
        "date": "2023-01-25T05:40:00.000Z",
        "voteCount": 1,
        "content": "oogle Cloud Spanner is a highly scalable, reliable, and fully managed relational database service that runs on Google's infrastructure. It's designed to handle large amounts of data and provide high availability, even in the face of failures. Spanner can be used to store and manage data for a variety of applications, including e-commerce websites.\n\nSpanner is a good choice for this scenario because it can handle high write throughput and provides 99.99% uptime. It's also a good fit for applications that need to be highly available, even in the face of failures."
      },
      {
        "date": "2023-01-10T03:20:00.000Z",
        "voteCount": 1,
        "content": "B- I will go with Spanner because it is a GLOBAL website and BT has a zonal location \nhttps://cloud.google.com/bigtable/docs/locations even it supports replication https://cloud.google.com/bigtable/docs/replication-overview\n\nIt doesn't mention ACID but it mentions durable"
      },
      {
        "date": "2023-01-03T02:11:00.000Z",
        "voteCount": 1,
        "content": "Low latency writes =&gt; BigTable\nNo mention ACID here\nthen my vote D"
      },
      {
        "date": "2022-12-26T07:48:00.000Z",
        "voteCount": 3,
        "content": "Scalable, =&gt;99,99, low write latency -&gt; Spanner"
      },
      {
        "date": "2022-12-24T14:41:00.000Z",
        "voteCount": 1,
        "content": "B: Use Cloud Spanner for data storage.\nCloud Spanner Service Level Agreement (SLA)\nDuring the term of the agreement under which Google has agreed to provide Google Cloud Platform to Customer (as applicable, the \"Agreement\"), the Covered Service will provide a Monthly Uptime Percentage to Customer as follows (the \"Service Level Objective\" or \"SLO\"):\nCovered Service\tMonthly Uptime Percentage\nCloud Spanner - Multi-Regional Instance\t&gt;= 99.999%\nCloud Spanner - Regional Instance\t&gt;= 99.99%"
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 106,
    "url": "https://www.examtopics.com/discussions/google/view/92694-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "Your organization has hundreds of Cloud SQL for MySQL instances. You want to follow Google-recommended practices to optimize platform costs. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Query Insights to identify idle instances.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRemove inactive user accounts.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun the Recommender API to identify overprovisioned instances.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBuild indexes on heavily accessed tables."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-05-03T21:28:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/sql/docs/mysql/recommender-sql-overprovisioned"
      },
      {
        "date": "2023-03-15T16:14:00.000Z",
        "voteCount": 3,
        "content": "C.\nThis is a cost question, so eliminate anything that doesn\u2019t address cost. Eliminate A, B and C. The right answer is to use Recommender.\nhttps://cloud.google.com/recommender/docs/overview"
      },
      {
        "date": "2022-12-30T03:06:00.000Z",
        "voteCount": 4,
        "content": "C\nThe Cloud SQL overprovisioned instance recommender helps you detect instances that are unnecessarily large for a given workload. It then provides recommendations on how to resize such instances and reduce cost. This page describes how this recommender works and how to use it.\nhttps://cloud.google.com/sql/docs/mysql/recommender-sql-overprovisioned#:~:text=The%20Cloud%20SQL%20overprovisioned%20instance%20recommender%20helps%20you%20detect%20instances%20that%20are%20unnecessarily%20large%20for%20a%20given%20workload.%20It%20then%20provides%20recommendations%20on%20how%20to%20resize%20such%20instances%20and%20reduce%20cost.%20This%20page%20describes%20how%20this%20recommender%20works%20and%20how%20to%20use%20it."
      },
      {
        "date": "2022-12-26T07:49:00.000Z",
        "voteCount": 4,
        "content": "https://cloud.google.com/sql/docs/mysql/recommender-sql-overprovisioned"
      },
      {
        "date": "2022-12-24T14:39:00.000Z",
        "voteCount": 1,
        "content": "C: Run the Recommender *** API to identify overprovisioned instances.\nThe Cloud SQL idle instance recommender helps you detect instances that might be idle and provides you insights and recommendations to help you reduce costs .\nThe Cloud SQL overprovisioned instance recommender helps you detect instances that are unnecessarily large for a given workload."
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 107,
    "url": "https://www.examtopics.com/discussions/google/view/92882-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "Your organization is running a critical production database on a virtual machine (VM) on Compute Engine. The VM has an ext4-formatted persistent disk for data files. The database will soon run out of storage space. You need to implement a solution that avoids downtime. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the Google Cloud Console, increase the size of the persistent disk, and use the resize2fs command to extend the disk.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the Google Cloud Console, increase the size of the persistent disk, and use the fdisk command to verify that the new space is ready to use",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the Google Cloud Console, create a snapshot of the persistent disk, restore the snapshot to a new larger disk, unmount the old disk, mount the new disk, and restart the database service.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the Google Cloud Console, create a new persistent disk attached to the VM, and configure the database service to move the files to the new disk."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 10,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-26T07:54:00.000Z",
        "voteCount": 7,
        "content": "If you are using ext4, use the resize2fs command to extend the file system\nhttps://cloud.google.com/compute/docs/disks/resize-persistent-disk#resize_partitions"
      },
      {
        "date": "2024-05-02T01:56:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/compute/docs/disks/resize-persistent-disk#resize_partitions"
      },
      {
        "date": "2024-01-26T06:18:00.000Z",
        "voteCount": 1,
        "content": "A, this is almost the same question as the one on PCA too"
      },
      {
        "date": "2023-11-14T06:54:00.000Z",
        "voteCount": 1,
        "content": "Ai wrong for: -Requires the database to be offline. -Can be prone to errors."
      },
      {
        "date": "2023-09-01T10:04:00.000Z",
        "voteCount": 1,
        "content": "Key words , Critical application , Avoid downtime"
      },
      {
        "date": "2023-03-16T09:04:00.000Z",
        "voteCount": 2,
        "content": "A.\nUsing fdisk by itself won\u2019t achieve anything. A is wrong. Snapshotting a disk with database instances in flight is a potential problem. C is wrong. D, with some tweaking could move things to a larger disk, but there would be downtime. D is therefore wrong. There are several steps to achieving this task and they start with A.  The link provided by chelbsik is spot on."
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 108,
    "url": "https://www.examtopics.com/discussions/google/view/92693-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "You want to migrate your on-premises PostgreSQL database to Compute Engine. You need to migrate this database with the minimum downtime possible. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPerform a full backup of your on-premises PostgreSQL, and then, in the migration window, perform an incremental backup.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a read replica on Cloud SQL, and then promote it to a read/write standalone instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Database Migration Service to migrate your database.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a hot standby on Compute Engine, and use PgBouncer to switch over the connections.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-03-16T09:15:00.000Z",
        "voteCount": 3,
        "content": "D.\nA does not represent the minimum downtime. Read replicas in Cloud SQL depend upon a primary instance also in Cloud SQL. We\u2019re talking about migrating to GCE, so B is wrong. The Database Migration Service migrates PostgreSQL to Cloud SQL or AlloyDB for PostgreSQL (Preview as of 3/16/23 which means it won\u2019t be on the exam - yet). That makes C wrong. That leaves D. Pgbouncer is a connection pooler used to minimize application downtime.\nhttps://cloud.google.com/architecture/migrating-postgresql-to-gcp"
      },
      {
        "date": "2022-12-30T03:17:00.000Z",
        "voteCount": 4,
        "content": "D\nPgBouncer maintains a pool for connections for each database and user combination. PgBouncer either creates a new database connection for a client or reuses an existing connection for the same user and database.\n+\nPgBouncer is a simple PostgreSQL connection pool that allows for several thousand connections at a time. Using Kubernetes Engine to run a Helm Chart w/ PgBouncer based on the great article from futuretech-industries, we were able to set up an easily deployable system to get the most out of our CloudSQL DBs without breaking the bank.\nhttps://medium.com/google-cloud/increasing-cloud-sql-postgresql-max-connections-w-pgbouncer-kubernetes-engine-49b0b2894820#:~:text=That%20is%20where,breaking%20the%20bank."
      },
      {
        "date": "2022-12-26T08:02:00.000Z",
        "voteCount": 4,
        "content": "B - we're not talking CloudSQL here\nC - you can only migrate PostgreSQL to CloudSQL for PostgreSQL or AlloyDB for PostgreSQL\nA is kinda fine, but D is better"
      },
      {
        "date": "2022-12-24T14:35:00.000Z",
        "voteCount": 2,
        "content": "D: Create a hot standby on ***** Compute Engine, and use PgBouncer to switch over the connections.\nTo perform the migration, you shut down the current master and then promote the subordinate Google Cloud replica to master. PgBouncer reroutes traffic to the new master node on Google Cloud."
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 109,
    "url": "https://www.examtopics.com/discussions/google/view/92692-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "You have an application that sends banking events to Bigtable cluster-a in us-east. You decide to add cluster-b in us-central1. Cluster-a replicates data to cluster-b. You need to ensure that Bigtable continues to accept read and write requests if one of the clusters becomes unavailable and that requests are routed automatically to the other cluster. What deployment strategy should you use?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the default app profile with single-cluster routing.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the default app profile with multi-cluster routing.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a custom app profile with multi-cluster routing.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a custom app profile with single-cluster routing."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 16,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 6,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-01-10T03:58:00.000Z",
        "voteCount": 10,
        "content": "C -\nEven the default profile can be single-cluster or multi-cluster \"The settings in an instance's default app profile depend on the number of clusters the instance had when you first created it:\n\nIf you created the instance with 1 cluster, the default app profile uses single-cluster routing, and it enables single-row transactions. This ensures that adding additional clusters later doesn't change the behavior of your existing applications.\""
      },
      {
        "date": "2023-03-16T09:39:00.000Z",
        "voteCount": 8,
        "content": "C.\nTo ensure you still get connected when a Bigtable instance has 2 or more clusters, you must use multi-cluster routing. The question states that a single cluster existed first, then a second cluster was added. Google\u2019s documentation states, \u201cif you created the instance with one cluster, the default app profile uses single-cluster routing. This ensures that adding additional clusters later does not change the behavior of your existing applications\u201d. Simply adding a second cluster does not change the default profile from single-cluster routing to multi-cluster routing. Since you need multi-cluster routing, you\u2019re going to need a custom app profile. So C is correct.\nhttps://cloud.google.com/bigtable/docs/app-profiles#default-app-profile"
      },
      {
        "date": "2023-09-14T01:11:00.000Z",
        "voteCount": 2,
        "content": "Per the link default profile can be updated for multi cloud routing. So answer should be  B\n\nhttps://cloud.google.com/bigtable/docs/replication-settings#high-availability:~:text=To%20configure%20your%20instance%20for%20a%20high%20availability%20(HA)%20use%20case%2C%20create%20a%20new%20app%20profile%20that%20uses%20multi%2Dcluster%20routing%2C%20or%20update%20the%20default%20app%20profile%20to%20use%20multi%2Dcluster%20routing."
      },
      {
        "date": "2024-04-30T04:59:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/bigtable/docs/replication-settings#high-availability\n\nTo configure your instance for a high availability (HA) use case, create a new app profile that uses multi-cluster routing, or update the default app profile to use multi-cluster routing. This configuration provides eventual consistency. You won't be able to enable single-row transactions because single-row transactions can cause data conflicts when you use multi-cluster routing"
      },
      {
        "date": "2024-04-24T01:21:00.000Z",
        "voteCount": 2,
        "content": "I think C is correct.\nTo make B correct it should say \"Update default app profile to use multi-cluster routing and then use it\". But it says just \"Use default app profile\". So B is not complete in this case."
      },
      {
        "date": "2024-02-12T05:30:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/bigtable/docs/replication-settings#high-availability said that  To configure your instance for a high availability (HA) use case, create a new app profile that uses multi-cluster routing, or update the default app profile to use multi-cluster routing."
      },
      {
        "date": "2024-01-26T06:40:00.000Z",
        "voteCount": 1,
        "content": "The default app profile does not change when you add or remove clusters. You must manually update the default app profile to change its settings. However, as a best practice you should create and use a new app profile instead of changing the default app profile.\nOnce the new default app with multi-cluster in place, the rest follows multi-cluster path."
      },
      {
        "date": "2023-10-04T20:49:00.000Z",
        "voteCount": 2,
        "content": "Since you created your cluster with only one cluster, the default app profile will have single-cluster routing. You want multi-cluster routing so that if one of the clusters is unavailable, requests are routed automatically to the other cluster. Because of this you need a custom app profile and multi-cluster routing."
      },
      {
        "date": "2023-09-14T01:12:00.000Z",
        "voteCount": 2,
        "content": "Default profile can be updated to use multi-cluster routing\n\nhttps://cloud.google.com/bigtable/docs/replication-settings#high-availability:~:text=To%20configure%20your%20instance%20for%20a%20high%20availability%20(HA)%20use%20case%2C%20create%20a%20new%20app%20profile%20that%20uses%20multi%2Dcluster%20routing%2C%20or%20update%20the%20default%20app%20profile%20to%20use%20multi%2Dcluster%20routing."
      },
      {
        "date": "2023-08-16T06:36:00.000Z",
        "voteCount": 2,
        "content": "C\n\nFrom https://cloud.google.com/bigtable/docs/app-profiles#default-app-profile\n\nThe default app profile does not change when you add or remove clusters. You must manually update the default app profile to change its settings. However, as a best practice you should create and use a new app profile instead of changing the default app profile."
      },
      {
        "date": "2023-07-30T13:56:00.000Z",
        "voteCount": 2,
        "content": "C. Create a custom app profile with multi-cluster routing. \nDefault with multi-cluster won't work as the BigTable was created with a single cluster initially. Hence Custom with multi-cluster option."
      },
      {
        "date": "2023-03-09T09:26:00.000Z",
        "voteCount": 3,
        "content": "Check the documentation: \n\nThe default app profile does not change when you add or remove clusters. You must manually update the default app profile to change its settings. However, as a best practice you should create and use a new app profile instead of changing the default app profile.\n\nhttps://cloud.google.com/bigtable/docs/app-profiles#how-they-work\n\nBest practice must be the C, a custom app profile."
      },
      {
        "date": "2023-03-08T22:56:00.000Z",
        "voteCount": 2,
        "content": "might be better option"
      },
      {
        "date": "2023-01-08T12:07:00.000Z",
        "voteCount": 3,
        "content": "C\nIf you created the instance with 1 cluster, the default app profile uses single-cluster routing, and it enables single-row transactions. This ensures that adding additional clusters later doesn't change the behavior of your existing applications."
      },
      {
        "date": "2023-01-08T12:04:00.000Z",
        "voteCount": 3,
        "content": "C\nhttps://cloud.google.com/bigtable/docs/app-profiles#:~:text=Multi%2Dcluster%20routing%20automatically%20routes,they%20are%20in%20different%20zones."
      },
      {
        "date": "2022-12-28T11:48:00.000Z",
        "voteCount": 2,
        "content": "A The default app profile does not change when you add or remove clusters. You must manually update the default app profile to change its settings. However, as a best practice you should create and use a new app profile instead of changing the default app profile."
      },
      {
        "date": "2022-12-28T11:49:00.000Z",
        "voteCount": 3,
        "content": "Correction! I meant C not A."
      },
      {
        "date": "2022-12-26T08:16:00.000Z",
        "voteCount": 2,
        "content": "Default profile can be single-cluster or multi-cluster\nhttps://cloud.google.com/bigtable/docs/app-profiles#:~:text=Multi%2Dcluster%20routing%20automatically%20routes,they%20are%20in%20different%20zones."
      },
      {
        "date": "2022-12-24T14:33:00.000Z",
        "voteCount": 1,
        "content": "B: Use the default app profile with multi-cluster *** routing.\nWhat is Bigtable cluster? A cluster represents the Bigtable service in a specific location. Each cluster belongs to a single Bigtable instance, and an instance can have clusters in up to 8 regions. When your application sends requests to a Bigtable instance, those requests are handled by one of the clusters in the instance."
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 110,
    "url": "https://www.examtopics.com/discussions/google/view/92689-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "Your organization works with sensitive data that requires you to manage your own encryption keys. You are working on a project that stores that data in a Cloud SQL database. You need to ensure that stored data is encrypted with your keys. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExport data periodically to a Cloud Storage bucket protected by Customer-Supplied Encryption Keys.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud SQL Auth proxy.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConnect to Cloud SQL using a connection that has SSL encryption.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse customer-managed encryption keys with Cloud SQL.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-30T05:01:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/sql/docs/mysql/configure-cmek\nhttps://cloud.google.com/sql/docs/postgres/configure-cmek\nhttps://cloud.google.com/sql/docs/sqlserver/configure-cmek\nhttps://cloud.google.com/sql/docs/sqlserver/configure-cmek#workflow_for_creating_a_instance_with_cmek"
      },
      {
        "date": "2023-03-16T09:48:00.000Z",
        "voteCount": 3,
        "content": "D.\nBy stored data, we assume it means stored in the database so there\u2019s no point in exporting it to encrypt it. A is wrong. Cloud SQL Auth Proxy is about secure connections to Cloud SQL, not encrypted data. B is wrong and so is C for the same reason. That leaves D. Just use CMEK when you create the instance."
      },
      {
        "date": "2022-12-26T08:17:00.000Z",
        "voteCount": 3,
        "content": "Standard CMEK quiestion, D"
      },
      {
        "date": "2022-12-24T14:31:00.000Z",
        "voteCount": 1,
        "content": "D: Use customer-managed encryption keys *** with Cloud SQL.\nHow do I encrypt a SQL database?\nEnable Transparent Data Encryption (TDE)\n\nCreate a master key. Create or obtain a certificate protected by the master key. Create a database encryption key and protect it by using the certificate. Set the database to use encryption."
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 111,
    "url": "https://www.examtopics.com/discussions/google/view/92688-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "Your team is building an application that stores and analyzes streaming time series financial data. You need a database solution that can perform time series-based scans with sub-second latency. The solution must scale into the hundreds of terabytes and be able to write up to 10k records per second and read up to 200 MB per second. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Firestore.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Bigtable\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse BigQuery.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Spanner."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-12-26T08:40:00.000Z",
        "voteCount": 8,
        "content": "Financial data, such as transaction histories, stock prices, and currency exchange rates.\nhttps://cloud.google.com/bigtable/docs/overview#what-its-good-for\n\nWith SSD:\nReads - up to 10,000 rows per second\nWrites - up to 10,000 rows per second\nScans - up to 220 MB/s\nhttps://cloud.google.com/bigtable/docs/performance#typical-workloads"
      },
      {
        "date": "2024-05-23T10:57:00.000Z",
        "voteCount": 1,
        "content": "Totally agree with B"
      },
      {
        "date": "2023-03-16T09:50:00.000Z",
        "voteCount": 2,
        "content": "B.\nProcessing time-series data is a classic use case for Bigtable. Sub-second latency and scaling to 100s of TBs seals the deal. It\u2019s B."
      },
      {
        "date": "2022-12-24T14:28:00.000Z",
        "voteCount": 1,
        "content": "C: Use BigQuery.\nAbout 330 100MB/sec dedicated hard-drives to read 1TB of data. A 330 Gigabit network to shuffle the 1.25 TB of data. 3,300 cores to uncompress 1TB of data and process 100 billion regular expressions at 1 \u03bcsec per. \n\u03bcsec: A microsecond is a unit of time in the International System of Units equal to one millionth of a second. Its symbol is \u03bcs, sometimes simplified to us when Unicode is not available. A microsecond is equal to 1000 nanoseconds or 1\u20441000 of a millisecond."
      },
      {
        "date": "2023-01-05T01:04:00.000Z",
        "voteCount": 2,
        "content": "time series data =&gt; Big Table per Google reccomandation"
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 112,
    "url": "https://www.examtopics.com/discussions/google/view/92687-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "You are designing a new gaming application that uses a highly transactional relational database to store player authentication and inventory data in Google Cloud. You want to launch the game in multiple regions. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Spanner to deploy the database.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Bigtable with clusters in multiple regions to deploy the database",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse BigQuery to deploy the database",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud SQL with a regional read replica to deploy the database."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-12-26T08:43:00.000Z",
        "voteCount": 6,
        "content": "Global, relational -&gt; Spanner"
      },
      {
        "date": "2023-03-16T09:55:00.000Z",
        "voteCount": 2,
        "content": "A.\nBigtable would not be a valid choice. BigQuery lends itself to OLAP not OLTP. Cloud SQL could not scale to multiple regions. It\u2019s Spanner. A."
      },
      {
        "date": "2022-12-24T14:26:00.000Z",
        "voteCount": 1,
        "content": "A: Use Cloud *** Spanner to deploy the database.\nCloud Spanner is a fully managed, mission-critical, relational database service that offers transactional consistency at global scale, automatic, synchronous replication for high availability, and support for two SQL dialects: Google Standard SQL (ANSI 2011 with extensions) and PostgreSQL."
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 113,
    "url": "https://www.examtopics.com/discussions/google/view/92908-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "You are designing a database strategy for a new web application in one region. You need to minimize write latency. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud SQL with cross-region replicas.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse high availability (HA) Cloud SQL with multiple zones.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse zonal Cloud SQL without high availability (HA).",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Spanner in a regional configuration.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 11,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 10,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-01-10T04:17:00.000Z",
        "voteCount": 10,
        "content": "D -  spanner\nABC it is about Cloud SQL and the options don't offer anything to improve writing latency"
      },
      {
        "date": "2022-12-26T13:21:00.000Z",
        "voteCount": 9,
        "content": "Another metric to note is that the latency for regional persistent disk with solid-state drives (SSD) is higher than it would be for a zonal persistent disk with SSD.\nhttps://cloud.google.com/sql/docs/mysql/high-availability#ha-performance"
      },
      {
        "date": "2023-12-15T13:48:00.000Z",
        "voteCount": 1,
        "content": "I think is D. A only  increase the HA"
      },
      {
        "date": "2024-03-12T05:06:00.000Z",
        "voteCount": 1,
        "content": "multiples zonas has more latency"
      },
      {
        "date": "2024-01-26T06:45:00.000Z",
        "voteCount": 1,
        "content": "The only one with write latency is regional Spanner"
      },
      {
        "date": "2023-09-12T13:20:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is D - Use Cloud Spanner in a regional configuration."
      },
      {
        "date": "2023-09-01T12:23:00.000Z",
        "voteCount": 2,
        "content": "its Cloud SQL vs Spanner . Spanner has advantage in writing latency"
      },
      {
        "date": "2023-08-25T01:51:00.000Z",
        "voteCount": 1,
        "content": "A, B, C. replicas and HA will help in improving read latency only. Cloud SQL enables customers to implement horizontal scaling of read-only workloads using read replicas. It might degrade the write latency.\nD. Spanner satisfies the requirement without sacrificing availability"
      },
      {
        "date": "2023-08-24T17:22:00.000Z",
        "voteCount": 1,
        "content": "I think D."
      },
      {
        "date": "2023-07-26T00:07:00.000Z",
        "voteCount": 3,
        "content": "Spanner has higher write latency than other options"
      },
      {
        "date": "2023-07-24T14:57:00.000Z",
        "voteCount": 2,
        "content": "D: just choose Cloud Spanner over Cloud SQL"
      },
      {
        "date": "2023-07-17T11:50:00.000Z",
        "voteCount": 2,
        "content": "A and B add latency. C screams of an anti-pattern to me (no HA). Spanner satisfies the requirement without sacrificing availability, so D."
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 114,
    "url": "https://www.examtopics.com/discussions/google/view/92685-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "You are running a large, highly transactional application on Oracle Real Application Cluster (RAC) that is multi-tenant and uses shared storage. You need a solution that ensures high-performance throughput and a low-latency connection between applications and databases. The solution must also support existing Oracle features and provide ease of migration to Google Cloud. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate to Compute Engine.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate to Bare Metal Solution for Oracle.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate to Google Kubernetes Engine (GKE)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate to Google Cloud VMware Engine"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-09-01T12:32:00.000Z",
        "voteCount": 1,
        "content": "BMS for Oracle"
      },
      {
        "date": "2023-03-16T10:11:00.000Z",
        "voteCount": 3,
        "content": "B.\nOracle is neither licensed nor supported in GCE. The only platform which supports RAC and all existing Oracle features is BMS."
      },
      {
        "date": "2022-12-26T09:08:00.000Z",
        "voteCount": 3,
        "content": "B for sure"
      },
      {
        "date": "2022-12-24T14:23:00.000Z",
        "voteCount": 2,
        "content": "B: Migrate to Bare Metal Solution ***** for Oracle."
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 115,
    "url": "https://www.examtopics.com/discussions/google/view/92684-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "You are choosing a new database backend for an existing application. The current database is running PostgreSQL on an on-premises VM and is managed by a database administrator and operations team. The application data is relational and has light traffic. You want to minimize costs and the migration effort for this application. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the existing database to Firestore.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the existing database to Cloud SQL for PostgreSQL.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the existing database to Cloud Spanner.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the existing database to PostgreSQL running on Compute Engine."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 3,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-03-16T10:17:00.000Z",
        "voteCount": 5,
        "content": "B.\nFirestore is a NoSQL database. Eliminate A. You could migrate to Spanner leveraging the PostgreSQL dialect, but costs need to be minimized so that wouldn\u2019t be the cheapest option. Especially since the load doesn\u2019t justify Spanner. Again, you could migrate like-for-like to a GCE VM, but that defeats minimizing the migration effort. The cheapest and easiest way to migrate would be Database Migration Service to Cloud SQL for PostgreSQL."
      },
      {
        "date": "2023-12-16T14:23:00.000Z",
        "voteCount": 1,
        "content": "I agree with u. The answer is B"
      },
      {
        "date": "2024-05-28T10:19:00.000Z",
        "voteCount": 1,
        "content": "Totally B"
      },
      {
        "date": "2022-12-26T09:09:00.000Z",
        "voteCount": 2,
        "content": "Migrate to CloudSQL"
      },
      {
        "date": "2022-12-24T14:23:00.000Z",
        "voteCount": 1,
        "content": "B: Migrate the existing database to Cloud SQL for PostgreSQL."
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 116,
    "url": "https://www.examtopics.com/discussions/google/view/92683-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "Your organization is currently updating an existing corporate application that is running in another public cloud to access managed database services in Google Cloud. The application will remain in the other public cloud while the database is migrated to Google Cloud. You want to follow Google-recommended practices for authentication. You need to minimize user disruption during the migration. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse workload identity federation to impersonate a service account.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAsk existing users to set their Google password to match their corporate password.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the application to Google Cloud, and use Identity and Access Management (IAM).",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Google Workspace Password Sync to replicate passwords into Google Cloud."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-03-16T13:51:00.000Z",
        "voteCount": 6,
        "content": "A.\nUpdating passwords represents user disruption. Eliminate B. Eliminate C for the same reason. D doesn\u2019t make sense, leaves A. From Google\u2019s documentation, \u201cTraditionally, applications running outside Google Cloud can use service account keys to access Google Cloud resources. However, service account keys are powerful credentials, and can present a security risk if they are not managed correctly.\n\nWith identity federation, you can use Identity and Access Management (IAM) to grant external identities IAM roles, including the ability to impersonate service accounts. This approach eliminates the maintenance and security burden associated with service account keys.\u201d\nhttps://cloud.google.com/iam/docs/workload-identity-federation"
      },
      {
        "date": "2024-04-30T05:12:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/iam/docs/workload-identity-federation"
      },
      {
        "date": "2022-12-26T09:17:00.000Z",
        "voteCount": 4,
        "content": "With identity federation, you can use Identity and Access Management (IAM) to grant external identities IAM roles, including the ability to impersonate service accounts. This lets you access resources directly, using a short-lived access token, and eliminates the maintenance and security burden associated with service account keys."
      },
      {
        "date": "2022-12-24T14:22:00.000Z",
        "voteCount": 1,
        "content": "A: Use workload identity ***** federation to impersonate a service account.\nUse identity federation to access resources from AWS, access resources from Microsoft Azure, access resources from an OIDC provider, or access resources from a SAML 2.0 provider. Learn how to manage workload identity pools using the Google Cloud CLI or the REST API."
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 117,
    "url": "https://www.examtopics.com/discussions/google/view/92682-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "You are configuring the networking of a Cloud SQL instance. The only application that connects to this database resides on a Compute Engine VM in the same project as the Cloud SQL instance. The VM and the Cloud SQL instance both use the same VPC network, and both have an external (public) IP address and an internal (private) IP address. You want to improve network security. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDisable and remove the internal IP address assignment.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDisable both the external IP address and the internal IP address, and instead rely on Private Google Access.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSpecify an authorized network with the CIDR range of the VM.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDisable and remove the external IP address assignment.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-05-02T01:37:00.000Z",
        "voteCount": 1,
        "content": "D: gcp best practices, not use internal ip unless strickly necessary"
      },
      {
        "date": "2024-03-26T03:40:00.000Z",
        "voteCount": 1,
        "content": "D makes most sense"
      },
      {
        "date": "2023-09-12T15:34:00.000Z",
        "voteCount": 3,
        "content": "The correct answer is C. Specify an authorized network with the CIDR range of the VM."
      },
      {
        "date": "2023-12-15T13:56:00.000Z",
        "voteCount": 1,
        "content": "They are already in the same VPC, why do you want to create another authorized network????"
      },
      {
        "date": "2023-03-16T14:03:00.000Z",
        "voteCount": 2,
        "content": "D.\nIt is always more secure to use an internal IP, so removing them doesn\u2019t make sense. Eliminate A. You can use Private Google Access when VM instances only have internal IP addresses, so disabling the internal IPs and use Private Google Access doesn\u2019t make sense. Eliminate B. Specifying an authorized network when they\u2019re on the same subnet doesn\u2019t make sense. Eliminate C. A way to improve network security would be to disable external IPs since they\u2019re not needed."
      },
      {
        "date": "2022-12-28T12:26:00.000Z",
        "voteCount": 2,
        "content": "D as both are in VPC they can communicate internally. \nPrivate Google Access enabled allows VM instances which only have internal IP addresses (no external IP addresses) to reach the external IP addresses of Google APIs and services"
      },
      {
        "date": "2022-12-27T19:31:00.000Z",
        "voteCount": 1,
        "content": "D. Within VPC, all traffic is open"
      },
      {
        "date": "2022-12-26T09:23:00.000Z",
        "voteCount": 4,
        "content": "B is not applicable here because Private Google Access requires internal IP address."
      },
      {
        "date": "2022-12-24T14:19:00.000Z",
        "voteCount": 1,
        "content": "B: Disable both the external IP address and the internal IP address, and instead rely on *** Private Google Access."
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 118,
    "url": "https://www.examtopics.com/discussions/google/view/92681-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "You are managing two different applications: Order Management and Sales Reporting. Both applications interact with the same Cloud SQL for MySQL database. The Order Management application reads and writes to the database 24/7, but the Sales Reporting application is read-only. Both applications need the latest data. You need to ensure that the Performance of the Order Management application is not affected by the Sales Reporting application. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a read replica for the Sales Reporting application.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate two separate databases in the instance, and perform dual writes from the Order Management application.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a Cloud SQL federated query for the Sales Reporting application.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tQueue up all the requested reports in PubSub, and execute the reports at night."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-12-26T09:25:00.000Z",
        "voteCount": 7,
        "content": "Read replica is more than enough"
      },
      {
        "date": "2023-03-16T14:09:00.000Z",
        "voteCount": 5,
        "content": "A.\nB could be done, but would need some application work to make it happen. Eliminate B. Federated queries are queries sent from BigQuery to Cloud SQL or Cloud Spanner with the results returned to BigQuery as a temporary table. So C is wrong. Even if D made sense, running all the reports at night would still impact the Order Management application since it accesses the database 24x7. That leaves A which is the classic solution to offload reads from the primary instance."
      },
      {
        "date": "2022-12-24T14:18:00.000Z",
        "voteCount": 3,
        "content": "A: Create a read replica for the ***** Sales Reporting application."
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 119,
    "url": "https://www.examtopics.com/discussions/google/view/92680-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "You are the DBA of an online tutoring application that runs on a Cloud SQL for PostgreSQL database. You are testing the implementation of the cross-regional failover configuration. The database in region R1 fails over successfully to region R2, and the database becomes available for the application to process data. During testing, certain scenarios of the application work as expected in region R2, but a few scenarios fail with database errors. The application-related database queries, when executed in isolation from Cloud SQL for PostgreSQL in region R2, work as expected. The application performs completely as expected when the database fails back to region R1. You need to identify the cause of the database errors in region R2. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDetermine whether the versions of Cloud SQL for PostgreSQL in regions R1 and R2 are different.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDetermine whether the database patches of Cloud SQI for PostgreSQL in regions R1 and R2 are different.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDetermine whether the failover of Cloud SQL for PostgreSQL from region R1 to region R2 is in progress or has completed successfully.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDetermine whether Cloud SQL for PostgreSQL in region R2 is a near-real-time copy of region R1 but not an exact copy.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 7,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 6,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-27T03:46:00.000Z",
        "voteCount": 7,
        "content": "Verify that the replica has processed all the transactions it has received from the primary. This ensures that when promoted, the replica reflects all transactions that were received before the primary became unavailable.\nhttps://cloud.google.com/sql/docs/postgres/replication/cross-region-replicas#verify_failover_criteria"
      },
      {
        "date": "2023-03-16T14:19:00.000Z",
        "voteCount": 5,
        "content": "D.\nSince this is Cloud SQL, the versions and patch levels should be the same since it\u2019s managed by Google. That eliminates A and B. The instance in region 2 must have been a read replica which was promoted to being the primary instance. You cannot access the new primary until Google completes the failover, so C doesn\u2019t make sense. That leaves D. Basically, the data is slightly out of sync."
      },
      {
        "date": "2023-08-16T07:12:00.000Z",
        "voteCount": 1,
        "content": "You can have different  patches\n\nhttps://cloud.google.com/sdk/gcloud/reference/sql/instances/patch"
      },
      {
        "date": "2024-04-30T05:21:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/sql/docs/postgres/replication/manage-replicas#promote-replica \n\nBefore promoting a read replica, if the primary is still available and serving clients, you should do the following:\n\nStop all writes to the primary instance.\nCheck the replication status of the replica (follow the instructions in the psql Client tab).\nVerify that the replica is replicating, and then wait until the replication lag reported by the replay_lag metric is 0.\nOtherwise, a newly promoted instance may be missing some transactions that were committed to the primary instance."
      },
      {
        "date": "2023-09-12T15:48:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is - C. Determine whether the failover of Cloud SQL for PostgreSQL from region R1 to region R2 is in progress or has completed successfully."
      },
      {
        "date": "2023-09-01T14:41:00.000Z",
        "voteCount": 2,
        "content": "will go by C"
      },
      {
        "date": "2023-08-16T07:10:00.000Z",
        "voteCount": 1,
        "content": "B\n\n Determine whether the database patches of Cloud SQL for PostgreSQL in regions R1 and R2 are different.\n\nThis is important because differences in database patches could lead to inconsistencies in behavior between the two regions. Ensuring that the database patches are consistent between the two regions is essential for stable and expected behavior.\n\nD does not makes sense because the question specify that when executed in isolation from Cloud SQL for PostgreSQL in region R2, work as expected."
      },
      {
        "date": "2023-03-08T23:36:00.000Z",
        "voteCount": 3,
        "content": "If it was a DATA gap of data between the databases; The SQLs will not return \"database-errors\" just different data sets. Therefore I'm not going for D. Yet if the failover is ongoing/in-progress/hanging or not completed - than some stuff will work and other not. And we will get \"database-errors\"   I'm going for C \n(A, B - versions should be managed by GCP)"
      },
      {
        "date": "2022-12-24T14:15:00.000Z",
        "voteCount": 1,
        "content": "B : Determine whether the database *** patches of Cloud SQI for PostgreSQL in regions R1 and R2 are different"
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 120,
    "url": "https://www.examtopics.com/discussions/google/view/92679-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "Your company wants to migrate its MySQL, PostgreSQL, and Microsoft SQL Server on-premises databases to Google Cloud. You need a solution that provides near-zero downtime, requires no application changes, and supports change data capture (CDC). What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the native export and import functionality of the source database.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a database on Google Cloud, and use database links to perform the migration.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a database on Google Cloud, and use Dataflow for database migration.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Database Migration Service.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-09-08T18:39:00.000Z",
        "voteCount": 2,
        "content": "D is right"
      },
      {
        "date": "2023-09-01T14:44:00.000Z",
        "voteCount": 1,
        "content": "Will go by D"
      },
      {
        "date": "2022-12-26T09:40:00.000Z",
        "voteCount": 3,
        "content": "Vote for D"
      },
      {
        "date": "2022-12-24T14:14:00.000Z",
        "voteCount": 1,
        "content": "The correct Answer is D:\nDatabase Migration Service\nSimplify migrations to the cloud. Available now for MySQL and PostgreSQL, with SQL Server and Oracle migrations in preview.\n\u2022\tMigrate to Cloud SQL and AlloyDB for PostgreSQL from on-premises, Google Cloud, or other clouds\n\u2022\tReplicate data continuously for minimal downtime migrations\n\u2022\tServerless and easy to set up"
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 121,
    "url": "https://www.examtopics.com/discussions/google/view/104138-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "You are managing a Cloud SQL for PostgreSQL instance in Google Cloud. You have a primary instance in region 1 and a read replica in region 2. After a failure of region 1, you need to make the Cloud SQL instance available again. You want to minimize data loss and follow Google-recommended practices. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRestore the Cloud SQL instance from the automatic backups in region 3.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRestore the Cloud SQL instance from the automatic backups in another zone in region 1.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCheck \"Lag Bytes\" in the monitoring dashboard for the primary instance in the read replica instance. Check the replication status using pg_catalog.pg_last_wal_receive_lsn(). Then, fail over to region 2 by promoting the read replica instance.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCheck your instance operational log for the automatic failover status. Look for time, type, and status of the operations. If the failover operation is successful, no action is necessary. Otherwise, manually perform gcloud sql instances failover <primaryinstancename>.\n\t\t\t\t\t\t\t\t\t\t</primaryinstancename>"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-03-27T17:06:00.000Z",
        "voteCount": 8,
        "content": "C.\nNo need for restores if you have a read replica which you can promote to be the new primary. Eliminate A and B. Failovers do not happen automatically to read replicas. You have to promote them. Eliminate D. That leaves C which is supported by Google's documentation.\nhttps://cloud.google.com/sql/docs/postgres/replication/cross-region-replicas#disaster_recovery"
      },
      {
        "date": "2024-03-26T03:07:00.000Z",
        "voteCount": 1,
        "content": "answer C makes sense, special thanks to dynamic dba"
      },
      {
        "date": "2023-12-16T14:30:00.000Z",
        "voteCount": 2,
        "content": "Agree with dynamic_dba too, thanks for the perfect document"
      },
      {
        "date": "2023-09-01T14:48:00.000Z",
        "voteCount": 1,
        "content": "C is the only option ,"
      },
      {
        "date": "2023-04-27T05:57:00.000Z",
        "voteCount": 3,
        "content": "agree with dynamic_dba. Read his reference link"
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 122,
    "url": "https://www.examtopics.com/discussions/google/view/103732-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "You need to issue a new server certificate because your old one is expiring. You need to avoid a restart of your Cloud SQL for MySQL instance. What should you do in your Cloud SQL instance?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIssue a rollback, and download your server certificate.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new client certificate, and download it.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new server certificate, and download it.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReset your SSL configuration, and download your server certificate."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-03-26T03:04:00.000Z",
        "voteCount": 1,
        "content": "answer is C , update the stinkin answers"
      },
      {
        "date": "2023-09-01T14:52:00.000Z",
        "voteCount": 1,
        "content": "will go by C"
      },
      {
        "date": "2023-06-13T11:06:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/sql/docs/mysql/manage-ssl-instance#rotate"
      },
      {
        "date": "2023-04-27T05:59:00.000Z",
        "voteCount": 1,
        "content": "vote for C"
      },
      {
        "date": "2023-03-27T17:09:00.000Z",
        "voteCount": 4,
        "content": "C.\nIssuing a rollback is nonsense. Eliminate A. The client certificate is unaffected. Eliminate B. Resetting the SSL configuration is not mentioned, but if you enforced SSL that would require an instance re-start. All you need to do is create a new server certificate and download it (to your clients). \nhttps://cloud.google.com/sql/docs/sqlserver/configure-ssl-instance#server-certs"
      },
      {
        "date": "2023-03-23T21:05:00.000Z",
        "voteCount": 1,
        "content": "You must restart an instance after enforcing SSL for the instance. However, you don't need to restart the instance after changing SSL/TLS certificates. If a restart is required, then this is done automatically during the SSL update event.\nhttps://cloud.google.com/sql/docs/mysql/configure-ssl-instance"
      },
      {
        "date": "2023-04-30T08:10:00.000Z",
        "voteCount": 1,
        "content": "Ans is C"
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 123,
    "url": "https://www.examtopics.com/discussions/google/view/103699-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "Your company is migrating all legacy applications to Google Cloud. All on-premises applications are using legacy Oracle 12c databases with Oracle Real Application Cluster (RAC) for high availability (HA) and Oracle Data Guard for disaster recovery. You need a solution that requires minimal code changes, provides the same high availability you have today on-premises, and supports a low latency network for migrated legacy applications. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the databases to Cloud Spanner.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the databases to Cloud SQL, and enable a standby database.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the databases to Compute Engine using regional persistent disks.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the databases to Bare Metal Solution for Oracle.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-06-27T05:49:00.000Z",
        "voteCount": 1,
        "content": "Pretty much anytime Oracle is mentioned the answer is always Bare Metal"
      },
      {
        "date": "2023-03-27T17:12:00.000Z",
        "voteCount": 2,
        "content": "D.\nBMS is the only Google database service which supports Oracle aside from GCVE. It allows you to use all native Oracle features including RAC. Since GCVE isn't mentioned, it has to be D - Bare Metal Solution."
      },
      {
        "date": "2023-03-23T13:23:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is D!"
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 124,
    "url": "https://www.examtopics.com/discussions/google/view/103698-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "Your company is evaluating Google Cloud database options for a mission-critical global payments gateway application. The application must be available 24/7 to users worldwide, horizontally scalable, and support open source databases. You need to select an automatically shardable, fully managed database with 99.999% availability and strong transactional consistency. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSelect Bare Metal Solution for Oracle.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSelect Cloud SQL.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSelect Bigtable.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSelect Cloud Spanner.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 6,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-04-30T05:33:00.000Z",
        "voteCount": 1,
        "content": "99.999% -&gt; spanner"
      },
      {
        "date": "2024-03-26T02:57:00.000Z",
        "voteCount": 1,
        "content": "horizontally scalable --&gt; Cloud spanner"
      },
      {
        "date": "2024-03-12T05:19:00.000Z",
        "voteCount": 1,
        "content": "Opensource --&gt; Cloudsql"
      },
      {
        "date": "2023-07-27T11:35:00.000Z",
        "voteCount": 3,
        "content": "The application must be available 24/7 to users worldwide, horizontally scalable, and support open source databases.\n\nOpen source databases .... The only solution is Cloud SQL..."
      },
      {
        "date": "2024-04-07T05:14:00.000Z",
        "voteCount": 1,
        "content": "but cloud sql doesn't provide 99.999 availability"
      },
      {
        "date": "2023-04-27T06:03:00.000Z",
        "voteCount": 4,
        "content": "worldwide, horizontally scalable, strong transactional consistency\nDefinitely Cloud Spanner"
      },
      {
        "date": "2023-03-27T17:15:00.000Z",
        "voteCount": 3,
        "content": "D.\nOracle is not open source. Eliminate A. Horizontally scalable means adding more servers to run the database. That eliminates Cloud SQL (B). Strong transactional consistency rules out Bigtable since Bigtable transactions are at the single row level. Eliminate C. That leaves D."
      },
      {
        "date": "2023-03-23T13:22:00.000Z",
        "voteCount": 2,
        "content": "The answers correct is D"
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 125,
    "url": "https://www.examtopics.com/discussions/google/view/104141-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "You are a DBA of Cloud SQL for PostgreSQL. You want the applications to have password-less authentication for read and write access to the database. Which authentication mechanism should you use?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Identity and Access Management (IAM) authentication.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Managed Active Directory authentication.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud SQL federated queries.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse PostgreSQL database's built-in authentication."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-03-27T17:17:00.000Z",
        "voteCount": 6,
        "content": "A.\nGoogle wants you to use IAM regardless. PostgreSQL built-in authentication requires a username and a password. That rules out D. Federated queries are queries originating from BigQuery to Cloud SQL. Eliminate C. AD is strongly tied to SQL Server. Eliminate B. That leaves A."
      },
      {
        "date": "2024-05-02T01:26:00.000Z",
        "voteCount": 1,
        "content": "A: https://cloud.google.com/sql/docs/postgres/authentication\nCloud SQL provides a set of predefined roles designed to help you control access to your Cloud SQL resources. You can also create your own custom roles, if the predefined roles don't provide the sets of permissions you need. In addition, the legacy basic roles (Editor, Viewer, and Owner) are also still available to you, although they don't provide the same fine-grained control as the Cloud SQL roles. In particular, the basic roles provide access to resources across Google Cloud, rather than just for Cloud SQL. For more information about basic Google Cloud roles, see Basic roles.\n\nYou can set an IAM policy at any level in the resource hierarchy: the organization level, the folder level, or the project level. Resources inherit the policies of all of their parent resources."
      },
      {
        "date": "2023-06-13T10:42:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/sql/docs/postgres/authentication"
      },
      {
        "date": "2023-04-14T08:00:00.000Z",
        "voteCount": 1,
        "content": "A.\nIAM Authentication is password less."
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 126,
    "url": "https://www.examtopics.com/discussions/google/view/103733-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "You are migrating your 2 TB on-premises PostgreSQL cluster to Compute Engine. You want to set up your new environment in an Ubuntu virtual machine instance in Google Cloud and seed the data to a new instance. You need to plan your database migration to ensure minimum downtime. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Take a full export while the database is offline.<br>2. Create a bucket in Cloud Storage.<br>3. Transfer the dump file to the bucket you just created.<br>4. Import the dump file into the Google Cloud primary server.<br>B.1. Take a full export while the database is offline.<br>2. Create a bucket in Cloud Storage.<br>3. Transfer the dump file to the bucket you just created.<br>4. Restore the backup into the Google Cloud primary server.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Take a full backup while the database is online.<br>2. Create a bucket in Cloud Storage.<br>3. Transfer the backup to the bucket you just created.<br>4. Restore the backup into the Google Cloud primary server.<br>5. Create a recovery.conf file in the $PG_DATA directory.<br>6. Stop the source database.<br>7. Transfer the write ahead logs to the bucket you created before.<br>8. Start the PostgreSQL service.<br>9. Wait until Google Cloud primary server syncs with the running primary server.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Take a full export while the database is online.<br>2. Create a bucket in Cloud Storage.<br>3. Transfer the dump file and write-ahead logs to the bucket you just created.<br>4. Restore the dump file into the Google Cloud primary server.<br>5. Create a recovery.conf file in the $PG_DATA directory.<br>6. Stop the source database.<br>7. Transfer the write-ahead logs to the bucket you created before.<br>8. Start the PostgreSQL service.<br>9. Wait until the Google Cloud primary server syncs with the running primary server."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-05-24T03:25:00.000Z",
        "voteCount": 1,
        "content": "Agree with C, As taking backup is faster than the export."
      },
      {
        "date": "2024-05-02T01:27:00.000Z",
        "voteCount": 1,
        "content": "c: https://cloud.google.com/architecture/migrating-postgresql-to-gcp"
      },
      {
        "date": "2023-09-14T04:22:00.000Z",
        "voteCount": 2,
        "content": "Couple of reasons for C option.\n1. Database remains online (see link below, and step 1, which says \"running master database\"\n2. Backups are faster then exports (which generates new files)\nhttps://cloud.google.com/architecture/migrating-postgresql-to-gcp"
      },
      {
        "date": "2023-08-16T08:38:00.000Z",
        "voteCount": 2,
        "content": "B\n\nThis approach minimizes downtime by exporting the database while it's offline, transferring it to Google Cloud Storage, and then restoring it into the new Google Cloud primary server. It's a straightforward and efficient method to migrate your PostgreSQL database.\n\nThe other options involve additional steps that are not necessary or may introduce unnecessary complexities and potential issues during the migration process.\n\nFor C and D:\n\nThe major issue with this option is the additional steps involving creating a recovery.conf file, stopping the source database, transferring write-ahead logs, and syncing with the running primary server. These steps are overly complex and can introduce unnecessary risks and potential complications during the migration process."
      },
      {
        "date": "2023-06-13T10:36:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/architecture/migrating-postgresql-to-gcp"
      },
      {
        "date": "2023-03-27T17:25:00.000Z",
        "voteCount": 3,
        "content": "C.\nFull exports are not possible offline. Eliminate A and B. Migrating to GCE means you can't use the Database Migration Service.  Note, Datastream CDC only supports MySQL and Oracle. To seed the PostgreSQL instance in GCE, a backup is needed created using pg_basebackup. An export won't cut it. That eliminates D and leaves C. Those 9 steps actually make sense."
      },
      {
        "date": "2023-03-23T21:25:00.000Z",
        "voteCount": 1,
        "content": "C, Its to take full backup not full export in this case."
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 127,
    "url": "https://www.examtopics.com/discussions/google/view/104142-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "You have deployed a Cloud SQL for SQL Server instance. In addition, you created a cross-region read replica for disaster recovery (DR) purposes. Your company requires you to maintain and monitor a recovery point objective (RPO) of less than 5 minutes. You need to verify that your cross-region read replica meets the allowed RPO. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud SQL instance monitoring.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Cloud Monitoring dashboard with available metrics from Cloud SQL.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud SQL logs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the SQL Server Always On Availability Group dashboard.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 12,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-03-27T17:29:00.000Z",
        "voteCount": 13,
        "content": "D.\nNote, you cannot create a read replica in Cloud SQL for SQL Server unless you use an Enterprise Edition. Which is also a requirement for configuring SQL Server AG. That's not a coincidence. That's how Cloud SQL for SQL Server creates SQL Server read replicas. To find out about the replication, use the AG Dashboard in SSMS. \nhttps://cloud.google.com/sql/docs/sqlserver/replication/manage-replicas#promote-replica"
      },
      {
        "date": "2024-09-27T21:23:00.000Z",
        "voteCount": 1,
        "content": "Always On Availability Groups are a feature of on-premises SQL Server or SQL Server running on VMs, not Cloud SQL"
      },
      {
        "date": "2024-04-30T05:39:00.000Z",
        "voteCount": 2,
        "content": "D- https://cloud.google.com/sql/docs/sqlserver/replication/manage-replicas#console_1"
      },
      {
        "date": "2024-03-26T02:40:00.000Z",
        "voteCount": 2,
        "content": "option D no doubt"
      },
      {
        "date": "2024-01-26T07:13:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/sql/docs/sqlserver/replication/manage-replicas#replication-status explains it."
      },
      {
        "date": "2023-11-27T10:02:00.000Z",
        "voteCount": 1,
        "content": "I thought it was B at first but apparently, we can use Cloud Monitoring to see all the metrics for the recovery an replication."
      },
      {
        "date": "2023-07-30T14:15:00.000Z",
        "voteCount": 3,
        "content": "D is the correct answer"
      },
      {
        "date": "2023-07-29T13:59:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/sql/docs/sqlserver/replication/manage-replicas#promote-replica"
      },
      {
        "date": "2023-07-20T10:01:00.000Z",
        "voteCount": 2,
        "content": "Vote for D!\nAgreed with dynamic_dba"
      },
      {
        "date": "2023-06-13T10:26:00.000Z",
        "voteCount": 2,
        "content": "https://medium.com/google-cloud/cloud-sql-recovering-from-regional-failure-in-10-minutes-or-less-mysql-fc055540a8f0"
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 128,
    "url": "https://www.examtopics.com/discussions/google/view/103734-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "You want to migrate an on-premises mission-critical PostgreSQL database to Cloud SQL. The database must be able to withstand a zonal failure with less than five minutes of downtime and still not lose any transactions. You want to follow Google-recommended practices for the migration. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTake nightly snapshots of the primary database instance, and restore them in a secondary zone.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBuild a change data capture (CDC) pipeline to read transactions from the primary instance, and replicate them to a secondary instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a read replica in another region, and promote the read replica if a failure occurs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable high availability (HA) for the database to make it regional.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-07-17T12:15:00.000Z",
        "voteCount": 1,
        "content": "D is the clear winner."
      },
      {
        "date": "2023-03-27T17:31:00.000Z",
        "voteCount": 4,
        "content": "D.\nMission critical means make the instance HA. Nothing else makes sense apart from D."
      },
      {
        "date": "2023-03-23T21:28:00.000Z",
        "voteCount": 1,
        "content": "D, zonal failure &gt; enable HA to recover"
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 129,
    "url": "https://www.examtopics.com/discussions/google/view/107785-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "You are migrating an on-premises application to Compute Engine and Cloud SQL. The application VMs will live in their own project, separate from the Cloud SQL instances which have their own project. What should you do to configure the networks?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new VPC network in each project, and use VPC Network Peering to connect the two together.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Shared VPC that both the application VMs and Cloud SQL instances will use.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the default networks, and leverage Cloud VPN to connect the two together.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPlace both the application VMs and the Cloud SQL instances in the default network of each project."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-04-28T03:55:00.000Z",
        "voteCount": 7,
        "content": "I believe it's option B as shared VPC can be used to connect multiple projects ."
      },
      {
        "date": "2024-06-17T13:31:00.000Z",
        "voteCount": 1,
        "content": "B.\nMore info on this matter:\nhttps://cloud.google.com/network-connectivity/docs/interconnect/how-to/enabling-multiple-networks-access-same-attachment"
      },
      {
        "date": "2024-05-24T04:08:00.000Z",
        "voteCount": 1,
        "content": "Why not A?"
      },
      {
        "date": "2024-05-02T01:20:00.000Z",
        "voteCount": 1,
        "content": "it's B"
      },
      {
        "date": "2024-03-26T02:36:00.000Z",
        "voteCount": 1,
        "content": "Option B"
      },
      {
        "date": "2023-06-13T09:35:00.000Z",
        "voteCount": 2,
        "content": "https://groups.google.com/g/google-cloud-sql-discuss/c/M5G5_HPXytY?pli=1"
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 130,
    "url": "https://www.examtopics.com/discussions/google/view/104143-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "Your DevOps team is using Terraform to deploy applications and Cloud SQL databases. After every new application change is rolled out, the environment is torn down and recreated, and the persistent database layer is lost. You need to prevent the database from being dropped. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet Terraform deletion_protection to true.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRerun terraform apply.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a read replica.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse point-in-time-recovery (PITR) to recover the database."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-03-27T17:34:00.000Z",
        "voteCount": 9,
        "content": "A.\nIt makes sense that a Terraform problem would use a Terra form  solution. From Google's documentation, \"For stateful resources, such as databases, ensure that deletion protection is enabled. The syntax is:\nlifecycle {\n\t\tprevent_destroy = true\n}\nhttps://cloud.google.com/docs/terraform/best-practices-for-terraform#stateful-resources"
      },
      {
        "date": "2024-05-02T01:20:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/docs/terraform/best-practices-for-terraform#stateful-resources"
      },
      {
        "date": "2023-09-02T15:02:00.000Z",
        "voteCount": 1,
        "content": "For stateful resources, such as databases, ensure that deletion protection is enabled."
      },
      {
        "date": "2023-07-20T10:07:00.000Z",
        "voteCount": 2,
        "content": "agreed with dynamic_dba"
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 131,
    "url": "https://www.examtopics.com/discussions/google/view/103700-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "Your company's mission-critical, globally available application is supported by a Cloud Spanner database. Experienced users of the application have read and write access to the database, but new users are assigned read-only access to the database. You need to assign the appropriate Cloud Spanner Identity and Access Management (IAM) role to new users being onboarded soon. What roles should you set up?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\troles/spanner.databaseReader\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\troles/spanner.databaseUser",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\troles/spanner.viewer",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\troles/spanner.backupWriter"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-03-27T17:36:00.000Z",
        "voteCount": 7,
        "content": "A.\nNothing to do with backups. Eliminate D. databaseUser allows read and write. Eliminate B. viewer is at the instance level and does not allow database reads. Eliminate C. Leaves A."
      },
      {
        "date": "2023-03-23T13:43:00.000Z",
        "voteCount": 5,
        "content": "See this link https://cloud.google.com/spanner/docs/iam"
      },
      {
        "date": "2024-05-02T01:23:00.000Z",
        "voteCount": 2,
        "content": "A:\nhttps://cloud.google.com/spanner/docs/iam#roles\n\nviewer can only see the databases not read data"
      },
      {
        "date": "2023-03-23T13:37:00.000Z",
        "voteCount": 5,
        "content": "The correct answer is A! With Spanner.viewer user cannot read database, but he can visualize only database List. https://cloud.google.com/spanner/docs/iam?hl=it"
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 132,
    "url": "https://www.examtopics.com/discussions/google/view/103701-exam-professional-cloud-database-engineer-topic-1-question/",
    "body": "Your company is shutting down their data center and migrating several MySQL and PostgreSQL databases to Google Cloud. Your database operations team is severely constrained by ongoing production releases and the lack of capacity for additional on-premises backups. You want to ensure that the scheduled migrations happen with minimal downtime and that the Google Cloud databases stay in sync with the on-premises data changes until the applications can cut over.<br><br>What should you do? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an external read replica to migrate the databases to Cloud SQL.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a read replica to migrate the databases to Cloud SQL.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Database Migration Service to migrate the databases to Cloud SQL.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a cross-region read replica to migrate the databases to Cloud SQL.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse replication from an external server to migrate the databases to Cloud SQL.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "CE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "CE",
        "count": 6,
        "isMostVoted": true
      },
      {
        "answer": "BE",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-03-27T17:38:00.000Z",
        "voteCount": 9,
        "content": "C, E.\nClassic use case for the Database Migration Service (C). E is effectively doing what the DMS does as well."
      },
      {
        "date": "2023-03-23T13:39:00.000Z",
        "voteCount": 5,
        "content": "Sorry CE"
      },
      {
        "date": "2024-04-01T00:14:00.000Z",
        "voteCount": 1,
        "content": "goodluck to all"
      },
      {
        "date": "2024-02-09T05:30:00.000Z",
        "voteCount": 1,
        "content": "This should be CE"
      },
      {
        "date": "2023-12-09T04:24:00.000Z",
        "voteCount": 3,
        "content": "where are questions 133-136 ???"
      },
      {
        "date": "2023-05-22T07:18:00.000Z",
        "voteCount": 5,
        "content": "CE correct answers!"
      },
      {
        "date": "2023-03-23T13:39:00.000Z",
        "voteCount": 1,
        "content": "BE correct answers!"
      }
    ],
    "examNameCode": "professional-cloud-database-engineer",
    "topicNumber": "1"
  }
]