[
  {
    "topic": 1,
    "index": 1,
    "url": "https://www.examtopics.com/discussions/google/view/24750-exam-professional-cloud-developer-topic-1-question-1/",
    "body": "You want to upload files from an on-premises virtual machine to Google Cloud Storage as part of a data migration. These files will be consumed by Cloud<br>DataProc Hadoop cluster in a GCP environment.<br>Which command should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tgsutil cp [LOCAL_OBJECT] gs://[DESTINATION_BUCKET_NAME]/\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tgcloud cp [LOCAL_OBJECT] gs://[DESTINATION_BUCKET_NAME]/",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\thadoop fs cp [LOCAL_OBJECT] gs://[DESTINATION_BUCKET_NAME]/",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tgcloud dataproc cp [LOCAL_OBJECT] gs://[DESTINATION_BUCKET_NAME]/"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 6,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-07-24T10:30:00.000Z",
        "voteCount": 5,
        "content": "Took the exam 2 weeks and got pass, however, NOT A SINGLE question is from here! Although it is still good for practice purpose, DO NOT just remember these questions to take the exam.\n\n-- My comment above is still waiting approval after two weeks, it seems someone doesn't want it to be published."
      },
      {
        "date": "2021-08-01T06:17:00.000Z",
        "voteCount": 2,
        "content": "why do you lie"
      },
      {
        "date": "2021-08-01T21:09:00.000Z",
        "voteCount": 1,
        "content": "My comment  is still waiting approval too."
      },
      {
        "date": "2024-10-12T05:03:00.000Z",
        "voteCount": 1,
        "content": "As updates to the Google Cloud SDK, the `gcloud` tool now includes a `storage` command group that allows you to interact with Google Cloud Storage (GCS) similarly to how you would with `gsutil` command"
      },
      {
        "date": "2024-10-10T07:42:00.000Z",
        "voteCount": 1,
        "content": "A. gsutil cp [LOCAL_OBJECT] gs://[DESTINATION_BUCKET_NAME]/\nThis command is used to copy files from local storage to Google Cloud Storage. It\u2019s the correct option for uploading data as part of a migration process that will be consumed by Cloud Dataproc."
      },
      {
        "date": "2023-11-13T18:40:00.000Z",
        "voteCount": 1,
        "content": "A is correct answer"
      },
      {
        "date": "2023-11-07T12:44:00.000Z",
        "voteCount": 1,
        "content": "Answer: A \nCloud Storage  =&gt; gsutil. Doesn't matter how the files are going to be consumed, the task is to upload them."
      },
      {
        "date": "2023-09-19T07:34:00.000Z",
        "voteCount": 1,
        "content": "I would go with A."
      },
      {
        "date": "2023-08-28T11:32:00.000Z",
        "voteCount": 1,
        "content": "for sure A"
      },
      {
        "date": "2023-05-20T12:00:00.000Z",
        "voteCount": 2,
        "content": "Anyone took exam recently, what percent of questions came from examtopics?"
      },
      {
        "date": "2023-03-06T02:12:00.000Z",
        "voteCount": 1,
        "content": "Anyone took exam recently, what percent of questions came from examtopics?"
      },
      {
        "date": "2023-02-17T19:00:00.000Z",
        "voteCount": 1,
        "content": "gsutil cp [LOCAL_OBJECT] gs://[DESTINATION_BUCKET_NAME]/"
      },
      {
        "date": "2022-12-18T04:17:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2022-08-19T04:36:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2021-11-04T16:06:00.000Z",
        "voteCount": 2,
        "content": "The exam is totally different from the one presented in the dump, it proves TOTALLY DIFFERENT.\nPlease update the questions with the original exam!!"
      },
      {
        "date": "2021-06-29T23:43:00.000Z",
        "voteCount": 2,
        "content": "There was a lot of GKE especially with istio."
      },
      {
        "date": "2021-06-23T23:18:00.000Z",
        "voteCount": 1,
        "content": "cloud storage command: gsutil"
      },
      {
        "date": "2021-06-07T09:12:00.000Z",
        "voteCount": 1,
        "content": "is this question set updated ?"
      },
      {
        "date": "2021-06-04T08:19:00.000Z",
        "voteCount": 2,
        "content": "Took the exam two days ago, only 4 of the questions from this set appeared in the actual exam, remaining 56 were new questions. Even after the June 2nd update, I don't see any of the new questions here."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 2,
    "url": "https://www.examtopics.com/discussions/google/view/25708-exam-professional-cloud-developer-topic-1-question-2/",
    "body": "You migrated your applications to Google Cloud Platform and kept your existing monitoring platform. You now find that your notification system is too slow for time critical problems.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReplace your entire monitoring platform with Stackdriver.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall the Stackdriver agents on your Compute Engine instances.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Stackdriver to capture and alert on logs, then ship them to your existing platform.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate some traffic back to your old platform and perform AB testing on the two platforms concurrently."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 10,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 4,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-10-16T10:44:00.000Z",
        "voteCount": 18,
        "content": "C\n\nThe task does not indicate that we should get rid of the old software. The pain point is slowness for time critical problems only. Thus we would use Stackdriver for the time critical alerts and still utilize the old platform for further analysis/storing of logs or whatever its business case is."
      },
      {
        "date": "2024-03-07T06:23:00.000Z",
        "voteCount": 1,
        "content": "the most balanced and least disruptive approach would likely be option C, \"Use Stackdriver to capture and alert on logs, then ship them to your existing platform\". This method allows for a seamless integration of Stackdriver's real-time alerting capabilities into your existing monitoring workflow without the need for a complete overhaul of your system."
      },
      {
        "date": "2023-11-13T18:41:00.000Z",
        "voteCount": 1,
        "content": "B is correct answer"
      },
      {
        "date": "2023-11-07T12:50:00.000Z",
        "voteCount": 3,
        "content": "Option C.\n\nNotifications from on-prem monitoring system are too slow &amp; applications are in GCP now =&gt; Stackdriver alerts on logs.\n\nThere's no mentioning whether the apps have been migrated to GCE, GKE, App Engine or Cloud Run, so \"Compute Engine instances\" come from an assumption."
      },
      {
        "date": "2023-09-19T07:36:00.000Z",
        "voteCount": 1,
        "content": "I would go with C."
      },
      {
        "date": "2023-08-20T15:51:00.000Z",
        "voteCount": 1,
        "content": "Option B , Install stack driver agents on the compute instances is the correct answer .\nusing stackdriver and shipping it to existing platform will have some delay"
      },
      {
        "date": "2023-05-03T06:44:00.000Z",
        "voteCount": 3,
        "content": "C\nyou have problems with notifications.\nC option allows you to use stackdriver to send alerts immediately and straight away after sends all this data to your on-prem monitoring platform"
      },
      {
        "date": "2023-02-02T01:31:00.000Z",
        "voteCount": 1,
        "content": "B is the correct answer. \nhttps://cloud.google.com/monitoring/agent/monitoring/installation"
      },
      {
        "date": "2023-01-02T12:37:00.000Z",
        "voteCount": 3,
        "content": "Think twice. You have working an expensive monitoring system i.e Splunk and you have the problem with unacceptable delay time between incident and notification. You need to fix this problem, not doing a revolution (changing monitoring system). You can leverage GCP Monitoring with alerting system which is out-of-the-box with no huge effort, because if you want or not logs are in cloud logging. Simply implement alerts and push logs to Splunk. Simples."
      },
      {
        "date": "2022-08-19T04:37:00.000Z",
        "voteCount": 2,
        "content": "A is correct"
      },
      {
        "date": "2022-05-25T09:36:00.000Z",
        "voteCount": 1,
        "content": "It's the rightest answer. C cannot work without Agent and there's not sense to send log to old monitoring"
      },
      {
        "date": "2022-05-25T09:32:00.000Z",
        "voteCount": 2,
        "content": "It's A 'cos you cannot use Stackdriver if you don't install Stackdriver agent on your compute engine."
      },
      {
        "date": "2022-06-18T04:48:00.000Z",
        "voteCount": 1,
        "content": "Hi\nDid you find any questions from here?"
      },
      {
        "date": "2022-04-04T01:34:00.000Z",
        "voteCount": 2,
        "content": "Community choice is C"
      },
      {
        "date": "2022-02-17T20:42:00.000Z",
        "voteCount": 2,
        "content": "I think it should be A than C \nApps have been migrated and why would you invest on C to send data back to existing system instead fix old system using direct connect or something"
      },
      {
        "date": "2022-02-17T20:45:00.000Z",
        "voteCount": 1,
        "content": "Nvm I think it should be C"
      },
      {
        "date": "2021-11-26T02:50:00.000Z",
        "voteCount": 1,
        "content": "The answer is C. The point is notification problem with low performance, not monitoring."
      },
      {
        "date": "2022-05-25T09:32:00.000Z",
        "voteCount": 1,
        "content": "Why can C work without stackdriver agent?"
      },
      {
        "date": "2021-07-20T19:17:00.000Z",
        "voteCount": 3,
        "content": "Agree C"
      },
      {
        "date": "2021-06-23T23:22:00.000Z",
        "voteCount": 3,
        "content": "This question is a little tricky.\nfirst, we can rule out B, D\nB is just the first step to collect metric, not involve how to handle metric in monitoring and alerting; D is about CI/CD.\nFor A and C, the question doesn't mention why the notification is slow, is it because of latency in collecting metric, or it's because transferring data from GCP to on-prem, for me, no matter what, I think only A can solve whole problem."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 3,
    "url": "https://www.examtopics.com/discussions/google/view/21310-exam-professional-cloud-developer-topic-1-question-3/",
    "body": "You are planning to migrate a MySQL database to the managed Cloud SQL database for Google Cloud. You have Compute Engine virtual machine instances that will connect with this Cloud SQL instance. You do not want to whitelist IPs for the Compute Engine instances to be able to access Cloud SQL.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable private IP for the Cloud SQL instance.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWhitelist a project to access Cloud SQL, and add Compute Engine instances in the whitelisted project.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a role in Cloud SQL that allows access to the database from external instances, and assign the Compute Engine instances to that role.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a CloudSQL instance on one project. Create Compute engine instances in a different project. Create a VPN between these two projects to allow internal access to CloudSQL."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-05-25T10:13:00.000Z",
        "voteCount": 26,
        "content": "The proposed answer seems incorrect, as according to the question application running access to Cloud SQL is run on the Compute Engine and the are no roles in Cloud SQL itself to manage Instance-level access control. According to https://cloud.google.com/sql/docs/mysql/connect-compute-engine there are 3 possible ways to connect from Compute Engine: 'Private IP', 'Public IP', 'Cloud SQL Proxy'.\nThere is no 'Cloud SQL Proxy' option in answers, 'Public IP' requires IP whitelisting what is unacceptable according to the question, so the only valid answer is 'Private IP'"
      },
      {
        "date": "2020-11-19T10:30:00.000Z",
        "voteCount": 9,
        "content": "the answer is A."
      },
      {
        "date": "2024-09-27T02:03:00.000Z",
        "voteCount": 1,
        "content": "Enabling private IP allows the Compute Engine instances and the Cloud SQL instance to communicate over a private, internal network within Google Cloud Platform (GCP), rather than relying on external IP whitelisting."
      },
      {
        "date": "2024-09-27T02:03:00.000Z",
        "voteCount": 1,
        "content": "Selecting Answer A, \"Enable private IP for the Cloud SQL instance,\" is the most efficient and secure method to allow your Google Compute Engine virtual machine instances to connect with a managed Cloud SQL database without the need to whitelist IP addresses. This approach involves configuring the Cloud SQL instance to use a private IP address that is accessible within your Google Cloud Platform (GCP) network. This setup ensures that your Compute Engine instances can securely connect to the Cloud SQL database over Google's private network, providing a high level of security as the database isn't exposed to the public internet. It simplifies the network configuration and avoids the management overhead and security risks associated with maintaining an IP whitelist."
      },
      {
        "date": "2023-01-01T01:43:00.000Z",
        "voteCount": 3,
        "content": "These are the options you can use to connect a Cloud SQL instance to a Compute Engine instance:\n\n1. Private IP: You can use the private IP of the Cloud SQL instance to connect to it from the Compute Engine instance. This requires that the Cloud SQL instance and the Compute Engine instance are in the same VPC network.\n\n2. Public IP: You can use the public IP of the Cloud SQL instance to connect to it from the Compute Engine instance. This requires that the Cloud SQL instance is configured to allow connections from the public IP of the Compute Engine instance."
      },
      {
        "date": "2023-01-01T01:44:00.000Z",
        "voteCount": 3,
        "content": "3. Cloud SQL Auth proxy: The Cloud SQL Auth proxy is a tool that allows you to connect to Cloud SQL instances from external applications. To use the Cloud SQL Auth proxy, you need to install it on the Compute Engine instance and use it to establish a connection to the Cloud SQL instance.\n\n4. Cloud SQL Auth proxy Docker image: The Cloud SQL Auth proxy Docker image is a Docker image that contains the Cloud SQL Auth proxy. You can use this Docker image to run the Cloud SQL Auth proxy in a Docker container on the Compute Engine instance. This allows you to easily deploy and manage the Cloud SQL Auth proxy on the Compute Engine instance."
      },
      {
        "date": "2023-01-01T01:46:00.000Z",
        "voteCount": 1,
        "content": "And off course, you can enable private IP on a Cloud SQL instance on Google Cloud Platform (GCP). Private IP allows you to access a Cloud SQL instance from within the same VPC network, without the need to use a public IP or whitelist IP addresses.\n\nTo enable private IP on a Cloud SQL instance, you need to do the following:\n\nCreate a VPC network: First, you need to create a VPC network in which the Cloud SQL instance and the Compute Engine instance will be placed.\n\nCreate a Cloud SQL instance: Next, you need to create a Cloud SQL instance and specify the VPC network that you created in step 1 as the network for the Cloud SQL instance.\n\nEnable private IP: Finally, you can enable private IP on the Cloud SQL instance by going to the \"Networking\" tab in the Cloud SQL instance's configuration page and selecting the \"Private IP\" option.\n\nOnce you have enabled private IP on the Cloud SQL instance, you can access it from the Compute Engine instance using the private IP of the Cloud SQL instance.\n\n\nAnswer is A"
      },
      {
        "date": "2022-08-19T23:23:00.000Z",
        "voteCount": 2,
        "content": "A is correct"
      },
      {
        "date": "2022-05-25T09:40:00.000Z",
        "voteCount": 2,
        "content": "The question is about \"connection\". Role assignment gives a set of permission to compute engine but doesn't allow connection."
      },
      {
        "date": "2022-05-13T07:53:00.000Z",
        "voteCount": 1,
        "content": "The best way would be to connect the compute engine instance to cloud sql with Cloud SQL Auth Proxy (https://cloud.google.com/sql/docs/mysql/roles-and-permissions#proxy-roles-permissions). But the way that C is phrased makes me think that A is correct\n\n\"...you can use the default Compute Engine service account associated with the Compute Engine instance. As with all accounts connecting to a Cloud SQL instance, the service account must have the Cloud SQL &gt; Client role.\""
      },
      {
        "date": "2022-02-08T12:53:00.000Z",
        "voteCount": 1,
        "content": "Right answer is A. Agree with emmet."
      },
      {
        "date": "2022-01-08T00:57:00.000Z",
        "voteCount": 1,
        "content": "Answer should be A"
      },
      {
        "date": "2021-11-26T02:46:00.000Z",
        "voteCount": 2,
        "content": "The answer is A, private ip allows the connection between gce and cloudsql, for other hand isn't possible access to from gce to cloudsql though roles without public ip with firewall rules, private ip o cloudproxy"
      },
      {
        "date": "2021-09-18T06:40:00.000Z",
        "voteCount": 1,
        "content": "Personally agree the option C, using a private IP will allow all compute engine instances to access the database. What if not all of the compute instances within the same VPC are allowed?"
      },
      {
        "date": "2022-05-25T09:39:00.000Z",
        "voteCount": 1,
        "content": "But the question is about connection. If you assign a role to compute engine, it'll have the permission to use Cloud SQL but couldn't allow to connect to it."
      },
      {
        "date": "2021-07-20T19:20:00.000Z",
        "voteCount": 1,
        "content": "agree C"
      },
      {
        "date": "2021-07-20T19:21:00.000Z",
        "voteCount": 1,
        "content": "sorry the answer is A"
      },
      {
        "date": "2021-06-23T23:22:00.000Z",
        "voteCount": 1,
        "content": "the answer is A."
      },
      {
        "date": "2021-06-19T17:52:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/sql/docs/mysql/connect-compute-engine#connect-gce-private-ip\n\nAnswer is A given the options presented"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 4,
    "url": "https://www.examtopics.com/discussions/google/view/40833-exam-professional-cloud-developer-topic-1-question-4/",
    "body": "You have deployed an HTTP(s) Load Balancer with the gcloud commands shown below.<br><img src=\"/assets/media/exam-media/04137/0000300001.jpg\" class=\"in-exam-image\"><br>Health checks to port 80 on the Compute Engine virtual machine instance are failing and no traffic is sent to your instances. You want to resolve the problem.<br>Which commands should you run?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tgcloud compute instances add-access-config ${NAME}-backend-instance-1",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tgcloud compute instances add-tags ${NAME}-backend-instance-1 --tags http-server",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tgcloud compute firewall-rules create allow-lb --network load-balancer --allow tcp --source-ranges 130.211.0.0/22,35.191.0.0/16 --direction INGRESS\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tgcloud compute firewall-rules create allow-lb --network load-balancer --allow tcp --destination-ranges 130.211.0.0/22,35.191.0.0/16 --direction EGRESS"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 3,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-02-27T05:37:00.000Z",
        "voteCount": 10,
        "content": "C\n\nthe source IP ranges for health checks (including legacy health checks if used for HTTP(S) Load Balancing) are:\n\n35.191.0.0/16\n130.211.0.0/22\n\nFurthermore it should be direction INGRESS since the health-check (ping) is coming into the load balancer/instance.\n\nsource: https://cloud.google.com/load-balancing/docs/health-checks"
      },
      {
        "date": "2021-06-19T20:45:00.000Z",
        "voteCount": 1,
        "content": "Yup I would go for C based on this"
      },
      {
        "date": "2024-03-07T06:44:00.000Z",
        "voteCount": 1,
        "content": "Option C is the correct choice because it addresses the issue of health check failures for the Compute Engine instances behind the HTTP(s) Load Balancer. By creating an ingress firewall rule, this command allows traffic from the load balancer\u2019s source IP ranges to reach the instances on the specified network. These source IP ranges (130.211.0.0/22 and 35.191.0.0/16) are used by Google Cloud load balancers for health checking. Without this rule, the health checks would fail because the load balancer could not communicate with the backend instances to verify their status, resulting in no traffic being routed to those instances. By implementing this firewall rule, you ensure that the health check traffic is permitted, which should resolve the traffic routing issue and allow the load balancer to function correctly."
      },
      {
        "date": "2023-09-19T07:38:00.000Z",
        "voteCount": 1,
        "content": "I would go with C."
      },
      {
        "date": "2023-01-01T02:50:00.000Z",
        "voteCount": 2,
        "content": "To resolve the problem, you should run the following command:\n\nCopy code\ngcloud compute firewall-rules create allow-lb --network load-balancer --allow tcp --source-ranges 130.211.0.0/22,35.191.0.0/16 --direction INGRESS\nThis will create a firewall rule that allows incoming TCP traffic from the specified IP ranges to the Load Balancer network. This should allow traffic to reach the instance group and the instances it contains.\n\nOption A will not help because it is used to add an external IP address to an instance, which is not necessary for the Load Balancer to work. Option B is not necessary because it is used to apply metadata to an instance, which is not related to the Load Balancer. Option D is not correct because it allows outgoing traffic from the Load Balancer network, which is not necessary for the Load Balancer to work.\n\nI hope this helps! Let me know if you have any other questions."
      },
      {
        "date": "2022-08-19T23:24:00.000Z",
        "voteCount": 1,
        "content": "C  is correct"
      },
      {
        "date": "2021-07-20T19:26:00.000Z",
        "voteCount": 1,
        "content": "C\ningress not egress"
      },
      {
        "date": "2021-01-17T12:08:00.000Z",
        "voteCount": 2,
        "content": "I would say B with predefined http-server tag on instance."
      },
      {
        "date": "2021-06-23T23:26:00.000Z",
        "voteCount": 2,
        "content": "even if you set tag, but if you don't set firewall rule based on tag, it still can't create connection from health check probe to backend service"
      },
      {
        "date": "2022-11-26T12:43:00.000Z",
        "voteCount": 1,
        "content": "If you check Http Server on vm creation, a FW rules with network tag \"http-server\" is created, but it didnt work the other way around"
      },
      {
        "date": "2020-12-26T17:23:00.000Z",
        "voteCount": 3,
        "content": "I choose C."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 5,
    "url": "https://www.examtopics.com/discussions/google/view/25711-exam-professional-cloud-developer-topic-1-question-5/",
    "body": "Your website is deployed on Compute Engine. Your marketing team wants to test conversion rates between 3 different website designs.<br>Which approach should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the website on App Engine and use traffic splitting.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the website on App Engine as three separate services.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the website on Cloud Functions and use traffic splitting.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the website on Cloud Functions as three separate functions."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 10,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-07-14T05:44:00.000Z",
        "voteCount": 11,
        "content": "Ans:A\n"
      },
      {
        "date": "2024-03-07T06:48:00.000Z",
        "voteCount": 1,
        "content": "option A is the best approach because it allows you to use App Engine's built-in traffic splitting feature to distribute traffic across different versions of your site. This feature is designed for scenarios exactly like A/B testing, making it possible to seamlessly and efficiently test conversion rates across multiple website designs."
      },
      {
        "date": "2023-09-19T07:39:00.000Z",
        "voteCount": 1,
        "content": "A is Correct."
      },
      {
        "date": "2022-09-01T03:45:00.000Z",
        "voteCount": 2,
        "content": "you have a URL for each version deployed in the same service."
      },
      {
        "date": "2022-08-19T23:25:00.000Z",
        "voteCount": 2,
        "content": "A  is correct"
      },
      {
        "date": "2022-08-19T16:46:00.000Z",
        "voteCount": 1,
        "content": "A of course"
      },
      {
        "date": "2022-06-13T03:01:00.000Z",
        "voteCount": 2,
        "content": "i think traffic splitting is more suitable"
      },
      {
        "date": "2022-05-25T09:44:00.000Z",
        "voteCount": 1,
        "content": "I vote A but It could be wrong 'cos the question is not detailed. It doesn't ask to let url remain the same. So B could be a good answer: in this way you have 3 url different without any need to manage splitting"
      },
      {
        "date": "2022-04-18T21:51:00.000Z",
        "voteCount": 2,
        "content": "A\nA is correct because it allows routing traffic to a single domain and split traffic based on IP or Cookie. \nB is not correct because the domain name will change based on the service. \nsource: Google Sample Questions"
      },
      {
        "date": "2022-02-22T18:53:00.000Z",
        "voteCount": 2,
        "content": "If you want to test three different websites and compare how would you use traffic splitting (33%)"
      },
      {
        "date": "2022-02-22T18:56:00.000Z",
        "voteCount": 2,
        "content": "I can safely remove compare keyword from my comment\u2026 so it will be back to A"
      },
      {
        "date": "2021-07-20T19:31:00.000Z",
        "voteCount": 3,
        "content": "A is correct"
      },
      {
        "date": "2021-06-19T23:48:00.000Z",
        "voteCount": 4,
        "content": "This is A for sure"
      },
      {
        "date": "2021-05-19T06:51:00.000Z",
        "voteCount": 4,
        "content": "this question is also asked in the example questions.\nCorrect answer is A, because traffic splitting is exactly for testing purpose and you can choose to redirect 100% percent traffic to a specific version."
      },
      {
        "date": "2021-02-25T09:53:00.000Z",
        "voteCount": 4,
        "content": "3 different website designs but the same service. I think is A"
      },
      {
        "date": "2021-06-23T23:33:00.000Z",
        "voteCount": 2,
        "content": "yes, the question is to test 'if the 3 website design is the same service?', absolutely it's."
      },
      {
        "date": "2021-01-17T12:11:00.000Z",
        "voteCount": 3,
        "content": "Answer is B with 3 appengine services. (3 urls to access the version you want).\nWith traffic splitting on versions, you can't chosse exactly the version you want..."
      },
      {
        "date": "2022-04-03T11:51:00.000Z",
        "voteCount": 1,
        "content": "No, you don`t need to have different urls. GCP will automatically split 33% of traffic to each version. You should have just 3 releases with different front end markup of one service. Answer A is correct"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 6,
    "url": "https://www.examtopics.com/discussions/google/view/70414-exam-professional-cloud-developer-topic-1-question-6/",
    "body": "You need to copy directory local-scripts and all of its contents from your local workstation to a Compute Engine virtual machine instance.<br>Which command should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tgsutil cp --project \u05d2\u20acmy-gcp-project\u05d2\u20ac -r ~/local-scripts/ gcp-instance-name:~/server-scripts/ --zone \u05d2\u20acus-east1-b\u05d2\u20ac",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tgsutil cp --project \u05d2\u20acmy-gcp-project\u05d2\u20ac -R ~/local-scripts/ gcp-instance-name:~/server-scripts/ --zone \u05d2\u20acus-east1-b\u05d2\u20ac",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tgcloud compute scp --project \u05d2\u20acmy-gcp-project\u05d2\u20ac --recurse ~/local-scripts/ gcp-instance-name:~/server-scripts/ --zone \u05d2\u20acus-east1-b\u05d2\u20ac\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tgcloud compute mv --project \u05d2\u20acmy-gcp-project\u05d2\u20ac --recurse ~/local-scripts/ gcp-instance-name:~/server-scripts/ --zone \u05d2\u20acus-east1-b\u05d2\u20ac"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-01-22T10:24:00.000Z",
        "voteCount": 5,
        "content": "Agreed C is correct option"
      },
      {
        "date": "2023-09-19T07:39:00.000Z",
        "voteCount": 1,
        "content": "C is correct."
      },
      {
        "date": "2022-08-19T23:25:00.000Z",
        "voteCount": 1,
        "content": "C  is correct"
      },
      {
        "date": "2022-08-19T16:46:00.000Z",
        "voteCount": 1,
        "content": "C because of scp"
      },
      {
        "date": "2022-05-25T09:46:00.000Z",
        "voteCount": 1,
        "content": "C is the correct answer"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 7,
    "url": "https://www.examtopics.com/discussions/google/view/36175-exam-professional-cloud-developer-topic-1-question-7/",
    "body": "You are deploying your application to a Compute Engine virtual machine instance with the Stackdriver Monitoring Agent installed. Your application is a unix process on the instance. You want to be alerted if the unix process has not run for at least 5 minutes. You are not able to change the application to generate metrics or logs.<br>Which alert condition should you configure?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUptime check",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tProcess health\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMetric absence",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMetric threshold"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 3,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-02-27T05:41:00.000Z",
        "voteCount": 5,
        "content": "B\n\nProcess-health policy\nA process-health policy can notify you if the number of processes that match a pattern crosses a threshold. This can be used to tell you, for example, that a process has stopped running.\n\n"
      },
      {
        "date": "2024-09-08T19:05:00.000Z",
        "voteCount": 1,
        "content": "Metric absence allows you to trigger an alert if no metrics (e.g., CPU usage, memory usage, etc.) related to the process are received for a specific period, such as 5 minutes"
      },
      {
        "date": "2024-03-07T07:01:00.000Z",
        "voteCount": 1,
        "content": "To be alerted if your Unix process has not run for at least 5 minutes, you should configure a **metric absence** alert condition. Since you cannot modify the application to generate metrics or logs, this approach allows you to monitor the absence of data for a specific duration. When the Unix process stops running, the metric data will be absent, triggering the alert.\n\nTherefore, the correct answer is **C. Metric absence**. This type of alerting policy will help you stay informed about any unexpected interruptions in your process execution."
      },
      {
        "date": "2023-09-19T07:40:00.000Z",
        "voteCount": 1,
        "content": "B is correct."
      },
      {
        "date": "2023-07-16T23:54:00.000Z",
        "voteCount": 1,
        "content": "Complete explanations for correct and incorrect answers can be seen on https://examlab.co/google/google-cloud-professional-cloud-developer"
      },
      {
        "date": "2023-05-03T09:24:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/monitoring/alerts/policies-in-json#json-process-health"
      },
      {
        "date": "2022-08-19T23:25:00.000Z",
        "voteCount": 1,
        "content": "B  is correct"
      },
      {
        "date": "2021-07-20T19:43:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2021-06-19T23:58:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/monitoring/uptime-checks:\n\"An uptime check is a request sent to a resource to see if it responds\"\n\nA is wrong\n\nMetric absence and threshold don't make sense\n\nProcess health is correct for sure so answer is B"
      },
      {
        "date": "2020-11-05T10:35:00.000Z",
        "voteCount": 2,
        "content": "B is correct answer\nhttps://cloud.google.com/monitoring/alerts/types-of-conditions#metric-threshold"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 8,
    "url": "https://www.examtopics.com/discussions/google/view/36176-exam-professional-cloud-developer-topic-1-question-8/",
    "body": "You have two tables in an ANSI-SQL compliant database with identical columns that you need to quickly combine into a single table, removing duplicate rows from the result set.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the JOIN operator in SQL to combine the tables.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse nested WITH statements to combine the tables.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the UNION operator in SQL to combine the tables.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the UNION ALL operator in SQL to combine the tables."
    ],
    "answer": "C",
    "answerDescription": "Reference:<br>https://www.techonthenet.com/sql/union_all.php",
    "votes": [
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-11-05T10:37:00.000Z",
        "voteCount": 7,
        "content": "C is correct answer here.\n\nThe only difference between Union and Union All is that Union All will not removes duplicate rows or records, instead, it just selects all the rows from all the tables which meets the conditions of your specifics query and combines them into the result table."
      },
      {
        "date": "2023-09-19T07:40:00.000Z",
        "voteCount": 1,
        "content": "C is correct."
      },
      {
        "date": "2022-12-11T23:28:00.000Z",
        "voteCount": 3,
        "content": "UNION removes duplicate rows.\nUNION ALL does not remove duplicate rows."
      },
      {
        "date": "2022-08-19T23:26:00.000Z",
        "voteCount": 1,
        "content": "C  is correct"
      },
      {
        "date": "2021-07-20T19:49:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2021-06-20T00:06:00.000Z",
        "voteCount": 2,
        "content": "If you know SQL well enough, C is the answer"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 9,
    "url": "https://www.examtopics.com/discussions/google/view/17899-exam-professional-cloud-developer-topic-1-question-9/",
    "body": "You have an application deployed in production. When a new version is deployed, some issues don't arise until the application receives traffic from users in production. You want to reduce both the impact and the number of users affected.<br>Which deployment strategy should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBlue/green deployment",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCanary deployment\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRolling deployment",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRecreate deployment"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 13,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-10-16T01:17:00.000Z",
        "voteCount": 1,
        "content": "B Canary deployment allows you to release a new version of your application to a small subset of users before rolling it out to everyone. This strategy helps you identify potential issues with the new version while minimizing the impact on users, as only a small portion of traffic is directed to the new version initially."
      },
      {
        "date": "2024-06-12T05:48:00.000Z",
        "voteCount": 1,
        "content": "this is A, Canary is a Testing strategy not a deployment strategy."
      },
      {
        "date": "2024-03-07T07:12:00.000Z",
        "voteCount": 1,
        "content": "B. Canary deployment: In this strategy, the new version of the application (the \"canary\") is rolled out to a small subset of users before it is made available to the entire user base. This allows you to monitor the performance and stability of the new version in the real-world production environment with actual traffic, but only affects a small group of users. If issues arise, the canary deployment can be rolled back with minimal impact."
      },
      {
        "date": "2024-02-13T00:36:00.000Z",
        "voteCount": 1,
        "content": "That's exactly what Canary Deployment is for."
      },
      {
        "date": "2023-11-08T13:57:00.000Z",
        "voteCount": 1,
        "content": "B: \nhttps://cloud.google.com/architecture/application-deployment-and-testing-strategies#canary_test_pattern"
      },
      {
        "date": "2023-09-19T07:41:00.000Z",
        "voteCount": 1,
        "content": "I would go with B as it is best suited for this senario."
      },
      {
        "date": "2023-01-08T23:43:00.000Z",
        "voteCount": 2,
        "content": "answer is B to reduce impact on users because it's a progressive release"
      },
      {
        "date": "2022-12-08T07:24:00.000Z",
        "voteCount": 1,
        "content": "I think A is correct because of the switching to green only happens after you perform all the tests on it. So you can also test traffic (to satisfy the question) This is the point of blue-green deployment as far as I understood it.\nB is not correct because the real traffic is switched partially  to new version immediately and so it effects some users.\n\nreference: https://digitalvarys.com/what-is-blue-gren-deployment/"
      },
      {
        "date": "2022-11-27T07:28:00.000Z",
        "voteCount": 1,
        "content": "Blue/Green is 100% users to Green, Canary id progressive. B."
      },
      {
        "date": "2022-08-19T23:26:00.000Z",
        "voteCount": 2,
        "content": "B is correct"
      },
      {
        "date": "2022-07-22T01:26:00.000Z",
        "voteCount": 1,
        "content": "For me it's B, in Canary Deployment only a percentage of users receives the new version and therefore in case of error immediately rollback , B/G immediately the new version , Green, receive all traffic and Blue marked as deprecated"
      },
      {
        "date": "2022-07-07T00:54:00.000Z",
        "voteCount": 1,
        "content": "I think the concept of google about B/G testing is that there is a shadow running next to production that receives the same traffic as production. When this shadow is not having any errors you can update the shadow to PROD. So no user is impacted, since all possible new errors will occur in the shadow and not in prod."
      },
      {
        "date": "2022-07-14T03:36:00.000Z",
        "voteCount": 1,
        "content": "Find more info here: https://cloud.google.com/architecture/implementing-deployment-and-testing-strategies-on-gke#perform_a_bluegreen_deployment"
      },
      {
        "date": "2022-05-25T09:50:00.000Z",
        "voteCount": 1,
        "content": "For me is B. But I cannot understand why all purchased exam test with this question, put Blue/Green as correct answer. It's so clear that Canary is the rightest one 'cos forward only a few of users to new deploy (not every as blue/green) and also allow the rollback action"
      },
      {
        "date": "2022-02-15T18:27:00.000Z",
        "voteCount": 1,
        "content": "B is correct. Blue Green(B/G) affects all users."
      },
      {
        "date": "2021-12-01T10:06:00.000Z",
        "voteCount": 3,
        "content": "1. Reducing impact \n2. Number of users affected \nIf you want meet both of the conditions, you need to choose Canary"
      },
      {
        "date": "2021-11-01T01:42:00.000Z",
        "voteCount": 1,
        "content": "I think it is B, but correct answer is A, hmmm.......\nif we think that we need to test 100% of the traffic, i.e. create a full working test, then right A, because if there is an error we can quickly go back to the old version"
      },
      {
        "date": "2021-07-20T19:51:00.000Z",
        "voteCount": 1,
        "content": "Agree B"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 10,
    "url": "https://www.examtopics.com/discussions/google/view/21441-exam-professional-cloud-developer-topic-1-question-10/",
    "body": "Your company wants to expand their users outside the United States for their popular application. The company wants to ensure 99.999% availability of the database for their application and also wants to minimize the read latency for their users across the globe.<br>Which two actions should they take? (Choose two.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a multi-regional Cloud Spanner instance with \"nam-asia-eur1\" configuration.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a multi-regional Cloud Spanner instance with \"nam3\" configuration.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a cluster with at least 3 Spanner nodes.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a cluster with at least 1 Spanner node.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a minimum of two Cloud Spanner instances in separate regions with at least one node.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Cloud Dataflow pipeline to replicate data across different databases."
    ],
    "answer": "AC",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AC",
        "count": 14,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-11-06T14:56:00.000Z",
        "voteCount": 9,
        "content": "The more number of node less read latency hence i will go with option A and C"
      },
      {
        "date": "2020-05-27T02:19:00.000Z",
        "voteCount": 7,
        "content": "I think the answer should be A) + something.\nThey wants 99.999% availability - only multi-regional instance fits this. To minimize read latency nam-asia-eur1 instance works best as it has replicas in Noth America, Europe and Asia regions.\n\n\nAs for second answer - I do not have strong opinion.. As per documentation \"Adding nodes gives each replica more CPU and RAM, which increases the replica's throughput\" and they recommend to choose number of nodes to \"keep high priority total CPU utilization under 65%\". So nodes are not about SLA and read latency. From another hand spanner \"Cloud Spanner automatically replicates your data between regions with strong consistency guarantees\" so no DataFlow pipeline needed to replicate data, unless the app has other DBs and ETL between Spanner and that DBs."
      },
      {
        "date": "2024-03-07T07:42:00.000Z",
        "voteCount": 1,
        "content": "To achieve 99.999% availability and minimize read latency for a globally distributed user base, the company should:\n- Deploy a multi-regional Cloud Spanner instance with the \"nam-asia-eur1\" configuration (Option A). This setup will provide the geographical distribution of data across three continents \u2014 North America, Asia, and Europe. The multi-regional nature of this option is designed to maintain high availability and ensure users across these regions experience low latency when accessing the database.\n- Ensure that the Cloud Spanner instance has a minimum of three nodes (Option C). This configuration will contribute to the high availability and fault tolerance of the database. Spanner's built-in replication across these nodes in different regions will further support the five nines (99.999%) availability target, while also providing scalability for read operations."
      },
      {
        "date": "2024-02-06T06:08:00.000Z",
        "voteCount": 1,
        "content": "Let's think about all possible answers: \n - B does not make sense, as \"nam3\" is not the global configuration we want;\n - D is not ok, 'cause 1 node is not enough\n - E is not enough, as we want a global configuration;\n - F is totally unrelated."
      },
      {
        "date": "2023-09-19T07:42:00.000Z",
        "voteCount": 1,
        "content": "AC are best suited here."
      },
      {
        "date": "2023-05-03T11:40:00.000Z",
        "voteCount": 2,
        "content": "99.999% availability and reduce latency\nOption A gives us 99.999% availability (think its typo in region name)\nOption C is about compute capacity, more nodes -&gt; less latency\nhttps://cloud.google.com/spanner/docs/instances#compute-capacity\nB - there is no such multi-region configuration nam3\nD - its better to create cluster with 3 nodes, not 1\nE,F - overengineering"
      },
      {
        "date": "2023-06-25T13:40:00.000Z",
        "voteCount": 2,
        "content": "there is `nam3` \nhttps://cloud.google.com/spanner/docs/instance-configurations"
      },
      {
        "date": "2023-04-17T10:02:00.000Z",
        "voteCount": 1,
        "content": "A - global and provides 99.999% availability\nC - more nodes - less latency"
      },
      {
        "date": "2022-12-20T07:58:00.000Z",
        "voteCount": 1,
        "content": "it's obvious"
      },
      {
        "date": "2022-12-12T02:43:00.000Z",
        "voteCount": 1,
        "content": "why not C and F"
      },
      {
        "date": "2022-11-27T07:36:00.000Z",
        "voteCount": 1,
        "content": "3 regions at least, and for those that says there is no region \"man-asia-eur1\", take a look at the console, its multiregion nomenclature!"
      },
      {
        "date": "2023-01-12T04:06:00.000Z",
        "voteCount": 1,
        "content": "There is according to their documentation: https://cloud.google.com/spanner/docs/instance-configurations#three_continents"
      },
      {
        "date": "2022-11-05T02:17:00.000Z",
        "voteCount": 4,
        "content": "There is no region called \"\"nam-asia-eur1\"\"\nA is wrong, \nB&amp;C is correct"
      },
      {
        "date": "2022-08-21T21:41:00.000Z",
        "voteCount": 2,
        "content": "We need multi-regin db (spanner) to satisfy 99,999% SLA and have multi-node to ensure resource needed."
      },
      {
        "date": "2022-08-19T23:26:00.000Z",
        "voteCount": 3,
        "content": "AC are correct"
      },
      {
        "date": "2022-07-22T02:00:00.000Z",
        "voteCount": 4,
        "content": "C because the number of nodes increases the computational capabilities ( queries for seconds and so minor latency),  F to cover worldwide availability; A it's wrong because the configuration \"nam-asia-eur1\" not exist, Google suggests \"nam-eur-asia1\" o \"nam-eur-asia3\", D it's wrong because the number of nodes is too few; E is too expensive a solution"
      },
      {
        "date": "2022-07-12T05:26:00.000Z",
        "voteCount": 1,
        "content": "A and C make sense"
      },
      {
        "date": "2022-01-07T11:41:00.000Z",
        "voteCount": 1,
        "content": "This should be A and F."
      },
      {
        "date": "2021-07-20T20:37:00.000Z",
        "voteCount": 3,
        "content": "Agree with A and C"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 11,
    "url": "https://www.examtopics.com/discussions/google/view/36292-exam-professional-cloud-developer-topic-1-question-11/",
    "body": "You need to migrate an internal file upload API with an enforced 500-MB file size limit to App Engine.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse FTP to upload files.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse CPanel to upload files.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse signed URLs to upload files.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the API to be a multipart file upload API.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 6,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 5,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-07-13T00:09:00.000Z",
        "voteCount": 1,
        "content": "C is correct: Signed URLs allow you to generate URLs with limited-time access to upload directly to Google Cloud Storage. This method bypasses App Engine's file upload limitations and allows clients to upload large files directly to Cloud Storage."
      },
      {
        "date": "2024-06-06T07:56:00.000Z",
        "voteCount": 1,
        "content": "D. Change the API to be a multipart file upload API: While multipart uploads can be helpful for large files, they don't address the core issue of App Engine's size limitations. The uploads would still need to go through App Engine, potentially exceeding the limits."
      },
      {
        "date": "2024-03-07T07:49:00.000Z",
        "voteCount": 1,
        "content": "C. Use signed URLs to upload files: Signed URLs are a secure way to give time-limited read or write access to a specific Google Cloud Storage object, without needing Google account credentials. You can create a signed URL that allows an object to be accessed with the specified restrictions such as HTTP method (PUT for uploads) and an expiration time. This method would allow your API users to upload files directly to Google Cloud Storage, which can handle large files efficiently. Your App Engine application can then process or reference these files as needed."
      },
      {
        "date": "2024-02-29T05:19:00.000Z",
        "voteCount": 1,
        "content": "https://stackoverflow.com/questions/45812595/google-cloud-storage-signed-urls-how-to-specify-a-maximum-file-size"
      },
      {
        "date": "2024-02-11T09:37:00.000Z",
        "voteCount": 3,
        "content": "While C is a very good option if you want to people to upload files, it does not solve the problem represented by the size."
      },
      {
        "date": "2023-09-19T07:43:00.000Z",
        "voteCount": 1,
        "content": "By changing the API to support multipart file uploads, you can maintain the functionality of your existing API while adapting it to the App Engine environment."
      },
      {
        "date": "2023-08-24T01:22:00.000Z",
        "voteCount": 2,
        "content": "The correct answer is C because signed url permits to upload a big file in multipart-mode"
      },
      {
        "date": "2023-06-26T17:30:00.000Z",
        "voteCount": 2,
        "content": "It should use multipart to upload big size files"
      },
      {
        "date": "2023-03-29T03:21:00.000Z",
        "voteCount": 1,
        "content": "How is C correct ? Isn't it used to give temporary access to objects in buckets ?"
      },
      {
        "date": "2022-08-19T23:29:00.000Z",
        "voteCount": 1,
        "content": "C  is correct"
      },
      {
        "date": "2021-07-21T01:19:00.000Z",
        "voteCount": 1,
        "content": "C is the best choice"
      },
      {
        "date": "2021-06-20T00:34:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/appengine/docs/standard/php/googlestorage/user_upload:\n\"Note that you must start uploading to this URL within 10 minutes of its creation. Also, you cannot change the URL in any way - it is signed and the signature is checked before your upload begins\"\n\nC is the answer"
      },
      {
        "date": "2020-11-06T15:07:00.000Z",
        "voteCount": 2,
        "content": "C is correct answer"
      },
      {
        "date": "2021-05-19T07:09:00.000Z",
        "voteCount": 2,
        "content": "true\nhttps://stackoverflow.com/a/18882565/8681600"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 12,
    "url": "https://www.examtopics.com/discussions/google/view/36293-exam-professional-cloud-developer-topic-1-question-12/",
    "body": "You are planning to deploy your application in a Google Kubernetes Engine (GKE) cluster. The application exposes an HTTP-based health check at /healthz. You want to use this health check endpoint to determine whether traffic should be routed to the pod by the load balancer.<br>Which code snippet should you include in your Pod configuration?<br>A.<br><img src=\"/assets/media/exam-media/04137/0000800001.jpg\" class=\"in-exam-image\"><br>B.<br><img src=\"/assets/media/exam-media/04137/0000800002.jpg\" class=\"in-exam-image\"><br>C.<br><img src=\"/assets/media/exam-media/04137/0000800003.jpg\" class=\"in-exam-image\"><br>D.<br><img src=\"/assets/media/exam-media/04137/0000800004.jpg\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "B",
    "answerDescription": "For the GKE ingress controller to use your readinessProbes as health checks, the Pods for an Ingress must exist at the time of Ingress creation. If your replicas are scaled to 0, the default health check will apply.",
    "votes": [],
    "comments": [
      {
        "date": "2023-11-08T16:40:00.000Z",
        "voteCount": 1,
        "content": "B:\nhttps://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/"
      },
      {
        "date": "2023-01-01T07:39:00.000Z",
        "voteCount": 3,
        "content": "Option B is the correct code snippet to include in the Pod configuration in order to use the /healthz endpoint as a readiness probe.\n\nThe liveness probe, specified in option A, is used to determine whether the application is running and responsive. If the liveness probe fails, the application is considered to be in a failed state and will be restarted.\n\nThe readiness probe, specified in option B, is used to determine when a Pod is ready to receive traffic. If the readiness probe fails, the Pod will not receive traffic from the load balancer until it becomes healthy again.\n\nOption C, loadbalancerHealthCheck, is not a valid field in a Pod configuration.\n\nOption D, healthCheck, is also not a valid field in a Pod configuration."
      },
      {
        "date": "2022-09-13T23:20:00.000Z",
        "voteCount": 3,
        "content": "Readiness probe marks the pod as \"Running\" and can START accepting traffic.\nHowever, after the pod is in a \"Running\" state, the Liveness probe comes in and acts as the health check for the entire lifecycle of the pod.\n\nSo think of it this way. The pod passes its initial check (readiness), accepts traffic for a while then crashes (logically, the pod can still be \"Running\").\nThe Liveness probe is responsible for detecting it. Otherwise, the LB could still pass traffic to a pod that can't serve traffic.\n\nTherefore, I believe it's A."
      },
      {
        "date": "2022-09-14T00:50:00.000Z",
        "voteCount": 1,
        "content": "On 10th read, It's more likely they mean the initial phase of the lifecycle so it's B.\nBut the wording here is terrible. Hopefully they refined the question in the real exam."
      },
      {
        "date": "2022-08-19T23:29:00.000Z",
        "voteCount": 2,
        "content": "B  is correct"
      },
      {
        "date": "2022-05-25T09:58:00.000Z",
        "voteCount": 1,
        "content": "B is correct according with https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/"
      },
      {
        "date": "2022-02-15T18:48:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/kubernetes-engine/docs/how-to/ingress-features\n\ningress backendconfig can set healthCheck, but this resource can not set httpGet \n\n```\napiVersion: cloud.google.com/v1\nkind: BackendConfig\nmetadata:\n  name: my-backendconfig\nspec:\n  healthCheck:\n    checkIntervalSec: INTERVAL\n    timeoutSec: TIMEOUT\n    healthyThreshold: HEALTH_THRESHOLD\n    unhealthyThreshold: UNHEALTHY_THRESHOLD\n    type: PROTOCOL\n    requestPath: PATH\n    port: PORT\n```"
      },
      {
        "date": "2021-07-24T20:02:00.000Z",
        "voteCount": 2,
        "content": "B is correct"
      },
      {
        "date": "2021-06-20T00:41:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/kubernetes-engine/docs/concepts/ingress:\n\"GKE can infer some or all of the parameters for a health check if the Serving Pods use a Pod template with a container whose readiness probe has attributes that can be interpreted as health check parameters. See Parameters from a readiness probe for implementation details and Default and inferred parameters for a list of attributes that can be used to create health check parameters. Only the GKE Ingress controller supports inferring parameters from a readiness probe.\"\n\nB is correct"
      },
      {
        "date": "2021-06-24T00:17:00.000Z",
        "voteCount": 2,
        "content": "yes, readness probe is for checking if the pod can accept traffic."
      },
      {
        "date": "2020-11-06T15:10:00.000Z",
        "voteCount": 2,
        "content": "B is correct answer\n\nhttps://cloud.google.com/blog/products/gcp/kubernetes-best-practices-setting-up-health-checks-with-readiness-and-liveness-probes"
      },
      {
        "date": "2022-09-25T21:17:00.000Z",
        "voteCount": 1,
        "content": "B correct"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 13,
    "url": "https://www.examtopics.com/discussions/google/view/39863-exam-professional-cloud-developer-topic-1-question-13/",
    "body": "Your teammate has asked you to review the code below. Its purpose is to efficiently add a large number of small rows to a BigQuery table.<br><img src=\"/assets/media/exam-media/04137/0000900001.jpg\" class=\"in-exam-image\"><br>Which improvement should you suggest your teammate make?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInclude multiple rows with each request.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPerform the inserts in parallel by creating multiple threads.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite each row to a Cloud Storage object, then load into BigQuery.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite each row to a Cloud Storage object in parallel, then load into BigQuery."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 13,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 9,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-12-14T11:20:00.000Z",
        "voteCount": 24,
        "content": "For me the correct answer is A.\nInfact the loop build a single InsertReqeust and send it.\nBut we can build all request in a list and use InsertAllRequest.newBuilder(tableId).setRows(rows).build() to send.\nhttps://cloud.google.com/bigquery/streaming-data-into-bigquery#streaminginsertexamples"
      },
      {
        "date": "2021-12-22T03:07:00.000Z",
        "voteCount": 6,
        "content": "Response should be A, because original code pushes one row at a time, which is more time consuming in contrast to batch processing. \n\nProposed answer C is incorrect, because we still have more overhead in sending each row in separate request than using batch processing."
      },
      {
        "date": "2024-07-13T00:15:00.000Z",
        "voteCount": 1,
        "content": "Correct answer A: Batching Rows: By batching multiple rows into a single request, you minimise the overhead of network communication and API call latency. BigQuery's InsertAllRequest supports inserting multiple rows in a single API call.\nParallel Inserts (Option B): While parallel processing can improve performance, it introduces complexity and may require handling concurrency issues. It's generally better to batch rows first.\nWriting to Cloud Storage (Options C &amp; D): Writing data to Cloud Storage and then loading it into BigQuery can be efficient for very large datasets, but it adds extra steps and complexity. It's more suitable for bulk load operations rather than small, frequent inserts."
      },
      {
        "date": "2024-07-10T06:13:00.000Z",
        "voteCount": 1,
        "content": "For smaller datasets or when simplicity is paramount: Including multiple rows with each request is often sufficient.\nFor larger datasets or when performance is critical: Parallel inserts are the way to go."
      },
      {
        "date": "2024-03-07T08:19:00.000Z",
        "voteCount": 1,
        "content": "A. Include multiple rows with each request:\nThis would be a very efficient way to batch the insert operations. BigQuery's insertAll method supports batched inserts, so instead of inserting each row in a separate request, you could group multiple rows into a single insertAll request. This approach reduces the number of HTTP requests made to the BigQuery service, which can improve throughput and reduce the risk of hitting rate limits."
      },
      {
        "date": "2023-11-20T12:10:00.000Z",
        "voteCount": 1,
        "content": "B - I was between A and B. Both options require changes in the code and Option B requires changes in the way you are managing the Collection. If you insert multiples rows at a time, you would still need to move through the ROWS in the collection one by one (remember, this is a loop) to then insert in bulk. If you first break the Collection into (n) subsets and then run the function in (n) threats, you would be moving through (n) subsets at a time, making (n) insertions at a time, all in parallel. That was my way of viewing it.\n\nOption A would actually not even make a change in performance (sort of), you would just be interacting with the database less. (if interacting less in faster then you would see a small decrease in insert latencies)"
      },
      {
        "date": "2023-09-19T07:44:00.000Z",
        "voteCount": 1,
        "content": "I would go with A."
      },
      {
        "date": "2023-02-27T08:05:00.000Z",
        "voteCount": 1,
        "content": "i'd choose A. for me it's same as batch insert/update recommended"
      },
      {
        "date": "2023-02-06T03:42:00.000Z",
        "voteCount": 1,
        "content": "A. Include multiple rows with each request.\n\nBatch inserts are more efficient than individual inserts and will increase write performance by reducing the overhead of creating and sending individual requests for each row. Parallel inserts could potentially lead to conflicting writes or cause resource exhaustion, and adding a step of writing to Cloud Storage and then loading into BigQuery can add additional overhead and complexity."
      },
      {
        "date": "2023-02-02T02:24:00.000Z",
        "voteCount": 1,
        "content": "A is the corret answer"
      },
      {
        "date": "2023-01-18T13:09:00.000Z",
        "voteCount": 1,
        "content": "answer A, biquery support multiple insert in one request\nhttps://cloud.google.com/bigquery/docs/samples/bigquery-table-insert-rows"
      },
      {
        "date": "2023-01-01T07:34:00.000Z",
        "voteCount": 1,
        "content": "A. Include multiple rows with each request.\n\nIt is generally more efficient to insert multiple rows in a single request, rather than making a separate request for each row. This reduces the overhead of making multiple HTTP requests, and can also improve performance by allowing BigQuery to perform more efficient batch operations. You can use the InsertAllRequest.RowToInsert.of(row) method to add multiple rows to a single request"
      },
      {
        "date": "2023-01-01T07:34:00.000Z",
        "voteCount": 1,
        "content": "For example, you could modify the code to collect the rows in a list and insert them in batches:\n\nList&lt;InsertAllRequest.RowToInsert&gt; rowsToInsert = new ArrayList&lt;&gt;();\nfor (Map&lt;String, String&gt; row : rows) {\n    rowsToInsert.add(InsertAllRequest.RowToInsert.of(row));\n    if (rowsToInsert.size() == BATCH_SIZE) {\n        InsertAllRequest insertRequest = InsertAllRequest.newBuilder(\n            \"datasetId\", \"tableId\", rowsToInsert).build();\n        service.insertAll(insertRequest);\n        rowsToInsert.clear();\n    }\n}\nif (!rowsToInsert.isEmpty()) {\n    InsertAllRequest insertRequest = InsertAllRequest.newBuilder(\n        \"datasetId\", \"tableId\", rowsToInsert).build();\n    service.insertAll(insertRequest);\n}\n\nThis will insert the rows in batches of BATCH_SIZE, which you can adjust based on the desired balance between performance and resource usage."
      },
      {
        "date": "2023-01-01T07:36:00.000Z",
        "voteCount": 1,
        "content": "Options B and D, which involve using multiple threads to perform the inserts or write the rows to Cloud Storage, may not necessarily improve the efficiency of the code. These options could potentially increase the complexity of the code and introduce additional overhead, without necessarily improving the performance of the inserts.\n\nOption C, writing each row to a Cloud Storage object before loading into BigQuery, would likely be less efficient than simply inserting the rows directly into BigQuery. It would involve additional steps and potentially increase the overall time it takes to write the rows to the table."
      },
      {
        "date": "2022-12-11T12:31:00.000Z",
        "voteCount": 1,
        "content": "vote A"
      },
      {
        "date": "2022-11-30T10:10:00.000Z",
        "voteCount": 1,
        "content": "Original code inserts one row at a time so no point on using parallel requests.."
      },
      {
        "date": "2022-09-02T00:14:00.000Z",
        "voteCount": 1,
        "content": "Parallel saving to the database can increase the total addition time and depends on many system conditions. While batch saving is optimized at the database core level."
      },
      {
        "date": "2022-08-19T23:29:00.000Z",
        "voteCount": 2,
        "content": "B is correct"
      },
      {
        "date": "2022-08-19T16:52:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/bigquery/docs/samples/bigquery-table-insert-rows"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 14,
    "url": "https://www.examtopics.com/discussions/google/view/36297-exam-professional-cloud-developer-topic-1-question-14/",
    "body": "You are developing a JPEG image-resizing API hosted on Google Kubernetes Engine (GKE). Callers of the service will exist within the same GKE cluster. You want clients to be able to get the IP address of the service.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDefine a GKE Service. Clients should use the name of the A record in Cloud DNS to find the service's cluster IP address.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDefine a GKE Service. Clients should use the service name in the URL to connect to the service.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDefine a GKE Endpoint. Clients should get the endpoint name from the appropriate environment variable in the client container.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDefine a GKE Endpoint. Clients should get the endpoint name from Cloud DNS."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 13,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-06-02T05:46:00.000Z",
        "voteCount": 19,
        "content": "It's B - Clients are in the cluster and therefore can use service dns names. \n\nhttps://kubernetes.io/docs/concepts/services-networking/dns-pod-service/\n\"Every Service defined in the cluster (including the DNS server itself) is assigned a DNS name. By default, a client Pod's DNS search list includes the Pod's own namespace and the cluster's default domain.\""
      },
      {
        "date": "2024-03-07T08:28:00.000Z",
        "voteCount": 1,
        "content": "B. This is the standard Kubernetes service discovery mechanism. When you define a Service in Kubernetes, it creates a DNS entry in the internal cluster DNS. Any pod in the cluster can then reach the service using the service name as a DNS name (e.g., http://service-name). This is the most straightforward and Kubernetes-native way to enable service discovery within a cluster."
      },
      {
        "date": "2023-10-16T19:52:00.000Z",
        "voteCount": 1,
        "content": "This is an example of Microservice Architecture, so Ans is : B"
      },
      {
        "date": "2023-09-19T07:44:00.000Z",
        "voteCount": 1,
        "content": "B is correct."
      },
      {
        "date": "2023-08-24T02:04:00.000Z",
        "voteCount": 1,
        "content": "https://www.exam-answer.com/gke-service-url-image-resizing-api"
      },
      {
        "date": "2023-06-03T17:11:00.000Z",
        "voteCount": 1,
        "content": "A.\nGKE endpoint is external facing, Opt C and D are out. Also exposing to endpoint won't expose all containers in the GKE cluster - if one service exposes to 4000 nodes with containers then does this mean the GKE would need to update 4000 times? This just doesn't make sense. Opt B use service name, in other words, CNAME, so it still has to go through Cloud DNS. Hence the opt A shall be correct."
      },
      {
        "date": "2023-05-21T11:32:00.000Z",
        "voteCount": 1,
        "content": "It's B"
      },
      {
        "date": "2023-05-04T08:47:00.000Z",
        "voteCount": 1,
        "content": "both A and B are valid\nOption A, DNS A record maps service FQDN to IP address, fqdn like service-name.default.svc.cluster.local\nB is more easier, just use http://service-name"
      },
      {
        "date": "2023-01-08T23:50:00.000Z",
        "voteCount": 1,
        "content": "answer is B because client are in the same cluster so service name can be used."
      },
      {
        "date": "2022-12-08T07:41:00.000Z",
        "voteCount": 1,
        "content": "Question reads \"IP address\" and I don't think that using B the IP can be obtained."
      },
      {
        "date": "2022-12-08T07:48:00.000Z",
        "voteCount": 1,
        "content": "C answer is suggesting to define endpoint in the service and others can use that endpoint (reading its name  from a variable) to ask the service what IP it has, that why I think C is correct."
      },
      {
        "date": "2022-11-08T04:15:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/kubernetes-engine/docs/concepts/service-discovery\n\n\"In Kubernetes, service discovery is implemented with automatically generated service names that map to the Service's IP address. Service names follow a standard specification: as follows: my-svc.my-namespace.svc.cluster-domain.example. Pods can also access external services through their names, such as example.com. \""
      },
      {
        "date": "2022-08-19T23:30:00.000Z",
        "voteCount": 3,
        "content": "B is correct"
      },
      {
        "date": "2022-08-16T09:02:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2022-08-19T02:16:00.000Z",
        "voteCount": 1,
        "content": "But qn states: \"..to get the IP address of the service\". Or not?"
      },
      {
        "date": "2022-07-12T05:31:00.000Z",
        "voteCount": 1,
        "content": "Should be B"
      },
      {
        "date": "2022-02-16T06:03:00.000Z",
        "voteCount": 1,
        "content": "B is collect.\n\nIf client and server are in same namespace, C is collect.But in this case, no condition of namespace. So client pod must be use server service name."
      },
      {
        "date": "2022-02-03T05:25:00.000Z",
        "voteCount": 1,
        "content": "D: see https://cloud.google.com/endpoints/docs/openapi/get-started-kubernetes-engine#configuring-endpoints-dns"
      },
      {
        "date": "2021-06-21T18:49:00.000Z",
        "voteCount": 1,
        "content": "A - use SVC to expose your pod, and get the cluster DNS service to get the IP"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 15,
    "url": "https://www.examtopics.com/discussions/google/view/39865-exam-professional-cloud-developer-topic-1-question-15/",
    "body": "You are using Cloud Build to build and test application source code stored in Cloud Source Repositories. The build process requires a build tool not available in the Cloud Build environment.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDownload the binary from the internet during the build process.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBuild a custom cloud builder image and reference the image in your build steps.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInclude the binary in your Cloud Source Repositories repository and reference it in your build scripts.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAsk to have the binary added to the Cloud Build environment by filing a feature request against the Cloud Build public Issue Tracker."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-12-14T11:33:00.000Z",
        "voteCount": 9,
        "content": "B is correct answer\nhttps://cloud.google.com/cloud-build/docs/configuring-builds/use-community-and-custom-builders#creating_a_custom_builder"
      },
      {
        "date": "2021-06-20T01:28:00.000Z",
        "voteCount": 2,
        "content": "I agree"
      },
      {
        "date": "2022-09-25T21:38:00.000Z",
        "voteCount": 1,
        "content": "If the task you want to perform requires capabilities that are not provided by a public image, a supported builder, or a community-contributed buider, you can build your own image and use it in a build step.\nB"
      },
      {
        "date": "2023-10-24T04:06:00.000Z",
        "voteCount": 1,
        "content": "I think is a generic solution because I can use it in other projects."
      },
      {
        "date": "2023-09-19T07:45:00.000Z",
        "voteCount": 1,
        "content": "B is correct."
      },
      {
        "date": "2023-05-21T11:34:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2022-08-19T23:33:00.000Z",
        "voteCount": 2,
        "content": "B  is correct"
      },
      {
        "date": "2022-08-19T23:30:00.000Z",
        "voteCount": 1,
        "content": "B  is correct"
      },
      {
        "date": "2022-06-13T23:59:00.000Z",
        "voteCount": 1,
        "content": "b https://cloud.google.com/cloud-build/docs/configuring-builds/use-community-and-custom-builders#creating_a_custom_builder"
      },
      {
        "date": "2022-02-16T06:05:00.000Z",
        "voteCount": 1,
        "content": "B is correct. \nBut in usual A is faster than B(download docker image speed &lt;&lt;&lt; download binary)"
      },
      {
        "date": "2023-05-04T09:08:00.000Z",
        "voteCount": 1,
        "content": "cache image"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 16,
    "url": "https://www.examtopics.com/discussions/google/view/36298-exam-professional-cloud-developer-topic-1-question-16/",
    "body": "You are deploying your application to a Compute Engine virtual machine instance. Your application is configured to write its log files to disk. You want to view the logs in Stackdriver Logging without changing the application code.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall the Stackdriver Logging Agent and configure it to send the application logs.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a Stackdriver Logging Library to log directly from the application to Stackdriver Logging.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tProvide the log file folder path in the metadata of the instance to configure it to send the application logs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the application to log to /var/log so that its logs are automatically sent to Stackdriver Logging."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-03-07T08:45:00.000Z",
        "voteCount": 1,
        "content": "A. The Stackdriver Logging Agent is a software agent that can collect logs from various sources on your virtual machine and send them to Stackdriver Logging. By configuring the agent, you can specify custom log file paths, and the agent will forward these logs to Stackdriver Logging. This is the most direct and least intrusive method when you cannot or do not want to change the application code."
      },
      {
        "date": "2023-09-29T03:21:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is A"
      },
      {
        "date": "2023-09-19T07:47:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2023-05-21T11:35:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer: A"
      },
      {
        "date": "2023-01-02T02:19:00.000Z",
        "voteCount": 1,
        "content": "A is correct here"
      },
      {
        "date": "2022-09-25T21:40:00.000Z",
        "voteCount": 1,
        "content": "A is correct  -We need to install stackdriver agent in the VM"
      },
      {
        "date": "2022-08-19T23:31:00.000Z",
        "voteCount": 3,
        "content": "A is correct"
      },
      {
        "date": "2022-08-19T23:30:00.000Z",
        "voteCount": 1,
        "content": "A  is correct"
      },
      {
        "date": "2022-06-14T00:00:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/logging/docs/agent/logging/installation"
      },
      {
        "date": "2021-06-20T01:32:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/logging/docs/agent/logging/installation:\n\"The Logging agent streams logs from your VM instances and from selected third-party software packages to Cloud Logging\"\n\nA is correct"
      },
      {
        "date": "2020-11-06T16:12:00.000Z",
        "voteCount": 4,
        "content": "A is correct option here"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 17,
    "url": "https://www.examtopics.com/discussions/google/view/36299-exam-professional-cloud-developer-topic-1-question-17/",
    "body": "Your service adds text to images that it reads from Cloud Storage. During busy times of the year, requests to Cloud Storage fail with an HTTP 429 \"Too Many<br>Requests\" status code.<br>How should you handle this error?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a cache-control header to the objects.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRequest a quota increase from the GCP Console.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRetry the request with a truncated exponential backoff strategy.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the storage class of the Cloud Storage bucket to Multi-regional."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-03-07T08:56:00.000Z",
        "voteCount": 1,
        "content": "C. Implementing a truncated exponential backoff strategy is a recommended practice for handling 429 errors. This approach involves waiting for a short period before retrying the failed request, with the wait time increasing (up to a maximum limit) with each successive retry. This can help to alleviate the load causing the rate limit to be hit and is a well-established pattern for handling such errors in distributed systems."
      },
      {
        "date": "2023-09-29T03:26:00.000Z",
        "voteCount": 1,
        "content": "Exponential backoff formula will generate delay"
      },
      {
        "date": "2023-09-19T07:47:00.000Z",
        "voteCount": 1,
        "content": "C is Correct"
      },
      {
        "date": "2023-04-26T09:16:00.000Z",
        "voteCount": 1,
        "content": "C is a must or your code won't do the job. But Request a quota increase will come afterwards or it will be a pain rely on exponential backoff...."
      },
      {
        "date": "2023-01-01T07:14:00.000Z",
        "voteCount": 1,
        "content": "To handle HTTP 429 \"Too Many Requests\" errors when requesting data from Cloud Storage, you should retry the request with a truncated exponential backoff strategy (C).\n\nAn HTTP 429 \"Too Many Requests\" status code indicates that the server is receiving too many requests and is unable to handle them all. In this situation, it is generally best to retry the request after a period of time, using a truncated exponential backoff strategy. This involves retrying the request with increasingly longer delays between each retry, up to a maximum delay. The delays can be generated using an exponential backoff formula, which increases the delay by a power of two on each retry. The retries can be truncated at a maximum delay to prevent the retries from taking too long."
      },
      {
        "date": "2023-01-01T07:14:00.000Z",
        "voteCount": 1,
        "content": "Adding a cache-control header to the objects (A) may not be sufficient to address the issue, as it only affects how the objects are cached by clients. Requesting a quota increase from the GCP Console (B) may help to alleviate the issue, but it may not be a sufficient solution on its own. Changing the storage class of the Cloud Storage bucket to Multi-regional (D) may also not be sufficient to address the issue, as it only affects the location of the data and does not directly address the issue of too many requests."
      },
      {
        "date": "2022-08-19T23:33:00.000Z",
        "voteCount": 3,
        "content": "C is correct"
      },
      {
        "date": "2022-01-18T18:48:00.000Z",
        "voteCount": 1,
        "content": "C is right one, choose proper backoff strategy"
      },
      {
        "date": "2020-11-06T16:15:00.000Z",
        "voteCount": 3,
        "content": "C is correct option here"
      },
      {
        "date": "2021-03-08T00:54:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/storage/docs/json_api/v1/status-codes"
      },
      {
        "date": "2021-06-20T01:36:00.000Z",
        "voteCount": 3,
        "content": "\"A Cloud Storage JSON API usage limit was exceeded. If your application tries to use more than its limit, additional requests will fail. Throttle your client's requests, and/or use truncated exponential backoff.\"\n\nC is correct"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 18,
    "url": "https://www.examtopics.com/discussions/google/view/69168-exam-professional-cloud-developer-topic-1-question-18/",
    "body": "You are building an API that will be used by Android and iOS apps. The API must:<br>* Support HTTPs<br>* Minimize bandwidth cost<br>* Integrate easily with mobile apps<br>Which API architecture should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRESTful APIs",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMQTT for APIs",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tgRPC-based APIs\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSOAP-based APIs"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 10,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 5,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-01-22T14:22:00.000Z",
        "voteCount": 9,
        "content": "In't C (gRPC) better because of the lower bandwidth (binary format)?\nIt can be used in mobile apps and supports HTTP(s)"
      },
      {
        "date": "2022-05-22T22:20:00.000Z",
        "voteCount": 2,
        "content": "Probably A is the best solution for \"Integrate easily with mobile apps\" requirement"
      },
      {
        "date": "2024-07-10T06:41:00.000Z",
        "voteCount": 1,
        "content": "For an API designed for mobile apps with the requirements you've outlined, a RESTful API architecture is the most suitable choice. Here's why:\n\nRESTful API Advantages:\n\nHTTPs Support: REST APIs are built on top of HTTP, making HTTPS integration seamless.\nBandwidth Minimization: REST APIs are designed for efficient data transfer. They use standard HTTP methods (GET, POST, PUT, DELETE) for CRUD operations, minimizing the amount of data sent over the network.\nEasy Mobile Integration: REST APIs are widely understood and supported by mobile development frameworks (like Android's Retrofit and iOS's Alamofire). They use JSON as a standard data format, which is easily parsed by mobile apps."
      },
      {
        "date": "2024-07-10T06:42:00.000Z",
        "voteCount": 1,
        "content": "Why Other Architectures Might Not Be Ideal:\n\nSOAP: While SOAP supports HTTPS, it's more complex and verbose, leading to higher bandwidth usage and potentially more difficult integration with mobile apps.\nGraphQL: GraphQL is excellent for flexible data fetching, but it can be more complex to implement and might not be as widely supported by mobile development frameworks as REST.\nAdditional Tips for Bandwidth Optimization:\n\nData Compression: Use gzip compression to reduce the size of data transferred.\nCaching: Implement caching mechanisms on both the server and client sides to reduce redundant data requests.\nAPI Versioning: Use versioning to avoid breaking changes and allow for gradual updates.\nPagination: For large datasets, implement pagination to fetch data in smaller chunks."
      },
      {
        "date": "2024-03-07T09:16:00.000Z",
        "voteCount": 1,
        "content": "C. gRPC-based APIs are designed to minimize bandwidth by using Protocol Buffers, a method of serializing structured data in an efficient and extensible format. gRPC is modern, fast, and supports HTTPS by default. It also provides features like streaming and efficient connection management, which can be advantageous for mobile apps that require efficient use of bandwidth and battery life."
      },
      {
        "date": "2024-02-13T00:51:00.000Z",
        "voteCount": 1,
        "content": "I'm going with gRPC was it is a Google Product, and we're talking about a GCP exam, so...\nREST APIs would still serve the purpose though."
      },
      {
        "date": "2023-11-13T12:22:00.000Z",
        "voteCount": 1,
        "content": "A, according to https://www.exam-answer.com/api-architecture-for-android-ios-apps"
      },
      {
        "date": "2023-09-19T07:48:00.000Z",
        "voteCount": 1,
        "content": "C is correct as it supports IOS and Android with low latency."
      },
      {
        "date": "2023-09-18T20:49:00.000Z",
        "voteCount": 1,
        "content": "Answer is C: gRPC is a high-performance, open-source universal RPC framework which supports IOS and Android as well."
      },
      {
        "date": "2023-08-24T02:14:00.000Z",
        "voteCount": 1,
        "content": "https://www.exam-answer.com/api-architecture-for-android-ios-apps#:~:text=The%20most%20suitable%20API%20architecture,used%20for%20building%20web%20APIs."
      },
      {
        "date": "2023-05-21T11:39:00.000Z",
        "voteCount": 1,
        "content": "gRPC-based APIs"
      },
      {
        "date": "2023-03-15T04:58:00.000Z",
        "voteCount": 1,
        "content": "gRPC supports ios and andriod also."
      },
      {
        "date": "2023-02-27T08:01:00.000Z",
        "voteCount": 2,
        "content": "https://www.imaginarycloud.com/blog/grpc-vs-rest/\n\ngRPC architectural style has promising features that can (and should) be explored. It is an excellent option for working with multi-language systems, real-time streaming, and for instance, when operating an IoT system that requires light-weight message transmission such as the serialized Protobuf messages allow. Moreover, gRPC should also be considered for mobile applications since they do not need a browser and can benefit from smaller messages, preserving mobiles' processors' speed."
      },
      {
        "date": "2023-02-02T23:09:00.000Z",
        "voteCount": 1,
        "content": "The correct is C"
      },
      {
        "date": "2023-01-01T07:09:00.000Z",
        "voteCount": 4,
        "content": "To support HTTPS, minimize bandwidth cost, and integrate easily with mobile apps, you should use gRPC-based APIs (C).\n\ngRPC (gRPC Remote Procedure Calls) is a modern, high-performance, open-source remote procedure call (RPC) framework that can be used to build APIs. It uses HTTP/2 as the underlying transport protocol and Protocol Buffers as the encoding format. gRPC is designed to be low-bandwidth, low-latency, and easily integrable with mobile apps. It also supports HTTPs out of the box.\n\nRESTful APIs (A) are a popular choice for building APIs, but they may not be as efficient as gRPC in terms of bandwidth usage, especially for APIs that transfer large amounts of data. MQTT (B) is a lightweight messaging protocol that is often used in IoT applications, but it may not be as well-suited for building APIs as gRPC. SOAP-based APIs (D) are an older style of API that has largely been replaced by more modern alternatives like gRPC."
      },
      {
        "date": "2022-10-29T07:55:00.000Z",
        "voteCount": 2,
        "content": "I think that the best answer is C:\n -  gRPC supports HTTPS;\n- minimize bandwidth because use a binary payload;\n- It is easy to integrate because it generate stubs and skeletons that hide the connection details.\n\nhttps://cloud.google.com/blog/products/api-management/understanding-grpc-openapi-and-rest-and-when-to-use-them"
      },
      {
        "date": "2022-10-16T22:53:00.000Z",
        "voteCount": 2,
        "content": "REST APIs are not bound to client-side technology. This allows you to access these APIs from a client-side web project, iOS app, IoT device, or Windows phone.\nIn the problem statement, they mentioned iOS and Android.\nIt is not officially supported by gRPC\nSo that points to option A, otherwise, C is nearest."
      },
      {
        "date": "2022-08-19T23:34:00.000Z",
        "voteCount": 2,
        "content": "A is correct"
      },
      {
        "date": "2022-01-22T14:43:00.000Z",
        "voteCount": 2,
        "content": "Agreed Option A is correct"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 19,
    "url": "https://www.examtopics.com/discussions/google/view/21517-exam-professional-cloud-developer-topic-1-question-19/",
    "body": "Your application takes an input from a user and publishes it to the user's contacts. This input is stored in a table in Cloud Spanner. Your application is more sensitive to latency and less sensitive to consistency.<br>How should you perform reads from Cloud Spanner for this application?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPerform Read-Only transactions.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPerform stale reads using single-read methods.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPerform strong reads using single-read methods.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPerform stale reads using read-write transactions."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-05-28T06:56:00.000Z",
        "voteCount": 15,
        "content": "As mentioned here https://cloud.google.com/spanner/docs/reference/rest/v1/TransactionOptions read-write transaction type has no options, and there is no way to make stale reads with this transaction type, so D) is definitely wrong.\nIn the question, low latency is more critical than consistency, so C) is not an option. Read-Only transactions can do stale reads as well as Single Read methods, but in the documentation https://cloud.google.com/spanner/docs/transactions#read-only_transactions , they encourage to use SingleRead methods where possible.\nMy vote is B)"
      },
      {
        "date": "2020-10-16T12:01:00.000Z",
        "voteCount": 6,
        "content": "I agree with B because of this statement: \"Your application is more sensitive to latency and less sensitive to consistency.\"\n\nAlso if your application is latency sensitive but tolerant of stale data, then stale reads can provide performance benefits.\nsource: https://cloud.google.com/spanner/docs/reads"
      },
      {
        "date": "2021-06-20T02:04:00.000Z",
        "voteCount": 6,
        "content": "https://cloud.google.com/spanner/docs/reads:\n\"A stale read is read at a timestamp in the past. If your application is latency sensitive but tolerant of stale data, then stale reads can provide performance benefits.\"\n\nB is the answer since the qn is asking about reading from Cloud Spanner; no writes involved"
      },
      {
        "date": "2024-03-07T09:23:00.000Z",
        "voteCount": 1,
        "content": "B. Perform stale reads using single-read methods: Stale reads (also known as bounded staleness reads) can execute with lower latency because they can serve data from a timestamp in the recent past and do not have to wait for ongoing writes to be completed. This allows the application to trade off some consistency for lower latency, which is suitable for this scenario."
      },
      {
        "date": "2023-09-18T21:00:00.000Z",
        "voteCount": 1,
        "content": "It Should be B.\nSince the application is more sensitive to latency and less sensitive to consistency performing stale reads is the best choice here. It will provide low latency at the cost of potentially returning stale data."
      },
      {
        "date": "2022-12-19T23:03:00.000Z",
        "voteCount": 1,
        "content": "Should be B"
      },
      {
        "date": "2022-11-30T10:22:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2022-08-19T23:34:00.000Z",
        "voteCount": 3,
        "content": "B is correct"
      },
      {
        "date": "2022-08-19T23:34:00.000Z",
        "voteCount": 1,
        "content": "B  is correct"
      },
      {
        "date": "2022-04-20T09:48:00.000Z",
        "voteCount": 2,
        "content": "in my humble opinion it's D because the application must write on user contacts table."
      },
      {
        "date": "2022-07-22T05:20:00.000Z",
        "voteCount": 1,
        "content": "the question is \"How should you perform reads from Cloud Spanner for this application?\" , so only read transaction...."
      },
      {
        "date": "2022-01-18T18:51:00.000Z",
        "voteCount": 1,
        "content": "B is right, data consistency is less sensitive."
      },
      {
        "date": "2022-01-08T01:54:00.000Z",
        "voteCount": 1,
        "content": "B is the correct answer"
      },
      {
        "date": "2021-11-17T03:03:00.000Z",
        "voteCount": 1,
        "content": "\"Your application takes an input from a user and publishes it to the user's contacts\"\nWhat does \"publishes\" mean? If \"write to database\" than my vote is D"
      },
      {
        "date": "2020-11-08T10:50:00.000Z",
        "voteCount": 4,
        "content": "B is correct answer here."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 20,
    "url": "https://www.examtopics.com/discussions/google/view/36478-exam-professional-cloud-developer-topic-1-question-20/",
    "body": "Your application is deployed in a Google Kubernetes Engine (GKE) cluster. When a new version of your application is released, your CI/CD tool updates the spec.template.spec.containers[0].image value to reference the Docker image of your new application version. When the Deployment object applies the change, you want to deploy at least 1 replica of the new version and maintain the previous replicas until the new replica is healthy.<br>Which change should you make to the GKE Deployment object shown below?<br><img src=\"/assets/media/exam-media/04137/0001300001.jpg\" class=\"in-exam-image\"><br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the Deployment strategy to RollingUpdate with maxSurge set to 0, maxUnavailable set to 1.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the Deployment strategy to RollingUpdate with maxSurge set to 1, maxUnavailable set to 0.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the Deployment strategy to Recreate with maxSurge set to 0, maxUnavailable set to 1.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the Deployment strategy to Recreate with maxSurge set to 1, maxUnavailable set to 0."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-11-08T11:06:00.000Z",
        "voteCount": 18,
        "content": "I will go with Option B for this. \n\nRollingUpdate: New pods are added gradually, and old pods are terminated gradually\nRecreate: All old pods are terminated before any new pods are added\n\nQuestion ask us to retain current version hence rolling update is better option here."
      },
      {
        "date": "2021-06-20T02:09:00.000Z",
        "voteCount": 7,
        "content": "https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-upgrades:\n\"The simplest way to take advantage of surge upgrade is to configure maxSurge=1 maxUnavailable=0. This means that only 1 surge node can be added to the node pool during an upgrade so only 1 node will be upgraded at a time. This setting is superior to the existing upgrade configuration (maxSurge=0 maxUnavailable=1) because it speeds up Pod restarts during upgrades while progressing conservatively.\"\n\nAnswer is B"
      },
      {
        "date": "2024-03-07T09:41:00.000Z",
        "voteCount": 1,
        "content": "B. Set the Deployment strategy to RollingUpdate with maxSurge set to 1, maxUnavailable set to 0:\n\nmaxSurge set to 1 allows the Deployment to exceed the desired number of Pods by one, permitting the creation of an additional new Pod before terminating the old ones, which aligns with the requirement.\nmaxUnavailable set to 0 ensures that all existing Pods must remain available during the update, which again meets the requirement."
      },
      {
        "date": "2024-02-13T00:54:00.000Z",
        "voteCount": 1,
        "content": "\"maxSurge=1\" means that at least one VM has to stay up during the RU process."
      },
      {
        "date": "2023-09-18T21:02:00.000Z",
        "voteCount": 1,
        "content": "Option B is the correct one."
      },
      {
        "date": "2023-01-01T07:11:00.000Z",
        "voteCount": 2,
        "content": "To deploy at least 1 replica of the new version and maintain the previous replicas until the new replica is healthy, you should set the Deployment strategy to RollingUpdate with maxSurge set to 1 and maxUnavailable set to 0 (B).\n\nThe RollingUpdate Deployment strategy allows you to specify the number of replicas that can be created or removed at a time as part of the update process. The maxSurge parameter specifies the maximum number of replicas that can be created in excess of the desired number of replicas, and the maxUnavailable parameter specifies the maximum number of replicas that can be unavailable at any given time.\n\nBy setting maxSurge to 1 and maxUnavailable to 0, you are telling the Deployment to create at least 1 new replica of the new version and to maintain all of the previous replicas until the new replica is healthy. This will ensure that at least 1 replica of the new version is always available, while allowing the Deployment to gradually roll out the update to the rest of the replicas."
      },
      {
        "date": "2023-01-01T07:11:00.000Z",
        "voteCount": 1,
        "content": "The Recreate Deployment strategy (C and D) would not be suitable for this use case, as it would involve replacing all of the replicas at once, rather than rolling out the update gradually."
      },
      {
        "date": "2022-12-19T23:05:00.000Z",
        "voteCount": 1,
        "content": "B is obvious"
      },
      {
        "date": "2022-11-30T10:25:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2022-08-19T23:35:00.000Z",
        "voteCount": 2,
        "content": "B  is correct"
      },
      {
        "date": "2022-01-18T18:53:00.000Z",
        "voteCount": 1,
        "content": "Obviously it's B"
      },
      {
        "date": "2022-01-08T02:26:00.000Z",
        "voteCount": 1,
        "content": "Answer is B"
      },
      {
        "date": "2022-01-08T01:56:00.000Z",
        "voteCount": 1,
        "content": "Answer is B"
      },
      {
        "date": "2022-01-02T09:44:00.000Z",
        "voteCount": 1,
        "content": "the answer is B."
      },
      {
        "date": "2021-02-06T17:12:00.000Z",
        "voteCount": 3,
        "content": "Recreate can't be maintain previous replica. Answer must be B."
      },
      {
        "date": "2020-12-29T23:21:00.000Z",
        "voteCount": 2,
        "content": "B is correct answer"
      },
      {
        "date": "2020-12-19T15:01:00.000Z",
        "voteCount": 1,
        "content": "You are not maintaining anything with Recreate strategy"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 21,
    "url": "https://www.examtopics.com/discussions/google/view/36481-exam-professional-cloud-developer-topic-1-question-21/",
    "body": "You plan to make a simple HTML application available on the internet. This site keeps information about FAQs for your application. The application is static and contains images, HTML, CSS, and Javascript. You want to make this application available on the internet with as few steps as possible.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpload your application to Cloud Storage.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpload your application to an App Engine environment.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Compute Engine instance with Apache web server installed. Configure Apache web server to host the application.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tContainerize your application first. Deploy this container to Google Kubernetes Engine (GKE) and assign an external IP address to the GKE pod hosting the application."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-09-18T21:04:00.000Z",
        "voteCount": 1,
        "content": "Option A: As we can host static applications on Cloud Storage."
      },
      {
        "date": "2023-07-24T10:29:00.000Z",
        "voteCount": 1,
        "content": "Explanation of Correct Answer\n\nUpload your application to Cloud Storage.\n\nCloud Storage is the correct answer because it is the simplest way to host a static website containing images, HTML, CSS, and JavaScript. Simply upload the static files to Cloud Storage, and they can be served on the internet with minimal configuration. Cloud Storage provides high availability and reliability, ensuring a fast and secure user experience.\n\nSource: https://examlab.co/google/google-cloud-professional-cloud-developer"
      },
      {
        "date": "2023-05-21T11:46:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer: A"
      },
      {
        "date": "2022-08-19T23:35:00.000Z",
        "voteCount": 3,
        "content": "A is correct"
      },
      {
        "date": "2022-08-16T09:08:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2022-01-18T18:54:00.000Z",
        "voteCount": 1,
        "content": "A is right"
      },
      {
        "date": "2021-06-25T20:37:00.000Z",
        "voteCount": 4,
        "content": "A is correct; provided link supports it"
      },
      {
        "date": "2020-11-08T11:17:00.000Z",
        "voteCount": 3,
        "content": "A, if its static then quickest way is via cloud storage."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 22,
    "url": "https://www.examtopics.com/discussions/google/view/36482-exam-professional-cloud-developer-topic-1-question-22/",
    "body": "Your company has deployed a new API to App Engine Standard environment. During testing, the API is not behaving as expected. You want to monitor the application over time to diagnose the problem within the application code without redeploying the application.<br>Which tool should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStackdriver Trace",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStackdriver Monitoring",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStackdriver Debug Snapshots",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStackdriver Debug Logpoints\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 11,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 4,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-11-08T11:24:00.000Z",
        "voteCount": 7,
        "content": "D is correct answer here."
      },
      {
        "date": "2024-07-10T07:28:00.000Z",
        "voteCount": 1,
        "content": "The best tool for this scenario is C. Stackdriver Debug Snapshots . Here's why:\n\nStackdriver Debug Snapshots allow you to take a snapshot of your application's state at a specific point in time. This includes the call stack and variable values. This lets you examine the code's execution path and the values of variables at the time of the issue, helping you pinpoint the problem within the application code."
      },
      {
        "date": "2024-07-10T07:28:00.000Z",
        "voteCount": 1,
        "content": "Here's why the other options are less suitable:\n\nA. Stackdriver Trace: Trace focuses on tracking the flow of requests through your application, providing insights into latency and performance bottlenecks. While helpful, it doesn't directly show you the state of variables within your code.\nB. Stackdriver Monitoring: Monitoring provides metrics and dashboards for overall application health and performance. It's great for identifying issues like high latency or errors, but it doesn't offer the granular code-level debugging that snapshots provide.\nD. Stackdriver Debug Logpoints: Logpoints allow you to add log statements to your application on the fly without redeploying. This is useful for adding debugging information, but it requires you to know where to add the log statements, which might not be obvious if you're unsure of the problem's location."
      },
      {
        "date": "2024-02-06T07:36:00.000Z",
        "voteCount": 1,
        "content": "You want to see what is the problem in the code without altering it. =&gt; Logpoints."
      },
      {
        "date": "2023-11-13T12:48:00.000Z",
        "voteCount": 1,
        "content": "You want to MONITOR the application - &gt; Stackdriver MONITORING"
      },
      {
        "date": "2023-11-13T12:48:00.000Z",
        "voteCount": 1,
        "content": "https://www.exam-answer.com/which-tool-should-use-monitor-application-stackdriver-monitoring"
      },
      {
        "date": "2023-09-18T22:34:00.000Z",
        "voteCount": 1,
        "content": "Option C: Stackdriver Debug Snapshots allow you to inspect the state of an application at any code location in production, without stopping or slowing down your applications."
      },
      {
        "date": "2023-08-24T02:45:00.000Z",
        "voteCount": 1,
        "content": "The Api required a monitoring tool, not troubleshooting"
      },
      {
        "date": "2023-05-21T11:48:00.000Z",
        "voteCount": 1,
        "content": "D. Stackdriver Debug Logpoints"
      },
      {
        "date": "2023-01-01T06:59:00.000Z",
        "voteCount": 1,
        "content": "Stackdriver Debug Snapshots is a feature of Stackdriver Debugger that allows you to capture a snapshot of the state of your application at a specific point in time. This snapshot includes information about the variables and the call stack at the time the snapshot was taken, as well as any log output that was generated.\n\nTo use Stackdriver Debug Snapshots to monitor your application, you would need to take periodic snapshots of your application and then analyze the snapshot data to identify any issues or problems. However, this would not be a real-time monitoring solution, and it would not allow you to continuously monitor your application for issues. Instead, it would be a way to investigate issues after they have occurred, by examining the state of the application at the time the snapshot was taken."
      },
      {
        "date": "2023-01-01T06:59:00.000Z",
        "voteCount": 1,
        "content": "Stackdriver Debug Logpoints is a feature of Stackdriver Debugger that allows you to insert logging statements into your code without modifying or redeploying your application. This can be useful for troubleshooting issues with your application, as it allows you to output data to the log without having to modify your code and redeploy the application.\n\nTo use Stackdriver Debug Logpoints to monitor your application, you would need to insert logpoints into your code at strategic points, and then analyze the log output to identify any issues or problems. However, this would not be a real-time monitoring solution, and it would not allow you to continuously monitor your application for issues. Instead, it would be a way to investigate issues after they have occurred, by examining the log output that was generated."
      },
      {
        "date": "2023-01-01T06:59:00.000Z",
        "voteCount": 1,
        "content": "Answer is B:\n\nTo monitor the application over time to diagnose a problem within the application code without redeploying the application, you should use Stackdriver Monitoring (B). Stackdriver Monitoring provides a range of tools that allow you to view and analyze performance metrics, traces, and logs for your application. This can help you identify and troubleshoot issues with your application."
      },
      {
        "date": "2022-12-15T09:31:00.000Z",
        "voteCount": 2,
        "content": "i think this question will become obsolete since Cloud debugger will be deprecated: Cloud Debugger is deprecated and will be shutdown May 31, 2023. See the deprecations page and release notes for more information. \nCloud Debugger is deprecated and is scheduled for shutdown on May 31 2023. For an alternative, use the open source CLI tool, Snapshot Debugger.\nhttps://cloud.google.com/debugger/docs/release-notes\n\nIn thi context i'll say D"
      },
      {
        "date": "2022-11-09T00:58:00.000Z",
        "voteCount": 1,
        "content": "\" You want to monitor the application over time to diagnose the problem within the application code\"\n\nIf it's only for moniroting it's B, but it mentions \"within the code\" so it should be D"
      },
      {
        "date": "2023-11-13T12:50:00.000Z",
        "voteCount": 1,
        "content": "But it says that you want to \"monitor the application over time\" first, not that you want to start debugging it already."
      },
      {
        "date": "2022-10-26T01:06:00.000Z",
        "voteCount": 3,
        "content": "D can only be monitored for 24 hours.\nFor long-term monitoring, Cloud Monitoring is the best choice.\nhttps://cloud.google.com/monitoring/docs/monitoring-overview?hl=ja#uptime-checks\nI vote for B."
      },
      {
        "date": "2022-10-17T22:41:00.000Z",
        "voteCount": 2,
        "content": "If it requires just for testing purposes then option D because log points expire after 24 hours automatically, while monitoring keeps metrics for 6 weeks."
      },
      {
        "date": "2022-08-22T21:34:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/debugger/docs/using/logpoints"
      },
      {
        "date": "2022-08-19T23:35:00.000Z",
        "voteCount": 3,
        "content": "D is correct"
      },
      {
        "date": "2022-06-14T00:08:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/debugger/docs/using/logpoints"
      },
      {
        "date": "2022-04-09T04:50:00.000Z",
        "voteCount": 1,
        "content": "Community choice is D"
      },
      {
        "date": "2021-07-18T01:51:00.000Z",
        "voteCount": 3,
        "content": "D) is the answer as the api is not behaving as expected we have to put some logs in order to understand what is going one. So we can analyse logs on a period time. With C) is only one shot not over time."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 23,
    "url": "https://www.examtopics.com/discussions/google/view/25853-exam-professional-cloud-developer-topic-1-question-23/",
    "body": "You want to use the Stackdriver Logging Agent to send an application's log file to Stackdriver from a Compute Engine virtual machine instance.<br>After installing the Stackdriver Logging Agent, what should you do first?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable the Error Reporting API on the project.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGrant the instance full access to all Cloud APIs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the application log file as a custom source.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Stackdriver Logs Export Sink with a filter that matches the application's log entries."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 10,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-03-07T10:02:00.000Z",
        "voteCount": 1,
        "content": "C. Configuring the application log file as a custom source is the crucial step after installing the Stackdriver Logging Agent. This involves specifying the path to the application's log file in the agent's configuration files, so the agent knows where to find and how to parse your custom log files."
      },
      {
        "date": "2023-12-03T08:54:00.000Z",
        "voteCount": 1,
        "content": "it's C"
      },
      {
        "date": "2023-09-18T22:49:00.000Z",
        "voteCount": 1,
        "content": "We need to configure the log source in StackDriver ager to read the logs."
      },
      {
        "date": "2023-08-24T02:52:00.000Z",
        "voteCount": 1,
        "content": "After installing StackDriver agent, you need to configure the new source from which to read the logs to be sent"
      },
      {
        "date": "2023-05-05T05:09:00.000Z",
        "voteCount": 1,
        "content": "first C then D"
      },
      {
        "date": "2022-10-26T01:47:00.000Z",
        "voteCount": 1,
        "content": "API must be allowed to output logs.\nhttps://cloud.google.com/logging/docs/agent/ops-agent/authorization\nFirst, do B, then do C.\n\nI vote for B."
      },
      {
        "date": "2022-11-15T01:28:00.000Z",
        "voteCount": 5,
        "content": "Answer B tells us to authorize the instance to ALL Cloud APIs. I don't see a world where this answer can be right as it breaks the least privilege principle quite heavily."
      },
      {
        "date": "2022-08-19T23:35:00.000Z",
        "voteCount": 3,
        "content": "C is correct"
      },
      {
        "date": "2022-06-14T00:09:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/logging/docs/agent/configuration"
      },
      {
        "date": "2022-01-18T18:57:00.000Z",
        "voteCount": 2,
        "content": "C is right"
      },
      {
        "date": "2022-01-08T02:32:00.000Z",
        "voteCount": 1,
        "content": "Answer is C"
      },
      {
        "date": "2021-06-25T21:27:00.000Z",
        "voteCount": 3,
        "content": "Send to Stackdriver so answer is C then"
      },
      {
        "date": "2021-01-17T14:15:00.000Z",
        "voteCount": 2,
        "content": "Answer is C"
      },
      {
        "date": "2020-11-08T11:27:00.000Z",
        "voteCount": 3,
        "content": "C is my answer."
      },
      {
        "date": "2020-12-24T09:09:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/logging/docs/agent/configuration#streaming_logs_from_additional_inputs"
      },
      {
        "date": "2020-07-15T20:26:00.000Z",
        "voteCount": 3,
        "content": "I think answer should be C unless your application log is in the default log directory\nhttps://cloud.google.com/logging/docs/agent/configuration"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 24,
    "url": "https://www.examtopics.com/discussions/google/view/25854-exam-professional-cloud-developer-topic-1-question-24/",
    "body": "Your company has a BigQuery data mart that provides analytics information to hundreds of employees. One user of wants to run jobs without interrupting important workloads. This user isn't concerned about the time it takes to run these jobs. You want to fulfill this request while minimizing cost to the company and the effort required on your part.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAsk the user to run the jobs as batch jobs.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a separate project for the user to run jobs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd the user as a job.user role in the existing project.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAllow the user to run jobs when important workloads are not running."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 12,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-06-25T21:36:00.000Z",
        "voteCount": 11,
        "content": "Option A makes the most sense\n\nB is wrong since it will incur more costs which is not what the qn wants\nC is definitely out as creating roles is not what the qn is asking for\nD is wrong as it would not minimise effort"
      },
      {
        "date": "2020-07-15T20:34:00.000Z",
        "voteCount": 5,
        "content": "Answer is A\nhttps://cloud.google.com/bigquery/docs/running-queries#batch"
      },
      {
        "date": "2021-05-19T08:07:00.000Z",
        "voteCount": 1,
        "content": "this seems like the perfect scenario for batch jobs"
      },
      {
        "date": "2024-03-07T10:09:00.000Z",
        "voteCount": 1,
        "content": "A. Ask the user to run the jobs as batch jobs.\n\nRunning BigQuery jobs as batch jobs is a good solution when there is no concern about how long it takes to complete these jobs. Batch jobs are executed when BigQuery has available resources, which ensures that they do not interfere with high-priority workloads. This is also a cost-effective solution since it does not require additional resources or the overhead of managing a separate project. BigQuery automatically prioritizes interactive jobs over batch jobs, so important workloads are less likely to be interrupted."
      },
      {
        "date": "2023-09-18T22:51:00.000Z",
        "voteCount": 1,
        "content": "Batch jobs in BigQuery are not subject to the usual quota limits and do not count towards your concurrent rate limit, which makes them suitable for running large queries and reducing costs. They are executed when system resources become available, so there might be a delay, but since the user isn\u2019t concerned about the time it takes to run these jobs, this would be a suitable solution"
      },
      {
        "date": "2023-05-21T11:53:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer:A"
      },
      {
        "date": "2023-05-12T23:47:00.000Z",
        "voteCount": 2,
        "content": "Definitly the correct answer is A"
      },
      {
        "date": "2023-01-07T23:32:00.000Z",
        "voteCount": 1,
        "content": "Option A is the correct answer. By running the jobs as batch jobs, the user can specify a priority level for their jobs, allowing them to be run when system resources are available. This minimizes the impact on important workloads and allows the user to run their jobs without interrupting other users. Additionally, batch jobs are generally less expensive to run than interactive queries, so this option would also minimize cost to the company. Option B is not a good solution because it would involve creating a separate project for the user to run their jobs, which would add unnecessary complexity and effort. Option C is not a good solution because the job.user role does not provide any additional permissions beyond those of the bigquery.user role, which the user likely already has. Option D is not a good solution because it would require manual intervention to determine when important workloads are not running, which would be difficult to manage and could lead to delays in running the user's jobs."
      },
      {
        "date": "2022-11-30T10:31:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2022-08-19T23:36:00.000Z",
        "voteCount": 2,
        "content": "A is correct"
      },
      {
        "date": "2022-01-18T18:59:00.000Z",
        "voteCount": 1,
        "content": "A is right"
      },
      {
        "date": "2022-01-08T02:42:00.000Z",
        "voteCount": 4,
        "content": "A is more suitable answer here"
      },
      {
        "date": "2020-11-08T11:41:00.000Z",
        "voteCount": 3,
        "content": "A is best answer"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 25,
    "url": "https://www.examtopics.com/discussions/google/view/21572-exam-professional-cloud-developer-topic-1-question-25/",
    "body": "You want to notify on-call engineers about a service degradation in production while minimizing development time.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Function to monitor resources and raise alerts.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Pub/Sub to monitor resources and raise alerts.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Stackdriver Error Reporting to capture errors and raise alerts.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Stackdriver Monitoring to monitor resources and raise alerts.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-05-29T05:00:00.000Z",
        "voteCount": 11,
        "content": "I don't think the correct answer is A) Cloud Functions are not about monitoring at all, but I have found one mention of using cloud functions for monitoring: https://cloud.google.com/solutions/serverless-web-performance-monitoring-using-cloud-functions . But the mentioned article is about WEB page performance and it does require a lot of efforts. The question does not have info about the kind of service to monitor, so I think the answer should be D) - \"Use Stackdriver Monitoring to monitor resources and raise alerts\""
      },
      {
        "date": "2021-06-25T21:38:00.000Z",
        "voteCount": 9,
        "content": "This is D for sure"
      },
      {
        "date": "2024-03-07T10:12:00.000Z",
        "voteCount": 1,
        "content": "D. Use Stackdriver Monitoring to monitor resources and raise alerts.\n\nStackdriver Monitoring provides out-of-the-box and custom monitoring capabilities for Google Cloud resources and applications. It allows you to create alerting policies that notify you when certain system metrics violate user-defined thresholds. This is a quick and effective way to set up alerts for resource monitoring and service degradation without the need for extensive development time."
      },
      {
        "date": "2023-09-18T22:54:00.000Z",
        "voteCount": 1,
        "content": "Stackdriver Monitoring is the best option here."
      },
      {
        "date": "2023-05-05T05:30:00.000Z",
        "voteCount": 1,
        "content": "D\nError Reporting is not about service degradation, more, Error Reporting uses Monitoring to send alerts.\nhttps://cloud.google.com/error-reporting/docs/notifications"
      },
      {
        "date": "2023-01-02T04:21:00.000Z",
        "voteCount": 1,
        "content": "D is correct for monitoring.\nI'm baffled by the \"correct\" answers given by the site, 80% of the time they are wrong."
      },
      {
        "date": "2022-11-30T11:51:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2022-08-19T23:36:00.000Z",
        "voteCount": 3,
        "content": "D is correct"
      },
      {
        "date": "2022-01-18T19:05:00.000Z",
        "voteCount": 4,
        "content": "D is right one"
      },
      {
        "date": "2022-04-14T06:54:00.000Z",
        "voteCount": 1,
        "content": "why not C?"
      },
      {
        "date": "2022-07-31T07:51:00.000Z",
        "voteCount": 1,
        "content": "Because \"service DEGRADATION\" is in question, not errors."
      },
      {
        "date": "2022-01-08T02:44:00.000Z",
        "voteCount": 3,
        "content": "StackDriver Monitoring should be used to monitor and raising the disputes."
      },
      {
        "date": "2022-01-02T10:05:00.000Z",
        "voteCount": 1,
        "content": "This is D for sure"
      },
      {
        "date": "2021-06-21T19:13:00.000Z",
        "voteCount": 2,
        "content": "D - https://cloud.google.com/blog/products/gcp/drilling-down-into-stackdriver-service-monitoring"
      },
      {
        "date": "2020-11-08T11:45:00.000Z",
        "voteCount": 5,
        "content": "D is correct answer here."
      },
      {
        "date": "2020-09-04T23:44:00.000Z",
        "voteCount": 4,
        "content": "Answer is D"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 26,
    "url": "https://www.examtopics.com/discussions/google/view/21457-exam-professional-cloud-developer-topic-1-question-26/",
    "body": "You are writing a single-page web application with a user-interface that communicates with a third-party API for content using XMLHttpRequest. The data displayed on the UI by the API results is less critical than other data displayed on the same web page, so it is acceptable for some requests to not have the API data displayed in the UI. However, calls made to the API should not delay rendering of other parts of the user interface. You want your application to perform well when the API response is an error or a timeout.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the asynchronous option for your requests to the API to false and omit the widget displaying the API results when a timeout or error is encountered.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the asynchronous option for your request to the API to true and omit the widget displaying the API results when a timeout or error is encountered.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCatch timeout or error exceptions from the API call and keep trying with exponential backoff until the API response is successful.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCatch timeout or error exceptions from the API call and display the error response in the UI widget."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-06-10T08:15:00.000Z",
        "voteCount": 11,
        "content": "Answer is B.\nApi should not delay rendering: asynchronous\nApplication perform well when Api error or timeout: omit the widget"
      },
      {
        "date": "2021-07-30T22:25:00.000Z",
        "voteCount": 6,
        "content": "Correct answer is B\n\nAsynchronous handling provides the ability to call the API in the background without blocking the rendering of other elements. If the response is received it can be rendered or omitted if a timeout occurs."
      },
      {
        "date": "2024-03-07T10:19:00.000Z",
        "voteCount": 1,
        "content": "B. Asynchronous requests allow the browser to continue processing other tasks while waiting for the API response. If the response is an error or a timeout, you can handle this gracefully by not displaying the widget or showing a message indicating that the data couldn't be loaded. This way, the performance of the rest of your page remains unaffected."
      },
      {
        "date": "2023-10-26T15:29:00.000Z",
        "voteCount": 1,
        "content": "About synchronous=true is more comfortable for user experience."
      },
      {
        "date": "2023-09-18T22:56:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer B."
      },
      {
        "date": "2023-08-24T04:22:00.000Z",
        "voteCount": 1,
        "content": "Setting the asynchronous option to true means that the requests will not block the main thread and will be executed in the background.  Furthermore, so as written in the description the widget can be omitted"
      },
      {
        "date": "2023-01-07T00:22:00.000Z",
        "voteCount": 1,
        "content": "A is not the correct answer because setting the asynchronous option for the API request to false will block rendering of the user interface until the API response is received. This can cause a delay in rendering other parts of the user interface and negatively impact the performance of the application. \n\nB is the correct answer because setting the asynchronous option for the API request to true allows the user interface to continue rendering while the API request is being processed, which improves the performance of the application. Omitting the widget displaying the API results when a timeout or error is encountered allows the application to continue functioning without waiting for a successful API response."
      },
      {
        "date": "2022-08-19T23:36:00.000Z",
        "voteCount": 3,
        "content": "B is correct"
      },
      {
        "date": "2022-01-18T19:14:00.000Z",
        "voteCount": 1,
        "content": "B is right one"
      },
      {
        "date": "2022-01-08T02:46:00.000Z",
        "voteCount": 1,
        "content": "Understanding the question correctly, the answer should be B"
      },
      {
        "date": "2021-07-04T04:41:00.000Z",
        "voteCount": 2,
        "content": "In will vote C, as we can catch the error and retry api again, it is like we us amazone we select a project and we see pricing and other content as the picture are loaded asyn."
      },
      {
        "date": "2021-02-07T06:56:00.000Z",
        "voteCount": 1,
        "content": "can't achieve using synchronous requests, so answer must be B."
      },
      {
        "date": "2020-11-08T11:55:00.000Z",
        "voteCount": 2,
        "content": "Answer is B"
      },
      {
        "date": "2020-05-29T05:10:00.000Z",
        "voteCount": 4,
        "content": "It should be B), isn't it? \nProposed answer A) uses synchronous behaviour so will block execution, it contradicts the question"
      },
      {
        "date": "2020-05-27T07:34:00.000Z",
        "voteCount": 1,
        "content": "\"calls made to the API should not delay rendering\" -&gt; so A cannot be the answer as it makes Synchronous requests. B is a better option"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 27,
    "url": "https://www.examtopics.com/discussions/google/view/69060-exam-professional-cloud-developer-topic-1-question-27/",
    "body": "You are creating a web application that runs in a Compute Engine instance and writes a file to any user's Google Drive. You need to configure the application to authenticate to the Google Drive API. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an OAuth Client ID that uses the https://www.googleapis.com/auth/drive.file scope to obtain an access token for each user.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an OAuth Client ID with delegated domain-wide authority.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the App Engine service account and https://www.googleapis.com/auth/drive.file scope to generate a signed JSON Web Token (JWT).",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the App Engine service account with delegated domain-wide authority."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 13,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-03-07T10:39:00.000Z",
        "voteCount": 1,
        "content": "A. Use an OAuth Client ID that uses the https://www.googleapis.com/auth/drive.file scope to obtain an access token for each user.\n\nTo write a file to a user's Google Drive from a web application, you need to obtain permission from each user to access their Google Drive account. This is typically done using OAuth 2.0, where users are redirected to a consent screen where they grant your application permission to access their Google Drive with the specified scope."
      },
      {
        "date": "2023-11-26T09:21:00.000Z",
        "voteCount": 1,
        "content": "correct answer is C. In the link you proposed about access view is clearly stated that you can prevent access to the underlying dataset and give access only to data that is in the view after applying the query."
      },
      {
        "date": "2023-09-18T22:58:00.000Z",
        "voteCount": 1,
        "content": "A is Correct."
      },
      {
        "date": "2023-01-09T00:04:00.000Z",
        "voteCount": 2,
        "content": "A Because need to allow all users so not link to a domain"
      },
      {
        "date": "2023-01-07T00:27:00.000Z",
        "voteCount": 1,
        "content": "I would've chosen option B if all users are in the same domain, it allows the application to authenticate to the Google Drive API with domain-wide authority, meaning that it will be able to access all users' Google Drive accounts within the domain. This is necessary because the application needs to be able to write a file to any user's Google Drive."
      },
      {
        "date": "2023-01-07T00:27:00.000Z",
        "voteCount": 1,
        "content": "But the question said any user (could be the same domain or different domains), In that case, option B would not be the best choice because it only allows for domain-wide authority. Instead, option A would be the best choice because it allows the application to obtain an access token for each individual user, regardless of whether they are in the same domain or a different domain. This ensures that the application has the necessary permissions to write a file to each user's Google Drive."
      },
      {
        "date": "2022-12-06T11:45:00.000Z",
        "voteCount": 2,
        "content": "A is correct"
      },
      {
        "date": "2022-08-22T22:31:00.000Z",
        "voteCount": 2,
        "content": "A is correct for me.\nhttps://developers.google.com/drive/api/guides/about-auth: \"So, when possible, use \"recommended\" scopes as they narrow access to specific functionality needed by an app. In most cases, providing narrow access means using the https://www.googleapis.com/auth/drive.file per-file access scope\" plus each user need their token to acces their own files."
      },
      {
        "date": "2022-08-19T23:37:00.000Z",
        "voteCount": 3,
        "content": "A is correct"
      },
      {
        "date": "2022-01-22T15:11:00.000Z",
        "voteCount": 2,
        "content": "Option A"
      },
      {
        "date": "2022-01-08T02:48:00.000Z",
        "voteCount": 2,
        "content": "A is correct because each user should have its own access token rather giving delegated wide domain access."
      },
      {
        "date": "2021-12-30T05:06:00.000Z",
        "voteCount": 2,
        "content": "A is the most suitable answer in my opinion. Auth tokens should be requested per user (So for each user, a token is requested by the application and the user needs to authorise the application)."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 28,
    "url": "https://www.examtopics.com/discussions/google/view/36490-exam-professional-cloud-developer-topic-1-question-28/",
    "body": "You are creating a Google Kubernetes Engine (GKE) cluster and run this command:<br><img src=\"/assets/media/exam-media/04137/0001700001.jpg\" class=\"in-exam-image\"><br>The command fails with the error:<br><img src=\"/assets/media/exam-media/04137/0001700002.jpg\" class=\"in-exam-image\"><br>You want to resolve the issue. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRequest additional GKE quota in the GCP Console.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRequest additional Compute Engine quota in the GCP Console.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOpen a support case to request additional GKE quota.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDecouple services in the cluster, and rewrite new clusters to function with fewer cores."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 15,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-03-07T10:49:00.000Z",
        "voteCount": 1,
        "content": "The issue with the command for creating the Google Kubernetes Engine (GKE) cluster is that it fails due to insufficient regional quota for CPUs. GKE clusters utilize Compute Engine resources, so when you encounter a quota issue like this, it is related to the Compute Engine quotas, not directly to GKE.\nTo resolve the issue, you should: B. Request additional Compute Engine quota in the GCP Console.\nCompute Engine quotas are set per region and include resources like CPUs, GPUs, and disk. When you create a GKE cluster, you're actually creating Compute Engine instances that will serve as nodes for the cluster. If your project doesn't have enough quota for the CPUs required to create the cluster, you need to request additional quota for CPUs in the relevant region."
      },
      {
        "date": "2023-09-18T23:00:00.000Z",
        "voteCount": 1,
        "content": "GKE uses Compute Engine so we need to increase Compute Engine Quota."
      },
      {
        "date": "2023-01-07T00:29:00.000Z",
        "voteCount": 1,
        "content": "Option A is incorrect because the error message mentions Compute Engine quota, not GKE quota. Option C is incorrect because you can request additional quota through the GCP Console, rather than opening a support case. Option D is not a solution to the issue, as it does not address the shortage of Compute Engine quota.\n\nCorrect answer B: you should request additional Compute Engine quota in the GCP Console."
      },
      {
        "date": "2022-10-24T03:48:00.000Z",
        "voteCount": 1,
        "content": "No such thing as a GKE quota"
      },
      {
        "date": "2022-10-18T10:39:00.000Z",
        "voteCount": 1,
        "content": "B is the most appropriate answer"
      },
      {
        "date": "2022-08-22T22:40:00.000Z",
        "voteCount": 4,
        "content": "The GKE node are Compute Engine instances, so if you need more CPUs you need to ask more quota of these.\n\nAsnwer is B for me."
      },
      {
        "date": "2022-08-19T23:37:00.000Z",
        "voteCount": 3,
        "content": "B is correct"
      },
      {
        "date": "2022-05-26T00:37:00.000Z",
        "voteCount": 3,
        "content": "B - According to documentation https://cloud.google.com/kubernetes-engine/docs/how-to/node-upgrades-quota (last chapter)"
      },
      {
        "date": "2022-01-18T19:16:00.000Z",
        "voteCount": 2,
        "content": "B is best one"
      },
      {
        "date": "2022-01-08T02:51:00.000Z",
        "voteCount": 1,
        "content": "As the error is refering CPU, the correct answer is B"
      },
      {
        "date": "2021-06-25T22:23:00.000Z",
        "voteCount": 4,
        "content": "https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-architecture:\n\"A cluster typically has one or more nodes, which are the worker machines that run your containerized applications and other workloads. The individual machines are Compute Engine VM instances that GKE creates on your behalf when you create a cluster.\"\n\nError message mentions \"CPU\" so this would refer to Compute Engine VMs\n\nAnswer is B"
      },
      {
        "date": "2020-12-29T23:31:00.000Z",
        "voteCount": 1,
        "content": "B is correct - https://cloud.google.com/kubernetes-engine/quotas#limits_per_cluster"
      },
      {
        "date": "2020-11-08T12:04:00.000Z",
        "voteCount": 3,
        "content": "Correct answer would be B, as its for number of node,"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 29,
    "url": "https://www.examtopics.com/discussions/google/view/55800-exam-professional-cloud-developer-topic-1-question-29/",
    "body": "You are parsing a log file that contains three columns: a timestamp, an account number (a string), and a transaction amount (a number). You want to calculate the sum of all transaction amounts for each unique account number efficiently.<br>Which data structure should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA linked list",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA hash table\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA two-dimensional array",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA comma-delimited string"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 8,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-09-18T23:01:00.000Z",
        "voteCount": 1,
        "content": "Hash Table will store unique values."
      },
      {
        "date": "2023-01-09T00:06:00.000Z",
        "voteCount": 1,
        "content": "B. A hash table for the efficient to find a spzcific number."
      },
      {
        "date": "2023-01-07T00:31:00.000Z",
        "voteCount": 2,
        "content": "B. A hash table. A hash table allows for fast insertion and lookup of data, which would be useful in this case for quickly looking up the transaction amount for a given account number and adding it to the total. A linked list, two-dimensional array, and comma-delimited string would not be as efficient for this purpose."
      },
      {
        "date": "2022-10-14T06:58:00.000Z",
        "voteCount": 1,
        "content": "A and C are obviously wrong.\n\nWith hash tables, you cannot store multiple values (amounts) in single key (account number), but this is exactly what you need to do.\n\nTwo dimensional array can be used to store all the couples Account-amount (timestamp is useless).\nSo my selected answer is C."
      },
      {
        "date": "2023-11-14T08:11:00.000Z",
        "voteCount": 1,
        "content": "The value is not limited to Sting type, though.\nWhat about storing unique account as a key, while keeping timestamp and transaction amount in an array as its value? ;)"
      },
      {
        "date": "2023-11-14T08:15:00.000Z",
        "voteCount": 1,
        "content": "The details don't really matter here - the account number has to be unique, so hash table is the only way from the provided options."
      },
      {
        "date": "2022-08-19T23:37:00.000Z",
        "voteCount": 3,
        "content": "B is correct"
      },
      {
        "date": "2022-04-09T05:38:00.000Z",
        "voteCount": 3,
        "content": "Hash table with the account number as the key, the timestamp is useless for this question, so we can safely discard it."
      },
      {
        "date": "2021-06-24T16:54:00.000Z",
        "voteCount": 1,
        "content": "in this case, if you use hashtable, the key must be account name+ timestamp, so I think linkedlist is better"
      },
      {
        "date": "2022-05-26T00:38:00.000Z",
        "voteCount": 1,
        "content": "You don't need to use timestamp for this request."
      },
      {
        "date": "2021-06-21T19:21:00.000Z",
        "voteCount": 2,
        "content": "Hash Table seems right - https://open4tech.com/array-vs-linked-list-vs-hash-table/"
      },
      {
        "date": "2021-06-25T22:32:00.000Z",
        "voteCount": 2,
        "content": "I agree with you on this one"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 30,
    "url": "https://www.examtopics.com/discussions/google/view/21578-exam-professional-cloud-developer-topic-1-question-30/",
    "body": "Your company has a BigQuery dataset named \"Master\" that keeps information about employee travel and expenses. This information is organized by employee department. That means employees should only be able to view information for their department. You want to apply a security framework to enforce this requirement with the minimum number of steps.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a separate dataset for each department. Create a view with an appropriate WHERE clause to select records from a particular dataset for the specific department. Authorize this view to access records from your Master dataset. Give employees the permission to this department-specific dataset.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a separate dataset for each department. Create a data pipeline for each department to copy appropriate information from the Master dataset to the specific dataset for the department. Give employees the permission to this department-specific dataset.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a dataset named Master dataset. Create a separate view for each department in the Master dataset. Give employees access to the specific view for their department.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a dataset named Master dataset. Create a separate table for each department in the Master dataset. Give employees access to the specific table for their department."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 11,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-03-08T23:29:00.000Z",
        "voteCount": 15,
        "content": "For me option c is correct.\ncreate view is easy on one dataset with appropriate where clause. And give permission to department.\nCreate different dataset(option A) for department is create more steps where question denying  it."
      },
      {
        "date": "2020-05-29T06:57:00.000Z",
        "voteCount": 9,
        "content": "I think that answer A) is better than B)\nAuthorized views being in the department-specific dataset will be able to read data from the master dataset(https://cloud.google.com/bigquery/docs/share-access-views). And Cloud IAM can set access on dataset level (https://cloud.google.com/bigquery/docs/dataset-access-controls)"
      },
      {
        "date": "2023-11-26T09:20:00.000Z",
        "voteCount": 1,
        "content": "But correct answer is C. In the link you proposed about access view is clearly stated that you can prevent access to the underlying dataset and give access only to data that is in the view after applying the query."
      },
      {
        "date": "2024-03-16T14:55:00.000Z",
        "voteCount": 1,
        "content": "I think that \"A\" is the best solution, \"C\" could be too but the dataset Master already exists."
      },
      {
        "date": "2024-03-07T10:57:00.000Z",
        "voteCount": 1,
        "content": "C. Create a dataset named Master dataset. Create a separate view for each department in the Master dataset. Give employees access to the specific view for their department.\n\nThis option is the most straightforward and requires the fewest steps to implement row-level security in BigQuery. By creating a view for each department with an appropriate WHERE clause that filters records based on the department, you can ensure that employees only see data that's relevant to them. The views act as a secure interface to the underlying data. You then grant each employee access to the view of their respective department. This method minimizes the number of datasets and tables you have to manage and leverages BigQuery's built-in access control mechanisms."
      },
      {
        "date": "2024-02-13T01:07:00.000Z",
        "voteCount": 1,
        "content": "Handling one, big dataset is surely easier than maintaining many smaller ones."
      },
      {
        "date": "2024-01-13T09:30:00.000Z",
        "voteCount": 1,
        "content": "rly, I don't understand why C says \"Create a dataset name Master\"  wtf, I have the dataset already"
      },
      {
        "date": "2023-11-26T09:22:00.000Z",
        "voteCount": 1,
        "content": "correct answer is C. In the link you proposed about access view is clearly stated that you can prevent access to the underlying dataset and give access only to data that is in the view after applying the query."
      },
      {
        "date": "2023-11-26T09:21:00.000Z",
        "voteCount": 1,
        "content": "correct answer is C. In the link you proposed about access view is clearly stated that you can prevent access to the underlying dataset and give access only to data that is in the view after applying the query. (https://cloud.google.com/bigquery/docs/share-access-views)"
      },
      {
        "date": "2023-09-19T01:38:00.000Z",
        "voteCount": 3,
        "content": "This approach allows you to maintain a single \u201cmaster\u201d dataset, while using views to control access to data based on department. This minimizes the number of steps required, as you don\u2019t need to create separate datasets or data pipelines for each department."
      },
      {
        "date": "2023-04-27T08:19:00.000Z",
        "voteCount": 1,
        "content": "Authorized views. So A since is the only one using authorized views. It may be a extra unnecessary step to create a dataset for each department. But it is a way to grant permission to users in this department, and keeps all in order..."
      },
      {
        "date": "2023-01-09T00:09:00.000Z",
        "voteCount": 1,
        "content": "correct answer c. the view answer the need of acess A is elminited because create dataset by department is more steps."
      },
      {
        "date": "2023-01-07T00:32:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is C. By creating a separate view for each department in the Master dataset, you can enforce the requirement that employees should only be able to view information for their department. This is the minimum number of steps required to implement this security framework. Option A is incorrect because it involves creating separate datasets for each department, which is unnecessary. Option B is incorrect because it involves creating data pipelines for each department, which is unnecessary. Option D is incorrect because it involves creating separate tables for each department, which is unnecessary."
      },
      {
        "date": "2022-08-19T23:37:00.000Z",
        "voteCount": 2,
        "content": "C is correct"
      },
      {
        "date": "2022-07-04T07:24:00.000Z",
        "voteCount": 1,
        "content": "It should be C.\nWhy create a separate dataset when you can get it done simply by creating a view in the same dataset."
      },
      {
        "date": "2022-06-29T01:15:00.000Z",
        "voteCount": 1,
        "content": "I vote C. Least steps compared to A."
      },
      {
        "date": "2022-05-26T00:43:00.000Z",
        "voteCount": 1,
        "content": "Why not C? Less steps. A unique dataset and n view for each department. I thinks is smarter and faster as approach."
      },
      {
        "date": "2022-01-18T19:24:00.000Z",
        "voteCount": 1,
        "content": "A is right"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 31,
    "url": "https://www.examtopics.com/discussions/google/view/36495-exam-professional-cloud-developer-topic-1-question-31/",
    "body": "You have an application in production. It is deployed on Compute Engine virtual machine instances controlled by a managed instance group. Traffic is routed to the instances via a HTTP(s) load balancer. Your users are unable to access your application. You want to implement a monitoring technique to alert you when the application is unavailable.<br>Which technique should you choose?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSmoke tests",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStackdriver uptime checks\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Load Balancing - heath checks",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tManaged instance group - heath checks"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-06-24T17:00:00.000Z",
        "voteCount": 5,
        "content": "C,D can both check but not 'alert', so I think the answer is B"
      },
      {
        "date": "2023-09-19T01:42:00.000Z",
        "voteCount": 1,
        "content": "Stackdriver Uptime Check is the correct option as we can configure it to send an alert when the service is down."
      },
      {
        "date": "2023-04-27T08:24:00.000Z",
        "voteCount": 1,
        "content": "Alert. So B."
      },
      {
        "date": "2022-08-19T23:38:00.000Z",
        "voteCount": 3,
        "content": "B is correct"
      },
      {
        "date": "2022-01-18T19:29:00.000Z",
        "voteCount": 2,
        "content": "B is right one"
      },
      {
        "date": "2021-07-17T18:41:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/load-balancing/docs/l7-internal\n\n\"If a backend becomes unhealthy, traffic is automatically redirected to healthy backends within the same region. If all backends are unhealthy, the load balancer returns an HTTP 503 Service Unavailable response.\"\n\"One or more backends must be connected to the backend service. Because the scope of an internal HTTP(S) load balancer is regional, not global, clients and backend VMs or endpoints must all be in the same region. Backends can be instance groups or NEGs in any of the following configurations:\nManaged instance groups (zonal or regional)\"\n\nI would take C as the answer since the application runs on the MIG and traffic is being controlled by the load balancer"
      },
      {
        "date": "2021-07-30T22:11:00.000Z",
        "voteCount": 6,
        "content": "Disregard what I said about C being the answer\n\nCorrect answer is B as Stackdriver or Cloud Monitoring uptime checks can be used to check if the application is unavailable.\n\nhttps://cloud.google.com/monitoring/uptime-checks:\n\"An uptime check is a request sent to a resource to see if it responds. You can use uptime checks to determine the availability of a VM instance, an App Engine service, a URL, or an AWS load balancer.\""
      },
      {
        "date": "2021-06-30T02:55:00.000Z",
        "voteCount": 1,
        "content": "B. Uptime check can provide the functionality of control the status of the VMs"
      },
      {
        "date": "2020-11-08T12:29:00.000Z",
        "voteCount": 2,
        "content": "B is correct answer, Uptime provide you a machanism to do halth check on URL."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 32,
    "url": "https://www.examtopics.com/discussions/google/view/25862-exam-professional-cloud-developer-topic-1-question-32/",
    "body": "You are load testing your server application. During the first 30 seconds, you observe that a previously inactive Cloud Storage bucket is now servicing 2000 write requests per second and 7500 read requests per second. Your application is now receiving intermittent 5xx and 429 HTTP responses from the Cloud Storage<br>JSON API as the demand escalates. You want to decrease the failed responses from the Cloud Storage API.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDistribute the uploads across a large number of individual storage buckets.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the XML API instead of the JSON API for interfacing with Cloud Storage.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPass the HTTP response codes back to clients that are invoking the uploads from your application.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLimit the upload rate from your application clients so that the dormant bucket's peak request rate is reached more gradually.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-07-15T22:48:00.000Z",
        "voteCount": 12,
        "content": "Answer is D\nhttps://cloud.google.com/storage/docs/request-rate#ramp-up"
      },
      {
        "date": "2021-06-26T17:56:00.000Z",
        "voteCount": 9,
        "content": "\"If you run into any issues such as increased latency or error rates, pause your ramp-up or reduce the request rate temporarily in order to give Cloud Storage more time to scale your bucket. You should use exponential backoff to retry your requests when:\n\nReceiving errors with 5xx and 429 response codes.\nReceiving errors with 408 response codes when performing resumable uploads.\""
      },
      {
        "date": "2024-03-07T12:59:00.000Z",
        "voteCount": 1,
        "content": "D. Limit the upload rate from your application clients so that the dormant bucket's peak request rate is reached more gradually.\n\nGoogle Cloud Storage buckets have throughput limits that ramp up as sustained traffic increases, especially for previously inactive buckets. When you start sending traffic to an inactive bucket, it takes time for Cloud Storage to automatically scale to accommodate the sudden increase in traffic. By gradually increasing the traffic to the bucket, you give the Cloud Storage infrastructure time to scale and handle the increased load without failing requests."
      },
      {
        "date": "2023-09-19T01:59:00.000Z",
        "voteCount": 1,
        "content": "Google Cloud Storage buckets have an initial limit on the request rate. If a bucket is inactive for a period of time, it will have a lower limit. As traffic increases, Google Cloud Storage dynamically increases the request rate limit. This process can take several minutes. If the traffic increases too quickly, you may see intermittent 5xx and 429 HTTP responses. By limiting the upload rate from your application clients, you allow the bucket\u2019s peak request rate to increase more gradually, reducing the chance of receiving these errors."
      },
      {
        "date": "2023-01-07T00:36:00.000Z",
        "voteCount": 1,
        "content": "To decrease the failed responses from the Cloud Storage API, you should limit the upload rate from your application clients so that the dormant bucket's peak request rate is reached more gradually. This will help prevent the bucket from being overwhelmed by a sudden increase in requests. \n\nA, distributing the uploads across a large number of individual storage buckets, may not necessarily decrease the failed responses and could potentially increase the complexity of the system.\n\nB, using the XML API instead of the JSON API, may not necessarily improve performance and could require significant changes to the application. \n\nC, passing the HTTP response codes back to clients, may not address the root cause of the issue and could lead to further errors."
      },
      {
        "date": "2022-12-06T12:02:00.000Z",
        "voteCount": 2,
        "content": "D is correct"
      },
      {
        "date": "2022-08-19T23:38:00.000Z",
        "voteCount": 3,
        "content": "D is correct"
      },
      {
        "date": "2022-07-19T07:51:00.000Z",
        "voteCount": 1,
        "content": "Vote for D"
      },
      {
        "date": "2022-05-26T00:59:00.000Z",
        "voteCount": 1,
        "content": "D is the answer according to mlyu"
      },
      {
        "date": "2022-01-18T19:30:00.000Z",
        "voteCount": 1,
        "content": "D is right one"
      },
      {
        "date": "2022-01-08T03:02:00.000Z",
        "voteCount": 1,
        "content": "Answer is D"
      },
      {
        "date": "2021-06-30T03:04:00.000Z",
        "voteCount": 1,
        "content": "For me the right answer is D."
      },
      {
        "date": "2021-01-17T14:58:00.000Z",
        "voteCount": 1,
        "content": "Answer is D"
      },
      {
        "date": "2020-11-08T12:34:00.000Z",
        "voteCount": 1,
        "content": "D is correct answer"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 33,
    "url": "https://www.examtopics.com/discussions/google/view/36504-exam-professional-cloud-developer-topic-1-question-33/",
    "body": "Your application is controlled by a managed instance group. You want to share a large read-only data set between all the instances in the managed instance group. You want to ensure that each instance can start quickly and can access the data set via its filesystem with very low latency. You also want to minimize the total cost of the solution.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMove the data to a Cloud Storage bucket, and mount the bucket on the filesystem using Cloud Storage FUSE.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMove the data to a Cloud Storage bucket, and copy the data to the boot disk of the instance via a startup script.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMove the data to a Compute Engine persistent disk, and attach the disk in read-only mode to multiple Compute Engine virtual machine instances.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMove the data to a Compute Engine persistent disk, take a snapshot, create multiple disks from the snapshot, and attach each disk to its own instance."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-07-18T02:20:00.000Z",
        "voteCount": 7,
        "content": "C: \nhttps://cloud.google.com/compute/docs/disks/sharing-disks-between-vms#use-multi-instances\nShare a disk in read-only mode between multiple VMs \nSharing static data between multiple VMs from one persistent disk is \"less expensive\" than replicating your data to unique disks for individual instances. \n\nhttps://cloud.google.com/compute/docs/disks/gcs-buckets#mount_bucket\nMounting a bucket as a file system\nYou can use the Cloud Storage FUSE tool to mount a Cloud Storage bucket to your Compute Engine instance. The mounted bucket behaves similarly to a persistent disk even though Cloud Storage buckets are object storage. \n\nhttps://github.com/GoogleCloudPlatform/gcsfuse/\nCloud Storage FUSE performance issues: Latency, Rate limit"
      },
      {
        "date": "2023-09-19T02:01:00.000Z",
        "voteCount": 1,
        "content": "Option C is Correct."
      },
      {
        "date": "2022-12-06T12:05:00.000Z",
        "voteCount": 1,
        "content": "A &amp; C are candidates. with very low latency &lt;- C"
      },
      {
        "date": "2023-11-14T12:34:00.000Z",
        "voteCount": 1,
        "content": "C also minimizes the total cost"
      },
      {
        "date": "2022-08-19T23:38:00.000Z",
        "voteCount": 2,
        "content": "C is correct"
      },
      {
        "date": "2022-07-19T08:16:00.000Z",
        "voteCount": 1,
        "content": "A &amp; C are correct answers, however the question states for low latency hence C is the correct one.."
      },
      {
        "date": "2021-07-18T02:02:00.000Z",
        "voteCount": 1,
        "content": "B) is not correct because we want each instance start quickly."
      },
      {
        "date": "2021-06-26T18:15:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/compute/docs/disks/sharing-disks-between-vms:\n\"Maximum attached instances: 2\"\n\nI would take B"
      },
      {
        "date": "2021-07-18T02:04:00.000Z",
        "voteCount": 2,
        "content": "Your link is for write mode not read only !. C) is the answer."
      },
      {
        "date": "2021-07-30T18:21:00.000Z",
        "voteCount": 2,
        "content": "You're right. C it is then"
      },
      {
        "date": "2021-06-24T17:05:00.000Z",
        "voteCount": 1,
        "content": "Both A and C can work, but FUSE will cause more latency"
      },
      {
        "date": "2021-02-18T18:36:00.000Z",
        "voteCount": 4,
        "content": "Option: C is correct. https://cloud.google.com/compute/docs/disks/add-persistent-disk#use-multi-instances\n\nOption-A: Technically will work. But not low latency. https://cloud.google.com/compute/docs/disks/gcs-buckets#mount_bucket"
      },
      {
        "date": "2020-11-08T13:35:00.000Z",
        "voteCount": 1,
        "content": "i will suggest option B keeping cost in mind."
      },
      {
        "date": "2020-11-13T15:25:00.000Z",
        "voteCount": 4,
        "content": "* C is correct"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 34,
    "url": "https://www.examtopics.com/discussions/google/view/21587-exam-professional-cloud-developer-topic-1-question-34/",
    "body": "You are developing an HTTP API hosted on a Compute Engine virtual machine instance that needs to be invoked by multiple clients within the same Virtual<br>Private Cloud (VPC). You want clients to be able to get the IP address of the service.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReserve a static external IP address and assign it to an HTTP(S) load balancing service's forwarding rule. Clients should use this IP address to connect to the service.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReserve a static external IP address and assign it to an HTTP(S) load balancing service's forwarding rule. Then, define an A record in Cloud DNS. Clients should use the name of the A record to connect to the service.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure that clients use Compute Engine internal DNS by connecting to the instance name with the url https://[INSTANCE_NAME].[ZONE].c. [PROJECT_ID].internal/.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure that clients use Compute Engine internal DNS by connecting to the instance name with the url https://[API_NAME]/[API_VERSION]/."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-05-29T08:27:00.000Z",
        "voteCount": 20,
        "content": "My vote is answer C)\n\"Virtual Private Cloud networks on Google Cloud have an internal DNS service that lets instances in the same network access each other by using internal DNS names\" \nThis name can be used for access: [INSTANCE_NAME].[ZONE].c.[PROJECT_ID].internal \nhttps://cloud.google.com/compute/docs/internal-dns#access_by_internal_DNS"
      },
      {
        "date": "2021-06-26T18:26:00.000Z",
        "voteCount": 5,
        "content": "Good find; C is supported by this"
      },
      {
        "date": "2024-03-07T13:12:00.000Z",
        "voteCount": 1,
        "content": "C. Ensure that clients use Compute Engine internal DNS by connecting to the instance name with the URL https://[INSTANCE_NAME].[ZONE].c.[PROJECT_ID].internal/.\n\nThis option allows clients within the same Virtual Private Cloud (VPC) to resolve the Compute Engine instance's internal IP address using internal DNS, which is a feature provided by Compute Engine. The internal DNS name is constructed using the instance name, zone, and project ID, and this DNS entry is automatically created and managed by Google Cloud. This method ensures that traffic between clients and the API does not leave the Google Cloud network, providing lower latency and enhanced security."
      },
      {
        "date": "2023-09-19T02:03:00.000Z",
        "voteCount": 1,
        "content": "This is a simple and effective way to enable communication between services within the same VPC without the need for external IP addresses or load balancing services."
      },
      {
        "date": "2023-01-07T00:39:00.000Z",
        "voteCount": 1,
        "content": "D, connecting to the instance name with the url https://[API_NAME]/[API_VERSION]/, may not work as it is not specified how clients would know the correct API name and version. \n\nC is the correct answer: By connecting to the instance name with the url https://[INSTANCE_NAME].[ZONE].c. [PROJECT_ID].internal/, clients can use Compute Engine's internal DNS to access the API hosted on the virtual machine instance within the same VPC. This will allow clients to access the API with low latency and without the need for a static external IP address."
      },
      {
        "date": "2022-12-06T12:06:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2022-08-29T19:39:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/compute/docs/internal-dns\nThe answer ic C."
      },
      {
        "date": "2022-08-19T23:38:00.000Z",
        "voteCount": 3,
        "content": "C is correct"
      },
      {
        "date": "2022-07-19T08:22:00.000Z",
        "voteCount": 1,
        "content": "vote for C"
      },
      {
        "date": "2022-01-18T19:37:00.000Z",
        "voteCount": 1,
        "content": "Vote for C"
      },
      {
        "date": "2022-01-08T03:10:00.000Z",
        "voteCount": 1,
        "content": "With no doubt, it is C"
      },
      {
        "date": "2022-01-02T11:06:00.000Z",
        "voteCount": 1,
        "content": "right answer is C"
      },
      {
        "date": "2021-06-30T03:08:00.000Z",
        "voteCount": 2,
        "content": "For me the right answer is C."
      },
      {
        "date": "2021-01-17T08:36:00.000Z",
        "voteCount": 4,
        "content": "I vote for C."
      },
      {
        "date": "2020-11-08T14:21:00.000Z",
        "voteCount": 3,
        "content": "my vote for C as well."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 35,
    "url": "https://www.examtopics.com/discussions/google/view/30620-exam-professional-cloud-developer-topic-1-question-35/",
    "body": "Your application is logging to Stackdriver. You want to get the count of all requests on all /api/alpha/* endpoints.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a Stackdriver counter metric for path:/api/alpha/.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a Stackdriver counter metric for endpoint:/api/alpha/*.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExport the logs to Cloud Storage and count lines matching /api/alpha.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExport the logs to Cloud Pub/Sub and count lines matching /api/alpha."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 7,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-09-05T00:24:00.000Z",
        "voteCount": 17,
        "content": "Ans: B\nB have the correct endpoint /api/alpha/*,\nA only get one endpoint counter"
      },
      {
        "date": "2023-01-03T23:33:00.000Z",
        "voteCount": 1,
        "content": "Agree. counter metric with applying regression filter to httpRequest.requestUrl should be able to get the count value. refer to: https://cloud.google.com/logging/docs/log4j2-vulnerability#log4j-search"
      },
      {
        "date": "2023-01-08T08:49:00.000Z",
        "voteCount": 2,
        "content": "B is the correct answer\n\"Create a filter that collects only the log entries that you want to count in your metric using the logging query language. You can also use regular expressions to create your metric's filters.\"\nhttps://cloud.google.com/logging/docs/logs-based-metrics/counter-metrics#console"
      },
      {
        "date": "2020-09-05T00:13:00.000Z",
        "voteCount": 10,
        "content": "Answer should be A"
      },
      {
        "date": "2020-12-24T14:11:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/logging/docs/logs-based-metrics/counter-metrics#console"
      },
      {
        "date": "2021-03-16T19:31:00.000Z",
        "voteCount": 4,
        "content": "a bit confused about A / B, it seems they mean the same thing."
      },
      {
        "date": "2024-07-17T00:18:00.000Z",
        "voteCount": 1,
        "content": "The best approach here is A. Add a Stackdriver counter metric for path:/api/alpha/.\n\nHere's why:\n\nStackdriver Metrics: Stackdriver metrics are specifically designed for tracking and aggregating data points over time. They are ideal for counting events like requests.\nPath-Based Filtering: You can define Stackdriver metrics with specific filters based on the request path. In this case, path:/api/alpha/ will capture all requests matching that pattern.\nEfficient Aggregation: Stackdriver automatically aggregates the metric data, providing you with the total count of requests to the /api/alpha/* endpoints."
      },
      {
        "date": "2024-07-17T00:18:00.000Z",
        "voteCount": 1,
        "content": "Why other options are less suitable:\n\n**B. Endpoint:/api/alpha/*: ** While this might seem like a good option, Stackdriver doesn't typically use the term \"endpoint\" for filtering. It primarily uses \"path\" for request path-based filtering.\nC. Export logs to Cloud Storage and count lines: This is a less efficient and more complex approach. You'd need to write custom code to parse the logs and count the matching lines, which adds overhead and potential for errors.\nD. Export logs to Cloud Pub/Sub and count lines: Similar to option C, this involves exporting logs and then processing them externally, which is less efficient than using Stackdriver metrics.\nIn summary: Adding a Stackdriver counter metric with the path:/api/alpha/ filter is the most efficient and straightforward way to get the count of all requests on all /api/alpha/* endpoints."
      },
      {
        "date": "2024-03-07T13:19:00.000Z",
        "voteCount": 2,
        "content": "A. Add a Stackdriver counter metric for path:/api/alpha/.\n\nIn Google Cloud's operations suite (formerly Stackdriver), you can create custom metrics to count specific events within your logs. You would set up a counter metric to capture and count log entries where the request path matches your specified pattern, such as /api/alpha/*. This would allow you to query and visualize the count of requests to these endpoints directly within Stackdriver Monitoring without the need to export the logs elsewhere.\nB. This option seems to be suggesting the correct action (creating a counter metric), but the syntax endpoint:/api/alpha/* is not correct for Stackdriver Monitoring. Custom metrics in Stackdriver are based on log data and the filter that matches the log entries, so you would specify the filter as part of creating the metric."
      },
      {
        "date": "2024-02-13T01:18:00.000Z",
        "voteCount": 1,
        "content": "If you don't export the metric, then you have nothing to count. \nI choose B because \"endpoint\" is more specific than \"path\"."
      },
      {
        "date": "2023-09-19T02:07:00.000Z",
        "voteCount": 1,
        "content": "Ans: B"
      },
      {
        "date": "2023-08-24T05:00:00.000Z",
        "voteCount": 1,
        "content": "This option will accurately track the number of requests made to all endpoints nested under /api/alpha/*."
      },
      {
        "date": "2023-08-17T04:30:00.000Z",
        "voteCount": 1,
        "content": "B is for counter"
      },
      {
        "date": "2023-02-27T07:54:00.000Z",
        "voteCount": 1,
        "content": "submiting just to confirm community response."
      },
      {
        "date": "2023-02-02T05:20:00.000Z",
        "voteCount": 1,
        "content": "B is the only one"
      },
      {
        "date": "2023-01-10T23:54:00.000Z",
        "voteCount": 1,
        "content": "B is the correct answer"
      },
      {
        "date": "2023-01-09T08:00:00.000Z",
        "voteCount": 1,
        "content": "answer is B with the correct endpoint and the goal of counter metris is to resolve the need to count calls"
      },
      {
        "date": "2023-01-07T00:42:00.000Z",
        "voteCount": 3,
        "content": "Option B is the correct choice because it involves creating a counter metric in Stackdriver specifically for requests to the /api/alpha/* endpoints. This will allow you to track the number of requests to these endpoints and view the data in Stackdriver."
      },
      {
        "date": "2023-01-07T00:42:00.000Z",
        "voteCount": 1,
        "content": "Option C is incorrect because it involves exporting the logs to Cloud Storage and manually counting the lines that match /api/alpha. This is a more time-consuming and error-prone approach compared to using a counter metric in Stackdriver."
      },
      {
        "date": "2023-01-07T00:42:00.000Z",
        "voteCount": 1,
        "content": "Option A is incorrect because the path:/api/alpha/ metric will track requests to any path that starts with /api/alpha/, not just requests to /api/alpha/* endpoints."
      },
      {
        "date": "2023-01-07T00:42:00.000Z",
        "voteCount": 1,
        "content": "Option D is also incorrect for similar reasons. Exporting the logs to Cloud Pub/Sub and counting the lines that match /api/alpha is more time-consuming and error-prone compared to using a counter metric in Stackdriver."
      },
      {
        "date": "2022-08-19T23:39:00.000Z",
        "voteCount": 2,
        "content": "C is correct"
      },
      {
        "date": "2022-04-19T07:21:00.000Z",
        "voteCount": 5,
        "content": "Ans should be B -&gt; (https://cloud.google.com/blog/products/management-tools/cloud-logging-gets-regular-expression-support)\npath=~\"/api/alpha/*\""
      },
      {
        "date": "2021-07-19T18:46:00.000Z",
        "voteCount": 3,
        "content": "ans: a\nhttps://cloud.google.com/logging/docs/view/basic-queries#searching_specific_fields\n\nhttps://cloud.google.com/monitoring/charts/metrics-selector#filter-option\n\nTo match any US zone that ends with \u201ca\u201d, you could use the the regular expression ^us.*.a$."
      },
      {
        "date": "2022-05-26T01:05:00.000Z",
        "voteCount": 1,
        "content": "documentation says: \"ends with a\". This question is different."
      },
      {
        "date": "2021-07-17T18:27:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/logging/docs/logs-based-metrics/troubleshooting#metric-name-restrictions\n\nI would take C"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 36,
    "url": "https://www.examtopics.com/discussions/google/view/21589-exam-professional-cloud-developer-topic-1-question-36/",
    "body": "You want to re-architect a monolithic application so that it follows a microservices model. You want to accomplish this efficiently while minimizing the impact of this change to the business.<br>Which approach should you take?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the application to Compute Engine and turn on autoscaling.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReplace the application's features with appropriate microservices in phases.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRefactor the monolithic application with appropriate microservices in a single effort and deploy it.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBuild a new application with the appropriate microservices separate from the monolith and replace it when it is complete."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 12,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-05-29T08:39:00.000Z",
        "voteCount": 15,
        "content": "The referenced article shows that the correct answer is B)\n\"The migration is done feature by feature, avoiding a large-scale migration event and its associated risks\""
      },
      {
        "date": "2024-03-07T13:34:00.000Z",
        "voteCount": 1,
        "content": "B. Replace the application's features with appropriate microservices in phases.\n\nWhen transitioning from a monolithic application to a microservices architecture, it is generally best to do it incrementally, rather than all at once. This allows you to break down the application into smaller, manageable pieces and make sure each piece is functioning correctly before moving on to the next. It minimizes risk, allows for easier troubleshooting, and reduces the impact on the business because you can gradually shift traffic to the new services as they are tested and deployed."
      },
      {
        "date": "2024-03-07T13:35:00.000Z",
        "voteCount": 2,
        "content": "Here's why the other options are less suitable:\n\nA. Deploying the application to Compute Engine with autoscaling does not change the architecture from monolithic to microservices. It may help with some scaling issues but does not achieve the goal of re-architecting the application.\n\nC. Refactoring the entire monolithic application into microservices in a single effort can be very risky. It can introduce complex issues that are hard to troubleshoot, and if something goes wrong, it could impact the entire business.\n\nD. Building a new application separate from the monolith and replacing it once complete is another approach, but it can be less efficient than replacing in phases. It requires a big-bang cutover, which can be risky. Phased approaches allow for gradual cutover and testing in production with real users, which can lead to a more reliable outcome."
      },
      {
        "date": "2024-02-07T04:31:00.000Z",
        "voteCount": 1,
        "content": "Option B is most logical in this situation."
      },
      {
        "date": "2023-09-19T02:08:00.000Z",
        "voteCount": 1,
        "content": "Option B is most logical in this situation."
      },
      {
        "date": "2023-08-24T05:06:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/architecture/microservices-architecture-refactoring-monoliths"
      },
      {
        "date": "2023-01-07T00:48:00.000Z",
        "voteCount": 1,
        "content": "Option B is the best choice because it allows you to gradually replace the features of the monolithic application with microservices, minimizing the impact on the business. This approach also allows you to test and validate each microservice before fully integrating it into the application."
      },
      {
        "date": "2023-01-07T00:48:00.000Z",
        "voteCount": 1,
        "content": "Option D is not a good choice because building a new application from scratch and replacing the monolith is likely to be a time-consuming and costly process that could disrupt the business. It is generally more efficient to gradually refactor an existing application into a microservices model rather than starting from scratch."
      },
      {
        "date": "2023-01-07T00:48:00.000Z",
        "voteCount": 1,
        "content": "Option C is not a good choice because refactoring the entire application into a microservices model in a single effort is likely to be a complex and risky process that could disrupt the business."
      },
      {
        "date": "2023-01-07T00:48:00.000Z",
        "voteCount": 1,
        "content": "Option A is not a good choice because it does not address the need to refactor the application into a microservices model. Autoscaling may help with resource management, but it does not address the underlying architecture of the application."
      },
      {
        "date": "2022-09-28T04:17:00.000Z",
        "voteCount": 1,
        "content": "Def B, how small is this monolith that it can be converted in 1 day! In addition to the fact that undoubtedly the app will break with this approach."
      },
      {
        "date": "2022-08-19T23:39:00.000Z",
        "voteCount": 3,
        "content": "B is correct"
      },
      {
        "date": "2022-06-29T01:28:00.000Z",
        "voteCount": 3,
        "content": "I vote B. Migrating a monolithic service is best when done feature by feature."
      },
      {
        "date": "2022-06-10T02:45:00.000Z",
        "voteCount": 2,
        "content": "B is the correct. You don't want to replace the monolithic application in one go as C suggests which kind of defeats the purpose."
      },
      {
        "date": "2022-02-10T09:30:00.000Z",
        "voteCount": 4,
        "content": "B strangler pattern"
      },
      {
        "date": "2022-01-19T01:43:00.000Z",
        "voteCount": 1,
        "content": "B is right one, refactor with multiple phases to minimize impact."
      },
      {
        "date": "2021-06-26T18:42:00.000Z",
        "voteCount": 2,
        "content": "Best answer is B here\n\nA is completely wrong\nC and D can be done but the amount of risk involved can be too great"
      },
      {
        "date": "2021-02-09T00:28:00.000Z",
        "voteCount": 1,
        "content": "it should be B"
      },
      {
        "date": "2020-11-08T14:40:00.000Z",
        "voteCount": 1,
        "content": "B is correct answer here."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 37,
    "url": "https://www.examtopics.com/discussions/google/view/21590-exam-professional-cloud-developer-topic-1-question-37/",
    "body": "Your existing application keeps user state information in a single MySQL database. This state information is very user-specific and depends heavily on how long a user has been using an application. The MySQL database is causing challenges to maintain and enhance the schema for various users.<br>Which storage option should you choose?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud SQL",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Storage",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Spanner",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Datastore/Firestore\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 15,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-05-29T08:42:00.000Z",
        "voteCount": 19,
        "content": "Question sais that there are challenges to maintain and enhance schema, so schemaless DB is more preferable, moreover google mention that Datastore/Firestore is good for users profiles (https://cloud.google.com/datastore/docs/concepts/overview#what_its_good_for)\nAnswer: D)"
      },
      {
        "date": "2021-06-26T18:45:00.000Z",
        "voteCount": 1,
        "content": "\"Datastore is ideal for applications that rely on highly available structured data at scale. You can use Datastore to store and query all of the following types of data:\n\nProduct catalogs that provide real-time inventory and product details for a retailer.\nUser profiles that deliver a customized experience based on the user\u2019s past activities and preferences.\nTransactions based on ACID properties, for example, transferring funds from one bank account to another.\""
      },
      {
        "date": "2024-06-28T19:17:00.000Z",
        "voteCount": 1,
        "content": "Resposta D"
      },
      {
        "date": "2024-03-07T13:39:00.000Z",
        "voteCount": 1,
        "content": "D. Cloud Datastore/Firestore\n\nFor user-specific state information that varies significantly and requires a schema that can evolve over time, a NoSQL database like Cloud Datastore or Firestore is typically more appropriate. These databases provide a flexible schema, which allows you to easily make changes as the application evolves and user requirements become more complex."
      },
      {
        "date": "2024-02-13T01:22:00.000Z",
        "voteCount": 1,
        "content": "The last sentence subtly implies a schema-enforced database is not the right solution for this use case."
      },
      {
        "date": "2023-09-19T02:11:00.000Z",
        "voteCount": 1,
        "content": "Since we need a flexible schema we can use Datastore/Firestore"
      },
      {
        "date": "2023-01-12T06:18:00.000Z",
        "voteCount": 3,
        "content": "The question is a bit misleading. If its asking to keep a MySQL storage option then Cloud SQL or Spanner are the only options. However, assuming that they want to move away from schema and also the need for stateful DB I would go for Datastore/Firestore."
      },
      {
        "date": "2023-01-07T00:51:00.000Z",
        "voteCount": 1,
        "content": "Out of the options provided, Cloud Datastore or Cloud Firestore would be the best choice for storing user state information that is very user-specific and depends heavily on how long a user has been using an application. This is because both Cloud Datastore and Cloud Firestore are NoSQL document databases designed for storing, retrieving, and managing semi-structured data at scale. They are well-suited for storing complex, hierarchical data structures and can handle a high volume of read and write operations. Additionally, Cloud Datastore and Cloud Firestore offer strong consistency and automatic scaling, which can help your application handle a high volume of users without requiring significant manual effort to maintain and enhance the schema."
      },
      {
        "date": "2023-01-07T00:52:00.000Z",
        "voteCount": 1,
        "content": "Cloud Storage is not suitable for storing user state information as it is an object storage service that is not designed for storing structured data."
      },
      {
        "date": "2023-01-07T00:52:00.000Z",
        "voteCount": 1,
        "content": "Cloud Spanner is a highly scalable, distributed database system, but it may not be the most efficient solution for storing user state information that depends heavily on how long a user has been using the application."
      },
      {
        "date": "2023-01-07T00:52:00.000Z",
        "voteCount": 1,
        "content": "Cloud SQL (MySQL) is the current storage option that is causing challenges to maintain and enhance the schema, so it is not a suitable solution."
      },
      {
        "date": "2022-09-27T08:51:00.000Z",
        "voteCount": 1,
        "content": "Option D"
      },
      {
        "date": "2022-08-29T19:47:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/datastore/docs/concepts/overview#what_its_good_for -&gt; \"User profiles that deliver a customized experience based on the user\u2019s past activities and preferences\".\nAnswer id D."
      },
      {
        "date": "2022-08-19T23:39:00.000Z",
        "voteCount": 3,
        "content": "D is correct"
      },
      {
        "date": "2022-04-20T11:41:00.000Z",
        "voteCount": 1,
        "content": "D for sure"
      },
      {
        "date": "2022-01-19T01:46:00.000Z",
        "voteCount": 3,
        "content": "D is right one."
      },
      {
        "date": "2022-01-08T03:16:00.000Z",
        "voteCount": 3,
        "content": "D is the answer"
      },
      {
        "date": "2020-11-08T14:45:00.000Z",
        "voteCount": 2,
        "content": "D is correct answer"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 38,
    "url": "https://www.examtopics.com/discussions/google/view/21785-exam-professional-cloud-developer-topic-1-question-38/",
    "body": "You are building a new API. You want to minimize the cost of storing and reduce the latency of serving images.<br>Which architecture should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tApp Engine backed by Cloud Storage",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCompute Engine backed by Persistent Disk",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTransfer Appliance backed by Cloud Filestore",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Content Delivery Network (CDN) backed by Cloud Storage\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 8,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-06-01T09:48:00.000Z",
        "voteCount": 17,
        "content": "Answer D) seems more suitable as Cloud Storage has low cost and CDN provides low serving latency"
      },
      {
        "date": "2020-11-08T14:50:00.000Z",
        "voteCount": 1,
        "content": "Agree this is best answer"
      },
      {
        "date": "2022-01-02T12:00:00.000Z",
        "voteCount": 5,
        "content": "But shouldn't there be something (like Compute Engine / App Engine) between the CDN and Cloud Store?\nThanks ?"
      },
      {
        "date": "2024-07-17T00:30:00.000Z",
        "voteCount": 1,
        "content": "The best architecture for minimizing storage cost and reducing latency for serving images is D. Cloud Content Delivery Network (CDN) backed by Cloud Storage .\n\nHere's why:\n\nCloud CDN: CDNs are designed to cache content closer to users, significantly reducing latency. They also handle the majority of image requests, reducing the load on your backend servers.\nCloud Storage: Cloud Storage is a cost-effective and scalable object storage solution. It's ideal for storing images and other static content.\nCost-Effective: CDNs are optimized for serving static content, making them more cost-effective than running your own servers. Cloud Storage offers tiered pricing, allowing you to optimize storage costs based on access frequency."
      },
      {
        "date": "2024-07-17T00:30:00.000Z",
        "voteCount": 1,
        "content": "Why other options are less suitable:\n\nA. App Engine backed by Cloud Storage: While App Engine can serve images from Cloud Storage, it's not as efficient as a CDN for serving static content. App Engine instances are designed for dynamic applications, and serving images can be resource-intensive.\nB. Compute Engine backed by Persistent Disk: This approach is more expensive and less efficient than using a CDN. Persistent disks are designed for persistent data, not for serving static content at high volumes.\nC. Transfer Appliance backed by Cloud Filestore: Transfer Appliance is for bulk data transfers, not for serving images. Cloud Filestore is a managed file system, not optimized for serving static content.\nIn summary: A CDN backed by Cloud Storage provides the best combination of cost-effectiveness, low latency, and scalability for serving images. It allows you to offload the majority of image requests to the CDN, reducing the load on your backend servers and minimizing storage costs."
      },
      {
        "date": "2024-03-07T13:44:00.000Z",
        "voteCount": 1,
        "content": "D. Cloud Content Delivery Network (CDN) backed by Cloud Storage\n\nUsing Cloud CDN backed by Cloud Storage is a cost-effective and performance-optimized solution for storing and serving images. Cloud Storage provides a durable and highly available object storage solution, while Cloud CDN leverages Google's globally distributed edge points of presence to cache and serve content closer to users, reducing latency."
      },
      {
        "date": "2023-09-19T02:13:00.000Z",
        "voteCount": 1,
        "content": "D is the best option here as CDN will reduce latency."
      },
      {
        "date": "2023-01-07T00:54:00.000Z",
        "voteCount": 2,
        "content": "D. Cloud Content Delivery Network (CDN) backed by Cloud Storage.\n\nA Cloud CDN is a content delivery network that uses Google's globally distributed edge points of presence to accelerate content delivery for websites and applications served out of Google Cloud. Cloud CDN stores and serves content from Google Cloud Storage, which allows for efficient and low-cost storage of images, as well as low latency in serving the images. The other options do not mention low latency or cost-effective storage as their primary benefits."
      },
      {
        "date": "2023-01-07T00:54:00.000Z",
        "voteCount": 1,
        "content": "A (App Engine backed by Cloud Storage) may not be suitable because App Engine may not be optimized for serving images, and it may not offer the lowest cost or latency for serving images. Option B (Compute Engine backed by Persistent Disk) may not be suitable because it may not offer the lowest cost for storing images, and it may not offer the lowest latency for serving images."
      },
      {
        "date": "2023-01-07T00:54:00.000Z",
        "voteCount": 1,
        "content": "C (Transfer Appliance backed by Cloud Filestore) may not be suitable because it is not designed for serving images, but rather for transferring large amounts of data."
      },
      {
        "date": "2023-01-07T00:54:00.000Z",
        "voteCount": 2,
        "content": "D (Cloud Content Delivery Network (CDN) backed by Cloud Storage) is the most suitable option because CDN is optimized for serving images and other static content, and it can reduce the latency of serving images by storing copies of the images closer to the users who are requesting them. In addition, using Cloud Storage to store the images can help minimize the cost of storing the images."
      },
      {
        "date": "2022-12-20T18:35:00.000Z",
        "voteCount": 1,
        "content": "A is correct, the rest how can you write an API?"
      },
      {
        "date": "2022-11-20T04:39:00.000Z",
        "voteCount": 1,
        "content": "I think A because CDN doesn't relate to API but to static resources"
      },
      {
        "date": "2022-09-28T04:34:00.000Z",
        "voteCount": 1,
        "content": "\"Cloud CDN content can be sourced from various types of backends:\n...\nBuckets in Cloud Storage\nAnswer is D"
      },
      {
        "date": "2022-09-27T08:53:00.000Z",
        "voteCount": 1,
        "content": "Answer is D"
      },
      {
        "date": "2022-09-02T03:34:00.000Z",
        "voteCount": 1,
        "content": "CDN is appropriated to serve static files. The right answer is B)"
      },
      {
        "date": "2022-08-19T23:39:00.000Z",
        "voteCount": 3,
        "content": "D is correct"
      },
      {
        "date": "2022-05-19T07:35:00.000Z",
        "voteCount": 2,
        "content": "D - Cloud CDN"
      },
      {
        "date": "2022-05-19T02:30:00.000Z",
        "voteCount": 4,
        "content": "For me A. With CDN i can't write API"
      },
      {
        "date": "2022-01-08T03:18:00.000Z",
        "voteCount": 2,
        "content": "D is more suitable answer in this case."
      },
      {
        "date": "2021-06-26T18:54:00.000Z",
        "voteCount": 3,
        "content": "Answer is D since CDN reduces latency and Cloud Storage is used for images"
      },
      {
        "date": "2022-01-02T12:00:00.000Z",
        "voteCount": 5,
        "content": "But shouldn't there be something (like Compute Engine / App Engine) between the CDN and Cloud Store?\nThanks"
      },
      {
        "date": "2022-09-28T04:35:00.000Z",
        "voteCount": 1,
        "content": "not neeeded see the docsthey say:\nhttps://cloud.google.com/cdn/docs/overview\nThat the CDN can be backed by cloud storage buckets"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 39,
    "url": "https://www.examtopics.com/discussions/google/view/22740-exam-professional-cloud-developer-topic-1-question-39/",
    "body": "Your company's development teams want to use Cloud Build in their projects to build and push Docker images to Container Registry. The operations team requires all Docker images to be published to a centralized, securely managed Docker registry that the operations team manages.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Container Registry to create a registry in each development team's project. Configure the Cloud Build build to push the Docker image to the project's registry. Grant the operations team access to each development team's registry.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a separate project for the operations team that has Container Registry configured. Assign appropriate permissions to the Cloud Build service account in each developer team's project to allow access to the operation team's registry.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a separate project for the operations team that has Container Registry configured. Create a Service Account for each development team and assign the appropriate permissions to allow it access to the operations team's registry. Store the service account key file in the source code repository and use it to authenticate against the operations team's registry.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a separate project for the operations team that has the open source Docker Registry deployed on a Compute Engine virtual machine instance. Create a username and password for each development team. Store the username and password in the source code repository and use it to authenticate against the operations team's Docker registry."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-06-10T06:02:00.000Z",
        "voteCount": 15,
        "content": "I think the correct answer is B)\nContainer Registry is a good choice to store containers in a secure manageable way. It is possible to have ContainerRegistry in One project and push to it from Cloud Build of another project by adding appropriate service account as a member of a Cloud Storage Bucket used to host containers with the role Cloud Build Service Account."
      },
      {
        "date": "2020-11-08T14:57:00.000Z",
        "voteCount": 4,
        "content": "Yes, B is best choice here."
      },
      {
        "date": "2024-03-07T13:52:00.000Z",
        "voteCount": 1,
        "content": "B. Create a separate project for the operations team that has Container Registry configured. Assign appropriate permissions to the Cloud Build service account in each developer team's project to allow access to the operation team's registry.\n\nThis approach aligns with best practices for managing access and centralizing the storage of Docker images while maintaining a high level of security and control"
      },
      {
        "date": "2023-09-19T02:16:00.000Z",
        "voteCount": 1,
        "content": "I would go with B."
      },
      {
        "date": "2023-08-24T05:25:00.000Z",
        "voteCount": 1,
        "content": "The option B is the best solution because it provides a centralized, securely managed Docker registry for all development teams to use, and the Cloud Build service account can be granted the necessary permissions to push Docker images to the registry."
      },
      {
        "date": "2023-05-13T16:59:00.000Z",
        "voteCount": 1,
        "content": "I think B is the correct one as well"
      },
      {
        "date": "2023-01-07T00:57:00.000Z",
        "voteCount": 1,
        "content": "B is the best choice because it allows the operations team to have control over the centralized, securely managed Docker registry while also allowing the development teams to use Cloud Build in their projects. In this option, the operations team can create a separate project with Container Registry configured and grant appropriate permissions to the Cloud Build service account in each developer team's project to allow access to the operations team's registry. This allows the development teams to build and push Docker images to the centralized registry while still following the operations team's requirements."
      },
      {
        "date": "2023-01-07T00:57:00.000Z",
        "voteCount": 1,
        "content": "D is not the best choice because it requires using an open source Docker Registry and creating a username and password for each development team, which may not be secure or efficient."
      },
      {
        "date": "2023-01-07T00:57:00.000Z",
        "voteCount": 1,
        "content": "C is not the best choice because it requires storing the service account key file in the source code repository, which may not be secure."
      },
      {
        "date": "2023-01-07T00:57:00.000Z",
        "voteCount": 1,
        "content": "A is not ideal because it requires granting the operations team access to each development team's registry, which may not be secure."
      },
      {
        "date": "2022-12-06T12:40:00.000Z",
        "voteCount": 1,
        "content": "B is correct. No point deploying 1 CR per project."
      },
      {
        "date": "2022-08-19T23:40:00.000Z",
        "voteCount": 2,
        "content": "B is correct"
      },
      {
        "date": "2022-01-08T03:57:00.000Z",
        "voteCount": 2,
        "content": "B is the suitable answer in this case"
      },
      {
        "date": "2021-06-30T04:23:00.000Z",
        "voteCount": 1,
        "content": "B is the best way to store and share images between different projects.\nFurthermore use of SA is a choice that match with GCP recommendations"
      },
      {
        "date": "2021-06-26T19:01:00.000Z",
        "voteCount": 1,
        "content": "\"centralized, securely managed Docker registry\"\n\nB is the answer"
      },
      {
        "date": "2021-02-09T00:37:00.000Z",
        "voteCount": 3,
        "content": "My answer is C , using service account to give permissions and storing it in a secured variable is a proper way"
      },
      {
        "date": "2021-02-23T18:49:00.000Z",
        "voteCount": 1,
        "content": "Storing SA key file in repo is not recommended. Everyone can access it. Instead we can use this. https://stackoverflow.com/questions/48602546/google-cloud-functions-how-to-securely-store-service-account-private-key-when"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 40,
    "url": "https://www.examtopics.com/discussions/google/view/36511-exam-professional-cloud-developer-topic-1-question-40/",
    "body": "You are planning to deploy your application in a Google Kubernetes Engine (GKE) cluster. Your application can scale horizontally, and each instance of your application needs to have a stable network identity and its own persistent disk.<br>Which GKE object should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeployment",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStatefulSet\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReplicaSet",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReplicaController"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-04-18T08:46:00.000Z",
        "voteCount": 11,
        "content": "B is the best option here. You can refer to : https://cloud.google.com/kubernetes-engine/docs/concepts/statefulset"
      },
      {
        "date": "2021-07-17T18:50:00.000Z",
        "voteCount": 3,
        "content": "\"The state information and other resilient data for any given StatefulSet Pod is maintained in persistent disk storage associated with the StatefulSet.\""
      },
      {
        "date": "2023-09-19T02:18:00.000Z",
        "voteCount": 1,
        "content": "A StatefulSet is the Kubernetes object best suited for workloads where each pod needs a stable network identity and its own persistent disk."
      },
      {
        "date": "2023-01-07T00:58:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is B. StatefulSet.\n\nStatefulSets are used to manage the deployment and scaling of stateful applications. They provide a stable network identity and persistent storage for each instance of the application. They are designed to work with applications that require a stable network identity and persistent storage, such as databases, message brokers, and other stateful applications. In contrast, Deployments are used to manage the deployment and scaling of stateless applications, which do not require a stable network identity or persistent storage. ReplicaSets and ReplicaControllers are similar to Deployments, but are older and less commonly used in GKE."
      },
      {
        "date": "2022-08-19T23:40:00.000Z",
        "voteCount": 3,
        "content": "B is correct"
      },
      {
        "date": "2022-07-26T03:36:00.000Z",
        "voteCount": 2,
        "content": "Once created, the StatefulSet ensures that the desired number of Pods are running and available at all times. The StatefulSet automatically replaces Pods that fail or are evicted from their nodes, and automatically associates new Pods with the storage resources, resource requests and limits, and other configurations defined in the StatefulSet's Pod specification"
      },
      {
        "date": "2021-02-07T12:22:00.000Z",
        "voteCount": 2,
        "content": "B is ok"
      },
      {
        "date": "2020-11-14T13:21:00.000Z",
        "voteCount": 2,
        "content": "B its OK"
      },
      {
        "date": "2020-11-13T02:40:00.000Z",
        "voteCount": 2,
        "content": "C doesn't provide a stable network identity and its own persistent disk"
      },
      {
        "date": "2020-11-13T15:31:00.000Z",
        "voteCount": 1,
        "content": "you are right"
      },
      {
        "date": "2020-11-08T15:08:00.000Z",
        "voteCount": 1,
        "content": "For me C is correct answer"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 41,
    "url": "https://www.examtopics.com/discussions/google/view/36594-exam-professional-cloud-developer-topic-1-question-41/",
    "body": "You are using Cloud Build to build a Docker image. You need to modify the build to execute unit and run integration tests. When there is a failure, you want the build history to clearly display the stage at which the build failed.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd RUN commands in the Dockerfile to execute unit and integration tests.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Cloud Build build config file with a single build step to compile unit and integration tests.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Cloud Build build config file that will spawn a separate cloud build pipeline for unit and integration tests.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Cloud Build build config file with separate cloud builder steps to compile and execute unit and integration tests.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-11-09T12:41:00.000Z",
        "voteCount": 10,
        "content": "D is correct answer here"
      },
      {
        "date": "2023-09-19T02:22:00.000Z",
        "voteCount": 1,
        "content": "I would go with D."
      },
      {
        "date": "2023-01-07T01:01:00.000Z",
        "voteCount": 1,
        "content": "D - Create a Cloud Build build config file with separate cloud builder steps to compile and execute unit and integration tests. This is the best option because it allows you to clearly specify and separate the different stages of the build process (compiling unit tests, executing unit tests, compiling integration tests, executing integration tests). This makes it easier to understand the build history and identify any failures that may occur. In addition, using separate build steps allows you to specify different properties (such as timeout values or environment variables) for each stage of the build process."
      },
      {
        "date": "2022-08-19T23:40:00.000Z",
        "voteCount": 3,
        "content": "D is correct"
      },
      {
        "date": "2022-08-19T17:13:00.000Z",
        "voteCount": 1,
        "content": "Vote D"
      },
      {
        "date": "2021-07-02T18:52:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/build/docs/build-config-file-schema:\n\"A build step specifies an action that you want Cloud Build to perform. For each build step, Cloud Build executes a docker container as an instance of docker run. Build steps are analogous to commands in a script and provide you with the flexibility of executing arbitrary instructions in your build.\"\n\nD makes the most sense here"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 42,
    "url": "https://www.examtopics.com/discussions/google/view/22768-exam-professional-cloud-developer-topic-1-question-42/",
    "body": "Your code is running on Cloud Functions in project A. It is supposed to write an object in a Cloud Storage bucket owned by project B. However, the write call is failing with the error \"403 Forbidden\".<br>What should you do to correct the problem?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGrant your user account the roles/storage.objectCreator role for the Cloud Storage bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGrant your user account the roles/iam.serviceAccountUser role for the service-PROJECTA@gcf-admin-robot.iam.gserviceaccount.com service account.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGrant the service-PROJECTA@gcf-admin-robot.iam.gserviceaccount.com service account the roles/storage.objectCreator role for the Cloud Storage bucket.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable the Cloud Storage API in project B."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-06-11T09:27:00.000Z",
        "voteCount": 17,
        "content": "The answer is C : the default service account use by cloud function is service-PROJECT_NUMBER@gcf-admin-robot.iam.gserviceaccount.com (cf. https://cloud.google.com/functions/docs/concepts/iam#troubleshooting_permission_errors)"
      },
      {
        "date": "2020-11-09T12:42:00.000Z",
        "voteCount": 2,
        "content": "Yes correct answer."
      },
      {
        "date": "2024-03-07T14:17:00.000Z",
        "voteCount": 1,
        "content": "C. Grant the service-PROJECTA@gcf-admin-robot.iam.gserviceaccount.com service account the roles/storage.objectCreator role for the Cloud Storage bucket.\n\nThe error \"403 Forbidden\" typically indicates a permissions issue. When a Google Cloud Function tries to access a resource in another project (in this case, a Cloud Storage bucket in project B), it does so using its associated service account. By default, this service account is PROJECT_ID@gcf-admin-robot.iam.gserviceaccount.com where PROJECT_ID is the ID of the project where the Cloud Function is running (project A)."
      },
      {
        "date": "2023-09-19T02:29:00.000Z",
        "voteCount": 1,
        "content": "Correct : C"
      },
      {
        "date": "2023-01-07T01:03:00.000Z",
        "voteCount": 1,
        "content": "C. Grant the service-PROJECTA@gcf-admin-robot.iam.gserviceaccount.com service account the roles/storage.objectCreator role for the Cloud Storage bucket.\n\nIn order for the Cloud Functions code running in project A to write to a Cloud Storage bucket in project B, the service account that is used to execute the code needs to be granted the appropriate permissions. In this case, you should grant the service-PROJECTA@gcf-admin-robot.iam.gserviceaccount.com service account the roles/storage.objectCreator role for the Cloud Storage bucket in project B. This will allow the code to write objects to the bucket. Option A would not work because it is the service account, not your user account, that needs to be granted permissions."
      },
      {
        "date": "2023-01-07T01:03:00.000Z",
        "voteCount": 1,
        "content": "Option B would not work because the roles/iam.serviceAccountUser role does not grant any permissions to access Cloud Storage. Option D would not solve the problem, as the Cloud Storage API is already enabled in both projects by default."
      },
      {
        "date": "2022-08-19T23:40:00.000Z",
        "voteCount": 3,
        "content": "C is correct"
      },
      {
        "date": "2022-01-08T04:05:00.000Z",
        "voteCount": 2,
        "content": "Answer is C"
      },
      {
        "date": "2021-10-25T22:17:00.000Z",
        "voteCount": 2,
        "content": "Appeared exam 26/10"
      },
      {
        "date": "2021-11-01T04:50:00.000Z",
        "voteCount": 2,
        "content": "How about the other question ? Does it appear also ?"
      },
      {
        "date": "2021-07-02T19:08:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/functions/docs/troubleshooting:\n\"The Cloud Functions service uses the Cloud Functions Service Agent service account (service-&lt;PROJECT_NUMBER&gt;@gcf-admin-robot.iam.gserviceaccount.com) when performing administrative actions on your project. By default this account is assigned the Cloud Functions cloudfunctions.serviceAgent role. This role is required for Cloud Pub/Sub, IAM, Cloud Storage and Firebase integrations. If you have changed the role for this service account, deployment fails.\"\n\nAnswer is C"
      },
      {
        "date": "2021-06-30T04:48:00.000Z",
        "voteCount": 1,
        "content": "Answer is C.\nservice-PROJECTA@gcf-admin-robot.iam.gserviceaccount.com is a google-managed SA."
      },
      {
        "date": "2021-04-30T20:17:00.000Z",
        "voteCount": 1,
        "content": "defenitely C"
      },
      {
        "date": "2020-06-10T08:24:00.000Z",
        "voteCount": 3,
        "content": "Seems there is no correct answer here... The correct answer should be grant add service account used by cloud function as a member to target bucket with roles/storage.objectCreator role"
      },
      {
        "date": "2023-07-27T05:16:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is the C but like you say, is not the best. To leave the default account is a bad procedure. The best answer must be \"Create a new service account and assing it to the cloud build, and grant the object creator permission to that account\"."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 43,
    "url": "https://www.examtopics.com/discussions/google/view/36596-exam-professional-cloud-developer-topic-1-question-43/",
    "body": "Case study -<br>This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.<br>To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.<br>At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.<br><br>To start the case study -<br>To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an<br>All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.<br><br>Company Overview -<br>HipLocal is a community application designed to facilitate communication between people in close proximity. It is used for event planning and organizing sporting events, and for businesses to connect with their local communities. HipLocal launched recently in a few neighborhoods in Dallas and is rapidly growing into a global phenomenon. Its unique style of hyper-local community communication and business outreach is in demand around the world.<br><br>Executive Statement -<br>We are the number one local community app; it's time to take our local community services global. Our venture capital investors want to see rapid growth and the same great experience for new local and virtual communities that come online, whether their members are 10 or 10000 miles away from each other.<br><br>Solution Concept -<br>HipLocal wants to expand their existing service, with updated functionality, in new regions to better serve their global customers. They want to hire and train a new team to support these regions in their time zones. They will need to ensure that the application scales smoothly and provides clear uptime data.<br><br>Existing Technical Environment -<br>HipLocal's environment is a mix of on-premises hardware and infrastructure running in Google Cloud Platform. The HipLocal team understands their application well, but has limited experience in global scale applications. Their existing technical environment is as follows:<br>* Existing APIs run on Compute Engine virtual machine instances hosted in GCP.<br>* State is stored in a single instance MySQL database in GCP.<br>* Data is exported to an on-premises Teradata/Vertica data warehouse.<br>* Data analytics is performed in an on-premises Hadoop environment.<br>* The application has no logging.<br>* There are basic indicators of uptime; alerts are frequently fired when the APIs are unresponsive.<br><br>Business Requirements -<br>HipLocal's investors want to expand their footprint and support the increase in demand they are seeing. Their requirements are:<br>* Expand availability of the application to new regions.<br>* Increase the number of concurrent users that can be supported.<br>* Ensure a consistent experience for users when they travel to different regions.<br>* Obtain user activity metrics to better understand how to monetize their product.<br>* Ensure compliance with regulations in the new regions (for example, GDPR).<br>* Reduce infrastructure management time and cost.<br>* Adopt the Google-recommended practices for cloud computing.<br><br>Technical Requirements -<br>* The application and backend must provide usage metrics and monitoring.<br>* APIs require strong authentication and authorization.<br>* Logging must be increased, and data should be stored in a cloud analytics platform.<br>* Move to serverless architecture to facilitate elastic scaling.<br>* Provide authorized access to internal apps in a secure manner.<br>HipLocal's .net-based auth service fails under intermittent load.<br>What should they do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse App Engine for autoscaling.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Functions for autoscaling.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a Compute Engine cluster for the service.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a dedicated Compute Engine virtual machine instance for the service."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 7,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 6,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-11-09T12:47:00.000Z",
        "voteCount": 11,
        "content": "A is correct answer here App engine, as app engine flexible support .net"
      },
      {
        "date": "2020-12-24T15:31:00.000Z",
        "voteCount": 3,
        "content": "A is wrong because appengine is single region: https://cloud.google.com/appengine/docs/locations. For me the correct answer is C, compute engine with instance Group."
      },
      {
        "date": "2021-01-02T14:50:00.000Z",
        "voteCount": 7,
        "content": "One of reqs is \"Move to serverless architecture to facilitate elastic scaling\". I vote for A."
      },
      {
        "date": "2021-02-23T18:52:00.000Z",
        "voteCount": 3,
        "content": "A is correct"
      },
      {
        "date": "2024-10-13T00:42:00.000Z",
        "voteCount": 1,
        "content": "App egnine , even its regional"
      },
      {
        "date": "2024-08-27T19:10:00.000Z",
        "voteCount": 1,
        "content": "for me its compute engine cluster"
      },
      {
        "date": "2024-07-17T03:09:00.000Z",
        "voteCount": 1,
        "content": "The best answer here is B. Use Cloud Functions for autoscaling.\n\nHere's why:\n\nServerless Architecture: Cloud Functions is a serverless platform, meaning you don't need to manage servers or infrastructure. This aligns with the technical requirement to move to a serverless architecture for elastic scaling.\nAutoscaling: Cloud Functions automatically scales based on demand, ensuring that your auth service can handle spikes in traffic without performance degradation. This addresses the business requirement of increasing the number of concurrent users.\nCost-Effective: Serverless platforms like Cloud Functions are cost-effective because you only pay for the resources you use. This aligns with the business requirement of reducing infrastructure management time and cost.\nImproved Reliability: Cloud Functions is a highly reliable platform, reducing the risk of downtime and improving the user experience. This addresses the business requirement of ensuring a consistent experience for users."
      },
      {
        "date": "2024-07-17T03:09:00.000Z",
        "voteCount": 1,
        "content": "Why other options are less suitable:\n\nA. Use App Engine for autoscaling: While App Engine is a good option for autoscaling, it's not as ideal for a specific service like authentication. Cloud Functions is more lightweight and focused on individual functions.\nC. Use a Compute Engine cluster for the service: This approach requires more manual management and configuration, making it less efficient and cost-effective than Cloud Functions.\nD. Use a dedicated Compute Engine virtual machine instance for the service: This is the least scalable option and doesn't address the issue of intermittent load.\nIn summary: Cloud Functions provides the best combination of autoscaling, serverless architecture, cost-effectiveness, and reliability to address HipLocal's challenges with their .NET-based auth service."
      },
      {
        "date": "2024-03-07T14:31:00.000Z",
        "voteCount": 2,
        "content": "B. Use Cloud Functions for autoscaling: Cloud Functions is a serverless execution environment that automatically scales based on the load. It is well-suited for applications with intermittent or unpredictable traffic patterns, such as HipLocal's auth service. The serverless nature of Cloud Functions also reduces infrastructure management overhead, aligning with the business requirement to reduce management time and cost. However, it's important to note that the runtime support for .NET in Cloud Functions is limited, and a migration or use of an alternative supported runtime might be necessary."
      },
      {
        "date": "2023-11-25T04:38:00.000Z",
        "voteCount": 1,
        "content": "A, since AppEngine can be deployed in several regions with a Load Balancer in front as demonstrated by Google. This will make the deployment serverless as requested, sticking to .net framework."
      },
      {
        "date": "2023-11-18T08:36:00.000Z",
        "voteCount": 1,
        "content": "Serverless architecture -&gt; App Engine"
      },
      {
        "date": "2023-11-08T23:39:00.000Z",
        "voteCount": 1,
        "content": "Vote for B"
      },
      {
        "date": "2023-09-23T19:20:00.000Z",
        "voteCount": 2,
        "content": "B is correct."
      },
      {
        "date": "2023-09-19T02:46:00.000Z",
        "voteCount": 1,
        "content": "Option C is correct as AppEngine is regional."
      },
      {
        "date": "2023-09-23T19:20:00.000Z",
        "voteCount": 1,
        "content": "I would go with Option B Cloud Function. Since it is a global resource and is scalable on demand."
      },
      {
        "date": "2023-11-18T08:34:00.000Z",
        "voteCount": 1,
        "content": "Cloud Functions isn't an optimal solution for an entire app."
      },
      {
        "date": "2023-07-12T18:37:00.000Z",
        "voteCount": 1,
        "content": "A is correct. Don't get thrown off by app engine being regional.\nGoogle has demonstracted you can deploy to multiple regions by creating multiple project and throwing a load balancer in front. Check their demo toward the end:\nhttps://www.youtube.com/watch?v=JCvzUTmKakQ&amp;ab_channel=GoogleCloudTech"
      },
      {
        "date": "2023-02-20T01:51:00.000Z",
        "voteCount": 1,
        "content": "I vote A. App Engine cannot run in multi-region. But we can create multiple projects for supporting different region app engine."
      },
      {
        "date": "2023-02-02T05:44:00.000Z",
        "voteCount": 1,
        "content": "C is correct, A option is for regional solutions"
      },
      {
        "date": "2023-01-13T04:48:00.000Z",
        "voteCount": 1,
        "content": "C is correct because app engine is regional only so it not answer the need of global application"
      },
      {
        "date": "2022-08-19T23:41:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2022-08-19T17:16:00.000Z",
        "voteCount": 1,
        "content": "A for \"serverless architecture to facilitate elastic scaling\""
      },
      {
        "date": "2022-07-26T03:57:00.000Z",
        "voteCount": 1,
        "content": "A is correct answer here App engine, as app engine flexible support .net"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 44,
    "url": "https://www.examtopics.com/discussions/google/view/70408-exam-professional-cloud-developer-topic-1-question-44/",
    "body": "Case study -<br>This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.<br>To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.<br>At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.<br><br>To start the case study -<br>To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an<br>All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.<br><br>Company Overview -<br>HipLocal is a community application designed to facilitate communication between people in close proximity. It is used for event planning and organizing sporting events, and for businesses to connect with their local communities. HipLocal launched recently in a few neighborhoods in Dallas and is rapidly growing into a global phenomenon. Its unique style of hyper-local community communication and business outreach is in demand around the world.<br><br>Executive Statement -<br>We are the number one local community app; it's time to take our local community services global. Our venture capital investors want to see rapid growth and the same great experience for new local and virtual communities that come online, whether their members are 10 or 10000 miles away from each other.<br><br>Solution Concept -<br>HipLocal wants to expand their existing service, with updated functionality, in new regions to better serve their global customers. They want to hire and train a new team to support these regions in their time zones. They will need to ensure that the application scales smoothly and provides clear uptime data.<br><br>Existing Technical Environment -<br>HipLocal's environment is a mix of on-premises hardware and infrastructure running in Google Cloud Platform. The HipLocal team understands their application well, but has limited experience in global scale applications. Their existing technical environment is as follows:<br>* Existing APIs run on Compute Engine virtual machine instances hosted in GCP.<br>* State is stored in a single instance MySQL database in GCP.<br>* Data is exported to an on-premises Teradata/Vertica data warehouse.<br>* Data analytics is performed in an on-premises Hadoop environment.<br>* The application has no logging.<br>* There are basic indicators of uptime; alerts are frequently fired when the APIs are unresponsive.<br><br>Business Requirements -<br>HipLocal's investors want to expand their footprint and support the increase in demand they are seeing. Their requirements are:<br>* Expand availability of the application to new regions.<br>* Increase the number of concurrent users that can be supported.<br>* Ensure a consistent experience for users when they travel to different regions.<br>* Obtain user activity metrics to better understand how to monetize their product.<br>* Ensure compliance with regulations in the new regions (for example, GDPR).<br>* Reduce infrastructure management time and cost.<br>* Adopt the Google-recommended practices for cloud computing.<br><br>Technical Requirements -<br>* The application and backend must provide usage metrics and monitoring.<br>* APIs require strong authentication and authorization.<br>* Logging must be increased, and data should be stored in a cloud analytics platform.<br>* Move to serverless architecture to facilitate elastic scaling.<br>* Provide authorized access to internal apps in a secure manner.<br>HipLocal's APIs are having occasional application failures. They want to collect application information specifically to troubleshoot the issue. What should they do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTake frequent snapshots of the virtual machines.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall the Cloud Logging agent on the virtual machines.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall the Cloud Monitoring agent on the virtual machines.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Trace to look for performance bottlenecks."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-03-28T09:09:00.000Z",
        "voteCount": 6,
        "content": "B is the correct answer"
      },
      {
        "date": "2022-05-18T22:13:00.000Z",
        "voteCount": 1,
        "content": "No.. C is the right answer"
      },
      {
        "date": "2024-07-17T03:15:00.000Z",
        "voteCount": 1,
        "content": "The best answer is D. Use Cloud Trace to look for performance bottlenecks.\n\nHere's why:\n\nCloud Trace for Troubleshooting: Cloud Trace is a distributed tracing tool that helps you understand the flow of requests through your application. It can pinpoint performance bottlenecks, identify slow calls, and highlight potential areas of failure. This is exactly what HipLocal needs to troubleshoot their occasional application failures."
      },
      {
        "date": "2024-07-17T03:15:00.000Z",
        "voteCount": 1,
        "content": "Let's break down why the other options are less suitable:\n\nA. Take frequent snapshots of the virtual machines: Snapshots are useful for backups and disaster recovery, but they don't provide real-time insights into application performance or errors.\nB. Install the Cloud Logging agent on the virtual machines: Cloud Logging is essential for collecting logs, but it doesn't provide the same level of detailed tracing information as Cloud Trace. Logs can help identify errors, but they don't show the flow of requests or pinpoint performance issues.\nC. Install the Cloud Monitoring agent on the virtual machines: Cloud Monitoring is great for collecting metrics and setting up alerts, but it doesn't provide the detailed tracing information needed for troubleshooting application failures."
      },
      {
        "date": "2024-03-07T14:34:00.000Z",
        "voteCount": 1,
        "content": "B. Install the Cloud Logging agent on the virtual machines.\n\nTo troubleshoot occasional application failures, HipLocal needs to collect detailed logs that provide insights into what's happening within their applications. Installing the Cloud Logging agent on the virtual machines is the most direct approach to achieving this. The Cloud Logging agent will collect logs from various applications and system components running on the VMs, which can be invaluable for diagnosing issues."
      },
      {
        "date": "2023-09-19T02:49:00.000Z",
        "voteCount": 1,
        "content": "I would go with B."
      },
      {
        "date": "2023-07-22T03:01:00.000Z",
        "voteCount": 1,
        "content": "C is the correct answer"
      },
      {
        "date": "2023-06-18T06:32:00.000Z",
        "voteCount": 1,
        "content": "B.\nA: No. This is for VM boot failed or other not related to API. The question didn't mention VM failed, nor the scenario.\nB: Yes.\nC: No. Monitoring agent is for metrics collection, such as memory. Not related to API.\nD: No. If the question stated something like the API works perfectly but slow, then this would be valid."
      },
      {
        "date": "2023-01-09T08:12:00.000Z",
        "voteCount": 1,
        "content": "They don't have logging so need to add logging agent so we can have logs to study. Tracr is for latency issue and it's not the issue here."
      },
      {
        "date": "2022-08-19T23:42:00.000Z",
        "voteCount": 1,
        "content": "D  is correct"
      },
      {
        "date": "2022-08-03T07:17:00.000Z",
        "voteCount": 1,
        "content": "I might be wrong here, but Cloud Trace is also kind of suitable for this use case, isn't it?\nReference - https://cloud.google.com/trace\n\nFast, automatic issue detection\nTrace continuously gathers and analyzes trace data from your project to automatically identify recent changes to your application's performance. These latency distributions, available through the Analysis Reports feature, can be compared over time or versions, and Cloud Trace will automatically alert you if it detects a significant shift in your app's latency profile."
      },
      {
        "date": "2022-08-10T04:06:00.000Z",
        "voteCount": 1,
        "content": "I will still go for option B here, as Trace is majorly used for finding out performance bottlenecks which is not specified in the problem statement."
      },
      {
        "date": "2022-02-27T13:35:00.000Z",
        "voteCount": 1,
        "content": "Yep they don\u2019t have any logging so it should be B"
      },
      {
        "date": "2022-01-22T02:05:00.000Z",
        "voteCount": 2,
        "content": "B is correct too."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 45,
    "url": "https://www.examtopics.com/discussions/google/view/36600-exam-professional-cloud-developer-topic-1-question-45/",
    "body": "Case study -<br>This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.<br>To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.<br>At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.<br><br>To start the case study -<br>To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an<br>All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.<br><br>Company Overview -<br>HipLocal is a community application designed to facilitate communication between people in close proximity. It is used for event planning and organizing sporting events, and for businesses to connect with their local communities. HipLocal launched recently in a few neighborhoods in Dallas and is rapidly growing into a global phenomenon. Its unique style of hyper-local community communication and business outreach is in demand around the world.<br><br>Executive Statement -<br>We are the number one local community app; it's time to take our local community services global. Our venture capital investors want to see rapid growth and the same great experience for new local and virtual communities that come online, whether their members are 10 or 10000 miles away from each other.<br><br>Solution Concept -<br>HipLocal wants to expand their existing service, with updated functionality, in new regions to better serve their global customers. They want to hire and train a new team to support these regions in their time zones. They will need to ensure that the application scales smoothly and provides clear uptime data.<br><br>Existing Technical Environment -<br>HipLocal's environment is a mix of on-premises hardware and infrastructure running in Google Cloud Platform. The HipLocal team understands their application well, but has limited experience in global scale applications. Their existing technical environment is as follows:<br>* Existing APIs run on Compute Engine virtual machine instances hosted in GCP.<br>* State is stored in a single instance MySQL database in GCP.<br>* Data is exported to an on-premises Teradata/Vertica data warehouse.<br>* Data analytics is performed in an on-premises Hadoop environment.<br>* The application has no logging.<br>* There are basic indicators of uptime; alerts are frequently fired when the APIs are unresponsive.<br><br>Business Requirements -<br>HipLocal's investors want to expand their footprint and support the increase in demand they are seeing. Their requirements are:<br>* Expand availability of the application to new regions.<br>* Increase the number of concurrent users that can be supported.<br>* Ensure a consistent experience for users when they travel to different regions.<br>* Obtain user activity metrics to better understand how to monetize their product.<br>* Ensure compliance with regulations in the new regions (for example, GDPR).<br>* Reduce infrastructure management time and cost.<br>* Adopt the Google-recommended practices for cloud computing.<br><br>Technical Requirements -<br>* The application and backend must provide usage metrics and monitoring.<br>* APIs require strong authentication and authorization.<br>* Logging must be increased, and data should be stored in a cloud analytics platform.<br>* Move to serverless architecture to facilitate elastic scaling.<br>* Provide authorized access to internal apps in a secure manner.<br>HipLocal has connected their Hadoop infrastructure to GCP using Cloud Interconnect in order to query data stored on persistent disks.<br>Which IP strategy should they use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate manual subnets.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an auto mode subnet.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate multiple peered VPCs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tProvision a single instance for NAT."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 7,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-10-13T00:44:00.000Z",
        "voteCount": 1,
        "content": "ince Cloud Interconnect is used, HipLocal is likely setting up a hybrid cloud environment. This requires careful planning of IP ranges to avoid conflicts and ensure smooth communication between on-premises infrastructure and the cloud, which manual subnets provide."
      },
      {
        "date": "2024-07-17T03:22:00.000Z",
        "voteCount": 1,
        "content": "The best answer here is B. Create an auto mode subnet.\n\nHere's why:\n\nAuto Mode Subnets: Auto mode subnets automatically assign internal IP addresses to instances within the subnet. This simplifies IP address management and eliminates the need for manual configuration.\nCloud Interconnect: Cloud Interconnect provides a dedicated, high-bandwidth connection between your on-premises network and Google Cloud. This allows for efficient data transfer between your Hadoop infrastructure and the persistent disks in GCP.\nSimplified Management: Auto mode subnets make it easier to manage IP addresses, especially when dealing with a hybrid environment like HipLocal's."
      },
      {
        "date": "2024-07-17T03:22:00.000Z",
        "voteCount": 1,
        "content": "Why other options are less suitable:\n\nA. Create manual subnets: Manual subnets require you to manually assign IP addresses to instances, which can be time-consuming and error-prone, especially in a dynamic environment.\nC. Create multiple peered VPCs: Peering VPCs is useful for connecting different VPCs, but it's not necessary for connecting your Hadoop infrastructure to GCP using Cloud Interconnect.\nD. Provision a single instance for NAT: While NAT can be used for outbound connectivity, it's not the most efficient or secure approach for connecting your Hadoop infrastructure to GCP.\nIn summary: Auto mode subnets provide the most efficient and manageable IP address strategy for HipLocal's hybrid environment, simplifying IP address management and ensuring seamless connectivity between their Hadoop infrastructure and GCP."
      },
      {
        "date": "2024-07-13T01:06:00.000Z",
        "voteCount": 1,
        "content": "Manual Subnets:\nControl Over IP Addressing: Creating manual subnets (also known as custom mode VPCs) provides precise control over IP addressing and subnet creation. This ensures that the IP ranges do not overlap and can be managed to meet specific requirements of the Hadoop infrastructure and Cloud Interconnect setup.\nSubnet Management: With manual subnets, HipLocal can create subnets that are optimised for their data traffic patterns and usage requirements, which is crucial for performance and efficient utilisation of network resources.\nIntegration: Custom mode VPCs allow for better integration with on-premises networks through Cloud Interconnect, ensuring a seamless and efficient network setup.\nWhy Not the B?\nAuto mode subnets automatically create subnets in each region with pre-defined IP ranges. This lack of control over IP address assignment and network segmentation is not suitable for complex and specific networking requirements like those of HipLocal\u2019s Hadoop infrastructure."
      },
      {
        "date": "2024-03-07T14:37:00.000Z",
        "voteCount": 1,
        "content": "B. Create an auto mode subnet.\n\nWhen integrating Hadoop infrastructure with Google Cloud Platform (GCP) via Cloud Interconnect and querying data stored on persistent disks, the IP strategy should simplify network management while ensuring efficient and secure data access. Creating an auto mode subnet in their VPC is a suitable approach for this scenario."
      },
      {
        "date": "2024-02-26T04:39:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2023-09-19T02:56:00.000Z",
        "voteCount": 1,
        "content": "For simplicity and ease of management, an auto mode subnet (option B) could be a good choice."
      },
      {
        "date": "2022-08-19T23:42:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2022-08-17T02:11:00.000Z",
        "voteCount": 4,
        "content": "A - Need to take control of the IP assignment thru manual subnet especially when establishing the connectivity between on-prem/cloud"
      },
      {
        "date": "2022-08-03T07:34:00.000Z",
        "voteCount": 2,
        "content": "I will go with auto mode subnet creation as it will automatically create a subnet inside each region. Moreover, one of the business requirements states that 'Reduce infrastructure management time and cost.'. Thus, with auto mode subnet we avoid infrastructure management."
      },
      {
        "date": "2021-07-02T19:32:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/architecture/hadoop/hadoop-gcp-migration-data\n\nI would take A based on the 2nd figure given in the link"
      },
      {
        "date": "2020-11-09T13:01:00.000Z",
        "voteCount": 1,
        "content": "A is correct answer here."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 46,
    "url": "https://www.examtopics.com/discussions/google/view/36601-exam-professional-cloud-developer-topic-1-question-46/",
    "body": "Case study -<br>This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.<br>To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.<br>At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.<br><br>To start the case study -<br>To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an<br>All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.<br><br>Company Overview -<br>HipLocal is a community application designed to facilitate communication between people in close proximity. It is used for event planning and organizing sporting events, and for businesses to connect with their local communities. HipLocal launched recently in a few neighborhoods in Dallas and is rapidly growing into a global phenomenon. Its unique style of hyper-local community communication and business outreach is in demand around the world.<br><br>Executive Statement -<br>We are the number one local community app; it's time to take our local community services global. Our venture capital investors want to see rapid growth and the same great experience for new local and virtual communities that come online, whether their members are 10 or 10000 miles away from each other.<br><br>Solution Concept -<br>HipLocal wants to expand their existing service, with updated functionality, in new regions to better serve their global customers. They want to hire and train a new team to support these regions in their time zones. They will need to ensure that the application scales smoothly and provides clear uptime data.<br><br>Existing Technical Environment -<br>HipLocal's environment is a mix of on-premises hardware and infrastructure running in Google Cloud Platform. The HipLocal team understands their application well, but has limited experience in global scale applications. Their existing technical environment is as follows:<br>* Existing APIs run on Compute Engine virtual machine instances hosted in GCP.<br>* State is stored in a single instance MySQL database in GCP.<br>* Data is exported to an on-premises Teradata/Vertica data warehouse.<br>* Data analytics is performed in an on-premises Hadoop environment.<br>* The application has no logging.<br>* There are basic indicators of uptime; alerts are frequently fired when the APIs are unresponsive.<br><br>Business Requirements -<br>HipLocal's investors want to expand their footprint and support the increase in demand they are seeing. Their requirements are:<br>* Expand availability of the application to new regions.<br>* Increase the number of concurrent users that can be supported.<br>* Ensure a consistent experience for users when they travel to different regions.<br>* Obtain user activity metrics to better understand how to monetize their product.<br>* Ensure compliance with regulations in the new regions (for example, GDPR).<br>* Reduce infrastructure management time and cost.<br>* Adopt the Google-recommended practices for cloud computing.<br><br>Technical Requirements -<br>* The application and backend must provide usage metrics and monitoring.<br>* APIs require strong authentication and authorization.<br>* Logging must be increased, and data should be stored in a cloud analytics platform.<br>* Move to serverless architecture to facilitate elastic scaling.<br>* Provide authorized access to internal apps in a secure manner.<br>Which service should HipLocal use to enable access to internal apps?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud VPN",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Armor",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tVirtual Private Cloud",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Identity-Aware Proxy\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "Reference:<br>https://cloud.google.com/iap/docs/cloud-iap-for-on-prem-apps-overview",
    "votes": [
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-12-31T17:37:00.000Z",
        "voteCount": 7,
        "content": "I think it should be D ."
      },
      {
        "date": "2023-09-19T03:09:00.000Z",
        "voteCount": 1,
        "content": "Cloud IAP works by verifying user identity and context of the request to determine if a user should be allowed to access the application. It provides secure application-level access control and does not require a traditional VPN connection."
      },
      {
        "date": "2022-08-19T23:43:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2022-03-28T09:18:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2022-03-28T09:19:00.000Z",
        "voteCount": 1,
        "content": "I think it is A, not D"
      },
      {
        "date": "2021-07-02T20:18:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/network-connectivity/docs/vpn/concepts/overview\nhttps://cloud.google.com/armor\n\nD is the answer for sure"
      },
      {
        "date": "2021-06-30T05:31:00.000Z",
        "voteCount": 1,
        "content": "D.\nhttps://cloud.google.com/iap/docs/concepts-overview"
      },
      {
        "date": "2020-11-09T13:12:00.000Z",
        "voteCount": 3,
        "content": "if internal app mens app hosted on-prem then option A seems to be correct one"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 47,
    "url": "https://www.examtopics.com/discussions/google/view/36602-exam-professional-cloud-developer-topic-1-question-47/",
    "body": "Case study -<br>This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.<br>To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.<br>At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.<br><br>To start the case study -<br>To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an<br>All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.<br><br>Company Overview -<br>HipLocal is a community application designed to facilitate communication between people in close proximity. It is used for event planning and organizing sporting events, and for businesses to connect with their local communities. HipLocal launched recently in a few neighborhoods in Dallas and is rapidly growing into a global phenomenon. Its unique style of hyper-local community communication and business outreach is in demand around the world.<br><br>Executive Statement -<br>We are the number one local community app; it's time to take our local community services global. Our venture capital investors want to see rapid growth and the same great experience for new local and virtual communities that come online, whether their members are 10 or 10000 miles away from each other.<br><br>Solution Concept -<br>HipLocal wants to expand their existing service, with updated functionality, in new regions to better serve their global customers. They want to hire and train a new team to support these regions in their time zones. They will need to ensure that the application scales smoothly and provides clear uptime data.<br><br>Existing Technical Environment -<br>HipLocal's environment is a mix of on-premises hardware and infrastructure running in Google Cloud Platform. The HipLocal team understands their application well, but has limited experience in global scale applications. Their existing technical environment is as follows:<br>* Existing APIs run on Compute Engine virtual machine instances hosted in GCP.<br>* State is stored in a single instance MySQL database in GCP.<br>* Data is exported to an on-premises Teradata/Vertica data warehouse.<br>* Data analytics is performed in an on-premises Hadoop environment.<br>* The application has no logging.<br>* There are basic indicators of uptime; alerts are frequently fired when the APIs are unresponsive.<br><br>Business Requirements -<br>HipLocal's investors want to expand their footprint and support the increase in demand they are seeing. Their requirements are:<br>* Expand availability of the application to new regions.<br>* Increase the number of concurrent users that can be supported.<br>* Ensure a consistent experience for users when they travel to different regions.<br>* Obtain user activity metrics to better understand how to monetize their product.<br>* Ensure compliance with regulations in the new regions (for example, GDPR).<br>* Reduce infrastructure management time and cost.<br>* Adopt the Google-recommended practices for cloud computing.<br><br>Technical Requirements -<br>* The application and backend must provide usage metrics and monitoring.<br>* APIs require strong authentication and authorization.<br>* Logging must be increased, and data should be stored in a cloud analytics platform.<br>* Move to serverless architecture to facilitate elastic scaling.<br>* Provide authorized access to internal apps in a secure manner.<br>HipLocal wants to reduce the number of on-call engineers and eliminate manual scaling.<br>Which two services should they choose? (Choose two.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Google App Engine services.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse serverless Google Cloud Functions.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Knative to build and deploy serverless applications.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Google Kubernetes Engine for automated deployments.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a large Google Compute Engine cluster for deployments."
    ],
    "answer": "CD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "CD",
        "count": 9,
        "isMostVoted": true
      },
      {
        "answer": "AB",
        "count": 5,
        "isMostVoted": false
      },
      {
        "answer": "BD",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-11-09T13:14:00.000Z",
        "voteCount": 17,
        "content": "A and B are correct option here."
      },
      {
        "date": "2024-07-17T03:37:00.000Z",
        "voteCount": 1,
        "content": "The two best services for HipLocal to choose are:\n\nB. Use serverless Google Cloud Functions.\nD. Use Google Kubernetes Engine for automated deployments.\nHere's why:\n\nCloud Functions: Cloud Functions is a serverless platform that automatically scales based on demand. This eliminates the need for manual scaling and reduces the workload on on-call engineers.\nKubernetes Engine: Kubernetes Engine (GKE) is a managed Kubernetes service that provides automated deployments, scaling, and self-healing capabilities. This reduces the need for manual intervention and frees up engineers to focus on other tasks."
      },
      {
        "date": "2024-07-17T03:37:00.000Z",
        "voteCount": 1,
        "content": "Let's look at why the other options are less ideal:\n\nA. Use Google App Engine services: App Engine is a good option for serverless applications, but it might not be as flexible as Cloud Functions for specific tasks like authentication or background processing.\nC. Use Knative to build and deploy serverless applications: Knative is a great option for building and deploying serverless applications, but it requires more configuration and management than Cloud Functions.\nE. Use a large Google Compute Engine cluster for deployments: While Compute Engine provides flexibility, it requires significant manual management for scaling and deployments, which goes against HipLocal's goals."
      },
      {
        "date": "2024-07-13T01:17:00.000Z",
        "voteCount": 1,
        "content": "Google App Engine and Google Cloud Functions are the best choices to meet HipLocal's requirements for reducing on-call engineer responsibilities and eliminating manual scaling. App Engine is a fully managed serverless platform that supports deployment to multiple regions, ensuring that your application can serve users around the world with low latency. You can choose the regions where you want to deploy your application, and Google manages the underlying infrastructure to provide high availability and automatic scaling. Cloud Functions can be deployed in multiple regions, allowing you to run your functions close to your users and other services they interact with. This reduces latency and improves performance."
      },
      {
        "date": "2024-03-07T14:46:00.000Z",
        "voteCount": 1,
        "content": "A. Google App Engine: This is a fully managed serverless platform that automatically scales your application up and down while balancing the load. With App Engine, you don't need to manage the underlying infrastructure, and it scales automatically in response to the traffic it receives. This can significantly reduce the operational overhead and the need for on-call engineers to handle scaling issues.\n\nB. Google Cloud Functions: This is another serverless execution environment that automatically scales the number of instances running your function in response to the incoming event rate. This is ideal for applications that respond to events (e.g., HTTP requests, Cloud Pub/Sub events). Like App Engine, it abstracts away infrastructure management and auto-scales based on demand."
      },
      {
        "date": "2023-09-19T03:13:00.000Z",
        "voteCount": 1,
        "content": "I Think It should be A nd B for serverless autoscaling. Since there are extra steps involved in configuring Knative it is not fit for this situation."
      },
      {
        "date": "2023-07-12T19:19:00.000Z",
        "voteCount": 1,
        "content": "you CAN go global with app engine and cloud functions"
      },
      {
        "date": "2023-01-12T06:44:00.000Z",
        "voteCount": 1,
        "content": "C and D because need to be global"
      },
      {
        "date": "2022-08-29T20:56:00.000Z",
        "voteCount": 2,
        "content": "App must be global"
      },
      {
        "date": "2022-08-19T23:44:00.000Z",
        "voteCount": 1,
        "content": "CD are correct"
      },
      {
        "date": "2022-07-27T22:33:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/kubernetes-engine/docs/concepts/traffic-management"
      },
      {
        "date": "2022-07-19T11:08:00.000Z",
        "voteCount": 1,
        "content": "Vote for C &amp; D"
      },
      {
        "date": "2022-07-04T22:37:00.000Z",
        "voteCount": 3,
        "content": "App Engine cannot be a solution here as it limits the application to be in a single region. We need to note that the case study has explicitly mentioned that the application needs to be global, which means multi-regional.\nSo, I will go with C &amp; D."
      },
      {
        "date": "2022-04-03T14:11:00.000Z",
        "voteCount": 1,
        "content": "C+D are correct. Because k8s is global plus on-premises nodes can be connected."
      },
      {
        "date": "2022-03-28T09:38:00.000Z",
        "voteCount": 2,
        "content": "AB is correct"
      },
      {
        "date": "2022-01-22T02:37:00.000Z",
        "voteCount": 1,
        "content": "App Engine and Cloud Functions are regional.\nD and E are correct, because they are global and they have autoscaler for deployments."
      },
      {
        "date": "2022-01-22T02:45:00.000Z",
        "voteCount": 1,
        "content": "Correction: C and D are correct.\n\nhttps://cloud.google.com/knative\nhttps://cloud.google.com/blog/products/serverless/knative-based-cloud-run-services-are-ga\nhttps://cloud.google.com/run/docs/multiple-regions"
      },
      {
        "date": "2022-01-19T01:07:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/appengine/docs/standard/python/how-instances-are-managed#apps_with_automatic_scaling"
      },
      {
        "date": "2021-07-02T20:20:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/appengine/docs/standard/python/how-instances-are-managed#apps_with_automatic_scaling\n\nA and B for sure; \"eliminate manual scaling\" as per what the qn states"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 48,
    "url": "https://www.examtopics.com/discussions/google/view/40744-exam-professional-cloud-developer-topic-1-question-48/",
    "body": "Case study -<br>This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.<br>To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.<br>At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.<br><br>To start the case study -<br>To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an<br>All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.<br><br>Company Overview -<br>HipLocal is a community application designed to facilitate communication between people in close proximity. It is used for event planning and organizing sporting events, and for businesses to connect with their local communities. HipLocal launched recently in a few neighborhoods in Dallas and is rapidly growing into a global phenomenon. Its unique style of hyper-local community communication and business outreach is in demand around the world.<br><br>Executive Statement -<br>We are the number one local community app; it's time to take our local community services global. Our venture capital investors want to see rapid growth and the same great experience for new local and virtual communities that come online, whether their members are 10 or 10000 miles away from each other.<br><br>Solution Concept -<br>HipLocal wants to expand their existing service, with updated functionality, in new regions to better serve their global customers. They want to hire and train a new team to support these regions in their time zones. They will need to ensure that the application scales smoothly and provides clear uptime data.<br><br>Existing Technical Environment -<br>HipLocal's environment is a mix of on-premises hardware and infrastructure running in Google Cloud Platform. The HipLocal team understands their application well, but has limited experience in global scale applications. Their existing technical environment is as follows:<br>* Existing APIs run on Compute Engine virtual machine instances hosted in GCP.<br>* State is stored in a single instance MySQL database in GCP.<br>* Data is exported to an on-premises Teradata/Vertica data warehouse.<br>* Data analytics is performed in an on-premises Hadoop environment.<br>* The application has no logging.<br>* There are basic indicators of uptime; alerts are frequently fired when the APIs are unresponsive.<br><br>Business Requirements -<br>HipLocal's investors want to expand their footprint and support the increase in demand they are seeing. Their requirements are:<br>* Expand availability of the application to new regions.<br>* Increase the number of concurrent users that can be supported.<br>* Ensure a consistent experience for users when they travel to different regions.<br>* Obtain user activity metrics to better understand how to monetize their product.<br>* Ensure compliance with regulations in the new regions (for example, GDPR).<br>* Reduce infrastructure management time and cost.<br>* Adopt the Google-recommended practices for cloud computing.<br><br>Technical Requirements -<br>* The application and backend must provide usage metrics and monitoring.<br>* APIs require strong authentication and authorization.<br>* Logging must be increased, and data should be stored in a cloud analytics platform.<br>* Move to serverless architecture to facilitate elastic scaling.<br>* Provide authorized access to internal apps in a secure manner.<br>In order to meet their business requirements, how should HipLocal store their application state?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse local SSDs to store state.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPut a memcache layer in front of MySQL.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMove the state storage to Cloud Spanner.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReplace the MySQL instance with Cloud SQL."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 9,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 7,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-12-25T02:31:00.000Z",
        "voteCount": 13,
        "content": "For me the answer is C. A is not valid because local SSD is volatile memory. B and D is bad solution because it don't reduce latency in world wide but they are a regional location."
      },
      {
        "date": "2021-05-20T02:30:00.000Z",
        "voteCount": 3,
        "content": "exactly, plus the state is already stored in a single instance MySQL database in GCP, so it basically says \"do nothing\". Spanner is correct here"
      },
      {
        "date": "2021-10-29T02:50:00.000Z",
        "voteCount": 1,
        "content": "\"State is stored in a single instance MySQL database in GCP.\" so Cloud SQL is enough! (Option D) so Correct answer is D\nSpanner would be costly (Option C)"
      },
      {
        "date": "2024-07-17T03:41:00.000Z",
        "voteCount": 1,
        "content": "Cloud Spanner's Strengths: Cloud Spanner is a globally distributed, relational database service that offers high availability, scalability, and strong consistency. These features are crucial for HipLocal's global expansion and the need to ensure a consistent experience for users across regions.\nScalability and Availability: Cloud Spanner can handle large amounts of data and automatically scales to meet demand. This addresses the business requirement of increasing the number of concurrent users and ensuring a consistent experience for users when they travel to different regions."
      },
      {
        "date": "2024-07-17T03:41:00.000Z",
        "voteCount": 1,
        "content": "Strong Consistency: Cloud Spanner provides strong consistency, meaning that all users see the same, up-to-date data, regardless of their location. This is essential for a community-based application like HipLocal, where users need to interact with the same information.\nGlobal Distribution: Cloud Spanner is globally distributed, allowing HipLocal to deploy their application in multiple regions and ensure low latency for users worldwide."
      },
      {
        "date": "2024-07-17T03:42:00.000Z",
        "voteCount": 1,
        "content": "Let's look at why the other options are less suitable:\n\nA. Use local SSDs to store state: Local SSDs are ephemeral storage and would be lost if a virtual machine instance is restarted. This is not a reliable solution for storing application state.\nB. Put a memcache layer in front of MySQL: Memcache can improve performance, but it doesn't address the scalability, availability, and consistency challenges of a single MySQL instance.\nD. Replace the MySQL instance with Cloud SQL: Cloud SQL is a managed MySQL service, but it doesn't offer the same level of global distribution, scalability, and consistency as Cloud Spanner."
      },
      {
        "date": "2024-07-13T01:20:00.000Z",
        "voteCount": 1,
        "content": "correct answer is C"
      },
      {
        "date": "2024-03-07T14:50:00.000Z",
        "voteCount": 1,
        "content": "C. Move the state storage to Cloud Spanner: Cloud Spanner is a fully managed, horizontally scalable database service with global distribution and strong consistency. It's well-suited for applications that require a high degree of scalability and reliability, making it a good choice for HipLocal as they expand globally.\nGiven HipLocal's need for scalability and global distribution, Cloud Spanner (option C) is likely the best fit. It provides the scalability and global reach necessary for their expansion, along with the relational database capabilities that their application likely relies on. Cloud SQL (option D) is also a good choice if their current scale and distribution requirements can be met within its limits."
      },
      {
        "date": "2023-11-18T11:09:00.000Z",
        "voteCount": 2,
        "content": "Moving the state storage to Cloud Spanner, is the best solution for HipLocal because Cloud Spanner is a globally distributed, horizontally scalable, and strongly consistent relational database service. \n\nCloud Spanner provides high availability and can handle automatic scaling, backups, and updates. In addition, it provides support for distributed transactions, and it's designed to provide low latency and high throughput. \n\nCloud Spanner can also help HipLocal meet their compliance requirements, as it supports HIPAA, GDPR, and other regulatory standards."
      },
      {
        "date": "2023-09-19T03:18:00.000Z",
        "voteCount": 1,
        "content": "Given HipLocal\u2019s requirements for global expansion and high availability, moving the state storage to Cloud Spanner (option C) would likely be the most suitable choice."
      },
      {
        "date": "2023-05-06T11:27:00.000Z",
        "voteCount": 2,
        "content": "to meet their business requirements\nspanner is very expensive, we need to reduce costs after all\ncloud sql with multi-regional replication fits our needs\nhttps://cloud.google.com/sql/docs/mysql/replication#replication_use_cases"
      },
      {
        "date": "2022-08-29T21:11:00.000Z",
        "voteCount": 2,
        "content": "https://stackoverflow.com/questions/60412688/whats-the-difference-between-google-cloud-spanner-and-cloud-sql#:~:text=The%20main%20difference%20between%20Cloud,of%20writes%20per%20second%2C%20globally.\n\nC is correct for me."
      },
      {
        "date": "2022-08-29T21:51:00.000Z",
        "voteCount": 1,
        "content": "But, Spanner is not a formal relational DB so Cloud SQL is the best solution, plus is has the data replication for global purpose."
      },
      {
        "date": "2022-08-19T23:44:00.000Z",
        "voteCount": 1,
        "content": "D  is correct"
      },
      {
        "date": "2022-07-19T11:18:00.000Z",
        "voteCount": 2,
        "content": "vote for D as also cloud SQL could be built for multi-region replica\nhttps://cloud.google.com/blog/products/databases/introducing-cross-region-replica-for-cloud-sql"
      },
      {
        "date": "2023-05-06T11:07:00.000Z",
        "voteCount": 1,
        "content": "replica is OK to read data.\nbut you have to write to master node which could be in a different continent\nhttps://cloud.google.com/sql/docs/mysql/replication#read-replicas\nhttps://cloud.google.com/sql/docs/mysql/replication#cross-region-read-replicas"
      },
      {
        "date": "2022-07-04T22:47:00.000Z",
        "voteCount": 1,
        "content": "Let's go with Cloud Spanner as it is the only supported global solution."
      },
      {
        "date": "2022-05-19T08:16:00.000Z",
        "voteCount": 1,
        "content": "C- Spanner, business requirements are to expand globally."
      },
      {
        "date": "2022-03-28T09:42:00.000Z",
        "voteCount": 2,
        "content": "Looks like C is the answer"
      },
      {
        "date": "2022-02-24T05:11:00.000Z",
        "voteCount": 2,
        "content": "CloudSql is better and cheaper option compared to Spanner"
      },
      {
        "date": "2022-04-03T14:14:00.000Z",
        "voteCount": 1,
        "content": "CloudSql is regional."
      },
      {
        "date": "2021-07-02T20:24:00.000Z",
        "voteCount": 3,
        "content": "\"Expand availability of the application to new regions\"\n\"Ensure a consistent experience for users when they travel to different regions\"\n\nThe above would indicate use of Cloud Spanner; answer should be C"
      },
      {
        "date": "2021-08-06T20:04:00.000Z",
        "voteCount": 1,
        "content": "Changing to D Cloud SQL since qn50 answer is Cloud SQL\nMatter of consistency I believe"
      },
      {
        "date": "2023-12-21T13:23:00.000Z",
        "voteCount": 1,
        "content": "each question for case study is independent"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 49,
    "url": "https://www.examtopics.com/discussions/google/view/36605-exam-professional-cloud-developer-topic-1-question-49/",
    "body": "Case study -<br>This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.<br>To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.<br>At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.<br><br>To start the case study -<br>To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an<br>All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.<br><br>Company Overview -<br>HipLocal is a community application designed to facilitate communication between people in close proximity. It is used for event planning and organizing sporting events, and for businesses to connect with their local communities. HipLocal launched recently in a few neighborhoods in Dallas and is rapidly growing into a global phenomenon. Its unique style of hyper-local community communication and business outreach is in demand around the world.<br><br>Executive Statement -<br>We are the number one local community app; it's time to take our local community services global. Our venture capital investors want to see rapid growth and the same great experience for new local and virtual communities that come online, whether their members are 10 or 10000 miles away from each other.<br><br>Solution Concept -<br>HipLocal wants to expand their existing service, with updated functionality, in new regions to better serve their global customers. They want to hire and train a new team to support these regions in their time zones. They will need to ensure that the application scales smoothly and provides clear uptime data.<br><br>Existing Technical Environment -<br>HipLocal's environment is a mix of on-premises hardware and infrastructure running in Google Cloud Platform. The HipLocal team understands their application well, but has limited experience in global scale applications. Their existing technical environment is as follows:<br>* Existing APIs run on Compute Engine virtual machine instances hosted in GCP.<br>* State is stored in a single instance MySQL database in GCP.<br>* Data is exported to an on-premises Teradata/Vertica data warehouse.<br>* Data analytics is performed in an on-premises Hadoop environment.<br>* The application has no logging.<br>* There are basic indicators of uptime; alerts are frequently fired when the APIs are unresponsive.<br><br>Business Requirements -<br>HipLocal's investors want to expand their footprint and support the increase in demand they are seeing. Their requirements are:<br>* Expand availability of the application to new regions.<br>* Increase the number of concurrent users that can be supported.<br>* Ensure a consistent experience for users when they travel to different regions.<br>* Obtain user activity metrics to better understand how to monetize their product.<br>* Ensure compliance with regulations in the new regions (for example, GDPR).<br>* Reduce infrastructure management time and cost.<br>* Adopt the Google-recommended practices for cloud computing.<br><br>Technical Requirements -<br>* The application and backend must provide usage metrics and monitoring.<br>* APIs require strong authentication and authorization.<br>* Logging must be increased, and data should be stored in a cloud analytics platform.<br>* Move to serverless architecture to facilitate elastic scaling.<br>* Provide authorized access to internal apps in a secure manner.<br>Which service should HipLocal use for their public APIs?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Armor",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Functions",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Endpoints\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tShielded Virtual Machines"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-03-07T14:51:00.000Z",
        "voteCount": 1,
        "content": "C. Cloud Endpoints\n\nCloud Endpoints is a service in Google Cloud that helps you create, deploy, and manage APIs. It offers features like monitoring, logging, authentication, API keys, and quota management. Cloud Endpoints is designed to ensure that APIs are scalable, secure, and easily manageable, which is crucial for HipLocal as they are planning to expand their services globally."
      },
      {
        "date": "2023-09-22T04:28:00.000Z",
        "voteCount": 1,
        "content": "c is the correct for api management"
      },
      {
        "date": "2023-09-19T03:20:00.000Z",
        "voteCount": 1,
        "content": "Google Cloud Endpoints is a distributed API management system that provides an API console, hosting, logging, monitoring, and other features to help you create, deploy, and manage APIs on a large scale."
      },
      {
        "date": "2022-08-19T23:45:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2022-07-19T12:20:00.000Z",
        "voteCount": 1,
        "content": "vote for C"
      },
      {
        "date": "2022-01-08T04:26:00.000Z",
        "voteCount": 1,
        "content": "The answer is C. Definitely Cloud Endpoints will be used in this case."
      },
      {
        "date": "2021-07-02T20:25:00.000Z",
        "voteCount": 1,
        "content": "This is C for sure; other options are wrong"
      },
      {
        "date": "2021-06-30T05:39:00.000Z",
        "voteCount": 1,
        "content": "The right answer could be C.\nhttps://cloud.google.com/endpoints/docs/openapi/about-cloud-endpoints"
      },
      {
        "date": "2020-11-09T13:31:00.000Z",
        "voteCount": 3,
        "content": "C is correct answer here."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 50,
    "url": "https://www.examtopics.com/discussions/google/view/36606-exam-professional-cloud-developer-topic-1-question-50/",
    "body": "Case study -<br>This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.<br>To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.<br>At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.<br><br>To start the case study -<br>To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an<br>All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.<br><br>Company Overview -<br>HipLocal is a community application designed to facilitate communication between people in close proximity. It is used for event planning and organizing sporting events, and for businesses to connect with their local communities. HipLocal launched recently in a few neighborhoods in Dallas and is rapidly growing into a global phenomenon. Its unique style of hyper-local community communication and business outreach is in demand around the world.<br><br>Executive Statement -<br>We are the number one local community app; it's time to take our local community services global. Our venture capital investors want to see rapid growth and the same great experience for new local and virtual communities that come online, whether their members are 10 or 10000 miles away from each other.<br><br>Solution Concept -<br>HipLocal wants to expand their existing service, with updated functionality, in new regions to better serve their global customers. They want to hire and train a new team to support these regions in their time zones. They will need to ensure that the application scales smoothly and provides clear uptime data.<br><br>Existing Technical Environment -<br>HipLocal's environment is a mix of on-premises hardware and infrastructure running in Google Cloud Platform. The HipLocal team understands their application well, but has limited experience in global scale applications. Their existing technical environment is as follows:<br>* Existing APIs run on Compute Engine virtual machine instances hosted in GCP.<br>* State is stored in a single instance MySQL database in GCP.<br>* Data is exported to an on-premises Teradata/Vertica data warehouse.<br>* Data analytics is performed in an on-premises Hadoop environment.<br>* The application has no logging.<br>* There are basic indicators of uptime; alerts are frequently fired when the APIs are unresponsive.<br><br>Business Requirements -<br>HipLocal's investors want to expand their footprint and support the increase in demand they are seeing. Their requirements are:<br>* Expand availability of the application to new regions.<br>* Increase the number of concurrent users that can be supported.<br>* Ensure a consistent experience for users when they travel to different regions.<br>* Obtain user activity metrics to better understand how to monetize their product.<br>* Ensure compliance with regulations in the new regions (for example, GDPR).<br>* Reduce infrastructure management time and cost.<br>* Adopt the Google-recommended practices for cloud computing.<br><br>Technical Requirements -<br>* The application and backend must provide usage metrics and monitoring.<br>* APIs require strong authentication and authorization.<br>* Logging must be increased, and data should be stored in a cloud analytics platform.<br>* Move to serverless architecture to facilitate elastic scaling.<br>* Provide authorized access to internal apps in a secure manner.<br>HipLocal wants to improve the resilience of their MySQL deployment, while also meeting their business and technical requirements.<br>Which configuration should they choose?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the current single instance MySQL on Compute Engine and several read-only MySQL servers on Compute Engine.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the current single instance MySQL on Compute Engine, and replicate the data to Cloud SQL in an external master configuration.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReplace the current single instance MySQL instance with Cloud SQL, and configure high availability.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReplace the current single instance MySQL instance with Cloud SQL, and Google provides redundancy without further configuration."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-11-09T13:39:00.000Z",
        "voteCount": 10,
        "content": "C is correct answer"
      },
      {
        "date": "2021-05-20T02:41:00.000Z",
        "voteCount": 2,
        "content": "true, though with high availability data can be replicated in multiple regions with the risk of not being compliant with GDPR.\nThe database should be replicated in specific regions, not globally, and I guess that this is achieved using multiple cloud sql instances rather than a single instance (or spanner) with high availability. Then those instances are kept synchronized with pubsub triggers.\nI'm not sure though"
      },
      {
        "date": "2022-05-27T06:52:00.000Z",
        "voteCount": 1,
        "content": "The HA configuration, sometimes called a cluster, provides data redundancy. A Cloud SQL instance configured for HA is also called a regional instance and is located in a primary and secondary zone within the configured region. \nSo the problem related to be GDPR compliant doesn't exists with Cloud SQL HA\nhttps://cloud.google.com/sql/docs/mysql/high-availability"
      },
      {
        "date": "2024-03-07T14:53:00.000Z",
        "voteCount": 1,
        "content": "C. Replace the current single instance MySQL instance with Cloud SQL, and configure high availability.\n\nGiven HipLocal's requirements for improved resilience and considering their business and technical needs, moving to Cloud SQL with a high availability (HA) configuration is the most suitable choice. Cloud SQL is a fully managed database service that simplifies database maintenance, backups, and scalability. The high availability configuration in Cloud SQL ensures that there is a failover replica in a different zone, which provides automatic failover in case of an outage, thus improving the resilience of the deployment."
      },
      {
        "date": "2023-11-01T03:24:00.000Z",
        "voteCount": 1,
        "content": "D is the correct answer a cloud sql provide redundancy without further configuration."
      },
      {
        "date": "2023-09-19T08:07:00.000Z",
        "voteCount": 1,
        "content": "C is correct."
      },
      {
        "date": "2022-09-28T02:29:00.000Z",
        "voteCount": 1,
        "content": "C is the correct answer."
      },
      {
        "date": "2022-08-19T23:46:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2022-01-18T00:34:00.000Z",
        "voteCount": 1,
        "content": "Probabilly is C, but they want use their implementation -&gt; B"
      },
      {
        "date": "2022-01-08T04:29:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer is C"
      },
      {
        "date": "2022-01-02T13:29:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2021-07-02T20:30:00.000Z",
        "voteCount": 2,
        "content": "MySQL; so use Cloud SQL\n\nhttps://cloud.google.com/sql/docs/mysql/high-availability\n\nAnswer is C"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 51,
    "url": "https://www.examtopics.com/discussions/google/view/36607-exam-professional-cloud-developer-topic-1-question-51/",
    "body": "Your application is running in multiple Google Kubernetes Engine clusters. It is managed by a Deployment in each cluster. The Deployment has created multiple replicas of your Pod in each cluster. You want to view the logs sent to stdout for all of the replicas in your Deployment in all clusters.<br>Which command should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tkubectl logs [PARAM]",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tgcloud logging read [PARAM]\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tkubectl exec \u05d2\u20ac\"it [PARAM] journalctl",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tgcloud compute ssh [PARAM] \u05d2\u20ac\"-command= \u05d2\u20acsudo journalctl\u05d2\u20ac"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-01-07T01:10:00.000Z",
        "voteCount": 6,
        "content": "Answer is A:\nTo view the logs sent to stdout for all replicas in a Deployment in multiple clusters, the correct command to use would be kubectl logs [PARAM].\n\nThe gcloud logging read command reads log entries from the specified logs. It does not allow you to view the logs of specific replicas in a Deployment across multiple clusters. kubectl logs allows you to view the logs of a specific Pod or Deployment across multiple clusters. You can specify the Deployment name and the relevant parameters to view the logs of all replicas in the Deployment.\n\nFor example, the following command would allow you to view the logs of all replicas in a Deployment named \"my-deployment\" in all clusters:\n\nkubectl logs -l app=my-deployment --all-containers"
      },
      {
        "date": "2023-01-07T01:10:00.000Z",
        "voteCount": 1,
        "content": "gcloud logging read [PARAM], can be used to read log entries from Stackdriver Logging, but it is not specifically designed for viewing the logs of Pods in a Kubernetes cluster. Additionally, gcloud logging read does not have a way to filter the log entries based on the Pod or Deployment, so it would not be possible to use it to view the logs for all of the replicas in a Deployment across multiple clusters"
      },
      {
        "date": "2022-08-19T23:46:00.000Z",
        "voteCount": 5,
        "content": "B is correct"
      },
      {
        "date": "2024-03-07T18:25:00.000Z",
        "voteCount": 1,
        "content": "B. gcloud logging read [PARAM]: Google Cloud's operations suite (formerly known as Stackdriver) aggregates logs from all the pods across all the GKE clusters. By using gcloud logging read, you can query these logs with specific parameters (like the name of the Deployment, container, or other filters) to view the combined logs from all replicas across all clusters. This command provides a centralized way to access logs at scale."
      },
      {
        "date": "2023-09-19T20:22:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2023-02-20T02:27:00.000Z",
        "voteCount": 2,
        "content": "choose B. gcloud logging also can be used for quering pod log\nhttps://stackoverflow.com/questions/62007471/how-to-view-container-logs-via-stackdriver-on-gke"
      },
      {
        "date": "2022-04-10T21:12:00.000Z",
        "voteCount": 2,
        "content": "B\nhttps://cloud.google.com/sdk/gcloud/reference/logging/read"
      },
      {
        "date": "2022-01-08T04:33:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is B"
      },
      {
        "date": "2021-11-16T17:05:00.000Z",
        "voteCount": 2,
        "content": "Correct answer is B"
      },
      {
        "date": "2021-11-01T13:07:00.000Z",
        "voteCount": 2,
        "content": "Correct answer is B\nhttps://cloud.google.com/logging/docs/reference/tools/gcloud-logging#examples_2"
      },
      {
        "date": "2021-10-01T01:38:00.000Z",
        "voteCount": 1,
        "content": "A and B"
      },
      {
        "date": "2021-07-18T03:33:00.000Z",
        "voteCount": 1,
        "content": "B) with parameters : resource.type=(\"k8s_container\" OR \"container\" OR \"k8s_cluster\" OR \"gke_cluster\" OR \"gke_nodepool\" OR \"k8s_node\")"
      },
      {
        "date": "2021-07-18T01:45:00.000Z",
        "voteCount": 3,
        "content": "B: gcloud logging read\n\nUsing the \"gcloud logging read\" command, select the appropriate cluster, node, pod, and container logs.\nhttps://cloud.google.com/stackdriver/docs/solutions/gke/using-logs#accessing_your_logs\n\nHowever if you use \"kubectl logs\" to see logs on CLI, logs won\u2019t be seen readable. It prints each line as a JSON object. \nhttps://medium.com/google-cloud/display-gke-logs-in-a-text-format-with-kubectl-db0169be0282"
      },
      {
        "date": "2021-07-03T17:25:00.000Z",
        "voteCount": 1,
        "content": "https://kubernetes.io/docs/reference/kubectl/cheatsheet/\n\nI would take A"
      },
      {
        "date": "2021-07-30T21:55:00.000Z",
        "voteCount": 3,
        "content": "Changing to B\n\nhttps://cloud.google.com/blog/products/management-tools/using-logging-your-apps-running-kubernetes-engine:\n\"gcloud command line tool \u2013 Using the gcloud logging read command, select the appropriate cluster, node, pod and container logs.\""
      },
      {
        "date": "2021-06-30T05:49:00.000Z",
        "voteCount": 3,
        "content": "B for me.\ngcloud logging read"
      },
      {
        "date": "2021-02-19T02:03:00.000Z",
        "voteCount": 3,
        "content": "Option: B. (Link1: https://cloud.google.com/blog/products/management-tools/finding-your-gke-logs, Link2: https://cloud.google.com/sdk/gcloud/reference/logging/read)"
      },
      {
        "date": "2020-11-09T13:53:00.000Z",
        "voteCount": 4,
        "content": "A seems to be correct answer."
      },
      {
        "date": "2021-05-20T02:44:00.000Z",
        "voteCount": 2,
        "content": "doesn't this view the logs of a single cluster?"
      },
      {
        "date": "2021-06-30T05:46:00.000Z",
        "voteCount": 1,
        "content": "You need to change the cluster connection's in order to view the logs for a specific deployments."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 52,
    "url": "https://www.examtopics.com/discussions/google/view/36608-exam-professional-cloud-developer-topic-1-question-52/",
    "body": "You are using Cloud Build to create a new Docker image on each source code commit to a Cloud Source Repositories repository. Your application is built on every commit to the master branch. You want to release specific commits made to the master branch in an automated method.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tManually trigger the build for new releases.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a build trigger on a Git tag pattern. Use a Git tag convention for new releases.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a build trigger on a Git branch name pattern. Use a Git branch naming convention for new releases.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCommit your source code to a second Cloud Source Repositories repository with a second Cloud Build trigger. Use this repository for new releases only."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-11-09T13:55:00.000Z",
        "voteCount": 19,
        "content": "B is correct answer"
      },
      {
        "date": "2024-03-07T18:32:00.000Z",
        "voteCount": 1,
        "content": "B. Create a build trigger on a Git tag pattern. Use a Git tag convention for new releases.\n\nThis approach is effective for managing releases in an automated yet controlled manner. By creating a Cloud Build trigger that activates based on a specific Git tag pattern, you can automate the build and deployment process for new releases. Git tags are often used to mark release points in the repository, so this aligns well with common development practices.\n\nWhen a commit is tagged in the repository with a specific pattern (e.g., \"v1.0\", \"release-*\"), Cloud Build can automatically trigger a build and potentially a deployment process. This allows for a more deliberate release process compared to triggering on every commit to the master branch."
      },
      {
        "date": "2023-09-19T20:24:00.000Z",
        "voteCount": 1,
        "content": "B is correct."
      },
      {
        "date": "2022-08-19T23:46:00.000Z",
        "voteCount": 3,
        "content": "B is correct"
      },
      {
        "date": "2022-07-27T23:34:00.000Z",
        "voteCount": 3,
        "content": "I don't know why people are selecting C , the qus says commit to master . C literally does not make sense how commit to a feature branch can trigger a master build."
      },
      {
        "date": "2022-04-26T19:15:00.000Z",
        "voteCount": 2,
        "content": "Vote B"
      },
      {
        "date": "2021-10-24T12:20:00.000Z",
        "voteCount": 1,
        "content": "I think C correct  answer"
      },
      {
        "date": "2021-07-18T03:38:00.000Z",
        "voteCount": 4,
        "content": "C) is not correct because the question says the commits are made in master branch. \nB) is good answer. When you want a release create a tag release-* , in Cloud Build use this pattern for tag."
      },
      {
        "date": "2021-07-03T17:40:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/source-repositories/docs/integrating-with-cloud-build:\n\"you can set a trigger to start a build on commits that are made to a particular branch, or on commits that contain a particular tag\"\n\nI would take C since the qn states that the commit is made to a branch"
      },
      {
        "date": "2021-10-30T18:54:00.000Z",
        "voteCount": 1,
        "content": "specific commits made to the master branch --&gt; Tag would work here, team can continue pushing further changes in master branch, tag will point to specific version, so I feel correct answer is B"
      },
      {
        "date": "2021-04-11T21:24:00.000Z",
        "voteCount": 1,
        "content": "C. https://cloud.google.com/build/docs/automating-builds/create-manage-triggers has either branch pattern or tag pattern"
      },
      {
        "date": "2021-02-28T03:13:00.000Z",
        "voteCount": 4,
        "content": "Also B is correct, but in the questions is specificied \"branch name\" , not tag"
      },
      {
        "date": "2021-06-20T03:01:00.000Z",
        "voteCount": 4,
        "content": "\"You want to release specific commits made to the master branch in an automated method\"\n\nLooks like they're committing to the master branch, hence a tag would make more sense than a branch name."
      },
      {
        "date": "2021-09-20T13:10:00.000Z",
        "voteCount": 10,
        "content": "\"Your application is built on every commit to the master branch. You want to release SPECIFIC commits made to the master branch in an automated method.\"\n\nhttps://cloud.google.com/source-repositories/docs/integrating-with-cloud-build#create_a_build_trigger\n\nBoth branch name and tag are valid triggers, but the question suggests we want to target specific commits to our master branch, therefore B, using a tag, seems more legit in this case."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 53,
    "url": "https://www.examtopics.com/discussions/google/view/36720-exam-professional-cloud-developer-topic-1-question-53/",
    "body": "You are designing a schema for a table that will be moved from MySQL to Cloud Bigtable. The MySQL table is as follows:<br><img src=\"/assets/media/exam-media/04137/0004000001.png\" class=\"in-exam-image\"><br>How should you design a row key for Cloud Bigtable for this table?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet Account_id as a key.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet Account_id_Event_timestamp as a key.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet Event_timestamp_Account_id as a key.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet Event_timestamp as a key."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-11-10T15:34:00.000Z",
        "voteCount": 21,
        "content": "correct answer is B\nhttps://cloud.google.com/bigtable/docs/schema-design"
      },
      {
        "date": "2021-07-03T17:44:00.000Z",
        "voteCount": 3,
        "content": "From the link:\n\"Include a timestamp as part of your row key if you often need to retrieve data based on the time when it was recorded.\n\nFor example, your application might need to record performance-related data, such as CPU and memory usage, once per second for a large number of machines. Your row key for this data could combine an identifier for the machine with a timestamp for the data (for example, machine_4223421#1425330757685). Keep in mind that row keys are sorted lexicographically.\""
      },
      {
        "date": "2024-03-07T18:46:00.000Z",
        "voteCount": 1,
        "content": "Designing an appropriate row key for Cloud Bigtable requires considering the access patterns and ensuring that the read and write operations are spread evenly across the key space to avoid hotspots.\n\nB. Set Account_id_Event_timestamp as a key.\n\nThis option is likely the best choice because:\n\nCombining Account_id with Event_timestamp in the row key would allow you to maintain a good level of data distribution while preserving the ability to query efficiently by Account_id and sort by Event_timestamp within each account. This aligns well with Bigtable\u2019s strengths in handling large, scalable, and sparse datasets.\n\nBy leading with Account_id, you group all events for a single account close together in the key space, which can be efficient for reads that are interested in the activity of a specific account."
      },
      {
        "date": "2023-09-19T20:32:00.000Z",
        "voteCount": 1,
        "content": "I would go with B."
      },
      {
        "date": "2023-01-07T01:15:00.000Z",
        "voteCount": 1,
        "content": "B. Set Account_id_Event_timestamp as a key.\n\nThe primary key in the MySQL table is a composite key consisting of Account_id and Event_timestamp, so it would make sense to use both of these values as the row key in Cloud Bigtable. This allows for efficient querying and sorting by both Account_id and Event_timestamp."
      },
      {
        "date": "2023-01-07T01:15:00.000Z",
        "voteCount": 1,
        "content": "A would not be a good choice because the row key would not include the Event_timestamp, which is part of the primary key in the MySQL table. Option"
      },
      {
        "date": "2023-01-07T01:15:00.000Z",
        "voteCount": 1,
        "content": "D would not be a good choice because it would not include the Account_id, which is also part of the primary key in the MySQL table."
      },
      {
        "date": "2023-01-07T01:15:00.000Z",
        "voteCount": 1,
        "content": "C would not be a good choice because it would make it difficult to query and sort by Account_id."
      },
      {
        "date": "2022-08-19T23:47:00.000Z",
        "voteCount": 3,
        "content": "B is correct"
      },
      {
        "date": "2022-07-25T05:52:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/bigtable/docs/schema-design#row-keys\n\nIt's B because :\n\"Row keys that start with a timestamp. This pattern causes sequential writes to be pushed onto a single node, creating a hotspot. If you put a timestamp in a row key, precede it with a high-cardinality value like a user ID to avoid hotspotting.\""
      },
      {
        "date": "2022-04-20T02:41:00.000Z",
        "voteCount": 1,
        "content": "Include a timestamp as part of your row key and avoid having timestamp at the start of the key"
      },
      {
        "date": "2020-11-13T03:39:00.000Z",
        "voteCount": 2,
        "content": "Should be C. Account id as the first part of the key has no benifit for search"
      },
      {
        "date": "2021-05-20T05:00:00.000Z",
        "voteCount": 4,
        "content": "https://cloud.google.com/bigtable/docs/schema-design#row-keys\navoid row keys that starts with a timestamp.\nAlso using a key such as userID_timestamp allows bigtable to query related rows in a range rather than parsing the entire database."
      },
      {
        "date": "2020-12-21T10:18:00.000Z",
        "voteCount": 6,
        "content": "https://cloud.google.com/bigtable/docs/schema-design#timestamps - avoid placing a timestamp at the start of the row key. I vote for B."
      },
      {
        "date": "2021-01-02T06:20:00.000Z",
        "voteCount": 8,
        "content": "\"Row keys that start with a timestamp. This will cause sequential writes to be pushed onto a single node, creating a hotspot. If you put a timestamp in a row key, you need to precede it with a high-cardinality value like a user ID to avoid hotspotting.\""
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 54,
    "url": "https://www.examtopics.com/discussions/google/view/36609-exam-professional-cloud-developer-topic-1-question-54/",
    "body": "You want to view the memory usage of your application deployed on Compute Engine.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall the Stackdriver Client Library.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall the Stackdriver Monitoring Agent.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Stackdriver Metrics Explorer.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Google Cloud Platform Console."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 6,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-02-19T02:35:00.000Z",
        "voteCount": 16,
        "content": "Option-B is correct. https://cloud.google.com/monitoring/api/metrics_agent#agent-memory (By default Memory metrics is not collected). To double confirm. Just goto Console-&gt;Operations-&gt;Monitoring-&gt;Dashboards-&gt;VM Instances-&gt;Memory Tab (Assume you have VM running already). You will see a info message saying that No agents detected. Monitoring agents collect memory metrics, disk metrics, and more. Learn more about agents and how to manage them across multiple VMs."
      },
      {
        "date": "2021-09-09T04:54:00.000Z",
        "voteCount": 3,
        "content": "Correct, see following link for more detail\nhttps://stackoverflow.com/questions/43991246/google-cloud-platform-how-to-monitor-memory-usage-of-vm-instances"
      },
      {
        "date": "2021-01-25T11:40:00.000Z",
        "voteCount": 8,
        "content": "For me B si the  correct answer as you can not read memory usage directly from stackdriver without the monitoring agent"
      },
      {
        "date": "2024-07-17T04:24:00.000Z",
        "voteCount": 1,
        "content": "The best answer here is C. Use the Stackdriver Metrics Explorer.\n\nHere's why:\n\nStackdriver Metrics Explorer: The Stackdriver Metrics Explorer is a powerful tool for visualizing and analyzing metrics collected from your Google Cloud resources. It allows you to view various metrics, including memory usage, over time. You can filter and group metrics based on different criteria, making it easy to identify trends and potential issues."
      },
      {
        "date": "2024-07-17T04:25:00.000Z",
        "voteCount": 1,
        "content": "Let's look at why the other options are less suitable:\n\nA. Install the Stackdriver Client Library: The Stackdriver Client Library is used for programmatically interacting with Stackdriver from your application code. While it can be used to collect and send metrics, it's not the primary tool for viewing memory usage.\nB. Install the Stackdriver Monitoring Agent: The Stackdriver Monitoring Agent is responsible for collecting metrics from your Compute Engine instances. It's a necessary component for monitoring, but it doesn't provide a user interface for viewing metrics.\nD. Use the Google Cloud Platform Console: The Google Cloud Platform Console provides a general overview of your resources, but it doesn't offer the same level of detail and flexibility as the Stackdriver Metrics Explorer for viewing memory usage."
      },
      {
        "date": "2024-03-07T18:56:00.000Z",
        "voteCount": 1,
        "content": "B. Install the Stackdriver Monitoring Agent.\n\nThe Stackdriver Monitoring Agent allows you to collect more system-level and third-party application metrics than what is provided by default with Google Cloud's operations suite. By installing the agent on your Compute Engine instances, you can collect detailed memory usage metrics, which can then be viewed in the Google Cloud Console or through the Metrics Explorer in Google Cloud's operations suite (formerly Stackdriver)."
      },
      {
        "date": "2023-09-19T20:34:00.000Z",
        "voteCount": 1,
        "content": "I would go with B."
      },
      {
        "date": "2022-08-19T23:47:00.000Z",
        "voteCount": 3,
        "content": "B is correct"
      },
      {
        "date": "2022-04-27T08:11:00.000Z",
        "voteCount": 1,
        "content": "Vote B"
      },
      {
        "date": "2021-07-03T17:53:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/monitoring/agent\n\nB is correct"
      },
      {
        "date": "2021-03-03T17:25:00.000Z",
        "voteCount": 1,
        "content": "Option: D"
      },
      {
        "date": "2021-02-19T08:29:00.000Z",
        "voteCount": 3,
        "content": "I think option B and C are correct answer - you need to install stack driver manager on VM to push logs into stack driver and then you can use stack driver metrics to view the metrics."
      },
      {
        "date": "2020-11-09T14:09:00.000Z",
        "voteCount": 5,
        "content": "C is correct answer."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 55,
    "url": "https://www.examtopics.com/discussions/google/view/36767-exam-professional-cloud-developer-topic-1-question-55/",
    "body": "You have an analytics application that runs hundreds of queries on BigQuery every few minutes using BigQuery API. You want to find out how much time these queries take to execute.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Stackdriver Monitoring to plot slot usage.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Stackdriver Trace to plot API execution time.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Stackdriver Trace to plot query execution time.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Stackdriver Monitoring to plot query execution times.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 6,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-11-11T06:52:00.000Z",
        "voteCount": 7,
        "content": "You dont need to enable trace for this, Best and correct option is D"
      },
      {
        "date": "2020-12-26T03:31:00.000Z",
        "voteCount": 8,
        "content": "D is correct answer: https://cloud.google.com/bigquery/docs/monitoring"
      },
      {
        "date": "2021-07-03T17:58:00.000Z",
        "voteCount": 2,
        "content": "\"Use Cloud Monitoring to view BigQuery metrics and create charts and alerts\""
      },
      {
        "date": "2021-02-19T02:39:00.000Z",
        "voteCount": 2,
        "content": "Use this link and locate BigQuery: https://cloud.google.com/monitoring/api/metrics_gcp#gcp-bigquery"
      },
      {
        "date": "2024-03-07T19:01:00.000Z",
        "voteCount": 1,
        "content": "D. Use Stackdriver Monitoring to plot query execution times.\n\nStackdriver Monitoring (now part of Google Cloud's operations suite) provides capabilities to monitor BigQuery and create custom dashboards to visualize various metrics, including query execution times. You can track how long your queries take to run by plotting the query_execution_times metric in a custom dashboard."
      },
      {
        "date": "2023-12-31T00:35:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2023-09-19T20:38:00.000Z",
        "voteCount": 1,
        "content": "Option C is best suited here."
      },
      {
        "date": "2023-08-24T06:01:00.000Z",
        "voteCount": 1,
        "content": "https://www.exam-answer.com/the-best-way-to-measure-query-execution-time-in-bigquery"
      },
      {
        "date": "2023-03-28T05:33:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is C. Use Stackdriver Trace to plot query execution time. Stackdriver Trace is a distributed tracing system that allows you to profile and debug your application's performance. It allows you to trace requests across multiple services, and it provides a detailed breakdown of where time is being spent within your application. Since you want to find out how much time your queries take to execute, using Stackdriver Trace to plot query execution time would be the most appropriate approach."
      },
      {
        "date": "2023-01-09T08:22:00.000Z",
        "voteCount": 1,
        "content": "D is correct\nhttps://cloud.google.com/bigquery/docs/monitoring"
      },
      {
        "date": "2022-08-19T23:47:00.000Z",
        "voteCount": 3,
        "content": "D is correct"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 56,
    "url": "https://www.examtopics.com/discussions/google/view/42296-exam-professional-cloud-developer-topic-1-question-56/",
    "body": "You are designing a schema for a Cloud Spanner customer database. You want to store a phone number array field in a customer table. You also want to allow users to search customers by phone number.<br>How should you design this schema?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a table named Customers. Add an Array field in a table that will hold phone numbers for the customer.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a table named Customers. Create a table named Phones. Add a CustomerId field in the Phones table to find the CustomerId from a phone number.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a table named Customers. Add an Array field in a table that will hold phone numbers for the customer. Create a secondary index on the Array field.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a table named Customers as a parent table. Create a table named Phones, and interleave this table into the Customer table. Create an index on the phone number field in the Phones table.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 8,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-01-13T17:55:00.000Z",
        "voteCount": 12,
        "content": "i vote D since it said 'interleave'"
      },
      {
        "date": "2021-02-19T02:45:00.000Z",
        "voteCount": 7,
        "content": "Correct. Just sharing a link: https://cloud.google.com/spanner/docs/schema-and-data-model#creating_a_hierarchy_of_interleaved_tables"
      },
      {
        "date": "2021-04-18T06:23:00.000Z",
        "voteCount": 7,
        "content": "Correct answer is C, as in the question states: \"You want to store a phone number array field in a customer table\". So... adding the phone number as array field and adding a secondary index should be the best option in this case."
      },
      {
        "date": "2021-05-20T05:11:00.000Z",
        "voteCount": 1,
        "content": "i say B, because if a user has more numbers you are storing the same user multiple times each time changing the phone number.\nHaving a second table for phone numbers and having a foreign key that points to the user with this phone number avoid this duplication problem."
      },
      {
        "date": "2024-07-17T07:40:00.000Z",
        "voteCount": 1,
        "content": "The best approach is D. Create a table named Customers as a parent table. Create a table named Phones and interleave this table into the Customer table. Create an index on the phone number field in the Phones table.\n\nHere's why:\n\nInterleaved Tables: Interleaved tables in Cloud Spanner are designed for efficient storage and retrieval of related data. By interleaving the Phones table into the Customers table, you ensure that data for a specific customer is stored together, improving query performance.\nIndexing for Search: Creating an index on the phone number field in the Phones table allows for efficient searching of customers based on their phone numbers. This is crucial for your requirement to allow users to search customers by phone number.\nScalability: This approach scales well as your customer database grows. Cloud Spanner automatically handles the distribution and scaling of data across multiple nodes."
      },
      {
        "date": "2024-07-17T07:40:00.000Z",
        "voteCount": 1,
        "content": "Let's look at why the other options are less suitable:\n\nA. Create a table named Customers. Add an Array field in a table that will hold phone numbers for the customer. This approach is inefficient for searching. You would need to scan the entire array field for each customer, which can be slow for large datasets.\nB. Create a table named Customers. Create a table named Phones. Add a CustomerId field in the Phones table to find the CustomerId from a phone number. This approach is less efficient than interleaving. You would need to join the Customers and Phones tables for every search, which can be slower than using interleaved tables.\nC. Create a table named Customers. Add an Array field in a table that will hold phone numbers for the customer. Create a secondary index on the Array field. Cloud Spanner doesn't support secondary indexes on array fields."
      },
      {
        "date": "2024-03-07T19:10:00.000Z",
        "voteCount": 1,
        "content": "D. Create a table named Customers as a parent table. Create a table named Phones, and interleave this table into the Customer table. Create an index on the phone number field in the Phones table.\n\nThis design will allow you to store multiple phone numbers for each customer and efficiently search for customers by their phone numbers. In Cloud Spanner, tables can be interleaved, which means that the child table's rows are co-located with the parent table's rows. This setup can offer better performance for certain types of queries and data models, especially when there's a strong relational structure."
      },
      {
        "date": "2023-09-19T20:40:00.000Z",
        "voteCount": 1,
        "content": "I will go with D."
      },
      {
        "date": "2023-01-07T01:20:00.000Z",
        "voteCount": 3,
        "content": "The correct answer is D. You should create a table named Customers as a parent table and a table named Phones, and interleave this table into the Customer table. You should also create an index on the phone number field in the Phones table. This allows you to store the phone number array field in the Customers table and search for customers by phone number using the index on the Phones table."
      },
      {
        "date": "2023-01-07T01:21:00.000Z",
        "voteCount": 3,
        "content": "C is not a valid solution because Cloud Spanner does not allow creating secondary indexes on array fields."
      },
      {
        "date": "2022-08-19T23:47:00.000Z",
        "voteCount": 3,
        "content": "D is correct"
      },
      {
        "date": "2022-07-04T23:36:00.000Z",
        "voteCount": 1,
        "content": "It's D."
      },
      {
        "date": "2022-06-26T00:45:00.000Z",
        "voteCount": 1,
        "content": "Search on ARRAY column is best here. Answer: A"
      },
      {
        "date": "2022-02-15T14:12:00.000Z",
        "voteCount": 3,
        "content": "D seems quite nice, but what do u thing about statement \"You want to store a phone number array field in a customer table.\", and interleave in another table, not customer one.\n\nIn sql there is array field, and using UNNEST function is possible to filter records based on array, then answer A"
      },
      {
        "date": "2022-01-08T04:50:00.000Z",
        "voteCount": 1,
        "content": "D is the correct answer"
      },
      {
        "date": "2021-07-03T19:24:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/spanner/docs/schema-design#creating-indexes:\n\"bad idea to create non-interleaved indexes on columns whose values are monotonically increasing or decreasing\"\n\nSince phone numbers monotonically increase/decrease, I would take D as the answer"
      },
      {
        "date": "2021-06-22T21:51:00.000Z",
        "voteCount": 4,
        "content": "https://cloud.google.com/spanner/docs/data-types   --&gt;can't set secondary index in array\nso I vote D"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 57,
    "url": "https://www.examtopics.com/discussions/google/view/36772-exam-professional-cloud-developer-topic-1-question-57/",
    "body": "You are deploying a single website on App Engine that needs to be accessible via the URL http://www.altostrat.com/.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tVerify domain ownership with Webmaster Central. Create a DNS CNAME record to point to the App Engine canonical name ghs.googlehosted.com.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tVerify domain ownership with Webmaster Central. Define an A record pointing to the single global App Engine IP address.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDefine a mapping in dispatch.yaml to point the domain www.altostrat.com to your App Engine service. Create a DNS CNAME record to point to the App Engine canonical name ghs.googlehosted.com.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDefine a mapping in dispatch.yaml to point the domain www.altostrat.com to your App Engine service. Define an A record pointing to the single global App Engine IP address."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-09-19T20:46:00.000Z",
        "voteCount": 1,
        "content": "A is correct."
      },
      {
        "date": "2023-01-09T08:38:00.000Z",
        "voteCount": 1,
        "content": "A, I have done that on a project and you don't need to fo routing with a dispatch.yaml file so A is enough to have a custom domain link to your app engine."
      },
      {
        "date": "2023-01-07T01:26:00.000Z",
        "voteCount": 1,
        "content": "Yes, you are correct guys that you can use a custom domain with App Engine and map it to your app. However, option A is incorrect because it is not sufficient to just create a DNS CNAME record pointing to the App Engine canonical name ghs.googlehosted.com. You also need to map the domain to your app in App Engine as described in option C or D."
      },
      {
        "date": "2023-01-07T01:27:00.000Z",
        "voteCount": 1,
        "content": "D is incorrect because it involves defining an A record, which can cause issues with latency and is not the recommended method for mapping a custom domain to App Engine. Additionally, it does not include the dispatch.yaml mapping, which is necessary to associate the custom domain with your App Engine service."
      },
      {
        "date": "2023-01-07T01:26:00.000Z",
        "voteCount": 2,
        "content": "The correct answer is C. To use a custom domain with App Engine, you need to define a mapping in the dispatch.yaml file to point the domain to your App Engine service. Additionally, you should create a DNS CNAME record to point to the App Engine canonical name ghs.googlehosted.com."
      },
      {
        "date": "2023-01-07T01:27:00.000Z",
        "voteCount": 1,
        "content": "Option A is incorrect because it does not include the dispatch.yaml mapping, which is necessary to associate the custom domain with your App Engine service."
      },
      {
        "date": "2023-01-07T01:27:00.000Z",
        "voteCount": 1,
        "content": "B is incorrect because it involves defining an A record, which can cause issues with latency and is not the recommended method for mapping a custom domain to App Engine."
      },
      {
        "date": "2022-08-19T23:48:00.000Z",
        "voteCount": 3,
        "content": "A is correct"
      },
      {
        "date": "2022-04-27T08:21:00.000Z",
        "voteCount": 2,
        "content": "Vote A"
      },
      {
        "date": "2022-01-19T18:09:00.000Z",
        "voteCount": 2,
        "content": "A is right"
      },
      {
        "date": "2021-11-30T01:07:00.000Z",
        "voteCount": 2,
        "content": "Agree with A. That is what we are doing in our current project."
      },
      {
        "date": "2021-07-18T01:34:00.000Z",
        "voteCount": 3,
        "content": "A: \nhttps://support.google.com/domains/answer/6009957?hl=en\napp.mydomain.com CNAME 1H  ghs.googlehosted.com."
      },
      {
        "date": "2021-07-03T19:37:00.000Z",
        "voteCount": 2,
        "content": "I would take B\n\nhttps://cloud.google.com/appengine/docs/flexible/dotnet/mapping-custom-domains?hl=fa:\n\"In A or AAAA records, the record data is an IP address\""
      },
      {
        "date": "2021-07-30T22:23:00.000Z",
        "voteCount": 1,
        "content": "Changing to A based on the link celia20200410 gave"
      },
      {
        "date": "2020-11-11T07:52:00.000Z",
        "voteCount": 3,
        "content": "A is correct option here."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 58,
    "url": "https://www.examtopics.com/discussions/google/view/36774-exam-professional-cloud-developer-topic-1-question-58/",
    "body": "You are running an application on App Engine that you inherited. You want to find out whether the application is using insecure binaries or is vulnerable to XSS attacks.<br>Which service should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Amor",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStackdriver Debugger",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Security Scanner\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStackdriver Error Reporting"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-09-19T20:47:00.000Z",
        "voteCount": 1,
        "content": "C is correct."
      },
      {
        "date": "2022-08-19T23:48:00.000Z",
        "voteCount": 3,
        "content": "C is correct"
      },
      {
        "date": "2022-04-27T08:21:00.000Z",
        "voteCount": 2,
        "content": "Vote C"
      },
      {
        "date": "2021-07-03T19:42:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/appengine/docs/standard/python/application-security:\n\"The Google Cloud Web Security Scanner discovers vulnerabilities by crawling your App Engine app, following all that links within the scope of your starting URLs, and attempting to exercise as many user inputs and event handlers as possible.\"\n\nhttps://cloud.google.com/security-command-center/docs/concepts-web-security-scanner-overview:\n\"Web Security Scanner custom scans provide granular information about application vulnerability findings, like outdated libraries, cross-site scripting, or use of mixed content\"\n\nC is correct"
      },
      {
        "date": "2020-11-11T07:53:00.000Z",
        "voteCount": 2,
        "content": "C is correct answer here.\n\nhttps://cloud.google.com/appengine/docs/standard/python/application-security"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 59,
    "url": "https://www.examtopics.com/discussions/google/view/36775-exam-professional-cloud-developer-topic-1-question-59/",
    "body": "You are working on a social media application. You plan to add a feature that allows users to upload images. These images will be 2 MB `\" 1 GB in size. You want to minimize their infrastructure operations overhead for this feature.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the application to accept images directly and store them in the database that stores other user information.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the application to create signed URLs for Cloud Storage. Transfer these signed URLs to the client application to upload images to Cloud Storage.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up a web server on GCP to accept user images and create a file store to keep uploaded files. Change the application to retrieve images from the file store.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a separate bucket for each user in Cloud Storage. Assign a separate service account to allow write access on each bucket. Transfer service account credentials to the client application based on user information. The application uses this service account to upload images to Cloud Storage."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-07-03T19:46:00.000Z",
        "voteCount": 5,
        "content": "\"upload images\" so use Cloud Storage; leaving B and D\n\n\"minimize their infrastructure operations\" so B is the answer\n\nAlso, signed URLs provide additional security"
      },
      {
        "date": "2023-09-19T20:49:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2022-08-19T23:48:00.000Z",
        "voteCount": 3,
        "content": "B is correct"
      },
      {
        "date": "2022-04-27T08:22:00.000Z",
        "voteCount": 2,
        "content": "Vote B"
      },
      {
        "date": "2020-11-11T08:16:00.000Z",
        "voteCount": 3,
        "content": "B seems to be logical answer here."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 60,
    "url": "https://www.examtopics.com/discussions/google/view/36776-exam-professional-cloud-developer-topic-1-question-60/",
    "body": "Your application is built as a custom machine image. You have multiple unique deployments of the machine image. Each deployment is a separate managed instance group with its own template. Each deployment requires a unique set of configuration values. You want to provide these unique values to each deployment but use the same custom machine image in all deployments. You want to use out-of-the-box features of Compute Engine.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPlace the unique configuration values in the persistent disk.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPlace the unique configuration values in a Cloud Bigtable table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPlace the unique configuration values in the instance template startup script.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPlace the unique configuration values in the instance template instance metadata.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-07-03T19:53:00.000Z",
        "voteCount": 7,
        "content": "A and B are wrong for sure\n\nhttps://cloud.google.com/compute/docs/instances/startup-scripts:\n\"A startup script is a file that contains commands that run when a virtual machine (VM) instance boots:\n\nAnswer is D"
      },
      {
        "date": "2020-11-11T08:21:00.000Z",
        "voteCount": 6,
        "content": "C would be correct answer here."
      },
      {
        "date": "2024-03-07T19:24:00.000Z",
        "voteCount": 1,
        "content": "D. Place the unique configuration values in the instance template instance metadata.\n\nThe instance metadata is a good place to store configuration values that are unique to each managed instance group while using the same machine image across all deployments. This allows you to use the same base image but customize the behavior of each instance group based on the metadata passed to them. Metadata can be accessed by the instances at startup, and scripts can be written to configure the instance based on these values."
      },
      {
        "date": "2023-09-19T20:54:00.000Z",
        "voteCount": 1,
        "content": "D is correct."
      },
      {
        "date": "2023-01-08T00:02:00.000Z",
        "voteCount": 2,
        "content": "Option D is the correct answer. Instance metadata is metadata that is associated with a Compute Engine instance and can be used to pass configuration values to the instance at startup. It can be accessed from within the instance itself, allowing you to use the same custom machine image in all deployments and still provide unique configuration values to each deployment. Option A is not a good solution because the persistent disk is not automatically attached to the instance at startup and is not intended for storing configuration values. Option B is not a good solution because Cloud Bigtable is a NoSQL database, which is not well-suited for storing configuration values. Option C is not a good solution because the startup script is executed after the instance has started, so it cannot be used to pass configuration values to the instance at startup."
      },
      {
        "date": "2022-08-29T22:05:00.000Z",
        "voteCount": 2,
        "content": "Configuration values should be metadata"
      },
      {
        "date": "2022-08-19T23:49:00.000Z",
        "voteCount": 3,
        "content": "D is correct"
      },
      {
        "date": "2022-05-16T19:21:00.000Z",
        "voteCount": 1,
        "content": "the answer should be D"
      },
      {
        "date": "2022-01-08T04:59:00.000Z",
        "voteCount": 2,
        "content": "Correct answer is D"
      },
      {
        "date": "2021-11-01T13:26:00.000Z",
        "voteCount": 1,
        "content": "No - In this case it is a stateful MIG - per this link the answer is indeed A:\nhttps://cloud.google.com/compute/docs/instance-groups/stateful-migs#per_instance_configs"
      },
      {
        "date": "2021-09-04T20:40:00.000Z",
        "voteCount": 3,
        "content": "Each deployment is a separate managed instance group with its own template. \nEach deployment requires a unique set of configuration values. \n\nEither be C or D\n\nConfiguration values should be metadata\n\nWhich leads the answer to be D"
      },
      {
        "date": "2021-06-30T06:32:00.000Z",
        "voteCount": 4,
        "content": "Option could be D."
      },
      {
        "date": "2021-03-03T23:30:00.000Z",
        "voteCount": 4,
        "content": "Option D: at the time of deployment, configuration values in the instance template instance metadata."
      },
      {
        "date": "2021-04-09T06:57:00.000Z",
        "voteCount": 5,
        "content": "Agree with Option D based on this ref: https://cloud.google.com/compute/docs/storing-retrieving-metadata#custom"
      },
      {
        "date": "2021-06-22T21:57:00.000Z",
        "voteCount": 4,
        "content": "agree with option D"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 61,
    "url": "https://www.examtopics.com/discussions/google/view/69407-exam-professional-cloud-developer-topic-1-question-61/",
    "body": "Your application performs well when tested locally, but it runs significantly slower after you deploy it to a Compute Engine instance. You need to diagnose the problem. What should you do?<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFile a ticket with Cloud Support indicating that the application performs faster locally.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Debugger snapshots to look at a point-in-time execution of the application.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Profiler to determine which functions within the application take the longest amount of time.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd logging commands to the application and use Cloud Logging to check where the latency problem occurs."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-03-07T19:30:00.000Z",
        "voteCount": 1,
        "content": "C. Use Cloud Profiler to determine which functions within the application take the longest amount of time.\n\nCloud Profiler is a tool provided by Google Cloud that helps you analyze and understand the performance characteristics of your application. It allows you to see where the application spends its time, how much CPU and memory it uses, and which functions or methods are the most time-consuming. This is particularly useful when you need to diagnose performance bottlenecks that are not apparent during local development but become evident in a production environment, such as a Compute Engine instance."
      },
      {
        "date": "2023-09-19T21:34:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2023-01-13T06:07:00.000Z",
        "voteCount": 2,
        "content": "A is incorrect because the argument \u201cit worked on my machine\u201d but doesn\u2019t work on Google Cloud is never valid.\nB is incorrect because Debugger snapshots only lets us review the application at a single point in time.\nC is correct because it provides latency per function and historical latency information.\nD is incorrect because while it works it requires a lot of work and is not the clear, optimal choice."
      },
      {
        "date": "2023-01-08T00:03:00.000Z",
        "voteCount": 1,
        "content": "Option C is the correct answer. Cloud Profiler is a performance analysis tool that allows you to identify performance issues in your application. It provides detailed information about the time spent executing different functions in your application, which can help you identify the cause of the performance issue. Option A is not a good solution because filing a ticket with Cloud Support will not help you diagnose the problem. Option B is not a good solution because Cloud Debugger snapshots provide information about the state of variables at a specific point in time, but they do not provide information about the time spent executing different functions. Option D is not a good solution because adding logging commands to the application and using Cloud Logging can help you identify where the latency problem occurs, but it will not provide information about the time spent executing different functions."
      },
      {
        "date": "2022-09-28T02:55:00.000Z",
        "voteCount": 1,
        "content": "C is the correct answer"
      },
      {
        "date": "2022-08-19T23:49:00.000Z",
        "voteCount": 3,
        "content": "C is correct"
      },
      {
        "date": "2022-07-25T07:18:00.000Z",
        "voteCount": 1,
        "content": "Correct is C"
      },
      {
        "date": "2022-01-08T05:01:00.000Z",
        "voteCount": 3,
        "content": "Correct answer is C"
      },
      {
        "date": "2022-01-03T20:40:00.000Z",
        "voteCount": 1,
        "content": "This should be C"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 62,
    "url": "https://www.examtopics.com/discussions/google/view/45279-exam-professional-cloud-developer-topic-1-question-62/",
    "body": "You have an application running in App Engine. Your application is instrumented with Stackdriver Trace. The /product-details request reports details about four known unique products at /sku-details as shown below. You want to reduce the time it takes for the request to complete.<br>What should you do?<br><img src=\"/assets/media/exam-media/04137/0004500001.jpg\" class=\"in-exam-image\"><br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the size of the instance class.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the Persistent Disk type to SSD.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange /product-details to perform the requests in parallel.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the /sku-details information in a database, and replace the webservice call with a database query."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-02-20T07:12:00.000Z",
        "voteCount": 9,
        "content": "I agree with this, answer is C"
      },
      {
        "date": "2023-09-19T21:36:00.000Z",
        "voteCount": 1,
        "content": "C is correct."
      },
      {
        "date": "2023-01-08T00:04:00.000Z",
        "voteCount": 1,
        "content": "Option C is the correct answer. By changing /product-details to perform the requests in parallel, you can reduce the time it takes for the request to complete by making multiple requests at the same time rather than sequentially. This will allow you to retrieve the information for all four products more quickly. Option A is not a good solution because increasing the size of the instance class may not necessarily reduce the time it takes for the request to complete. Option B is not a good solution because changing the Persistent Disk type to SSD will not have any impact on the time it takes for the request to complete. Option D is not a good solution because storing the /sku-details information in a database and replacing the webservice call with a database query will not necessarily reduce the time it takes for the request to complete, and it will add unnecessary complexity to the application."
      },
      {
        "date": "2022-09-21T11:20:00.000Z",
        "voteCount": 1,
        "content": "C feels right"
      },
      {
        "date": "2022-08-19T23:49:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2022-08-16T21:04:00.000Z",
        "voteCount": 1,
        "content": "Vote C"
      },
      {
        "date": "2021-07-09T17:46:00.000Z",
        "voteCount": 1,
        "content": "App Engine is serverless so A and B are eliminated\n\nBetween C and D, I'd take C. \n\nhttps://cloud.google.com/appengine/docs/standard/java/datastore/queries:\n\"This solution is no longer recommended\""
      },
      {
        "date": "2021-07-17T20:40:00.000Z",
        "voteCount": 1,
        "content": "On 2nd thought, App Engine has instance classes:\n\nhttps://cloud.google.com/appengine/docs/standard\n\nSo I'll change my answer to A"
      },
      {
        "date": "2021-07-30T22:34:00.000Z",
        "voteCount": 3,
        "content": "Pardon my fickle-mindedness\n\nBack to C as options A &amp; B are wrong as they do not have a direct correlation with the issue and there is nothing to suggest they need to be increased."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 63,
    "url": "https://www.examtopics.com/discussions/google/view/36778-exam-professional-cloud-developer-topic-1-question-63/",
    "body": "Your company has a data warehouse that keeps your application information in BigQuery. The BigQuery data warehouse keeps 2 PBs of user data. Recently, your company expanded your user base to include EU users and needs to comply with these requirements:<br>\u2711 Your company must be able to delete all user account information upon user request.<br>\u2711 All EU user data must be stored in a single region specifically for EU users.<br>Which two actions should you take? (Choose two.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse BigQuery federated queries to query data from Cloud Storage.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a dataset in the EU region that will keep information about EU users only.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Cloud Storage bucket in the EU region to store information for EU users only.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRe-upload your data using to a Cloud Dataflow pipeline by filtering your user records out.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse DML statements in BigQuery to update/delete user records based on their requests.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "BE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BE",
        "count": 8,
        "isMostVoted": true
      },
      {
        "answer": "BD",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-11-11T08:29:00.000Z",
        "voteCount": 11,
        "content": "B and E is correct answer for me."
      },
      {
        "date": "2021-04-01T19:59:00.000Z",
        "voteCount": 2,
        "content": "Second point \"must be stored in a single region specifically for EU users \"  will achieve through option C only hence for me C and E is correct answer"
      },
      {
        "date": "2021-04-07T22:24:00.000Z",
        "voteCount": 3,
        "content": "Bigquery dataset can choose single region to store data, so B would be better"
      },
      {
        "date": "2021-07-09T17:59:00.000Z",
        "voteCount": 5,
        "content": "https://cloud.google.com/bigquery/docs/reference/standard-sql/data-manipulation-language\nThe link above supports E since \"delete all user account information upon user request\" as per qn\n\nhttps://cloud.google.com/architecture/bigquery-data-warehouse:\n\"A dataset is bound to a location. The dataset locations are as follows:\nMulti-regional: A large geographic area, such as the United States, that contains two or more geographic places.\"\n\nB is the other answer"
      },
      {
        "date": "2024-07-17T08:15:00.000Z",
        "voteCount": 1,
        "content": "The two actions you should take are:\n\nB. Create a dataset in the EU region that will keep information about EU users only. This directly addresses the requirement to store EU user data in a single EU region.\nE. Use DML statements in BigQuery to update/delete user records based on their requests. This allows you to efficiently delete user account information upon request, fulfilling the GDPR requirement."
      },
      {
        "date": "2024-07-17T08:16:00.000Z",
        "voteCount": 1,
        "content": "Here's why the other options are less suitable:\n\nA. Use BigQuery federated queries to query data from Cloud Storage: Federated queries are useful for accessing data in other sources, but they don't solve the problem of storing EU user data in a specific region or providing a mechanism for deleting user data.\nC. Create a Cloud Storage bucket in the EU region to store information for EU users only: While Cloud Storage can be used for data storage, it's not the primary data warehouse for your application. You'd need to manage data movement between BigQuery and Cloud Storage, which adds complexity.\nD. Re-upload your data using to a Cloud Dataflow pipeline by filtering your user records out: This is a time-consuming and potentially disruptive process. It's not the most efficient way to handle user data deletion requests."
      },
      {
        "date": "2024-03-07T19:38:00.000Z",
        "voteCount": 1,
        "content": "B. Creating a separate dataset in the EU region for EU users allows your company to ensure that all data for these users is stored in a specific geographic location, complying with regional data residency requirements. BigQuery allows you to select the region where your dataset resides, and having a dedicated dataset for EU users makes it easier to manage and enforce policies specific to EU data.\n\nE. Using Data Manipulation Language (DML) statements in BigQuery (such as DELETE and UPDATE) enables your company to comply with requests to delete or modify user account information. This capability is essential for adhering to regulations like the GDPR, which may require companies to delete users' personal data upon request."
      },
      {
        "date": "2023-09-19T21:51:00.000Z",
        "voteCount": 1,
        "content": "Best option is BE."
      },
      {
        "date": "2023-01-08T00:06:00.000Z",
        "voteCount": 1,
        "content": "B and E are the correct answers. To comply with the requirements, you should create a dataset in the EU region that will keep information about EU users only. This will allow you to store all EU user data in a single region specifically for EU users. Additionally, you should use DML statements in BigQuery to update or delete user records based on their requests. This will allow you to delete all user account information upon user request as required."
      },
      {
        "date": "2023-01-08T00:07:00.000Z",
        "voteCount": 1,
        "content": "A is not a good solution because using BigQuery federated queries to query data from Cloud Storage does not address either of the requirements. Federated queries allow you to query data that is stored outside of BigQuery, such as in Cloud Storage, but they do not help you store data in a specific region or delete data upon request"
      },
      {
        "date": "2023-01-08T00:07:00.000Z",
        "voteCount": 1,
        "content": "C is not a good solution because creating a Cloud Storage bucket in the EU region does not address either of the requirements. A Cloud Storage bucket is simply a storage location, and it does not allow you to store data in a specific region or delete data upon request."
      },
      {
        "date": "2023-01-08T00:06:00.000Z",
        "voteCount": 1,
        "content": "D is not a good solution because re-uploading your data using a Cloud Dataflow pipeline is unnecessarily complex and does not address either of the requirements. Filtering user records out during the re-upload process does not allow you to store data in a specific region or delete data upon request."
      },
      {
        "date": "2022-08-19T23:50:00.000Z",
        "voteCount": 2,
        "content": "BE are correct sorry"
      },
      {
        "date": "2022-08-19T23:50:00.000Z",
        "voteCount": 1,
        "content": "BD are correct"
      },
      {
        "date": "2022-07-25T07:25:00.000Z",
        "voteCount": 1,
        "content": "Cloud Storage is out of scope !!"
      },
      {
        "date": "2022-04-27T17:55:00.000Z",
        "voteCount": 1,
        "content": "Vote BE"
      },
      {
        "date": "2022-01-08T05:19:00.000Z",
        "voteCount": 1,
        "content": "B and E are the correct answer"
      },
      {
        "date": "2021-11-30T01:29:00.000Z",
        "voteCount": 2,
        "content": "B &amp; E. Data is already stored in BigQuery, I do not see any reason to have anything to do with Cloud Storage. Also, BigQuery allows DML to do updates and deletes. So I would choose B &amp; E"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 64,
    "url": "https://www.examtopics.com/discussions/google/view/36779-exam-professional-cloud-developer-topic-1-question-64/",
    "body": "Your App Engine standard configuration is as follows:<br>service: production<br>instance_class: B1<br>You want to limit the application to 5 instances.<br>Which code snippet should you include in your configuration?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tmanual_scaling: instances: 5 min_pending_latency: 30ms",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tmanual_scaling: max_instances: 5 idle_timeout: 10m",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tbasic_scaling: instances: 5 min_pending_latency: 30ms",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tbasic_scaling: max_instances: 5 idle_timeout: 10m\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 10,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-07-17T08:19:00.000Z",
        "voteCount": 2,
        "content": "The correct answer is B. manual_scaling: max_instances: 5 idle_timeout: 10m .\n\nHere's why:\n\nmanual_scaling : You're using App Engine Standard, which requires you to explicitly define the scaling method. manual_scaling is the correct choice for limiting the number of instances.\nmax_instances: 5 : This setting directly limits the maximum number of instances to 5, fulfilling your requirement.\nidle_timeout: 10m : This setting defines how long an idle instance will remain active before being shut down. It's optional but helps manage costs by automatically shutting down unused instances."
      },
      {
        "date": "2024-07-17T08:19:00.000Z",
        "voteCount": 1,
        "content": "Let's break down why the other options are incorrect:\n\nA. manual_scaling: instances: 5 min_pending_latency: 30ms : While instances: 5 sets the number of instances, min_pending_latency is used for automatic scaling, which is not applicable in this case.\nC. basic_scaling: instances: 5 min_pending_latency: 30ms : basic_scaling is for automatic scaling based on request rate. You want manual scaling, so this is incorrect.\nD. basic_scaling: max_instances: 5 idle_timeout: 10m : Similar to option C, basic_scaling is not the correct scaling method for this scenario.\nIn summary: Option B provides the correct configuration for limiting your App Engine Standard application to 5 instances using manual scaling."
      },
      {
        "date": "2024-03-07T19:44:00.000Z",
        "voteCount": 1,
        "content": "Basic Scaling with max_instances (Option D): Basic scaling is suitable for applications that do not need to keep instances running all the time but may require instances to handle requests and then shut down when idle. The max_instances parameter sets the maximum number of instances, and idle_timeout specifies the amount of time that an instance can stay idle before it is shut down."
      },
      {
        "date": "2023-09-19T21:52:00.000Z",
        "voteCount": 1,
        "content": "D is correct."
      },
      {
        "date": "2023-08-25T01:38:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is D, only configuration permitted and lawful according to documentation:\nhttps://cloud.google.com/appengine/docs/standard/reference/app-yaml?tab=python#manual_scaling"
      },
      {
        "date": "2023-01-08T00:14:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is D, which specifies the max_instances parameter of the basic_scaling configuration to limit the application to a maximum of 5 instances. The basic_scaling configuration is used for applications that are driven by user activity, and it allows you to specify the maximum number of instances that you want to run using the max_instances parameter."
      },
      {
        "date": "2023-01-08T00:14:00.000Z",
        "voteCount": 1,
        "content": "B is not a good solution because the manual_scaling configuration is not being used, and the idle_timeout parameter has no effect on the maximum number of instances."
      },
      {
        "date": "2023-01-08T00:14:00.000Z",
        "voteCount": 1,
        "content": "A is not a good solution because the manual_scaling configuration is not being used, and the min_pending_latency parameter has no effect on the maximum number of instances."
      },
      {
        "date": "2023-01-08T00:14:00.000Z",
        "voteCount": 1,
        "content": "C is not a good solution because the min_pending_latency parameter is used to specify a minimum amount of time that a request should take before an instance is started, but it has no effect on the maximum number of instances."
      },
      {
        "date": "2022-08-19T23:51:00.000Z",
        "voteCount": 4,
        "content": "D is correct"
      },
      {
        "date": "2022-07-29T00:19:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/appengine/docs/legacy/standard/python/how-instances-are-managed#scaling_types"
      },
      {
        "date": "2022-07-25T07:31:00.000Z",
        "voteCount": 1,
        "content": "C because the question says limit to 5 instances, not at max 5 instance"
      },
      {
        "date": "2023-11-15T09:26:00.000Z",
        "voteCount": 1,
        "content": "but seems no min_latency_instance for basic scaling\n\nhttps://cloud.google.com/appengine/docs/standard/reference/app-yaml?tab=python#basic_scaling"
      },
      {
        "date": "2023-11-15T09:27:00.000Z",
        "voteCount": 1,
        "content": "sorry, no option min_pending_latency for basic scaling i mean"
      },
      {
        "date": "2022-05-25T11:33:00.000Z",
        "voteCount": 2,
        "content": "Option D - Max of 5"
      },
      {
        "date": "2022-01-08T05:21:00.000Z",
        "voteCount": 1,
        "content": "D is the correct answer"
      },
      {
        "date": "2021-07-09T18:03:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/appengine/docs/standard/python/how-instances-are-managed:\n\"A service with basic scaling is configured by setting the maximum number of instances in the max_instances parameter of the basic_scaling setting. The number of live instances scales with the processing volume.\"\n\nAnswer is D"
      },
      {
        "date": "2021-02-19T20:19:00.000Z",
        "voteCount": 2,
        "content": "Option: D is correct. Link: https://cloud.google.com/appengine/docs/standard/python/how-instances-are-managed"
      },
      {
        "date": "2021-02-19T01:45:00.000Z",
        "voteCount": 1,
        "content": "Ya Answer is D\nhttps://cloud.google.com/appengine/docs/standard/python/config/appref#scaling_elements"
      },
      {
        "date": "2020-11-11T08:36:00.000Z",
        "voteCount": 4,
        "content": "D is correct answer here."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 65,
    "url": "https://www.examtopics.com/discussions/google/view/40500-exam-professional-cloud-developer-topic-1-question-65/",
    "body": "Your analytics system executes queries against a BigQuery dataset. The SQL query is executed in batch and passes the contents of a SQL file to the BigQuery<br>CLI. Then it redirects the BigQuery CLI output to another process. However, you are getting a permission error from the BigQuery CLI when the queries are executed.<br>You want to resolve the issue. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGrant the service account BigQuery Data Viewer and BigQuery Job User roles.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGrant the service account BigQuery Data Editor and BigQuery Data Viewer roles.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a view in BigQuery from the SQL query and SELECT* from the view in the CLI.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new dataset in BigQuery, and copy the source table to the new dataset Query the new dataset and table from the CLI."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-12-21T13:39:00.000Z",
        "voteCount": 14,
        "content": "I think A is the correct one."
      },
      {
        "date": "2024-07-17T08:26:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is A. Grant the service account BigQuery Data Viewer and BigQuery Job User roles.\n\nHere's why:\n\nBigQuery Data Viewer: This role allows the service account to read data from BigQuery tables. This is necessary for the BigQuery CLI to execute the SQL queries.\nBigQuery Job User: This role allows the service account to run queries and jobs in BigQuery. This is essential for the BigQuery CLI to execute the SQL queries and return results."
      },
      {
        "date": "2024-07-17T08:26:00.000Z",
        "voteCount": 1,
        "content": "Let's break down why the other options are less suitable:\n\nB. Grant the service account BigQuery Data Editor and BigQuery Data Viewer roles: While BigQuery Data Editor allows for data modification, it's not strictly necessary for executing queries. Granting BigQuery Data Viewer and BigQuery Job User roles is sufficient.\nC. Create a view in BigQuery from the SQL query and SELECT from the view in the CLI: * Creating a view can simplify queries, but it doesn't address the underlying permission issue. The service account still needs the necessary permissions to access the view and execute queries.\nD. Create a new dataset in BigQuery and copy the source table to the new dataset Query the new dataset and table from the CLI: This approach adds unnecessary complexity and data movement. It doesn't solve the permission issue."
      },
      {
        "date": "2024-03-07T19:48:00.000Z",
        "voteCount": 1,
        "content": "A. Grant the service account BigQuery Data Viewer and BigQuery Job User roles.\n\nThe permission error from the BigQuery CLI suggests that the service account used to execute the queries does not have the necessary permissions. To resolve this, you need to ensure that the service account has the appropriate roles:\n\nBigQuery Data Viewer role: This role allows the service account to read data from BigQuery tables and views. It's necessary for the service account to access and read the dataset against which the queries are being executed.\n\nBigQuery Job User role: This role allows the service account to create and run jobs in BigQuery, including query jobs, which is necessary for executing SQL queries."
      },
      {
        "date": "2024-02-13T01:48:00.000Z",
        "voteCount": 1,
        "content": "A is the one. No need to edit data is specified."
      },
      {
        "date": "2023-09-19T21:57:00.000Z",
        "voteCount": 1,
        "content": "A is correct."
      },
      {
        "date": "2023-01-08T01:12:00.000Z",
        "voteCount": 3,
        "content": "The correct answer is Option A. In order to allow the analytics system to execute queries against the BigQuery dataset, the service account must be granted the BigQuery Data Viewer and BigQuery Job User roles. The BigQuery Data Viewer role allows the service account to read data from tables, and the BigQuery Job User role allows the service account to run jobs, which includes executing queries. Option B is not a good solution because the BigQuery Data Editor role allows the service account to modify data in tables, which is not necessary to execute queries. Option C is not a good solution because creating a view in BigQuery and selecting from the view in the CLI will not resolve the permission issue. Option D is not a good solution because creating a new dataset and copying the source table to the new dataset will not resolve the permission issue."
      },
      {
        "date": "2022-08-19T23:51:00.000Z",
        "voteCount": 3,
        "content": "A is correct"
      },
      {
        "date": "2022-07-25T23:12:00.000Z",
        "voteCount": 1,
        "content": "A it's correct for the principle of least privilege"
      },
      {
        "date": "2022-01-08T09:30:00.000Z",
        "voteCount": 1,
        "content": "According to the best practice - \"User should have least privilege i.e. only those permissions which are required to perform an operation\" - Option A is Correct."
      },
      {
        "date": "2021-07-09T18:13:00.000Z",
        "voteCount": 3,
        "content": "Principle of least privilege so A is the answer"
      },
      {
        "date": "2021-04-01T21:45:00.000Z",
        "voteCount": 4,
        "content": "Ans A only \nhttps://cloud.google.com/bigquery/docs/access-control#bigquery\nIn Ans B Bigquery Data Editor is not required."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 66,
    "url": "https://www.examtopics.com/discussions/google/view/40501-exam-professional-cloud-developer-topic-1-question-66/",
    "body": "Your application is running on Compute Engine and is showing sustained failures for a small number of requests. You have narrowed the cause down to a single<br>Compute Engine instance, but the instance is unresponsive to SSH.<br>What should you do next?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReboot the machine.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable and check the serial port output.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDelete the machine and create a new one.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTake a snapshot of the disk and attach it to a new machine."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-12-21T13:44:00.000Z",
        "voteCount": 5,
        "content": "Difficult to choose because either B(https://cloud.google.com/compute/docs/troubleshooting/troubleshooting-ssh#debug_with_serial_console) or D(https://cloud.google.com/compute/docs/troubleshooting/troubleshooting-ssh#inspect_vm) is recommended by google. I'd stay with B."
      },
      {
        "date": "2021-05-20T05:59:00.000Z",
        "voteCount": 4,
        "content": "Using the first link you provided: https://cloud.google.com/compute/docs/troubleshooting/troubleshooting-ssh#debug_with_serial_console\n\nWe recommend that you review the logs from the serial console for connection errors. You can access the serial console from your local workstation by using a browser.\n\nEnable read/write access to an instance's serial console, so you can log into the console and troubleshoot problems with the instance. This approach is useful when you cannot log in with SSH, or if the instance has no connection to the network. The serial console remains accessible in both of these situations"
      },
      {
        "date": "2024-07-17T08:32:00.000Z",
        "voteCount": 1,
        "content": "The best course of action is B. Enable and check the serial port output.\n\nHere's why:\n\nSerial Port Output: The serial port output provides a log of the instance's boot process and any error messages that might occur during startup. This is a valuable resource for diagnosing issues when an instance is unresponsive to SSH.\nTroubleshooting: By examining the serial port output, you can often identify the root cause of the unresponsiveness, such as a failed boot process, disk errors, or configuration problems."
      },
      {
        "date": "2024-07-17T08:32:00.000Z",
        "voteCount": 1,
        "content": "Let's look at why the other options are less ideal:\n\nA. Reboot the machine: While rebooting might resolve some temporary issues, it doesn't provide any insight into the cause of the problem. If the issue is persistent, rebooting won't solve it.\nC. Delete the machine and create a new one: This is a drastic measure that should be avoided unless absolutely necessary. It involves data loss and reconfiguration, which can be time-consuming and disruptive.\nD. Take a snapshot of the disk and attach it to a new machine: This approach can be useful for recovering data, but it doesn't address the underlying issue causing the instance to be unresponsive."
      },
      {
        "date": "2024-03-07T19:53:00.000Z",
        "voteCount": 1,
        "content": "B. Enable and check the serial port output.\n\nWhen a Compute Engine instance becomes unresponsive to SSH, one of the best ways to diagnose the issue is to check the serial port output. The serial console can provide valuable information about the state of the instance and any errors that may be occurring at the system level. This is particularly useful when you can't connect via SSH. Google Cloud allows you to enable interactive access to the serial console, which can be a crucial tool for troubleshooting."
      },
      {
        "date": "2023-09-20T07:53:00.000Z",
        "voteCount": 1,
        "content": "B is correct."
      },
      {
        "date": "2023-01-08T01:17:00.000Z",
        "voteCount": 4,
        "content": "Option B is correct. According to Google Cloud documentation, if a Compute Engine instance is unresponsive to SSH and you have narrowed the cause down to a single instance, you should enable and check the serial port output. The serial port output is a log of system messages and can help you diagnose the issue causing the instance to become unresponsive. To enable and check the serial port output, you can access the serial console as the root user from your local workstation using a browser. This will allow you to review the logs and potentially identify the cause of the problem."
      },
      {
        "date": "2023-01-08T01:17:00.000Z",
        "voteCount": 1,
        "content": "Option A is not a good solution because rebooting the machine may not resolve the issue that is causing the instance to become unresponsive."
      },
      {
        "date": "2023-01-08T01:17:00.000Z",
        "voteCount": 1,
        "content": "Option C is not a good solution because deleting the machine and creating a new one will not resolve the issue that caused the original instance to become unresponsive."
      },
      {
        "date": "2023-01-08T01:17:00.000Z",
        "voteCount": 2,
        "content": "Option D is not a good solution because taking a snapshot of the disk and attaching it to a new machine will not resolve the issue that caused the original instance to become unresponsive."
      },
      {
        "date": "2022-08-19T23:51:00.000Z",
        "voteCount": 2,
        "content": "B is correct"
      },
      {
        "date": "2022-01-08T09:32:00.000Z",
        "voteCount": 2,
        "content": "B is correct, because it will be the first step to check the serial port 22 is responsive or not."
      },
      {
        "date": "2021-07-09T18:23:00.000Z",
        "voteCount": 3,
        "content": "I would take B as the answer\n\n\"What should you do next?\"\n\nSSH is port 22 so it makes sense to check for the port"
      },
      {
        "date": "2021-02-19T20:23:00.000Z",
        "voteCount": 4,
        "content": "Option-B is most suitable: In a real world I will try this and even if doesn't work then I will try Option-D which will help me to troubleshoot in details."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 67,
    "url": "https://www.examtopics.com/discussions/google/view/36918-exam-professional-cloud-developer-topic-1-question-67/",
    "body": "You configured your Compute Engine instance group to scale automatically according to overall CPU usage. However, your application's response latency increases sharply before the cluster has finished adding up instances. You want to provide a more consistent latency experience for your end users by changing the configuration of the instance group autoscaler.<br>Which two configuration changes should you make? (Choose two.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd the label \u05d2\u20acAUTOSCALE\u05d2\u20ac to the instance group template.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDecrease the cool-down period for instances added to the group.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the target CPU usage for the instance group autoscaler.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDecrease the target CPU usage for the instance group autoscaler.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRemove the health-check for individual VMs in the instance group."
    ],
    "answer": "BD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BD",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-12-21T13:49:00.000Z",
        "voteCount": 16,
        "content": "I'd choose B and D."
      },
      {
        "date": "2020-12-27T03:43:00.000Z",
        "voteCount": 5,
        "content": "For me the answer is B and D.\n\"A cool down period value that is significantly longer causing a delay in scaling out\".\nhttps://cloud.google.com/compute/docs/autoscaler#cool_down_period\nhttps://cloud.google.com/compute/docs/autoscaler/scaling-cpu#scaling_based_on_cpu_utilization"
      },
      {
        "date": "2024-03-07T20:02:00.000Z",
        "voteCount": 1,
        "content": "B. Decrease the cool-down period for instances added to the group: The cool-down period is the time the autoscaler waits after a new instance is healthy before it collects usage metrics from the instance. A shorter cool-down period allows the autoscaler to react more quickly to changes in load, potentially starting to scale up sooner when there is a sudden increase in traffic.\n\nD. Decrease the target CPU usage for the instance group autoscaler: Lowering the target CPU utilization means that the autoscaler will start adding instances sooner as the CPU usage approaches the lower target. This can help to alleviate the issue where response latency increases sharply because new instances are added before the CPU usage hits a higher threshold."
      },
      {
        "date": "2023-09-19T22:01:00.000Z",
        "voteCount": 1,
        "content": "B and D are the best option here."
      },
      {
        "date": "2023-01-08T01:18:00.000Z",
        "voteCount": 4,
        "content": "Options B and D are correct. To provide a more consistent latency experience for your end users, you should make the following configuration changes:\n\nOption B: Decrease the cool-down period for instances added to the group. The cool-down period is the time that must pass before the instance group autoscaler can add more instances after it has already added instances to the group. Decreasing the cool-down period can allow the instance group to scale more quickly in response to changes in CPU usage, which may help to reduce latency.\n\nOption D: Decrease the target CPU usage for the instance group autoscaler. The target CPU usage is the average CPU usage that the instance group autoscaler aims to maintain for the group. Decreasing the target CPU usage may allow the instance group to scale down more quickly in response to changes in CPU usage, which may also help to reduce latency."
      },
      {
        "date": "2023-01-08T01:19:00.000Z",
        "voteCount": 1,
        "content": "Option E is not a correct solution because removing the health-check for individual VMs in the instance group may not improve latency and could potentially cause other issues with the instance group."
      },
      {
        "date": "2023-01-08T01:19:00.000Z",
        "voteCount": 1,
        "content": "Option C is not a correct solution because increasing the target CPU usage for the instance group autoscaler will not help to reduce latency."
      },
      {
        "date": "2023-01-08T01:19:00.000Z",
        "voteCount": 1,
        "content": "Option A is not a correct solution because adding the label \"AUTOSCALE\" to the instance group template will not affect the configuration of the instance group autoscaler"
      },
      {
        "date": "2022-08-19T23:52:00.000Z",
        "voteCount": 3,
        "content": "BD are correct"
      },
      {
        "date": "2022-02-24T19:36:00.000Z",
        "voteCount": 3,
        "content": "I would choose these two"
      },
      {
        "date": "2021-07-09T18:28:00.000Z",
        "voteCount": 4,
        "content": "Adding label won't solve the issue so A is wrong for sure\nRemoving health check is not recommended so E is wrong as well\nIncrease CPU target is wrong since scaling will take place at a higher usage which is not what we want\n\nB and D are the correct options"
      },
      {
        "date": "2020-11-13T04:04:00.000Z",
        "voteCount": 2,
        "content": "D is more correct than C. If C, the auto-scale up will be further delayed"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 68,
    "url": "https://www.examtopics.com/discussions/google/view/40503-exam-professional-cloud-developer-topic-1-question-68/",
    "body": "You have an application controlled by a managed instance group. When you deploy a new version of the application, costs should be minimized and the number of instances should not increase. You want to ensure that, when each new instance is created, the deployment only continues if the new instance is healthy.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPerform a rolling-action with maxSurge set to 1, maxUnavailable set to 0.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPerform a rolling-action with maxSurge set to 0, maxUnavailable set to 1\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPerform a rolling-action with maxHealthy set to 1, maxUnhealthy set to 0.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPerform a rolling-action with maxHealthy set to 0, maxUnhealthy set to 1."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 11,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 7,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-12-21T13:57:00.000Z",
        "voteCount": 22,
        "content": "B(maxSurge = 0, maxUnavailable = 1)"
      },
      {
        "date": "2021-04-07T22:33:00.000Z",
        "voteCount": 4,
        "content": "\"costs should be minimized and the number of instances should not increase\", maxSurge = 1 will increase the number of instances, so B would be the correct answer"
      },
      {
        "date": "2021-07-09T18:38:00.000Z",
        "voteCount": 5,
        "content": "\"number of instances should not increase\"\n\nB would be correct"
      },
      {
        "date": "2024-10-05T04:08:00.000Z",
        "voteCount": 1,
        "content": "maxSurge = 0, maxUnavailable = 1)"
      },
      {
        "date": "2024-07-17T08:44:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is A. Perform a rolling-action with maxSurge set to 1, maxUnavailable set to 0.\n\nHere's why:\n\nRolling Update: A rolling update is the best approach for deploying new versions of your application to a managed instance group while minimizing downtime and ensuring a smooth transition.\nmaxSurge : This parameter controls the maximum number of instances that can be added to the group during the update. Setting maxSurge to 1 means that only one additional instance will be created beyond the target size of the group. This helps to minimize costs by avoiding unnecessary instance creation.\nmaxUnavailable : This parameter controls the maximum number of instances that can be taken offline during the update. Setting maxUnavailable to 0 ensures that no existing instances are taken offline until the newly created instance is healthy. This guarantees that the deployment only continues if the new instance is healthy."
      },
      {
        "date": "2024-07-17T08:44:00.000Z",
        "voteCount": 1,
        "content": "Let's break down why the other options are incorrect:\n\nB. Perform a rolling-action with maxSurge set to 0, maxUnavailable set to 1: This would allow one instance to be taken offline before the new instance is healthy, potentially causing downtime.\nC. Perform a rolling-action with maxHealthy set to 1, maxUnhealthy set to 0: maxHealthy and maxUnhealthy are not valid parameters for rolling updates. They are used for regional instance group managers, not managed instance groups.\nD. Perform a rolling-action with maxHealthy set to 0, maxUnhealthy set to 1: Similar to option C, these parameters are not applicable to managed instance groups."
      },
      {
        "date": "2024-03-07T20:07:00.000Z",
        "voteCount": 2,
        "content": "Here's the reasoning:\n\nmaxSurge: This parameter determines the number of additional instances that can be created above the target size of the instance group during the update. Setting maxSurge to 1 allows the instance group to create one extra instance beyond its target size. This extra instance is used to start a new version of your application, ensuring that there's always a running instance during the update process.\n\nmaxUnavailable: This parameter specifies the number of instances that can be unavailable at any time during the update. Setting maxUnavailable to 0 ensures that there is no reduction in the number of available instances below the target size during the update process."
      },
      {
        "date": "2023-11-24T07:39:00.000Z",
        "voteCount": 2,
        "content": "maxSurge controls in a rolling update how many resources can be added above threshold of the MIG (managed instance group),\n  while maxUnavailable controls the max number of instances that can be taken offline during update at the same time"
      },
      {
        "date": "2023-11-05T14:15:00.000Z",
        "voteCount": 1,
        "content": "the correct answer is A i think\nIf you do not want any unavailable machines during an update, set the maxUnavailable value to 0 and the maxSurge value to greater than 0. With these settings, Compute Engine removes each old machine only after its replacement new machine is created and running.\n\nhttps://cloud.google.com/compute/docs/instance-groups/rolling-out-updates-to-managed-instance-groups#max_surge"
      },
      {
        "date": "2023-09-19T22:06:00.000Z",
        "voteCount": 1,
        "content": "Option A is best suited here as if we go with option B it does not ensures that deployment only continue if the new instance is healthy."
      },
      {
        "date": "2023-03-28T06:51:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is A.\n\nPerforming a rolling update with maxSurge set to 1 and maxUnavailable set to 0 ensures that the deployment only continues if the new instance is healthy. The maxSurge parameter ensures that only one new instance is created at a time, while the maxUnavailable parameter ensures that the number of healthy instances does not decrease during the deployment. This will minimize costs by not creating unnecessary instances and will also ensure that the deployment is safe and does not impact the application's availability.\n\nOption B is incorrect because setting maxUnavailable to 1 would mean that one instance will be taken offline at a time, which could impact the application's availability during the deployment.\n\nOptions C and D are incorrect because maxHealthy and maxUnhealthy are not valid parameters for a rolling update."
      },
      {
        "date": "2023-05-07T12:42:00.000Z",
        "voteCount": 1,
        "content": "\"costs should be minimized and the number of instances should not increase\"\nits B\nwith maxUnavailable=1, maxSurge=0 - you instance group will be decreased by 1 which is not prohibited"
      },
      {
        "date": "2023-02-02T06:29:00.000Z",
        "voteCount": 1,
        "content": "the correct answers is A"
      },
      {
        "date": "2023-01-08T01:26:00.000Z",
        "voteCount": 2,
        "content": "the correct answer would be A. Perform a rolling-action with maxSurge set to 1, maxUnavailable set to 0. This will minimize costs by only creating one new instance at a time, and will only continue the deployment if the new instance is healthy, ensuring a consistent latency experience for end users.\n\nhttps://cloud.google.com/compute/docs/instance-groups/rolling-out-updates-to-managed-instance-groups#max_surge"
      },
      {
        "date": "2023-01-08T01:27:00.000Z",
        "voteCount": 1,
        "content": "Option B is not a correct solution because setting maxSurge to 0 and maxUnavailable to 1 will not allow the deployment to continue if the new instance is not healthy. Option C is not a correct solution because maxHealthy and maxUnhealthy are not valid options for rolling-actions. Option D is not a correct solution because setting maxHealthy to 0 and maxUnhealthy to 1 will not allow the deployment to continue if the new instance is not healthy."
      },
      {
        "date": "2022-10-11T06:37:00.000Z",
        "voteCount": 2,
        "content": "This is the best site here.\nhttps://tech-lab.sios.jp/archives/18553\nThe site is in Japanese, so please translate and read it."
      },
      {
        "date": "2022-08-19T23:52:00.000Z",
        "voteCount": 2,
        "content": "B is correct"
      },
      {
        "date": "2022-05-31T12:49:00.000Z",
        "voteCount": 1,
        "content": "maxSurge specifies the maximum number (or percentage) of pods above the specified number of replicas (is the maximum number of new pods that will be created at a time) and maxUnavailable is the maximum number of old pods that will be deleted at a time.\n\nmaxSurge = 0 would mean no extra pods with be created."
      },
      {
        "date": "2022-04-21T02:55:00.000Z",
        "voteCount": 2,
        "content": "i'll go with A(  number of instances should not increase--- for this number shouldn't increase than target size i think, it doesn't mean no new instances should be created,\nthey are asking in question, like when new instance is being created, so surge &gt;0,)\nhttps://cloud.google.com/compute/docs/instance-groups/rolling-out-updates-to-managed-instance-groups#max_unavailable\n\nIf you do not want any unavailable machines during an update, set the maxUnavailable value to 0 and the maxSurge value to greater than 0. With these settings, Compute Engine removes each old machine only after its replacement new machine is created and running."
      },
      {
        "date": "2022-03-30T01:26:00.000Z",
        "voteCount": 2,
        "content": "As others suggested, B is the correct option.\n\nI am adding this to highlight the community choice."
      },
      {
        "date": "2022-02-27T14:36:00.000Z",
        "voteCount": 1,
        "content": "Yes it should be B as question states deployment should stop if it is unhealthy\u2026 the only we can happen is to make it to 0 for maxSurge =1"
      },
      {
        "date": "2022-02-27T14:39:00.000Z",
        "voteCount": 1,
        "content": "I am confused here \u2026 it\u2019s either A or B"
      },
      {
        "date": "2022-02-27T14:44:00.000Z",
        "voteCount": 1,
        "content": "Ok thanks for link reference \nExcerpt"
      },
      {
        "date": "2022-02-27T14:44:00.000Z",
        "voteCount": 1,
        "content": "Note: If you set both maxSurge and maxUnavailable properties and both properties resolve to 0, the Updater automatically sets maxUnavailable=1, to ensure that the automated update can always proceed."
      },
      {
        "date": "2022-02-27T14:45:00.000Z",
        "voteCount": 1,
        "content": "That will be B"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 69,
    "url": "https://www.examtopics.com/discussions/google/view/55845-exam-professional-cloud-developer-topic-1-question-69/",
    "body": "Your application requires service accounts to be authenticated to GCP products via credentials stored on its host Compute Engine virtual machine instances. You want to distribute these credentials to the host instances as securely as possible.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse HTTP signed URLs to securely provide access to the required resources.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the instance's service account Application Default Credentials to authenticate to the required resources.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGenerate a P12 file from the GCP Console after the instance is deployed, and copy the credentials to the host instance before starting the application.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCommit the credential JSON file into your application's source repository, and have your CI/CD process package it with the software that is deployed to the instance."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 6,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-07-17T08:49:00.000Z",
        "voteCount": 1,
        "content": "The most secure approach is B. Use the instance's service account Application Default Credentials to authenticate to the required resources.\n\nHere's why:\n\nApplication Default Credentials (ADC): ADC is a Google Cloud feature that allows applications running on Google Cloud to automatically authenticate using the service account associated with the instance. This eliminates the need to store credentials directly on the instance, reducing security risks.\nNo Manual Credential Management: ADC handles authentication automatically, eliminating the need to manually manage credentials, which can be error-prone and introduce security vulnerabilities.\nKey Rotation: Google Cloud automatically rotates service account keys, further enhancing security."
      },
      {
        "date": "2024-07-17T08:49:00.000Z",
        "voteCount": 1,
        "content": "Let's look at why the other options are less secure:\n\nA. Use HTTP signed URLs to securely provide access to the required resources: HTTP signed URLs are useful for providing temporary access to specific resources, but they don't address the core issue of securely storing and managing service account credentials.\nC. Generate a P12 file from the GCP Console after the instance is deployed and copy the credentials to the host instance before starting the application: This approach requires storing credentials on the instance, which is a security risk. It also introduces manual steps that can be error-prone.\nD. Commit the credential JSON file into your application's source repository and have your CI/CD process package it with the software that is deployed to the instance: This is a major security risk. Storing credentials in source code exposes them to anyone with access to the repository."
      },
      {
        "date": "2023-09-19T22:10:00.000Z",
        "voteCount": 1,
        "content": "Option B is Correct: This approach ensures that the credentials are securely managed and automatically provided to the instances when needed."
      },
      {
        "date": "2023-09-19T22:10:00.000Z",
        "voteCount": 1,
        "content": "This approach ensures that the credentials are securely managed and automatically provided to the instances when needed."
      },
      {
        "date": "2023-01-11T23:53:00.000Z",
        "voteCount": 1,
        "content": "Answer B because best practice is to not store file with account service information when possible. With compute engine, the account service of the vm can be used to call google api if the roles are added to this account service."
      },
      {
        "date": "2023-01-08T01:28:00.000Z",
        "voteCount": 3,
        "content": "B. Use the instance's service account Application Default Credentials to authenticate to the required resources.\n\nUsing the instance's service account Application Default Credentials is the most secure method for distributing credentials to the host instances. This method allows the instance to automatically authenticate with the required resources using the instance's built-in service account, without requiring the credentials to be stored on the instance or transmitted over the network. This eliminates the risk of the credentials being compromised or exposed. Additionally, this method is the most convenient, as it requires no manual steps to set up the credentials on the instance."
      },
      {
        "date": "2022-08-19T23:52:00.000Z",
        "voteCount": 2,
        "content": "I think B is correct"
      },
      {
        "date": "2022-05-31T12:55:00.000Z",
        "voteCount": 1,
        "content": "I'm also considering this part -- \"distribute these credentials to the host instances as securely as possible\"\n\nThis falls under B."
      },
      {
        "date": "2022-05-28T08:07:00.000Z",
        "voteCount": 1,
        "content": "Your application requires service accounts to be authenticated to GCP products via credentials stored on its host Compute Engine virtual machine instances. \nThe application requires the credentials to be stored on the VM instance, so I think the application code points to a file stored in the Instance."
      },
      {
        "date": "2021-09-04T21:59:00.000Z",
        "voteCount": 4,
        "content": "Answer is B\n\nhttps://cloud.google.com/docs/authentication/production#automatically\n\nIf the environment variable GOOGLE_APPLICATION_CREDENTIALS isn't set, ADC uses the service account that is attached to the resource that is running your code."
      },
      {
        "date": "2021-07-09T18:51:00.000Z",
        "voteCount": 2,
        "content": "\"authenticated to GCP\" is the key part of the qn\n\nhttps://cloud.google.com/iam/docs/creating-managing-service-account-keys:\n\"To use a service account from outside of Google Cloud, such as on other platforms or on-premises, you must first establish the identity of the service account\"\n\"You can create service account keys in JSON or PKCS#12 (P12) format. \"\n\nC is the answer"
      },
      {
        "date": "2021-07-30T22:17:00.000Z",
        "voteCount": 4,
        "content": "https://cloud.google.com/compute/docs/api/how-tos/authorization:\n\"If you run applications on your Compute Engine instances, application default credentials can get credentials through built-in service accounts\"\n\nAnswer is B not C"
      },
      {
        "date": "2021-06-22T07:50:00.000Z",
        "voteCount": 1,
        "content": "Only C sounds right"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 70,
    "url": "https://www.examtopics.com/discussions/google/view/40504-exam-professional-cloud-developer-topic-1-question-70/",
    "body": "Your application is deployed in a Google Kubernetes Engine (GKE) cluster. You want to expose this application publicly behind a Cloud Load Balancing HTTP(S) load balancer.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a GKE Ingress resource.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a GKE Service resource.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a GKE Ingress resource with type: LoadBalancer.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a GKE Service resource with type: LoadBalancer."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 8,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 5,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-12-21T14:05:00.000Z",
        "voteCount": 7,
        "content": "A(https://cloud.google.com/kubernetes-engine/docs/tutorials/http-balancer)"
      },
      {
        "date": "2021-05-21T00:21:00.000Z",
        "voteCount": 1,
        "content": "an ingress works if you already have a service, like an https load balancer or a NodePort."
      },
      {
        "date": "2022-05-30T12:38:00.000Z",
        "voteCount": 2,
        "content": "For me the right answer is D.\nThe Ingress Object create a global http(s) L.B. The advantage of having a layer 7 load balancer is that we can configure advanced things at network layer 7. For example to route traffic based on the path of the request and so on. In the question it seems that is not needed, so for me a Load Balancer Service is ok since it creates a network load balancer (TCP/UDP) that is ok for HTTP or HTTPS exposing (since we can configure any TCP/UDP port to be exposed to The Internet)"
      },
      {
        "date": "2021-11-01T13:43:00.000Z",
        "voteCount": 5,
        "content": "A) \nThe important part of the question is this \"...expose this application publicly behind a Cloud Load Balancing HTTP(S) load balancer.\" This means it is an L7 exposure using HTTPS (a Service of type \"LoadBalancer\" would only create an L4 exposure - IP only... No HTTPS).\n\nSo Ingress is the choice you should make. And in GKE, luckily this is one thing - create an ingress and the LB will be attached automagically ;D"
      },
      {
        "date": "2024-07-17T08:52:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is A. Configure a GKE Ingress resource.\n\nHere's why:\n\nIngress for External Load Balancing: Ingress is the Kubernetes resource specifically designed for managing external HTTP(S) traffic to services within your cluster. When you create an Ingress resource, GKE automatically provisions and configures a Cloud Load Balancing HTTP(S) load balancer to route traffic to your application."
      },
      {
        "date": "2024-07-17T08:52:00.000Z",
        "voteCount": 1,
        "content": "Let's break down why the other options are incorrect:\n\nB. Configure a GKE Service resource: Services in Kubernetes are used to expose applications within the cluster. While you can create a Service of type LoadBalancer , this will create an internal load balancer within the cluster, not an external one.\nC. Configure a GKE Ingress resource with type: LoadBalancer: Ingress resources don't have a type field. The LoadBalancer type is used for Services, not Ingress.\nD. Configure a GKE Service resource with type: LoadBalancer: This would create an internal load balancer within the cluster, not an external one."
      },
      {
        "date": "2024-07-11T06:42:00.000Z",
        "voteCount": 1,
        "content": "A. Configure a GKE Ingress resource. This option correctly configures the Ingress resource which is designed to expose HTTP and HTTPS routes from outside the cluster to services within the cluster.\nB. Configure a GKE Service resource.\nThis option only exposes the application internally within the cluster unless the service type is specified as LoadBalancer.\nC. Configure a GKE Ingress resource with type: LoadBalancer.\nThis option is incorrect because Ingress resources do not have a type: LoadBalancer property. The Ingress resource itself manages the creation of a load balancer.\nD. Configure a GKE Service resource with type: LoadBalancer.\nThis option creates a TCP/UDP load balancer, not an HTTP(S) load balancer, which does not meet the requirements for an HTTP(S) load balancer."
      },
      {
        "date": "2024-04-29T02:19:00.000Z",
        "voteCount": 1,
        "content": "You want to expose this application publicly behind a Cloud Load Balancing HTTP(S) load balancer. Hence using Service with type LoadBalancer"
      },
      {
        "date": "2023-09-19T22:16:00.000Z",
        "voteCount": 3,
        "content": "To expose your application publicly behind a Cloud Load Balancing HTTP(S) load balancer in a Google Kubernetes Engine (GKE) cluster, we should configure a GKE Ingress resource. This approach allows you to define rules for routing external HTTP(S) traffic to internal services based on hostnames and URL paths."
      },
      {
        "date": "2023-02-04T02:12:00.000Z",
        "voteCount": 4,
        "content": "The correct answer is D\n\nConfiguring a GKE ingress resource is not enough, you also need to expose the service with the type NodePort and then configure the ingress resource to point to that service.\n\nD is sufficient, then D is the correct answer. A lacks some more work."
      },
      {
        "date": "2023-01-08T01:39:00.000Z",
        "voteCount": 2,
        "content": "To expose your application publicly behind a Cloud Load Balancing HTTP(S) load balancer in a GKE cluster, you should configure a GKE Ingress resource or a GKE Service resource with type: LoadBalancer.\n\nTo configure a GKE Ingress resource, you need to define rules for routing HTTP(S) traffic to the application in the cluster. This is done by creating an Ingress object, which is associated with one or more Service objects, each of which is associated with a set of Pods. The GKE Ingress controller will then create a Google Cloud HTTP(S) Load Balancer and configure it according to the information in the Ingress and its associated Services.\n\nAlternatively, you can configure a GKE Service resource with type: LoadBalancer to expose your application publicly. This will create a Cloud Load Balancing HTTP(S) load balancer and associate it with the Service. The Service will then route traffic to the application Pods."
      },
      {
        "date": "2022-08-19T23:52:00.000Z",
        "voteCount": 2,
        "content": "A is correct"
      },
      {
        "date": "2022-07-29T01:28:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/kubernetes-engine/docs/concepts/ingress\nGKE Ingress for HTTP(S) Load Balancing \nThis page provides a general overview of what Ingress for HTTP(S) Load Balancing is and how it works. Google Kubernetes Engine (GKE) provides a built-in and managed Ingress controller called GKE Ingress. This controller implements Ingress resources as Google Cloud load balancers for HTTP(S) workloads in GKE."
      },
      {
        "date": "2022-05-25T04:27:00.000Z",
        "voteCount": 2,
        "content": "I think it's D.\nThey need a Load Balancer too. \nIngress just permits to expose a cluster, then A answer is not complete according to requirement."
      },
      {
        "date": "2022-05-26T05:12:00.000Z",
        "voteCount": 1,
        "content": "It's A... 'Cos Kind: Ingress create automatically a LB HTTP(S)"
      },
      {
        "date": "2022-12-31T03:08:00.000Z",
        "voteCount": 1,
        "content": "kind Service in GKE creates TCP/IP LB. If you want to leverage HTTP(s) you need to create Ingress class."
      },
      {
        "date": "2021-07-16T08:04:00.000Z",
        "voteCount": 4,
        "content": "(D) is not correct as service with type LoadBalancer create network load balancer not http load balancer. \n(A) is correct ingress will create http balancer without the need of specify type\nhttps://cloud.google.com/kubernetes-engine/docs/concepts/ingress#overview"
      },
      {
        "date": "2021-07-09T18:58:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/kubernetes-engine/docs/tutorials/http-balancer:\n\"When you specify kind: Service with type: LoadBalancer in the resource manifest, GKE creates a Service of type LoadBalancer\"\n\nD is correct"
      },
      {
        "date": "2021-07-30T18:44:00.000Z",
        "voteCount": 5,
        "content": "Changing my answer to A\nhttps://cloud.google.com/kubernetes-engine/docs/concepts/ingress:\n\"In GKE, an Ingress object defines rules for routing HTTP(S) traffic to applications running in a cluster. An Ingress object is associated with one or more Service objects, each of which is associated with a set of Pods. To learn more about how Ingress exposes applications using Services, see Service networking overview.\n\nWhen you create an Ingress object, the GKE Ingress controller creates a Google Cloud HTTP(S) Load Balancer and configures it according to the information in the Ingress and its associated Services.\""
      },
      {
        "date": "2021-05-21T00:20:00.000Z",
        "voteCount": 3,
        "content": "answer is D"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 71,
    "url": "https://www.examtopics.com/discussions/google/view/40505-exam-professional-cloud-developer-topic-1-question-71/",
    "body": "Your company is planning to migrate their on-premises Hadoop environment to the cloud. Increasing storage cost and maintenance of data stored in HDFS is a major concern for your company. You also want to make minimal changes to existing data analytics jobs and existing architecture.<br>How should you proceed with the migration?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate your data stored in Hadoop to BigQuery. Change your jobs to source their information from BigQuery instead of the on-premises Hadoop environment.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate Compute Engine instances with HDD instead of SSD to save costs. Then perform a full migration of your existing environment into the new one in Compute Engine instances.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Cloud Dataproc cluster on Google Cloud Platform, and then migrate your Hadoop environment to the new Cloud Dataproc cluster. Move your HDFS data into larger HDD disks to save on storage costs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Cloud Dataproc cluster on Google Cloud Platform, and then migrate your Hadoop code objects to the new cluster. Move your data to Cloud Storage and leverage the Cloud Dataproc connector to run jobs on that data.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 3,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-12-21T14:07:00.000Z",
        "voteCount": 7,
        "content": "I'd choose D."
      },
      {
        "date": "2021-07-10T17:15:00.000Z",
        "voteCount": 6,
        "content": "https://cloud.google.com/architecture/hadoop/hadoop-gcp-migration-overview:\n\"Keeping your data in a persistent HDFS cluster using Dataproc is more expensive than storing your data in Cloud Storage, which is what we recommend, as explained later. Keeping data in an HDFS cluster also limits your ability to use your data with other Google Cloud products.\"\n\"Google Cloud includes Dataproc, which is a managed Hadoop and Spark environment. You can use Dataproc to run most of your existing jobs with minimal alteration, so you don't need to move away from all of the Hadoop tools you already know\"\n\nD is the answer"
      },
      {
        "date": "2023-09-19T22:29:00.000Z",
        "voteCount": 1,
        "content": "Option D is correct."
      },
      {
        "date": "2023-08-29T06:46:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2023-01-08T01:41:00.000Z",
        "voteCount": 1,
        "content": "Option D is the most appropriate choice because it allows you to migrate your Hadoop code objects to a Cloud Dataproc cluster, which is a fully-managed Apache Hadoop and Apache Spark service on Google Cloud. This will allow you to make minimal changes to your existing data analytics jobs and existing architecture. Additionally, moving your data to Cloud Storage and using the Cloud Dataproc connector to run jobs on that data will allow you to take advantage of the scalability, durability, and security of Cloud Storage while also minimizing storage costs."
      },
      {
        "date": "2023-01-08T01:42:00.000Z",
        "voteCount": 1,
        "content": "A would involve a significant change to your existing data analytics jobs and architecture, as it would involve migrating your data to BigQuery and changing your jobs to source their information from BigQuery instead of Hadoop.\n\nB is not a feasible option because Compute Engine instances do not have the capability to run HDFS.\n\nC would not allow you to save on storage costs as it involves moving your data to larger HDD disks rather than a more cost-effective storage solution like Cloud Storage."
      },
      {
        "date": "2022-08-19T23:53:00.000Z",
        "voteCount": 2,
        "content": "D is correct"
      },
      {
        "date": "2022-08-19T23:53:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2022-05-17T06:55:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 72,
    "url": "https://www.examtopics.com/discussions/google/view/40506-exam-professional-cloud-developer-topic-1-question-72/",
    "body": "Your data is stored in Cloud Storage buckets. Fellow developers have reported that data downloaded from Cloud Storage is resulting in slow API performance.<br>You want to research the issue to provide details to the GCP support team.<br>Which command should you run?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tgsutil test \u05d2\u20ac\"o output.json gs://my-bucket",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tgsutil perfdiag \u05d2\u20ac\"o output.json gs://my-bucket\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tgcloud compute scp example-instance:~/test-data \u05d2\u20ac\"o output.json gs://my-bucket",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tgcloud services test \u05d2\u20ac\"o output.json gs://my-bucket"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-12-21T14:11:00.000Z",
        "voteCount": 14,
        "content": "B(https://cloud.google.com/storage/docs/gsutil/commands/perfdiag#providing-diagnostic-output-to-cloud-storage-team)"
      },
      {
        "date": "2021-07-10T17:21:00.000Z",
        "voteCount": 1,
        "content": "Spot-on link and answer"
      },
      {
        "date": "2024-07-17T08:58:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is B. gsutil perfdiag -o output.json gs://my-bucket .\n\nHere's why:\n\ngsutil perfdiag : This command is specifically designed to run a suite of diagnostic tests on a Cloud Storage bucket. It measures various performance aspects, including latency, throughput, and listing operations. This is exactly what you need to gather detailed information about the slow download performance.\n-o output.json : This option specifies that the results of the diagnostic tests should be saved to a JSON file named output.json . This file will contain detailed performance metrics that you can provide to the GCP support team."
      },
      {
        "date": "2024-07-17T08:58:00.000Z",
        "voteCount": 1,
        "content": "Let's break down why the other options are incorrect:\n\nA. gsutil test -o output.json gs://my-bucket : gsutil test is not a valid command. There's no command in gsutil called test .\nC. gcloud compute scp example-instance:~/test-data -o output.json gs://my-bucket : This command is used to copy files from a Compute Engine instance to a Cloud Storage bucket. It doesn't perform any performance diagnostics.\nD. gcloud services test -o output.json gs://my-bucket : gcloud services test is used to test the availability and performance of Google Cloud services, but it's not specific to Cloud Storage."
      },
      {
        "date": "2023-09-19T22:31:00.000Z",
        "voteCount": 1,
        "content": "To research the issue of slow API performance when downloading data from Cloud Storage, you can use the gsutil perfdiag command. This command runs a set of tests to report the actual performance of a Cloud Storage bucket and provides detailed information on the performance of individual operations."
      },
      {
        "date": "2023-01-08T01:43:00.000Z",
        "voteCount": 2,
        "content": "B. gsutil perfdiag -o output.json gs://my-bucket\n\nThe gsutil perfdiag command is used to diagnose performance issues with Cloud Storage. It can be used to perform various tests such as download, upload, and metadata operations. By using the -o flag, you can specify an output file where the results of the tests will be stored in JSON format. This output file can then be provided to the GCP support team to help them investigate the issue."
      },
      {
        "date": "2022-12-04T06:04:00.000Z",
        "voteCount": 1,
        "content": "CORRECT"
      },
      {
        "date": "2022-08-19T23:53:00.000Z",
        "voteCount": 2,
        "content": "B is correct"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 73,
    "url": "https://www.examtopics.com/discussions/google/view/36568-exam-professional-cloud-developer-topic-1-question-73/",
    "body": "You are using Cloud Build build to promote a Docker image to Development, Test, and Production environments. You need to ensure that the same Docker image is deployed to each of these environments.<br>How should you identify the Docker image in your build?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the latest Docker image tag.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a unique Docker image name.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the digest of the Docker image.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a semantic version Docker image tag."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 10,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-06-06T21:29:00.000Z",
        "voteCount": 20,
        "content": "C, since digests are immutable, whilst docker tags are mutable (hence not D). \nhttps://cloud.google.com/architecture/using-container-images"
      },
      {
        "date": "2021-01-24T12:21:00.000Z",
        "voteCount": 9,
        "content": "For me it's D, it's not a best practice to use image with the latest tag. And using the semantic version will ensure that all the environment use the exact same image with the wanted code."
      },
      {
        "date": "2021-02-19T21:07:00.000Z",
        "voteCount": 3,
        "content": "This is correct"
      },
      {
        "date": "2022-12-31T03:14:00.000Z",
        "voteCount": 3,
        "content": "You are not correct. The question is to ensure the images are the same not about docker image naming convention best practices. The only way to compare two images and say they are the same is digest hash. You can mistakenly tag two different images using the same semver tag."
      },
      {
        "date": "2024-07-17T09:03:00.000Z",
        "voteCount": 1,
        "content": "The best answer is C. Use the digest of the Docker image.\n\nHere's why:\n\nDocker Image Digest: A digest is a unique identifier for a specific Docker image. It's a cryptographic hash of the image's contents, ensuring that you're always deploying the exact same image across environments."
      },
      {
        "date": "2024-07-17T09:04:00.000Z",
        "voteCount": 1,
        "content": "Let's break down why the other options are less ideal:\n\nA. Use the latest Docker image tag: The latest tag is mutable. If you push a new image with the latest tag, it will overwrite the previous latest image. This can lead to inconsistencies across environments if different environments pull the latest tag at different times.\nB. Use a unique Docker image name: While using unique names can help with organization, it doesn't guarantee that you're deploying the same image across environments. You could accidentally push a different image with the same name, leading to inconsistencies.\nD. Use a semantic version Docker image tag: Semantic versioning is a good practice for managing software releases, but it doesn't guarantee that the image content is identical across environments. You could accidentally push a new image with the same semantic version but different content."
      },
      {
        "date": "2024-03-07T20:40:00.000Z",
        "voteCount": 1,
        "content": "C. Use the digest of the Docker image.\n\nWhen promoting Docker images across different environments in a CI/CD pipeline, it's crucial to ensure that exactly the same image is deployed to each environment. The most reliable way to identify a Docker image is by using its digest.\n\nHere's why using the digest is the best approach:\n\nThe digest is a SHA256 hash of the image's content and configuration, which uniquely identifies an image. If anything about the image changes, the digest changes. This means that if you deploy an image by its digest, you are guaranteed to deploy the exact same image in each environment.\n\nUsing the digest is more reliable than using tags like 'latest' or semantic versioning. Tags can be moved to point to different images, but digests are immutable. Once an image is pushed to a registry, its digest can never change."
      },
      {
        "date": "2023-09-19T22:34:00.000Z",
        "voteCount": 1,
        "content": "I would go with C."
      },
      {
        "date": "2023-03-29T01:54:00.000Z",
        "voteCount": 2,
        "content": "C. Use the digest of the Docker image.\n\nUsing the digest of the Docker image is the most reliable way to ensure that the exact same Docker image is deployed to each environment. The digest is a hash of the image content and metadata, which is unique to each image. This means that even if the image is tagged with different versions or names, the digest will remain the same as long as the content and metadata are identical.\n\nOn the other hand, using the latest Docker image tag or a semantic version tag may not guarantee that the exact same image is deployed to each environment. These tags are mutable and can be overwritten or updated, which could result in different images being deployed to different environments.\n\nUsing a unique Docker image name could work, but it may be more difficult to manage and track multiple images with different names, especially if there are many environments or frequent updates."
      },
      {
        "date": "2023-01-12T00:01:00.000Z",
        "voteCount": 1,
        "content": "Anser C because nees to be sure that the same image for the 3 envs. A tag version can be change between the deployment of the env."
      },
      {
        "date": "2023-01-08T01:44:00.000Z",
        "voteCount": 3,
        "content": "C. Use the digest of the Docker image.\n\nThe digest of the Docker image is a unique identifier for the specific version of the image. By using the digest, you can ensure that the same exact version of the image is deployed to each environment. Using the latest tag or a unique image name may not necessarily guarantee that the same version is deployed, as these tags may change over time. Using a semantic version tag would only ensure that the same version is deployed if you follow a strict versioning policy and only update the image by incrementing the patch or minor version number."
      },
      {
        "date": "2022-11-30T23:48:00.000Z",
        "voteCount": 1,
        "content": "C is the answer"
      },
      {
        "date": "2022-08-19T23:54:00.000Z",
        "voteCount": 2,
        "content": "D is correct"
      },
      {
        "date": "2022-05-17T07:05:00.000Z",
        "voteCount": 3,
        "content": "C is 100% correct"
      },
      {
        "date": "2022-02-26T08:22:00.000Z",
        "voteCount": 4,
        "content": "Read the question, it asks to ensure that the 'same' Docker image is deployed to every environment, so to identify the docker image, we have to use digests"
      },
      {
        "date": "2022-02-19T21:36:00.000Z",
        "voteCount": 3,
        "content": "C is correct.\nanother answers are not immutable."
      },
      {
        "date": "2022-08-18T00:34:00.000Z",
        "voteCount": 1,
        "content": "\"By design, the Git commit hash is immutable and references a specific version of your software\"..\nhttps://cloud.google.com/architecture/best-practices-for-building-containers"
      },
      {
        "date": "2021-07-10T17:34:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/architecture/best-practices-for-building-containers#tagging_using_semantic_versioning\n\nAnswer is D"
      },
      {
        "date": "2023-11-29T02:25:00.000Z",
        "voteCount": 1,
        "content": "I vote for this. We are likely looking for the best-practice way to 'promote' an image through dev, test, and prod environments. It is normal to use a tag with standard naming convention to identify/select images to promote e.g. tag v1.0.0. Using the digest would work, but this is not normal practice."
      },
      {
        "date": "2020-11-09T05:24:00.000Z",
        "voteCount": 5,
        "content": "for me the correct answer is A)"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 74,
    "url": "https://www.examtopics.com/discussions/google/view/40512-exam-professional-cloud-developer-topic-1-question-74/",
    "body": "Your company has created an application that uploads a report to a Cloud Storage bucket. When the report is uploaded to the bucket, you want to publish a message to a Cloud Pub/Sub topic. You want to implement a solution that will take a small amount to effort to implement.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the Cloud Storage bucket to trigger Cloud Pub/Sub notifications when objects are modified.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an App Engine application to receive the file; when it is received, publish a message to the Cloud Pub/Sub topic.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Cloud Function that is triggered by the Cloud Storage bucket. In the Cloud Function, publish a message to the Cloud Pub/Sub topic.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an application deployed in a Google Kubernetes Engine cluster to receive the file; when it is received, publish a message to the Cloud Pub/Sub topic."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-12-21T15:07:00.000Z",
        "voteCount": 16,
        "content": "Since one of reqs is \"You want to implement a solution that will take a small amount to effort to implement\" I'd choose A because no code has to be written. However option C works great as well and is recommended by https://cloud.google.com/storage/docs/pubsub-notifications#other_notification_options."
      },
      {
        "date": "2023-05-08T04:06:00.000Z",
        "voteCount": 1,
        "content": "you link says that you use cloud functions only when you dont want to publish message to pubsub"
      },
      {
        "date": "2024-07-17T09:09:00.000Z",
        "voteCount": 1,
        "content": "The best answer is A. Configure the Cloud Storage bucket to trigger Cloud Pub/Sub notifications when objects are modified.\n\nHere's why:\n\nSimplicity and Ease of Implementation: This approach is the most straightforward and requires minimal effort. Cloud Storage offers built-in integration with Cloud Pub/Sub, allowing you to configure notifications directly within the bucket settings.\nNo Additional Code: You don't need to write any custom code or deploy additional applications. Cloud Storage handles the notification process automatically.\nScalability and Reliability: Cloud Pub/Sub is a highly scalable and reliable messaging service, ensuring that your notifications are delivered efficiently and reliably."
      },
      {
        "date": "2024-07-17T09:09:00.000Z",
        "voteCount": 1,
        "content": "Let's look at why the other options are less ideal:\n\nB. Create an App Engine application to receive the file; when it is received, publish a message to the Cloud Pub/Sub topic. This approach requires you to create and deploy an App Engine application, which adds complexity and overhead.\nC. Create a Cloud Function that is triggered by the Cloud Storage bucket. In the Cloud Function, publish a message to the Cloud Pub/Sub topic. While Cloud Functions are a good option for event-driven tasks, this approach still requires you to write and deploy code, making it more complex than using built-in Cloud Storage notifications.\nD. Create an application deployed in a Google Kubernetes Engine cluster to receive the file; when it is received, publish a message to the Cloud Pub/Sub topic. This is the most complex and resource-intensive approach. It requires you to create and manage a Kubernetes cluster, deploy an application, and handle the notification process within your application code."
      },
      {
        "date": "2024-03-07T20:45:00.000Z",
        "voteCount": 1,
        "content": "A. Configure the Cloud Storage bucket to trigger Cloud Pub/Sub notifications when objects are modified.\n\nThis solution is straightforward and requires minimal effort to implement. Google Cloud Storage offers native support for publishing messages to Cloud Pub/Sub topics in response to changes in your bucket, like uploading a new file. By configuring Cloud Storage to automatically send a message to a Pub/Sub topic when a new report is uploaded, you can easily set up a real-time notification system without the need to write and maintain additional code."
      },
      {
        "date": "2023-09-19T22:42:00.000Z",
        "voteCount": 1,
        "content": "A is correct."
      },
      {
        "date": "2023-01-08T02:38:00.000Z",
        "voteCount": 4,
        "content": "C. Create a Cloud Function that is triggered by the Cloud Storage bucket. In the Cloud Function, publish a message to the Cloud Pub/Sub topic. This would be the most straightforward and easiest solution to implement, as it only requires creating a Cloud Function and setting it up to be triggered by the Cloud Storage bucket. This solution would not require deploying any additional resources, such as App Engine or a Kubernetes cluster, and would not require any significant code changes to the application uploading the report."
      },
      {
        "date": "2023-01-10T06:11:00.000Z",
        "voteCount": 1,
        "content": "A does exactly the same just without Cloud Functions. Notifications triggered by a Cloud Storage event, work as a trigger for PubSub. No need for Cloud Functions."
      },
      {
        "date": "2023-01-16T02:33:00.000Z",
        "voteCount": 2,
        "content": "Changing to option C. Option A state \"when objects are modified\" which is not the request here."
      },
      {
        "date": "2023-05-08T04:05:00.000Z",
        "voteCount": 1,
        "content": "documentation says that you use cloud functions only when you dont want to publish message to pubsub.\nbut you need to publish message to pubsub - therefore answer is A\n https://cloud.google.com/storage/docs/pubsub-notifications#other_notification_options\nhttps://cloud.google.com/storage/docs/pubsub-notifications#overview"
      },
      {
        "date": "2022-08-19T23:54:00.000Z",
        "voteCount": 3,
        "content": "I think A is correct"
      },
      {
        "date": "2022-06-01T11:15:00.000Z",
        "voteCount": 1,
        "content": "A -- Least steps for the overall requirement. \nOption C, tho suggested, won't need to trigger pub/sub unless it requires heavyweight or  many subsequent steps. (https://cloud.google.com/storage/docs/pubsub-notifications#:~:text=don%27t%20want%20to%20manage%20a%20Pub/Sub%20topic)"
      },
      {
        "date": "2022-05-20T07:17:00.000Z",
        "voteCount": 1,
        "content": "Also, aren't cloud storage objects immutable you can't modify but version them."
      },
      {
        "date": "2022-05-20T07:16:00.000Z",
        "voteCount": 1,
        "content": "A : https://cloud.google.com/storage/docs/pubsub-notifications"
      },
      {
        "date": "2022-01-24T02:22:00.000Z",
        "voteCount": 2,
        "content": "A not is correct because trigger must to be only in insert and not modify. only C is possible"
      },
      {
        "date": "2022-01-08T10:17:00.000Z",
        "voteCount": 3,
        "content": "Answer A takes least effort to implement the solution"
      },
      {
        "date": "2021-02-19T21:17:00.000Z",
        "voteCount": 4,
        "content": "Option-A required least amount of effort to implement. https://cloud.google.com/storage/docs/reporting-changes#enabling"
      },
      {
        "date": "2021-07-10T17:40:00.000Z",
        "voteCount": 2,
        "content": "Agree; just a one line code and you're done"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 75,
    "url": "https://www.examtopics.com/discussions/google/view/43616-exam-professional-cloud-developer-topic-1-question-75/",
    "body": "Your teammate has asked you to review the code below, which is adding a credit to an account balance in Cloud Datastore.<br>Which improvement should you suggest your teammate make?<br><img src=\"/assets/media/exam-media/04137/0005200001.png\" class=\"in-exam-image\"><br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGet the entity with an ancestor query.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGet and put the entity in a transaction.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a strongly consistent transactional database.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDon't return the account entity from the function."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-03-07T20:51:00.000Z",
        "voteCount": 1,
        "content": "B. Get and put the entity in a transaction.\n\nThe code provided is updating an account balance in Cloud Datastore. To ensure data integrity and consistency, such an update should be done within a transaction to avoid issues with concurrent updates which could result in an incorrect balance if the account is accessed by multiple processes at the same time."
      },
      {
        "date": "2023-09-19T22:45:00.000Z",
        "voteCount": 1,
        "content": "Put the entity in transaction"
      },
      {
        "date": "2023-01-08T02:40:00.000Z",
        "voteCount": 3,
        "content": "B. Get and put the entity in a transaction.\n\nIt is a good practice to perform a get and put within a transaction to ensure that the update to the balance is atomic. This prevents other processes from reading the balance and making updates to it simultaneously, which could lead to incorrect or inconsistent results. By using a transaction, your teammate can ensure that the balance is updated correctly and consistently."
      },
      {
        "date": "2022-08-19T23:54:00.000Z",
        "voteCount": 3,
        "content": "B is correct"
      },
      {
        "date": "2022-01-08T10:19:00.000Z",
        "voteCount": 1,
        "content": "B is correct option in this case"
      },
      {
        "date": "2021-07-10T18:02:00.000Z",
        "voteCount": 4,
        "content": "https://cloud.google.com/datastore/docs/concepts/transactions#uses_for_transactions:\n\"This requires a transaction because the value of balance in an entity may be updated by another user after this code fetches the object, but before it saves the modified object. Without a transaction, the user's request uses the value of balance prior to the other user's update, and the save overwrites the new value. With a transaction, the application is told about the other user's update.\"\n\nB is the answer"
      },
      {
        "date": "2021-01-31T02:46:00.000Z",
        "voteCount": 1,
        "content": "The answer is B."
      },
      {
        "date": "2021-02-19T22:56:00.000Z",
        "voteCount": 2,
        "content": "Yes Option-B is correct. Here is the link: https://cloud.google.com/datastore/docs/concepts/transactions#using_transactions"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 76,
    "url": "https://www.examtopics.com/discussions/google/view/52221-exam-professional-cloud-developer-topic-1-question-76/",
    "body": "Your company stores their source code in a Cloud Source Repositories repository. Your company wants to build and test their code on each source code commit to the repository and requires a solution that is managed and has minimal operations overhead.<br>Which method should they use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Build with a trigger configured for each source code commit.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Jenkins deployed via the Google Cloud Platform Marketplace, configured to watch for source code commits.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a Compute Engine virtual machine instance with an open source continuous integration tool, configured to watch for source code commits.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a source code commit trigger to push a message to a Cloud Pub/Sub topic that triggers an App Engine service to build the source code."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-07-10T18:12:00.000Z",
        "voteCount": 6,
        "content": "https://cloud.google.com/build/docs/automating-builds/create-manage-triggers#:~:text=A%20Cloud%20Build%20trigger%20automatically,changes%20that%20match%20certain%20criteria.\n\nA is the answer"
      },
      {
        "date": "2023-01-08T02:43:00.000Z",
        "voteCount": 1,
        "content": "A. Use Cloud Build with a trigger configured for each source code commit.\n\nCloud Build is a fully managed service for building, testing, and deploying software quickly. It integrates with Cloud Source Repositories and can be triggered by source code commits, which makes it an ideal solution for building and testing code on each commit. It requires minimal operations overhead as it is fully managed by Google Cloud."
      },
      {
        "date": "2022-08-19T23:54:00.000Z",
        "voteCount": 3,
        "content": "A is correct"
      },
      {
        "date": "2022-08-19T17:54:00.000Z",
        "voteCount": 2,
        "content": "Cloud Build"
      },
      {
        "date": "2021-05-09T12:24:00.000Z",
        "voteCount": 4,
        "content": "Answer is A"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 77,
    "url": "https://www.examtopics.com/discussions/google/view/40520-exam-professional-cloud-developer-topic-1-question-77/",
    "body": "You are writing a Compute Engine hosted application in project A that needs to securely authenticate to a Cloud Pub/Sub topic in project B.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the instances with a service account owned by project B. Add the service account as a Cloud Pub/Sub publisher to project A.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the instances with a service account owned by project A. Add the service account as a publisher on the topic.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Application Default Credentials to use the private key of a service account owned by project B. Add the service account as a Cloud Pub/Sub publisher to project A.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Application Default Credentials to use the private key of a service account owned by project A. Add the service account as a publisher on the topic"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 3,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-12-21T21:51:00.000Z",
        "voteCount": 13,
        "content": "I vote for B."
      },
      {
        "date": "2023-09-19T22:49:00.000Z",
        "voteCount": 1,
        "content": "I would go with B."
      },
      {
        "date": "2023-01-08T02:44:00.000Z",
        "voteCount": 1,
        "content": "Option B is the correct answer because it involves creating a service account in project A and adding it as a publisher to the Cloud Pub/Sub topic in project B. This allows the Compute Engine instances in project A to authenticate to the Cloud Pub/Sub topic in project B using the service account's credentials. The other options do not involve creating a service account in project A or adding it as a publisher to the Cloud Pub/Sub topic in project B, so they are not valid solutions."
      },
      {
        "date": "2023-01-08T02:44:00.000Z",
        "voteCount": 1,
        "content": "Option A is incorrect because it is not a secure way to authenticate to a Cloud Pub/Sub topic in project B. In this option, the instances in project A are using a service account owned by project B, but the service account is not added as a publisher on the topic. This means that the service account does not have the necessary permissions to publish messages to the topic."
      },
      {
        "date": "2023-01-08T02:45:00.000Z",
        "voteCount": 1,
        "content": "Option D is incorrect because it does not authenticate to the Cloud Pub/Sub topic in project B. In this option, Application Default Credentials are being used to authenticate to the topic, but the private key of a service account owned by project A is being used. This service account does not have the necessary permissions to publish messages to the topic in project B."
      },
      {
        "date": "2023-01-08T02:45:00.000Z",
        "voteCount": 1,
        "content": "Option C is incorrect because it does not properly authenticate to the Cloud Pub/Sub topic in project B. In this option, Application Default Credentials are being used to authenticate to the topic, but the private key of a service account owned by project B is being used. While the service account may have the necessary permissions to publish messages to the topic, using Application Default Credentials with a private key is not a secure way to authenticate to Cloud Pub/Sub."
      },
      {
        "date": "2022-08-19T23:55:00.000Z",
        "voteCount": 2,
        "content": "B is correct"
      },
      {
        "date": "2021-12-02T17:38:00.000Z",
        "voteCount": 3,
        "content": "why not D?"
      },
      {
        "date": "2022-08-04T21:53:00.000Z",
        "voteCount": 2,
        "content": "Application Default Credentials would work only if the resource/project has already been set up with the GOOGLE_APPLICATION_CREDENTIALS or the service account key for that project. \nSo, if you were to use the Application Default Credentials then you are assuming that any one of the above two scenarios has already been met. And you can not answer the question based on assumptions!"
      },
      {
        "date": "2022-08-04T21:54:00.000Z",
        "voteCount": 2,
        "content": "Reference - https://cloud.google.com/docs/authentication/production"
      },
      {
        "date": "2021-07-10T18:16:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/pubsub/docs/access-control:\n\"For example, suppose a service account in Cloud Project A wants to publish messages to a topic in Cloud Project B. You could accomplish this by granting the service account Edit permission in Cloud Project B\"\n\nB is the answer"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 78,
    "url": "https://www.examtopics.com/discussions/google/view/40574-exam-professional-cloud-developer-topic-1-question-78/",
    "body": "You are developing a corporate tool on Compute Engine for the finance department, which needs to authenticate users and verify that they are in the finance department. All company employees use G Suite.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Cloud Identity-Aware Proxy on the HTTP(s) load balancer and restrict access to a Google Group containing users in the finance department. Verify the provided JSON Web Token within the application.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Cloud Identity-Aware Proxy on the HTTP(s) load balancer and restrict access to a Google Group containing users in the finance department. Issue client-side certificates to everybody in the finance team and verify the certificates in the application.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Cloud Armor Security Policies to restrict access to only corporate IP address ranges. Verify the provided JSON Web Token within the application.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Cloud Armor Security Policies to restrict access to only corporate IP address ranges. Issue client side certificates to everybody in the finance team and verify the certificates in the application."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-12-22T11:27:00.000Z",
        "voteCount": 17,
        "content": "I'd say A(https://cloud.google.com/endpoints/docs/openapi/authenticating-users-google-id)."
      },
      {
        "date": "2021-07-10T18:28:00.000Z",
        "voteCount": 5,
        "content": "https://cloud.google.com/armor/docs/security-policy-overview#:~:text=Google%20Cloud%20Armor%20security%20policies%20enable%20you%20to%20allow%20or,Private%20Cloud%20(VPC)%20networks.:\n\"Google Cloud Armor security policies protect your application by providing Layer 7 filtering and by scrubbing incoming requests for common web attacks or other Layer 7 attributes to potentially block traffic before it reaches your load balanced backend services or backend buckets\"\n\nC and D are wrong.\n\nhttps://cloud.google.com/endpoints/docs/openapi/authenticating-users-google-id:\n\"To authenticate a user, a client application must send a JSON Web Token (JWT) in the authorization header of the HTTP request to your backend API\"\n\nA is correct"
      },
      {
        "date": "2024-03-07T21:05:00.000Z",
        "voteCount": 1,
        "content": "A. Enable Cloud Identity-Aware Proxy (IAP) on the HTTP(s) load balancer and restrict access to a Google Group containing users in the finance department. Verify the provided JSON Web Token within the application.\n\nCloud IAP allows you to manage access to your web applications running on Compute Engine by verifying a user\u2019s identity and determining if that user should be allowed to access the application. You can integrate Cloud IAP with Google Groups to restrict access to specific groups within your G Suite domain, such as a group for the finance department. When a user authenticates via Cloud IAP, a JSON Web Token (JWT) is issued that can be used within your application to further verify the user's identity and departmental membership."
      },
      {
        "date": "2023-09-19T22:51:00.000Z",
        "voteCount": 1,
        "content": "A is correct."
      },
      {
        "date": "2023-01-08T02:47:00.000Z",
        "voteCount": 1,
        "content": "Option A is the correct solution because it uses Cloud Identity-Aware Proxy (IAP) to authenticate and authorize users to access the application. IAP verifies the identity of users accessing the application through G Suite and checks if they are members of the specified Google Group. IAP also verifies the JSON Web Token (JWT) provided in the request to ensure that the request is legitimate."
      },
      {
        "date": "2023-01-08T02:47:00.000Z",
        "voteCount": 1,
        "content": "Option D is not a correct solution because it combines the use of Cloud Armor Security Policies and client-side certificates, but does not have a way to authenticate and authorize users. It also does not have a way to verify the legitimacy of the requests."
      },
      {
        "date": "2023-01-08T02:47:00.000Z",
        "voteCount": 1,
        "content": "Option C is not a correct solution because it uses Cloud Armor Security Policies to restrict access based on IP addresses, but does not have a way to authenticate and authorize users."
      },
      {
        "date": "2023-01-08T02:47:00.000Z",
        "voteCount": 1,
        "content": "Option B is not a correct solution because it does not use IAP to authenticate and authorize users. It only issues client-side certificates to users in the finance department, but does not have a way to verify that the user presenting the certificate is actually the owner of the certificate."
      },
      {
        "date": "2022-08-19T23:55:00.000Z",
        "voteCount": 2,
        "content": "A is correct"
      },
      {
        "date": "2022-05-28T04:52:00.000Z",
        "voteCount": 1,
        "content": "A should be the answer -- IAP x G-Suite"
      },
      {
        "date": "2022-03-31T01:25:00.000Z",
        "voteCount": 1,
        "content": "Community choice is A"
      },
      {
        "date": "2022-03-29T22:34:00.000Z",
        "voteCount": 1,
        "content": "Ans is B\nhttps://cloud.google.com/iap/docs/tutorial-gce"
      },
      {
        "date": "2022-05-17T20:28:00.000Z",
        "voteCount": 1,
        "content": "Not B.\nAnswer is A"
      },
      {
        "date": "2021-09-04T22:20:00.000Z",
        "voteCount": 3,
        "content": "A\nIAP and JWT \nhttps://cloud.google.com/iap/docs/signed-headers-howto#securing_iap_headers"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 79,
    "url": "https://www.examtopics.com/discussions/google/view/40836-exam-professional-cloud-developer-topic-1-question-79/",
    "body": "Your API backend is running on multiple cloud providers. You want to generate reports for the network latency of your API.<br>Which two steps should you take? (Choose two.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Zipkin collector to gather data.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Fluentd agent to gather data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Stackdriver Trace to generate reports.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Stackdriver Debugger to generate report.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Stackdriver Profiler to generate report."
    ],
    "answer": "AC",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AC",
        "count": 9,
        "isMostVoted": true
      },
      {
        "answer": "BC",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "BD",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-12-27T05:38:00.000Z",
        "voteCount": 17,
        "content": "for me the solution is A and C:\nhttps://cloud.google.com/trace/docs/zipkin"
      },
      {
        "date": "2021-01-24T12:28:00.000Z",
        "voteCount": 6,
        "content": "AC as Zipkin is used for to gather data for latency issues and SD trace purpose is to enable us to have a better view on the application code latency"
      },
      {
        "date": "2024-07-17T22:21:00.000Z",
        "voteCount": 1,
        "content": "The two steps you should take are:\n\nB. Use Fluentd agent to gather data. Fluentd is a great tool for collecting logs and metrics from various sources, including your API backend running on multiple cloud providers. It can be configured to collect network latency data and send it to a centralized location for analysis.\nC. Use Stackdriver Trace to generate reports. Stackdriver Trace is a distributed tracing system that helps you understand the performance of your applications. It can be used to collect and analyze network latency data, providing insights into bottlenecks and performance issues."
      },
      {
        "date": "2024-07-17T22:22:00.000Z",
        "voteCount": 1,
        "content": "Here's why the other options are less suitable:\n\nA. Use Zipkin collector to gather data. While Zipkin is a popular distributed tracing system, it's not directly integrated with Stackdriver. Using Zipkin would require additional configuration and integration to send data to Stackdriver for reporting.\nD. Use Stackdriver Debugger to generate reports. Stackdriver Debugger is designed for debugging code, not for generating network latency reports. It focuses on inspecting the state of your application at specific points in time.\nE. Use Stackdriver Profiler to generate reports. Stackdriver Profiler is used for profiling your application's performance, focusing on CPU usage and memory allocation. It's not the ideal tool for analyzing network latency."
      },
      {
        "date": "2024-03-07T21:11:00.000Z",
        "voteCount": 1,
        "content": "For generating reports on network latency for an API that is distributed across multiple cloud providers, you would typically need to gather trace data and then analyze it:\n\nA. Use Zipkin collector to gather data: Zipkin is a distributed tracing system that helps gather timing data needed to troubleshoot latency problems in service architectures. You can use Zipkin collectors to gather trace data from your API backend regardless of where it's running. This trace data can provide insights into the latency of different service calls.\n\nC. Use Stackdriver Trace to generate reports: Stackdriver Trace (part of Google Cloud\u2019s operations suite) allows you to analyze how requests propagate through your application and receive detailed latency reports for your API. If you are already using Stackdriver on Google Cloud, you can extend its usage to analyze trace data collected from other cloud providers as well."
      },
      {
        "date": "2023-08-25T04:58:00.000Z",
        "voteCount": 1,
        "content": "solution is AC: \nhttps://cloud.google.com/trace/docs/zipkin"
      },
      {
        "date": "2023-03-29T03:22:00.000Z",
        "voteCount": 2,
        "content": "The two steps you should take to generate reports for the network latency of your API running on multiple cloud providers are:\n\nA. Use Zipkin collector to gather data: Zipkin is a distributed tracing system that helps you gather data about the latency of requests made to your API. It allows you to trace requests as they flow through your system, and provides insight into the performance of your services. You can use Zipkin collectors to collect data from multiple cloud providers, and then generate reports to analyze the latency of your API.\n\nC. Use Stackdriver Trace to generate reports: Stackdriver Trace is a distributed tracing system that helps you trace requests across multiple services and provides detailed performance data about your applications. It allows you to visualize and analyze the performance of your API and its dependencies. You can use Stackdriver Trace to generate reports about the network latency of your API running on multiple cloud providers.\n\nTherefore, the correct options are A and C."
      },
      {
        "date": "2023-01-08T02:54:00.000Z",
        "voteCount": 1,
        "content": "The correct answer would be: A. Use Zipkin collector to gather data and C. Use Stackdriver Trace to generate reports.\n\nUsing Zipkin collector will allow you to gather data from your instrumented application running on multiple cloud providers. Stackdriver Trace can then be used to generate reports based on this data.\n\nOption B, using Fluentd agent, is not related to generating reports on network latency for an API.\n\nOption D, using Stackdriver Debugger, is not related to generating reports on network latency for an API.\n\nOption E, using Stackdriver Profiler, is not related to generating reports on network latency for an API."
      },
      {
        "date": "2022-11-22T18:39:00.000Z",
        "voteCount": 1,
        "content": "A/C\nhttps://cloud.google.com/trace/docs/zipkin"
      },
      {
        "date": "2022-08-19T23:56:00.000Z",
        "voteCount": 1,
        "content": "BD are correct"
      },
      {
        "date": "2022-07-29T04:36:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/trace/docs/zipkin#frequently_asked_questions\n use a Zipkin server to receive traces from Zipkin clients and forward those traces to Cloud Trace for analysis."
      },
      {
        "date": "2022-01-24T02:45:00.000Z",
        "voteCount": 1,
        "content": "CE. E for latency Cloud Profiler. For tracing can be use also zipkin but better tracing"
      },
      {
        "date": "2022-01-08T10:39:00.000Z",
        "voteCount": 2,
        "content": "A and C are correct solution."
      },
      {
        "date": "2021-07-17T23:53:00.000Z",
        "voteCount": 4,
        "content": "AC\nA: to support multiple cloud providers \nhttps://cloud.google.com/trace \nZipkin tracers to submit data to Cloud Trace. Projects running on App Engine are automatically captured.\n\nC: to generate reports for the network latency\nhttps://cloud.google.com/trace/docs/quickstart#analysis_reports_window"
      },
      {
        "date": "2021-07-10T18:32:00.000Z",
        "voteCount": 3,
        "content": "\"latency\" is the key word here so C is one of the answers; Stackdriver Trace\n\nhttps://cloud.google.com/trace/docs/zipkin:\n\"receive traces from Zipkin clients and forward those traces to Cloud Trace for analysis.\"\n\nA is the other answer"
      },
      {
        "date": "2021-06-22T22:08:00.000Z",
        "voteCount": 1,
        "content": "I think the answer is AC"
      },
      {
        "date": "2021-04-06T09:33:00.000Z",
        "voteCount": 1,
        "content": "for me it is C and E, as profiler is used for performance analysis"
      },
      {
        "date": "2020-12-26T19:53:00.000Z",
        "voteCount": 2,
        "content": "I would go with BC"
      },
      {
        "date": "2020-12-27T05:39:00.000Z",
        "voteCount": 1,
        "content": "C is correct. But B is used for logging and not for monitoring."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 80,
    "url": "https://www.examtopics.com/discussions/google/view/50397-exam-professional-cloud-developer-topic-1-question-80/",
    "body": "Case study -<br>This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.<br>To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.<br>At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.<br><br>To start the case study -<br>To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an<br>All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.<br><br>Company Overview -<br>HipLocal is a community application designed to facilitate communication between people in close proximity. It is used for event planning and organizing sporting events, and for businesses to connect with their local communities. HipLocal launched recently in a few neighborhoods in Dallas and is rapidly growing into a global phenomenon. Its unique style of hyper-local community communication and business outreach is in demand around the world.<br><br>Executive Statement -<br>We are the number one local community app; it's time to take our local community services global. Our venture capital investors want to see rapid growth and the same great experience for new local and virtual communities that come online, whether their members are 10 or 10000 miles away from each other.<br><br>Solution Concept -<br>HipLocal wants to expand their existing service, with updated functionality, in new regions to better serve their global customers. They want to hire and train a new team to support these regions in their time zones. They will need to ensure that the application scales smoothly and provides clear uptime data.<br><br>Existing Technical Environment -<br>HipLocal's environment is a mix of on-premises hardware and infrastructure running in Google Cloud Platform. The HipLocal team understands their application well, but has limited experience in global scale applications. Their existing technical environment is as follows:<br>* Existing APIs run on Compute Engine virtual machine instances hosted in GCP.<br>* State is stored in a single instance MySQL database in GCP.<br>* Data is exported to an on-premises Teradata/Vertica data warehouse.<br>* Data analytics is performed in an on-premises Hadoop environment.<br>* The application has no logging.<br>* There are basic indicators of uptime; alerts are frequently fired when the APIs are unresponsive.<br><br>Business Requirements -<br>HipLocal's investors want to expand their footprint and support the increase in demand they are seeing. Their requirements are:<br>* Expand availability of the application to new regions.<br>* Increase the number of concurrent users that can be supported.<br>* Ensure a consistent experience for users when they travel to different regions.<br>* Obtain user activity metrics to better understand how to monetize their product.<br>* Ensure compliance with regulations in the new regions (for example, GDPR).<br>* Reduce infrastructure management time and cost.<br>* Adopt the Google-recommended practices for cloud computing.<br><br>Technical Requirements -<br>* The application and backend must provide usage metrics and monitoring.<br>* APIs require strong authentication and authorization.<br>* Logging must be increased, and data should be stored in a cloud analytics platform.<br>* Move to serverless architecture to facilitate elastic scaling.<br>* Provide authorized access to internal apps in a secure manner.<br>Which database should HipLocal use for storing user activity?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBigQuery\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud SQL",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Spanner",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Datastore"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 5,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-04-18T07:17:00.000Z",
        "voteCount": 13,
        "content": "In the case study is stated: \"Obtain user activity metrics to better understand how to monetize their product\", which means that they'll need to analyse the user activity, so... I'll go with answer A (BigQuery)"
      },
      {
        "date": "2021-07-10T18:43:00.000Z",
        "voteCount": 3,
        "content": "Agree with you on this one"
      },
      {
        "date": "2024-07-17T22:26:00.000Z",
        "voteCount": 1,
        "content": "The best answer here is A. BigQuery . Here's why:\n\nScalability and Analytics: HipLocal needs to store and analyze user activity data to understand how to monetize their product. BigQuery is designed for petabyte-scale data storage and offers powerful analytics capabilities. It's ideal for handling the large volumes of user activity data that HipLocal will generate as they expand globally.\nCost-Effectiveness: BigQuery is pay-per-use, making it cost-effective for storing large amounts of data that might not be accessed frequently. This aligns with HipLocal's goal of reducing infrastructure management costs.\nCompliance: BigQuery is a compliant platform, meeting requirements like GDPR. This is crucial for HipLocal as they expand into new regions with varying data privacy regulations."
      },
      {
        "date": "2024-07-17T22:26:00.000Z",
        "voteCount": 1,
        "content": "Why other options are less suitable:\n\nB. Cloud SQL: While Cloud SQL is a good choice for relational databases, it's not designed for the massive scale and analytics needs of user activity data. It can be expensive for large datasets.\nC. Cloud Spanner: Cloud Spanner is a globally distributed, strongly consistent database. While it's excellent for transactional data, it's overkill for user activity data that doesn't require strong consistency. It's also more complex and expensive than BigQuery.\nD. Cloud Datastore: Cloud Datastore is a NoSQL database that's good for real-time updates. However, its query capabilities are less powerful than BigQuery, and it's not as well-suited for large-scale analytics."
      },
      {
        "date": "2024-03-07T21:15:00.000Z",
        "voteCount": 1,
        "content": "Here's why Cloud Spanner is the best fit for HipLocal's needs:\n\nGlobal Scalability: Cloud Spanner can scale horizontally to handle increased loads and number of concurrent users, which is aligned with the rapid growth that HipLocal is experiencing.\n\nStrong Consistency: It provides strong consistency guarantees, ensuring that users have a consistent experience regardless of the region they're accessing the application from.\n\nHigh Availability: Spanner's built-in replication across multiple regions makes it highly available, which helps to meet uptime requirements and ensure compliance with regulations like GDPR that may require data to be stored in certain regions.\n\nManaged Service: As a fully managed service, it reduces the time and cost associated with infrastructure management, which meets the business requirement to minimize management overhead."
      },
      {
        "date": "2023-09-19T22:59:00.000Z",
        "voteCount": 1,
        "content": "For Storing user data Datastore is best."
      },
      {
        "date": "2023-09-20T08:09:00.000Z",
        "voteCount": 1,
        "content": "I think A would be better fit for this. Please ignore the above answer."
      },
      {
        "date": "2023-05-15T15:24:00.000Z",
        "voteCount": 1,
        "content": "A (BigQuery) is more apropiated for user activities.\n\nIf it was manage user states, I would consider  D (Datastore/Firestore) but this is not the case.\nSo, from my point of view, A is the correct answer."
      },
      {
        "date": "2022-08-19T23:56:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2022-07-29T04:38:00.000Z",
        "voteCount": 2,
        "content": "Bigquery for user activity analysis . And also the user activity is kind of raw data which being used to segment user or according age , choice etc so Bigquery fits best fr this use cases"
      },
      {
        "date": "2022-05-20T07:40:00.000Z",
        "voteCount": 1,
        "content": "Toss b/w A and D . It depends what needs to be done on user activity, if analytics then A. (Big query)  else if customizing customer experience then D (Datastore)"
      },
      {
        "date": "2022-02-25T05:02:00.000Z",
        "voteCount": 1,
        "content": "I agree with having to use Datstore"
      },
      {
        "date": "2022-01-24T02:56:00.000Z",
        "voteCount": 1,
        "content": "A. database only for user activity"
      },
      {
        "date": "2022-01-14T01:43:00.000Z",
        "voteCount": 2,
        "content": "I choose D, datastore."
      },
      {
        "date": "2021-09-19T12:54:00.000Z",
        "voteCount": 3,
        "content": "#37 \"Your existing application keeps user state information in a single MySQL database. This state information is very user-specific and depends heavily on how long a user has been using an application. The MySQL database is causing challenges to maintain and enhance the schema for various users. Which storage option should you choose?\"\n\nhttps://cloud.google.com/datastore/docs/concepts/overview#what_its_good_for\n\n\"Datastore/Firestore can store and query the following types of data: \nUser profiles that deliver a customized experience based on the user\u2019s past activities and preferences\"\n\nI feel like this is a toss up between these two since we're talking about user profiles/data, would vote for D here bc MySQL offers a really rigid schema and isn't well suited to massive scaling either."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 81,
    "url": "https://www.examtopics.com/discussions/google/view/58038-exam-professional-cloud-developer-topic-1-question-81/",
    "body": "Case study -<br>This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.<br>To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.<br>At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.<br><br>To start the case study -<br>To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an<br>All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.<br><br>Company Overview -<br>HipLocal is a community application designed to facilitate communication between people in close proximity. It is used for event planning and organizing sporting events, and for businesses to connect with their local communities. HipLocal launched recently in a few neighborhoods in Dallas and is rapidly growing into a global phenomenon. Its unique style of hyper-local community communication and business outreach is in demand around the world.<br><br>Executive Statement -<br>We are the number one local community app; it's time to take our local community services global. Our venture capital investors want to see rapid growth and the same great experience for new local and virtual communities that come online, whether their members are 10 or 10000 miles away from each other.<br><br>Solution Concept -<br>HipLocal wants to expand their existing service, with updated functionality, in new regions to better serve their global customers. They want to hire and train a new team to support these regions in their time zones. They will need to ensure that the application scales smoothly and provides clear uptime data.<br><br>Existing Technical Environment -<br>HipLocal's environment is a mix of on-premises hardware and infrastructure running in Google Cloud Platform. The HipLocal team understands their application well, but has limited experience in global scale applications. Their existing technical environment is as follows:<br>* Existing APIs run on Compute Engine virtual machine instances hosted in GCP.<br>* State is stored in a single instance MySQL database in GCP.<br>* Data is exported to an on-premises Teradata/Vertica data warehouse.<br>* Data analytics is performed in an on-premises Hadoop environment.<br>* The application has no logging.<br>* There are basic indicators of uptime; alerts are frequently fired when the APIs are unresponsive.<br><br>Business Requirements -<br>HipLocal's investors want to expand their footprint and support the increase in demand they are seeing. Their requirements are:<br>* Expand availability of the application to new regions.<br>* Increase the number of concurrent users that can be supported.<br>* Ensure a consistent experience for users when they travel to different regions.<br>* Obtain user activity metrics to better understand how to monetize their product.<br>* Ensure compliance with regulations in the new regions (for example, GDPR).<br>* Reduce infrastructure management time and cost.<br>* Adopt the Google-recommended practices for cloud computing.<br><br>Technical Requirements -<br>* The application and backend must provide usage metrics and monitoring.<br>* APIs require strong authentication and authorization.<br>* Logging must be increased, and data should be stored in a cloud analytics platform.<br>* Move to serverless architecture to facilitate elastic scaling.<br>* Provide authorized access to internal apps in a secure manner.<br>HipLocal is configuring their access controls.<br>Which firewall configuration should they implement?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBlock all traffic on port 443.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAllow all traffic into the network.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAllow traffic on port 443 for a specific tag.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAllow all traffic on port 443 into the network."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-05-27T04:13:00.000Z",
        "voteCount": 7,
        "content": "It depends on which authentication we are talking about. If it is an authentication to internal app, the answer is C (with specific tag). If it is an authentication to 'the' app that HipLocal offers to general users, the answer is D (with tag, all users outside that tag will be rejected). It is not clear to me, on which tag we are talking here."
      },
      {
        "date": "2024-07-17T22:32:00.000Z",
        "voteCount": 1,
        "content": "The best answer is C. Allow traffic on port 443 for a specific tag. Here's why:\n\nSecurity: Blocking all traffic on port 443 (option A) would prevent HTTPS communication, which is essential for secure web traffic. Allowing all traffic (option B) would be extremely insecure and leave the network vulnerable to attacks. Allowing all traffic on port 443 (option D) would also be insecure, as it would allow any device to access the network on that port.\nGranular Control: Using tags to control access on port 443 (option C) provides a granular and secure approach. HipLocal can create a specific tag for authorized devices or services and only allow traffic from those tagged resources on port 443. This ensures that only trusted entities can access the network over HTTPS."
      },
      {
        "date": "2024-07-17T22:32:00.000Z",
        "voteCount": 1,
        "content": "Example:\n\nHipLocal could create a tag called \"trusted-services\" and apply it to their web servers and load balancers. They could then configure their firewall to allow traffic on port 443 only from resources with the \"trusted-services\" tag. This would prevent unauthorized access to their network while allowing legitimate HTTPS traffic.\n\nIn summary: Option C provides the most secure and flexible approach to configuring HipLocal's firewall, allowing them to control access to their network on port 443 while maintaining security."
      },
      {
        "date": "2023-09-19T23:02:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2023-05-08T05:26:00.000Z",
        "voteCount": 1,
        "content": "app is running on compute engine\ni assume nginx Is running on compute instance and you need to expose 443 and 80 for network tag"
      },
      {
        "date": "2022-08-19T23:56:00.000Z",
        "voteCount": 1,
        "content": "C  is correct"
      },
      {
        "date": "2022-03-31T18:46:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2021-07-16T17:56:00.000Z",
        "voteCount": 3,
        "content": "Port 443 -&gt; HTTPS\n\nBlocking traffic on 443 does not make sense so A is wrong\nAllow all traffic is definitely not secure so B is out too\n\nBetween C and D I'll take C"
      },
      {
        "date": "2021-07-31T03:33:00.000Z",
        "voteCount": 4,
        "content": "On second thought, correct answer is D as the application needs to be exposed externally the port 443 can be opened for all traffic."
      },
      {
        "date": "2022-01-22T10:30:00.000Z",
        "voteCount": 7,
        "content": "I would take C, to use tags as well, so that only traffic to selected VMs is allowed from outside, probably you don't want to expose every VM via port 443? \nhttps://cloud.google.com/vpc/docs/add-remove-network-tags"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 82,
    "url": "https://www.examtopics.com/discussions/google/view/40579-exam-professional-cloud-developer-topic-1-question-82/",
    "body": "Case study -<br>This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.<br>To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.<br>At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.<br><br>To start the case study -<br>To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an<br>All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.<br><br>Company Overview -<br>HipLocal is a community application designed to facilitate communication between people in close proximity. It is used for event planning and organizing sporting events, and for businesses to connect with their local communities. HipLocal launched recently in a few neighborhoods in Dallas and is rapidly growing into a global phenomenon. Its unique style of hyper-local community communication and business outreach is in demand around the world.<br><br>Executive Statement -<br>We are the number one local community app; it's time to take our local community services global. Our venture capital investors want to see rapid growth and the same great experience for new local and virtual communities that come online, whether their members are 10 or 10000 miles away from each other.<br><br>Solution Concept -<br>HipLocal wants to expand their existing service, with updated functionality, in new regions to better serve their global customers. They want to hire and train a new team to support these regions in their time zones. They will need to ensure that the application scales smoothly and provides clear uptime data.<br><br>Existing Technical Environment -<br>HipLocal's environment is a mix of on-premises hardware and infrastructure running in Google Cloud Platform. The HipLocal team understands their application well, but has limited experience in global scale applications. Their existing technical environment is as follows:<br>* Existing APIs run on Compute Engine virtual machine instances hosted in GCP.<br>* State is stored in a single instance MySQL database in GCP.<br>* Data is exported to an on-premises Teradata/Vertica data warehouse.<br>* Data analytics is performed in an on-premises Hadoop environment.<br>* The application has no logging.<br>* There are basic indicators of uptime; alerts are frequently fired when the APIs are unresponsive.<br><br>Business Requirements -<br>HipLocal's investors want to expand their footprint and support the increase in demand they are seeing. Their requirements are:<br>* Expand availability of the application to new regions.<br>* Increase the number of concurrent users that can be supported.<br>* Ensure a consistent experience for users when they travel to different regions.<br>* Obtain user activity metrics to better understand how to monetize their product.<br>* Ensure compliance with regulations in the new regions (for example, GDPR).<br>* Reduce infrastructure management time and cost.<br>* Adopt the Google-recommended practices for cloud computing.<br><br>Technical Requirements -<br>* The application and backend must provide usage metrics and monitoring.<br>* APIs require strong authentication and authorization.<br>* Logging must be increased, and data should be stored in a cloud analytics platform.<br>* Move to serverless architecture to facilitate elastic scaling.<br>* Provide authorized access to internal apps in a secure manner.<br>HipLocal's data science team wants to analyze user reviews.<br>How should they prepare the data?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Cloud Data Loss Prevention API for redaction of the review dataset.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Cloud Data Loss Prevention API for de-identification of the review dataset.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Cloud Natural Language Processing API for redaction of the review dataset.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Cloud Natural Language Processing API for de-identification of the review dataset."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 6,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-12-29T00:50:00.000Z",
        "voteCount": 19,
        "content": "Answer is B . Data loss prevention api is used for de-identification not natural language api"
      },
      {
        "date": "2024-07-17T22:36:00.000Z",
        "voteCount": 1,
        "content": "The best answer here is B. Use the Cloud Data Loss Prevention API for de-identification of the review dataset. Here's why:\n\nData Loss Prevention API: This API is specifically designed for identifying and protecting sensitive data. It can be used to de-identify data, which means replacing sensitive information with non-sensitive substitutes. This is ideal for user reviews, as they might contain personal information like names, addresses, or other details that need to be protected.\nDe-identification: De-identification is the process of removing or replacing sensitive information in a dataset while preserving its usefulness for analysis. This is crucial for HipLocal's data science team, as they need to analyze user reviews without compromising user privacy."
      },
      {
        "date": "2024-07-17T22:36:00.000Z",
        "voteCount": 1,
        "content": "Why other options are less suitable:\n\nA. Cloud Data Loss Prevention API for redaction: Redaction involves completely removing sensitive information from a dataset. While this can be effective, it might not be the best approach for user reviews, as it could remove valuable context and insights.\nC. Cloud Natural Language Processing API for redaction: The Natural Language Processing API is designed for understanding and analyzing text. It's not specifically designed for data protection or de-identification.\nD. Cloud Natural Language Processing API for de-identification: The Natural Language Processing API is not equipped for de-identification. It's primarily focused on tasks like sentiment analysis, entity recognition, and text summarization."
      },
      {
        "date": "2024-03-07T21:26:00.000Z",
        "voteCount": 1,
        "content": "B. Use the Cloud Data Loss Prevention API for de-identification of the review dataset.\n\nFor analyzing user reviews, especially if they contain sensitive user information, it's important to protect user privacy. The Cloud Data Loss Prevention (DLP) API provides ways to de-identify sensitive data, which includes redaction, masking, tokenization, and other transformation techniques to obscure or remove sensitive information.\n\nDe-identification refers to the process of removing or altering information that could be used to identify an individual, making the data safe for analysis without exposing personal information. This is crucial when handling user data to ensure compliance with privacy regulations and maintain user trust."
      },
      {
        "date": "2024-02-13T02:39:00.000Z",
        "voteCount": 1,
        "content": "Of course it's DLP.\nNLP API makes no sense here."
      },
      {
        "date": "2023-12-22T12:58:00.000Z",
        "voteCount": 1,
        "content": "Answer is B\nhttps://cloud.google.com/dlp/docs/deidentify-sensitive-data"
      },
      {
        "date": "2023-11-20T11:36:00.000Z",
        "voteCount": 1,
        "content": "https://www.exam-answer.com/hiplocal-data-preparation\n\n\"De-identification is the process of removing or obfuscating personally identifiable information (PII) from a dataset, so that individuals cannot be identified. In this case, the data science team needs to analyze user reviews, which could potentially contain PII such as names, email addresses, or other personal information. To protect the privacy of the users, the data should be de-identified before it is analyzed.\n\nThe Cloud Natural Language Processing API provides various features such as entity recognition, sentiment analysis, and syntax analysis. The API also includes a feature for de-identification, which can be used to remove PII from text data. This feature uses machine learning models to identify and mask or replace PII in the text.\n\nIn contrast, the Cloud Data Loss Prevention API is designed to identify and redact sensitive data, such as credit card numbers, social security numbers, or other types of PII. It is not intended for general de-identification of text data.\""
      },
      {
        "date": "2023-09-19T23:05:00.000Z",
        "voteCount": 1,
        "content": "D is correct."
      },
      {
        "date": "2023-04-23T08:57:00.000Z",
        "voteCount": 1,
        "content": "The Cloud Natural Language Processing API can help to extract insights from the user reviews, such as sentiment analysis and entity recognition.  Additionally, de-identification can help to protect user privacy by removing any personal information from the review data."
      },
      {
        "date": "2022-08-19T23:57:00.000Z",
        "voteCount": 1,
        "content": "B  is correct"
      },
      {
        "date": "2022-05-28T05:24:00.000Z",
        "voteCount": 1,
        "content": "It looks like 'redaction' is a type of 'de-identification'.\nhttps://cloud.google.com/dlp/docs/transformations-reference#redaction"
      },
      {
        "date": "2022-05-28T05:30:00.000Z",
        "voteCount": 2,
        "content": "B. \nI suspect this is more an English problem than a cloud problem. \"redaction of the review dataset\" means removing the review itself. \"de-identification of the review dataset\" means you keep the review text itself, but mask the reviewer's identity so that we do not know any more who wrote it."
      },
      {
        "date": "2022-01-22T10:24:00.000Z",
        "voteCount": 1,
        "content": "A or B?\n\nwhat speaks for de-identification over reduction?\n\nreduction: replace sensitive data with a mask\nde-identification: replace sensitive data, while keeping possibility of re-identification by trusted party\n\nreduction protects user's data even more, whereas de-identification might be better for analyzing the data and link them together, right?"
      },
      {
        "date": "2022-01-08T10:44:00.000Z",
        "voteCount": 1,
        "content": "B is the correct answer"
      },
      {
        "date": "2021-12-02T01:32:00.000Z",
        "voteCount": 1,
        "content": "I would take C as the purpose is to \"analyze user reviews\". Generally there is not sensitive data in reviews so I eliminate A and B. Natural Language Processing API is for analyzing things like reviews and comments, it has nothing to do with de-identification."
      },
      {
        "date": "2021-12-11T01:22:00.000Z",
        "voteCount": 1,
        "content": "Reviewing this question again, the question asks \"how to prepare the data\" so I change my mind to B, to de-identify the data by Cloud Data Loss Prevention first. After that Natural Language  Processing can be used to analyze the data."
      },
      {
        "date": "2021-11-01T07:18:00.000Z",
        "voteCount": 1,
        "content": "Answer B: https://cloud.google.com/dlp/docs/deidentify-sensitive-data"
      },
      {
        "date": "2021-11-01T07:19:00.000Z",
        "voteCount": 1,
        "content": "Backs up: \"Ensure compliance with regulations in the new regions (for example, GDPR).\""
      },
      {
        "date": "2021-07-18T00:14:00.000Z",
        "voteCount": 4,
        "content": "B: https://cloud.google.com/architecture/de-identification-re-identification-pii-using-cloud-dlp\nDe-identification of PII in large-scale datasets using Cloud DLP\nCloud DLP enables transformations such as redaction, masking, tokenization, bucketing, and other methods of de-identification."
      },
      {
        "date": "2021-07-16T18:04:00.000Z",
        "voteCount": 2,
        "content": "I would take C; Cloud Natural Language Processing API for redaction\n\nData Loss Prevention or DLP is not meant for analytics so A and B are wrong while de-identification is for DLP"
      },
      {
        "date": "2020-12-27T05:58:00.000Z",
        "voteCount": 1,
        "content": "For me the solution is C"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 83,
    "url": "https://www.examtopics.com/discussions/google/view/53266-exam-professional-cloud-developer-topic-1-question-83/",
    "body": "Case study -<br>This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.<br>To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.<br>At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.<br><br>To start the case study -<br>To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an<br>All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.<br><br>Company Overview -<br>HipLocal is a community application designed to facilitate communication between people in close proximity. It is used for event planning and organizing sporting events, and for businesses to connect with their local communities. HipLocal launched recently in a few neighborhoods in Dallas and is rapidly growing into a global phenomenon. Its unique style of hyper-local community communication and business outreach is in demand around the world.<br><br>Executive Statement -<br>We are the number one local community app; it's time to take our local community services global. Our venture capital investors want to see rapid growth and the same great experience for new local and virtual communities that come online, whether their members are 10 or 10000 miles away from each other.<br><br>Solution Concept -<br>HipLocal wants to expand their existing service, with updated functionality, in new regions to better serve their global customers. They want to hire and train a new team to support these regions in their time zones. They will need to ensure that the application scales smoothly and provides clear uptime data.<br><br>Existing Technical Environment -<br>HipLocal's environment is a mix of on-premises hardware and infrastructure running in Google Cloud Platform. The HipLocal team understands their application well, but has limited experience in global scale applications. Their existing technical environment is as follows:<br>* Existing APIs run on Compute Engine virtual machine instances hosted in GCP.<br>* State is stored in a single instance MySQL database in GCP.<br>* Data is exported to an on-premises Teradata/Vertica data warehouse.<br>* Data analytics is performed in an on-premises Hadoop environment.<br>* The application has no logging.<br>* There are basic indicators of uptime; alerts are frequently fired when the APIs are unresponsive.<br><br>Business Requirements -<br>HipLocal's investors want to expand their footprint and support the increase in demand they are seeing. Their requirements are:<br>* Expand availability of the application to new regions.<br>* Increase the number of concurrent users that can be supported.<br>* Ensure a consistent experience for users when they travel to different regions.<br>* Obtain user activity metrics to better understand how to monetize their product.<br>* Ensure compliance with regulations in the new regions (for example, GDPR).<br>* Reduce infrastructure management time and cost.<br>* Adopt the Google-recommended practices for cloud computing.<br><br>Technical Requirements -<br>* The application and backend must provide usage metrics and monitoring.<br>* APIs require strong authentication and authorization.<br>* Logging must be increased, and data should be stored in a cloud analytics platform.<br>* Move to serverless architecture to facilitate elastic scaling.<br>* Provide authorized access to internal apps in a secure manner.<br>In order for HipLocal to store application state and meet their stated business requirements, which database service should they migrate to?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Spanner\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Datastore",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Memorystore as a cache",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSeparate Cloud SQL clusters for each region"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 5,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-07-17T22:48:00.000Z",
        "voteCount": 10,
        "content": "https://cloud.google.com/blog/products/databases/spanner-relational-database-for-all-size-applications-faqs\nhttps://cloud.google.com/architecture/best-practices-cloud-spanner-gaming-database#select_a_data_locality_to_meet_compliance_requirements\nhttps://cloud.google.com/blog/products/gcp/introducing-cloud-spanner-a-global-database-service-for-mission-critical-applications\n\nA. Cloud Spanner\n- global service \n- supports durably store application data \n- supports GDPR, to meet data locality"
      },
      {
        "date": "2021-11-01T07:21:00.000Z",
        "voteCount": 1,
        "content": "Agree with Celia."
      },
      {
        "date": "2024-07-17T22:39:00.000Z",
        "voteCount": 1,
        "content": "The best answer here is A. Cloud Spanner . Here's why:\n\nGlobal Scalability and Consistency: HipLocal needs to expand globally and ensure a consistent experience for users across regions. Cloud Spanner is a globally distributed, strongly consistent database that can handle high volumes of data and transactions across multiple regions. This ensures that users have a seamless experience regardless of their location.\nStrong Consistency: HipLocal's application state needs to be consistent, especially for critical operations like user authentication, event scheduling, and payment processing. Cloud Spanner's strong consistency guarantees that all data is updated across all replicas simultaneously, preventing data inconsistencies and ensuring data integrity.\nScalability: HipLocal needs to increase the number of concurrent users they can support. Cloud Spanner is designed for scalability, allowing HipLocal to easily add capacity as their user base grows."
      },
      {
        "date": "2024-07-17T22:39:00.000Z",
        "voteCount": 1,
        "content": "Reduced Management Overhead: Cloud Spanner is a fully managed service, meaning HipLocal doesn't need to worry about managing infrastructure, backups, or security patches. This reduces their management overhead and allows them to focus on developing their application."
      },
      {
        "date": "2024-07-17T22:39:00.000Z",
        "voteCount": 1,
        "content": "Why other options are less suitable:\n\nB. Cloud Datastore: While Cloud Datastore is a good choice for NoSQL data, it's eventually consistent, meaning data might not be immediately updated across all replicas. This could lead to inconsistencies in application state, which is not ideal for HipLocal's requirements.\nC. Cloud Memorystore as a cache: Cloud Memorystore is a great caching solution, but it's not a primary database. It's used to improve performance by storing frequently accessed data in memory. It's not suitable for storing application state or handling transactions.\nD. Separate Cloud SQL clusters for each region: This approach would require managing multiple databases, which can be complex and expensive. It also wouldn't provide the same level of global consistency and scalability as Cloud Spanner."
      },
      {
        "date": "2024-03-07T21:29:00.000Z",
        "voteCount": 1,
        "content": "Here's how Cloud Spanner aligns with HipLocal's business requirements:\n\nIt allows for a global distribution of databases, ensuring users have a consistent experience no matter their location.\nCloud Spanner's horizontal scalability supports an increasing number of concurrent users, which is necessary for HipLocal's rapid growth.\nIt offers high availability and regional data replication, which can help HipLocal ensure compliance with various regional data regulations like GDPR.\nManaged service reduces the infrastructure management time and cost, meeting another of HipLocal's key requirements."
      },
      {
        "date": "2023-09-19T23:07:00.000Z",
        "voteCount": 1,
        "content": "A is best suited."
      },
      {
        "date": "2023-07-29T21:30:00.000Z",
        "voteCount": 1,
        "content": "I think A"
      },
      {
        "date": "2023-02-06T10:51:00.000Z",
        "voteCount": 1,
        "content": "Cloud Spanner is a highly scalable, globally-distributed database service offered by Google Cloud, but it may not be the best fit for HipLocal's needs. While Cloud Spanner provides automatic and instant scaling, strong consistency guarantees, and high availability, it also comes with a higher operational overhead and cost compared to other Google Cloud databases. Additionally, Cloud Spanner is designed for large, mission-critical applications that require strict consistency guarantees across multiple regions, which may not be necessary for HipLocal's current requirements.\n\nIn this case, it would be more appropriate for HipLocal to separate Cloud SQL clusters for each region to store their application state, as this solution would provide the necessary data storage capabilities and be more cost-effective for their current requirements."
      },
      {
        "date": "2023-02-09T06:57:00.000Z",
        "voteCount": 2,
        "content": "Changing to A because of this requirement: Ensure a consistent experience for users when they travel to different regions."
      },
      {
        "date": "2022-08-19T23:57:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2021-07-16T18:10:00.000Z",
        "voteCount": 1,
        "content": "This is similar to qn 48; I would say A is the answer"
      },
      {
        "date": "2021-08-06T20:02:00.000Z",
        "voteCount": 1,
        "content": "Changing to D as qn 50 answer is Cloud SQL; consistency sake"
      },
      {
        "date": "2021-11-01T07:24:00.000Z",
        "voteCount": 2,
        "content": "From case study instructions: \"Each question is independent of the other questions in this case study.\"\nI am leaning towards Spanner here. What do you think?"
      },
      {
        "date": "2021-05-21T04:27:00.000Z",
        "voteCount": 3,
        "content": "I guess that the answer is D because of the GDPR"
      },
      {
        "date": "2021-06-23T05:51:00.000Z",
        "voteCount": 1,
        "content": "Tend to agree with you"
      },
      {
        "date": "2022-02-25T05:09:00.000Z",
        "voteCount": 1,
        "content": "Seriously you want to spin up cloud sql in every region? \u2026"
      },
      {
        "date": "2022-08-18T01:56:00.000Z",
        "voteCount": 1,
        "content": "\" Reduce infrastructure management time and cost\"..  Cloud Spanner is a database for business critical applications that completely replaces one or more Data Warehouses...  is out of scope. I Vote Cloud SQL"
      },
      {
        "date": "2023-05-08T06:55:00.000Z",
        "voteCount": 1,
        "content": "what about management time for cloud sql clusters?\nyou can tune your cloud spanner instances, nobody forces you to use 1000 nodes.\n1. Cloud Spanner charges for the amount of storage used per month. The pricing starts at $0.30/GB/month for regional storage and $0.60/GB/month for multi-regional storage.\n2. Cloud Spanner charges for the number of nodes used per month. The pricing starts at $0.90/hour/node for regional instances and $1.44/hour/node for multi-regional instances.\n3. Cloud Spanner charges for the amount of data processed per month. The pricing starts at $0.06/GB for regional instances and $0.12/GB for multi-regional instances"
      },
      {
        "date": "2023-05-08T06:58:00.000Z",
        "voteCount": 1,
        "content": "isnt it affordable for hiplocal company that wants to \"left footprint\"??"
      },
      {
        "date": "2022-11-15T02:42:00.000Z",
        "voteCount": 1,
        "content": "So spanner doesnt  support  GDPR??? kkkkkkkkkkkkkk"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 84,
    "url": "https://www.examtopics.com/discussions/google/view/40580-exam-professional-cloud-developer-topic-1-question-84/",
    "body": "You have an application deployed in production. When a new version is deployed, you want to ensure that all production traffic is routed to the new version of your application. You also want to keep the previous version deployed so that you can revert to it if there is an issue with the new version.<br>Which deployment strategy should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBlue/green deployment\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCanary deployment",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRolling deployment",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRecreate deployment"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 7,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-12-22T11:49:00.000Z",
        "voteCount": 21,
        "content": "Blue/green seems to be more appropriate(https://www.redhat.com/en/topics/devops/what-is-blue-green-deployment)"
      },
      {
        "date": "2024-07-17T22:43:00.000Z",
        "voteCount": 1,
        "content": "The best answer here is A. Blue/green deployment . Here's why:\n\nBlue/Green Deployment: This strategy involves running two identical environments: a \"blue\" environment (the current production version) and a \"green\" environment (the new version). Traffic is initially routed to the blue environment. Once the green environment is fully deployed and tested, traffic is switched over to the green environment. This allows for a quick rollback to the blue environment if there are issues with the new version."
      },
      {
        "date": "2024-07-17T22:43:00.000Z",
        "voteCount": 1,
        "content": "Let's look at why the other options are less suitable:\n\nB. Canary Deployment: This strategy gradually rolls out the new version to a small subset of users while the majority of traffic continues to use the old version. This is useful for testing and monitoring the new version before full rollout, but it doesn't immediately route all traffic to the new version.\nC. Rolling Deployment: This strategy gradually replaces instances of the old version with instances of the new version. While it allows for a gradual rollout, it doesn't guarantee that all traffic is immediately routed to the new version.\nD. Recreate Deployment: This strategy involves completely replacing the old deployment with the new one. This is a simple approach, but it doesn't provide a way to easily revert to the previous version if there are issues."
      },
      {
        "date": "2023-09-19T23:08:00.000Z",
        "voteCount": 1,
        "content": "This can be achieved by using Blue/Green approach."
      },
      {
        "date": "2023-02-04T03:43:00.000Z",
        "voteCount": 1,
        "content": "Definitely Blue/Green Deployment"
      },
      {
        "date": "2022-08-19T23:58:00.000Z",
        "voteCount": 3,
        "content": "Sorry I mean A is correct"
      },
      {
        "date": "2022-08-19T23:57:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2022-05-20T07:57:00.000Z",
        "voteCount": 2,
        "content": "A - Blue green seems appropriate."
      },
      {
        "date": "2022-04-02T11:26:00.000Z",
        "voteCount": 1,
        "content": "Community choice is A: Blue/Green"
      },
      {
        "date": "2021-07-16T18:11:00.000Z",
        "voteCount": 1,
        "content": "This is the same as qn 9;  answer is B"
      },
      {
        "date": "2021-07-23T19:53:00.000Z",
        "voteCount": 5,
        "content": "no... qn 9 is canary. because it wants to minimize the impact, you runs two versions together for testing.  This one wants all traffic use the new version.  so, it can be A or C.  A is recommended and clear cut; BUT costly.  C is cheaper; but, it does not routed all to new version IMMEDATELY.  so.  I pick A, since, question did not mention cost."
      },
      {
        "date": "2021-07-30T19:00:00.000Z",
        "voteCount": 3,
        "content": "Yes you're right I didn't read the qn thoroughly so answer is A"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 85,
    "url": "https://www.examtopics.com/discussions/google/view/69686-exam-professional-cloud-developer-topic-1-question-85/",
    "body": "You are porting an existing Apache/MySQL/PHP application stack from a single machine to Google<br>Kubernetes Engine. You need to determine how to containerize the application. Your approach should follow Google-recommended best practices for availability.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPackage each component in a separate container. Implement readiness and liveness probes.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPackage the application in a single container. Use a process management tool to manage each component.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPackage each component in a separate container. Use a script to orchestrate the launch of the components.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPackage the application in a single container. Use a bash script as an entrypoint to the container, and then spawn each component as a background job."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-01-06T05:14:00.000Z",
        "voteCount": 6,
        "content": "A. Package each component in a separate container. Implement readiness and liveness probes.\n\nThis is the recommended approach for containerizing an application for use on Kubernetes. By packaging each component in a separate container, you can ensure that each component is isolated and can be managed independently. You can then use readiness and liveness probes to monitor the health and availability of each component, which will help ensure the overall availability of the application."
      },
      {
        "date": "2023-01-06T05:14:00.000Z",
        "voteCount": 1,
        "content": "D. Package the application in a single container. Use a bash script as an entrypoint to the container, and then spawn each component as a background job.\n\nThis option is not recommended because it does not follow best practices for containerization. By packaging the entire application in a single container, you would not be able to manage the individual components of the application independently, which could make it more difficult to ensure their availability. Additionally, using a bash script to spawn each component as a background job is not an effective way to manage and monitor the availability of the components."
      },
      {
        "date": "2023-01-06T05:14:00.000Z",
        "voteCount": 1,
        "content": "C. Package each component in a separate container. Use a script to orchestrate the launch of the components.\n\nThis option is not recommended because it does not follow best practices for containerization. While packaging each component in a separate container is a good approach, using a script to orchestrate the launch of the components is not an effective way to ensure their availability. Instead, you should use readiness and liveness probes to monitor the health and availability of each component."
      },
      {
        "date": "2023-01-06T05:14:00.000Z",
        "voteCount": 1,
        "content": "B. Package the application in a single container. Use a process management tool to manage each component.\n\nThis option is not recommended because it does not follow best practices for containerization. By packaging the entire application in a single container, you would not be able to manage the individual components of the application independently, which could make it more difficult to ensure their availability."
      },
      {
        "date": "2023-09-19T23:10:00.000Z",
        "voteCount": 1,
        "content": "A is best suited here."
      },
      {
        "date": "2022-12-22T07:42:00.000Z",
        "voteCount": 1,
        "content": "A is the answer.\n\nhttps://cloud.google.com/architecture/best-practices-for-building-containers#package_a_single_app_per_container\nWhen you start working with containers, it's a common mistake to treat them as virtual machines that can run many different things simultaneously. A container can work this way, but doing so reduces most of the advantages of the container model. For example, take a classic Apache/MySQL/PHP stack: you might be tempted to run all the components in a single container. However, the best practice is to use two or three different containers: one for Apache, one for MySQL, and potentially one for PHP if you are running PHP-FPM."
      },
      {
        "date": "2022-11-15T02:16:00.000Z",
        "voteCount": 1,
        "content": "the best practice is to use two or three different containers: one for Apache, one for MySQL, and potentially one for PHP if you are running PHP-FPM.\n\nBecause a container is designed to have the same lifecycle as the app it hosts, each of your containers should contain only one app. When a container starts, so should the app, and when the app stops, so should the container. The following diagram shows this best practice.\n\nhttps://cloud.google.com/architecture/best-practices-for-building-containers#package_a_single_app_per_container\nAnswer A"
      },
      {
        "date": "2022-11-16T19:53:00.000Z",
        "voteCount": 1,
        "content": "did you take the exam?"
      },
      {
        "date": "2022-11-20T23:33:00.000Z",
        "voteCount": 2,
        "content": "Nope , not yet. im doing so soon"
      },
      {
        "date": "2022-08-19T23:58:00.000Z",
        "voteCount": 2,
        "content": "A  is correct"
      },
      {
        "date": "2022-01-08T11:06:00.000Z",
        "voteCount": 3,
        "content": "According to me \"A\" is the correct answer, because the best practice says \"classic Apache/MySQL/PHP stack: you might be tempted to run all the components in a single container. However, the best practice is to use two or three different containers: one for Apache, one for MySQL, and potentially one for PHP if you are running PHP-FPM.\""
      },
      {
        "date": "2022-01-18T05:23:00.000Z",
        "voteCount": 1,
        "content": "Agree with Option A.\n https://cloud.google.com/blog/products/containers-kubernetes/7-best-practices-for-building-containers"
      },
      {
        "date": "2022-01-18T05:24:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/architecture/best-practices-for-building-containers"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 86,
    "url": "https://www.examtopics.com/discussions/google/view/69737-exam-professional-cloud-developer-topic-1-question-86/",
    "body": "You are developing an application that will be launched on Compute Engine instances into multiple distinct projects, each corresponding to the environments in your software development process (development, QA, staging, and production). The instances in each project have the same application code but a different configuration. During deployment, each instance should receive the application's configuration based on the environment it serves. You want to minimize the number of steps to configure this flow. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWhen creating your instances, configure a startup script using the gcloud command to determine the project name that indicates the correct environment.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn each project, configure a metadata key \u05d2\u20acenvironment\u05d2\u20ac whose value is the environment it serves. Use your deployment tool to query the instance metadata and configure the application based on the \u05d2\u20acenvironment\u05d2\u20ac value.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy your chosen deployment tool on an instance in each project. Use a deployment job to retrieve the appropriate configuration file from your version control system, and apply the configuration when deploying the application on each instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDuring each instance launch, configure an instance custom-metadata key named \u05d2\u20acenvironment\u05d2\u20ac whose value is the environment the instance serves. Use your deployment tool to query the instance metadata, and configure the application based on the \u05d2\u20acenvironment\u05d2\u20ac value."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 7,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-07-17T23:57:00.000Z",
        "voteCount": 1,
        "content": "The best answer is D. During each instance launch, configure an instance custom-metadata key named \u2018environment\u2019 whose value is the environment the instance serves. Use your deployment tool to query the instance metadata, and configure the application based on the \u2018environment\u2019 value.\n\nHere's why:\n\nSimplicity and Efficiency: This approach is the most straightforward and efficient. It leverages existing Compute Engine features (custom metadata) and avoids the need for additional scripts or deployment tools.\nCentralized Configuration: Custom metadata is associated with the instance itself, making it a centralized and reliable source of environment information.\nFlexibility: Your deployment tool can easily query the instance metadata during deployment, allowing you to dynamically configure the application based on the environment."
      },
      {
        "date": "2024-07-17T23:57:00.000Z",
        "voteCount": 1,
        "content": "Let's analyze why the other options are less ideal:\n\nA. Startup script using gcloud command: While startup scripts can be used, they require additional code and complexity. They also rely on the project name, which might not be the most reliable way to determine the environment.\nB. Metadata key \u2018environment\u2019 in each project: This approach requires configuring the metadata key in each project, which can be tedious and error-prone. It also doesn't provide a centralized way to manage environment information.\nC. Deployment tool on an instance in each project: This approach introduces additional complexity and overhead. It requires deploying and managing a separate deployment tool in each project, which can be challenging to maintain."
      },
      {
        "date": "2023-11-21T01:06:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/compute/docs/metadata/setting-custom-metadata#set-custom-project-wide-metadata"
      },
      {
        "date": "2023-09-19T23:52:00.000Z",
        "voteCount": 1,
        "content": "Option B is correct we usually put this details in Environment variable."
      },
      {
        "date": "2023-01-06T05:12:00.000Z",
        "voteCount": 2,
        "content": "Option A, configuring a startup script using the gcloud command to determine the project name, is not a feasible solution because it requires additional steps to be taken during instance launch. Option C, deploying a deployment tool on an instance in each project and using a deployment job to retrieve the appropriate configuration file, is not a feasible solution because it requires additional steps to be taken during instance launch and involves the use of a separate deployment tool. Option D, configuring an instance custom-metadata key named \"environment\" during each instance launch, is not a feasible solution because it requires additional steps to be taken during instance launch."
      },
      {
        "date": "2023-01-06T05:12:00.000Z",
        "voteCount": 2,
        "content": "Answer is B\nYou can configure a metadata key named \"environment\" in each project, with a value corresponding to the environment it serves (development, QA, staging, or production). Then, you can use your deployment tool to query the instance metadata and configure the application based on the \"environment\" value. This allows you to minimize the number of steps to configure the flow, as you only need to set the \"environment\" value in each project and use your deployment tool to query the metadata."
      },
      {
        "date": "2022-12-22T07:40:00.000Z",
        "voteCount": 1,
        "content": "B is the answer.\n\nhttps://cloud.google.com/compute/docs/metadata/setting-custom-metadata#set-custom"
      },
      {
        "date": "2022-11-15T02:06:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/compute/docs/metadata/querying-metadata"
      },
      {
        "date": "2022-11-15T02:12:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/compute/docs/metadata/setting-custom-metadata#set-custom"
      },
      {
        "date": "2022-11-16T19:53:00.000Z",
        "voteCount": 1,
        "content": "Did you take the exam?"
      },
      {
        "date": "2022-08-19T23:58:00.000Z",
        "voteCount": 2,
        "content": "B is correct"
      },
      {
        "date": "2022-04-27T21:24:00.000Z",
        "voteCount": 2,
        "content": "Vote B"
      },
      {
        "date": "2022-04-09T03:34:00.000Z",
        "voteCount": 2,
        "content": "Answer should be B"
      },
      {
        "date": "2022-02-28T03:55:00.000Z",
        "voteCount": 3,
        "content": "D, \n'environment' is not in one of the default key\nhttps://cloud.google.com/compute/docs/metadata/default-metadata-values"
      },
      {
        "date": "2022-02-25T05:28:00.000Z",
        "voteCount": 3,
        "content": "For Answer D : \nQuestion says minimize steps and adding metadata to each instance seems longer route ?"
      },
      {
        "date": "2022-01-09T03:24:00.000Z",
        "voteCount": 4,
        "content": "I vote B"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 87,
    "url": "https://www.examtopics.com/discussions/google/view/69736-exam-professional-cloud-developer-topic-1-question-87/",
    "body": "You are developing an ecommerce application that stores customer, order, and inventory data as relational tables inside Cloud Spanner. During a recent load test, you discover that Spanner performance is not scaling linearly as expected. Which of the following is the cause?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe use of 64-bit numeric types for 32-bit numbers.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe use of the STRING data type for arbitrary-precision values.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe use of Version 1 UUIDs as primary keys that increase monotonically.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe use of LIKE instead of STARTS_WITH keyword for parameterized SQL queries."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-01-24T12:02:00.000Z",
        "voteCount": 8,
        "content": "C is correct https://cloud.google.com/spanner/docs/schema-and-data-model#choosing_a_primary_key"
      },
      {
        "date": "2024-07-18T00:04:00.000Z",
        "voteCount": 1,
        "content": "The most likely cause of the performance issue is C. The use of Version 1 UUIDs as primary keys that increase monotonically. Here's why:\n\nVersion 1 UUIDs and Monotonicity: Version 1 UUIDs are generated based on timestamps and MAC addresses. When used as primary keys, they can lead to performance issues in Cloud Spanner due to their non-monotonic nature. As new UUIDs are generated, they are not guaranteed to be in a sequential order, which can cause fragmentation in the underlying storage and lead to slower queries."
      },
      {
        "date": "2024-07-18T00:04:00.000Z",
        "voteCount": 1,
        "content": "Let's analyze why the other options are less likely:\n\nA. The use of 64-bit numeric types for 32-bit numbers: While using larger data types than necessary can impact storage space, it's unlikely to significantly affect performance in a way that would cause non-linear scaling.\nB. The use of the STRING data type for arbitrary-precision values: Using the STRING data type for numeric values can impact performance, but it's not the primary cause of non-linear scaling. Cloud Spanner is optimized for numeric data types.\nD. The use of LIKE instead of STARTS_WITH keyword for parameterized SQL queries: While using LIKE can be less efficient than STARTS_WITH for certain queries, it's unlikely to cause a significant performance bottleneck that would prevent linear scaling."
      },
      {
        "date": "2024-03-07T21:53:00.000Z",
        "voteCount": 1,
        "content": "C. The use of Version 1 UUIDs as primary keys that increase monotonically.\n\nWhen designing schemas for Cloud Spanner, it is important to consider how the choice of primary keys can impact performance, especially under heavy load. Cloud Spanner splits data among servers based on the primary key values, so if the keys are monotonically increasing, as is the case with Version 1 UUIDs, new inserts are constantly added to the end of the table. This can create hotspots, where a single node receives a disproportionate amount of read and write requests, leading to performance bottlenecks and preventing linear scaling."
      },
      {
        "date": "2023-09-19T23:56:00.000Z",
        "voteCount": 1,
        "content": "C is the best option."
      },
      {
        "date": "2023-01-06T05:08:00.000Z",
        "voteCount": 2,
        "content": "In Cloud Spanner, the use of Version 1 UUIDs as primary keys that increase monotonically can cause performance issues because they are not evenly distributed. This can lead to hot regions, where a disproportionate number of requests are sent to a specific node or range of nodes, causing those nodes to become overloaded and leading to decreased performance. To improve performance, you should consider using primary keys that are more evenly distributed, such as hash-based keys or random integers."
      },
      {
        "date": "2023-01-06T05:09:00.000Z",
        "voteCount": 1,
        "content": "A, the use of 64-bit numeric types for 32-bit numbers, is not likely to cause performance issues in Cloud Spanner.\n\nB, the use of the STRING data type for arbitrary-precision values, is not likely to cause performance issues in Cloud Spanner.\n\nD, the use of LIKE instead of STARTS_WITH keyword for parameterized SQL queries, is not likely to cause performance issues in Cloud Spanner."
      },
      {
        "date": "2022-12-22T07:37:00.000Z",
        "voteCount": 1,
        "content": "C is the answer.\n\nhttps://cloud.google.com/spanner/docs/schema-design#primary-key-prevent-hotspots\nSchema design best practice #1: Do not choose a column whose value monotonically increases or decreases as the first key part for a high write rate table."
      },
      {
        "date": "2022-11-15T02:00:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/spanner/docs/schema-design#uuid_primary_key\nAns C"
      },
      {
        "date": "2022-08-19T23:58:00.000Z",
        "voteCount": 2,
        "content": "C is correct"
      },
      {
        "date": "2022-05-20T08:12:00.000Z",
        "voteCount": 2,
        "content": "C - Version 1 is not recommended."
      },
      {
        "date": "2022-04-02T11:34:00.000Z",
        "voteCount": 2,
        "content": "Community choice is C"
      },
      {
        "date": "2022-01-09T03:23:00.000Z",
        "voteCount": 3,
        "content": "I vote C"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 88,
    "url": "https://www.examtopics.com/discussions/google/view/69735-exam-professional-cloud-developer-topic-1-question-88/",
    "body": "You are developing an application that reads credit card data from a Pub/Sub subscription. You have written code and completed unit testing. You need to test the<br>Pub/Sub integration before deploying to Google Cloud. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a service to publish messages, and deploy the Pub/Sub emulator. Generate random content in the publishing service, and publish to the emulator.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a service to publish messages to your application. Collect the messages from Pub/Sub in production, and replay them through the publishing service.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a service to publish messages, and deploy the Pub/Sub emulator. Collect the messages from Pub/Sub in production, and publish them to the emulator.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a service to publish messages, and deploy the Pub/Sub emulator. Publish a standard set of testing messages from the publishing service to the emulator.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 6,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-01-09T03:23:00.000Z",
        "voteCount": 6,
        "content": "I vote D"
      },
      {
        "date": "2024-07-18T01:34:00.000Z",
        "voteCount": 1,
        "content": "The best answer is A. Create a service to publish messages and deploy the Pub/Sub emulator. Generate random content in the publishing service and publish to the emulator.\n\nHere's why:\n\nPub/Sub Emulator: The Pub/Sub emulator provides a local environment that mimics the behavior of the production Pub/Sub service. This allows you to test your application's Pub/Sub integration without needing to deploy to Google Cloud.\nTesting with Random Data: Generating random content in the publishing service ensures that your application can handle various data formats and scenarios. This helps identify potential issues that might not be caught by unit testing alone.\nControlled Environment: Using the emulator gives you a controlled environment where you can easily manipulate the messages being published, allowing you to test different scenarios and edge cases."
      },
      {
        "date": "2024-07-18T01:34:00.000Z",
        "voteCount": 1,
        "content": "Let's analyze why the other options are less suitable:\n\nB. Collect messages from production and replay: This approach is risky because it involves using real production data, which might contain sensitive information. It also doesn't provide a controlled environment for testing.\nC. Collect messages from production and publish to the emulator: This approach still involves using real production data, which can be problematic. It also doesn't allow for testing with random data or controlled scenarios.\nD. Publish a standard set of testing messages: While using a standard set of messages is helpful, it might not cover all possible scenarios. Generating random content provides a more comprehensive test."
      },
      {
        "date": "2023-11-21T01:16:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/pubsub/docs/emulator"
      },
      {
        "date": "2023-07-16T22:20:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2022-12-22T07:34:00.000Z",
        "voteCount": 1,
        "content": "D is the answer."
      },
      {
        "date": "2022-10-24T22:34:00.000Z",
        "voteCount": 1,
        "content": "Vote D"
      },
      {
        "date": "2022-08-19T23:59:00.000Z",
        "voteCount": 2,
        "content": "D  is correct"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 89,
    "url": "https://www.examtopics.com/discussions/google/view/69226-exam-professional-cloud-developer-topic-1-question-89/",
    "body": "You are designing an application that will subscribe to and receive messages from a single Pub/Sub topic and insert corresponding rows into a database. Your application runs on Linux and leverages preemptible virtual machines to reduce costs. You need to create a shutdown script that will initiate a graceful shutdown.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite a shutdown script that uses inter-process signals to notify the application process to disconnect from the database.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite a shutdown script that broadcasts a message to all signed-in users that the Compute Engine instance is going down and instructs them to save current work and sign out.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite a shutdown script that writes a file in a location that is being polled by the application once every five minutes. After the file is read, the application disconnects from the database.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite a shutdown script that publishes a message to the Pub/Sub topic announcing that a shutdown is in progress. After the application reads the message, it disconnects from the database."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 15,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-02-25T16:52:00.000Z",
        "voteCount": 7,
        "content": "IMO it should be A"
      },
      {
        "date": "2022-01-01T16:52:00.000Z",
        "voteCount": 5,
        "content": "I vote A"
      },
      {
        "date": "2024-07-18T01:50:00.000Z",
        "voteCount": 2,
        "content": "The best answer is A. Write a shutdown script that uses inter-process signals to notify the application process to disconnect from the database. Here's why:\n\nInter-process Signals: Signals are a standard mechanism in Unix-like systems for communicating between processes. They are a reliable and efficient way to notify the application process that a shutdown is imminent.\nGraceful Shutdown: Using signals allows the application to gracefully disconnect from the database, ensuring data integrity and preventing potential data loss. The application can handle the signal, perform necessary cleanup tasks, and then exit gracefully."
      },
      {
        "date": "2024-07-18T01:51:00.000Z",
        "voteCount": 2,
        "content": "Let's analyze why the other options are less suitable:\n\nB. Broadcast message to users: This approach is not a reliable way to initiate a graceful shutdown. It relies on user interaction and might not be effective if users are not actively using the application.\nC. Write a file for polling: This approach is less efficient and introduces a delay. The application needs to poll the file every five minutes, which can introduce latency and might not be timely enough for a graceful shutdown.\nD. Publish a message to the Pub/Sub topic: This approach is not ideal because it introduces additional complexity and relies on the application's ability to process messages from the same topic it's subscribing to. It also might not be a reliable way to ensure a timely shutdown."
      },
      {
        "date": "2024-03-07T22:08:00.000Z",
        "voteCount": 1,
        "content": "A. Write a shutdown script that uses inter-process signals to notify the application process to disconnect from the database.\n\nIn the scenario of using preemptible virtual machines for running an application that interacts with a database, a graceful shutdown is essential to ensure data consistency and prevent potential issues like incomplete transactions. A shutdown script utilizing inter-process signals is an efficient and direct way to manage this process."
      },
      {
        "date": "2023-11-21T01:57:00.000Z",
        "voteCount": 1,
        "content": "I vote A:\n- https://cloud.google.com/compute/docs/instances/preemptible#preemption\n- https://cloud.google.com/compute/docs/shutdownscript\n\nOption D is not good as we only have a SINGLE pub/sub topic that is also receiving other messages. I wouldn't rely on the new (shutdown) message to come through and be read by the app timely to disconnect from the db."
      },
      {
        "date": "2023-11-17T12:48:00.000Z",
        "voteCount": 1,
        "content": "Option D is a suitable approach for initiating a graceful shutdown in a scenario where the application needs to receive a notification to disconnect from the database before the virtual machine is preempted. Here's how the process works:"
      },
      {
        "date": "2023-11-17T12:49:00.000Z",
        "voteCount": 1,
        "content": "Shutdown Script: Write a shutdown script that is executed when the instance is being preempted.\n\nPublish a Message to Pub/Sub: In the shutdown script, publish a message to the Pub/Sub topic, indicating that a shutdown is in progress. This message serves as a notification to the application.\n\nApplication Subscription: The application subscribes to the Pub/Sub topic and continuously listens for incoming messages"
      },
      {
        "date": "2023-11-17T12:49:00.000Z",
        "voteCount": 1,
        "content": "Graceful Shutdown: When the application receives the shutdown message from Pub/Sub, it initiates a graceful shutdown, including disconnecting from the database."
      },
      {
        "date": "2023-11-24T11:00:00.000Z",
        "voteCount": 1,
        "content": "You have only one topic, so if there are multiple messages in the queue before the one announcing the disconnect, then a lot of time can pass before the retrival of the message"
      },
      {
        "date": "2023-09-20T00:06:00.000Z",
        "voteCount": 1,
        "content": "I will go with A"
      },
      {
        "date": "2023-08-25T05:29:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/compute/docs/instances/preemptible#preemption"
      },
      {
        "date": "2023-01-06T05:06:00.000Z",
        "voteCount": 2,
        "content": "It's A\nTo handle the preemption notice and initiate a graceful shutdown, you should write a shutdown script that uses inter-process signals to notify the application process to disconnect from the database. The application can then initiate a graceful shutdown by completing any in-progress tasks and disconnecting from the database, ensuring that data is not lost or corrupted during the shutdown process. This is the most reliable method for initiating a graceful shutdown in response to a preemption notice, as it allows the application to respond directly to the signal and initiate the shutdown process."
      },
      {
        "date": "2023-01-06T05:06:00.000Z",
        "voteCount": 1,
        "content": "Option B, broadcasting a message to signed-in users and instructing them to save current work and sign out, is not relevant to the shutdown process of the application. Option C, writing a file in a location that is being polled by the application, is not a reliable method for initiating a graceful shutdown because the application may not read the file in a timely manner. Option D, publishing a message to the Pub/Sub topic announcing that a shutdown is in progress, is not a reliable method for initiating a graceful shutdown because the application may not read the message in a timely manner."
      },
      {
        "date": "2022-08-19T23:59:00.000Z",
        "voteCount": 2,
        "content": "A is correct"
      },
      {
        "date": "2022-07-27T01:24:00.000Z",
        "voteCount": 1,
        "content": "it's A\nCompute Engine sends a preemption notice to the instance in the form of an ACPI G2 Soft Off signal. You can use a shutdown script to handle the preemption notice and complete cleanup actions before the instance stops.\nhttps://cloud.google.com/compute/docs/instances/preemptible#preemption"
      },
      {
        "date": "2022-05-17T08:10:00.000Z",
        "voteCount": 2,
        "content": "A seems correct, guys"
      },
      {
        "date": "2022-04-01T07:30:00.000Z",
        "voteCount": 2,
        "content": "Looks like D"
      },
      {
        "date": "2022-01-22T07:08:00.000Z",
        "voteCount": 1,
        "content": "C does not make sense, because you don't know when pub sub message will be consumed (there might other events be in the queue before), so I'll go with option A"
      },
      {
        "date": "2022-01-08T11:18:00.000Z",
        "voteCount": 2,
        "content": "According to me , it should be D"
      },
      {
        "date": "2022-01-08T11:20:00.000Z",
        "voteCount": 2,
        "content": "Because for preemptible instances the script should run within 30 seconds of the instance being shutdown or restarted, in this case a pub sub trigger would be faster to perform."
      },
      {
        "date": "2022-04-15T11:51:00.000Z",
        "voteCount": 2,
        "content": "The issue with option D is that we have a SINGLE PubSub topic that is used to send the rows to be inserted; sending a completely different message seems wrong."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 90,
    "url": "https://www.examtopics.com/discussions/google/view/69687-exam-professional-cloud-developer-topic-1-question-90/",
    "body": "You work for a web development team at a small startup. Your team is developing a Node.js application using Google Cloud services, including Cloud Storage and Cloud Build. The team uses a Git repository for version control. Your manager calls you over the weekend and instructs you to make an emergency update to one of the company's websites, and you're the only developer available. You need to access Google Cloud to make the update, but you don't have your work laptop. You are not allowed to store source code locally on a non-corporate computer. How should you set up your developer environment?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a text editor and the Git command line to send your source code updates as pull requests from a public computer.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a text editor and the Git command line to send your source code updates as pull requests from a virtual machine running on a public computer.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Shell and the built-in code editor for development. Send your source code updates as pull requests.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a Cloud Storage bucket to store the source code that you need to edit. Mount the bucket to a public computer as a drive, and use a code editor to update the code. Turn on versioning for the bucket, and point it to the team's Git repository."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-11-20T09:41:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2023-09-20T00:08:00.000Z",
        "voteCount": 1,
        "content": "C is correct."
      },
      {
        "date": "2022-12-21T09:03:00.000Z",
        "voteCount": 2,
        "content": "C is the answer.\n\nhttps://cloud.google.com/shell/docs\nloud Shell is an interactive shell environment for Google Cloud that lets you learn and experiment with Google Cloud and manage your projects and resources from your web browser.\n\nWith Cloud Shell, the Google Cloud CLI and other utilities you need are pre-installed, fully authenticated, up-to-date, and always available when you need them. Cloud Shell comes with a built-in code editor with an integrated Cloud Code experience, allowing you to develop, build, debug, and deploy your cloud-based apps entirely in the cloud."
      },
      {
        "date": "2022-08-19T23:59:00.000Z",
        "voteCount": 2,
        "content": "C is correct"
      },
      {
        "date": "2022-04-27T21:25:00.000Z",
        "voteCount": 1,
        "content": "Vote C"
      },
      {
        "date": "2022-02-25T16:59:00.000Z",
        "voteCount": 4,
        "content": "Cloud shell is the answer"
      },
      {
        "date": "2022-01-08T11:23:00.000Z",
        "voteCount": 4,
        "content": "C should be the correct answer because the source code should not be stores locally in the public pc."
      },
      {
        "date": "2022-01-21T05:19:00.000Z",
        "voteCount": 2,
        "content": "Yes agree about Option C \nhttps://cloud.google.com/shell/docs"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 91,
    "url": "https://www.examtopics.com/discussions/google/view/69688-exam-professional-cloud-developer-topic-1-question-91/",
    "body": "Your team develops services that run on Google Kubernetes Engine. You need to standardize their log data using Google-recommended practices and make the data more useful in the fewest number of steps. What should you do? (Choose two.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate aggregated exports on application logs to BigQuery to facilitate log analytics.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate aggregated exports on application logs to Cloud Storage to facilitate log analytics.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite log output to standard output (stdout) as single-line JSON to be ingested into Cloud Logging as structured logs.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMandate the use of the Logging API in the application code to write structured logs to Cloud Logging.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMandate the use of the Pub/Sub API to write structured data to Pub/Sub and create a Dataflow streaming pipeline to normalize logs and write them to BigQuery for analytics."
    ],
    "answer": "AC",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AC",
        "count": 20,
        "isMostVoted": true
      },
      {
        "answer": "CD",
        "count": 10,
        "isMostVoted": false
      },
      {
        "answer": "AD",
        "count": 8,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-01-22T06:20:00.000Z",
        "voteCount": 10,
        "content": "I go for A, C"
      },
      {
        "date": "2022-01-08T11:28:00.000Z",
        "voteCount": 9,
        "content": "C,D are the correct in this case."
      },
      {
        "date": "2024-07-18T05:29:00.000Z",
        "voteCount": 1,
        "content": "C. Single-line JSON to Cloud Logging: This is the most straightforward and efficient way to standardize logs. By writing logs as single-line JSON, you ensure consistent formatting and make it easy for Cloud Logging to parse and analyze the data. Cloud Logging automatically handles ingestion and storage.\nD. Logging API for Structured Logs: Using the Logging API directly allows for more control over log formatting and metadata. You can include specific labels, severity levels, and other information to make your logs more informative. This approach also ensures that logs are written directly to Cloud Logging, eliminating the need for additional processing steps."
      },
      {
        "date": "2024-07-18T05:29:00.000Z",
        "voteCount": 1,
        "content": "Why other options are less ideal:\n\nA. Aggregated Exports to BigQuery: While BigQuery is excellent for analytics, this approach requires additional steps to configure and manage exports. It's not the most efficient way to standardize logs initially.\nB. Aggregated Exports to Cloud Storage: Similar to option A, this adds complexity and requires additional processing to analyze the data in Cloud Storage.\nE. Pub/Sub and Dataflow: This is a more complex solution that involves multiple services and requires significant development effort. It's overkill for simply standardizing logs."
      },
      {
        "date": "2024-07-13T02:31:00.000Z",
        "voteCount": 1,
        "content": "Correct answer AC"
      },
      {
        "date": "2024-04-16T18:35:00.000Z",
        "voteCount": 1,
        "content": "A: Obvious\n\nC: https://cloud.google.com/kubernetes-engine/docs/concepts/about-logs#best_practices:~:text=Structured%20logging%3A%20The%20logging%20agent%20integrated%20with%20GKE%20will%20read%20JSON%20documents%20serialized%20to%20single%2Dline%20strings%20and%20written%20to%20standard%20output%20or%20standard%20error%20and%20will%20send%20them%20to%20Google%20Cloud%20Observability%20as%20structured%20log%20entries."
      },
      {
        "date": "2024-03-07T22:18:00.000Z",
        "voteCount": 3,
        "content": "C. Writing log output to standard output (stdout) as single-line JSON: This is a recommended practice for containerized applications running on Kubernetes. Kubernetes captures everything written to stdout and stderr and routes it to its logging agent (in this case, Cloud Logging in GKE). By structuring logs as single-line JSON, you enable Cloud Logging to ingest them as structured logs, which are more queryable and readable. This approach is efficient and does not require any changes in the application to use specific logging APIs.\n\nA. Create aggregated exports on application logs to BigQuery: Exporting logs to BigQuery allows for powerful analytics capabilities. BigQuery is well-suited for running fast, SQL-like queries on large datasets. By exporting logs to BigQuery, you can perform more complex analyses and gain deeper insights from your log data."
      },
      {
        "date": "2023-12-26T14:52:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/kubernetes-engine/docs/concepts/about-logs#best_practices"
      },
      {
        "date": "2023-12-22T13:54:00.000Z",
        "voteCount": 2,
        "content": "fewest steps + make log useful (analytics)"
      },
      {
        "date": "2023-12-26T14:52:00.000Z",
        "voteCount": 2,
        "content": "based on that link i change my answer to C and D"
      },
      {
        "date": "2023-12-26T14:52:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/kubernetes-engine/docs/concepts/about-logs#best_practices"
      },
      {
        "date": "2023-11-24T19:31:00.000Z",
        "voteCount": 2,
        "content": "in the fewest number of steps  --&gt; C"
      },
      {
        "date": "2023-11-21T13:15:00.000Z",
        "voteCount": 2,
        "content": "A &amp; C:\n\nOption A to \u201cmake the data more useful\u201d, as BigQuery will allow us to use big data analysis capabilities on the stored logs: https://cloud.google.com/logging/docs/export/aggregated_sinks#supported-destinations\n\nOption C to \u201cto standardize their log data\u201d creating structured logs: https://cloud.google.com/kubernetes-engine/docs/concepts/about-logs#best_practices\n\nOption D is also a viable solution but C is preferred, considering the \u201cfewest number of steps\u201d requirement.\n\nChoosing C and D together makes no sense, as both aim to achieve the same goal."
      },
      {
        "date": "2023-11-17T12:55:00.000Z",
        "voteCount": 2,
        "content": "Write log output to standard output (stdout) as single-line JSON:\n\nThis practice allows you to use structured logs, specifically in JSON format, making it easier to parse and analyze log data.\nCloud Logging can ingest logs from standard output, and structured logs enhance the usability of log data.\nMandate the use of the Logging API in the application code to write structured logs to Cloud Logging:\n\nUsing the Logging API allows your applications to send structured log data directly to Cloud Logging.\nStructured logs provide more context and are easier to filter, search, and analyze within Cloud Logging."
      },
      {
        "date": "2023-11-21T13:02:00.000Z",
        "voteCount": 1,
        "content": "Both options for the same purpose then? Why would one implement option C having implemented option D?"
      },
      {
        "date": "2023-09-20T00:14:00.000Z",
        "voteCount": 2,
        "content": "Option A: Create aggregated exports on application logs to BigQuery. This will facilitate log analytics by exporting application logs to BigQuery, which is a fully-managed, serverless data warehouse. BigQuery allows you to perform advanced analytics on your log data, including running complex queries and visualizing the results.\n\nOption C: Write log output to standard output (stdout) as single-line JSON to be ingested into Cloud Logging as structured logs. This approach involves writing log output to standard output in a specific format (single-line JSON) that can be easily ingested by Cloud Logging. By using structured logs, you can take advantage of advanced querying and filtering capabilities provided by Cloud Logging."
      },
      {
        "date": "2023-08-25T05:36:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/stackdriver/docs/solutions/gke/managing-logs#best_practices"
      },
      {
        "date": "2023-06-06T07:09:00.000Z",
        "voteCount": 1,
        "content": "CD. Only C and D mentioned Cloud Logging. Other options involve extra steps and won't come out free.\n\"When you create a new GKE cluster, Cloud Operations for GKE integration with Cloud Logging and Cloud Monitoring is enabled by default.\"\nhttps://cloud.google.com/stackdriver/docs/solutions/gke/managing-logs#:~:text=When%20you%20create%20a%20new%20GKE%20cluster%2C%20Cloud%20Operations%20for%20GKE%20integration%20with%20Cloud%20Logging%20and%20Cloud%20Monitoring%20is%20enabled%20by%20default."
      },
      {
        "date": "2023-05-02T17:49:00.000Z",
        "voteCount": 1,
        "content": "fewest number of steps A &amp;C"
      },
      {
        "date": "2023-02-25T06:51:00.000Z",
        "voteCount": 1,
        "content": "fewest number of steps -&gt; i believe this sentence is the key. option D would take take.\nalso: https://cloud.google.com/stackdriver/docs/solutions/gke/managing-logs#best_practices"
      },
      {
        "date": "2023-01-06T04:51:00.000Z",
        "voteCount": 2,
        "content": "Anser is C&amp;D\nTo standardize log data and make it more useful in the most efficient way, it is recommended to write log output to standard output (stdout) as single-line JSON to be ingested into Cloud Logging as structured logs. This method allows for easy and efficient ingestion of structured log data into Cloud Logging, which can then be easily queried and analyzed. Additionally, mandating the use of the Logging API in the application code allows for the writing of structured logs directly from the application code, improving the usability and reliability of the logs."
      },
      {
        "date": "2023-01-06T04:54:00.000Z",
        "voteCount": 2,
        "content": "A and B, which involve creating aggregated exports of log data to either BigQuery or Cloud Storage, are not necessary for standardizing log data. These options may be useful for storing and analyzing log data, but they are not necessary for standardizing the format of the log data. To standardize log data, it is sufficient to write log output to standard output (stdout) as single-line JSON, which can be ingested into Cloud Logging as structured logs."
      },
      {
        "date": "2023-01-06T04:55:00.000Z",
        "voteCount": 2,
        "content": "E, which involves using the Pub/Sub API and creating a Dataflow streaming pipeline to normalize logs and write them to BigQuery for analytics, is a more complex solution that requires more steps and is not necessary for standardizing log data. While this option may be useful for storing and analyzing log data, it is not necessary for standardizing the format of the log data. To standardize log data, it is sufficient to write log output to stdout and use the Logging API to write structured logs to Cloud Logging."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 92,
    "url": "https://www.examtopics.com/discussions/google/view/69738-exam-professional-cloud-developer-topic-1-question-92/",
    "body": "You are designing a deployment technique for your new applications on Google Cloud. As part of your deployment planning, you want to use live traffic to gather performance metrics for both new and existing applications. You need to test against the full production load prior to launch. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse canary deployment",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse blue/green deployment",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse rolling updates deployment",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse A/B testing with traffic mirroring during deployment\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 15,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-02-19T06:40:00.000Z",
        "voteCount": 8,
        "content": "D due to \"full production load\""
      },
      {
        "date": "2024-07-18T05:33:00.000Z",
        "voteCount": 1,
        "content": "A/B Testing with Traffic Mirroring: This approach allows you to send a portion of your live production traffic to both the new and existing versions of your application simultaneously. This provides a realistic test environment with real-world load, allowing you to gather performance metrics for both versions side-by-side. Traffic mirroring ensures that the new version is tested under the same conditions as the existing version, providing a more accurate comparison."
      },
      {
        "date": "2024-07-18T05:33:00.000Z",
        "voteCount": 1,
        "content": "A. Canary Deployment: While canary deployment is useful for gradual rollouts and testing, it primarily focuses on testing the new version in a limited environment. It doesn't necessarily provide a full production load test.\nB. Blue/Green Deployment: Blue/green deployment involves switching traffic between two identical environments. While it allows for zero downtime deployments, it doesn't provide a way to test both versions simultaneously with live traffic.\nC. Rolling Updates Deployment: Rolling updates gradually replace instances with new versions. This approach doesn't offer a way to test both versions with live traffic simultaneously."
      },
      {
        "date": "2023-11-20T09:51:00.000Z",
        "voteCount": 2,
        "content": "Canary will only redirect a small portion of the traffic, while A/B with mirroring will test the new version in full load"
      },
      {
        "date": "2023-09-20T00:19:00.000Z",
        "voteCount": 2,
        "content": "A/B testing with traffic mirroring during deployment. This technique allows you to divert a portion of the live production traffic to the new application version while still serving the majority of the traffic to the existing version. By comparing the performance metrics of both versions under real-world conditions, you can assess the impact of the new deployment on your application\u2019s performance and stability."
      },
      {
        "date": "2023-02-01T06:17:00.000Z",
        "voteCount": 3,
        "content": "\"You need to test against the full production load prior to launch\" It's impossible with canary. \n\"A/B testing with traffic mirroring during deployment\" is the only one possibility we have to test the entire traffic before the roll out."
      },
      {
        "date": "2022-12-21T08:56:00.000Z",
        "voteCount": 1,
        "content": "D is the answer."
      },
      {
        "date": "2022-10-01T20:32:00.000Z",
        "voteCount": 1,
        "content": "D, the question requires more than just \"a load\" rather the \"full load\" the only strategy where this happens is A/B testing"
      },
      {
        "date": "2022-08-20T00:00:00.000Z",
        "voteCount": 2,
        "content": "D is correct"
      },
      {
        "date": "2022-08-04T23:05:00.000Z",
        "voteCount": 1,
        "content": "After giving a deliberate thought, I think it's option D. \nThe keyword here is 'gather performance metrics,' and as everyone must know A/B testing's whole purpose is to gather performance metrics."
      },
      {
        "date": "2022-07-11T02:15:00.000Z",
        "voteCount": 2,
        "content": "Correct Answer is Shadow test pattern, as it is not in the option selected D  \nhttps://cloud.google.com/architecture/application-deployment-and-testing-strategies#shadow_test_pattern"
      },
      {
        "date": "2022-05-17T16:59:00.000Z",
        "voteCount": 2,
        "content": "D is my answer"
      },
      {
        "date": "2022-04-27T22:45:00.000Z",
        "voteCount": 1,
        "content": "Vote A"
      },
      {
        "date": "2022-04-01T07:33:00.000Z",
        "voteCount": 1,
        "content": "Full production - A/B testing, option D"
      },
      {
        "date": "2022-03-25T18:13:00.000Z",
        "voteCount": 2,
        "content": "Canary can test live production traffic on production (Answer A) https://cloud.google.com/architecture/application-deployment-and-testing-strategies#key_benefits_4"
      },
      {
        "date": "2022-06-07T01:34:00.000Z",
        "voteCount": 1,
        "content": "is it possible that right answer is not present. To me the answer is \"Shadow Test Pattern\" https://cloud.google.com/architecture/application-deployment-and-testing-strategies#shadow_test_pattern. Do you agree ?"
      },
      {
        "date": "2022-03-01T20:03:00.000Z",
        "voteCount": 2,
        "content": "Correct answer is A:\n\nCanary deployment is a technique to reduce the risk of introducing a software update in production by slowly rolling out the change to a small subset of users before making it available to everybody. \n\nThis deployment technique is one where the SRE of an application development team relies on a router or load balancer to target individual routes. They target a small fragment of the overall user base with the newer version of the application. Once this new set of users are have used the application important metrics will be collected and analyzed to decide whether the new update is good for a full scale rolled to all the users or whether it needs to be rolled back for further troubleshooting."
      },
      {
        "date": "2022-02-25T17:13:00.000Z",
        "voteCount": 3,
        "content": "you want to use live traffic to gather performance metrics for both new and existing applications"
      },
      {
        "date": "2022-01-21T05:27:00.000Z",
        "voteCount": 1,
        "content": "Agree with Option A"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 93,
    "url": "https://www.examtopics.com/discussions/google/view/69739-exam-professional-cloud-developer-topic-1-question-93/",
    "body": "You support an application that uses the Cloud Storage API. You review the logs and discover multiple HTTP 503 Service Unavailable error responses from the<br>API. Your application logs the error and does not take any further action. You want to implement Google-recommended retry logic to improve success rates.<br>Which approach should you take?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRetry the failures in batch after a set number of failures is logged.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRetry each failure at a set time interval up to a maximum number of times.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRetry each failure at increasing time intervals up to a maximum number of tries.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRetry each failure at decreasing time intervals up to a maximum number of tries."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-11-20T09:52:00.000Z",
        "voteCount": 1,
        "content": "Exponential backoff with limit"
      },
      {
        "date": "2023-02-25T06:54:00.000Z",
        "voteCount": 1,
        "content": "exponential backoff algorithm retries"
      },
      {
        "date": "2022-12-21T08:53:00.000Z",
        "voteCount": 1,
        "content": "C is the answer.\n\nhttps://cloud.google.com/storage/docs/retry-strategy#exponential-backoff\nTruncated exponential backoff is a standard error handling strategy for network applications in which a client periodically retries a failed request with increasing delays between requests.\n\nAn exponential backoff algorithm retries requests exponentially, increasing the waiting time between retries up to a maximum backoff time."
      },
      {
        "date": "2022-08-20T00:01:00.000Z",
        "voteCount": 2,
        "content": "C is correct"
      },
      {
        "date": "2022-01-21T05:28:00.000Z",
        "voteCount": 4,
        "content": "Agree with Option C"
      },
      {
        "date": "2022-01-09T03:35:00.000Z",
        "voteCount": 3,
        "content": "I vote C\nhttps://cloud.google.com/storage/docs/retry-strategy"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 94,
    "url": "https://www.examtopics.com/discussions/google/view/69715-exam-professional-cloud-developer-topic-1-question-94/",
    "body": "You need to redesign the ingestion of audit events from your authentication service to allow it to handle a large increase in traffic. Currently, the audit service and the authentication system run in the same Compute Engine virtual machine. You plan to use the following Google Cloud tools in the new architecture:<br>\u2711 Multiple Compute Engine machines, each running an instance of the authentication service<br>\u2711 Multiple Compute Engine machines, each running an instance of the audit service<br>\u2711 Pub/Sub to send the events from the authentication services.<br>How should you set up the topics and subscriptions to ensure that the system can handle a large volume of messages and can scale efficiently?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate one Pub/Sub topic. Create one pull subscription to allow the audit services to share the messages.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate one Pub/Sub topic. Create one pull subscription per audit service instance to allow the services to share the messages.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate one Pub/Sub topic. Create one push subscription with the endpoint pointing to a load balancer in front of the audit services.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate one Pub/Sub topic per authentication service. Create one pull subscription per topic to be used by one audit service.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate one Pub/Sub topic per authentication service. Create one push subscription per topic, with the endpoint pointing to one audit service."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 11,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "E",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-01-25T08:29:00.000Z",
        "voteCount": 12,
        "content": "https://cloud.google.com/pubsub/docs/subscriber\n\"Multiple subscribers can make pull calls to the same \"shared\" subscription. Each subscriber will receive a subset of the messages.\"\nResponse is A.\nWith C and D you can't scale efficiently, because you have to create a topic for each new instance of the authentication service."
      },
      {
        "date": "2022-08-04T23:21:00.000Z",
        "voteCount": 1,
        "content": "This seems to be a smart answer and follows the logic with which I was thinking."
      },
      {
        "date": "2023-01-13T06:10:00.000Z",
        "voteCount": 5,
        "content": "A is correct. This is the most flexible way to scale, allowing the authentication and audit services to be sized independently according to load.\nB is incorrect. This will cause messages to be duplicated, one copy per subscription.\nC is incorrect. This will allow the system to scale, but push subscriptions are less suited to handle large volumes of messages.\nD is incorrect. This will allow the system to scale, however each audit service will listen to all subscriptions.\nE. is incorrect. This will allow the system to scale, however it will require each audit service to listen to all subscriptions. Also push subscriptions are less suited to handle large volumes of messages."
      },
      {
        "date": "2024-07-18T21:35:00.000Z",
        "voteCount": 1,
        "content": "The best approach here is C. Create one Pub/Sub topic. Create one push subscription with the endpoint pointing to a load balancer in front of the audit services.\n\nHere's why:\n\nScalability: A single Pub/Sub topic allows you to centralize all audit events. This makes it easy to scale the authentication services by adding more instances without needing to change the Pub/Sub configuration.\nLoad Balancing: Using a push subscription with a load balancer in front of the audit services ensures that the incoming messages are distributed evenly across the available audit service instances. This prevents any single instance from becoming overloaded.\nEfficiency: Push subscriptions are more efficient than pull subscriptions for high-volume scenarios. This is because the Pub/Sub service actively pushes messages to the subscribers, reducing the need for the audit services to constantly poll for new messages."
      },
      {
        "date": "2024-07-18T21:35:00.000Z",
        "voteCount": 1,
        "content": "Why other options are less ideal:\n\nA. Create one Pub/Sub topic. Create one pull subscription to allow the audit services to share the messages. This would create a bottleneck at the single pull subscription, as all audit services would need to compete for messages.\nB. Create one Pub/Sub topic. Create one pull subscription per audit service instance to allow the services to share the messages. This would be less efficient than push subscriptions and could lead to uneven message distribution.\nD. Create one Pub/Sub topic per authentication service. Create one pull subscription per topic to be used by one audit service. This would require a lot of topics and subscriptions, making the system more complex to manage.\nE. Create one Pub/Sub topic per authentication service. Create one push subscription per topic, with the endpoint pointing to one audit service. This would create a one-to-one relationship between authentication services and audit services, which is not scalable."
      },
      {
        "date": "2023-11-20T09:57:00.000Z",
        "voteCount": 1,
        "content": "Most simple and efficient one is A"
      },
      {
        "date": "2023-11-17T13:02:00.000Z",
        "voteCount": 1,
        "content": "Option E is a more scalable and efficient solution for handling a large volume of messages and scaling efficiently."
      },
      {
        "date": "2023-09-20T00:27:00.000Z",
        "voteCount": 1,
        "content": "I would go with A."
      },
      {
        "date": "2023-09-04T18:26:00.000Z",
        "voteCount": 1,
        "content": "In my opinion, Option C would be the most efficient way to handle the scenario. Here's why:\n\nSingle Topic: Having one Pub/Sub topic keeps things simpler and allows all authentication service instances to publish to the same topic.\n\nPush Subscription with Load Balancer: This allows incoming messages to be distributed among all available audit service instances. The load balancer would handle distributing the load, making it easier for the audit service to scale out as needed.\n\nOption C ensures both scalability and efficient handling of a large volume of messages."
      },
      {
        "date": "2023-05-08T09:52:00.000Z",
        "voteCount": 1,
        "content": "i go for A + custom pubsub metric on autoscale"
      },
      {
        "date": "2023-01-06T04:43:00.000Z",
        "voteCount": 1,
        "content": "Answer is E, in which there is one topic per authentication service and one push subscription per topic, with the endpoint pointing to one audit service, is a better option because it allows the audit services to scale horizontally to handle a large volume of messages, and it allows the messages to be processed in parallel. Each authentication service will send messages directly to its own topic, which will be handled by a specific audit service. This will ensure that the system can scale horizontally to handle a large volume of messages, and it will also allow the audit services to process the messages in parallel."
      },
      {
        "date": "2023-01-06T04:44:00.000Z",
        "voteCount": 1,
        "content": "A, in which there is only one topic and one pull subscription, would not allow the audit services to scale horizontally to handle a large volume of messages, as they would all be pulling messages from the same subscription. If the volume of messages increased, the audit services would not be able to process them all in a timely manner, as they would be competing for messages from the same subscription.\n\nB, in which there is only one topic and one pull subscription per audit service, would also not allow the audit services to scale horizontally, as they would all be pulling messages from the same topic"
      },
      {
        "date": "2023-01-06T04:45:00.000Z",
        "voteCount": 1,
        "content": "C, in which there is only one topic and one push subscription with a load balancer endpoint, would not allow the audit services to scale horizontally to handle a large volume of messages. The messages would all be sent to the same endpoint, which would be handled by the load balancer. If the volume of messages increased, the load balancer would not be able to distribute the messages to the audit services in a timely manner, as it would have to process all of the messages itself before forwarding them to the audit services. This could lead to bottlenecks if the volume of messages increased."
      },
      {
        "date": "2023-01-06T04:45:00.000Z",
        "voteCount": 1,
        "content": "D, in which there is one topic per authentication service and one pull subscription per topic, you may encounter issues with scaling efficiently as the number of authentication service instances increases. This is because you would have to create a new topic for each new instance of the authentication service, and each audit service would have to pull messages from a different topic. This would not allow the audit services to process the messages in parallel, as each audit service would be pulling messages from a different topic and processing them sequentially. This could lead to bottlenecks if the volume of messages increased."
      },
      {
        "date": "2022-12-21T08:51:00.000Z",
        "voteCount": 1,
        "content": "A is the answer."
      },
      {
        "date": "2022-08-20T00:01:00.000Z",
        "voteCount": 2,
        "content": "A is correct I think"
      },
      {
        "date": "2022-01-21T05:31:00.000Z",
        "voteCount": 1,
        "content": "While this can be between C and D , I would go with Option D considering the large volume mentioned in question"
      },
      {
        "date": "2022-01-09T03:47:00.000Z",
        "voteCount": 2,
        "content": "I vote C"
      },
      {
        "date": "2022-01-08T22:42:00.000Z",
        "voteCount": 2,
        "content": "Agree with D"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 95,
    "url": "https://www.examtopics.com/discussions/google/view/69794-exam-professional-cloud-developer-topic-1-question-95/",
    "body": "You are developing a marquee stateless web application that will run on Google Cloud. The rate of the incoming user traffic is expected to be unpredictable, with no traffic on some days and large spikes on other days. You need the application to automatically scale up and down, and you need to minimize the cost associated with running the application. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBuild the application in Python with Firestore as the database. Deploy the application to Cloud Run.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBuild the application in C# with Firestore as the database. Deploy the application to App Engine flexible environment.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBuild the application in Python with CloudSQL as the database. Deploy the application to App Engine standard environment.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBuild the application in Python with Firestore as the database. Deploy the application to a Compute Engine managed instance group with autoscaling."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 15,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-01-22T05:42:00.000Z",
        "voteCount": 11,
        "content": "Why C? I chose option A because of the DB option.\n\nboth Cloud run (A) and App Engine Standard (C)  can scale to zero, so we need to find out the correct DB Firestore vs CloudSQL \nsince we don\u2019t know any details if data structures require relational or noSQL, I\u2019d go for Firestore because it is more flexible in scalability than CloudSQL, and also you only pay per storage usage + operations"
      },
      {
        "date": "2023-04-20T13:21:00.000Z",
        "voteCount": 1,
        "content": "\"you need to minimize the cost associated with running the application\"\ncloud run is cheaper, with 0.10 time granularity"
      },
      {
        "date": "2022-02-25T20:23:00.000Z",
        "voteCount": 6,
        "content": "I agree with A as it is the only one fits for scale up and down ="
      },
      {
        "date": "2024-04-16T19:35:00.000Z",
        "voteCount": 1,
        "content": "A: Building the application in Python with Firestore as the database and deploying the application to Cloud Run is a good approach. Cloud Run is designed to scale up and down automatically, even down to zero, which can help minimize costs when there's no traffic. Firestore is a serverless, NoSQL document database that can scale automatically to meet your application's needs.\n\nWhy C is rejected?\nC: While App Engine standard environment can scale down to zero instances, CloudSQL is not serverless and you are billed for the time that the database instance is running, which could increase costs when compared to Firestore."
      },
      {
        "date": "2023-11-20T09:59:00.000Z",
        "voteCount": 1,
        "content": "Also Firestore has a free tier quota"
      },
      {
        "date": "2023-11-17T13:04:00.000Z",
        "voteCount": 1,
        "content": "Option A is a suitable choice for building a stateless web application with unpredictable traffic, aiming to automatically scale up and down while minimizing costs"
      },
      {
        "date": "2023-09-20T00:28:00.000Z",
        "voteCount": 1,
        "content": "A is best suited here."
      },
      {
        "date": "2023-02-04T05:06:00.000Z",
        "voteCount": 1,
        "content": "Both Cloud Run and App Engine Standard Environment allow scaling to zero (which minimize the cost), but Cloud SQL can't be minimized to zero while firestore is measured based on CPU usage.\nSo from the cost point of view, A is the answer"
      },
      {
        "date": "2023-01-06T04:30:00.000Z",
        "voteCount": 2,
        "content": "Answer is A: To minimize the cost of running the application and to allow it to automatically scale up and down based on incoming traffic, you should build the application in Python with Firestore as the database, and deploy it to Cloud Run. \n\nB and C, which involve deploying the application to App Engine, may also allow the application to automatically scale, but they may not be as cost-effective as Cloud Run. Option D, which involves deploying the application to a Compute Engine managed instance group, would allow the application to automatically scale, but it would not be as cost-effective as Cloud Run, as you would have to pay for the resources that you use even when there is no traffic."
      },
      {
        "date": "2022-12-21T08:19:00.000Z",
        "voteCount": 1,
        "content": "A is the answer."
      },
      {
        "date": "2022-08-20T00:02:00.000Z",
        "voteCount": 2,
        "content": "A is correct"
      },
      {
        "date": "2022-08-19T18:35:00.000Z",
        "voteCount": 1,
        "content": "A because of \"stateless web app\""
      },
      {
        "date": "2022-03-25T18:23:00.000Z",
        "voteCount": 1,
        "content": "It's simple, we need a stateless web app (not relational DB as Cloud SQL), and sometimes it scales down to zero utilization (only GAE flex &amp; Cloud Run can do this). I'll go with option A as well."
      },
      {
        "date": "2022-03-25T18:24:00.000Z",
        "voteCount": 1,
        "content": "And if you were wondering why not B, C# is not a supported language for GAE"
      },
      {
        "date": "2023-11-13T13:31:00.000Z",
        "voteCount": 1,
        "content": "GAE flex don't scale to zero"
      },
      {
        "date": "2022-03-01T20:13:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer is A;\nThey are talking about minimize cost, with Cloud SQL isn't cheaper than Firestore."
      },
      {
        "date": "2022-01-21T20:48:00.000Z",
        "voteCount": 2,
        "content": "Agree with Option C"
      },
      {
        "date": "2022-01-09T20:37:00.000Z",
        "voteCount": 1,
        "content": "I vote C"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 96,
    "url": "https://www.examtopics.com/discussions/google/view/69716-exam-professional-cloud-developer-topic-1-question-96/",
    "body": "You have written a Cloud Function that accesses other Google Cloud resources. You want to secure the environment using the principle of least privilege. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new service account that has Editor authority to access the resources. The deployer is given permission to get the access token.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new service account that has a custom IAM role to access the resources. The deployer is given permission to get the access token.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new service account that has Editor authority to access the resources. The deployer is given permission to act as the new service account.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new service account that has a custom IAM role to access the resources. The deployer is given permission to act as the new service account.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 8,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-01-08T23:04:00.000Z",
        "voteCount": 7,
        "content": "Agree with D"
      },
      {
        "date": "2024-04-16T19:39:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/functions/docs/securing/function-identity#individual:~:text=In%20order%20to,you%20this%20permission"
      },
      {
        "date": "2023-12-26T21:44:00.000Z",
        "voteCount": 2,
        "content": "Quoted from https://cloud.google.com/functions/docs/securing/function-identity#individual \n\"In order to deploy a function with a user-managed service account, the deployer must have the iam.serviceAccounts.actAs permission on the service account being deployed\""
      },
      {
        "date": "2023-11-22T02:00:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/functions/docs/securing/function-identity#individual"
      },
      {
        "date": "2023-09-20T02:25:00.000Z",
        "voteCount": 1,
        "content": "This approach allows you to create a service account with a custom IAM role that provides only the necessary permissions required by your Cloud Function. By granting the deployer permission to get the access token, you ensure that they can obtain the necessary credentials to deploy and manage the Cloud Function."
      },
      {
        "date": "2022-12-21T08:17:00.000Z",
        "voteCount": 2,
        "content": "D is the answer.\n\nhttps://cloud.google.com/functions/docs/securing/function-identity#per-function_identity"
      },
      {
        "date": "2022-08-20T00:02:00.000Z",
        "voteCount": 2,
        "content": "D is correct"
      },
      {
        "date": "2022-08-05T01:58:00.000Z",
        "voteCount": 1,
        "content": "D should be the correct choice here.\nIn Google Cloud, the resource(which can be a Cloud Function, a VM, etc.) always acts as a service account while accessing other resources."
      },
      {
        "date": "2022-05-27T00:37:00.000Z",
        "voteCount": 1,
        "content": "What 'deployer' means here? The function itself? or the user who set up the function?"
      },
      {
        "date": "2022-05-28T06:38:00.000Z",
        "voteCount": 1,
        "content": "B. \nhttps://cloud.google.com/functions/docs/securing/authenticating"
      },
      {
        "date": "2022-05-29T00:49:00.000Z",
        "voteCount": 4,
        "content": "Changed the mind to D. (the note above is when you *invoke* the function, not to access other GCP services). \nhttps://cloud.google.com/functions/docs/securing/function-identity\n\"While IAM-defined service accounts are the preferred method for managing access in Google Cloud, some services might require other modes, such as an API key, OAuth 2.0 client, or service account key.\" \nand \n\"Note: In order to deploy a function with a user-managed service account, the deployer must have the iam.serviceAccounts.actAs permission on the service account being deployed.\""
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 97,
    "url": "https://www.examtopics.com/discussions/google/view/70391-exam-professional-cloud-developer-topic-1-question-97/",
    "body": "You are a SaaS provider deploying dedicated blogging software to customers in your Google Kubernetes Engine (GKE) cluster. You want to configure a secure multi-tenant platform to ensure that each customer has access to only their own blog and can't affect the workloads of other customers. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Application-layer Secrets on the GKE cluster to protect the cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy a namespace per tenant and use Network Policies in each blog deployment.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse GKE Audit Logging to identify malicious containers and delete them on discovery.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBuild a custom image of the blogging software and use Binary Authorization to prevent untrusted image deployments."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-01-21T20:05:00.000Z",
        "voteCount": 7,
        "content": "Option B is correct\nhttps://cloud.google.com/kubernetes-engine/docs/concepts/multitenancy-overview"
      },
      {
        "date": "2023-09-20T02:27:00.000Z",
        "voteCount": 2,
        "content": "This approach involves creating a separate namespace for each customer (tenant) and using Network Policies to enforce isolation between the namespaces. By deploying a namespace per tenant, you can ensure that each customer has access only to their own blog and cannot affect the workloads of other customers."
      },
      {
        "date": "2023-08-25T05:53:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/kubernetes-engine/docs/concepts/multitenancy-overview"
      },
      {
        "date": "2022-12-21T08:13:00.000Z",
        "voteCount": 1,
        "content": "B is the answer.\n\nhttps://cloud.google.com/kubernetes-engine/docs/concepts/multitenancy-overview#what_is_multi-tenancy\nAlthough Kubernetes cannot guarantee perfectly secure isolation between tenants, it does offer features that may be sufficient for specific use cases. You can separate each tenant and their Kubernetes resources into their own namespaces. You can then use policies to enforce tenant isolation. Policies are usually scoped by namespace and can be used to restrict API access, to constrain resource usage, and to restrict what containers are allowed to do."
      },
      {
        "date": "2022-11-14T23:30:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/kubernetes-engine/docs/concepts/multitenancy-overview#:~:text=For%20example%2C%20a,the%20cluster%20operates."
      },
      {
        "date": "2022-11-14T01:08:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/kubernetes-engine/docs/concepts/multitenancy-overview#network_policies\nAnswer B"
      },
      {
        "date": "2022-08-20T00:03:00.000Z",
        "voteCount": 2,
        "content": "B is correct"
      },
      {
        "date": "2022-05-17T17:12:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 98,
    "url": "https://www.examtopics.com/discussions/google/view/69717-exam-professional-cloud-developer-topic-1-question-98/",
    "body": "You have decided to migrate your Compute Engine application to Google Kubernetes Engine. You need to build a container image and push it to Artifact Registry using Cloud Build. What should you do? (Choose two.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun gcloud builds submit in the directory that contains the application source code.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun gcloud run deploy app-name --image gcr.io/$PROJECT_ID/app-name in the directory that contains the application source code.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun gcloud container images add-tag gcr.io/$PROJECT_ID/app-name gcr.io/$PROJECT_ID/app-name:latest in the directory that contains the application source code.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the application source directory, create a file named cloudbuild.yaml that contains the following contents: <img src=\"/assets/media/exam-media/04137/0006800001.png\" class=\"in-exam-image\">\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the application source directory, create a file named cloudbuild.yaml that contains the following contents: <img src=\"/assets/media/exam-media/04137/0006900001.png\" class=\"in-exam-image\">"
    ],
    "answer": "AD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AD",
        "count": 13,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-09-20T02:32:00.000Z",
        "voteCount": 1,
        "content": "I will go with AD."
      },
      {
        "date": "2023-01-06T03:46:00.000Z",
        "voteCount": 2,
        "content": "To build a container image and push it to Artifact Registry using Cloud Build, you should:\n\nRun gcloud builds submit in the directory that contains the application source code. This command will trigger Cloud Build to build the container image and push it to Artifact Registry.\n\nIn the application source directory, create a file named cloudbuild.yaml that contains the instructions for building and pushing the container image. The file should contain the following steps:\n\nsteps:\n-name: 'grc.io/cloud-builders/docker'\n args: ['build','-t','grc.io/$PROJECT_ID','app-name','.']\n-name: 'grc.io/cloud-builders/docker'\n args: ['push','grc.io/$PROJECT_ID/app-name']\n\nThis file will be used by Cloud Build to build and push the container image."
      },
      {
        "date": "2023-01-06T10:34:00.000Z",
        "voteCount": 2,
        "content": "B is incorrect because it uses the gcloud run deploy command, which is used to deploy a container image to Cloud Run, not to Artifact Registry.\n\nC is incorrect because it uses the gcloud container images add-tag command, which is used to add a tag to an existing container image in Container Registry, not to build and push a new container image to Artifact Registry.\n\nE is incorrect because it uses the gcloud app deploy command, which is used to deploy an application to App Engine, not to build and push a container image to Artifact Registry."
      },
      {
        "date": "2022-12-21T08:10:00.000Z",
        "voteCount": 2,
        "content": "AD is the answer.\n\nhttps://cloud.google.com/build/docs/building/build-containers#store-images"
      },
      {
        "date": "2022-08-20T00:03:00.000Z",
        "voteCount": 2,
        "content": "AD are correct"
      },
      {
        "date": "2022-08-14T19:10:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/build/docs/building/build-containers"
      },
      {
        "date": "2022-08-05T02:14:00.000Z",
        "voteCount": 1,
        "content": "Yup, it's A and D"
      },
      {
        "date": "2022-05-26T10:46:00.000Z",
        "voteCount": 2,
        "content": "agree with AD"
      },
      {
        "date": "2022-01-22T05:21:00.000Z",
        "voteCount": 3,
        "content": "A to submit cloud build https://cloud.google.com/sdk/gcloud/reference/builds/submit\nD describes the cloud build steps (docker build + push)"
      },
      {
        "date": "2022-01-22T04:52:00.000Z",
        "voteCount": 1,
        "content": "did anybody notice that D) has container URLs \"gcr.io\", which is container registry, not artifact registry?\nnot saying that there isn't a better alternative though"
      },
      {
        "date": "2022-01-22T04:58:00.000Z",
        "voteCount": 1,
        "content": "ok seems it is now possible to use such domains (preview feature)\nhttps://cloud.google.com/artifact-registry/docs/transition/setup-gcr-repo"
      },
      {
        "date": "2022-01-21T20:06:00.000Z",
        "voteCount": 1,
        "content": "Agree with Option B and D"
      },
      {
        "date": "2022-01-23T07:46:00.000Z",
        "voteCount": 2,
        "content": "On further analysis Option A seems better than B as per this site . Correct options are A and D.\nhttps://cloud.google.com/artifact-registry/docs/configure-cloud-build"
      },
      {
        "date": "2022-01-09T04:57:00.000Z",
        "voteCount": 3,
        "content": "I vote A, D"
      },
      {
        "date": "2022-01-08T23:19:00.000Z",
        "voteCount": 1,
        "content": "D is correct, but I am confused between B and C, because question says about creating the container using cloud build, while option B states about the cloud run deployment."
      },
      {
        "date": "2022-01-08T23:20:00.000Z",
        "voteCount": 1,
        "content": "B seems more correct. B and D are more suitable"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 99,
    "url": "https://www.examtopics.com/discussions/google/view/69718-exam-professional-cloud-developer-topic-1-question-99/",
    "body": "You are developing an internal application that will allow employees to organize community events within your company. You deployed your application on a single Compute Engine instance. Your company uses Google Workspace (formerly G Suite), and you need to ensure that the company employees can authenticate to the application from anywhere. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a public IP address to your instance, and restrict access to the instance using firewall rules. Allow your company's proxy as the only source IP address.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd an HTTP(S) load balancer in front of the instance, and set up Identity-Aware Proxy (IAP). Configure the IAP settings to allow your company domain to access the website.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up a VPN tunnel between your company network and your instance's VPC location on Google Cloud. Configure the required firewall rules and routing information to both the on-premises and Google Cloud networks.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a public IP address to your instance, and allow traffic from the internet. Generate a random hash, and create a subdomain that includes this hash and points to your instance. Distribute this DNS address to your company's employees."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-09-20T02:35:00.000Z",
        "voteCount": 1,
        "content": "B is correct."
      },
      {
        "date": "2022-12-21T08:07:00.000Z",
        "voteCount": 3,
        "content": "B is the answer.\n\nhttps://cloud.google.com/iap/docs/concepts-overview#how_iap_works\nWhen an application or resource is protected by IAP, it can only be accessed through the proxy by principals, also known as users, who have the correct Identity and Access Management (IAM) role. When you grant a user access to an application or resource by IAP, they're subject to the fine-grained access controls implemented by the product in use without requiring a VPN. When a user tries to access an IAP-secured resource, IAP performs authentication and authorization checks."
      },
      {
        "date": "2022-08-20T00:03:00.000Z",
        "voteCount": 2,
        "content": "B is correct"
      },
      {
        "date": "2022-01-21T20:08:00.000Z",
        "voteCount": 3,
        "content": "Option B is the best"
      },
      {
        "date": "2022-01-09T05:07:00.000Z",
        "voteCount": 3,
        "content": "I vote B\nhttps://cloud.google.com/blog/topics/developers-practitioners/control-access-your-web-sites-identity-aware-proxy"
      },
      {
        "date": "2022-01-08T23:25:00.000Z",
        "voteCount": 2,
        "content": "B is more suitable answer, because the question states that the employee should access the application from everywhere, IAP will allow your access depending on your workspace credentials."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 100,
    "url": "https://www.examtopics.com/discussions/google/view/69719-exam-professional-cloud-developer-topic-1-question-100/",
    "body": "Your development team is using Cloud Build to promote a Node.js application built on App Engine from your staging environment to production. The application relies on several directories of photos stored in a Cloud Storage bucket named webphotos-staging in the staging environment. After the promotion, these photos must be available in a Cloud Storage bucket named webphotos-prod in the production environment. You want to automate the process where possible. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tManually copy the photos to webphotos-prod.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a startup script in the application's app.yami file to move the photos from webphotos-staging to webphotos-prod.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a build step in the cloudbuild.yaml file before the promotion step with the arguments: <img src=\"/assets/media/exam-media/04137/0007000001.png\" class=\"in-exam-image\">\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a build step in the cloudbuild.yaml file before the promotion step with the arguments: <img src=\"/assets/media/exam-media/04137/0007000002.png\" class=\"in-exam-image\">"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-01-06T03:38:00.000Z",
        "voteCount": 4,
        "content": "C.Add a build step in the cloudbuild.yaml file before the promotion step with the arguments:\n-name: gcr.io/cloud-builders/gsutil\nargs: ['cp','-r','gs://webphotos-staging','gs://webphotos-prod']\nwaitFor: ['-']\n\nYou should add a build step in the cloudbuild.yaml file before the promotion step with the arguments shown above. This build step will use the gsutil tool to copy the photos from the webphotos-staging bucket to the webphotos-prod bucket. The -r flag tells gsutil to copy all files in the bucket recursively, and the waitFor parameter tells Cloud Build to wait for this step to complete before continuing with the promotion step."
      },
      {
        "date": "2022-12-21T05:12:00.000Z",
        "voteCount": 1,
        "content": "C is the answer."
      },
      {
        "date": "2022-08-20T00:04:00.000Z",
        "voteCount": 2,
        "content": "C is correct"
      },
      {
        "date": "2022-07-13T22:41:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/storage/docs/gsutil/commands/cp"
      },
      {
        "date": "2022-01-21T20:11:00.000Z",
        "voteCount": 3,
        "content": "Agree with Option C"
      },
      {
        "date": "2022-01-08T23:29:00.000Z",
        "voteCount": 2,
        "content": "Agree with C"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 101,
    "url": "https://www.examtopics.com/discussions/google/view/69772-exam-professional-cloud-developer-topic-1-question-101/",
    "body": "You are developing a web application that will be accessible over both HTTP and HTTPS and will run on Compute Engine instances. On occasion, you will need to SSH from your remote laptop into one of the Compute Engine instances to conduct maintenance on the app. How should you configure the instances while following Google-recommended best practices?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up a backend with Compute Engine web server instances with a private IP address behind a TCP proxy load balancer.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the firewall rules to allow all ingress traffic to connect to the Compute Engine web servers, with each server having a unique external IP address.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Cloud Identity-Aware Proxy API for SSH access. Then configure the Compute Engine servers with private IP addresses behind an HTTP(s) load balancer for the application web traffic.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up a backend with Compute Engine web server instances with a private IP address behind an HTTP(S) load balancer. Set up a bastion host with a public IP address and open firewall ports. Connect to the web instances using the bastion host."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 9,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-07-18T22:34:00.000Z",
        "voteCount": 1,
        "content": "Option D: The Best Practice\nSecurity: Using a bastion host with a public IP address provides a secure jump point. Your web servers remain behind a firewall with private IP addresses, making them less vulnerable to direct attacks.\nScalability: Bastion hosts can be easily scaled and managed, allowing you to control access to your web server instances.\nSSH Access: You can securely SSH into the bastion host and then tunnel to your web server instances."
      },
      {
        "date": "2024-07-18T22:35:00.000Z",
        "voteCount": 1,
        "content": "Option A: TCP Proxy Load Balancer\nNot Ideal for Web Applications: TCP load balancers are better suited for applications that use TCP protocols, not HTTP/HTTPS.\nSSH Access: While you could potentially use a TCP load balancer for SSH, it's not the recommended approach.\nOption B: Open Firewall Rules\nMajor Security Risk: Exposing your web servers directly to the internet with public IP addresses is a significant security vulnerability.\nOption C: Cloud Identity-Aware Proxy (IAP) for SSH\nNot Designed for SSH: IAP is primarily designed for secure access to web applications, not for SSH. While you could potentially use IAP for SSH, it's not a standard or recommended practice."
      },
      {
        "date": "2023-09-20T16:39:00.000Z",
        "voteCount": 1,
        "content": "VM can only connect through IAM with public IP so C wouldn't work\nbastion host is one of options instead - https://cloud.google.com/compute/docs/connect/ssh-internal-ip"
      },
      {
        "date": "2023-11-22T02:30:00.000Z",
        "voteCount": 1,
        "content": "\"This document describes how to connect to a virtual machine (VM) instance through its internal IP address, using Identity-Aware Proxy (IAP) TCP forwarding.\"\nhttps://cloud.google.com/compute/docs/connect/ssh-using-iap"
      },
      {
        "date": "2023-09-20T02:39:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2023-04-21T04:05:00.000Z",
        "voteCount": 1,
        "content": "i go for C\nhttps://cloud.google.com/compute/docs/connect/ssh-using-iap\nIAP TCP forwarding enables you to establish an encrypted tunnel over which you can forward SSH connections to VMs. When you connect to a VM that uses IAP, IAP wraps the SSH connection inside HTTPS before forwarding the connection to the VM. Then, IAP checks if the you have the required IAM permissions and if you do, grants access to the VM.\n\nIf you need to connect to a VM that doesn't have external IP addresses and you can't use IAP, review the other methods listed in Connection options for internal-only VMs."
      },
      {
        "date": "2023-04-21T04:09:00.000Z",
        "voteCount": 1,
        "content": "D is wrong. \nBastion host VMs\tYou have a specific use case, like session recording, and you can't use IAP"
      },
      {
        "date": "2023-02-27T06:47:00.000Z",
        "voteCount": 1,
        "content": "i would choose C: https://medium.com/@larry_nguyen/use-identity-aware-proxy-iap-instead-of-bastion-host-to-connect-to-private-virtual-machines-in-9885bc7c12dd"
      },
      {
        "date": "2023-01-10T06:38:00.000Z",
        "voteCount": 3,
        "content": "D. is a recommended way to configure the instances while following Google-recommended best practices.\n\nThis approach provides several benefits:\n\nThe web server instances are only accessible through the load balancer and not directly via their private IP addresses, which improves security.\nThe bastion host acts as a secure jump box that allows you to SSH into the web server instances, while only allowing incoming SSH connections on a specific IP address (the bastion host's public IP).\nThe firewall rules on the web server instances can be configured to only allow connections from the bastion host's IP, further reducing the attack surface.\nIt is a more recommended to have a bastion host that is authorized by your organization to connect to private instances this way it can provide a better security to your instances. And also in terms of compliance, it will also follow the best practices of your organization."
      },
      {
        "date": "2023-01-10T06:38:00.000Z",
        "voteCount": 3,
        "content": "C is a valid approach, but it may not be the best option for all use cases.\n\nCloud IAP allows you to control access to resources in your project by using identity and access management (IAM) roles, which is a good way to secure SSH access. However, this option does not address the issue of securing incoming web traffic, which is a separate concern. Configuring the servers with private IP addresses behind an HTTP(s) load balancer would help with securing the web traffic, but it does not provide an additional layer of security for SSH access. Additionally, it does not have the concept of secure jump host, which is a security best practice in protecting your instances from unwanted incoming connections."
      },
      {
        "date": "2022-12-21T05:10:00.000Z",
        "voteCount": 1,
        "content": "C is the answer.\n\nhttps://cloud.google.com/iap"
      },
      {
        "date": "2022-11-14T00:38:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/solutions/connecting-securely#storing_host_keys_by_enabling_guest_attributes\nAnswer C"
      },
      {
        "date": "2022-08-20T00:04:00.000Z",
        "voteCount": 2,
        "content": "C is correct"
      },
      {
        "date": "2022-08-05T21:15:00.000Z",
        "voteCount": 1,
        "content": "I feel both C and D are correct for this scenario. \nThe only reason I would go with option C is that it would be easier to set up than setting up a bastion host."
      },
      {
        "date": "2022-07-13T22:45:00.000Z",
        "voteCount": 1,
        "content": "With TCP forwarding, IAP can protect SSH and RDP access to your VMs hosted on Google Cloud. Your VM instances don't even need public IP addresses.\nhttps://cloud.google.com/iap"
      },
      {
        "date": "2022-05-18T20:04:00.000Z",
        "voteCount": 2,
        "content": "C is my answer, guys"
      },
      {
        "date": "2022-05-13T01:14:00.000Z",
        "voteCount": 2,
        "content": "D should be the answer (https://cloud.google.com/solutions/connecting-securely#external) But the bastion host should also be protected by IAP"
      },
      {
        "date": "2022-05-05T12:34:00.000Z",
        "voteCount": 1,
        "content": "C should be correct (https://cloud.google.com/iap/docs/using-tcp-forwarding#tunneling_ssh_connections)"
      },
      {
        "date": "2022-04-02T04:21:00.000Z",
        "voteCount": 2,
        "content": "Ans is D"
      },
      {
        "date": "2022-05-19T21:08:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/solutions/connecting-securely#external"
      },
      {
        "date": "2022-01-09T10:17:00.000Z",
        "voteCount": 4,
        "content": "I vote C"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 102,
    "url": "https://www.examtopics.com/discussions/google/view/69720-exam-professional-cloud-developer-topic-1-question-102/",
    "body": "You have a mixture of packaged and internally developed applications hosted on a Compute Engine instance that is running Linux. These applications write log records as text in local files. You want the logs to be written to Cloud Logging. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPipe the content of the files to the Linux Syslog daemon.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall a Google version of fluentd on the Compute Engine instance.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall a Google version of collectd on the Compute Engine instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUsing cron, schedule a job to copy the log files to Cloud Storage once a day."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 15,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-02-26T08:15:00.000Z",
        "voteCount": 10,
        "content": "Collectd is used for Monitoring agents \nFluentd is for cloud logging agent"
      },
      {
        "date": "2022-01-08T23:48:00.000Z",
        "voteCount": 7,
        "content": "Agree with B"
      },
      {
        "date": "2022-12-21T03:09:00.000Z",
        "voteCount": 1,
        "content": "B is the answer.\n\nhttps://cloud.google.com/stackdriver/docs/solutions/agents/ops-agent\nThe Ops Agent is the primary agent for collecting telemetry from your Compute Engine instances. Combining logging and metrics into a single agent, the Ops Agent uses Fluent Bit for logs, which supports high-throughput logging, and the OpenTelemetry Collector for metrics.\n\nYou can configure the Ops Agent to support parsing of log files from third-party applications."
      },
      {
        "date": "2022-08-20T00:04:00.000Z",
        "voteCount": 4,
        "content": "B is correct"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 103,
    "url": "https://www.examtopics.com/discussions/google/view/69229-exam-professional-cloud-developer-topic-1-question-103/",
    "body": "You want to create `fully baked` or `golden` Compute Engine images for your application. You need to bootstrap your application to connect to the appropriate database according to the environment the application is running on (test, staging, production). What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEmbed the appropriate database connection string in the image. Create a different image for each environment.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWhen creating the Compute Engine instance, add a tag with the name of the database to be connected. In your application, query the Compute Engine API to pull the tags for the current instance, and use the tag to construct the appropriate database connection string.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWhen creating the Compute Engine instance, create a metadata item with a key of \u05d2\u20acDATABASE\u05d2\u20ac and a value for the appropriate database connection string. In your application, read the \u05d2\u20acDATABASE\u05d2\u20ac environment variable, and use the value to connect to the appropriate database.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWhen creating the Compute Engine instance, create a metadata item with a key of \u05d2\u20acDATABASE\u05d2\u20ac and a value for the appropriate database connection string. In your application, query the metadata server for the \u05d2\u20acDATABASE\u05d2\u20ac value, and use the value to connect to the appropriate database.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-01-05T23:46:00.000Z",
        "voteCount": 6,
        "content": "I vote D.\nhttps://cloud.google.com/compute/docs/metadata/querying-metadata"
      },
      {
        "date": "2023-01-10T22:34:00.000Z",
        "voteCount": 1,
        "content": "D. When creating the Compute Engine instance, create a metadata item with a key of \"DATABASE\" and a value for the appropriate database connection string. In your application, query the metadata server for the \"DATABASE\" value, and use the value to connect to the appropriate database.\n\nThis approach allows you to create a single golden image that is agnostic to the environment it is running in, while still allowing the appropriate database connection to be set at runtime. The metadata item is stored with the instance, so it can be read by your application at any time. This method allows you to avoid creating different images for different environments, and to use the same image for all environments.\n\nYou can create metadata item by using the gcloud command line tool or the API to set metadata for a Compute Engine instance. Once set, the metadata can be easily accessed by your application via the instance metadata server."
      },
      {
        "date": "2022-12-21T03:07:00.000Z",
        "voteCount": 1,
        "content": "D is the answer.\n\nhttps://cloud.google.com/compute/docs/metadata/setting-custom-metadata"
      },
      {
        "date": "2022-08-20T00:04:00.000Z",
        "voteCount": 2,
        "content": "D is correct"
      },
      {
        "date": "2022-08-05T21:27:00.000Z",
        "voteCount": 1,
        "content": "It should be D!"
      },
      {
        "date": "2022-02-26T08:36:00.000Z",
        "voteCount": 3,
        "content": "Thanks HotSpa27 it is indeed D \nEvery VM stores its metadata on a metadata server. Use these instructions to query these metadata values. You can only query the metadata server programmatically from within a VM"
      },
      {
        "date": "2022-01-01T18:41:00.000Z",
        "voteCount": 3,
        "content": "D:\n\nhttps://cloud.google.com/compute/docs/metadata/overview"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 104,
    "url": "https://www.examtopics.com/discussions/google/view/69773-exam-professional-cloud-developer-topic-1-question-104/",
    "body": "You are developing a microservice-based application that will be deployed on a Google Kubernetes Engine cluster. The application needs to read and write to a<br>Spanner database. You want to follow security best practices while minimizing code changes. How should you configure your application to retrieve Spanner credentials?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the appropriate service accounts, and use Workload Identity to run the pods.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the application credentials as Kubernetes Secrets, and expose them as environment variables.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the appropriate routing rules, and use a VPC-native cluster to directly connect to the database.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the application credentials using Cloud Key Management Service, and retrieve them whenever a database connection is made."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 24,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 7,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-08-14T21:55:00.000Z",
        "voteCount": 8,
        "content": "https://cloud.google.com/blog/products/containers-kubernetes/introducing-workload-identity-better-authentication-for-your-gke-applications\n\nA Cloud IAM service account is an identity that an application can use to make requests to Google APIs. As an application developer, you could generate individual IAM service accounts for each application, and then download and store the keys as a Kubernetes secret that you manually rotate. Not only is this process burdensome, but service account keys only expire every 10 years (or until you manually rotate them). In the case of a breach or compromise, an unaccounted-for key could mean prolonged access for an attacker. This potential blind spot, plus the management overhead of key inventory and rotation, makes using service account keys as secrets a less than ideal method for authenticating GKE workloads."
      },
      {
        "date": "2022-08-18T06:21:00.000Z",
        "voteCount": 2,
        "content": "Exact...\nand it's a recent alternative to secrets ... why would google want you to ignore it? :)"
      },
      {
        "date": "2022-03-26T12:27:00.000Z",
        "voteCount": 5,
        "content": "I assume that nobody read through the official docs and GCP Best practices for K8s and Cloud SQL. https://cloud.google.com/sql/docs/mysql/connect-kubernetes-engine#secrets\n\n\"A database credentials Secret includes the name of the database user you are connecting as, and the user's database password.\"\n\nThe best answer here is B, having K8s Secrets is the go-to method to configure and store sensitive information within a cluster such as Spanner credentials"
      },
      {
        "date": "2024-07-18T22:54:00.000Z",
        "voteCount": 1,
        "content": "The best answer here is A. Configure the appropriate service accounts, and use Workload Identity to run the pods.\n\nHere's why:\n\nWorkload Identity: Workload Identity is a Google Cloud feature that allows Kubernetes service accounts to act as Google Cloud IAM service accounts. This means your pods can authenticate to Spanner without needing to store credentials directly within the pod."
      },
      {
        "date": "2024-07-18T22:54:00.000Z",
        "voteCount": 1,
        "content": "Let's break down why the other options are less ideal:\n\nB. Store credentials as Kubernetes Secrets: While this approach works, it's less secure than Workload Identity. Storing credentials in Secrets exposes them to potential security risks within the cluster.\nC. VPC-native cluster and routing rules: This approach focuses on network connectivity but doesn't address the core issue of secure credential management.\nD. Cloud Key Management Service (KMS): KMS is excellent for managing encryption keys, but it's not the primary solution for retrieving Spanner credentials. KMS is more suited for encrypting data at rest."
      },
      {
        "date": "2024-07-18T22:54:00.000Z",
        "voteCount": 1,
        "content": "Why Workload Identity is the Best Practice:\n\nSecurity: Workload Identity eliminates the need to store sensitive credentials within your pods, making your application more secure.\nSimplified Management: You can manage access control and permissions through Google Cloud IAM, which is easier than managing credentials within your Kubernetes cluster.\nIntegration with Google Cloud: Workload Identity seamlessly integrates with Google Cloud services, making it a natural choice for applications running on GKE."
      },
      {
        "date": "2023-11-17T13:39:00.000Z",
        "voteCount": 1,
        "content": "Option A is the recommended approach for securely configuring your microservice-based application to retrieve Spanner credentials on Google Kubernetes Engine (GKE)"
      },
      {
        "date": "2023-09-20T02:47:00.000Z",
        "voteCount": 2,
        "content": "This approach involves configuring service accounts with the necessary permissions to access the Spanner database. By using Workload Identity, you can associate these service accounts with your Kubernetes Engine pods, allowing them to authenticate and retrieve Spanner credentials automatically."
      },
      {
        "date": "2023-04-21T05:17:00.000Z",
        "voteCount": 1,
        "content": "i go for B\nquestion is about how to RETRIEVE db creds\nhttps://cloud.google.com/sql/docs/mysql/connect-kubernetes-engine#secrets\nA is about how to connect to spanner"
      },
      {
        "date": "2023-02-05T09:29:00.000Z",
        "voteCount": 2,
        "content": "Google recommends using service accounts and work load identity whenever possible"
      },
      {
        "date": "2023-04-25T02:02:00.000Z",
        "voteCount": 1,
        "content": "Exactly!"
      },
      {
        "date": "2023-01-10T22:36:00.000Z",
        "voteCount": 1,
        "content": "A. Configure the appropriate service accounts, and use Workload Identity to run the pods.\n\nWorkload Identity is a way to associate Kubernetes service accounts with Google Cloud IAM service accounts, allowing your pods to authenticate to Google Cloud services using their IAM identity. This means that you don't have to store application credentials in your code or in Kubernetes Secrets, and you can manage the permissions of your application in Google Cloud IAM.\n\nYou would need to create service account in cloud IAM and a Kubernetes service account and then map them to use Workload Identity.\nYou can also use gcloud command line to map the Kubernetes service account to the desired IAM service account. Then in your application, you can use the Kubernetes service account to authenticate to Spanner, which will authenticate as the mapped IAM service account.\n\nThis way you don't have to hardcode credentials in your code, and you can easily manage the permissions of your application using Google Cloud IAM."
      },
      {
        "date": "2022-12-21T03:04:00.000Z",
        "voteCount": 1,
        "content": "A is the answer.\n\nhttps://cloud.google.com/kubernetes-engine/docs/concepts/workload-identity#what_is\nApplications running on GKE might need access to Google Cloud APIs such as Compute Engine API, BigQuery Storage API, or Machine Learning APIs.\n\nWorkload Identity allows a Kubernetes service account in your GKE cluster to act as an IAM service account. Pods that use the configured Kubernetes service account automatically authenticate as the IAM service account when accessing Google Cloud APIs. Using Workload Identity allows you to assign distinct, fine-grained identities and authorization for each application in your cluster."
      },
      {
        "date": "2022-11-20T23:47:00.000Z",
        "voteCount": 2,
        "content": "Answer is A\n Store the application credentials as Kubernetes Secrets, and expose them as environment variables"
      },
      {
        "date": "2022-11-20T23:48:00.000Z",
        "voteCount": 1,
        "content": "Sorry i dwant to paste the link to A, not answer B. B is wrong. \nhttps://kubernetes.io/docs/concepts/configuration/secret/#alternatives-to-secrets\nAnswer A"
      },
      {
        "date": "2022-11-20T23:50:00.000Z",
        "voteCount": 1,
        "content": "It cant be B because \nBecause Secrets can be created independently of the Pods that use them, there is less risk of the Secret (and its data) being exposed during the workflow of creating, viewing, and editing Pods. Kubernetes, and applications that run in your cluster, can also take additional precautions with Secrets, such as avoiding writing secret data to nonvolatile storage.\n\nSecrets are similar to ConfigMaps but are specifically intended to hold confidential data.\n\nCaution:\nKubernetes Secrets are, by default, stored unencrypted in the API server's underlying data store (etcd). Anyone with API access can retrieve or modify a Secret, and so can anyone with access to etcd. Additionally, anyone who is authorized to create a Pod in a namespace can use that access to read any Secret in that namespace; this includes indirect access such as the ability to create a Deployment."
      },
      {
        "date": "2022-08-20T00:05:00.000Z",
        "voteCount": 4,
        "content": "I think A is correct"
      },
      {
        "date": "2022-07-13T23:04:00.000Z",
        "voteCount": 1,
        "content": "A and B ,both are correct. Curently in my project we are using A for allowing pods to query Bigquery. So A and B both seems to be correct."
      },
      {
        "date": "2022-08-05T21:39:00.000Z",
        "voteCount": 1,
        "content": "A service account will only allow you to establish your workload identity(basically authenticate the identity of your cluster pods).\nBut, in order to establish a database connection, you would need to connect it using the DB credentials( like host, user id, password, and database name to connect to). To securely store such credentials, Google recommends using a Secret Manager. So the answer would be B!"
      },
      {
        "date": "2022-05-28T06:48:00.000Z",
        "voteCount": 3,
        "content": "B. \nThe question is not about how to connect/access Cloud Spanner, but is how to \"retrieve Spanner *credentials*\"."
      },
      {
        "date": "2022-05-20T09:46:00.000Z",
        "voteCount": 1,
        "content": "A and B -&gt; It should be select 2 best options question."
      },
      {
        "date": "2022-05-19T18:03:00.000Z",
        "voteCount": 3,
        "content": "I think B is more suitable in this situation"
      },
      {
        "date": "2022-02-26T08:38:00.000Z",
        "voteCount": 2,
        "content": "Yes A is the option"
      },
      {
        "date": "2022-01-09T10:23:00.000Z",
        "voteCount": 4,
        "content": "I vote A\nhttps://cloud.google.com/kubernetes-engine/docs/concepts/workload-identity"
      },
      {
        "date": "2022-01-15T09:53:00.000Z",
        "voteCount": 1,
        "content": "Yes could be Option A , also Option B could work, not sure if Option B is not right considering the question states minimum code changes?"
      },
      {
        "date": "2022-02-28T15:24:00.000Z",
        "voteCount": 1,
        "content": "yes is better b"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 105,
    "url": "https://www.examtopics.com/discussions/google/view/69774-exam-professional-cloud-developer-topic-1-question-105/",
    "body": "You are deploying your application on a Compute Engine instance that communicates with Cloud SQL. You will use Cloud SQL Proxy to allow your application to communicate to the database using the service account associated with the application's instance. You want to follow the Google-recommended best practice of providing minimum access for the role assigned to the service account. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAssign the Project Editor role.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAssign the Project Owner role.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAssign the Cloud SQL Client role.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAssign the Cloud SQL Editor role."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-01-09T10:27:00.000Z",
        "voteCount": 7,
        "content": "I vote C\nhttps://cloud.google.com/sql/docs/mysql/roles-and-permissions"
      },
      {
        "date": "2024-04-16T20:49:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/sql/docs/mysql/roles-and-permissions#:~:text=When%20you%20use%20an%20account%20to%20connect%20to%20a%20Cloud%20SQL%20instance%2C%20the%20account%20must%20have%20the%20Cloud%20SQL%20%3E%20Client%20role%20(roles/cloudsql.client)%2C%20which%20includes%20the%20permissions%20required%20for%20connecting."
      },
      {
        "date": "2023-09-20T02:56:00.000Z",
        "voteCount": 1,
        "content": "Cloud SQL Client role: This role provides the necessary permissions to interact with Cloud SQL while minimizing access to other resources."
      },
      {
        "date": "2023-01-10T22:38:00.000Z",
        "voteCount": 1,
        "content": "C. Assign the Cloud SQL Client role.\n\nThe Cloud SQL Client role has the minimal set of permissions required to access Cloud SQL instances. This role includes permissions to connect to and use a Cloud SQL instance, but it doesn't include permissions to create, delete or manage the instance itself. This role should be granted to the service account associated with your Compute Engine instance, in order to allow your application to connect to the Cloud SQL instance using the Cloud SQL Proxy.\n\nYou can assign the Cloud SQL Client role to a service account by using the Cloud Console, the gcloud command-line tool, or the Cloud Identity and Access Management (IAM) API. Once the role is assigned, your application will be able to authenticate to Cloud SQL using the service account and the Cloud SQL Proxy.\n\nIt is important to note that the permissions granted by this role should be limited to the specific Cloud SQL instance that the application needs to connect to and not the entire project, to minimize the access and follow the principle of least privilege."
      },
      {
        "date": "2022-12-21T03:02:00.000Z",
        "voteCount": 1,
        "content": "C is the answer.\n\nhttps://cloud.google.com/sql/docs/mysql/roles-and-permissions#proxy-roles-permissions\nIf you are connecting to a Cloud SQL instance from a Compute Engine instance using Cloud SQL Auth proxy, you can use the default Compute Engine service account associated with the Compute Engine instance.\n\nAs with all accounts connecting to a Cloud SQL instance, the service account must have the Cloud SQL &gt; Client role."
      },
      {
        "date": "2022-08-20T00:05:00.000Z",
        "voteCount": 3,
        "content": "C is correct"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 106,
    "url": "https://www.examtopics.com/discussions/google/view/69767-exam-professional-cloud-developer-topic-1-question-106/",
    "body": "Your team develops stateless services that run on Google Kubernetes Engine (GKE). You need to deploy a new service that will only be accessed by other services running in the GKE cluster. The service will need to scale as quickly as possible to respond to changing load. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a Vertical Pod Autoscaler to scale the containers, and expose them via a ClusterIP Service.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a Vertical Pod Autoscaler to scale the containers, and expose them via a NodePort Service.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a Horizontal Pod Autoscaler to scale the containers, and expose them via a ClusterIP Service.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a Horizontal Pod Autoscaler to scale the containers, and expose them via a NodePort Service."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-11-26T01:07:00.000Z",
        "voteCount": 1,
        "content": "Horizontal Pod Autoscaler (HPA) scales the number of pod replicas based on CPU usage or other select metrics, which is suitable for quick scaling with load changes. ClusterIP is appropriate for services only accessible within the cluster. This combination seems to meet all the requirements."
      },
      {
        "date": "2023-09-20T02:59:00.000Z",
        "voteCount": 1,
        "content": "HPA automatically scales the number of pods in a deployment based on CPU utilization or other custom metrics. By using HPA, you can ensure that your service scales quickly to respond to changing load while minimizing manual intervention. Exposing the service via a ClusterIP Service allows other services running in the GKE cluster to access it securely without exposing it to the public internet."
      },
      {
        "date": "2023-01-10T22:39:00.000Z",
        "voteCount": 1,
        "content": "C. Use a Horizontal Pod Autoscaler to scale the containers, and expose them via a ClusterIP Service.\n\nWhen dealing with services that are only accessed by other services in the same GKE cluster, it's usually best to use a ClusterIP Service. This type of service allows pods to be accessed by other pods within the cluster using their IP address, but doesn't expose them to the outside world."
      },
      {
        "date": "2023-01-10T22:39:00.000Z",
        "voteCount": 1,
        "content": "A Horizontal Pod Autoscaler is used to automatically scale the number of replicas of a deployment based on certain metrics. This is useful for scaling based on CPU utilization, memory usage, or custom metrics. By using a Horizontal Pod Autoscaler, you can respond quickly to changes in load by automatically creating or deleting replicas of your pods, so the service can handle the traffic.\n\nYou can expose the service via a ClusterIP Service by creating one in Kubernetes and configuring the selector to match the replicas running in your deployment. This allows other services to discover and communicate with your new service by its ClusterIP."
      },
      {
        "date": "2022-12-21T03:00:00.000Z",
        "voteCount": 1,
        "content": "C is the answer.\n\nhttps://cloud.google.com/kubernetes-engine/docs/concepts/service#services_of_type_clusterip\nWhen you create a Service of type ClusterIP, Kubernetes creates a stable IP address that is accessible from nodes in the cluster.\n\nhttps://cloud.google.com/kubernetes-engine/docs/concepts/horizontalpodautoscaler\nThe Horizontal Pod Autoscaler changes the shape of your Kubernetes workload by automatically increasing or decreasing the number of Pods in response to the workload's CPU or memory consumption, or in response to custom metrics reported from within Kubernetes or external metrics from sources outside of your cluster."
      },
      {
        "date": "2022-08-20T00:06:00.000Z",
        "voteCount": 2,
        "content": "C is correct"
      },
      {
        "date": "2022-01-09T09:34:00.000Z",
        "voteCount": 3,
        "content": "I vote C"
      },
      {
        "date": "2022-01-15T10:02:00.000Z",
        "voteCount": 2,
        "content": "Agree Option C \nhttps://cloud.google.com/kubernetes-engine/docs/concepts/service"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 107,
    "url": "https://www.examtopics.com/discussions/google/view/69769-exam-professional-cloud-developer-topic-1-question-107/",
    "body": "You recently migrated a monolithic application to Google Cloud by breaking it down into microservices. One of the microservices is deployed using Cloud<br>Functions. As you modernize the application, you make a change to the API of the service that is backward-incompatible. You need to support both existing callers who use the original API and new callers who use the new API. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLeave the original Cloud Function as-is and deploy a second Cloud Function with the new API. Use a load balancer to distribute calls between the versions.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLeave the original Cloud Function as-is and deploy a second Cloud Function that includes only the changed API. Calls are automatically routed to the correct function.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLeave the original Cloud Function as-is and deploy a second Cloud Function with the new API. Use Cloud Endpoints to provide an API gateway that exposes a versioned API.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRe-deploy the Cloud Function after making code changes to support the new API. Requests for both versions of the API are fulfilled based on a version identifier included in the call."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 13,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-02-26T08:57:00.000Z",
        "voteCount": 7,
        "content": "Based on the link \u2026 where it says for backward incompatible strategy use two separate deployments/instances v1 and v2 and only C option is inline with the link"
      },
      {
        "date": "2023-01-10T22:41:00.000Z",
        "voteCount": 1,
        "content": "Answer is C\n\nWhen making backward-incompatible changes to an API, it's important to provide a way for existing callers to continue using the old API while still supporting new callers who use the new API. One way to do this is by deploying a new version of the Cloud Function that includes the new API, and leaving the old function as-is.\n\nBy using Cloud Endpoints you can create an API Gateway that can handle multiple versions of the API, so that requests to different versions of the API can be routed to the corresponding Cloud Function. This allows you to maintain both versions of the API and have control over which version is exposed to the users.\n\nThis approach allows you to continue supporting existing callers while also introducing new features to the application through the new version. Also, it gives you a lot more flexibility in terms of rollout, testing, and monitoring."
      },
      {
        "date": "2022-12-21T02:57:00.000Z",
        "voteCount": 1,
        "content": "C is the answer.\n\nhttps://cloud.google.com/endpoints/docs/openapi/versioning-an-api#backwards-incompatible\nWhen you make changes to your API that breaks your customers' client code, as a best practice, increment the major version number of your API. Endpoints can run more than one major version of an API concurrently. By providing both versions of the API, your customers can pick which version they want to use and control when they migrate to the new version."
      },
      {
        "date": "2022-11-24T00:01:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/architecture/migrating-a-monolithic-app-to-microservices-gke#versioning\nAnswer C"
      },
      {
        "date": "2022-11-11T01:53:00.000Z",
        "voteCount": 1,
        "content": "Answer D feels more appropriate based on the below URL:\n\nhttps://cloud.google.com/endpoints/docs/openapi/versioning-an-api#backwards-incompatible"
      },
      {
        "date": "2022-08-20T00:06:00.000Z",
        "voteCount": 3,
        "content": "C is correct"
      },
      {
        "date": "2022-01-09T09:52:00.000Z",
        "voteCount": 2,
        "content": "I vote D\nhttps://cloud.google.com/endpoints/docs/openapi/versioning-an-api"
      },
      {
        "date": "2022-01-15T10:12:00.000Z",
        "voteCount": 3,
        "content": "As per this link there is a reference that both versions need to be available simultaneously. I would go with Option C"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 108,
    "url": "https://www.examtopics.com/discussions/google/view/70075-exam-professional-cloud-developer-topic-1-question-108/",
    "body": "You are developing an application that will allow users to read and post comments on news articles. You want to configure your application to store and display user-submitted comments using Firestore. How should you design the schema to support an unknown number of comments and articles?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore each comment in a subcollection of the article.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd each comment to an array property on the article.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore each comment in a document, and add the comment's key to an array property on the article.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore each comment in a document, and add the comment's key to an array property on the user profile."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 19,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 12,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 6,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-01-21T14:24:00.000Z",
        "voteCount": 21,
        "content": "Selected: A\nFirestore has a \u201chierarchical structure\u201d: collection contains documents, document can contain (sub)collections\nD does not make sense bc why do you want to link comments to the user profile instead of the article?\nhttps://stackoverflow.com/questions/48634227/limitation-to-number-of-documents-under-one-collection-in-firebase-firestore\n\u201cThere is no documented limit to the number of documents that can be stored in a Cloud Firestore collection. The system is designed to scale to huge data sets.\u201d"
      },
      {
        "date": "2022-02-28T15:29:00.000Z",
        "voteCount": 1,
        "content": "the subcollection has a limit of 1Mb of data, so for unknown number of comments is not valid, the answer is D"
      },
      {
        "date": "2022-05-06T05:42:00.000Z",
        "voteCount": 6,
        "content": "That's wrong, the 1MB limit it's on the document inside the subcollection (not on the subcollection itself. \n\n\"Document size - Cloud Firestore is optimized for small documents and enforces a 1MB size limit on documents. If your array can expand arbitrarily, it is better to use a subcollection, which has better scaling performance.\"\n\nCheck out also the example in the subcollections documentation, showing a rooms-messages hierarchy example.\nhttps://firebase.google.com/docs/firestore/data-model#subcollections"
      },
      {
        "date": "2022-03-26T13:38:00.000Z",
        "voteCount": 9,
        "content": "Why D and not C? For my understanding, we need to keep a relation between the articles and their comments. I don't see how the user profile could come in handy... but please let me know if I misunderstood something. For me, Ans C makes more sense."
      },
      {
        "date": "2024-07-18T23:42:00.000Z",
        "voteCount": 1,
        "content": "The best answer here is A. Store each comment in a subcollection of the article.\n\nHere's why:\n\nScalability: Storing comments as subcollections within the article document allows you to handle an unlimited number of comments per article. Firestore scales well with large numbers of subcollections.\nQuerying: You can easily query for comments related to a specific article using the collectionGroup() query. This allows you to retrieve all comments for an article without needing to know the specific comment IDs.\nPerformance: Firestore's subcollection structure optimizes for reading and writing comments related to a specific article."
      },
      {
        "date": "2024-07-18T23:42:00.000Z",
        "voteCount": 1,
        "content": "Let's look at why the other options are less ideal:\n\nB. Add each comment to an array property on the article: This approach is limited by the maximum document size in Firestore (1 MB). You'll run into issues if you have a large number of comments for a single article.\nC. Store each comment in a document and add the comment's key to an array property on the article: This approach is less efficient for querying comments related to a specific article. You'd need to perform multiple queries to retrieve all comments.\nD. Store each comment in a document and add the comment's key to an array property on the user profile: This approach is not ideal for retrieving comments related to a specific article. You'd need to query the user profile for each comment, which is inefficient."
      },
      {
        "date": "2024-07-13T03:12:00.000Z",
        "voteCount": 1,
        "content": "Firestore has a hierarchical structure"
      },
      {
        "date": "2024-04-16T21:03:00.000Z",
        "voteCount": 1,
        "content": "It can't be D because:\nStoring each comment in a document and adding the comment's key to an array property on the user profile wouldn't efficiently link comments to articles."
      },
      {
        "date": "2023-12-22T14:58:00.000Z",
        "voteCount": 1,
        "content": "Answer is D in my opinion for \"display user-submitted comments\" and \"unknown number of comments and articles\""
      },
      {
        "date": "2023-11-21T12:14:00.000Z",
        "voteCount": 3,
        "content": "Answer is A"
      },
      {
        "date": "2023-11-20T15:09:00.000Z",
        "voteCount": 3,
        "content": "As per previous comments also appointed, there is not such a limitation on size for a Subcollection, and it does not make sense to store the relation with User profile like answer D"
      },
      {
        "date": "2023-11-17T13:47:00.000Z",
        "voteCount": 2,
        "content": "Option A is the recommended approach for structuring data in Firestore to support an unknown number of comments and articles. Firestore is a NoSQL document-oriented database, and using subcollections provides a flexible and scalable way to organize related data"
      },
      {
        "date": "2023-09-21T08:22:00.000Z",
        "voteCount": 1,
        "content": "I would go with C."
      },
      {
        "date": "2023-06-30T23:18:00.000Z",
        "voteCount": 2,
        "content": "the correct answer is A. reference: https://firebase.google.com/docs/firestore/data-model"
      },
      {
        "date": "2023-06-10T12:59:00.000Z",
        "voteCount": 1,
        "content": "Answer is D"
      },
      {
        "date": "2023-05-17T10:40:00.000Z",
        "voteCount": 1,
        "content": "Firestore has a \"hierarchical structure\""
      },
      {
        "date": "2023-02-01T06:11:00.000Z",
        "voteCount": 1,
        "content": "It is recommended to add the comment document IDs to an array property on the corresponding article document, rather than on a user profile. This approach allows you to easily retrieve all comments for a specific article by querying the comments collection using the article ID and then filtering the results based on the IDs in the article's comments array.\n\nStoring the comment IDs in the article document also avoids the need to make multiple read operations to retrieve the comments for a given article, which can be slow and increase latency.\n\nFor example, you could create an array property named \"comments\" in the article document and add the comment document IDs to this array every time a user submits a new comment for the article. This allows you to efficiently retrieve all comments for a given article by querying the comments collection and filtering based on the IDs in the article's \"comments\" array."
      },
      {
        "date": "2023-01-19T00:11:00.000Z",
        "voteCount": 2,
        "content": "Close to this example: https://firebase.google.com/docs/firestore/data-model#subcollections"
      },
      {
        "date": "2023-04-25T02:21:00.000Z",
        "voteCount": 2,
        "content": "Exactly this, thanks for sharing the docs here, I change my answer because of you!"
      },
      {
        "date": "2023-01-10T22:44:00.000Z",
        "voteCount": 3,
        "content": "Answer is A. Store each comment in a subcollection of the article, because it allows for easy scaling and querying of the comments as the data increases. With this approach, you can easily fetch the comments associated with an article by querying the subcollection of that article, instead of querying all the comments in a single collection. It also allows you to query specific comments and articles easily, since you have the reference to the specific article they are associated with."
      },
      {
        "date": "2023-01-10T22:44:00.000Z",
        "voteCount": 2,
        "content": "Storing each comment in a document, and adding the comment's key to an array property on the user profile (Option D) would make it more difficult to fetch all the comments associated with a specific article. Additionally, it would also make it more difficult to query the comments and articles since you would have to go through the user profile to find the comment's key and then use that to find the comment's information. The subcollection approach allows for better organization and querying of the data, making it a better choice in this scenario."
      },
      {
        "date": "2023-01-10T22:45:00.000Z",
        "voteCount": 1,
        "content": "C. Store each comment in a document, and add the comment's key to an array property on the article would work, but it may not be the best solution for this use case. While it would allow you to query for all the comments associated with an article by finding the document with the article and reading the array property of its key.\n\nHowever, this approach would make it more difficult to scale the data as the number of comments grows, because it would require you to retrieve all the comment keys in the array of the article and then perform additional queries to retrieve the actual comment information one by one. This could slow down the application as the number of comments increase, and make it more difficult to handle high-load situations."
      },
      {
        "date": "2023-01-10T22:45:00.000Z",
        "voteCount": 1,
        "content": "Another reason for not choosing C is that, it might also pose an issue for data consistency as comments can change over time and updating the comment document would not automatically update the array property on the article, creating inconsistencies in the data."
      },
      {
        "date": "2022-12-28T01:52:00.000Z",
        "voteCount": 1,
        "content": "https://firebase.google.com/docs/firestore/best-practices#high_read_write_and_delete_rates_to_a_narrow_document_range"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 109,
    "url": "https://www.examtopics.com/discussions/google/view/69770-exam-professional-cloud-developer-topic-1-question-109/",
    "body": "You recently developed an application. You need to call the Cloud Storage API from a Compute<br>Engine instance that doesn't have a public IP address. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Carrier Peering",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse VPC Network Peering",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Shared VPC networks",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Private Google Access\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 14,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-09-20T03:10:00.000Z",
        "voteCount": 2,
        "content": "Private Google Access allows your Compute Engine instances to access Google Cloud APIs and services without requiring a public IP address. It enables outbound connectivity to Google APIs and services using internal IP addresses."
      },
      {
        "date": "2023-01-13T06:06:00.000Z",
        "voteCount": 4,
        "content": "A is not correct because Carrier Peering enables you to access Google applications, such as Google Workspace, by using a service provider to obtain enterprise-grade network services that connect your infrastructure to Google.\nB is not correct because VPC Network Peering enables you to peer VPC networks so that workloads in different VPC networks can communicate in a private RFC 1918 space. Traffic stays within Google's network and doesn't traverse the public internet.\nC is not correct because Shared VPC allows an organization to connect resources from multiple projects to a common VPC network so that they can communicate with each other securely and efficiently using internal IPs from that network.\nD is correct because Private Google Access is an option available for each subnetwork. When it is enabled, instances in the subnetwork can communicate with public Google API endpoints even if the instances don't have external IP addresses."
      },
      {
        "date": "2023-01-10T22:47:00.000Z",
        "voteCount": 1,
        "content": "D. Use Private Google Access\n\nPrivate Google Access is a feature that enables access to Google Cloud APIs and services for instances that don't have a public IP address. With this feature, you can allow your Compute Engine instances in a VPC network to access Google services over the private IP addresses, without the need for a NAT gateway or VPN.\n\nThis feature is especially useful when you want to access Google APIs and services from an instance that doesn't have internet access or a public IP address. In this case, you can enable Private Google Access on the VPC network that your Compute Engine instances belong to, and they will be able to call the Cloud Storage API using the private IP address.\n\nTo enable Private Google Access, you can use the gcloud command-line tool, the Cloud Console, or the REST API. This feature is also available for other services like BigQuery and Cloud SQL as well, to access them from instances without a public IP address"
      },
      {
        "date": "2022-12-21T02:39:00.000Z",
        "voteCount": 1,
        "content": "D is the answer.\n\nhttps://cloud.google.com/vpc/docs/private-google-access\nVM instances that only have internal IP addresses (no external IP addresses) can use Private Google Access. They can reach the external IP addresses of Google APIs and services. The source IP address of the packet can be the primary internal IP address of the network interface or an address in an alias IP range that is assigned to the interface. If you disable Private Google Access, the VM instances can no longer reach Google APIs and services; they can only send traffic within the VPC network."
      },
      {
        "date": "2022-08-20T00:07:00.000Z",
        "voteCount": 2,
        "content": "D is correct"
      },
      {
        "date": "2022-08-05T22:14:00.000Z",
        "voteCount": 1,
        "content": "Yup, it's D."
      },
      {
        "date": "2022-02-26T10:03:00.000Z",
        "voteCount": 4,
        "content": "I vote for D as well"
      },
      {
        "date": "2022-01-09T10:10:00.000Z",
        "voteCount": 4,
        "content": "I vote D\nhttps://cloud.google.com/vpc/docs/private-google-access"
      },
      {
        "date": "2022-01-15T10:17:00.000Z",
        "voteCount": 1,
        "content": "Yes agree should be Option D"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 110,
    "url": "https://www.examtopics.com/discussions/google/view/69771-exam-professional-cloud-developer-topic-1-question-110/",
    "body": "You are a developer working with the CI/CD team to troubleshoot a new feature that your team introduced. The CI/CD team used HashiCorp Packer to create a new Compute Engine image from your development branch. The image was successfully built, but is not booting up. You need to investigate the issue with the CI/<br>CD team. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new feature branch, and ask the build team to rebuild the image.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tShut down the deployed virtual machine, export the disk, and then mount the disk locally to access the boot logs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall Packer locally, build the Compute Engine image locally, and then run it in your personal Google Cloud project.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCheck Compute Engine OS logs using the serial port, and check the Cloud Logging logs to confirm access to the serial port.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-01-09T10:16:00.000Z",
        "voteCount": 9,
        "content": "I vote D\nhttps://cloud.google.com/compute/docs/troubleshooting/troubleshooting-using-serial-console"
      },
      {
        "date": "2022-01-15T10:20:00.000Z",
        "voteCount": 4,
        "content": "Agree with Option D"
      },
      {
        "date": "2024-07-18T23:48:00.000Z",
        "voteCount": 1,
        "content": "The best approach here is D. Check Compute Engine OS logs using the serial port and check the Cloud Logging logs to confirm access to the serial port.\n\nHere's why:\n\nSerial Port Logs: Compute Engine instances have a serial port that captures boot messages and other system-level information. This is often the most valuable source of information when troubleshooting boot failures.\nCloud Logging: If you've enabled serial port logging in your project, the boot messages will be captured and sent to Cloud Logging. This provides a centralized location for reviewing the logs."
      },
      {
        "date": "2024-07-18T23:49:00.000Z",
        "voteCount": 1,
        "content": "Let's look at why the other options are less ideal:\n\nA. Create a new feature branch and ask the build team to rebuild the image: This is a time-consuming and potentially unnecessary step. It's better to investigate the issue with the existing image first.\nB. Shut down the deployed virtual machine, export the disk, and then mount the disk locally to access the boot logs: This is a complex and potentially disruptive process. It's better to leverage the serial port logs for initial troubleshooting.\nC. Install Packer locally, build the Compute Engine image locally, and then run it in your personal Google Cloud project: This approach might help isolate the issue, but it doesn't address the root cause of the problem in the CI/CD pipeline."
      },
      {
        "date": "2023-07-19T06:56:00.000Z",
        "voteCount": 1,
        "content": "D is the answer - the other are too long."
      },
      {
        "date": "2023-01-10T22:49:00.000Z",
        "voteCount": 2,
        "content": "Answer is D\n\nIf the Compute Engine image is not booting up, one of the first steps to troubleshoot the issue would be to check the OS logs to see what might be causing the problem. Compute Engine provides access to the serial console logs of a virtual machine, which can be accessed through the Cloud Console or the gcloud command-line tool. This will allow you to see the output of the virtual machine's boot process and identify any errors or issues that might be preventing it from starting up.\n\nAdditionally, you should also check the Cloud Logging logs to confirm that you have access to the serial port. It may be possible that the firewall rules or IAM permissions are blocking access to the serial port and causing the image not to boot. So, you should check the logs for any errors related to access or firewall rules.\n\nBy checking the OS logs and the Cloud Logging logs, you and the CI/CD team can get a better understanding of what might be causing the issue and take steps to fix it."
      },
      {
        "date": "2022-12-20T07:17:00.000Z",
        "voteCount": 1,
        "content": "D is the answer.\n\nhttps://cloud.google.com/compute/docs/troubleshooting/vm-startup#identify_the_reason_why_the_boot_disk_isnt_booting\n\nIdentify the reason why the boot disk isn't booting\n- Examine your virtual machine instance's serial port output.\nAn instance's BIOS, bootloader, and kernel prints their debug messages into the instance's serial port output, providing valuable information about any errors or issues that the instance experienced. If you enable serial port output logging to Cloud Logging, you can access this information even when your instance is not running."
      },
      {
        "date": "2022-12-05T04:40:00.000Z",
        "voteCount": 1,
        "content": "In option D, what does it mean by \"confirm access to the serial port\"?\nIf I need to see the boot logs, then how the checking the access to serial port gonna help?"
      },
      {
        "date": "2022-11-21T02:42:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/compute/docs/troubleshooting/troubleshooting-using-serial-console#connecting_to_a_serial_console_with_a_login_prompt\nAnswer D"
      },
      {
        "date": "2022-08-20T00:07:00.000Z",
        "voteCount": 3,
        "content": "D is correct"
      },
      {
        "date": "2022-01-12T05:03:00.000Z",
        "voteCount": 3,
        "content": "D is more suitable"
      },
      {
        "date": "2022-02-26T12:26:00.000Z",
        "voteCount": 4,
        "content": "This is interesting to learn that if a compute engine isn\u2019t bootable and you can connect still"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 111,
    "url": "https://www.examtopics.com/discussions/google/view/69722-exam-professional-cloud-developer-topic-1-question-111/",
    "body": "You manage an application that runs in a Compute Engine instance. You also have multiple backend services executing in stand-alone Docker containers running in Compute Engine instances. The Compute Engine instances supporting the backend services are scaled by managed instance groups in multiple regions. You want your calling application to be loosely coupled. You need to be able to invoke distinct service implementations that are chosen based on the value of an HTTP header found in the request. Which Google Cloud feature should you use to invoke the backend services?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTraffic Director\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tService Directory",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAnthos Service Mesh",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInternal HTTP(S) Load Balancing"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 12,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-05-06T06:00:00.000Z",
        "voteCount": 9,
        "content": "\"the backend services are scaled by managed instance groups in multiple regions\", the Internal load balancer is a regional service, so It's A in my opinion.\n\n- \"An internal HTTP(S) load balancer routes internal traffic to the service running on the VM. Traffic Director works with Cloud Load Balancing to provide a managed ingress experience. You set up an external or internal load balancer, and then configure that load balancer to send traffic to your microservices.\""
      },
      {
        "date": "2024-07-18T23:58:00.000Z",
        "voteCount": 1,
        "content": "Traffic Director: Traffic Director is a Google Cloud service designed for advanced traffic management across multiple regions and zones. It allows you to route traffic based on various criteria, including HTTP headers. This makes it ideal for loosely coupled applications where you want to dynamically choose backend services based on request attributes.\n\nHow Traffic Director Works:\nRouting Rules: You define routing rules in Traffic Director that specify how traffic should be directed based on HTTP headers, path patterns, or other criteria.\nBackend Services: You associate backend services with your routing rules. These backend services can be managed instance groups, network endpoint groups (NEGs), or other supported backends.\nTraffic Distribution: When a request arrives, Traffic Director evaluates the routing rules and directs the traffic to the appropriate backend service based on the matching criteria."
      },
      {
        "date": "2024-07-18T23:59:00.000Z",
        "voteCount": 1,
        "content": "Let's look at why the other options are less ideal:\n\nB. Service Directory: Service Directory is primarily used for service discovery, allowing applications to find and connect to services within a cluster or across multiple clusters. While it can be used for routing, it's not as robust as Traffic Director for advanced traffic management.\nC. Anthos Service Mesh: Anthos Service Mesh is a powerful service mesh solution that provides features like traffic management, security, and observability. However, it's typically used for microservices running in Kubernetes environments, not for stand-alone Docker containers in Compute Engine.\nD. Internal HTTP(S) Load Balancing: Internal Load Balancing is designed for routing traffic within a VPC network. While it can handle basic routing, it doesn't offer the advanced features and flexibility of Traffic Director for dynamic routing based on HTTP headers."
      },
      {
        "date": "2024-04-16T21:09:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/traffic-director/docs/overview#traffic_management:~:text=Advanced%20traffic%20management%2C%20including%20routing%20and%20request%20manipulation%20(based%20on%20hostname%2C%20path%2C%20headers%2C%20cookies%2C%20and%20more)%2C%20enables%20you%20to%20determine%20how%20traffic%20flows%20between%20your%20services"
      },
      {
        "date": "2023-11-17T13:52:00.000Z",
        "voteCount": 1,
        "content": "Anthos Service Mesh is a service mesh solution that can be used to invoke distinct service implementations based on the value of an HTTP header in the request. It provides a platform-agnostic way to connect, manage, and secure microservices running on Google Cloud or other environments"
      },
      {
        "date": "2023-09-20T03:15:00.000Z",
        "voteCount": 1,
        "content": "Traffic Director is a Google Cloud feature that provides a global traffic management control plane for service mesh architectures. It allows you to configure and manage traffic routing across multiple services and environments."
      },
      {
        "date": "2023-08-05T03:46:00.000Z",
        "voteCount": 2,
        "content": "I go for A: Traffic director is used to direct the traffic to services running in the different regions or within a region based on the HTTP header values."
      },
      {
        "date": "2023-07-19T06:59:00.000Z",
        "voteCount": 1,
        "content": "A as its the only one that let's you router based on headers"
      },
      {
        "date": "2023-04-21T08:52:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/traffic-director/docs/overview#traffic_management\nAdvanced traffic management, including routing and request manipulation (based on hostname, path, headers, cookies, and more), enables you to determine how traffic flows between your services. You can also apply actions like retries, redirects, and weight-based traffic splitting for canary deployments. Advanced patterns like fault injection, traffic mirroring, and outlier detection enable DevOps use cases that improve your resiliency."
      },
      {
        "date": "2023-02-27T06:39:00.000Z",
        "voteCount": 1,
        "content": "A: https://cloud.google.com/traffic-director/docs/set-up-gce-vms"
      },
      {
        "date": "2023-01-10T22:52:00.000Z",
        "voteCount": 1,
        "content": "Traffic Director provides global traffic management for service meshes and hybrid deployments. It allows you to configure routing rules based on the values of HTTP headers, so you can direct traffic to different service implementations based on the value of an HTTP header found in the request. With Traffic Director, you can route traffic to different services running in different regions, and it also supports automatic failover, so you can ensure high availability for your backend services.\n\nIn your case, you can configure Traffic Director to inspect the value of an HTTP header in the request, and then route the traffic to the appropriate service implementation running in different regions. This allows your application to invoke the backend services in a loosely coupled way, and ensures that the backend services can scale independently of the calling application."
      },
      {
        "date": "2023-01-10T22:52:00.000Z",
        "voteCount": 1,
        "content": "It also works with MIGs to allow you to manage the scaling of the backend services in different regions, and the combination of Traffic Director and MIG allow you to provide service availability in multiple regions."
      },
      {
        "date": "2023-01-10T22:52:00.000Z",
        "voteCount": 1,
        "content": "Option D, Internal HTTP(S) Load Balancing, is a feature that allows you to distribute incoming traffic to a group of instances in a Virtual Private Cloud (VPC) network. It works by creating a load balancer, and configuring it to forward traffic to the instances that you specify. This feature is useful for situations where you want to distribute traffic to multiple instances running in the same region.\n\nHowever, in this scenario where you want to invoke distinct service implementations that are chosen based on the value of an HTTP header found in the request. The Internal HTTP(S) Load Balancer doesn't offer this type of feature. It typically directs traffic based on the IP or hostname and port, but it's not capable of inspecting the value of an HTTP header like Traffic Director does."
      },
      {
        "date": "2023-01-10T22:52:00.000Z",
        "voteCount": 1,
        "content": "Therefore, Traffic Director would be a better choice for this scenario as it allows for more complex and fine-grained traffic management, with the capability to inspect the value of an HTTP header and route the traffic to the appropriate service implementation."
      },
      {
        "date": "2023-04-21T08:54:00.000Z",
        "voteCount": 2,
        "content": "do not post chatgpt answers!"
      },
      {
        "date": "2022-12-20T07:14:00.000Z",
        "voteCount": 2,
        "content": "D is the answer.\n\nhttps://cloud.google.com/load-balancing/docs/l7-internal/traffic-management\nInternal HTTP(S) Load Balancing supports advanced traffic management functionality that enables you to use the following features:\n- Traffic steering. Intelligently route traffic based on HTTP(S) parameters (for example, host, path, headers, and other request parameters)."
      },
      {
        "date": "2022-08-20T00:08:00.000Z",
        "voteCount": 2,
        "content": "I think A is correct"
      },
      {
        "date": "2022-08-05T23:00:00.000Z",
        "voteCount": 3,
        "content": "I believe, it should be A."
      },
      {
        "date": "2022-08-05T23:01:00.000Z",
        "voteCount": 1,
        "content": "I went through the link shared by Blueocean, and also the Google documentation for Traffic Director, and I feel the correct answer should be option A.\nHere's my take on this -\nHTTP(s) load balancer does allow the option of Traffic Steering which identifies the header values and based on that it directs the traffic. But, here it is important to note that the traffic is directed to the specific Compute Engine instance and not to the service endpoint!\nOn the other hand, Traffic Director also allows Traffic Steering which directs the traffic based on header values to the specific service endpoint, which is what is needed in the above scenario. It also supports the point of loosely coupled services.\nSources - \nhttps://cloud.google.com/traffic-director\nhttps://cloud.google.com/traffic-director/docs/features"
      },
      {
        "date": "2022-08-05T23:02:00.000Z",
        "voteCount": 1,
        "content": "Here's what the documentation says, 'Traffic Director supports advanced request routing features like traffic splitting, enabling use cases like canarying, url rewrites/redirects, fault injection, traffic mirroring, and advanced routing capabilities based on various header values, including cookies. Traffic Director also supports many advanced traffic policies with the inclusion of many load-balancing schemes, circuit breaking, and backend outlier detections.' \n\nAlso, in the above question, there's a statement specifying the need for the application to be loosely coupled. To support this point, I found out one line on the Traffic Director's documentation which goes like this, 'This separation of application logic from networking logic lets you improve your development velocity, increase service availability, and introduce modern DevOps practices to your organization.'"
      },
      {
        "date": "2022-01-15T10:24:00.000Z",
        "voteCount": 2,
        "content": "Agree with Option D \nhttps://cloud.google.com/load-balancing/docs/l7-internal/traffic-management"
      },
      {
        "date": "2022-01-09T01:00:00.000Z",
        "voteCount": 3,
        "content": "Agree with D"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 112,
    "url": "https://www.examtopics.com/discussions/google/view/69601-exam-professional-cloud-developer-topic-1-question-112/",
    "body": "Your team is developing an ecommerce platform for your company. Users will log in to the website and add items to their shopping cart. Users will be automatically logged out after 30 minutes of inactivity. When users log back in, their shopping cart should be saved. How should you store users' session and shopping cart information while following Google-recommended best practices?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the session information in Pub/Sub, and store the shopping cart information in Cloud SQL.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the shopping cart information in a file on Cloud Storage where the filename is the SESSION ID.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the session and shopping cart information in a MySQL database running on multiple Compute Engine instances.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the session information in Memorystore for Redis or Memorystore for Memcached, and store the shopping cart information in Firestore.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 22,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-01-09T01:02:00.000Z",
        "voteCount": 9,
        "content": "Should be D definitely"
      },
      {
        "date": "2023-09-21T07:01:00.000Z",
        "voteCount": 1,
        "content": "D is correct."
      },
      {
        "date": "2023-01-13T05:57:00.000Z",
        "voteCount": 1,
        "content": "A is not correct because local memory is lost on process termination, so you would lose the cart information.\nB is not correct because accessing a Cloud Storage bucket is slow and expensive for session information. This is not a Google Cloud best practice.\nC is not correct because BigQuery wouldn't be able to handle the frequent updates made to carts and sessions.\nD is correct because Memorystore is fast and a standard solution to store session information, and Firestore is ideal for small structured data such as a shopping cart. The user will be mapped to the shopping cart with a new session, if required."
      },
      {
        "date": "2023-01-10T22:55:00.000Z",
        "voteCount": 1,
        "content": "Answer is D\nWhen storing session and shopping cart information for an ecommerce platform, it's important to consider scalability, reliability, and security. One solution that follows Google-recommended best practices would be to use Memorystore for Redis or Memorystore for Memcached to store session information and Firestore to store shopping cart information.\n\nMemorystore can store session information and easily handle a large number of concurrent connections, which is crucial for an ecommerce platform where users are logged in and adding items to their shopping cart frequently.\n\nFirestore can easily handle large amounts of semi-structured data, such as a shopping cart's item. Firestore is also a scalable and reliable solution, and it supports automatic scaling and replication.\n\nBy separating the session information and shopping cart information into different services, you can also increase security and avoid any potential data breaches. Using different services will also allows you to scale them independently."
      },
      {
        "date": "2022-12-20T07:04:00.000Z",
        "voteCount": 1,
        "content": "D is the answer."
      },
      {
        "date": "2022-11-10T10:26:00.000Z",
        "voteCount": 1,
        "content": "anyone actually seen this on a test? is A actually correct?"
      },
      {
        "date": "2022-11-21T03:13:00.000Z",
        "voteCount": 1,
        "content": "D is correct\nhttps://cloud.google.com/memorystore/docs/redis/redis-overview"
      },
      {
        "date": "2022-08-20T00:08:00.000Z",
        "voteCount": 3,
        "content": "D is correct"
      },
      {
        "date": "2022-08-05T22:59:00.000Z",
        "voteCount": 1,
        "content": "Agree with D"
      },
      {
        "date": "2022-04-27T23:33:00.000Z",
        "voteCount": 1,
        "content": "Vote D"
      },
      {
        "date": "2022-01-06T15:08:00.000Z",
        "voteCount": 4,
        "content": "vote D"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 113,
    "url": "https://www.examtopics.com/discussions/google/view/69764-exam-professional-cloud-developer-topic-1-question-113/",
    "body": "You are designing a resource-sharing policy for applications used by different teams in a Google Kubernetes Engine cluster. You need to ensure that all applications can access the resources needed to run. What should you do? (Choose two.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSpecify the resource limits and requests in the object specifications.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a namespace for each team, and attach resource quotas to each namespace.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a LimitRange to specify the default compute resource requirements for each namespace.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Kubernetes service account (KSA) for each application, and assign each KSA to the namespace.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Anthos Policy Controller to enforce label annotations on all namespaces. Use taints and tolerations to allow resource sharing for namespaces."
    ],
    "answer": "BC",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BC",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "AB",
        "count": 4,
        "isMostVoted": false
      },
      {
        "answer": "AC",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-01-09T09:26:00.000Z",
        "voteCount": 13,
        "content": "I vote B, C\nhttps://kubernetes.io/docs/concepts/policy/resource-quotas/\nhttps://kubernetes.io/docs/concepts/policy/limit-range/"
      },
      {
        "date": "2022-01-15T10:44:00.000Z",
        "voteCount": 6,
        "content": "Yes agree B, C \nhttps://cloud.google.com/blog/products/containers-kubernetes/kubernetes-best-practices-resource-requests-and-limits"
      },
      {
        "date": "2024-07-19T00:29:00.000Z",
        "voteCount": 1,
        "content": "B. Create a namespace for each team and attach resource quotas to each namespace. This is a fundamental best practice for multi-tenancy in Kubernetes. Namespaces provide isolation and organization, and resource quotas allow you to control the maximum resources that applications within a namespace can consume. This ensures fair resource allocation and prevents one team from monopolizing resources.\nC. Create a LimitRange to specify the default compute resource requirements for each namespace. LimitRanges define minimum and maximum resource limits for objects (like Pods) within a namespace. This helps standardize resource usage and prevents applications from requesting excessive resources."
      },
      {
        "date": "2024-07-19T00:29:00.000Z",
        "voteCount": 1,
        "content": "Let's look at why the other options are less ideal:\n\nA. Specify the resource limits and requests in the object specifications. While this is important for individual Pods, it doesn't provide a centralized way to manage resource sharing across multiple teams.\nD. Create a Kubernetes service account (KSA) for each application and assign each KSA to the namespace. Service accounts are for authentication and authorization, not for resource management.\nE. Use the Anthos Policy Controller to enforce label annotations on all namespaces. Use taints and tolerations to allow resource sharing for namespaces. Anthos Policy Controller is a powerful tool for policy enforcement, but it's not the primary mechanism for managing resource quotas and limits. Taints and tolerations are used for node affinity and scheduling, not for resource allocation."
      },
      {
        "date": "2023-11-25T20:22:00.000Z",
        "voteCount": 1,
        "content": "When it comes to Google's recommended best practices for Kubernetes, especially in the context of Google Kubernetes Engine (GKE), the emphasis is generally placed on setting specific resource requests and limits for each pod and container (Option A). This approach aligns with Kubernetes best practices, as it ensures efficient and reliable operation of applications by maximizing infrastructure utilization and guaranteeing smooth application performance\u200b\u200b.\n\nThis granular level of configuration, where resource requests and limits are explicitly set for each workload, is key to operating applications as efficiently and reliably as possible in Kubernetes clusters. It allows for the classification of pods into different Quality of Service (QoS) classes, such as 'Guaranteed' and 'Burstable', which further aids in resource management and scheduling decisions\u200b\u200b."
      },
      {
        "date": "2023-09-21T07:06:00.000Z",
        "voteCount": 1,
        "content": "Specify the resource limits and requests in the object specifications. This will ensure that each application is allocated the resources it needs to run, and that no application can consume more resources than it was allocated.\nCreate a namespace for each team, and attach resource quotas to each namespace. This will allow you to isolate each team's applications from each other, and to ensure that each team's applications are not consuming more resources than they were allocated."
      },
      {
        "date": "2023-08-05T04:26:00.000Z",
        "voteCount": 2,
        "content": "I go with B and C.  Defining namespaces and Limiting resource quotas for each team in order to avoid resource collisions and hunger."
      },
      {
        "date": "2023-01-10T23:04:00.000Z",
        "voteCount": 1,
        "content": "In the context of the problem statement, B and C are appropriate solution for ensuring that all applications can access the resources needed to run:\n\nB. Create a namespace for each team, and attach resource quotas to each namespace. This way, you can set limits on the resources that a team can consume, so that one team does not consume all the resources of the cluster, and that resources are shared among all teams in a fair way.\n\nC. Create a LimitRange to specify the default compute resource requirements for each namespace. LimitRanges allow you to set default limits and requests for all the pods in a specific namespace, it also ensure that pods in that namespace can never consume more resources than the LimitRange defined.\n\nYou can use a combination of resource limits, quotas, and limit ranges to prevent a single team or application from consuming too many resources, as well as to ensure that all teams and applications have access to the resources they need to run."
      },
      {
        "date": "2023-01-10T23:04:00.000Z",
        "voteCount": 1,
        "content": "Option A: Specify the resource limits and requests in the object specifications, is a valid method for controlling the resources that a pod or container needs, but it may not be sufficient by itself to fully manage the resources in a multi-tenant cluster where multiple teams and applications need to share resources.\n\nWhen you set resource limits and requests on the pod or container level, you have a fine-grained control over the resources that a specific pod or container needs, but it doesn't provide a way to set limits or quotas on the level of a whole team or namespace. It also doesn't provide a default configuration for all pods created in a namespace.\n\nBy itself, this method does not give you the visibility and control you need over the overall resource usage across multiple teams and applications. With creating a namespace per team and attaching quotas, you can limit the resources each team can use, and with LimitRange you can ensure that no pod created in the namespace can go beyond specific limits."
      },
      {
        "date": "2022-12-20T06:59:00.000Z",
        "voteCount": 2,
        "content": "BC is the answer.\n\nhttps://kubernetes.io/docs/concepts/policy/resource-quotas/\nA resource quota, defined by a ResourceQuota object, provides constraints that limit aggregate resource consumption per namespace. It can limit the quantity of objects that can be created in a namespace by type, as well as the total amount of compute resources that may be consumed by resources in that namespace.\n\nhttps://kubernetes.io/docs/concepts/policy/limit-range/\nA LimitRange is a policy to constrain the resource allocations (limits and requests) that you can specify for each applicable object kind (such as Pod or PersistentVolumeClaim) in a namespace."
      },
      {
        "date": "2022-11-21T03:37:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/blog/products/containers-kubernetes/kubernetes-best-practices-resource-requests-and-limits\nAns B,C"
      },
      {
        "date": "2022-08-20T00:09:00.000Z",
        "voteCount": 4,
        "content": "BC are correct"
      },
      {
        "date": "2022-08-16T06:38:00.000Z",
        "voteCount": 3,
        "content": "A&amp;B as obvious !"
      },
      {
        "date": "2022-08-18T07:34:00.000Z",
        "voteCount": 1,
        "content": "why A&amp;B?"
      },
      {
        "date": "2022-08-05T23:07:00.000Z",
        "voteCount": 1,
        "content": "I will go with B and C."
      },
      {
        "date": "2022-07-12T06:56:00.000Z",
        "voteCount": 2,
        "content": "B,C are the right options"
      },
      {
        "date": "2022-04-27T10:33:00.000Z",
        "voteCount": 1,
        "content": "A, B\nLimitRanges are great and all, but they don't actually guarantee that running containers have the resources available that they need, since they're not object-specific."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 114,
    "url": "https://www.examtopics.com/discussions/google/view/69765-exam-professional-cloud-developer-topic-1-question-114/",
    "body": "You are developing a new application that has the following design requirements:<br>\u2711 Creation and changes to the application infrastructure are versioned and auditable.<br>\u2711 The application and deployment infrastructure uses Google-managed services as much as possible.<br>\u2711 The application runs on a serverless compute platform.<br>How should you design the application's architecture?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Store the application and infrastructure source code in a Git repository. 2. Use Cloud Build to deploy the application infrastructure with Terraform. 3. Deploy the application to a Cloud Function as a pipeline step.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Deploy Jenkins from the Google Cloud Marketplace, and define a continuous integration pipeline in Jenkins. 2. Configure a pipeline step to pull the application source code from a Git repository. 3. Deploy the application source code to App Engine as a pipeline step.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a continuous integration pipeline on Cloud Build, and configure the pipeline to deploy the application infrastructure using Deployment Manager templates. 2. Configure a pipeline step to create a container with the latest application source code. 3. Deploy the container to a Compute Engine instance as a pipeline step.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Deploy the application infrastructure using gcloud commands. 2. Use Cloud Build to define a continuous integration pipeline for changes to the application source code. 3. Configure a pipeline step to pull the application source code from a Git repository, and create a containerized application. 4. Deploy the new container on Cloud Run as a pipeline step."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 19,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 14,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-04-16T01:38:00.000Z",
        "voteCount": 6,
        "content": "A is the correct choice.\n\nB - use Jenkins as the deployment tool instead of Cloud Build (The application and deployment infrastructure uses Google-managed services as much as possible).\nC - uses Compute Engine to run containers. CE is not serverless.\nD - we can't version gcloud commands"
      },
      {
        "date": "2023-11-23T11:16:00.000Z",
        "voteCount": 2,
        "content": "Cloud Functions are intended for single-purpose functions, not an entire app.\n\nD is a far better fit here and nobody is talking about \"versioning gcloud commands\" - Cloud Run has revisions (=versions), which meets the task's criteria."
      },
      {
        "date": "2023-01-11T00:17:00.000Z",
        "voteCount": 5,
        "content": "Option D is the best fit for designing the architecture of the new application as it satisfies all the design requirements of versioning and auditing the infrastructure changes, using Google-managed services and deploying the application on a serverless compute platform. The approach includes:\n\n- Deploy the application infrastructure using gcloud commands.\n- Use Cloud Build to define a continuous integration pipeline for changes to the application source code.\n- Configure a pipeline step to pull the application source code from a Git repository, and create a containerized application.\n- Deploy the new container on Cloud Run as a pipeline step.\nIt's worth noting that all options could potentially satisfy the requirements, as long as they use Google-managed services and track infrastructure creation and changes, the choice of different services, platform and tools depend on the specific requirements of your application and development preferences."
      },
      {
        "date": "2023-03-04T01:58:00.000Z",
        "voteCount": 1,
        "content": "What about the versioning aspect?"
      },
      {
        "date": "2023-01-11T00:18:00.000Z",
        "voteCount": 2,
        "content": "Option A: 1. Store the application and infrastructure source code in a Git repository. 2. Use Cloud Build to deploy the application infrastructure with Terraform. 3. Deploy the application to a Cloud Function as a pipeline step, can potentially satisfies the requirement of versioning and auditing the infrastructure changes, but it may not meet the other two requirements of using Google-managed services and deploying the application on a serverless compute platform:\n\n- By using Terraform, which is a third-party infrastructure as code tool, it is not a Google-managed service and it may not have the same level of integration as Google-managed services.\n- Cloud Functions are a serverless compute platform, but it's mainly used to run event-driven, short-lived functions, while it's not a suitable choice for running long running processes, web servers and so on."
      },
      {
        "date": "2023-01-11T00:18:00.000Z",
        "voteCount": 1,
        "content": "In addition, deploying the infrastructure using Terraform, which is not fully integrated with the google cloud, may lead to additional cost and management effort.\nAlso, deploying the application on Cloud Functions may not be able to meet some of the requirements like long running processes, stateful workloads and other requirements that Cloud Run can fulfill."
      },
      {
        "date": "2023-01-11T00:18:00.000Z",
        "voteCount": 1,
        "content": "Although, all of the options may have their own merits and depending on the specific requirement of the application any of them can be suitable, but considering all the requirements stated in the question option D could be the best fit."
      },
      {
        "date": "2024-07-19T07:52:00.000Z",
        "voteCount": 1,
        "content": "D. 1. Deploy the application infrastructure using gcloud commands. 2. Use Cloud Build to define a continuous integration pipeline for changes to the application source code. 3. Configure a pipeline step to pull the application source code from a Git repository and create a containerized application. 4. Deploy the new container on Cloud Run as a pipeline step.\nVersioned and Auditable Infrastructure: While Terraform (option A) is a great choice for infrastructure as code, using gcloud commands directly allows for version control and auditing through your Git repository. This ensures a clear history of infrastructure changes.\nGoogle-Managed Services: Cloud Build, Cloud Run, and Git repositories are all Google-managed services, fulfilling the requirement for using Google-managed services as much as possible.\nServerless Compute: Cloud Run is a serverless platform that perfectly fits the requirement for a serverless compute environment."
      },
      {
        "date": "2024-07-19T07:52:00.000Z",
        "voteCount": 1,
        "content": "Why the other options are less suitable:\n\nA. While Terraform is great for infrastructure as code, it's not as directly integrated with Google Cloud's CI/CD tools as gcloud commands.\nB. Jenkins is a third-party tool, not a Google-managed service. It also doesn't inherently provide the same level of integration with Google Cloud's CI/CD tools as Cloud Build.\nC. Deployment Manager is a good option for infrastructure as code, but it's not as flexible as gcloud commands for direct control. Also, deploying to a Compute Engine instance doesn't meet the serverless requirement."
      },
      {
        "date": "2024-07-13T03:20:00.000Z",
        "voteCount": 1,
        "content": "Correct answer D based on the requirements"
      },
      {
        "date": "2024-07-12T01:12:00.000Z",
        "voteCount": 1,
        "content": "I agree with omermahgoub."
      },
      {
        "date": "2024-04-16T21:19:00.000Z",
        "voteCount": 2,
        "content": "D would be true if &amp; only it didn't mention using gcloud commands to deploy application infrastructure"
      },
      {
        "date": "2023-11-23T11:19:00.000Z",
        "voteCount": 3,
        "content": "I vote D:\n- gcloud, Cloud Build, Cloud Run - are Google-managed services\n- Cloud Run has revisions (=versions)\n- Cloud Run is serverless\n\nA is wrong, as Cloud Functions are intended for single-purpose functions - not an entire app."
      },
      {
        "date": "2023-09-21T07:09:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2023-08-05T04:35:00.000Z",
        "voteCount": 2,
        "content": "My answer is A:\nVersion and auditable: GIT\nGCP managed deployment infrastructure: Cloud build, Cloud Deployment Manager, Terraform\nServerless: Cloud Functions"
      },
      {
        "date": "2023-04-30T21:31:00.000Z",
        "voteCount": 1,
        "content": "It is definitely A vs. B, though.\nI still think the deciding factor is \"Creation and changes to the application infrastructure are versioned and auditable\".\nWhether to deploy to Cloud Run or Cloud Functions is irrelevant because we don't know the contents of the application.\nBoth are serverless."
      },
      {
        "date": "2023-03-04T01:51:00.000Z",
        "voteCount": 3,
        "content": "What put me off A is that at the end there is deploy to ` Cloud Function` and it should be all serverless applications and not just a cloud function, that is what Cloud Run should do."
      },
      {
        "date": "2022-12-20T06:54:00.000Z",
        "voteCount": 1,
        "content": "A is the answer."
      },
      {
        "date": "2022-11-21T03:41:00.000Z",
        "voteCount": 1,
        "content": "Ans A\nhttps://cloud.google.com/docs/ci-cd/products#featured-products-for-cicd"
      },
      {
        "date": "2022-09-23T03:59:00.000Z",
        "voteCount": 2,
        "content": "D is correct, applications should not be deployed on cloud functions."
      },
      {
        "date": "2022-08-20T00:09:00.000Z",
        "voteCount": 4,
        "content": "A is correct"
      },
      {
        "date": "2022-05-18T06:47:00.000Z",
        "voteCount": 1,
        "content": "B and C are not correct (we cant use jenkins in option B, and cant use compute engine as it is  not serverless in option C)\n\nSo in A and D option - \noption A is not right becoz we can deploy on cloud function not suitable as serverless compute \nSo i think Answer is D"
      },
      {
        "date": "2022-05-19T22:30:00.000Z",
        "voteCount": 2,
        "content": "Changing my answer to A, becoz of versioning. can't use gcloud commands in versioning in option D"
      },
      {
        "date": "2022-04-27T23:37:00.000Z",
        "voteCount": 2,
        "content": "Vote A"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 115,
    "url": "https://www.examtopics.com/discussions/google/view/69603-exam-professional-cloud-developer-topic-1-question-115/",
    "body": "You are creating and running containers across different projects in Google Cloud. The application you are developing needs to access Google Cloud services from within Google Kubernetes Engine (GKE). What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAssign a Google service account to the GKE nodes.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a Google service account to run the Pod with Workload Identity.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the Google service account credentials as a Kubernetes Secret.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a Google service account with GKE role-based access control (RBAC)."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-01-21T20:56:00.000Z",
        "voteCount": 7,
        "content": "Option B \nhttps://cloud.google.com/kubernetes-engine/docs/concepts/workload-identity"
      },
      {
        "date": "2024-04-16T21:20:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/kubernetes-engine/docs/concepts/workload-identity#:~:text=Workload%20Identity%20Federation%20for%20GKE%20is%20the%20recommended%20way%20for%20your%20workloads%20running%20on%20Google%20Kubernetes%20Engine%20(GKE)%20to%20access%20Google%20Cloud%20services%20in%20a%20secure%20and%20manageable%20way."
      },
      {
        "date": "2023-09-21T07:14:00.000Z",
        "voteCount": 1,
        "content": "The best way to access Google Cloud services from within Google Kubernetes Engine (GKE) is to use a Google service account to run the Pod with Workload Identity.\n\nWorkload Identity allows your pods to authenticate to Google Cloud services using their Kubernetes service account credentials, without you having to expose any sensitive credentials in your code."
      },
      {
        "date": "2023-08-05T05:15:00.000Z",
        "voteCount": 2,
        "content": "Application images runs as a container within a POD as a process.  So Pod should be identified as a principle here and it should have a service account to access other services within GKE cluster."
      },
      {
        "date": "2023-01-11T00:21:00.000Z",
        "voteCount": 2,
        "content": "In summary, using Workload Identity allows you to authenticate your application to Google Cloud services using the same identity that runs the application, this makes it simple to manage the access and permissions to resources, and also ensures that your application only has the necessary permissions to access the services."
      },
      {
        "date": "2023-01-11T00:20:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is B: Use a Google service account to run the Pod with Workload Identity.\n\nWorkload Identity allows you to authenticate to Google Cloud services using the same identity that runs your application, instead of creating and managing a separate service account. This simplifies the process of granting permissions to your application, and ensures that it only has the necessary access to resources.\n\nWhen you assign a Google service account to GKE nodes (Option A), it can be difficult to manage the permissions needed by the application and also could be a security issue since it grants access to all the services that the service account has permissions to."
      },
      {
        "date": "2023-01-11T00:21:00.000Z",
        "voteCount": 1,
        "content": "When you assign a Google service account to GKE nodes (Option A), it can be difficult to manage the permissions needed by the application and also could be a security issue since it grants access to all the services that the service account has permissions to."
      },
      {
        "date": "2023-01-11T00:21:00.000Z",
        "voteCount": 1,
        "content": "Storing the Google service account credentials as a Kubernetes Secret (Option C) can be a security concern, since the credentials may be easily accessed by unauthorized parties."
      },
      {
        "date": "2023-01-11T00:20:00.000Z",
        "voteCount": 1,
        "content": "Use a Google service account with GKE role-based access control (RBAC) (Option D) is not the recommended approach, while RBAC is good to restrict and manage access to resources, it's not the best fit for authenticating the workloads to access the Google Cloud services."
      },
      {
        "date": "2022-12-20T05:12:00.000Z",
        "voteCount": 1,
        "content": "B is the answer.\n\nhttps://cloud.google.com/kubernetes-engine/docs/concepts/workload-identity#what_is\nApplications running on GKE might need access to Google Cloud APIs such as Compute Engine API, BigQuery Storage API, or Machine Learning APIs.\n\nWorkload Identity allows a Kubernetes service account in your GKE cluster to act as an IAM service account. Pods that use the configured Kubernetes service account automatically authenticate as the IAM service account when accessing Google Cloud APIs. Using Workload Identity allows you to assign distinct, fine-grained identities and authorization for each application in your cluster."
      },
      {
        "date": "2022-08-20T00:10:00.000Z",
        "voteCount": 2,
        "content": "B is correct"
      },
      {
        "date": "2022-08-05T23:33:00.000Z",
        "voteCount": 1,
        "content": "I will go with option B."
      },
      {
        "date": "2022-04-27T23:38:00.000Z",
        "voteCount": 1,
        "content": "Vote B"
      },
      {
        "date": "2022-04-09T07:20:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is B"
      },
      {
        "date": "2022-01-06T15:55:00.000Z",
        "voteCount": 4,
        "content": "vote B"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 116,
    "url": "https://www.examtopics.com/discussions/google/view/69604-exam-professional-cloud-developer-topic-1-question-116/",
    "body": "You have containerized a legacy application that stores its configuration on an NFS share. You need to deploy this application to Google Kubernetes Engine<br>(GKE) and do not want the application serving traffic until after the configuration has been retrieved. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the gsutil utility to copy files from within the Docker container at startup, and start the service using an ENTRYPOINT script.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a PersistentVolumeClaim on the GKE cluster. Access the configuration files from the volume, and start the service using an ENTRYPOINT script.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the COPY statement in the Dockerfile to load the configuration into the container image. Verify that the configuration is available, and start the service using an ENTRYPOINT script.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a startup script to the GKE instance group to mount the NFS share at node startup. Copy the configuration files into the container, and start the service using an ENTRYPOINT script."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 13,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-02-20T04:35:00.000Z",
        "voteCount": 8,
        "content": "It's not necessary to mount NFS to each node in GKE. Just create PVC point to shared NFS, mount to container, and use configuration in ENTRYPOINT. Vote B"
      },
      {
        "date": "2022-02-26T14:08:00.000Z",
        "voteCount": 1,
        "content": "I am not convinced but it does seem to be best option among all options"
      },
      {
        "date": "2024-07-19T07:59:00.000Z",
        "voteCount": 1,
        "content": "The best answer here is B. Create a PersistentVolumeClaim on the GKE cluster. Access the configuration files from the volume and start the service using an ENTRYPOINT script. \n[1] \n\nHere's why:\n\nPersistentVolumeClaim (PVC): A PVC is the Kubernetes way to request storage. By creating a PVC that points to your NFS share, you ensure that your pods have access to the configuration files. This approach is more robust and scalable than copying files at startup.\nENTRYPOINT Script: An ENTRYPOINT script is the ideal way to handle the startup logic. You can use it to:\nMount the PVC volume.\nVerify that the configuration files are present.\nStart your application's main process."
      },
      {
        "date": "2023-09-21T07:15:00.000Z",
        "voteCount": 1,
        "content": "B is correct."
      },
      {
        "date": "2023-08-05T05:26:00.000Z",
        "voteCount": 1,
        "content": "B is more formal and standardized way to mount NFS onto the worker node compared to A where it asks us to create a startup script to mount the volume."
      },
      {
        "date": "2023-08-05T05:22:00.000Z",
        "voteCount": 1,
        "content": "With PersisentVolumeClaim object, we can claim the volume what we need dynamically.  The storage class will be defined by network administrator.  Container/Pod needs to wait until it reads configuration from the mounted volume before serving traffic to its clients."
      },
      {
        "date": "2023-07-19T07:05:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/kubernetes-engine/docs/concepts/persistent-volumes. PersistentVolume resources are used to manage durable storage in a cluster. In GKE, a PersistentVolume is typically backed by a persistent disk. You can also use other storage solutions like NFS. Filestore is a NFS solution on Google Cloud"
      },
      {
        "date": "2023-01-11T00:25:00.000Z",
        "voteCount": 1,
        "content": "B and D are the main candidate answers\n\nOption B: allows the application to be stateless and have no dependencies on the filesystem of the host.\n\nD: is a good solution since it allows the application to access its configuration as soon as the application starts, without having to copy the configuration files into the container.\n\nBut the best option is B, because it allows the application to be stateless and have no dependencies on the filesystem of the host. This approach is more flexible, makes it easy to update the configuration files, and reduces the size of the container image."
      },
      {
        "date": "2023-01-07T09:13:00.000Z",
        "voteCount": 1,
        "content": "B is correct using default tools"
      },
      {
        "date": "2022-12-28T01:37:00.000Z",
        "voteCount": 1,
        "content": "https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes\n\n \nhttps://cloud.google.com/filestore/docs/accessing-fileshares\n\nhttps://cloud.google.com/storage/docs/gcs-fuse"
      },
      {
        "date": "2022-12-20T05:10:00.000Z",
        "voteCount": 1,
        "content": "B is the answer."
      },
      {
        "date": "2022-08-20T00:10:00.000Z",
        "voteCount": 3,
        "content": "B is correct"
      },
      {
        "date": "2022-05-19T18:27:00.000Z",
        "voteCount": 2,
        "content": "I think B is more suitable in this situation"
      },
      {
        "date": "2022-02-28T15:33:00.000Z",
        "voteCount": 2,
        "content": "B and C are valid, but B use native tools of Kubernetes, so is a best practice and easy to implement."
      },
      {
        "date": "2022-02-28T15:34:00.000Z",
        "voteCount": 1,
        "content": "answer is B"
      },
      {
        "date": "2022-01-06T16:13:00.000Z",
        "voteCount": 4,
        "content": "vote C"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 117,
    "url": "https://www.examtopics.com/discussions/google/view/69723-exam-professional-cloud-developer-topic-1-question-117/",
    "body": "Your team is developing a new application using a PostgreSQL database and Cloud Run. You are responsible for ensuring that all traffic is kept private on Google<br>Cloud. You want to use managed services and follow Google-recommended best practices. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Enable Cloud SQL and Cloud Run in the same project. 2. Configure a private IP address for Cloud SQL. Enable private services access. 3. Create a Serverless VPC Access connector. 4. Configure Cloud Run to use the connector to connect to Cloud SQL.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Install PostgreSQL on a Compute Engine virtual machine (VM), and enable Cloud Run in the same project. 2. Configure a private IP address for the VM. Enable private services access. 3. Create a Serverless VPC Access connector. 4. Configure Cloud Run to use the connector to connect to the VM hosting PostgreSQL.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Use Cloud SQL and Cloud Run in different projects. 2. Configure a private IP address for Cloud SQL. Enable private services access. 3. Create a Serverless VPC Access connector. 4. Set up a VPN connection between the two projects. Configure Cloud Run to use the connector to connect to Cloud SQL.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Install PostgreSQL on a Compute Engine VM, and enable Cloud Run in different projects. 2. Configure a private IP address for the VM. Enable private services access. 3. Create a Serverless VPC Access connector. 4. Set up a VPN connection between the two projects. Configure Cloud Run to use the connector to access the VM hosting PostgreSQL"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 14,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-16T21:26:00.000Z",
        "voteCount": 1,
        "content": "A is correct.\n\nB &amp; D is rejected easily\nC: its adding unnecessary complexity by putting them in  different project. Why would be even do that"
      },
      {
        "date": "2024-01-14T06:46:00.000Z",
        "voteCount": 1,
        "content": "Should be B, your app is PostgresSQL + Cloud Run, uses Cloud SQL is change the premise"
      },
      {
        "date": "2024-01-14T06:51:00.000Z",
        "voteCount": 1,
        "content": "mmm well, I think its A because CloudSQL can be postgresql"
      },
      {
        "date": "2023-09-21T07:17:00.000Z",
        "voteCount": 1,
        "content": "I would go with A."
      },
      {
        "date": "2023-08-05T05:33:00.000Z",
        "voteCount": 2,
        "content": "The key here is, Google Managed services and follow Google-recommended best practices: Definitely Cloud SQL instead of Postgres SQL that is almost an unmanaged service managed by custom configurations set by customers."
      },
      {
        "date": "2023-01-11T00:27:00.000Z",
        "voteCount": 1,
        "content": "The answer would be A. By using Cloud SQL and Cloud Run in the same project, you can take advantage of the built-in security features and managed services provided by Google Cloud. By configuring a private IP address for Cloud SQL and enabling private services access, you can ensure that all traffic is kept private. You can also create a Serverless VPC Access connector and configure Cloud Run to use this connector to connect to Cloud SQL. This configuration will allow your application to connect to the database securely and privately, following Google-recommended best practices."
      },
      {
        "date": "2022-12-20T05:08:00.000Z",
        "voteCount": 1,
        "content": "A is the answer.\n\nhttps://cloud.google.com/vpc/docs/serverless-vpc-access\nServerless VPC Access makes it possible for you to connect directly to your Virtual Private Cloud network from serverless environments such as Cloud Run, App Engine, or Cloud Functions. Configuring Serverless VPC Access allows your serverless environment to send requests to your VPC network using internal DNS and internal IP addresses (as defined by RFC 1918 and RFC 6598). The responses to these requests also use your internal network."
      },
      {
        "date": "2022-11-21T04:38:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/sql/docs/postgres/connect-run#configure\nhttps://cloud.google.com/sql/docs/postgres/connect-run#private-ip\nAnswer A"
      },
      {
        "date": "2022-08-20T00:10:00.000Z",
        "voteCount": 2,
        "content": "A is correct"
      },
      {
        "date": "2022-08-05T23:50:00.000Z",
        "voteCount": 1,
        "content": "Options C and D are crossed out as they suggest using different projects.\nTo choose between option A and B, why should we install PostgreSQL explicitly, if it is already present in CloudSQL. So, I will go with CloudSQL whiz Option A."
      },
      {
        "date": "2022-04-27T23:41:00.000Z",
        "voteCount": 2,
        "content": "Vote A"
      },
      {
        "date": "2022-01-15T20:34:00.000Z",
        "voteCount": 3,
        "content": "Option A is best option"
      },
      {
        "date": "2022-01-09T09:00:00.000Z",
        "voteCount": 1,
        "content": "I vote A\nhttps://cloud.google.com/sql/docs/postgres/connect-run#private-ip"
      },
      {
        "date": "2022-01-09T01:33:00.000Z",
        "voteCount": 2,
        "content": "Correct option is A"
      },
      {
        "date": "2022-01-09T01:32:00.000Z",
        "voteCount": 2,
        "content": "I will go for option A, as mentioned in the question, managed services to be used, in this case using a Cloud SQL would be correct. If a PostgreSQL is installed in a compute engine, then it will be customer managed rather google service managed."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 118,
    "url": "https://www.examtopics.com/discussions/google/view/69762-exam-professional-cloud-developer-topic-1-question-118/",
    "body": "You are developing an application that will allow clients to download a file from your website for a specific period of time. How should you design the application to complete this task while following Google-recommended best practices?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the application to send the file to the client as an email attachment.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGenerate and assign a Cloud Storage-signed URL for the file. Make the URL available for the client to download.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a temporary Cloud Storage bucket with time expiration specified, and give download permissions to the bucket. Copy the file, and send it to the client.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGenerate the HTTP cookies with time expiration specified. If the time is valid, copy the file from the Cloud Storage bucket, and make the file available for the client to download."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-01-15T20:36:00.000Z",
        "voteCount": 7,
        "content": "Agree with Option B"
      },
      {
        "date": "2022-01-09T09:01:00.000Z",
        "voteCount": 5,
        "content": "Yes, I vote B"
      },
      {
        "date": "2024-04-16T21:28:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/storage/docs/access-control/signed-urls#:~:text=X%2DGoog%2DDate%3A%20The,604800%20seconds%20(7%20days)"
      },
      {
        "date": "2023-09-21T07:18:00.000Z",
        "voteCount": 1,
        "content": "B is correct."
      },
      {
        "date": "2023-01-11T00:28:00.000Z",
        "voteCount": 2,
        "content": "B. Generate and assign a Cloud Storage-signed URL for the file. Make the URL available for the client to download.\nThe best approach is to use a Cloud Storage signed URL, which allows you to give time-limited read access to a specific file in your bucket. Once the URL is generated, it can be shared with the client to download the file. This approach provides an easy way to control access to your files, and allows you to revoke access at any time by simply invalidating the URL. It also ensures that the file is stored and served securely via Cloud Storage and is durable, highly available and performant way to serve files."
      },
      {
        "date": "2022-12-20T05:06:00.000Z",
        "voteCount": 1,
        "content": "B is the answer.\n\nhttps://cloud.google.com/storage/docs/access-control/signed-urls\nA signed URL is a URL that provides limited permission and time to make a request. Signed URLs contain authentication information in their query string, allowing users without credentials to perform specific actions on a resource. When you generate a signed URL, you specify a user or service account which must have sufficient permission to make the request that the signed URL will make. After you generate a signed URL, anyone who possesses it can use the signed URL to perform specified actions, such as reading an object, within a specified period of time."
      },
      {
        "date": "2022-08-20T00:11:00.000Z",
        "voteCount": 2,
        "content": "B is correct"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 119,
    "url": "https://www.examtopics.com/discussions/google/view/70092-exam-professional-cloud-developer-topic-1-question-119/",
    "body": "Your development team has been asked to refactor an existing monolithic application into a set of composable microservices. Which design aspects should you implement for the new application? (Choose two.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDevelop the microservice code in the same programming language used by the microservice caller.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an API contract agreement between the microservice implementation and microservice caller.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRequire asynchronous communications between all microservice implementations and microservice callers.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure that sufficient instances of the microservice are running to accommodate the performance requirements.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement a versioning scheme to permit future changes that could be incompatible with the current interface."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 20,
        "isMostVoted": true
      },
      {
        "answer": "E",
        "count": 8,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-04-27T23:43:00.000Z",
        "voteCount": 9,
        "content": "Vote BE"
      },
      {
        "date": "2024-07-19T08:20:00.000Z",
        "voteCount": 1,
        "content": "The two design aspects you should implement for refactoring a monolithic application into microservices are:\n\nB. Create an API contract agreement between the microservice implementation and microservice caller. This is crucial for microservices to communicate effectively. A well-defined API contract ensures that changes to one service don't break other services that depend on it. This promotes independent development and deployment.\nE. Implement a versioning scheme to permit future changes that could be incompatible with the current interface. Microservices are expected to evolve independently. Versioning allows for backward compatibility and graceful transitions when changes are made to the API. This prevents breaking changes and ensures smooth integration."
      },
      {
        "date": "2024-07-19T08:20:00.000Z",
        "voteCount": 1,
        "content": "Let's look at why the other options are less critical:\n\nA. Develop the microservice code in the same programming language used by the microservice caller. While using the same language can simplify initial development, it's not a strict requirement. Microservices can be written in different languages as long as they adhere to the API contract.\nC. Require asynchronous communications between all microservice implementations and microservice callers. Asynchronous communication is often beneficial for microservices, but it's not always necessary. Synchronous communication can be appropriate in some cases, especially for tightly coupled services.\nD. Ensure that sufficient instances of the microservice are running to accommodate the performance requirements. This is important for scaling and performance, but it's more of an operational concern than a design aspect. It's handled through deployment strategies and monitoring."
      },
      {
        "date": "2024-04-16T21:31:00.000Z",
        "voteCount": 2,
        "content": "B &amp; E is the answer"
      },
      {
        "date": "2023-09-21T07:21:00.000Z",
        "voteCount": 2,
        "content": "BD : is the correct one here.\nCreate an API contract agreement between the microservice implementation and microservice caller. This will help to ensure that the microservices are decoupled from each other, and that the caller can be updated without having to update the implementation.\nEnsure that sufficient instances of the microservice are running to accommodate the performance requirements. This is important because microservices are often scaled independently, and you need to make sure that each microservice can handle the expected load."
      },
      {
        "date": "2023-08-05T05:41:00.000Z",
        "voteCount": 1,
        "content": "Versioning schema is the evolutionary process in API development.\nFor eg: \n/v1/users/1234\n/v2/users/1234"
      },
      {
        "date": "2023-08-05T05:39:00.000Z",
        "voteCount": 1,
        "content": "API contract design is the first step in design-first approach while creating APIs (REST). We can also follow code-first approach while providing solution via public APIs.  Design first approach is more flexible and provides sufficient amount of time for the dev team to gather requirements and to understand what customer really wants."
      },
      {
        "date": "2023-02-21T02:13:00.000Z",
        "voteCount": 1,
        "content": "D &amp; E\uff0c\nB and E are same thing. D is considering capacity of micro service."
      },
      {
        "date": "2023-01-11T00:30:00.000Z",
        "voteCount": 1,
        "content": "B. Guarantees that the two parties are communicating in a well-defined way, which makes the microservices more flexible, composable, and easy to understand.\n\nE. Allows to make changes to the service's API while still maintaining backward compatibility. With versioning, new and old consumers can continue to use the service without interruption as new features are added.\n\nOn the other hand, developing the microservice code in the same programming language as the microservice caller does not promote loose coupling, and it may also increase the complexity of the system as it will depend on language-specific features. Asynchronous communications are also not always necessary and depend on the use case and requirement. Ensuring sufficient instances of the microservice are running can be done by using a scalability strategy such as Auto-scaling, and this is not a specific design aspect."
      },
      {
        "date": "2023-01-07T09:25:00.000Z",
        "voteCount": 1,
        "content": "BE with the recommendation on https://cloud.google.com/architecture/migrating-a-monolithic-app-to-microservices-gke"
      },
      {
        "date": "2022-12-20T05:04:00.000Z",
        "voteCount": 2,
        "content": "BE is the answer.\n\nhttps://cloud.google.com/architecture/migrating-a-monolithic-app-to-microservices-gke#api_contracts\nEach microservice should be invoked only from a set of interfaces. Each interface should in turn be clearly defined by a contract that can be implemented using an API definition language like the OpenAPI Initiative specification or RAML. Having well-defined API contracts and interfaces allows you to develop tests as a main component of your solution (for example, by applying test-driven development) against these API interfaces.\n\nhttps://cloud.google.com/architecture/migrating-a-monolithic-app-to-microservices-gke#versioning\nTo give you flexibility in managing updates that might break existing clients, you should implement a versioning scheme for your microservices. Versioning lets you deploy updated versions of a microservice without affecting the clients that are using an existing version."
      },
      {
        "date": "2022-11-21T05:00:00.000Z",
        "voteCount": 1,
        "content": "1. When you incrementally migrate services, configure communication between services and monolith to go through well-defined API contracts. Answer B\n\n2. https://cloud.google.com/architecture/microservices-architecture-refactoring-monoliths#design_interservice_communication Ans C\nAns B,C"
      },
      {
        "date": "2022-11-21T05:02:00.000Z",
        "voteCount": 1,
        "content": "To support C\nhttps://cloud.google.com/architecture/microservices-architecture-interservice-communication#logical_separation_of_service\nIn this document, you isolate the payment service from the rest of the application. All flows in the original Online Boutique application are synchronous. \"In the refactored application, the payment process is converted to an asynchronous flow. Therefore, when you receive a purchase request, instead of processing it immediately, you provide a \"request received\" confirmation to the user. In the background, an asynchronous request is triggered to the payment service to process the payment.\""
      },
      {
        "date": "2022-11-21T05:12:00.000Z",
        "voteCount": 1,
        "content": "To support B\n*When you incrementally migrate services, configure communication between services and monolith to go through well-defined API contracts.\nhttps://cloud.google.com/architecture/microservices-architecture-refactoring-monoliths"
      },
      {
        "date": "2022-11-21T05:17:00.000Z",
        "voteCount": 1,
        "content": "I personally do not see or find anything supporting E at all..."
      },
      {
        "date": "2022-08-20T00:12:00.000Z",
        "voteCount": 4,
        "content": "BE are correct"
      },
      {
        "date": "2022-07-13T05:59:00.000Z",
        "voteCount": 4,
        "content": "Note to admin: change this question's answer to reflect a 2 answer selection"
      },
      {
        "date": "2022-07-11T23:23:00.000Z",
        "voteCount": 3,
        "content": "B &amp; E seems best"
      },
      {
        "date": "2022-04-04T19:02:00.000Z",
        "voteCount": 2,
        "content": "B and D"
      },
      {
        "date": "2022-05-28T13:08:00.000Z",
        "voteCount": 1,
        "content": "For me to ensure that sufficient instances of the microservice are running to accommodate the performance requirements is not needed because of the big variety of services that provide autoscaling. \nBE"
      },
      {
        "date": "2022-03-03T09:21:00.000Z",
        "voteCount": 2,
        "content": "Agree with B and E.\nhttps://cloud.google.com/appengine/docs/standard/java/designing-microservice-api#using_strong_contracts"
      },
      {
        "date": "2022-02-26T15:55:00.000Z",
        "voteCount": 2,
        "content": "This one is tricky one as C and E could be better option as well but there was no mention about reliability\u2026"
      },
      {
        "date": "2022-02-26T15:55:00.000Z",
        "voteCount": 4,
        "content": "I will agree with B and E"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 120,
    "url": "https://www.examtopics.com/discussions/google/view/70093-exam-professional-cloud-developer-topic-1-question-120/",
    "body": "You deployed a new application to Google Kubernetes Engine and are experiencing some performance degradation. Your logs are being written to Cloud<br>Logging, and you are using a Prometheus sidecar model for capturing metrics. You need to correlate the metrics and data from the logs to troubleshoot the performance issue and send real-time alerts while minimizing costs. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate custom metrics from the Cloud Logging logs, and use Prometheus to import the results using the Cloud Monitoring REST API.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExport the Cloud Logging logs and the Prometheus metrics to Cloud Bigtable. Run a query to join the results, and analyze in Google Data Studio.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExport the Cloud Logging logs and stream the Prometheus metrics to BigQuery. Run a recurring query to join the results, and send notifications using Cloud Tasks.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExport the Prometheus metrics and use Cloud Monitoring to view them as external metrics. Configure Cloud Monitoring to create log-based metrics from the logs, and correlate them with the Prometheus data.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-09-21T07:30:00.000Z",
        "voteCount": 1,
        "content": "This option is the most cost-effective because it does not require you to export any data to Bigtable or BigQuery. It is also the most efficient option because it allows you to correlate the metrics and logs in real time."
      },
      {
        "date": "2023-08-05T05:54:00.000Z",
        "voteCount": 1,
        "content": "Correlate Promethius metrics and Cloud logging logs: We need to compare these two logs. Promethius is an external metrics which can be a library dependency used in the application.  To compare Apple vs apple, we need to bring Prometheus metrics to GCP and should configure Cloud monitoring to treat them as an external metric."
      },
      {
        "date": "2023-01-11T00:32:00.000Z",
        "voteCount": 1,
        "content": "This option allows you to use Cloud Monitoring to view the Prometheus metrics and create log-based metrics from the logs. This allows you to correlate the metrics and logs in one place. By using Cloud Monitoring, you can also set up alerting rules and dashboards which can help you to identify and troubleshoot the performance issues in real-time and with low costs.\nIt's not necessary to export the data to another storage to perform the correlation and to set up notifications, it can all be done directly in the Cloud Monitoring, taking advantage of its features."
      },
      {
        "date": "2023-01-11T00:32:00.000Z",
        "voteCount": 1,
        "content": "D. Export the Prometheus metrics and use Cloud Monitoring to view them as external metrics. Configure Cloud Monitoring to create log-based metrics from the logs, and correlate them with the Prometheus data."
      },
      {
        "date": "2022-12-19T07:30:00.000Z",
        "voteCount": 1,
        "content": "D is the answer.\n\nhttps://cloud.google.com/stackdriver/docs/solutions/gke/prometheus#viewing_metrics"
      },
      {
        "date": "2022-08-20T00:12:00.000Z",
        "voteCount": 2,
        "content": "D is correc"
      },
      {
        "date": "2022-08-06T00:49:00.000Z",
        "voteCount": 1,
        "content": "D is a fair choice here!"
      },
      {
        "date": "2022-07-19T03:46:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/stackdriver/docs/solutions/gke/prometheus#viewing_metrics"
      },
      {
        "date": "2022-04-09T07:31:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer is D"
      },
      {
        "date": "2022-02-26T16:01:00.000Z",
        "voteCount": 1,
        "content": "I agree with D as well \u2026 looking for minimizing the costs = use Cloud Monitoring which has alerting bult-in"
      },
      {
        "date": "2022-01-15T20:40:00.000Z",
        "voteCount": 4,
        "content": "Agree with Option D"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 121,
    "url": "https://www.examtopics.com/discussions/google/view/69758-exam-professional-cloud-developer-topic-1-question-121/",
    "body": "You have been tasked with planning the migration of your company's application from on-premises to Google Cloud. Your company's monolithic application is an ecommerce website. The application will be migrated to microservices deployed on Google Cloud in stages. The majority of your company's revenue is generated through online sales, so it is important to minimize risk during the migration. You need to prioritize features and select the first functionality to migrate. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the Product catalog, which has integrations to the frontend and product database.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate Payment processing, which has integrations to the frontend, order database, and third-party payment vendor.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate Order fulfillment, which has integrations to the order database, inventory system, and third-party shipping vendor.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the Shopping cart, which has integrations to the frontend, cart database, inventory system, and payment processing system."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 11,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-02-26T17:32:00.000Z",
        "voteCount": 5,
        "content": "I agree with Option A, as the question states \u201cfirst one\u201d"
      },
      {
        "date": "2024-07-21T23:46:00.000Z",
        "voteCount": 1,
        "content": "Minimal Risk: Migrating the product catalog first minimizes risk because it's a foundational component of your ecommerce website. It doesn't directly impact the core revenue-generating processes like payment or order fulfillment.\nIndependent Functionality: The product catalog is relatively independent, with integrations primarily to the frontend and product database. This makes it easier to isolate and migrate without disrupting other critical parts of the application.\nEarly Validation: Migrating the product catalog allows you to validate your migration process, test your infrastructure, and gain experience with Google Cloud before moving on to more complex functionalities.\nFoundation for Future Migrations: A successful product catalog migration sets the stage for future migrations of more complex features. It provides a solid foundation for building out your microservices architecture on Google Cloud."
      },
      {
        "date": "2024-07-21T23:46:00.000Z",
        "voteCount": 1,
        "content": "Why other options are less ideal:\n\nB. Payment Processing: Migrating payment processing first carries a high risk of disrupting revenue generation. It involves multiple integrations and dependencies, making it more complex and prone to errors.\nC. Order Fulfillment: Similar to payment processing, order fulfillment is a critical revenue-generating process. Migrating it first would introduce significant risk and potential disruption to customer orders.\nD. Shopping Cart: The shopping cart is a complex feature with multiple integrations, including payment processing and inventory. Migrating it first would be challenging and risky, potentially impacting customer checkout experiences."
      },
      {
        "date": "2023-09-21T07:36:00.000Z",
        "voteCount": 1,
        "content": "I will go with A as it has less dependencies."
      },
      {
        "date": "2023-08-05T06:02:00.000Z",
        "voteCount": 1,
        "content": "A should be the first MVP item in the list.\nKey is to avoid risks in the initial stages of transition."
      },
      {
        "date": "2023-01-11T00:39:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/architecture/migrating-a-monolithic-app-to-microservices-gke#example_migrating_a_shopping_cart\n\nBased on the guide referenced, the answer would be D. Migrate the Shopping cart, which has integrations to the frontend, cart database, inventory system, and payment processing system. The guide recommends migrating functionality with the least dependencies and level of complexity first, which the shopping cart functionality has fewer dependencies and less complexity than the other options presented. This will minimize risk while still providing value to the business and allowing further migration of more complex functionality."
      },
      {
        "date": "2023-01-11T00:39:00.000Z",
        "voteCount": 1,
        "content": "Migrating the product catalog first may be a good option if the product catalog is a separate service that doesn't rely on other services and can be deployed independently. However, if it is closely tied to other services such as the frontend or product database, migrating it first may introduce complexity and increase the risk of the migration, as the other dependent services would need to be migrated or integrated with the new product catalog service in parallel."
      },
      {
        "date": "2022-12-19T07:27:00.000Z",
        "voteCount": 1,
        "content": "A is the answer.\n\nhttps://cloud.google.com/architecture/migrating-a-monolithic-app-to-microservices-gke#choosing_an_initial_migration_effort"
      },
      {
        "date": "2022-11-21T05:25:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/architecture/migrating-a-monolithic-app-to-microservices-gke#example_migrating_a_shopping_cart\nAnswer A"
      },
      {
        "date": "2022-09-22T00:27:00.000Z",
        "voteCount": 2,
        "content": "I don't agree with option A. Google docs says:\n\n\"When you plan your migration, it's tempting to start with features that are trivial to migrate. This might represent a quick win, but might not be the best learning experience for your team. Instead of going straight to the migration, you should spend time evaluating all of the features and create plans for their migration.\"\n\n\"According to this evaluation framework, the ideal candidate for the initial migration effort should be challenging enough to be meaningful, but simple enough to minimize the risk of failure. The initial migration process should also:\n\nRequire little refactoring, considering both the feature itself and the related business processes.\nBe stateless\u2014that is, have no external data requirements.\nHave few or no dependencies.\"\n\nI think it's between options B &amp; C since the third-party vendors already have a microservices architecture going on.\n\nhttps://cloud.google.com/architecture/migrating-a-monolithic-app-to-microservices-gke#:~:text=When%20you%20plan,for%20their%20migration."
      },
      {
        "date": "2022-11-06T04:58:00.000Z",
        "voteCount": 2,
        "content": "as it says in your link:\nA migration plan example\n\nThe following list shows an example of a migration order:\n\nPlatform frontend; that is, the user interface\nStateless features, such as a currency-conversion service\nFeatures with independent datasets (datasets that have no dependencies on other datasets), such as a service to list your brick-and-mortar stores\nFeatures with shared datasets\u2014the business logic of the ecommerce platform\n\n\nso the first one should be the user interface = product catalogue"
      },
      {
        "date": "2022-08-20T00:13:00.000Z",
        "voteCount": 2,
        "content": "A is  correct"
      },
      {
        "date": "2022-05-17T18:33:00.000Z",
        "voteCount": 1,
        "content": "A is correct."
      },
      {
        "date": "2022-04-05T07:51:00.000Z",
        "voteCount": 2,
        "content": "I will vote C. It is the only service where a temporary disruption will not impact all sales on the website (because it is not embedded in the frontend)."
      },
      {
        "date": "2022-01-16T05:40:00.000Z",
        "voteCount": 2,
        "content": "Agree Option A , in order to keep the disruption as minimum as possible by migrating minimum features"
      },
      {
        "date": "2022-01-09T08:24:00.000Z",
        "voteCount": 3,
        "content": "Yes, I vote A"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 122,
    "url": "https://www.examtopics.com/discussions/google/view/69760-exam-professional-cloud-developer-topic-1-question-122/",
    "body": "Your team develops services that run on Google Kubernetes Engine. Your team's code is stored in Cloud Source Repositories. You need to quickly identify bugs in the code before it is deployed to production. You want to invest in automation to improve developer feedback and make the process as efficient as possible.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Spinnaker to automate building container images from code based on Git tags.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Build to automate building container images from code based on Git tags.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Spinnaker to automate deploying container images to the production environment.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Build to automate building container images from code based on forked versions."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 13,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-09-21T07:40:00.000Z",
        "voteCount": 1,
        "content": "B is correct."
      },
      {
        "date": "2023-08-05T06:08:00.000Z",
        "voteCount": 1,
        "content": "We need to have the following steps in Cloud Build to identify bugs\"\n1) Static code analysis\n2) Code Vulnerability scanning\n3) Docker Image vulnerability scanning\nUsing Grype is one such example."
      },
      {
        "date": "2023-07-03T06:00:00.000Z",
        "voteCount": 1,
        "content": "I say D.\n\nBoth B and D could work. However why not use traditional git workflows and keep separate branches for separate tasks. This way each image can be tested independently."
      },
      {
        "date": "2023-01-11T00:41:00.000Z",
        "voteCount": 1,
        "content": "Option B is appropriate because it uses Cloud Build, a service that can automatically build container images from code stored in Cloud Source Repositories based on Git tags. This allows developers to quickly identify bugs in their code before it is deployed to production, by automating the building process and improving developer feedback.\n\nOption A uses Spinnaker, which is a multi-cloud continuous delivery platform that can automate building, testing, and deploying container images. However, it does not specifically mention using git tags to trigger builds, thus for this particular use case it might not be the best fit."
      },
      {
        "date": "2023-01-11T00:41:00.000Z",
        "voteCount": 1,
        "content": "Option C also uses Spinnaker, and is similar to A, Spinnaker can automate deploying container images to the production environment, but it's not specific to building and identifying bugs before deploying, so it might not be the best fit for this use case.\n\nOption D uses Cloud Build, but it's not specific to building images based on git tags, it's more general and focuses on building images based on forked versions, which might not be needed in this case."
      },
      {
        "date": "2022-12-19T07:23:00.000Z",
        "voteCount": 1,
        "content": "B is the answer."
      },
      {
        "date": "2022-11-14T05:02:00.000Z",
        "voteCount": 2,
        "content": "D ) You need to quickly identify bugs in the code before it is deployed to production. So it's for developer to fork the code and test the build. Later they can push the changes using PR to the master repo/branch."
      },
      {
        "date": "2022-09-14T03:35:00.000Z",
        "voteCount": 2,
        "content": "cloud build is the google service so it stands to reason to use that."
      },
      {
        "date": "2022-08-20T00:13:00.000Z",
        "voteCount": 3,
        "content": "I think b is correct"
      },
      {
        "date": "2022-04-05T11:20:00.000Z",
        "voteCount": 1,
        "content": "I vote D. Because every developer before merge to the master should test build in his branch. It will expose bugs. Once branch merged to the master, master build pipeline comes up."
      },
      {
        "date": "2022-02-26T19:28:00.000Z",
        "voteCount": 4,
        "content": "I would disagree with A as Spinnaker is for deployment not for building images \n\nSo either B or C : C is stating to deploy to production but the question is to give feedback to developer before it goes to production \n\nSo the only close answer is B but it is half answer \n\nPerhaps choice A was poorly written instead deploy build then it could be A"
      },
      {
        "date": "2022-02-26T19:29:00.000Z",
        "voteCount": 1,
        "content": "I mean instead of deploy it was typed incorrect as build for A"
      },
      {
        "date": "2022-01-09T08:41:00.000Z",
        "voteCount": 3,
        "content": "I vote B"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 123,
    "url": "https://www.examtopics.com/discussions/google/view/69759-exam-professional-cloud-developer-topic-1-question-123/",
    "body": "Your team is developing an application in Google Cloud that executes with user identities maintained by Cloud Identity. Each of your application's users will have an associated Pub/Sub topic to which messages are published, and a Pub/Sub subscription where the same user will retrieve published messages. You need to ensure that only authorized users can publish and subscribe to their own specific Pub/Sub topic and subscription. What should you do?<br><img src=\"/assets/media/exam-media/04137/0008100001.jpg\" class=\"in-exam-image\"><br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBind the user identity to the pubsub.publisher and pubsub.subscriber roles at the resource level.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGrant the user identity the pubsub.publisher and pubsub.subscriber roles at the project level.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGrant the user identity a custom role that contains the pubsub.topics.create and pubsub.subscriptions.create permissions.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the application to run as a service account that has the pubsub.publisher and pubsub.subscriber roles."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 12,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-09-21T07:42:00.000Z",
        "voteCount": 1,
        "content": "A is correct."
      },
      {
        "date": "2023-08-05T06:12:00.000Z",
        "voteCount": 1,
        "content": "Granting IAM at resource level is enough.\nIf project level permission is given then user will be having publisher and subscriber roles for all the pub-sub topics created within the project.  So this should be avoided according to the question asked."
      },
      {
        "date": "2023-02-27T06:30:00.000Z",
        "voteCount": 1,
        "content": "A -&gt; resource level"
      },
      {
        "date": "2023-01-11T00:43:00.000Z",
        "voteCount": 2,
        "content": "A. Bind the user identity to the pubsub.publisher and pubsub.subscriber roles at the resource level.\n\nBy binding the user identity to the pubsub.publisher and pubsub.subscriber roles at the resource level, you can ensure that each user can only publish and subscribe to their specific Pub/Sub topic and subscription. This allows for granular permissions management and ensures that each user can only access the resources they are authorized to.\n\nThe other options are not suitable in this case because,"
      },
      {
        "date": "2023-01-11T00:43:00.000Z",
        "voteCount": 2,
        "content": "B. Granting the user identity the pubsub.publisher and pubsub.subscriber roles at the project level would give the user access to all topics and subscriptions within the project and not specific to a user.\n\nC. Granting the user identity a custom role that contains the pubsub.topics.create and pubsub.subscriptions.create permissions would allow user to create topics and subscriptions but not access to their specific topic or subscription.\n\nD. Configuring the application to run as a service account that has the pubsub.publisher and pubsub.subscriber roles would not provide granular permissions management for the user."
      },
      {
        "date": "2023-02-03T00:53:00.000Z",
        "voteCount": 2,
        "content": "why do you write all these compositions , you write unneccesary paragraphs always, as if we dnto have documents and often times you will be giving wrong explanations. i beleve just pasting a link to supoort your answer is enough as well have access to the documentation"
      },
      {
        "date": "2023-02-03T00:54:00.000Z",
        "voteCount": 1,
        "content": "believe"
      },
      {
        "date": "2022-12-19T07:19:00.000Z",
        "voteCount": 1,
        "content": "A is the answer."
      },
      {
        "date": "2022-08-20T00:14:00.000Z",
        "voteCount": 2,
        "content": "A is correct"
      },
      {
        "date": "2022-08-06T01:09:00.000Z",
        "voteCount": 2,
        "content": "I think it should be option A since the authorization should be at the user level for a specific resource."
      },
      {
        "date": "2022-04-28T00:16:00.000Z",
        "voteCount": 2,
        "content": "Vote A"
      },
      {
        "date": "2022-04-05T11:23:00.000Z",
        "voteCount": 2,
        "content": "I would choose D"
      },
      {
        "date": "2022-01-09T08:40:00.000Z",
        "voteCount": 3,
        "content": "I vote A"
      },
      {
        "date": "2022-01-21T21:05:00.000Z",
        "voteCount": 4,
        "content": "Agree with Option A"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 124,
    "url": "https://www.examtopics.com/discussions/google/view/70094-exam-professional-cloud-developer-topic-1-question-124/",
    "body": "You are evaluating developer tools to help drive Google Kubernetes Engine adoption and integration with your development environment, which includes VS Code and IntelliJ. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Code to develop applications.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Cloud Shell integrated Code Editor to edit code and configuration files.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a Cloud Notebook instance to ingest and process data and deploy models.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Shell to manage your infrastructure and applications from the command line."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 13,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-22T00:09:00.000Z",
        "voteCount": 1,
        "content": "Direct Integration with IDEs: Cloud Code is specifically designed to integrate with popular IDEs like VS Code and IntelliJ. This provides a seamless development experience within your familiar environment.\nKubernetes-Focused Features: Cloud Code offers features tailored for Kubernetes development, including:\nBuild and Deployment: Automate container image building, deployment to GKE, and integration with CI/CD tools like Cloud Build.\nDebugging and Monitoring: Debug applications running on Kubernetes clusters directly from your IDE, and monitor their performance and logs.\nYAML Editing: Provides assistance for editing Kubernetes YAML files, including validation, linting, and auto-completion.\nStreamlined Workflow: Cloud Code helps streamline your workflow by bringing all the necessary tools and services for Kubernetes development into your IDE, reducing context switching and improving productivity."
      },
      {
        "date": "2024-07-22T00:10:00.000Z",
        "voteCount": 1,
        "content": "Why other options are less ideal:\n\nB. Cloud Shell Integrated Code Editor: While Cloud Shell is a useful tool for managing infrastructure and running commands, its integrated code editor is not as feature-rich or integrated with Kubernetes development as Cloud Code.\nC. Cloud Notebook Instance: Cloud Notebooks are excellent for data science and machine learning tasks, but they are not the primary tool for developing and deploying applications on Kubernetes.\nD. Cloud Shell for Command Line Management: Cloud Shell is great for managing infrastructure and running commands, but it doesn't provide the same level of IDE integration and Kubernetes-specific features as Cloud Code."
      },
      {
        "date": "2023-09-21T07:45:00.000Z",
        "voteCount": 1,
        "content": "A is correct."
      },
      {
        "date": "2023-08-05T06:27:00.000Z",
        "voteCount": 2,
        "content": "Google Cloud code plugin can be installed on Intellij and VS code IDEs.\nThis provides very flexibility for developer to work with GKE platform."
      },
      {
        "date": "2023-07-04T07:18:00.000Z",
        "voteCount": 1,
        "content": "Cloud Code is a set of plugins for VS Code and IntelliJ that provides an integrated development experience for working with Kubernetes and Google Cloud"
      },
      {
        "date": "2023-01-11T00:45:00.000Z",
        "voteCount": 1,
        "content": "A. Use Cloud Code to develop applications.\n\nCloud Code is a set of plugins for VS Code and IntelliJ that provides an integrated development experience for working with Kubernetes and Google Cloud. The plugins include features such as interactive cluster and resource management, one-click Kubernetes cluster creation, and built-in debugging and diagnostics. It also supports to quickly deploy and debug applications using the Kubernetes and Google Cloud SDKs, Also, it allows developers to easily perform tasks like deploying and debugging applications, managing resources, and running local development environments. Cloud Code is a great tool for teams looking to streamline their development process for Kubernetes and Google Cloud."
      },
      {
        "date": "2023-01-11T00:45:00.000Z",
        "voteCount": 1,
        "content": "The Cloud Shell integrated Code Editor is a command-line text editor that you can use to edit code and configuration files within the Cloud Shell environment. While it can be useful for small changes or quick tests, it may not provide the same level of functionality or convenience as a dedicated development environment such as VS Code or IntelliJ. Additionally, Cloud Shell is primarily intended for managing infrastructure and applications from the command line, and may not offer the best workflow or experience for developing applications. For these reasons, it may be more beneficial to use a more robust development environment like VS Code or IntelliJ to develop and manage your applications on Google Kubernetes Engine."
      },
      {
        "date": "2022-12-19T07:17:00.000Z",
        "voteCount": 1,
        "content": "A is the answer.\n\nhttps://cloud.google.com/code/docs\nCloud Code provides IDE support for the full development cycle of Kubernetes and Cloud Run applications, from creating and customizing a new application from sample templates to running your finished application. Cloud Code supports you along the way with run-ready samples, out-of-the-box configuration snippets, and a tailored debugging experience \u2014 making developing with Kubernetes and Cloud Run a whole lot easier!"
      },
      {
        "date": "2022-08-20T00:14:00.000Z",
        "voteCount": 3,
        "content": "A is correct"
      },
      {
        "date": "2022-02-26T19:35:00.000Z",
        "voteCount": 3,
        "content": "Agreed with Option A \u2026 referred link"
      },
      {
        "date": "2022-01-15T20:44:00.000Z",
        "voteCount": 3,
        "content": "Agree with Option A"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 125,
    "url": "https://www.examtopics.com/discussions/google/view/69761-exam-professional-cloud-developer-topic-1-question-125/",
    "body": "You are developing an ecommerce web application that uses App Engine standard environment and Memorystore for Redis. When a user logs into the app, the application caches the user's information (e.g., session, name, address, preferences), which is stored for quick retrieval during checkout.<br>While testing your application in a browser, you get a 502 Bad Gateway error. You have determined that the application is not connecting to Memorystore. What is the reason for this error?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYour Memorystore for Redis instance was deployed without a public IP address.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYou configured your Serverless VPC Access connector in a different region than your App Engine instance.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe firewall rule allowing a connection between App Engine and Memorystore was removed during an infrastructure update by the DevOps team.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYou configured your application to use a Serverless VPC Access connector on a different subnet in a different availability zone than your App Engine instance."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 15,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 4,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-01-13T21:30:00.000Z",
        "voteCount": 6,
        "content": "B is the correct answer in this case, A is wrong because according to the best practice and security purpose gcp doesn't allow public ip for redis server."
      },
      {
        "date": "2024-07-22T00:26:00.000Z",
        "voteCount": 1,
        "content": "The most likely reason for the 502 Bad Gateway error and the inability to connect to Memorystore is B. You configured your Serverless VPC Access connector in a different region than your App Engine instance.\n\nHere's why:\n\nServerless VPC Access and Regions: App Engine standard environment applications can only communicate with services outside of their environment (like Memorystore) through Serverless VPC Access connectors. These connectors act as a bridge between your App Engine application and your VPC network, where Memorystore resides. Crucially, the connector and the App Engine instance must be in the same region ."
      },
      {
        "date": "2024-07-22T00:26:00.000Z",
        "voteCount": 1,
        "content": "Let's analyze why the other options are less likely:\n\nA. Memorystore without a Public IP: While Memorystore instances can be deployed without a public IP, this wouldn't prevent your App Engine application from connecting. Serverless VPC Access allows communication through private IPs.\nC. Firewall Rule Removal: While a firewall rule could prevent access, it's less likely to be the primary cause. If the firewall rule was removed, you'd likely see a different error message, not a 502 Bad Gateway.\nD. Different Subnet and Availability Zone: While using a different subnet or availability zone within the same region might introduce latency, it wouldn't completely prevent connectivity. The primary issue is the region mismatch between the connector and App Engine."
      },
      {
        "date": "2024-04-16T22:07:00.000Z",
        "voteCount": 1,
        "content": "B: If you configured your Serverless VPC Access connector in a different region than your App Engine instance, this could cause connectivity issues. Serverless VPC Access connectors and the resources they connect to must be in the same region."
      },
      {
        "date": "2024-03-22T00:15:00.000Z",
        "voteCount": 1,
        "content": "C. The firewall rule allowing a connection between App Engine and Memorystore was removed during an infrastructure update by the DevOps team.\n\nHere's why this scenario aligns with the error:\n\n502 Bad Gateway: This error typically indicates that a server (in this case, App Engine) is unable to communicate with an upstream server (Memorystore for Redis) due to a configuration issue.\nFirewall Rule Removal: If a firewall rule previously allowed App Engine to connect to Memorystore, removing it would block communication and cause the connection failure."
      },
      {
        "date": "2023-09-23T13:50:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/vpc/docs/configure-serverless-vpc-access\nIn the Region field, select a region for your connector. This must match the region of your serverless service.\nIf your service or job is in the region us-central or europe-west, use us-central1 or europe-west1."
      },
      {
        "date": "2023-09-21T07:49:00.000Z",
        "voteCount": 2,
        "content": "1\n\n1\nThe most likely reason for the 502 Bad Gateway error is that the firewall rule allowing a connection between App Engine and Memorystore was removed during an infrastructure update by the DevOps team.\n\nThis is because App Engine needs to be able to connect to Memorystore in order to retrieve the cached user information. If the firewall rule is removed, App Engine will not be able to connect to Memorystore and the application will fail."
      },
      {
        "date": "2023-08-05T06:40:00.000Z",
        "voteCount": 1,
        "content": "502 Bad gateway issue is common more in App Engine Flexible environments than Std due to memory issues.  Here I go with option D since pointing connector to a different subnet than App engine instance could cause Bad Gateway issue.  A is not correct, because even with the different region with the same subnet, App engine instance do not get issues while connecting to Memeorystore."
      },
      {
        "date": "2023-06-17T16:54:00.000Z",
        "voteCount": 2,
        "content": "C.\nA: No. The public IP is not mandatory.\nB: No. App Engine instance region can be different with Serverless VPC Access connector. \nLink here: https://support.google.com/a/answer/10620692?hl=en . \n\"We support VPC access connectors in 6 regions (us-central, us-west1, us-east1, asia-southeast1, asia-east1, and europe-west1). .... Note: Support for additional regions is coming soon.\" Although the document didn't mention directly, what if an app in App Engine in southamerica-east1-a would like to connect to Cloud SQL in us region? Note that the diagram here is REALLY mis-leading:\nhttps://cloud.google.com/vpc/docs/serverless-vpc-access#example_2\nC: Yes. This is the only possible answer.\nD: No. Serverless VPC Access connector shall be configured with a different subnet. See:\nhttps://cloud.google.com/vpc/docs/configure-serverless-vpc-access#console\n\"Every connector requires its own /28 subnet to place connector instances on. A subnet cannot be used by other resources such as VMs, Private Service Connect, or load balancers.\""
      },
      {
        "date": "2023-01-13T06:08:00.000Z",
        "voteCount": 2,
        "content": "A is not correct because Cloud Run connects to Memorystore via the Serverless VPC Connector. Connections are over private networks. Public addresses are not required.\nB is correct. All of the components must be in the same region.\nC is not correct because for connectivity between Cloud Run and Memorystore all that is required is a Serverless VPN Connector.\nD is not correct. The Serverless VPC Connector is configured with a non-overlapping subnet that is not associated with the VPC."
      },
      {
        "date": "2023-01-11T01:05:00.000Z",
        "voteCount": 2,
        "content": "While both B and D refer to the configuration of the Serverless VPC Access connector and could potentially cause issues with the application's ability to connect to Memorystore, they are slightly different.\n\nFor B:\nHaving the connector in a different region than the App Engine instance could result in increased latency and potential connectivity issues, but it would not necessarily prevent the App Engine instance from connecting to Memorystore.\n\nFor D:\nThis option is more specific and is indicating that if the connector is on different subnet or availability zone from App Engine instance it could cause issues with the application's ability to connect to Memorystore, it is less likely for this situation to cause latency or performance issues, but it will affect the connectivity of the App Engine to Memorystore.\n\nBoth B and D refer to misconfiguration of the Serverless VPC Access connector, but option D is more specific and directly relates to connectivity issue and is more likely to be the root cause of the 502 Bad Gateway error encountered."
      },
      {
        "date": "2022-12-19T07:15:00.000Z",
        "voteCount": 1,
        "content": "B is the answer.\n\nhttps://cloud.google.com/vpc/docs/serverless-vpc-access#how_it_works\nServerless VPC Access is based on a resource called a connector. A connector handles traffic between your serverless environment and your VPC network. When you create a connector in your Google Cloud project, you attach it to a specific VPC network and region. You can then configure your serverless services to use the connector for outbound network traffic."
      },
      {
        "date": "2022-08-20T00:14:00.000Z",
        "voteCount": 2,
        "content": "B is correct"
      },
      {
        "date": "2022-08-06T01:15:00.000Z",
        "voteCount": 1,
        "content": "Yes! This should be B"
      },
      {
        "date": "2022-01-09T08:49:00.000Z",
        "voteCount": 4,
        "content": "I vote B\nhttps://cloud.google.com/vpc/docs/configure-serverless-vpc-access"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 126,
    "url": "https://www.examtopics.com/discussions/google/view/70128-exam-professional-cloud-developer-topic-1-question-126/",
    "body": "Your team develops services that run on Google Cloud. You need to build a data processing service and will use Cloud Functions. The data to be processed by the function is sensitive. You need to ensure that invocations can only happen from authorized services and follow Google-recommended best practices for securing functions. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Identity-Aware Proxy in your project. Secure function access using its permissions.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a service account with the Cloud Functions Viewer role. Use that service account to invoke the function.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a service account with the Cloud Functions Invoker role. Use that service account to invoke the function.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an OAuth 2.0 client ID for your calling service in the same project as the function you want to secure. Use those credentials to invoke the function."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 13,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-03-03T09:46:00.000Z",
        "voteCount": 5,
        "content": "For me C. In link1 we can see how google suggests to use service accounts and in link2 we can see that the invoker role exists.\nLink1: https://cloud.google.com/functions/docs/securing#authentication Link2: https://cloud.google.com/functions/docs/reference/iam/roles#cloud-functions-roles"
      },
      {
        "date": "2024-07-22T00:32:00.000Z",
        "voteCount": 1,
        "content": "The best answer here is C. Create a service account with the Cloud Functions Invoker role. Use that service account to invoke the function.\n\nHere's why:\n\nCloud Functions Invoker Role: This role specifically grants the permission to invoke Cloud Functions. It's the most granular and appropriate role for this scenario, ensuring that the service account can only invoke Cloud Functions and nothing else.\nLeast Privilege: Using the Cloud Functions Invoker role adheres to the principle of least privilege, granting only the necessary permissions to the service account. This minimizes the risk of unauthorized access or actions.\nService Account Authentication: Service accounts are designed for machine-to-machine authentication. They provide a secure and reliable way to authenticate your calling service to the Cloud Function."
      },
      {
        "date": "2024-07-22T00:32:00.000Z",
        "voteCount": 1,
        "content": "Why other options are less ideal:\n\nA. Identity-Aware Proxy: Identity-Aware Proxy (IAP) is primarily used to secure web applications and APIs, not for controlling access to Cloud Functions.\nB. Cloud Functions Viewer Role: The Cloud Functions Viewer role only allows viewing Cloud Functions, not invoking them. It's not suitable for controlling access to the function.\nD. OAuth 2.0 Client ID: While OAuth 2.0 is a common authentication protocol, it's not the recommended approach for securing Cloud Functions. Service accounts provide a more streamlined and secure method for machine-to-machine authentication."
      },
      {
        "date": "2024-07-22T00:33:00.000Z",
        "voteCount": 1,
        "content": "In summary: Creating a service account with the Cloud Functions Invoker role and using it to invoke the function is the most secure and efficient way to restrict access to your sensitive data processing function, following Google-recommended best practices.\n\nAdditional Security Considerations:\n\nSecret Management: Store the service account credentials securely using Google Cloud Secret Manager.\nNetwork Security: Consider using VPC Service Controls to further restrict network access to your Cloud Function.\nLogging and Monitoring: Enable logging and monitoring for your Cloud Function to track invocations and identify any potential security issues."
      },
      {
        "date": "2023-11-22T14:20:00.000Z",
        "voteCount": 1,
        "content": "IAP is not available for Cloud Functions, so the only possible option is C"
      },
      {
        "date": "2023-11-22T14:20:00.000Z",
        "voteCount": 1,
        "content": "IAP is not available for Cloud Functions, so the only possible answer is C"
      },
      {
        "date": "2023-09-21T07:53:00.000Z",
        "voteCount": 1,
        "content": "1\nThe best way to ensure that invocations of a Cloud Function that processes sensitive data can only happen from authorized services and follows Google-recommended best practices is to enable Identity-Aware Proxy in your project and secure function access using its permissions."
      },
      {
        "date": "2023-11-22T14:19:00.000Z",
        "voteCount": 1,
        "content": "IAP is not available for Cloud Functions"
      },
      {
        "date": "2023-08-05T07:15:00.000Z",
        "voteCount": 1,
        "content": "Since this is service to service communication, cloud function invoker role should be provided to the service that wants to invoke cloud function in the data processing pipeline."
      },
      {
        "date": "2023-02-25T05:59:00.000Z",
        "voteCount": 1,
        "content": "vote c"
      },
      {
        "date": "2023-01-11T01:24:00.000Z",
        "voteCount": 2,
        "content": "The best approach is to use a combination of authn, authz, and encryption\n\n1. Enable IAP to ensure that only authenticated and authorized users or services can access Cloud Function\n2. Set up an appropriate level of access control using IAM roles and policies, such as roles/cloudfunctions.invoker, to ensure that only authorized services can invoke your Cloud Function, This can be done by creating a service account for the calling function, assign the appropriate invoker role to the service account on the data processing function and use the service account credentials in the calling function\n3. Use Google-provided libraries or resources, such as KMS or Cloud Storage, to encrypt and store sensitive data\n4. Apply security best practices such as limiting the scope of the service account, and using Cloud IAP to protect access to your Cloud Function\n5. consider using Cloud Event that ensure your function is triggered only by authorized events, you can use Cloud Event to ensure that your function is invoked only by specific event types that you have configured"
      },
      {
        "date": "2023-01-11T01:25:00.000Z",
        "voteCount": 1,
        "content": "Options B and D are not correct. The Cloud Functions Viewer role does not have the necessary permissions to invoke a Cloud Function and creating an OAuth 2.0 client ID for your calling service is not enough to secure a Cloud Function."
      },
      {
        "date": "2023-01-11T01:25:00.000Z",
        "voteCount": 1,
        "content": "Option C is correct that creating a service account with the appropriate invoker role is one step in securing your Cloud Function, however it should be used in conjunction with other security measures."
      },
      {
        "date": "2023-01-11T01:25:00.000Z",
        "voteCount": 1,
        "content": "Option A is correct in that it's important to enable IAP to ensure that only authenticated and authorized users or services can access your Cloud Function, but it's not enough by itself to secure your function."
      },
      {
        "date": "2023-01-11T01:25:00.000Z",
        "voteCount": 1,
        "content": "By following the above steps, you can ensure that your Cloud Function is secure and can only be invoked by authorized services."
      },
      {
        "date": "2023-11-22T14:19:00.000Z",
        "voteCount": 1,
        "content": "IAP is not available for Cloud Functions, so the correct answer is C"
      },
      {
        "date": "2022-12-19T07:06:00.000Z",
        "voteCount": 1,
        "content": "C is the answer.\n\nhttps://cloud.google.com/functions/docs/securing/authenticating"
      },
      {
        "date": "2022-12-13T10:16:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2022-11-10T03:08:00.000Z",
        "voteCount": 4,
        "content": "ANSWER C\nhttps://medium.com/google-cloud/how-to-securely-invoke-a-cloud-function-from-google-kubernetes-engine-running-on-another-gcp-79797ec2b2c6"
      },
      {
        "date": "2022-08-20T00:15:00.000Z",
        "voteCount": 1,
        "content": "I think D is correct"
      },
      {
        "date": "2022-08-06T01:45:00.000Z",
        "voteCount": 1,
        "content": "I will go with option C."
      },
      {
        "date": "2022-05-09T04:19:00.000Z",
        "voteCount": 2,
        "content": "C :\nhttps://cloud.google.com/functions/docs/securing/authenticating#authenticating_function_to_function_calls"
      },
      {
        "date": "2022-04-28T00:24:00.000Z",
        "voteCount": 1,
        "content": "Vote C"
      },
      {
        "date": "2022-02-28T05:46:00.000Z",
        "voteCount": 1,
        "content": "I believe this is C"
      },
      {
        "date": "2022-02-26T19:49:00.000Z",
        "voteCount": 2,
        "content": "Agreed D \u2026 \n\nFrom link reference below \nThe tokens themselves are created using the OAuth 2 framework, and its extension, Open Identity Connect, but the sequence is complex and error-prone, and the use of Cloud Client Libraries to manage the process is highly recommended."
      },
      {
        "date": "2022-03-03T09:45:00.000Z",
        "voteCount": 2,
        "content": "Why not C?\nIn link1 we can see how google suggests to use service accounts and in link2 we can see that the invoker role exists.\nLink1: https://cloud.google.com/functions/docs/securing#authentication\nLink2: https://cloud.google.com/functions/docs/reference/iam/roles#cloud-functions-roles"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 127,
    "url": "https://www.examtopics.com/discussions/google/view/69612-exam-professional-cloud-developer-topic-1-question-127/",
    "body": "You are deploying your applications on Compute Engine. One of your Compute Engine instances failed to launch. What should you do? (Choose two.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDetermine whether your file system is corrupted.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAccess Compute Engine as a different SSH user.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTroubleshoot firewall rules or routes on an instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCheck whether your instance boot disk is completely full.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCheck whether network traffic to or from your instance is being dropped."
    ],
    "answer": "AD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AD",
        "count": 22,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-22T00:40:00.000Z",
        "voteCount": 1,
        "content": "When a Compute Engine instance fails to launch, you should consider various factors that could cause the failure. Here are the two most relevant actions to take:\n\nA. Determine whether your file system is corrupted.\n\nA corrupted file system can prevent the instance from booting properly. You can check and repair the file system using a recovery process, such as attaching the boot disk to another instance for analysis.\nD. Check whether your instance boot disk is completely full.\n\nIf the boot disk is full, the instance may not be able to start because the operating system needs some free disk space to function properly. You can check the disk usage and free up space if necessary."
      },
      {
        "date": "2024-07-22T00:41:00.000Z",
        "voteCount": 1,
        "content": "Other Options:\n\nB. Access Compute Engine as a different SSH user.\n\nWhile accessing as a different SSH user might help if you have SSH access issues, it is unlikely to resolve a failure to launch the instance itself.\nC. Troubleshoot firewall rules or routes on an instance.\n\nFirewall rules and routes affect network traffic to and from the instance but are less likely to be the root cause of a launch failure.\nE. Check whether network traffic to or from your instance is being dropped.\n\nSimilar to option C, dropped network traffic impacts connectivity but is not a primary cause for an instance failing to launch.\nTherefore, the most appropriate actions are A and D to diagnose and resolve issues preventing your Compute Engine instance from launching."
      },
      {
        "date": "2024-04-16T22:13:00.000Z",
        "voteCount": 2,
        "content": "A &amp; D\nhttps://cloud.google.com/compute/docs/troubleshooting/vm-startup#identify_the_reason_why_the_boot_disk_isnt_booting\n\n\nNetwork issues shouldn't stop a compute engine from booting up so C &amp; E are out. \nB cant be true because how can u SSH if it doesnt boot"
      },
      {
        "date": "2023-09-21T07:56:00.000Z",
        "voteCount": 1,
        "content": "I will go with AD."
      },
      {
        "date": "2023-08-05T07:19:00.000Z",
        "voteCount": 2,
        "content": "Compute engine will not launch when either of these happens.\n1) When its file system is corrupted.\n2) When its boot disk is full."
      },
      {
        "date": "2023-02-25T06:00:00.000Z",
        "voteCount": 4,
        "content": "https://cloud.google.com/compute/docs/troubleshooting/vm-startup#identify_the_reason_why_the_boot_disk_isnt_booting\n- Verify that your boot disk is not full.\nIf your boot disk is completely full and your operating system does not support automatic resizing, you won't be able to connect to your instance. You must create a new instance and recreate the boot disk.\n\n- Verify that your disk has a valid file system.\nIf your file system is corrupted or otherwise invalid, you won't be able to launch your instance."
      },
      {
        "date": "2022-12-19T04:53:00.000Z",
        "voteCount": 3,
        "content": "AD is the answer.\n\nhttps://cloud.google.com/compute/docs/troubleshooting/vm-startup#identify_the_reason_why_the_boot_disk_isnt_booting\n- Verify that your boot disk is not full.\nIf your boot disk is completely full and your operating system does not support automatic resizing, you won't be able to connect to your instance. You must create a new instance and recreate the boot disk. \n\n- Verify that your disk has a valid file system.\nIf your file system is corrupted or otherwise invalid, you won't be able to launch your instance."
      },
      {
        "date": "2022-08-20T00:15:00.000Z",
        "voteCount": 4,
        "content": "AD are correct"
      },
      {
        "date": "2022-08-06T01:48:00.000Z",
        "voteCount": 2,
        "content": "A and D seem right here."
      },
      {
        "date": "2022-04-28T00:26:00.000Z",
        "voteCount": 3,
        "content": "Vote AD"
      },
      {
        "date": "2022-01-16T10:27:00.000Z",
        "voteCount": 2,
        "content": "Options D for sure , in the rest Option A seems the nearest one as one of the troubleshooting steps is to check the file system"
      },
      {
        "date": "2022-01-11T08:24:00.000Z",
        "voteCount": 2,
        "content": "A &amp; D! \nIf the failure is on the launch changing the SSH user will not help. Network traffic, Network routes, Firewall rules...are not influencing the instance boot!"
      },
      {
        "date": "2022-01-09T01:57:00.000Z",
        "voteCount": 3,
        "content": "AD should be the correct one, because launching VM failure does not depends on network connectivity of that VM. Network comes into the picture when vm boots."
      },
      {
        "date": "2022-01-06T23:06:00.000Z",
        "voteCount": 3,
        "content": "I vote AD.\nhttps://cloud.google.com/compute/docs/troubleshooting/vm-startup\nVerify that your disk has a valid file system.\u3000\nVerify that your boot disk is not full."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 128,
    "url": "https://www.examtopics.com/discussions/google/view/70129-exam-professional-cloud-developer-topic-1-question-128/",
    "body": "Your web application is deployed to the corporate intranet. You need to migrate the web application to Google Cloud. The web application must be available only to company employees and accessible to employees as they travel. You need to ensure the security and accessibility of the web application while minimizing application changes. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the application to check authentication credentials for each HTTP(S) request to the application.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Identity-Aware Proxy to allow employees to access the application through its public IP address.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a Compute Engine instance that requests users to log in to their corporate account. Change the web application DNS to point to the proxy Compute Engine instance. After authenticating, the Compute Engine instance forwards requests to and from the web application.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a Compute Engine instance that requests users to log in to their corporate account. Change the web application DNS to point to the proxy Compute Engine instance. After authenticating, the Compute Engine issues an HTTP redirect to a public IP address hosting the web application."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 13,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 7,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-01-16T10:30:00.000Z",
        "voteCount": 9,
        "content": "Agree with Option B"
      },
      {
        "date": "2022-11-16T23:26:00.000Z",
        "voteCount": 2,
        "content": "why  public IP yet it must only be accessible to the employees only? B is wrong"
      },
      {
        "date": "2022-12-22T03:35:00.000Z",
        "voteCount": 1,
        "content": "it's Google public IP, https://cloud.google.com/iap/docs/managing-access"
      },
      {
        "date": "2022-12-25T03:10:00.000Z",
        "voteCount": 1,
        "content": "If its B, it must not use public IP, That makes B wrong. the answer is C. its already in coorporate intranet, why use public IP?"
      },
      {
        "date": "2023-02-07T05:40:00.000Z",
        "voteCount": 1,
        "content": "How the users are going to authenticate to Compute Engine?"
      },
      {
        "date": "2024-07-22T00:53:00.000Z",
        "voteCount": 1,
        "content": "The best solution here is B. Configure Identity-Aware Proxy to allow employees to access the application through its public IP address.\n\nHere's why:\n\nMinimal Application Changes: Identity-Aware Proxy (IAP) is designed to handle authentication and authorization without requiring significant changes to your web application. It acts as a secure gateway, intercepting requests and verifying user identities before forwarding them to your application.\nSecure Access: IAP provides strong security by integrating with your existing corporate identity provider (e.g., Google Workspace, Active Directory). It ensures that only authorized employees with valid credentials can access the application."
      },
      {
        "date": "2024-07-22T00:53:00.000Z",
        "voteCount": 1,
        "content": "Accessibility for Traveling Employees: IAP allows employees to access the application from anywhere with an internet connection, as long as they have the necessary credentials. This eliminates the need for VPNs or other complex network configurations.\nCentralized Management: IAP simplifies security management by providing a centralized platform for controlling access to your application. You can easily add or remove users, define access policies, and monitor activity.\nWhy other options are less ideal:\n\nA. Authentication in the Application: This approach requires significant changes to your web application to handle authentication logic, which can be complex and error-prone. It also doesn't provide the same level of security and centralized management as IAP."
      },
      {
        "date": "2024-07-22T00:53:00.000Z",
        "voteCount": 1,
        "content": "C. Proxy Compute Engine Instance: While this approach could work, it requires setting up and managing a separate Compute Engine instance, which adds complexity and overhead. It also doesn't leverage the built-in security features of IAP.\nD. HTTP Redirect: This approach would expose your web application's public IP address, potentially compromising security. It also doesn't provide the same level of authentication and authorization as IAP.\nIn summary: Identity-Aware Proxy is the most efficient and secure way to migrate your web application to Google Cloud while ensuring accessibility for traveling employees and minimizing application changes. It provides a robust and centralized solution for authentication, authorization, and secure access control."
      },
      {
        "date": "2024-04-16T22:16:00.000Z",
        "voteCount": 1,
        "content": "B is the answer. IAP is the solution in these kind of scenarios. \nDon't be alerted by mention of public IP. It's completely fine to deploy an internal app on public IP as long as u have proper authentication. Since the question mentions \"accessible to employees as they travel\", this is how many companies deploy such internal tools."
      },
      {
        "date": "2023-09-21T08:01:00.000Z",
        "voteCount": 1,
        "content": "I will go with B."
      },
      {
        "date": "2023-02-27T06:24:00.000Z",
        "voteCount": 2,
        "content": "i'd choose b: https://cloud.google.com/blog/topics/developers-practitioners/control-access-your-web-sites-identity-aware-proxy"
      },
      {
        "date": "2023-01-11T01:33:00.000Z",
        "voteCount": 2,
        "content": "This approach allows you to use Google Cloud infrastructure to authenticate users against the corporate intranet before providing access to the web application, without making major changes to the web application. By configuring a Compute Engine instance as a proxy and changing the web application's DNS to point to this proxy, you can ensure that only employees who have been authenticated against the corporate intranet are able to access the web application. This approach also allows the employees to access the web application while they are traveling, as long as they have internet access."
      },
      {
        "date": "2023-01-11T01:33:00.000Z",
        "voteCount": 1,
        "content": "Identity-Aware Proxy (IAP) is a feature of Google Cloud Platform that allows you to secure access to resources by using identity and context-based access control. IAP allows you to restrict access to a resource (such as a web application) to only authenticated and authorized users or service accounts.\n\nHowever, in this scenario, since the web application is hosted on the corporate intranet, it will not have a public IP address and it will not be accessible from the internet. And It's not possible to use IAP to restrict access to an intranet-hosted application by its IP address."
      },
      {
        "date": "2023-01-11T01:34:00.000Z",
        "voteCount": 1,
        "content": "Additionally, IAP is designed to work with resources that are hosted on Google Cloud, and it may not be possible to configure it to work with an intranet-hosted application without making significant changes to the application and the intranet infrastructure.\n\nThat's why the best solution would be to use a VPN connection or a reverse proxy to allow employees to access the application as if they were on the intranet while they are traveling or to secure the access to the intranet-hosted web application from the internet."
      },
      {
        "date": "2022-12-22T03:35:00.000Z",
        "voteCount": 3,
        "content": "B is correct."
      },
      {
        "date": "2022-12-19T04:50:00.000Z",
        "voteCount": 2,
        "content": "B is the answer.\n\nhttps://cloud.google.com/iap/docs/concepts-overview\nIAP lets you establish a central authorization layer for applications accessed by HTTPS, so you can use an application-level access control model instead of relying on network-level firewalls.\n\nIAP policies scale across your organization. You can define access policies centrally and apply them to all of your applications and resources. When you assign a dedicated team to create and enforce policies, you protect your project from incorrect policy definition or implementation in any application."
      },
      {
        "date": "2022-12-18T14:13:00.000Z",
        "voteCount": 3,
        "content": "B, while employees are traveling, they don't have access to the intranet, so they need to use the public IP. IAP secures the public endpoint."
      },
      {
        "date": "2022-08-20T00:17:00.000Z",
        "voteCount": 3,
        "content": "C seems right"
      },
      {
        "date": "2022-08-06T01:58:00.000Z",
        "voteCount": 2,
        "content": "I would completely agree with BackendBoi's comment. I would have picked option B only if it would have not been said to access through public IP. Out of all the options, option C seems the best pick. I had read somewhere that the proxy compute engine is used for securing access to main compute engine instance hosting application."
      },
      {
        "date": "2022-04-07T04:12:00.000Z",
        "voteCount": 3,
        "content": "I tend to C. A is bad because sending the credentials in each HTTP(s) request is bad and inefficient. B requires each user to have a Google Workspace account, which is not a given for the corporate intranet. On top of that there is no mention that the application checks for the token in the header, so a public IP would still expose the application. C would work, but its ineffective. D is useless if the application is still exposed through the public IP. None of these solutions are great, but C is the least bad of the bunch."
      },
      {
        "date": "2022-05-18T10:31:00.000Z",
        "voteCount": 1,
        "content": "You couldn't opt anyone ? I suggest you to skip this in exam :)"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 129,
    "url": "https://www.examtopics.com/discussions/google/view/69757-exam-professional-cloud-developer-topic-1-question-129/",
    "body": "You have an application that uses an HTTP Cloud Function to process user activity from both desktop browser and mobile application clients. This function will serve as the endpoint for all metric submissions using HTTP POST.<br>Due to legacy restrictions, the function must be mapped to a domain that is separate from the domain requested by users on web or mobile sessions. The domain for the Cloud Function is https://fn.example.com. Desktop and mobile clients use the domain https://www.example.com. You need to add a header to the function's<br>HTTP response so that only those browser and mobile sessions can submit metrics to the Cloud Function. Which response header should you add?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAccess-Control-Allow-Origin: *",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAccess-Control-Allow-Origin: https://*.example.com",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAccess-Control-Allow-Origin: https://fn.example.com",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAccess-Control-Allow-origin: https://www.example.com\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 12,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-22T21:44:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is D. Access-Control-Allow-origin: https://www.example.com  . Here's why:\n\nCORS (Cross-Origin Resource Sharing): The scenario you've described involves a classic CORS issue. Your Cloud Function (on https://fn.example.com ) is being accessed from a different origin ( https://www.example.com ). Browsers have security restrictions that prevent requests from one domain to another without explicit permission.\nAccess-Control-Allow-Origin Header: This header is used to tell the browser which origins are allowed to make requests to your Cloud Function."
      },
      {
        "date": "2023-09-21T08:07:00.000Z",
        "voteCount": 1,
        "content": "D is correct."
      },
      {
        "date": "2023-08-05T07:33:00.000Z",
        "voteCount": 1,
        "content": "It is like requesting service from front end to back-end service.  Here front-end service domain is https://www.example.com and back-end service domain where cloud function runs is https://fn.example.com"
      },
      {
        "date": "2023-02-25T06:05:00.000Z",
        "voteCount": 2,
        "content": "vote d"
      },
      {
        "date": "2022-12-19T04:44:00.000Z",
        "voteCount": 1,
        "content": "D is the answer.\n\nhttps://cloud.google.com/functions/docs/samples/functions-http-cors"
      },
      {
        "date": "2022-08-20T00:17:00.000Z",
        "voteCount": 2,
        "content": "D is correct"
      },
      {
        "date": "2022-03-29T10:16:00.000Z",
        "voteCount": 4,
        "content": "I agree with D but just a little detail (idk if it was a typo)  ... the word \"origin\" must be \"Origin\"... besides that seems correct"
      },
      {
        "date": "2022-02-26T19:56:00.000Z",
        "voteCount": 4,
        "content": "I agree it should be D"
      },
      {
        "date": "2022-01-16T10:35:00.000Z",
        "voteCount": 3,
        "content": "Should be Option D, Option A will allow all and not only specific as requested in the question"
      },
      {
        "date": "2022-01-09T08:21:00.000Z",
        "voteCount": 2,
        "content": "I vote D"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 130,
    "url": "https://www.examtopics.com/discussions/google/view/69724-exam-professional-cloud-developer-topic-1-question-130/",
    "body": "You have an HTTP Cloud Function that is called via POST. Each submission's request body has a flat, unnested JSON structure containing numeric and text data. After the Cloud Function completes, the collected data should be immediately available for ongoing and complex analytics by many users in parallel. How should you persist the submissions?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDirectly persist each POST request's JSON data into Datastore.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTransform the POST request's JSON data, and stream it into BigQuery.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTransform the POST request's JSON data, and store it in a regional Cloud SQL cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPersist each POST request's JSON data as an individual file within Cloud Storage, with the file name containing the request identifier."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 23,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-01-09T02:08:00.000Z",
        "voteCount": 15,
        "content": "B should be the correct one because question has mentioned for analytics of the data."
      },
      {
        "date": "2024-07-22T22:03:00.000Z",
        "voteCount": 1,
        "content": "The best answer here is B. Transform the POST request's JSON data and stream it into BigQuery. Here's why:\n\nBigQuery's Strengths for Analytics: BigQuery is specifically designed for large-scale, complex analytics. It offers:\nScalability: Handles massive datasets with ease.\nParallel Processing: Allows many users to query the data concurrently without performance degradation.\nSQL Support: Provides a familiar and powerful query language for data exploration.\nData Exploration Tools: Comes with built-in tools for data visualization and analysis."
      },
      {
        "date": "2024-07-22T22:04:00.000Z",
        "voteCount": 1,
        "content": "Why the other options are less suitable:\nA. Datastore: Datastore is a NoSQL database, great for storing structured data, but not ideal for complex analytics. It's not designed for the kind of parallel querying and data exploration that BigQuery excels at.\nC. Cloud SQL: While Cloud SQL can handle analytics, it's generally more expensive than BigQuery for large-scale operations. It's also not as optimized for parallel queries.\nD. Cloud Storage: Cloud Storage is great for storing files, but it's not a database. You'd need to build your own analytics infrastructure on top of it, which would be more complex and less efficient than using BigQuery."
      },
      {
        "date": "2023-09-21T08:10:00.000Z",
        "voteCount": 1,
        "content": "B is correct since we need to do complex analytics."
      },
      {
        "date": "2023-08-05T07:39:00.000Z",
        "voteCount": 1,
        "content": "The key here is \"Collected data should be IMMEDIATELY available for ongoing and complex analytics\", and hence option B is correct."
      },
      {
        "date": "2023-02-25T06:06:00.000Z",
        "voteCount": 1,
        "content": "\"data should be immediately available for ongoing and complex analytics\" -&gt; B"
      },
      {
        "date": "2023-01-11T05:57:00.000Z",
        "voteCount": 1,
        "content": "B. Transform the POST request's JSON data, and stream it into BigQuery.\n\nBigQuery is a highly scalable data warehouse that is well suited for handling large amounts of data and complex analytics in near real-time. By streaming the JSON data from your Cloud Function directly into BigQuery, you can make the collected data immediately available for analytics by many users in parallel. BigQuery support various data types including json, so you can store your request body without any transformation.\n\nA. Directly persist each POST request's JSON data into Datastore.\nDatastore is a NoSQL document database that can be used to store structured data, but it's not designed to handle the high volume of data that you need to analyze in near real-time. And it would require additional processing to be available for analysis."
      },
      {
        "date": "2023-01-11T05:58:00.000Z",
        "voteCount": 1,
        "content": "C. Transform the POST request's JSON data, and store it in a regional Cloud SQL cluster.\nCloud SQL is a fully-managed MySQL, PostgreSQL, and SQL Server database service, which is more suited for transactional workloads, rather than for storing large amounts of data for analytics purposes."
      },
      {
        "date": "2023-01-11T05:57:00.000Z",
        "voteCount": 1,
        "content": "Storing the data as individual files in Cloud Storage may not be the best approach for immediate and parallel analytics as it would require additional processing and data manipulation to make it available for analytics purposes."
      },
      {
        "date": "2023-01-11T05:57:00.000Z",
        "voteCount": 1,
        "content": "D. Persist each POST request's JSON data as an individual file within Cloud Storage, with the file name containing the request identifier."
      },
      {
        "date": "2023-01-11T05:58:00.000Z",
        "voteCount": 1,
        "content": "A. Directly persist each POST request's JSON data into Datastore.\nDatastore is a NoSQL document database that can be used to store structured data, but it's not designed to handle the high volume of data that you need to analyze in near real-time. And it would require additional processing to be available for analysis."
      },
      {
        "date": "2022-12-17T20:08:00.000Z",
        "voteCount": 1,
        "content": "B is the answer."
      },
      {
        "date": "2022-08-20T00:18:00.000Z",
        "voteCount": 2,
        "content": "B is correct"
      },
      {
        "date": "2022-02-26T19:58:00.000Z",
        "voteCount": 2,
        "content": "Option B\u2026 can\u2019t disagree enough Analytics = BigQuery"
      },
      {
        "date": "2022-01-16T10:40:00.000Z",
        "voteCount": 2,
        "content": "Is between Option B and D. Option B talks about transforming JSON data but no where in question we get to understand this need. So even though is BigQuery for analytics purposes Option D is more suitable."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 131,
    "url": "https://www.examtopics.com/discussions/google/view/69725-exam-professional-cloud-developer-topic-1-question-131/",
    "body": "Your security team is auditing all deployed applications running in Google Kubernetes Engine. After completing the audit, your team discovers that some of the applications send traffic within the cluster in clear text. You need to ensure that all application traffic is encrypted as quickly as possible while minimizing changes to your applications and maintaining support from Google. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Network Policies to block traffic between applications.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall Istio, enable proxy injection on your application namespace, and then enable mTLS.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDefine Trusted Network ranges within the application, and configure the applications to allow traffic only from those networks.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an automated process to request SSL Certificates for your applications from Let's Encrypt and add them to your applications."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 13,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-01-09T07:39:00.000Z",
        "voteCount": 8,
        "content": "I vote B\nhttps://cloud.google.com/istio/docs/istio-on-gke/installing\n(deprecated)"
      },
      {
        "date": "2022-01-21T21:20:00.000Z",
        "voteCount": 6,
        "content": "https://cloud.google.com/service-mesh/docs/by-example/mtls  option B"
      },
      {
        "date": "2024-07-22T22:09:00.000Z",
        "voteCount": 1,
        "content": "Istio and mTLS: Istio is a service mesh that provides a powerful way to manage and secure traffic between applications in a Kubernetes cluster. Enabling mTLS (mutual TLS) with Istio ensures that all communication between services is encrypted and authenticated.\n\nBenefits of using Istio and mTLS:\nMinimal Application Changes: Istio works transparently with your applications. You don't need to modify your application code to enable encryption.\nCentralized Management: Istio provides a single point of control for managing security policies across your cluster.\nGoogle Support: Istio is a Google-supported project, so you can rely on Google's expertise and documentation.\nComprehensive Security: mTLS provides both encryption and authentication, ensuring that only authorized services can communicate with each other."
      },
      {
        "date": "2024-07-22T22:09:00.000Z",
        "voteCount": 1,
        "content": "Why the other options are less suitable:\nA. Network Policies: While Network Policies can restrict traffic, they don't inherently encrypt it. They are more about controlling which pods can communicate with others, not about securing the communication itself.\nC. Trusted Network Ranges: This approach focuses on network-level security but doesn't address the core issue of application-to-application communication. It also requires changes to your applications to configure them to trust specific networks.\nD. Let's Encrypt: Let's Encrypt is great for securing public-facing applications, but it's not designed for internal service-to-service communication within a Kubernetes cluster. It would require significant changes to your applications and infrastructure."
      },
      {
        "date": "2023-12-27T20:49:00.000Z",
        "voteCount": 2,
        "content": "Answer: B\nIstio enhances the security of microservices by providing features such as mutual TLS (Transport Layer Security) authentication between services, access controls, and encryption of communication channels."
      },
      {
        "date": "2023-09-21T20:35:00.000Z",
        "voteCount": 2,
        "content": "Istio is a service mesh that can be used to encrypt traffic between applications in a GKE cluster. It does this by injecting a sidecar proxy into each pod. The sidecar proxy intercepts all traffic to and from the pod and encrypts it using mTLS (mutual TLS)."
      },
      {
        "date": "2023-08-05T10:42:00.000Z",
        "voteCount": 1,
        "content": "Istio is suitable for providing cutting edge concerns to the services running in the GKE cluster. Istio provides security, fault tolerance and resiliency out of the box."
      },
      {
        "date": "2023-01-11T05:59:00.000Z",
        "voteCount": 1,
        "content": "B. Install Istio, enable proxy injection on your application namespace, and then enable mTLS.\n\nIstio is a service mesh that runs within your Kubernetes cluster and provides a set of features, such as traffic management, service discovery, and automatic encryption of traffic between services using mutual Transport Layer Security (mTLS). By installing Istio and enabling proxy injection on your application namespace, you can quickly and easily enable mTLS for all traffic within the cluster without making changes to your applications. Once the proxy injection is enabled, Istio automatically adds the necessary sidecar proxies to each pod in the namespace and configures them to encrypt traffic."
      },
      {
        "date": "2023-01-11T05:59:00.000Z",
        "voteCount": 1,
        "content": "C. Define Trusted Network ranges within the application, and configure the applications to allow traffic only from those networks.\nIt does not provide any encryption for the traffic, it only allows traffic from specific IP ranges."
      },
      {
        "date": "2023-01-11T05:59:00.000Z",
        "voteCount": 1,
        "content": "D. Use an automated process to request SSL Certificates for your applications from Let's Encrypt and add them to your applications.\nIt can encrypt the traffic between the client and the application but it doesn't cover the traffic inside the cluster."
      },
      {
        "date": "2023-01-11T05:59:00.000Z",
        "voteCount": 1,
        "content": "A. Use Network Policies to block traffic between applications\nnetwork policies are used to control traffic between pods in the cluster, it can help to secure the communication but it doesn't provide any encryption"
      },
      {
        "date": "2022-12-17T20:06:00.000Z",
        "voteCount": 1,
        "content": "B is the answer.\n\nhttps://cloud.google.com/istio/docs/istio-on-gke/overview\nIstio gives you the following benefits:\n- Secure service-to-service communication in a cluster with strong identity-based authentication and authorization."
      },
      {
        "date": "2022-08-20T00:18:00.000Z",
        "voteCount": 3,
        "content": "B is correct"
      },
      {
        "date": "2022-07-12T00:32:00.000Z",
        "voteCount": 2,
        "content": "B should work. It's the only answer with a solution without blocking or restricting the cluster traffic"
      },
      {
        "date": "2022-05-17T20:05:00.000Z",
        "voteCount": 2,
        "content": "B is correct"
      },
      {
        "date": "2022-04-25T03:52:00.000Z",
        "voteCount": 1,
        "content": "will go with D. A,C are no where in context( traffic should be encrypted)\nif there is Anthos Service Mesh instead of Istio in B then it is definitely B."
      },
      {
        "date": "2022-03-29T10:53:00.000Z",
        "voteCount": 2,
        "content": "This question/answers are outdated... Google stop supporting Istio implementations and suggest to migrate to ASM. Option B seems more reasonable, but depends on the date it was written."
      },
      {
        "date": "2022-01-09T02:11:00.000Z",
        "voteCount": 2,
        "content": "The question is not about blocking the traffic. D is the correct answer."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 132,
    "url": "https://www.examtopics.com/discussions/google/view/70130-exam-professional-cloud-developer-topic-1-question-132/",
    "body": "You migrated some of your applications to Google Cloud. You are using a legacy monitoring platform deployed on-premises for both on-premises and cloud- deployed applications. You discover that your notification system is responding slowly to time-critical problems in the cloud applications. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReplace your monitoring platform with Cloud Monitoring.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall the Cloud Monitoring agent on your Compute Engine instances.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate some traffic back to your old platform. Perform A/B testing on the two platforms concurrently.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Logging and Cloud Monitoring to capture logs, monitor, and send alerts. Send them to your existing platform.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 17,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-02-28T17:53:00.000Z",
        "voteCount": 8,
        "content": "He migrated only 'some' of applications, not all of them to GCP."
      },
      {
        "date": "2024-07-22T22:28:00.000Z",
        "voteCount": 1,
        "content": "Leverage Existing Infrastructure: This approach allows you to take advantage of the existing monitoring platform you've already invested in. You're not throwing it away, but rather integrating it with Google Cloud's powerful monitoring and logging services.\nHow it works:\nCloud Logging and Monitoring: Use Cloud Logging to collect logs from your cloud applications and Cloud Monitoring to monitor metrics.\nAlerting: Configure Cloud Monitoring to send alerts based on predefined thresholds or patterns in your logs and metrics.\nIntegration: Set up a mechanism to forward these alerts from Cloud Monitoring to your existing on-premises monitoring platform. This could involve using a webhook, a message queue, or other integration methods."
      },
      {
        "date": "2024-07-22T22:28:00.000Z",
        "voteCount": 1,
        "content": "Why the other options are less ideal:\nA. Replace your monitoring platform: This is a significant undertaking and might not be feasible in the short term. It also means losing the historical data and configurations you have in your existing platform.\nB. Install the Cloud Monitoring agent: This only addresses monitoring within Google Cloud, not the integration with your existing platform.\nC. Migrate traffic back: This is a step backward and doesn't solve the core issue of slow notifications. It also introduces complexity and potential performance issues."
      },
      {
        "date": "2023-09-21T20:51:00.000Z",
        "voteCount": 1,
        "content": "I will go with D."
      },
      {
        "date": "2023-02-27T06:18:00.000Z",
        "voteCount": 1,
        "content": "D, but if solution used is GCE logging and monitoring wouldn't be there since GCE do not have direct integration. \n\ni feel this question might be \"incomplete\""
      },
      {
        "date": "2023-01-11T06:02:00.000Z",
        "voteCount": 1,
        "content": "D. Use Cloud Logging and Cloud Monitoring to capture logs, monitor, and send alerts. Send them to your existing platform.\nis a valid option if your aim to integrate the on-premise monitoring platform with the cloud monitoring platform, this way you can have a holistic view of all your application performance.\n\nYou can also use Google Cloud's Stackdriver service to integrate the monitoring, logging and tracing across both on-premise and cloud. Stackdriver can be used to get unified view of all your application performance and trace the root cause of an issue."
      },
      {
        "date": "2023-01-11T06:02:00.000Z",
        "voteCount": 1,
        "content": "not all of the applications have been migrated, in this scenario, a hybrid monitoring solution would be a good approach. You can keep using the legacy on-premises monitoring platform for the on-premises applications, and use Google Cloud Monitoring for the cloud-deployed applications. This approach would allow you to maintain visibility into both on-premises and cloud-deployed applications in a single monitoring interface, and send alerts to a centralized notification system.\n\nYou can use Cloud Monitoring to discover resources running in your on-premises infrastructure by using the Cloud Monitoring Agent that can be installed on the machines running on-premises. it will help you to monitor on-premise machines with Cloud Monitoring."
      },
      {
        "date": "2022-12-17T20:04:00.000Z",
        "voteCount": 1,
        "content": "D is the answer."
      },
      {
        "date": "2022-09-12T22:55:00.000Z",
        "voteCount": 1,
        "content": "Not all applications were migrated"
      },
      {
        "date": "2022-08-20T00:18:00.000Z",
        "voteCount": 2,
        "content": "D is correct"
      },
      {
        "date": "2022-04-18T05:43:00.000Z",
        "voteCount": 1,
        "content": "Vote D"
      },
      {
        "date": "2022-02-26T20:02:00.000Z",
        "voteCount": 2,
        "content": "I vote for A"
      },
      {
        "date": "2022-01-16T10:49:00.000Z",
        "voteCount": 4,
        "content": "Agree with Option D"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 133,
    "url": "https://www.examtopics.com/discussions/google/view/69752-exam-professional-cloud-developer-topic-1-question-133/",
    "body": "You recently deployed your application in Google Kubernetes Engine, and now need to release a new version of your application. You need the ability to instantly roll back to the previous version in case there are issues with the new version. Which deployment model should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPerform a rolling deployment, and test your new application after the deployment is complete.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPerform A/B testing, and test your application periodically after the new tests are implemented.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPerform a blue/green deployment, and test your new application after the deployment is. complete.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPerform a canary deployment, and test your new application periodically after the new version is deployed."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-01-21T21:21:00.000Z",
        "voteCount": 8,
        "content": "Option C is correct"
      },
      {
        "date": "2023-09-21T20:53:00.000Z",
        "voteCount": 1,
        "content": "C is correct."
      },
      {
        "date": "2023-08-05T10:55:00.000Z",
        "voteCount": 1,
        "content": "The key here is rolling back to the previous deployments if we find issues with the current(latest) deployment.  With Canary, only certain portion of the traffic is allowed to newer version that does on the fly testing.  With A/B, certain portion of the traffic is split to the dedicated testers to confirm everything is fine with the newer version."
      },
      {
        "date": "2023-01-11T06:03:00.000Z",
        "voteCount": 1,
        "content": "C. Perform a blue/green deployment, and test your new application after the deployment is complete.\n\nA Blue/Green deployment is a technique that allows you to release new versions of an application while maintaining the ability to roll back to the previous version if there are issues. It works by having two identical production environments: one, the \"green\" environment, that is serving traffic, and another, the \"blue\" environment, that is idle. When you want to release a new version of your application, you deploy it to the \"blue\" environment, test it to make sure it is working as expected and then switch traffic to the \"blue\" environment.\nThis way you can have zero-downtime deployment and if there's any issues with the new version you can easily roll back to the previous version by switching the traffic back to the green environment."
      },
      {
        "date": "2023-01-11T06:03:00.000Z",
        "voteCount": 2,
        "content": "B. Perform A/B testing, and test your application periodically after the new tests are implemented.\nA/B testing is used to test different versions of an application, it doesn't provide an instant rollback in case of issues."
      },
      {
        "date": "2023-01-11T06:03:00.000Z",
        "voteCount": 2,
        "content": "A. Perform a rolling deployment, and test your new application after the deployment is complete.\nThis model does not allow an instant rollback as it does not have a parallel environment to switch traffic to."
      },
      {
        "date": "2023-01-11T06:03:00.000Z",
        "voteCount": 2,
        "content": "D. Perform a canary deployment, and test your new application periodically after the new version is deployed.\nCanary deployment is similar to blue/green deployment but in this approach it's rolling out the new version of application gradually, it's used to test the new version with a small percentage of the traffic before rolling it out to the entire environment, if there are issues it's harder to roll back since it's already rolled out to some of the users."
      },
      {
        "date": "2022-12-17T20:01:00.000Z",
        "voteCount": 1,
        "content": "C is the answer.\n\nhttps://cloud.google.com/architecture/application-deployment-and-testing-strategies#choosing_the_right_strategy"
      },
      {
        "date": "2022-10-22T22:14:00.000Z",
        "voteCount": 1,
        "content": "ABCD can roll back.\nC the answer becomes because the condition must be an immediate rollback.\nhttps://cloud.google.com/architecture/application-deployment-and-testing-strategies#choosing_the_right_strategy"
      },
      {
        "date": "2022-10-22T22:15:00.000Z",
        "voteCount": 1,
        "content": "immediate -&gt; instant"
      },
      {
        "date": "2022-08-20T00:19:00.000Z",
        "voteCount": 2,
        "content": "C seems correct"
      },
      {
        "date": "2022-05-20T13:48:00.000Z",
        "voteCount": 1,
        "content": "Agree with b/g deployment"
      },
      {
        "date": "2022-04-18T05:44:00.000Z",
        "voteCount": 1,
        "content": "Vote C"
      },
      {
        "date": "2022-01-11T04:52:00.000Z",
        "voteCount": 3,
        "content": "In my opinion is C!\nD could work but we do not have detailed information about the traffic (synthetic or not, if we want to move it gradually or not...) for this reason Blue/Green is probably enough"
      },
      {
        "date": "2022-01-09T07:41:00.000Z",
        "voteCount": 2,
        "content": "I vote C"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 134,
    "url": "https://www.examtopics.com/discussions/google/view/70411-exam-professional-cloud-developer-topic-1-question-134/",
    "body": "You developed a JavaScript web application that needs to access Google Drive's API and obtain permission from users to store files in their Google Drives. You need to select an authorization approach for your application. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an API key.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a SAML token.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a service account.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an OAuth Client ID.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 9,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-07-22T22:38:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is D. Create an OAuth Client ID. Here's why:\n\nOAuth 2.0 for User Authorization: OAuth 2.0 is the standard protocol for delegated authorization. It allows your web application to request limited access to a user's Google Drive account without requiring their Google account password.\nOAuth Client ID: To use OAuth 2.0, you need to create an OAuth Client ID in the Google Cloud Console. This ID represents your application and is used to identify it during the authorization process."
      },
      {
        "date": "2024-07-22T22:38:00.000Z",
        "voteCount": 1,
        "content": "Why the other options are incorrect:\nA. API Key: API keys are used for general API access, but they don't provide user-specific authorization. They are not suitable for granting access to a user's Google Drive.\nB. SAML Token: SAML (Security Assertion Markup Language) is primarily used for single sign-on (SSO) and identity federation. It's not the standard approach for authorizing access to Google Drive.\nC. Service Account: Service accounts are used to grant access to Google Cloud resources on behalf of your application, not individual users. They are not suitable for user-specific authorization."
      },
      {
        "date": "2023-09-21T20:56:00.000Z",
        "voteCount": 1,
        "content": "OAuth 2.0 is an authorization framework that enables applications to obtain limited access to user accounts on an HTTP service, such as Google Drive. OAuth 2.0 is the preferred authorization approach for JavaScript web applications because it provides a secure and user-friendly way to obtain permission from users to access their Google Drive accounts."
      },
      {
        "date": "2023-09-21T20:56:00.000Z",
        "voteCount": 1,
        "content": "Wrongly Selected C It should be D.\nOAuth 2.0 is an authorization framework that enables applications to obtain limited access to user accounts on an HTTP service, such as Google Drive. OAuth 2.0 is the preferred authorization approach for JavaScript web applications because it provides a secure and user-friendly way to obtain permission from users to access their Google Drive accounts."
      },
      {
        "date": "2023-08-05T11:04:00.000Z",
        "voteCount": 2,
        "content": "We need to have Oauth 2.1 flow.  The client app should have client id and secret key generated from Google drive application.  This way the user can login to their google drive account and can perform CRUD operations.  The best thing here is, the client app is not aware of the user credentials, and it is very secure.  The most common way of getting access token is from authorization code flow with PKCE.  PKCE, since it is a JS client app."
      },
      {
        "date": "2023-01-11T06:04:00.000Z",
        "voteCount": 1,
        "content": "D. Create an OAuth Client ID.\n\nOAuth is an authorization framework that allows third-party applications to access resources on behalf of a user, without having to handle the user's credentials. To use Google Drive's API, your application needs to obtain permission from the user to access their Google Drive, and the best way to do this is through OAuth.\nYou would need to create an OAuth 2.0 client ID and integrate it into your application. This will allow your application to redirect users to the Google OAuth 2.0 server, where they can grant permission to your application to access their Google Drive."
      },
      {
        "date": "2023-01-11T06:04:00.000Z",
        "voteCount": 1,
        "content": "A. Create an API key is not secure enough to give the permission of the user Google drive access\nB. Create a SAML token: SAML is used for identity and access management, it doesn't give access to user's google drive.\nC. Create a service account: Service account is used for server to server communication, it doesn't allow for user-level access to their Google Drive."
      },
      {
        "date": "2022-12-17T19:59:00.000Z",
        "voteCount": 1,
        "content": "D is the answer.\n\nhttps://developers.google.com/drive/api/guides/api-specific-auth"
      },
      {
        "date": "2022-08-20T00:20:00.000Z",
        "voteCount": 3,
        "content": "D is correct"
      },
      {
        "date": "2022-08-06T08:00:00.000Z",
        "voteCount": 1,
        "content": "Yes! it's D."
      },
      {
        "date": "2022-04-09T08:07:00.000Z",
        "voteCount": 1,
        "content": "Correct answer - D"
      },
      {
        "date": "2022-01-22T05:39:00.000Z",
        "voteCount": 4,
        "content": "Option D\nhttps://developers.google.com/drive/api/v3/about-auth"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 135,
    "url": "https://www.examtopics.com/discussions/google/view/69756-exam-professional-cloud-developer-topic-1-question-135/",
    "body": "You manage an ecommerce application that processes purchases from customers who can subsequently cancel or change those purchases. You discover that order volumes are highly variable and the backend order-processing system can only process one request at a time. You want to ensure seamless performance for customers regardless of usage volume. It is crucial that customers' order update requests are performed in the sequence in which they were generated. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSend the purchase and change requests over WebSockets to the backend.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSend the purchase and change requests as REST requests to the backend.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a Pub/Sub subscriber in pull mode and use a data store to manage ordering.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a Pub/Sub subscriber in push mode and use a data store to manage ordering."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 7,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-01-09T07:57:00.000Z",
        "voteCount": 9,
        "content": "I vote C\nhttps://cloud.google.com/pubsub/docs/pull"
      },
      {
        "date": "2022-01-23T09:30:00.000Z",
        "voteCount": 2,
        "content": "Agreed considering only one request can be processed at a time"
      },
      {
        "date": "2022-01-23T09:31:00.000Z",
        "voteCount": 3,
        "content": "And there are a large number of incoming requests Pub Sub is needed"
      },
      {
        "date": "2024-09-05T09:11:00.000Z",
        "voteCount": 1,
        "content": "Why the other options are not as suitable:\n\nOption A: WebSockets provide real-time communication, but they might not be suitable for all scenarios, especially if the backend needs to process requests asynchronously.\nOption B: Using REST requests directly can lead to performance bottlenecks under high load, as the backend might become overwhelmed with requests.\nOption C: A Pub/Sub subscriber in pull mode requires the frontend to actively poll for messages, which can introduce latency and overhead."
      },
      {
        "date": "2023-09-21T20:59:00.000Z",
        "voteCount": 1,
        "content": "C is correct."
      },
      {
        "date": "2023-08-05T11:08:00.000Z",
        "voteCount": 1,
        "content": "Pull model so that application handle the requests by pulling requests one by one.  This is called event driven architecture where the response to client from the app will happen asynchronously."
      },
      {
        "date": "2023-01-11T06:05:00.000Z",
        "voteCount": 1,
        "content": "C. Use a Pub/Sub subscriber in pull mode and use a data store to manage ordering.\n\nTo ensure that customer order update requests are performed in the sequence in which they were generated, the recommended approach is to use a Pub/Sub subscriber in pull mode, together with a data store to manage ordering.\nThis approach allows the backend system to process requests one at a time, while maintaining the order of requests. By using a pull-based subscription, the backend system can control the rate at which messages are consumed from the Pub/Sub topic, and can ensure that requests are processed in the correct order. The data store can be used to maintain a queue of requests, where each request is added to the queue in the order that it was generated, and then processed by the backend system."
      },
      {
        "date": "2023-01-11T06:05:00.000Z",
        "voteCount": 1,
        "content": "A. Send the purchase and change requests over WebSockets to the backend.\nWebSockets are a protocol for bidirectional communication between a client and server, it may not ensure that requests are processed in the order they were generated.\n\nB. Send the purchase and change requests as REST requests to the backend.\nSending the request as REST does not ensure that requests are processed in the order they were generated, it also would not allow controlling the rate at which requests are consumed.\n\nD. Use a Pub/Sub subscriber in push mode and use a data store to manage ordering.\nPush-based subscription don't allow controlling the rate at which requests are consumed, it also may not ensure that requests are processed in the order they were generated."
      },
      {
        "date": "2022-12-17T19:57:00.000Z",
        "voteCount": 1,
        "content": "C is the answer."
      },
      {
        "date": "2022-08-20T00:20:00.000Z",
        "voteCount": 2,
        "content": "C is correct"
      },
      {
        "date": "2022-07-12T07:15:00.000Z",
        "voteCount": 1,
        "content": "Correct answer C"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 136,
    "url": "https://www.examtopics.com/discussions/google/view/70393-exam-professional-cloud-developer-topic-1-question-136/",
    "body": "Your company needs a database solution that stores customer purchase history and meets the following requirements:<br>\u2711 Customers can query their purchase immediately after submission.<br>\u2711 Purchases can be sorted on a variety of fields.<br>\u2711 Distinct record formats can be stored at the same time.<br>Which storage option satisfies these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFirestore in Native mode\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Storage using an object read",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud SQL using a SQL SELECT statement",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFirestore in Datastore mode using a global query"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-01-21T21:29:00.000Z",
        "voteCount": 6,
        "content": "Agree with Option A \nhttps://cloud.google.com/datastore/docs/firestore-or-datastore"
      },
      {
        "date": "2023-09-21T21:02:00.000Z",
        "voteCount": 1,
        "content": "The answer is A. Firestore in Native mode.\n\nFirestore in Native mode is a NoSQL document database that is designed for scalability, performance, and ease of use. It is a good choice for storing customer purchase history because it meets all of the requirements"
      },
      {
        "date": "2023-08-06T04:01:00.000Z",
        "voteCount": 2,
        "content": "Firestore is for storing semi structured data.  It is optimized for high reads and low writes.  Since each document can store different collection types, ( MONGO DB ), fire store is suitable for the above requirements."
      },
      {
        "date": "2023-01-11T06:06:00.000Z",
        "voteCount": 1,
        "content": "A. Firestore in Native mode\n\nFirestore in Native mode satisfies these requirements. It is a NoSQL document database, which means that it stores semi-structured data, and each document can have its own fields and structure. This allows for storing distinct record formats at the same time, which is a requirement. Firestore also has strong query performance and support, customers can query their purchase immediately after submission, and purchases can be sorted on a variety of fields, it is highly optimized to support real-time queries, you can retrieve data with low latency."
      },
      {
        "date": "2023-01-11T06:06:00.000Z",
        "voteCount": 1,
        "content": "B. Cloud Storage using an object read\nCloud Storage is an object storage service and it is not optimized for real-time queries as it does not support secondary indexes or SQL-like queries.\n\nC. Cloud SQL using a SQL SELECT statement\nCloud SQL is a relational database service that supports SQL statements and it would be possible to use SQL SELECT statements to sort purchase by different fields but it not optimized for real-time queries and the distinct record formats may be challenging to implement.\n\nD. Firestore in Datastore mode using a global query\nFirestore in Datastore mode is a previous generation of Firestore and it does not support the same level of query support and performance as Firestore in Native mode, it may also face challenges to support real-time query and distinct record formats."
      },
      {
        "date": "2022-12-17T19:55:00.000Z",
        "voteCount": 2,
        "content": "A is the answer."
      },
      {
        "date": "2022-09-05T04:15:00.000Z",
        "voteCount": 1,
        "content": "@megn they mean that each record can have a different shape, the data is not consistent."
      },
      {
        "date": "2022-08-20T00:20:00.000Z",
        "voteCount": 2,
        "content": "A is correct"
      },
      {
        "date": "2022-07-03T23:52:00.000Z",
        "voteCount": 3,
        "content": "Firestore is the next major version of Datastore and a re-branding of the product. Taking the best of Datastore and the Firebase Realtime Database, Firestore is a NoSQL document database built for automatic scaling, high performance, and ease of application development.\n\nFirestore introduces new features such as:\n\nA new, strongly consistent storage layer\nA collection and document data model\nReal-time updates\nMobile and Web client libraries\nFirestore is backwards compatible with Datastore, but the new data model, real-time updates, and mobile and web client library features are not. To access all of the new Firestore features, you must use Firestore in Native mode."
      },
      {
        "date": "2022-05-27T11:09:00.000Z",
        "voteCount": 1,
        "content": "What do they mean by \"Distinct record formats\"?"
      },
      {
        "date": "2022-05-17T20:17:00.000Z",
        "voteCount": 3,
        "content": "firestore native mode:\nA new, strongly consistent storage layer\nA collection and document data model\nReal-time updates\nMobile and Web client libraries"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 137,
    "url": "https://www.examtopics.com/discussions/google/view/69749-exam-professional-cloud-developer-topic-1-question-137/",
    "body": "You recently developed a new service on Cloud Run. The new service authenticates using a custom service and then writes transactional information to a Cloud<br>Spanner database. You need to verify that your application can support up to 5,000 read and 1,000 write transactions per second while identifying any bottlenecks that occur. Your test infrastructure must be able to autoscale. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBuild a test harness to generate requests and deploy it to Cloud Run. Analyze the VPC Flow Logs using Cloud Logging.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Google Kubernetes Engine cluster running the Locust or JMeter images to dynamically generate load tests. Analyze the results using Cloud Trace.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Cloud Task to generate a test load. Use Cloud Scheduler to run 60,000 Cloud Task transactions per minute for 10 minutes. Analyze the results using Cloud Monitoring.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Compute Engine instance that uses a LAMP stack image from the Marketplace, and use Apache Bench to generate load tests against the service. Analyze the results using Cloud Trace."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-01-09T07:24:00.000Z",
        "voteCount": 7,
        "content": "I vote B\nhttps://cloud.google.com/architecture/distributed-load-testing-using-gke"
      },
      {
        "date": "2023-09-21T21:07:00.000Z",
        "voteCount": 1,
        "content": "B is correct."
      },
      {
        "date": "2023-08-06T04:05:00.000Z",
        "voteCount": 1,
        "content": "The key here is \"Your test infrastructure must be able to autoscale\" and load testing."
      },
      {
        "date": "2023-01-11T06:08:00.000Z",
        "voteCount": 1,
        "content": "B. Create a Google Kubernetes Engine cluster running the Locust or JMeter images to dynamically generate load tests. Analyze the results using Cloud Trace.\n\nTo verify that your application can support up to 5,000 read and 1,000 write transactions per second and to identify any bottlenecks that occur, you can use a load testing tool such as Locust or JMeter to generate load tests on your Cloud Run service. These tools allow you to simulate a high number of concurrent requests and help you determine the maximum number of requests your service can handle.\n\nYou can run the load testing tool on a Google Kubernetes Engine (GKE) cluster which will support autoscale feature, this way you can handle the high number of requests, and use Cloud Trace to analyze the results, which will give you insights into the performance and any bottlenecks."
      },
      {
        "date": "2023-01-11T06:08:00.000Z",
        "voteCount": 1,
        "content": "A. Build a test harness to generate requests and deploy it to Cloud Run. Analyze the VPC Flow Logs using Cloud Logging.\nVPC flow logs would not provide the transaction details, it's more useful to troubleshoot issues at the network level.\n\nC. Create a Cloud Task to generate a test load. Use Cloud Scheduler to run 60,000 Cloud Task transactions per minute for 10 minutes. Analyze the results using Cloud Monitoring.\nAlthough cloud task is a good solution for scheduling the test loads, it's not the best solution for load testing since it doesn't support dynamic loading and it would be hard to get the fine-grained details about the performance.\n\nD. Create a Compute Engine instance that uses a LAMP stack image from the Marketplace, and use Apache Bench to"
      },
      {
        "date": "2022-12-17T19:51:00.000Z",
        "voteCount": 1,
        "content": "B is the answer.\n\nhttps://cloud.google.com/architecture/distributed-load-testing-using-gke"
      },
      {
        "date": "2022-08-20T00:20:00.000Z",
        "voteCount": 2,
        "content": "B is correct"
      },
      {
        "date": "2022-07-04T01:30:00.000Z",
        "voteCount": 1,
        "content": "This tutorial explains how to use Google Kubernetes Engine (GKE) to deploy a distributed load testing framework that uses multiple containers to create traffic for a simple REST-based API. This tutorial load-tests a web application deployed to App Engine that exposes REST-style endpoints to respond to incoming HTTP POST requests.\n\nYou can use this same pattern to create load testing frameworks for a variety of scenarios and applications, such as messaging systems, data stream management systems, and database systems."
      },
      {
        "date": "2022-04-03T05:58:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is B"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 138,
    "url": "https://www.examtopics.com/discussions/google/view/69747-exam-professional-cloud-developer-topic-1-question-138/",
    "body": "You are using Cloud Build for your CI/CD pipeline to complete several tasks, including copying certain files to Compute Engine virtual machines. Your pipeline requires a flat file that is generated in one builder in the pipeline to be accessible by subsequent builders in the same pipeline. How should you store the file so that all the builders in the pipeline can access it?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore and retrieve the file contents using Compute Engine instance metadata.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOutput the file contents to a file in /workspace. Read from the same /workspace file in the subsequent build step.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse gsutil to output the file contents to a Cloud Storage object. Read from the same object in the subsequent build step.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a build argument that runs an HTTP POST via curl to a separate web server to persist the value in one builder. Use an HTTP GET via curl from the subsequent build step to read the value."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 8,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-01-09T07:03:00.000Z",
        "voteCount": 10,
        "content": "I vote B\nhttps://cloud.google.com/build/docs/build-config-file-schema"
      },
      {
        "date": "2024-09-10T00:02:00.000Z",
        "voteCount": 1,
        "content": "Cloud Storage is designed for this: Cloud Storage is a robust, scalable, and reliable object storage service. It's perfect for storing files that need to be accessed by multiple parts of your CI/CD pipeline.\nShared access: Cloud Storage objects can be accessed by any builder in your pipeline, as long as they have the necessary permissions.\nSimplicity: Using gsutil to interact with Cloud Storage is straightforward and well-documented."
      },
      {
        "date": "2024-09-10T00:02:00.000Z",
        "voteCount": 1,
        "content": "A. Store and retrieve the file contents using Compute Engine instance metadata: Instance metadata is primarily for managing the instance itself, not for sharing data between builders in a pipeline.\nB. Output the file contents to a file in /workspace. Read from the same /workspace file in the subsequent build step: This approach is limited to the current build step. If the build step is restarted, the file in /workspace will be lost.\nD. Add a build argument that runs an HTTP POST via curl to a separate web server to persist the value in one builder. Use an HTTP GET via curl from the subsequent build step to read the value: This is overly complex and introduces unnecessary dependencies on external services."
      },
      {
        "date": "2023-08-06T04:09:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is B.  Save your flat file under /workspace folder and hence the same file can be used for other build steps.  Very simple and straight forward approach though. :)"
      },
      {
        "date": "2023-02-27T06:13:00.000Z",
        "voteCount": 1,
        "content": "I vote B\nhttps://cloud.google.com/build/docs/build-config-file-schema"
      },
      {
        "date": "2023-01-11T06:09:00.000Z",
        "voteCount": 1,
        "content": "The best approach is to output the file contents to a file in /workspace directory in one build step and read from the same /workspace file in the subsequent build step . This way, the file is easily accessible by all builders in the pipeline as they all run in the same environment and share the same file system. And it's the easiest and simplest way of sharing the file between the builds in the pipeline."
      },
      {
        "date": "2022-12-17T19:48:00.000Z",
        "voteCount": 1,
        "content": "B is the answer.\n\nhttps://cloud.google.com/build/docs/configuring-builds/pass-data-between-steps#passing_data_using_workspaces\nTo pass data between build steps, store the assets produced by the build step in /workspace and these assets will be available to any subsequent build steps."
      },
      {
        "date": "2022-11-27T23:23:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/build/docs/configuring-builds/pass-data-between-steps#passing_data_using_workspaces\nhttps://cloud.google.com/build/docs/configuring-builds/pass-data-between-steps"
      },
      {
        "date": "2022-11-27T23:23:00.000Z",
        "voteCount": 1,
        "content": "Answer B"
      },
      {
        "date": "2022-11-14T08:42:00.000Z",
        "voteCount": 1,
        "content": "B is wrong \n\nhttps://cloud.google.com/build/docs/build-config-file-schema\n\n\nUse the dir field in a build step to set a working directory to use when running the step's container. If you set the dir field in the build step, the working directory is set to /workspace/&lt;dir&gt;. If this value is a relative path, it is relative to the build's working directory. If this value is absolute, it may be outside the build's working directory, in which case the contents of the path may NOT be persisted across build step executions"
      },
      {
        "date": "2022-11-16T23:16:00.000Z",
        "voteCount": 1,
        "content": "Ans B is correct \nhttps://cloud.google.com/build/docs/configuring-builds/pass-data-between-steps#passing_data_using_workspaces"
      },
      {
        "date": "2022-11-27T23:18:00.000Z",
        "voteCount": 1,
        "content": "whatsa your answer then"
      },
      {
        "date": "2022-11-16T20:03:00.000Z",
        "voteCount": 1,
        "content": "did you take the exam recently?"
      },
      {
        "date": "2022-08-20T00:21:00.000Z",
        "voteCount": 2,
        "content": "B is correct"
      },
      {
        "date": "2022-07-04T01:48:00.000Z",
        "voteCount": 1,
        "content": "To pass data between build steps, store the assets produced by the build step in /workspace and these assets will be available to any subsequent build steps."
      },
      {
        "date": "2022-05-19T18:49:00.000Z",
        "voteCount": 1,
        "content": "agree with b"
      },
      {
        "date": "2022-05-09T05:21:00.000Z",
        "voteCount": 1,
        "content": "I Vote B\nhttps://cloud.google.com/build/docs/configuring-builds/pass-data-between-steps#passing_data_using_workspaces"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 139,
    "url": "https://www.examtopics.com/discussions/google/view/91966-exam-professional-cloud-developer-topic-1-question-139/",
    "body": "Your company\u2019s development teams want to use various open source operating systems in their Docker builds. When images are created in published containers in your company\u2019s environment, you need to scan them for Common Vulnerabilities and Exposures (CVEs). The scanning process must not impact software development agility. You want to use managed services where possible. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable the Vulnerability scanning setting in the Container Registry.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Cloud Function that is triggered on a code check-in and scan the code for CVEs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDisallow the use of non-commercially supported base images in your development environment.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Monitoring to review the output of Cloud Build to determine whether a vulnerable version has been used."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-09-21T21:14:00.000Z",
        "voteCount": 1,
        "content": "A is correct."
      },
      {
        "date": "2023-08-06T04:11:00.000Z",
        "voteCount": 1,
        "content": "A is a very straight forward option.  One more choice would be using vulnerability scanning tools like Grype ( open source ) in the build step itself with cloud build."
      },
      {
        "date": "2023-01-11T06:11:00.000Z",
        "voteCount": 2,
        "content": "A. Enable the Vulnerability scanning setting in the Container Registry would be the best solution in this case.\n\nIt would allow you to automatically scan images for known vulnerabilities and detect any issues as soon as they're pushed to the registry. This will help to identify vulnerabilities early in the development cycle, allowing the development teams to take action before images are deployed to production. This approach is automated, does not impact development agility and since it is a built-in feature of the Container Registry, it is a managed service and therefore, it does not require additional maintenance and management."
      },
      {
        "date": "2023-01-11T06:12:00.000Z",
        "voteCount": 2,
        "content": "Option B, Create a Cloud Function that is triggered on a code check-in and scan the code for CVEs, would impact development agility as it would add an additional step to the development process which can slow down the development teams and impact the development process.\n\nOption C, Disallow the use of non-commercially supported base images in the development environment, would limit the flexibility of the development teams, and they may not be able to use the best tools for the job which can negatively impact the quality of the end-product.\n\nOption D, Use Cloud Monitoring to review the output of Cloud Build to determine whether a vulnerable version has been used, is a good practice to detect and alert on potential issues as soon as possible, but it is an additional step that needs to be set up and maintained. Additionally, it does not handle the vulnerability scanning on its own but rather acts as an additional layer of security."
      },
      {
        "date": "2022-12-25T03:02:00.000Z",
        "voteCount": 1,
        "content": "https://docs.docker.com/engine/scan/\nAnswer A"
      },
      {
        "date": "2022-12-17T19:46:00.000Z",
        "voteCount": 1,
        "content": "A is the answer.\n\nhttps://cloud.google.com/container-analysis/docs/os-overview"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 140,
    "url": "https://www.examtopics.com/discussions/google/view/89923-exam-professional-cloud-developer-topic-1-question-140/",
    "body": "You are configuring a continuous integration pipeline using Cloud Build to automate the deployment of new container images to Google Kubernetes Engine (GKE). The pipeline builds the application from its source code, runs unit and integration tests in separate steps, and pushes the container to Container Registry. The application runs on a Python web server.<br><br>The Dockerfile is as follows:<br><br><br>FROM python:3.7-alpine -<br><br>COPY . /app -<br><br>WORKDIR /app -<br>RUN pip install -r requirements.txt<br>CMD [ \"gunicorn\", \"-w 4\", \"main:app\" ]<br><br>You notice that Cloud Build runs are taking longer than expected to complete. You want to decrease the build time. What should you do? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSelect a virtual machine (VM) size with higher CPU for Cloud Build runs.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy a Container Registry on a Compute Engine VM in a VPC, and use it to store the final images.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCache the Docker image for subsequent builds using the -- cache-from argument in your build config file.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the base image in the Dockerfile to ubuntu:latest, and install Python 3.7 using a package manager utility.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore application source code on Cloud Storage, and configure the pipeline to use gsutil to download the source code."
    ],
    "answer": "AC",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AC",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-09-21T21:17:00.000Z",
        "voteCount": 1,
        "content": "AC is correct."
      },
      {
        "date": "2023-08-06T04:16:00.000Z",
        "voteCount": 1,
        "content": "Apart from A and C, one more good option would be to copy the app directory only after RUN pip install so that we can avoid this copying part repeatedly after each layer build."
      },
      {
        "date": "2023-01-13T06:04:00.000Z",
        "voteCount": 2,
        "content": "A is correct because a high-CPU virtual machine type can increase the speed of your build.\nB is not correct because a Container Registry on a VM will not speed up the build.\nC is correct because the same container is used in subsequent steps for testing and to be pushed to the registry.\nD is not correct because an ubuntu container image will be significantly larger than the python:3.7-alpine image.\nE is not correct because storing the application source code on Cloud Storage does not decrease the time to build the application."
      },
      {
        "date": "2023-01-11T06:17:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/build/docs/optimize-builds/increase-vcpu-for-builds\nhttps://cloud.google.com/build/docs/optimize-builds/building-leaner-containers#building_leaner_containers\n\nYes, answer A and C are both valid solutions based on the articles you linked.\n\nIncreasing the number of vCPUs allocated to the Cloud Build VM can help to decrease build time because it provides the build environment with more CPU resources to use, which can help to speed up the build process. This can be achieved by selecting a VM size with higher CPU for Cloud Build runs.\n\nas mentioned, caching the Docker image for subsequent builds can also help to decrease build time by reusing previously built image layers. This can be achieved by adding the --cache-from argument to the build command in the build config file, which tells Cloud Build to use the specified images as a cache source."
      },
      {
        "date": "2023-01-11T06:17:00.000Z",
        "voteCount": 1,
        "content": "Option E Storing application source code on Cloud Storage and configuring the pipeline to use gsutil to download the source code can also be a good way to optimize the pipeline. However, it may be less effective than option A and C, so it may be less beneficial to be chosen as a single solution.\n\nIn summary, option A and C are the best solutions that can help to optimize the CI/CD pipeline in this scenario as they directly impact the build process and it also depend on the current infrastructure and requirements of your pipeline if you consider using other options."
      },
      {
        "date": "2022-12-25T02:59:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/build/docs/optimize-builds/increase-vcpu-for-builds\nAnswer A\nhttps://cloud.google.com/build/docs/optimize-builds/building-leaner-containers#building_leaner_containers\nAnswer C"
      },
      {
        "date": "2022-12-17T19:42:00.000Z",
        "voteCount": 1,
        "content": "AC is the answer.\n\nhttps://cloud.google.com/build/docs/optimize-builds/increase-vcpu-for-builds\nBy default, Cloud Build runs your builds on a standard virtual machine (VM). In addition to the standard VM, Cloud Build provides several high-CPU VM types to run builds. To increase the speed of your build, select a machine with a higher vCPU to run builds. Keep in mind that although selecting a high vCPU machine increases your build speed, it may also increase the startup time of your build as Cloud Build only starts non-standard machines on demand.\n\nhttps://cloud.google.com/build/docs/optimize-builds/speeding-up-builds#using_a_cached_docker_image\nThe easiest way to increase the speed of your Docker image build is by specifying a cached image that can be used for subsequent builds. You can specify the cached image by adding the --cache-from argument in your build config file, which will instruct Docker to build using that image as a cache source."
      },
      {
        "date": "2022-12-13T01:20:00.000Z",
        "voteCount": 1,
        "content": "A and C are correct"
      },
      {
        "date": "2022-12-11T04:59:00.000Z",
        "voteCount": 1,
        "content": "why don't I see community voted progress bar?"
      },
      {
        "date": "2022-12-03T22:32:00.000Z",
        "voteCount": 4,
        "content": "IMHO\nD - alpine is a much smaller distro\nB and E - does not make sense\n\nhttps://cloud.google.com/build/docs/optimize-builds/increase-vcpu-for-builds\nhttps://cloud.google.com/build/docs/optimize-builds/speeding-up-builds"
      },
      {
        "date": "2022-12-11T04:59:00.000Z",
        "voteCount": 1,
        "content": "thank you so much"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 141,
    "url": "https://www.examtopics.com/discussions/google/view/89924-exam-professional-cloud-developer-topic-1-question-141/",
    "body": "You are building a CI/CD pipeline that consists of a version control system, Cloud Build, and Container Registry. Each time a new tag is pushed to the repository, a Cloud Build job is triggered, which runs unit tests on the new code builds a new Docker container image, and pushes it into Container Registry. The last step of your pipeline should deploy the new container to your production Google Kubernetes Engine (GKE) cluster. You need to select a tool and deployment strategy that meets the following requirements:<br>\u2022 Zero downtime is incurred<br>\u2022 Testing is fully automated<br>\u2022 Allows for testing before being rolled out to users<br>\u2022 Can quickly rollback if needed<br><br>What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTrigger a Spinnaker pipeline configured as an A/B test of your new code and, if it is successful, deploy the container to production.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTrigger a Spinnaker pipeline configured as a canary test of your new code and, if it is successful, deploy the container to production.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTrigger another Cloud Build job that uses the Kubernetes CLI tools to deploy your new container to your GKE cluster, where you can perform a canary test.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTrigger another Cloud Build job that uses the Kubernetes CLI tools to deploy your new container to your GKE cluster, where you can perform a shadow test.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 8,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-09-10T03:20:00.000Z",
        "voteCount": 1,
        "content": "Zero Downtime: Canary testing allows you to gradually roll out your new container to a small percentage of users while the rest continue using the previous version. This ensures zero downtime during the deployment process.\nAutomated Testing: Spinnaker is a powerful CI/CD platform that can automate your canary testing process, including monitoring the new container's performance and health.\nTesting Before Rollout: Canary testing allows you to test the new container in a production environment with real users, but with limited impact. This helps identify any issues before a full rollout.\nQuick Rollback: If the canary test reveals problems, Spinnaker can quickly roll back to the previous version, minimizing disruption to users."
      },
      {
        "date": "2024-09-10T03:21:00.000Z",
        "voteCount": 1,
        "content": "A. Trigger a Spinnaker pipeline configured as an A/B test of your new code and, if it is successful, deploy the container to production: While A/B testing is valuable for comparing different versions of your application, it's not the best choice for zero-downtime deployments. A/B tests typically involve routing traffic to different versions, which can lead to downtime during the transition.\nC. Trigger another Cloud Build job that uses the Kubernetes CLI tools to deploy your new container to your GKE cluster, where you can perform a canary test: While you can manually perform canary testing using the Kubernetes CLI, it's not as efficient or automated as using a dedicated tool like Spinnaker."
      },
      {
        "date": "2024-09-10T03:21:00.000Z",
        "voteCount": 1,
        "content": "D. Trigger another Cloud Build job that uses the Kubernetes CLI tools to deploy your new container to your GKE cluster, where you can perform a shadow test: Shadow testing involves running the new container alongside the existing one, but without routing traffic to it. This is useful for performance testing but doesn't provide real-world user feedback."
      },
      {
        "date": "2023-09-22T01:00:00.000Z",
        "voteCount": 1,
        "content": "Spinnaker is a cloud native continuous delivery platform that can be used to deploy applications to a variety of cloud providers, including Google Kubernetes Engine (GKE). Spinnaker is a good choice for deploying applications to GKE because it provides a number of features that make it easy to deploy applications quickly and reliably, including:\n\nCanary deployments: Canary deployments allow you to deploy a new version of your application to a small subset of users before rolling it out to all users. This allows you to test the new version of your application and identify any problems before they impact all of your users.\nRollback: Spinnaker can be used to quickly rollback to a previous version of your application if you encounter any problems with the new version."
      },
      {
        "date": "2023-08-06T04:30:00.000Z",
        "voteCount": 1,
        "content": "Shadow testing is the right choice.  Canary is not suitable here since the requirement is to test before rolling new version to users.  Option A also comes very closer since it has A/B testing that is done only after releasing the newer version to users that is only small amount of traffic is diverted to dedicated users (testers) who gives faster feedback about newer product/service."
      },
      {
        "date": "2023-01-11T06:22:00.000Z",
        "voteCount": 2,
        "content": "Option D, triggering another Cloud Build job that uses the Kubernetes CLI tools to deploy your new container to your GKE cluster, where you can perform a shadow test, could meet the requirements you specified.\n\nShadow testing is a technique where you can test the new version of an application by mirroring user traffic to it, without impacting the user requests to the current version. This way, you can test the new version of your application in a real-world environment with real user traffic, which allows for testing before being rolled out to users and allows for a quick rollback if needed. And with the use of Kubernetes CLI tools you can automate this process, so the testing and deployment is fully automated."
      },
      {
        "date": "2022-12-17T19:38:00.000Z",
        "voteCount": 2,
        "content": "D is the answer.\n\nhttps://cloud.google.com/architecture/implementing-deployment-and-testing-strategies-on-gke#perform_a_shadow_test\nWith a shadow test, you test the new version of your application by mirroring user traffic from the current application version without impacting the user requests."
      },
      {
        "date": "2022-12-11T05:07:00.000Z",
        "voteCount": 1,
        "content": "vote D"
      },
      {
        "date": "2022-12-03T22:47:00.000Z",
        "voteCount": 2,
        "content": "IMHO by eliminating\nB and C - uses canary which letting the users use the new version without testing\nA - canary is often a synonym of A/B testing"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 142,
    "url": "https://www.examtopics.com/discussions/google/view/89925-exam-professional-cloud-developer-topic-1-question-142/",
    "body": "Your operations team has asked you to create a script that lists the Cloud Bigtable, Memorystore, and Cloud SQL databases running within a project. The script should allow users to submit a filter expression to limit the results presented. How should you retrieve the data?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the HBase API, Redis API, and MySQL connection to retrieve database lists. Combine the results, and then apply the filter to display the results",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the HBase API, Redis API, and MySQL connection to retrieve database lists. Filter the results individually, and then combine them to display the results",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun gcloud bigtable instances list, gcloud redis instances list, and gcloud sql databases list. Use a filter within the application, and then display the results",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun gcloud bigtable instances list, gcloud redis instances list, and gcloud sql databases list. Use --filter flag with each command, and then display the results\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-09-22T01:07:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2023-08-06T05:12:00.000Z",
        "voteCount": 1,
        "content": "Easy and simple.  List all the different types of instances and apply '--filter' option in a command."
      },
      {
        "date": "2023-01-11T06:24:00.000Z",
        "voteCount": 1,
        "content": "Option D is correct, running gcloud bigtable instances list, gcloud redis instances list, and gcloud sql databases list and using the --filter flag with each command can be used to filter the results before displaying them. This would allow users to submit a filter expression to limit the results presented as specified in the question. As per the google official documentation."
      },
      {
        "date": "2022-12-25T02:52:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/sdk/gcloud/reference/topic/filters\nAnswer D"
      },
      {
        "date": "2022-12-17T19:31:00.000Z",
        "voteCount": 2,
        "content": "D is the answer.\n\nhttps://cloud.google.com/sdk/gcloud/reference/topic/filters\nMost gcloud commands return a list of resources on success. By default they are pretty-printed on the standard output. The --format=NAME[ATTRIBUTES](PROJECTION) and --filter=EXPRESSION flags along with projections can be used to format and change the default output to a more meaningful result.\nUse the --format flag to change the default output format of a command. For details run $ gcloud topic formats.\n\nUse the --filter flag to select resources to be listed. Resource filters are described in detail below."
      },
      {
        "date": "2022-12-14T02:43:00.000Z",
        "voteCount": 1,
        "content": "option D"
      },
      {
        "date": "2022-12-11T05:06:00.000Z",
        "voteCount": 1,
        "content": "vote D"
      },
      {
        "date": "2022-12-03T22:52:00.000Z",
        "voteCount": 2,
        "content": "IMHO can't see the purpose of using HBase\n\nAnswer is D. use the --filter flag\nhttps://cloud.google.com/sdk/gcloud/reference/topic/filters"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 143,
    "url": "https://www.examtopics.com/discussions/google/view/89912-exam-professional-cloud-developer-topic-1-question-143/",
    "body": "You need to deploy a new European version of a website hosted on Google Kubernetes Engine. The current and new websites must be accessed via the same HTTP(S) load balancer's external IP address, but have different domain names. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDefine a new Ingress resource with a host rule matching the new domain",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the existing Ingress resource with a host rule matching the new domain\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new Service of type LoadBalancer specifying the existing IP address as the loadBalancerIP",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGenerate a new Ingress resource and specify the existing IP address as the kubernetes.io/ingress.global-static-ip-name annotation value"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 8,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-09-10T03:33:00.000Z",
        "voteCount": 1,
        "content": "Single Load Balancer with Multiple Host Rules:\n\nIn Google Kubernetes Engine, you can configure a single Ingress resource with multiple host rules that route traffic based on different domain names. This allows you to use the same load balancer but handle traffic differently depending on the domain.\nBy modifying the existing Ingress resource to include a host rule for the new domain, the same load balancer IP can be used to serve both the current website and the new European version.\nHost Rule Matching:\n\nHost rules in the Ingress resource enable domain-based routing, where traffic is directed to the correct service based on the requested domain. This is ideal for scenarios like yours where you need to serve multiple websites under the same load balancer but different domain names."
      },
      {
        "date": "2024-09-10T03:33:00.000Z",
        "voteCount": 1,
        "content": "A. New Ingress resource: Defining a new Ingress resource with a host rule would create a second Ingress resource, which could lead to conflicts or issues since you want to use the same external IP. The existing Ingress should be modified instead.\n\nC. New Service with LoadBalancer type: This would create a new load balancer, which goes against the requirement to use the same external IP. Also, setting the loadBalancerIP does not achieve domain-based routing.\n\nD. New Ingress resource with static IP: While this allows you to assign an existing static IP, it's unnecessary if you're already using the same load balancer. Modifying the existing Ingress with additional host rules is more efficient and aligns with the goal."
      },
      {
        "date": "2023-09-22T01:11:00.000Z",
        "voteCount": 1,
        "content": "B is correct."
      },
      {
        "date": "2023-08-06T05:19:00.000Z",
        "voteCount": 1,
        "content": "Right answer is B.  Existing Ingress resource needs to be updated to add new domain for the new service that runs within the cluster of worker nodes. It looks like this:\nUser ---&gt; HTTP(S) Load balance IP   --------&gt; Domain 1 -----&gt; Older version of application.\n                                                             --------&gt; Domain 2 -----&gt; New version of application."
      },
      {
        "date": "2023-01-11T06:27:00.000Z",
        "voteCount": 1,
        "content": "Based on the requirements and the references\n\nhttps://kubernetes.io/docs/concepts/services-networking/ingress/#name-based-virtual-hosting\nhttps://cloud.google.com/kubernetes-engine/docs/tutorials/configuring-domain-name-static-ip\n\nB. You should modify the existing Ingress resource with a host rule matching the new domain. This will allow you to route traffic to the new website while still using the same IP address and load balancer. This approach allows you to use name-based virtual hosting, which supports routing HTTP traffic to multiple host names at the same IP address. It also enables you to reuse the existing IP address and load balancer, which means that the existing website and the new website can be accessed through the same IP address while having different domain names."
      },
      {
        "date": "2022-12-17T18:26:00.000Z",
        "voteCount": 1,
        "content": "B is the answer.\n\nhttps://kubernetes.io/docs/concepts/services-networking/ingress/#name-based-virtual-hosting\nName-based virtual hosts support routing HTTP traffic to multiple host names at the same IP address."
      },
      {
        "date": "2022-12-03T23:04:00.000Z",
        "voteCount": 1,
        "content": "Answer is D\nhttps://cloud.google.com/kubernetes-engine/docs/tutorials/configuring-domain-name-static-ip"
      },
      {
        "date": "2022-12-03T16:50:00.000Z",
        "voteCount": 3,
        "content": "\"must be accessed via the same HTTP(S) load balancer's external IP address\" means re-use the existing ingress resource"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 144,
    "url": "https://www.examtopics.com/discussions/google/view/89926-exam-professional-cloud-developer-topic-1-question-144/",
    "body": "You are developing a single-player mobile game backend that has unpredictable traffic patterns as users interact with the game throughout the day and night. You want to optimize costs by ensuring that you have enough resources to handle requests, but minimize over-provisioning. You also want the system to handle traffic spikes efficiently. Which compute platform should you use?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Run\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCompute Engine with managed instance groups",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCompute Engine with unmanaged instance groups",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGoogle Kubernetes Engine using cluster autoscaling"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 9,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-07-13T03:53:00.000Z",
        "voteCount": 1,
        "content": "correct answer A"
      },
      {
        "date": "2024-04-13T20:37:00.000Z",
        "voteCount": 1,
        "content": "\"handle traffic spikes efficiently\"\nCloud run the fastest to autoscale among the given options. \nGKE could be considered too, but Cloud Run s cheaper"
      },
      {
        "date": "2023-12-04T11:21:00.000Z",
        "voteCount": 1,
        "content": "Cloud Run is the cheapest solution among these options and can scale up and down to 0 instances."
      },
      {
        "date": "2023-09-22T01:15:00.000Z",
        "voteCount": 1,
        "content": "Google Kubernetes Engine (GKE) is a managed Kubernetes service that allows you to deploy and run containerized applications. GKE is a good choice for running a single-player mobile game backend because it can be easily scaled up or down to meet the needs of your game.\nCloud Run is a serverless computing platform that allows you to run code without managing servers. Cloud Run is a good choice for running simple applications, but it is not as scalable as GKE."
      },
      {
        "date": "2023-08-06T05:24:00.000Z",
        "voteCount": 1,
        "content": "I go with A.  The requirement is to optimize the cost while scaling for unexpected spikes in the traffic.  Cloud Run is the cheapest among all the other options given."
      },
      {
        "date": "2023-04-23T07:38:00.000Z",
        "voteCount": 1,
        "content": "Bing chose D: For a single-player mobile game backend with unpredictable traffic patterns and a need to optimize costs while handling traffic spikes efficiently, Google Kubernetes Engine (GKE) using cluster autoscaling (option D) would be a good choice. GKE\u2019s cluster autoscaler automatically resizes the number of nodes in a node pool based on the demands of your workloads. This helps ensure that you have enough resources to handle requests while minimizing over-provisioning and optimizing costs."
      },
      {
        "date": "2022-12-26T10:25:00.000Z",
        "voteCount": 1,
        "content": "Answer A"
      },
      {
        "date": "2022-12-27T01:00:00.000Z",
        "voteCount": 1,
        "content": "Did you take the exam?"
      },
      {
        "date": "2023-01-06T04:40:00.000Z",
        "voteCount": 1,
        "content": "Not yet"
      },
      {
        "date": "2022-12-18T15:26:00.000Z",
        "voteCount": 4,
        "content": "Compute Engine answers are eliminated because they can't scale quickly enough. \nGKE Answer is ruled out because you can end up overprovisioned, also cannot scale out to add more nodes quickly enough."
      },
      {
        "date": "2022-12-17T02:09:00.000Z",
        "voteCount": 1,
        "content": "A is the answer."
      },
      {
        "date": "2022-12-11T05:26:00.000Z",
        "voteCount": 1,
        "content": "vote D"
      },
      {
        "date": "2022-12-03T23:10:00.000Z",
        "voteCount": 1,
        "content": "Answer is D to lessen the over-provisioning\nNot A - the app is not containerized\nNot sure with Compute Engine\n\nhttps://cloud.google.com/blog/products/containers-kubernetes/gke-best-practices-to-lessen-over-provisioning"
      },
      {
        "date": "2022-12-17T02:09:00.000Z",
        "voteCount": 3,
        "content": "GKE requires app to be containerised."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 145,
    "url": "https://www.examtopics.com/discussions/google/view/89927-exam-professional-cloud-developer-topic-1-question-145/",
    "body": "The development teams in your company want to manage resources from their local environments. You have been asked to enable developer access to each team\u2019s Google Cloud projects. You want to maximize efficiency while following Google-recommended best practices. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd the users to their projects, assign the relevant roles to the users, and then provide the users with each relevant Project ID.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd the users to their projects, assign the relevant roles to the users, and then provide the users with each relevant Project Number.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate groups, add the users to their groups, assign the relevant roles to the groups, and then provide the users with each relevant Project ID.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate groups, add the users to their groups, assign the relevant roles to the groups, and then provide the users with each relevant Project Number."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-13T20:39:00.000Z",
        "voteCount": 1,
        "content": ". Create groups, add the users to their groups, assign the relevant roles to the groups, and then provide the users with each relevant Project ID"
      },
      {
        "date": "2023-09-22T01:20:00.000Z",
        "voteCount": 1,
        "content": "This is the most efficient and secure way to enable developer access to Google Cloud projects. By creating groups and assigning roles to the groups, you can minimize the administrative overhead of managing user permissions. You can also provide developers with access to the projects they need, while limiting their access to other resources."
      },
      {
        "date": "2023-08-06T05:32:00.000Z",
        "voteCount": 1,
        "content": "I choose C.  Adding users to a group and assigning the role to a group is a good practice as IAM is concerned.  The project ID is the more user-friendly identifier and the one which most Cloud APIs and user interfaces use when interfacing with you, the customer.\n\nThe project number is an internal implementation detail and is the key that most Google Cloud services use for storing data in their databases; most API calls implicitly translate the ID to the number when performing queries for project details."
      },
      {
        "date": "2022-12-26T10:24:00.000Z",
        "voteCount": 1,
        "content": "Answer C"
      },
      {
        "date": "2022-12-17T02:06:00.000Z",
        "voteCount": 1,
        "content": "C is the answer."
      },
      {
        "date": "2022-12-14T02:47:00.000Z",
        "voteCount": 1,
        "content": "option C"
      },
      {
        "date": "2022-12-11T05:27:00.000Z",
        "voteCount": 2,
        "content": "vote C"
      },
      {
        "date": "2022-12-03T23:15:00.000Z",
        "voteCount": 1,
        "content": "Best practice is to create a group\nnot sure between project ID and project number"
      },
      {
        "date": "2023-06-14T23:50:00.000Z",
        "voteCount": 1,
        "content": "What is the difference between C &amp; D? I can use both project ID and project number to find a project is the GCP console"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 146,
    "url": "https://www.examtopics.com/discussions/google/view/89928-exam-professional-cloud-developer-topic-1-question-146/",
    "body": "Your company\u2019s product team has a new requirement based on customer demand to autoscale your stateless and distributed service running in a Google Kubernetes Engine (GKE) duster. You want to find a solution that minimizes changes because this feature will go live in two weeks. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy a Vertical Pod Autoscaler, and scale based on the CPU load.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy a Vertical Pod Autoscaler, and scale based on a custom metric.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy a Horizontal Pod Autoscaler, and scale based on the CPU toad.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy a Horizontal Pod Autoscaler, and scale based on a custom metric."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 12,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-09-10T08:10:00.000Z",
        "voteCount": 1,
        "content": "Minimal Changes: Horizontal Pod Autoscaler (HPA) is a built-in Kubernetes feature that requires minimal configuration. You can quickly enable it and configure it to scale based on CPU utilization, which is a standard metric readily available in Kubernetes.\nStateless and Distributed Service: HPA is well-suited for stateless and distributed services. It scales by adding or removing replicas of your service, ensuring that your application remains distributed and handles load efficiently.\nTwo-Week Deadline: HPA is a straightforward solution that can be deployed and configured within a two-week timeframe."
      },
      {
        "date": "2024-09-10T08:10:00.000Z",
        "voteCount": 1,
        "content": "A. Deploy a Vertical Pod Autoscaler, and scale based on the CPU load: Vertical Pod Autoscaler (VPA) scales the resources (CPU and memory) of individual pods, not the number of pods. This might not be the most efficient approach for a stateless and distributed service.\nB. Deploy a Vertical Pod Autoscaler, and scale based on a custom metric: VPA with custom metrics requires more effort to set up and configure. It's not the most efficient solution for a quick deployment.\nD. Deploy a Horizontal Pod Autoscaler, and scale based on a custom metric: While HPA with custom metrics can be powerful, it requires more time to set up and configure. For a two-week deadline, using CPU load as a metric is a simpler and faster approach."
      },
      {
        "date": "2023-09-22T03:52:00.000Z",
        "voteCount": 1,
        "content": "C is correct."
      },
      {
        "date": "2023-08-06T05:39:00.000Z",
        "voteCount": 1,
        "content": "Since minimum number of changes, I go with C.  Scaling based on the custom metrics might take more time compared to built in CPU load metric.  Also, we need to see that application is stateless.  So simple CPU metric is enough as a scaling parameter."
      },
      {
        "date": "2023-08-08T17:59:00.000Z",
        "voteCount": 1,
        "content": "Have you given the exam yet. Are these questions similar to actual exam questions?"
      },
      {
        "date": "2023-01-13T05:57:00.000Z",
        "voteCount": 1,
        "content": "A. Incorrect: This doesn\u2019t help with a distributed application.\nB. Incorrect: This would work, but would require Cloud Monitoring integration and possible application modification. This would also not apply to a distributed application.\nC. Correct: This will require the least number of changes to the code and fits the requirements.\nD. Incorrect: This would work, but would require Cloud Monitoring integration and possible application modification."
      },
      {
        "date": "2022-12-25T02:40:00.000Z",
        "voteCount": 1,
        "content": "Answer C\nScale based on the percent utilization of CPUs across nodes. This can be cost effective, letting you maximize CPU resource utilization. Because CPU usage is a trailing metric, however, your users might experience latency while a scale-up is in progress."
      },
      {
        "date": "2022-12-17T02:03:00.000Z",
        "voteCount": 2,
        "content": "C is the answer.\n\nhttps://cloud.google.com/kubernetes-engine/docs/concepts/horizontalpodautoscaler\nThe Horizontal Pod Autoscaler changes the shape of your Kubernetes workload by automatically increasing or decreasing the number of Pods in response to the workload's CPU or memory consumption, or in response to custom metrics reported from within Kubernetes or external metrics from sources outside of your cluster."
      },
      {
        "date": "2022-12-09T13:20:00.000Z",
        "voteCount": 3,
        "content": "AB are wrong because it is recommended to start with HPA if you have nothing \nD would take time and effort since you have to tune the metric\nC is right because is the most simple entry level solution for autoscaling due the unknown new  requirements"
      },
      {
        "date": "2022-12-07T20:14:00.000Z",
        "voteCount": 1,
        "content": "I think D is option."
      },
      {
        "date": "2022-12-03T23:22:00.000Z",
        "voteCount": 3,
        "content": "there are too many typos here but if it is really typo then the answer is C"
      },
      {
        "date": "2022-12-07T02:35:00.000Z",
        "voteCount": 1,
        "content": "Please share your views about why it's not D? Question doesn't say anything about increasing load utilization but about new(addition) requirements."
      },
      {
        "date": "2022-12-17T02:04:00.000Z",
        "voteCount": 2,
        "content": "scaling based on CPU load will be sufficient. you don't need to create custom metric."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 147,
    "url": "https://www.examtopics.com/discussions/google/view/89929-exam-professional-cloud-developer-topic-1-question-147/",
    "body": "Your application is composed of a set of loosely coupled services orchestrated by code executed on Compute Engine. You want your application to easily bring up new Compute Engine instances that find and use a specific version of a service. How should this be configured?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDefine your service endpoint information as metadata that is retrieved at runtime and used to connect to the desired service.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDefine your service endpoint information as label data that is retrieved at runtime and used to connect to the desired service.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDefine your service endpoint information to be retrieved from an environment variable at runtime and used to connect to the desired service.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDefine your service to use a fixed hostname and port to connect to the desired service. Replace the service at the endpoint with your new version."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 8,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-09-10T08:23:00.000Z",
        "voteCount": 1,
        "content": "Dynamic Discovery: Using metadata allows your Compute Engine instances to dynamically discover the endpoint information for the specific service version they need. This eliminates the need for hardcoding endpoints or relying on fixed hostnames and ports.\nLoose Coupling: Metadata promotes loose coupling between your services. Each service can be updated independently without affecting other services, as long as the metadata is updated accordingly.\nScalability: Metadata is a scalable solution. You can easily manage and update endpoint information for a large number of services without modifying the code of each Compute Engine instance."
      },
      {
        "date": "2024-09-10T08:23:00.000Z",
        "voteCount": 1,
        "content": "B. Define your service endpoint information as label data that is retrieved at runtime and used to connect to the desired service: Labels are primarily used for tagging and filtering resources, not for dynamic service discovery.\nC. Define your service endpoint information to be retrieved from an environment variable at runtime and used to connect to the desired service: While environment variables can be used for configuration, they are not as flexible or scalable as metadata for managing service endpoints.\nD. Define your service to use a fixed hostname and port to connect to the desired service. Replace the service at the endpoint with your new version: This approach is inflexible and requires manual updates to the service endpoint whenever a new version is deployed."
      },
      {
        "date": "2024-04-13T22:54:00.000Z",
        "voteCount": 1,
        "content": "Go with A\n\nwhy not B?\nLabels are used for organizing Google Cloud resources, not for storing configuration data that your application needs to run."
      },
      {
        "date": "2023-09-22T03:55:00.000Z",
        "voteCount": 1,
        "content": "The best answer is: A. Define your service endpoint information as metadata that is retrieved at runtime and used to connect to the desired service.\n\nThis is the most flexible and scalable way to configure your application to easily bring up new Compute Engine instances that find and use a specific version of a service."
      },
      {
        "date": "2023-08-06T06:09:00.000Z",
        "voteCount": 2,
        "content": "It is either A or C.\nWe can define a host URL as metadata in a virtual machine instance that orchestrates different services based on urls defined as metadata.  One more way is to retrieve the urls from environment variables.  Environment variables can be passed from,\n1) command line\n2) docker file\n3) kubernetes deployment descriptor\n4) through config server - application properities / yml file and so on.\n\nThe easier way is to define it as metadata in the compute engine instance itself."
      },
      {
        "date": "2023-07-30T02:20:00.000Z",
        "voteCount": 1,
        "content": "I think B"
      },
      {
        "date": "2023-05-03T18:33:00.000Z",
        "voteCount": 1,
        "content": "Answer is [B] ."
      },
      {
        "date": "2023-02-07T08:48:00.000Z",
        "voteCount": 2,
        "content": "An example of how you can retrieve the endpoint information from a label in Python:\n\nimport google.auth\nfrom google.cloud import compute\n\n# Authenticate and create a client for the Compute Engine API\ncredentials, project = google.auth.default()\ncompute_client = compute.Client(credentials=credentials, project=project)\n\n# Get the instance based on the instance name\ninstance_name = \"example-instance\"\ninstance = compute_client.instance(instance_name)\n\n# Get the endpoint information from the instance's labels\nendpoint = instance.labels.get(\"endpoint\")"
      },
      {
        "date": "2023-02-10T08:15:00.000Z",
        "voteCount": 4,
        "content": "Ansuwer is A:\nLabels are used to categorize and organize resources in Google Cloud Platform, such as Compute Engine instances. While they can also be used to store endpoint information, they may not be as flexible as metadata when it comes to dynamically retrieving information at runtime. Additionally, labels are associated with individual resources, so updating the label data would require modifying the specific resource, rather than a centralized metadata store.\n\nIn some cases, using labels may be more appropriate, such as when you want to categorize and organize your resources, but for managing service endpoints in a loosely coupled architecture, metadata is generally a more flexible and scalable solution."
      },
      {
        "date": "2022-12-25T02:38:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/apis/design/glossary#api_service_endpoint\nhttps://cloud.google.com/compute/docs/metadata/overview\nAnswer A\nAnswer A"
      },
      {
        "date": "2022-12-17T02:00:00.000Z",
        "voteCount": 2,
        "content": "A is the answer.\n\nhttps://cloud.google.com/service-infrastructure/docs/service-metadata/reference/rest#service-endpoint"
      },
      {
        "date": "2022-12-03T23:37:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer: A. best practice\nB - There's no label data\nC - harder to commit env?\nD - not sure about this"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 148,
    "url": "https://www.examtopics.com/discussions/google/view/91883-exam-professional-cloud-developer-topic-1-question-148/",
    "body": "You are developing a microservice-based application that will run on Google Kubernetes Engine (GKE). Some of the services need to access different Google Cloud APIs. How should you set up authentication of these services in the cluster following Google-recommended best practices? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the service account attached to the GKE node.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Workload Identity in the cluster via the gcloud command-line tool.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAccess the Google service account keys from a secret management service.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the Google service account keys in a central secret management service.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse gcloud to bind the Kubernetes service account and the Google service account using roles/iam.workloadIdentity.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "BE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BE",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-09-22T03:59:00.000Z",
        "voteCount": 1,
        "content": "BE is correct."
      },
      {
        "date": "2023-08-06T06:13:00.000Z",
        "voteCount": 1,
        "content": "I go with B and E.  They are almost same."
      },
      {
        "date": "2023-02-27T06:03:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity"
      },
      {
        "date": "2023-01-13T05:56:00.000Z",
        "voteCount": 2,
        "content": "A is incorrect. While it could work, all the services are using the same service account, there is no separation of permissions, and no detailed logging.\nB and E together connect GKE and Google service accounts, so GKE can authenticate a service with a Google service account.\nC is incorrect. While this is feasible, it\u2019s not the recommended practice for workload identity because of the mandatory key rotation of the service accounts.\nD is incorrect. While this is feasible, it\u2019s not the recommended practice for workload identity because of the mandatory key rotation of the service accounts.\nE and B together connect GKE and Google service accounts, so GKE can authenticate a service with a Google service account."
      },
      {
        "date": "2022-12-25T02:26:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/kubernetes-engine/docs/tutorials/authenticating-to-cloud-platform#use_workload_identity\nAnswer B\n\nhttps://cloud.google.com/kubernetes-engine/docs/how-to/kubernetes-service-accounts\nAnswer E"
      },
      {
        "date": "2022-12-17T01:58:00.000Z",
        "voteCount": 2,
        "content": "BE is the answer.\n\nhttps://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 149,
    "url": "https://www.examtopics.com/discussions/google/view/90878-exam-professional-cloud-developer-topic-1-question-149/",
    "body": "Your development team has been tasked with maintaining a .NET legacy application. The application incurs occasional changes and was recently updated. Your goal is to ensure that the application provides consistent results while moving through the CI/CD pipeline from environment to environment. You want to minimize the cost of deployment while making sure that external factors and dependencies between hosting environments are not problematic. Containers are not yet approved in your organization. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRewrite the application using .NET Core, and deploy to Cloud Run. Use revisions to separate the environments.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Build to deploy the application as a new Compute Engine image for each build. Use this image in each environment.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the application using MS Web Deploy, and make sure to always use the latest, patched MS Windows Server base image in Compute Engine.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Build to package the application, and deploy to a Google Kubernetes Engine cluster. Use namespaces to separate the environments."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-09-22T04:01:00.000Z",
        "voteCount": 1,
        "content": "B is correct."
      },
      {
        "date": "2023-08-06T06:20:00.000Z",
        "voteCount": 1,
        "content": "The key is containers are not supported.  The best possible option is, use cloud build to build the application and deploy under the virtual machine instance.  Create a snapshot of the disk and create image out of it. Or create an image directly.  Use this image as instance template for other environments."
      },
      {
        "date": "2022-12-25T02:18:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/architecture/modernization-path-dotnet-applications-google-cloud#take_advantage_of_compute_engine\nThe reason why B is better than D, hence had to paste the link above.\nAnswer B"
      },
      {
        "date": "2022-12-17T01:55:00.000Z",
        "voteCount": 2,
        "content": "B is the answer.\n\nhttps://cloud.google.com/architecture/modernization-path-dotnet-applications-google-cloud#phase_1_rehost_in_the_cloud"
      },
      {
        "date": "2022-12-09T19:59:00.000Z",
        "voteCount": 3,
        "content": "AD are wrong because containers are not yet approved!\nfor the simplicity part i thinks is B \n\nhttps://cloud.google.com/architecture/modernization-path-dotnet-applications-google-cloud"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 150,
    "url": "https://www.examtopics.com/discussions/google/view/91837-exam-professional-cloud-developer-topic-1-question-150/",
    "body": "The new version of your containerized application has been tested and is ready to deploy to production on Google Kubernetes Engine. You were not able to fully load-test the new version in pre-production environments, and you need to make sure that it does not have performance problems once deployed. Your deployment must be automated. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Load Balancing to slowly ramp up traffic between versions. Use Cloud Monitoring to look for performance issues.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the application via a continuous delivery pipeline using canary deployments. Use Cloud Monitoring to look for performance issues. and ramp up traffic as the metrics support it.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the application via a continuous delivery pipeline using blue/green deployments. Use Cloud Monitoring to look for performance issues, and launch fully when the metrics support it.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the application using kubectl and set the spec.updateStrategv.type to RollingUpdate. Use Cloud Monitoring to look for performance issues, and run the kubectl rollback command if there are any issues."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 9,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-09-22T04:05:00.000Z",
        "voteCount": 1,
        "content": "B. Deploy the application via a continuous delivery pipeline using canary deployments. Use Cloud Monitoring to look for performance issues, and ramp up traffic as the metrics support it."
      },
      {
        "date": "2023-08-06T06:38:00.000Z",
        "voteCount": 1,
        "content": "I go with B.  The key is depolyment must be automated.  With canary and CI/CD pipeline in place, we can adjust the traffic based on the input from Canary users."
      },
      {
        "date": "2023-06-08T12:52:00.000Z",
        "voteCount": 1,
        "content": "B. Automated deployment can be done with Cloud Deploy.\nA: No. Not relevant and I can't find documents for Cloud Load Balancing supports canary deployment.\nC: No. Blue / green is not possible because \"not able to fully load-test the new version in pre-production environments\" - either no budget or other causes.\nD: No. Not automated and in-place upgrade will have performance hit."
      },
      {
        "date": "2023-04-22T02:15:00.000Z",
        "voteCount": 3,
        "content": "deployment should be automated\nhttps://cloud.google.com/deploy/docs/deployment-strategies/canary#types_of_canary"
      },
      {
        "date": "2023-03-31T04:07:00.000Z",
        "voteCount": 3,
        "content": "B. Deploy the application via a continuous delivery pipeline using canary deployments. Use Cloud Monitoring to look for performance issues, and ramp up traffic as the metrics support it.\n\nCanary deployment strategy can be used to mitigate risk in the production deployment process. In this strategy, a small subset of traffic is routed to the new version of the application, while the rest of the traffic is sent to the current version. This allows for real-time monitoring of the new version's performance before fully rolling it out to all users. If there are any issues or performance problems, the traffic can be immediately routed back to the previous version. Cloud Monitoring can be used to monitor performance metrics and make informed decisions about when to ramp up traffic to the new version"
      },
      {
        "date": "2023-02-27T06:06:00.000Z",
        "voteCount": 1,
        "content": "i'd choose d."
      },
      {
        "date": "2023-01-11T08:49:00.000Z",
        "voteCount": 1,
        "content": "Based on the link provided by the guys on the comments, after reviewing the links, I can see that\n\nOption A \"Use Cloud Load Balancing to slowly ramp up traffic between versions. Use Cloud Monitoring to look for performance issues\" is a good approach, using Cloud Load Balancing, traffic is gradually shifted between the versions, and by using Cloud monitoring, you can detect any performance issues early on."
      },
      {
        "date": "2023-04-22T02:11:00.000Z",
        "voteCount": 1,
        "content": "deployment should be automated"
      },
      {
        "date": "2023-01-11T08:49:00.000Z",
        "voteCount": 1,
        "content": "Option C \"Deploy the application via a continuous delivery pipeline using blue/green deployments. Use Cloud Monitoring to look for performance issues, and launch fully when the metrics support it\" is also a good approach, as it allows you to test the new version of the application on a production-like environment and compare it against the previous version using real traffic, and once the metrics are good, switch all traffic to the new version.\n\nOption D \"Deploy the application using kubectl and set the spec.updateStrategy.type to RollingUpdate. Use Cloud Monitoring to look for performance issues, and run the kubectl rollback command if there are any issues\" is also a good option, in this case as well you are incrementally rolling out the new version, and monitoring its performance, if any issues occur, you can roll back the update."
      },
      {
        "date": "2023-01-11T08:49:00.000Z",
        "voteCount": 1,
        "content": "Ultimately, the approach you choose will depend on the specifics of your application and infrastructure, but any of these options can work well if implemented correctly."
      },
      {
        "date": "2023-01-06T04:45:00.000Z",
        "voteCount": 2,
        "content": "Answer D\nhttps://kubernetes.io/docs/tutorials/kubernetes-basics/update/update-intro/\nhttps://cloud.google.com/kubernetes-engine/docs/how-to/updating-apps#overview\nKindly master the requirements of the question, and be very aware of the question's key words"
      },
      {
        "date": "2023-11-20T07:14:00.000Z",
        "voteCount": 1,
        "content": "deployment shouldn't be automated ?"
      },
      {
        "date": "2022-12-25T02:11:00.000Z",
        "voteCount": 2,
        "content": "https://kubernetes.io/docs/tutorials/kubernetes-basics/update/update-intro/\nhttps://cloud.google.com/kubernetes-engine/docs/how-to/updating-apps#overview\nAnswer D.\nThe rest need testing before..."
      },
      {
        "date": "2022-12-16T07:01:00.000Z",
        "voteCount": 1,
        "content": "C is the answer.\n\nhttps://cloud.google.com/architecture/implementing-deployment-and-testing-strategies-on-gke#perform_a_bluegreen_deployment"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 151,
    "url": "https://www.examtopics.com/discussions/google/view/89931-exam-professional-cloud-developer-topic-1-question-151/",
    "body": "Users are complaining that your Cloud Run-hosted website responds too slowly during traffic spikes. You want to provide a better user experience during traffic peaks. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRead application configuration and static data from the database on application startup.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPackage application configuration and static data into the application image during build time.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPerform as much work as possible in the background after the response has been returned to the user.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure that timeout exceptions and errors cause the Cloud Run instance to exit quickly so a replacement instance can be started."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 7,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-08-06T06:48:00.000Z",
        "voteCount": 1,
        "content": "If images and config info available in the image (within the namespace of the hosting system) then latency is less.  Can serve resources easily."
      },
      {
        "date": "2023-01-11T08:51:00.000Z",
        "voteCount": 2,
        "content": "B. Package application configuration and static data into the application image during build time.\n\nBy packaging application configuration and static data into the application image during build time, the application can quickly serve requests without having to make additional requests to a database, thus reducing response time. Additionally, you might consider caching static data in the application to reduce latency and provide faster responses to user requests, also you could move some of the computation that is not time critical to be done asynchronously."
      },
      {
        "date": "2023-01-11T08:51:00.000Z",
        "voteCount": 1,
        "content": "Option A \"Read application configuration and static data from the database on application startup\" will put more load on database during traffic spike, this will slow down the application's response time.\n\nOption C \"Perform as much work as possible in the background after the response has been returned to the user\" could be a good approach, it allows the user to receive a response quickly, but the background work could take a long time and cause a delay in processing and might not be acceptable for certain use-cases.\n\nOption D \"Ensure that timeout exceptions and errors cause the Cloud Run instance to exit quickly so a replacement instance can be started\" this is good practice and can help ensure that when an instance is having problems, it can be quickly replaced with a new one, but this will not improve the user experience during traffic peaks, but instead it will minimize the impact of a failed instance on the service's availability."
      },
      {
        "date": "2023-01-08T09:33:00.000Z",
        "voteCount": 1,
        "content": "B is the answer from the recommendation of google because \n\"For starters, on Cloud Run, the size of your container image does not affect cold start or request processing time\" so you can add the configuration and static data."
      },
      {
        "date": "2022-12-25T01:29:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/blog/topics/developers-practitioners/3-ways-optimize-cloud-run-response-times\nAnswer B"
      },
      {
        "date": "2022-12-22T05:40:00.000Z",
        "voteCount": 3,
        "content": "B is the answer.\n\nhttps://cloud.google.com/blog/topics/developers-practitioners/3-ways-optimize-cloud-run-response-times\nInstead of computing things upon startup, compute them lazily. The initialization of global variables always occurs during startup, which increases cold start time. Use lazy initialization for infrequently used objects to defer the time cost and decrease cold start times."
      },
      {
        "date": "2022-12-04T00:21:00.000Z",
        "voteCount": 1,
        "content": "Not A - should store static data as global variables\nNot B - the larger the image, could slow down the startup time \nNot D - no errors were mentioned, app is only slowing down when traffic spikes\n\nC - process in backgrounds"
      },
      {
        "date": "2022-12-21T07:21:00.000Z",
        "voteCount": 2,
        "content": "I disagree, the answer is B; with image caching the startup time won't change significantly. As for answer c, Cloud Run can't do any background work unless you use \"always-on\" cpu allocation, but that comes with a big increase in cost."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 152,
    "url": "https://www.examtopics.com/discussions/google/view/89953-exam-professional-cloud-developer-topic-1-question-152/",
    "body": "You are a developer working on an internal application for payroll processing. You are building a component of the application that allows an employee to submit a timesheet, which then initiates several steps:<br><br>\u2022 An email is sent to the employee and manager, notifying them that the timesheet was submitted.<br>\u2022 A timesheet is sent to payroll processing for the vendor's API.<br>\u2022 A timesheet is sent to the data warehouse for headcount planning.<br><br>These steps are not dependent on each other and can be completed in any order. New steps are being considered and will be implemented by different development teams. Each development team will implement the error handling specific to their step. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy a Cloud Function for each step that calls the corresponding downstream system to complete the required action.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Pub/Sub topic for each step. Create a subscription for each downstream development team to subscribe to their step's topic.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Pub/Sub topic for timesheet submissions. Create a subscription for each downstream development team to subscribe to the topic.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a timesheet microservice deployed to Google Kubernetes Engine. The microservice calls each downstream step and waits for a successful response before calling the next step."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-09-22T21:51:00.000Z",
        "voteCount": 1,
        "content": "Pub/Sub is a messaging service that allows you to decouple microservices and other applications. It is a good choice for this use case because it is scalable, reliable, and easy to use."
      },
      {
        "date": "2023-08-06T07:01:00.000Z",
        "voteCount": 1,
        "content": "This is a tricky question.  The context is in developing team developing the application. So C is the best fit.  After the development, when the application is running then each timesheet submit event can publish 3 events/messages so that 3 independent microservices for each operation can kick in parallel and perform the tasks."
      },
      {
        "date": "2023-01-11T08:53:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2022-12-25T01:25:00.000Z",
        "voteCount": 3,
        "content": "Answer C"
      },
      {
        "date": "2022-12-16T06:55:00.000Z",
        "voteCount": 2,
        "content": "C is the answer."
      },
      {
        "date": "2022-12-13T03:21:00.000Z",
        "voteCount": 1,
        "content": "option c"
      },
      {
        "date": "2022-12-04T04:17:00.000Z",
        "voteCount": 3,
        "content": "One to many pattern, C is correct"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 153,
    "url": "https://www.examtopics.com/discussions/google/view/91835-exam-professional-cloud-developer-topic-1-question-153/",
    "body": "You are designing an application that uses a microservices architecture. You are planning to deploy the application in the cloud and on-premises. You want to make sure the application can scale up on demand and also use managed services as much as possible. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy open source Istio in a multi-cluster deployment on multiple Google Kubernetes Engine (GKE) clusters managed by Anthos.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a GKE cluster in each environment with Anthos, and use Cloud Run for Anthos to deploy your application to each cluster.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall a GKE cluster in each environment with Anthos, and use Cloud Build to create a Deployment for your application in each cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a GKE cluster in the cloud and install open-source Kubernetes on-premises. Use an external load balancer service to distribute traffic across the two environments."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-09-22T22:01:00.000Z",
        "voteCount": 1,
        "content": "Anthos with Cloud Run is the best option here."
      },
      {
        "date": "2023-08-06T07:12:00.000Z",
        "voteCount": 1,
        "content": "Anthos supports GKE cluster creation in both On-premises and GCP cloud environments.  Cloud run for Anthos supports autoscaling in both the environments."
      },
      {
        "date": "2023-03-31T04:21:00.000Z",
        "voteCount": 2,
        "content": "B. Create a GKE cluster in each environment with Anthos, and use Cloud Run for Anthos to deploy your application to each cluster.\n\nUsing Anthos to manage Kubernetes clusters in both cloud and on-premises environments allows for consistency in deployment and management across both environments. Deploying the application using Cloud Run for Anthos allows for easy scaling on demand and use of managed services such as Cloud SQL and Memorystore. Additionally, Cloud Run for Anthos can be deployed to both GKE clusters and on-premises Kubernetes clusters, allowing for a consistent deployment experience across environments."
      },
      {
        "date": "2022-12-25T01:24:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/anthos/run/docs/deploy-application\nAnswer B"
      },
      {
        "date": "2022-12-16T06:54:00.000Z",
        "voteCount": 1,
        "content": "B is the answer.\n\nhttps://cloud.google.com/anthos/run\nIntegrated with Anthos, Cloud Run for Anthos provides a flexible serverless development platform for hybrid and multicloud environments. Cloud Run for Anthos is Google's managed and fully supported Knative offering, an open source project that enables serverless workloads on Kubernetes."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 154,
    "url": "https://www.examtopics.com/discussions/google/view/90343-exam-professional-cloud-developer-topic-1-question-154/",
    "body": "You want to migrate an on-premises container running in Knative to Google Cloud. You need to make sure that the migration doesn't affect your application's deployment strategy, and you want to use a fully managed service. Which Google Cloud service should you use to deploy your container?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Run\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCompute Engine",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGoogle Kubernetes Engine",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tApp Engine flexible environment"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-09-22T22:06:00.000Z",
        "voteCount": 1,
        "content": "A is correct."
      },
      {
        "date": "2023-08-06T07:19:00.000Z",
        "voteCount": 1,
        "content": "A is perfect since Cloud Run is built on Knative."
      },
      {
        "date": "2022-12-25T01:21:00.000Z",
        "voteCount": 1,
        "content": "Answer A"
      },
      {
        "date": "2022-12-16T06:48:00.000Z",
        "voteCount": 2,
        "content": "A is the answer.\n\nhttps://cloud.google.com/blog/products/serverless/knative-based-cloud-run-services-are-ga"
      },
      {
        "date": "2022-12-13T03:51:00.000Z",
        "voteCount": 1,
        "content": "A. container running in knative"
      },
      {
        "date": "2022-12-07T01:00:00.000Z",
        "voteCount": 1,
        "content": "Cloud run"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 155,
    "url": "https://www.examtopics.com/discussions/google/view/89933-exam-professional-cloud-developer-topic-1-question-155/",
    "body": "This architectural diagram depicts a system that streams data from thousands of devices. You want to ingest data into a pipeline, store the data, and analyze the data using SQL statements. Which Google Cloud services should you use for steps 1, 2, 3, and 4?<br><br><img src=\"https://img.examtopics.com/professional-cloud-developer/image1.png\">",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. App Engine<br>2. Pub/Sub<br>3. BigQuery<br>4. Firestore",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Dataflow<br>2. Pub/Sub<br>3. Firestore<br>4. BigQuery",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Pub/Sub<br>2. Dataflow<br>3. BigQuery<br>4. Firestore",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Pub/Sub<br>2. Dataflow<br>3. Firestore<br>4. BigQuery\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-09-22T22:25:00.000Z",
        "voteCount": 1,
        "content": "D is correct."
      },
      {
        "date": "2023-08-06T07:28:00.000Z",
        "voteCount": 2,
        "content": "Data ingest -&gt; Pub sub\nPipeline -&gt; Dataflow\nTransaction -&gt; Firestore\nAnalytics -&gt; BigQuery"
      },
      {
        "date": "2022-12-25T01:20:00.000Z",
        "voteCount": 2,
        "content": "Answer D"
      },
      {
        "date": "2022-12-16T06:46:00.000Z",
        "voteCount": 1,
        "content": "D is the answer."
      },
      {
        "date": "2022-12-13T03:55:00.000Z",
        "voteCount": 1,
        "content": "option D"
      },
      {
        "date": "2022-12-04T00:41:00.000Z",
        "voteCount": 3,
        "content": "1. Pub/Sub - for ingest\n2. Dataflow - dataflow pipeline\n3. Firestore - transaction DB\n4. BigQuery - analytics"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 156,
    "url": "https://www.examtopics.com/discussions/google/view/90345-exam-professional-cloud-developer-topic-1-question-156/",
    "body": "Your company just experienced a Google Kubernetes Engine (GKE) API outage due to a zone failure. You want to deploy a highly available GKE architecture that minimizes service interruption to users in the event of a future zone failure. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy Zonal clusters",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy Regional clusters\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy Multi-Zone clusters",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy GKE on-premises clusters"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-09-22T22:33:00.000Z",
        "voteCount": 1,
        "content": "B is correct."
      },
      {
        "date": "2023-08-06T07:37:00.000Z",
        "voteCount": 1,
        "content": "Regional cluster with master plane to be in multiple zones is a correct option."
      },
      {
        "date": "2022-12-25T01:17:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/blog/products/containers-kubernetes/best-practices-for-creating-a-highly-available-gke-cluster\nAnswer B\nhttps://cloud.google.com/kubernetes-engine/docs/concepts/types-of-clusters#regional_clusters"
      },
      {
        "date": "2022-12-16T06:44:00.000Z",
        "voteCount": 4,
        "content": "B is the answer.\n\nhttps://cloud.google.com/kubernetes-engine/docs/concepts/types-of-clusters#regional_clusters\nA regional cluster has multiple replicas of the control plane, running in multiple zones within a given region. Nodes in a regional cluster can run in multiple zones or a single zone depending on the configured node locations. By default, GKE replicates each node pool across three zones of the control plane's region. When you create a cluster or when you add a new node pool, you can change the default configuration by specifying the zone(s) in which the cluster's nodes run. All zones must be within the same region as the control plane."
      },
      {
        "date": "2022-12-13T03:58:00.000Z",
        "voteCount": 1,
        "content": "regional cluster"
      },
      {
        "date": "2022-12-09T09:47:00.000Z",
        "voteCount": 1,
        "content": "Regional cluster replicates in at least 3 zones"
      },
      {
        "date": "2022-12-07T01:03:00.000Z",
        "voteCount": 1,
        "content": "Regional cluster for protection against zonal outages"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 157,
    "url": "https://www.examtopics.com/discussions/google/view/91832-exam-professional-cloud-developer-topic-1-question-157/",
    "body": "Your team develops services that run on Google Cloud. You want to process messages sent to a Pub/Sub topic, and then store them. Each message must be processed exactly once to avoid duplication of data and any data conflicts. You need to use the cheapest and most simple solution. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tProcess the messages with a Dataproc job, and write the output to storage.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tProcess the messages with a Dataflow streaming pipeline using Apache Beam's PubSubIO package, and write the output to storage.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tProcess the messages with a Cloud Function, and write the results to a BigQuery location where you can run a job to deduplicate the data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRetrieve the messages with a Dataflow streaming pipeline, store them in Cloud Bigtable, and use another Dataflow streaming pipeline to deduplicate messages."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-09-22T22:37:00.000Z",
        "voteCount": 1,
        "content": "B is correct."
      },
      {
        "date": "2023-08-06T07:39:00.000Z",
        "voteCount": 1,
        "content": "Dataflow ensures that the data will be processed only once."
      },
      {
        "date": "2023-01-04T01:57:00.000Z",
        "voteCount": 4,
        "content": "Answer is B\n\nhttps://cloud.google.com/blog/products/data-analytics/handling-duplicate-data-in-streaming-pipeline-using-pubsub-dataflow\n\"...because Pub/Sub provides each message with a unique message_id, Dataflow uses it to deduplicate messages by default if you use the built-in Apache Beam PubSubIO\""
      },
      {
        "date": "2022-12-24T03:33:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/pubsub/docs/stream-messages-dataflow\nhttps://cloud.google.com/community/tutorials/pubsub-spring-dedup-messages\nhttps://cloud.google.com/blog/products/data-analytics/handling-duplicate-data-in-streaming-pipeline-using-pubsub-dataflow"
      },
      {
        "date": "2022-12-16T06:42:00.000Z",
        "voteCount": 1,
        "content": "B is the answer.\n\nhttps://cloud.google.com/dataflow/docs/concepts/streaming-with-cloud-pubsub"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 158,
    "url": "https://www.examtopics.com/discussions/google/view/91536-exam-professional-cloud-developer-topic-1-question-158/",
    "body": "You are running a containerized application on Google Kubernetes Engine. Your container images are stored in Container Registry. Your team uses CI/CD practices. You need to prevent the deployment of containers with known critical vulnerabilities. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\u2022 Use Web Security Scanner to automatically crawl your application<br>\u2022 Review your application logs for scan results, and provide an attestation that the container is free of known critical vulnerabilities<br>\u2022 Use Binary Authorization to implement a policy that forces the attestation to be provided before the container is deployed",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\u2022 Use Web Security Scanner to automatically crawl your application<br>\u2022 Review the scan results in the scan details page in the Cloud Console, and provide an attestation that the container is free of known critical vulnerabilities<br>\u2022 Use Binary Authorization to implement a policy that forces the attestation to be provided before the container is deployed",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\u2022 Enable the Container Scanning API to perform vulnerability scanning<br>\u2022 Review vulnerability reporting in Container Registry in the Cloud Console, and provide an attestation that the container is free of known critical vulnerabilities<br>\u2022 Use Binary Authorization to implement a policy that forces the attestation to be provided before the container is deployed",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\u2022 Enable the Container Scanning API to perform vulnerability scanning<br>\u2022 Programmatically review vulnerability reporting through the Container Scanning API, and provide an attestation that the container is free of known critical vulnerabilities<br>\u2022 Use Binary Authorization to implement a policy that forces the attestation to be provided before the container is deployed\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 7,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-07-12T03:21:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2023-11-11T18:21:00.000Z",
        "voteCount": 3,
        "content": "i think c is correct"
      },
      {
        "date": "2023-09-22T23:14:00.000Z",
        "voteCount": 2,
        "content": "D is correct."
      },
      {
        "date": "2023-08-06T07:45:00.000Z",
        "voteCount": 1,
        "content": "Using container scanning API is a better choice."
      },
      {
        "date": "2023-01-08T09:47:00.000Z",
        "voteCount": 1,
        "content": "Answer is D, use the default tools provided by google like container analysis."
      },
      {
        "date": "2022-12-25T01:13:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/container-analysis/docs/automated-scanning-howto#view-code\nhttps://cloud.google.com/binary-authorization/docs\nAnswer D"
      },
      {
        "date": "2022-12-16T06:40:00.000Z",
        "voteCount": 2,
        "content": "D is the answer.\n\nhttps://cloud.google.com/binary-authorization/docs/creating-attestations-kritis"
      },
      {
        "date": "2022-12-14T00:51:00.000Z",
        "voteCount": 1,
        "content": "I would go for D\nhttps://cloud.google.com/container-analysis/docs/os-overview"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 159,
    "url": "https://www.examtopics.com/discussions/google/view/91830-exam-professional-cloud-developer-topic-1-question-159/",
    "body": "You have an on-premises application that authenticates to the Cloud Storage API using a user-managed service account with a user-managed key. The application connects to Cloud Storage using Private Google Access over a Dedicated Interconnect link. You discover that requests from the application to access objects in the Cloud Storage bucket are failing with a 403 Permission Denied error code. What is the likely cause of this issue?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe folder structure inside the bucket and object paths have changed.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe permissions of the service account\u2019s predefined role have changed.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe service account key has been rotated but not updated on the application server.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe Interconnect link from the on-premises data center to Google Cloud is experiencing a temporary outage."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 10,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 8,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-02-08T03:46:00.000Z",
        "voteCount": 6,
        "content": "The correct option is B. The 403 Permission Denied error code indicates that the service account is authenticated, but it doesn't have sufficient permissions to access the Cloud Storage bucket. If the error code were 401 Unauthorized, it would suggest that the authentication failed, which could be caused by a rotated key, as in option C. However, in this case, the error code is 403, which indicates a problem with the permissions of the service account, making option B the most likely cause."
      },
      {
        "date": "2024-07-12T03:27:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2024-06-03T07:27:00.000Z",
        "voteCount": 1,
        "content": "User-Managed Service Accounts and Keys: When you use a user-managed service account with a user-managed key, you are responsible for generating and distributing the key. If the key is rotated (for security best practices), you must update your application to use the new key.\n403 Permission Denied: This error typically indicates that the credentials being used for authentication are invalid or lack the necessary permissions. If the key was rotated and not updated, the application will continue to use the old, invalid key, resulting in this error."
      },
      {
        "date": "2023-10-27T03:26:00.000Z",
        "voteCount": 1,
        "content": "B is the correct answer. 403 denotes user is authentication but not authorized."
      },
      {
        "date": "2023-09-22T23:19:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2023-08-06T07:54:00.000Z",
        "voteCount": 1,
        "content": "The client id/service account key has been updated for the storage bucket but that was not being notified to the client applications or application server that calls cloud storage bucket."
      },
      {
        "date": "2023-01-28T01:37:00.000Z",
        "voteCount": 2,
        "content": "A user-managed service account authenticates to the Cloud Storage API using a key, which is a unique identifier that proves the identity of the service account. If the key is rotated, meaning it is replaced with a new one, the application will no longer be able to authenticate using the old key, resulting in a 403 Permission Denied error. To resolve this issue, the application server must be updated with the new key."
      },
      {
        "date": "2023-01-19T05:54:00.000Z",
        "voteCount": 1,
        "content": "Anwser B with status code 403 =&gt; Forbidden so the first authentication is working just the service has not enough permission to access the document."
      },
      {
        "date": "2023-01-17T11:18:00.000Z",
        "voteCount": 1,
        "content": "The answer is between B or C. \n\nI will choose C because the question has a context with account service by file with a key. With this setup, the cause of issue 403 will be key is not valid anymore after a rotation. For another context with only account service without a key generated, the B is the first check but with a key, you need to check if the key is valid before searching others causes."
      },
      {
        "date": "2023-01-17T21:44:00.000Z",
        "voteCount": 1,
        "content": "The HTTP 403 Forbidden response status code indicates that the server understands the request but refuses to authorize it. This status is similar to 401 , but for the 403 Forbidden status code, re-authenticating makes no difference. The access is tied to the application logic, such as insufficient rights to a resource. \n\nThe reason for denied access is the reason we get 403. as the question says, do not copy what others are saying , do a research and apply your knowledge to this if you have any practical knowledge. the answer is B"
      },
      {
        "date": "2023-01-19T05:53:00.000Z",
        "voteCount": 2,
        "content": "Yes agree with your comments, Answer is B"
      },
      {
        "date": "2023-01-11T09:03:00.000Z",
        "voteCount": 1,
        "content": "C. The service account key has been rotated but not updated on the application server.\n\nWhen a user-managed service account key is rotated in Google Cloud, the new key must also be updated on the application server that authenticates to the Cloud Storage API using that key. Failure to update the key on the application server will result in requests to the API failing with a 403 Permission Denied error code.\n\nOption B \"The permissions of the service account\u2019s predefined role have changed\" would also result in 403 error, but it would be a role issue, not a key issue."
      },
      {
        "date": "2023-01-12T02:48:00.000Z",
        "voteCount": 1,
        "content": "I dnt know if you have studied cloud security, GCP cloud security and are you actually doing these practically??"
      },
      {
        "date": "2023-01-12T02:46:00.000Z",
        "voteCount": 1,
        "content": "But the key has a role, so i literrally do not understand your last statement, actually provide a link to your answer because i dnt think The documentation can lieoi provided links because i needed to support what i know by what is written."
      },
      {
        "date": "2023-01-12T02:52:00.000Z",
        "voteCount": 1,
        "content": "The question explicitly says \"'What is the likely cause of this issue?\"' and i answered that by providing links, you are arguing but you dnt provide any links, i do not copy answers from someone , i do a research hence even if i know the answer off head i try to provide links for the sake of others like you, i dnt make baseless arguments"
      },
      {
        "date": "2023-01-06T04:48:00.000Z",
        "voteCount": 1,
        "content": "Answer B\nhttps://cloud.google.com/storage/docs/troubleshooting#access-permission\nhttps://cloud.google.com/appengine/docs/legacy/standard/python/googlecloudstorageclient/errors\nhttps://cloud.google.com/storage/docs/xml-api/reference-status#403%E2%80%94forbidden"
      },
      {
        "date": "2023-01-11T09:04:00.000Z",
        "voteCount": 1,
        "content": "The links you've provided are helpful resources for troubleshooting 403 \"Permission Denied\" errors when working with Cloud Storage.\n\nYou're correct, the 403 \"Permission Denied\" error can be caused by various reasons, such as an issue with the folder structure inside the bucket or an issue with the predefined role permissions, but based on the context and the error message it seems that the most likely cause is the service account key being rotated and not updated on the application server as I mentioned earlier.\n\nAdditionally, the links you provided provide more information about the possible causes for 403 error, such as the permissions that are associated with the object and the bucket, user authentication and role-based access control. Also, it's important to check the Cloud Storage access logs to determine the cause of the error and take appropriate action."
      },
      {
        "date": "2023-01-12T02:45:00.000Z",
        "voteCount": 1,
        "content": "so whats your argument because i provided the links to prove my point , where are your links? i chose the answer that is suppoerted, hence oi provided links. im not seeing anywhere where B is supoorted because according to the dicumentation its not B and according to my practical knowledge in GCP it cant be B."
      },
      {
        "date": "2023-01-12T08:23:00.000Z",
        "voteCount": 1,
        "content": "it cant be C i mean.... B is the answer thats what the links are saying"
      },
      {
        "date": "2022-12-25T01:02:00.000Z",
        "voteCount": 2,
        "content": "Answer B\nhttps://cloud.google.com/storage/docs/troubleshooting#access-permission\nhttps://cloud.google.com/appengine/docs/legacy/standard/python/googlecloudstorageclient/errors\nhttps://cloud.google.com/storage/docs/xml-api/reference-status#403%E2%80%94forbidden"
      },
      {
        "date": "2022-12-16T06:33:00.000Z",
        "voteCount": 1,
        "content": "C is the answer."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 160,
    "url": "https://www.examtopics.com/discussions/google/view/91829-exam-professional-cloud-developer-topic-1-question-160/",
    "body": "You are using the Cloud Client Library to upload an image in your application to Cloud Storage. Users of the application report that occasionally the upload does not complete and the client library reports an HTTP 504 Gateway Timeout error. You want to make the application more resilient to errors. What changes to the application should you make?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite an exponential backoff process around the client library call.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite a one-second wait time backoff process around the client library call.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDesign a retry button in the application and ask users to click if the error occurs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a queue for the object and inform the users that the application will try again in 10 minutes."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-09-22T23:20:00.000Z",
        "voteCount": 1,
        "content": "A is correct."
      },
      {
        "date": "2023-08-06T07:59:00.000Z",
        "voteCount": 1,
        "content": "Exponential back off strategy is a better choice for retry approach. This is for resiliency."
      },
      {
        "date": "2023-01-08T09:54:00.000Z",
        "voteCount": 1,
        "content": "When issuing link to charges on server like 504 uses the exponential-backoff"
      },
      {
        "date": "2023-01-06T04:51:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/storage/docs/retry-strategy#exponential-backoff\nAnswer A"
      },
      {
        "date": "2022-12-25T00:28:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/storage/docs/json_api/v1/status-codes#504_Gateway_Timeout\nAnswer A"
      },
      {
        "date": "2022-12-16T06:30:00.000Z",
        "voteCount": 1,
        "content": "A is the answer."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 161,
    "url": "https://www.examtopics.com/discussions/google/view/89955-exam-professional-cloud-developer-topic-1-question-161/",
    "body": "You are building a mobile application that will store hierarchical data structures in a database. The application will enable users working offline to sync changes when they are back online. A backend service will enrich the data in the database using a service account. The application is expected to be very popular and needs to scale seamlessly and securely. Which database and IAM role should you use?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud SQL, and assign the roles/cloudsql.editor role to the service account.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Bigtable, and assign the roles/bigtable.viewer role to the service account.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Firestore in Native mode and assign the roles/datastore.user role to the service account.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Firestore in Datastore mode and assign the roles/datastore.viewer role to the service account."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-09-22T23:22:00.000Z",
        "voteCount": 1,
        "content": "C is correct."
      },
      {
        "date": "2023-08-06T08:10:00.000Z",
        "voteCount": 2,
        "content": "IAM role should be roles/datastore.user role and not a viewer role as of option D.  Firestore is suitable for storing semi structured and hierarchical mobile data."
      },
      {
        "date": "2023-07-05T14:47:00.000Z",
        "voteCount": 1,
        "content": "C. Use Firestore in Native mode and assign the roles/datastore.user role to the service account.\nroles/datastore.user role - have permissions to Read/write access to data in a Datastore mode database. Intended for application developers and service accounts.\nhttps://cloud.google.com/datastore/docs/access/iam"
      },
      {
        "date": "2022-12-24T23:54:00.000Z",
        "voteCount": 3,
        "content": "Answer C\nhttps://cloud.google.com/architecture/building-scalable-apps-with-cloud-firestore"
      },
      {
        "date": "2022-12-16T06:27:00.000Z",
        "voteCount": 2,
        "content": "C is the answer.\n\nhttps://firebase.google.com/docs/firestore/manage-data/enable-offline\nCloud Firestore supports offline data persistence. This feature caches a copy of the Cloud Firestore data that your app is actively using, so your app can access the data when the device is offline. You can write, read, listen to, and query the cached data. When the device comes back online, Cloud Firestore synchronizes any local changes made by your app to the Cloud Firestore backend."
      },
      {
        "date": "2022-12-13T04:38:00.000Z",
        "voteCount": 1,
        "content": "option C"
      },
      {
        "date": "2022-12-04T04:27:00.000Z",
        "voteCount": 1,
        "content": "https://firebase.google.com/docs/firestore/manage-data/enable-offline"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 162,
    "url": "https://www.examtopics.com/discussions/google/view/91579-exam-professional-cloud-developer-topic-1-question-162/",
    "body": "Your application is deployed on hundreds of Compute Engine instances in a managed instance group (MIG) in multiple zones. You need to deploy a new instance template to fix a critical vulnerability immediately but must avoid impact to your service. What setting should be made to the MIG after updating the instance template?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the Max Surge to 100%.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the Update mode to Opportunistic.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the Maximum Unavailable to 100%.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the Minimum Wait time to 0 seconds.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 18,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 7,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-18T20:55:00.000Z",
        "voteCount": 9,
        "content": "You can eliminate:\n\nB. Because the MIG needs to be updated immediately, which not what Opportunistic does\nC. Because max unavailable at 100% will cause downtime\n\nSo that leaves A, and D. \n\nIf you choose A. The MIG will spin up hundreds of new machines, to replace the existing one, and shutdown the old ones. This is the fastest method, but could be costly, or you could run into quota issues.\n\nIf you choose D, the MIG will spin up 3 VMs at a time (maxSurge default to 3), and then it will bring up one at a time, as soon as more surge slots are available, so it wont be really that fast.\n\n\nI think D is the most sensible in this case."
      },
      {
        "date": "2023-12-05T13:29:00.000Z",
        "voteCount": 2,
        "content": "I vote D.\n\nThere are 2 requirements: to deploy the new instance template immediately and to avoid impact. Option D matches the urgency of the issue well but it also allows to control (minimize) the level of disruption to the service.\n\nhttps://cloud.google.com/compute/docs/instance-groups/updating-migs#choosing_between_automated_and_selective_updates"
      },
      {
        "date": "2023-09-22T23:28:00.000Z",
        "voteCount": 2,
        "content": "When you set the update mode to Opportunistic, the group will continue to serve requests from existing instances while the new instances are being created and started. Once the new instances are ready, the group will start routing requests to them. The group will continue to serve requests from both the old and new instances until all of the old instances have been terminated."
      },
      {
        "date": "2023-08-06T08:14:00.000Z",
        "voteCount": 2,
        "content": "The key here is fixing the vulnerability immediately which is not possible with Opportunistic mode."
      },
      {
        "date": "2023-05-01T14:59:00.000Z",
        "voteCount": 1,
        "content": "I think updating \"deployed on hundreds of Compute Engine instances\" is impossible with Opportunistic mode.\nSo I agree with D."
      },
      {
        "date": "2023-05-19T03:16:00.000Z",
        "voteCount": 1,
        "content": "And also said changes immediately, so opportunistic mode is not suitable as well.\nAgree with D"
      },
      {
        "date": "2023-04-22T11:19:00.000Z",
        "voteCount": 1,
        "content": "i go for D\nyou've updated template and want apply changes immediately"
      },
      {
        "date": "2023-02-28T07:02:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/compute/docs/instance-groups/rolling-out-updates-to-managed-instance-groups#minimum_wait_time\n\nUse the minReadySec option to specify the amount of time to wait before considering a new or restarted instance as updated. Use this option to control the rate at which the automated update is deployed. The timer starts when both of the following conditions are satisfied:\n\nThe instance's status is RUNNING.\nIf health checking is enabled, when the health check returns HEALTHY.\n\nHowever: minReadySec is only available in the beta Compute Engine API and might be deprecated in a future release."
      },
      {
        "date": "2023-02-03T05:43:00.000Z",
        "voteCount": 2,
        "content": "Setting the \"Minimum Wait time\" to 0 seconds means that there is no delay in launching the new instances after the instance template is updated, allowing you to deploy the fix for the critical vulnerability immediately. On the other hand, setting the \"Update mode to Opportunistic\" would mean that the new instances are created at an opportune time and may result in a delay in deploying the fix. In this scenario, where a critical vulnerability needs to be fixed immediately, it's important to deploy the fix as soon as possible, making the \"Minimum Wait time to 0 seconds\" the better approach."
      },
      {
        "date": "2022-12-24T23:45:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/compute/docs/instance-groups/updating-migs#opportunistic_updates\nAnswer B"
      },
      {
        "date": "2022-12-16T06:21:00.000Z",
        "voteCount": 4,
        "content": "B is the answer.\n\nhttps://cloud.google.com/compute/docs/instance-groups/rolling-out-updates-to-managed-instance-groups#type\nAlternatively, if an automated update is potentially too disruptive, you can choose to perform an opportunistic update. The MIG applies an opportunistic update only when you manually initiate the update on selected instances or when new instances are created. New instances can be created when you or another service, such as an autoscaler, resizes the MIG. Compute Engine does not actively initiate requests to apply opportunistic updates on existing instances."
      },
      {
        "date": "2022-12-14T06:41:00.000Z",
        "voteCount": 1,
        "content": "Answer?"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 163,
    "url": "https://www.examtopics.com/discussions/google/view/91828-exam-professional-cloud-developer-topic-1-question-163/",
    "body": "You made a typo in a low-level Linux configuration file that prevents your Compute Engine instance from booting to a normal run level. You just created the Compute Engine instance today and have done no other maintenance on it, other than tweaking files. How should you correct this error?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDownload the file using scp, change the file, and then upload the modified version",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure and log in to the Compute Engine instance through SSH, and change the file",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure and log in to the Compute Engine instance through the serial port, and change the file\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure and log in to the Compute Engine instance using a remote desktop client, and change the file"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 6,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-08-06T08:18:00.000Z",
        "voteCount": 4,
        "content": "If booting issue with Compute engine instance, then serial port access is one of the solution.  SSH, RDP and SCP are not possible."
      },
      {
        "date": "2023-05-01T15:23:00.000Z",
        "voteCount": 1,
        "content": "According to the explanation \"prevents your Compute Engine instance from booting to a normal run level\".\nSo I think sshd deamon has not launched yet and you can't use ssh. \nI can't think of a correct answer to anything other than C."
      },
      {
        "date": "2023-03-31T04:48:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is B: Configure and log in to the Compute Engine instance through SSH, and change the file.\n\nThis is the recommended method to make changes to a Linux configuration file on a Compute Engine instance. SSH allows secure remote access to the instance's command line interface, and it is designed to enable you to make changes to the instance's configuration files.\n\nOption A, downloading and uploading the modified version of the file, is not the recommended method as it requires more steps and can introduce errors.\n\nOption C, using the serial port, may be used in some cases, but it is not the recommended method as it requires more steps and can be more complex.\n\nOption D, using a remote desktop client, is not applicable as Linux instances on Compute Engine do not come with a graphical user interface (GUI) by default."
      },
      {
        "date": "2023-05-01T15:26:00.000Z",
        "voteCount": 1,
        "content": "Computer access through serial ports has been the method used by server administrators for a long time, so it is normal for professionals to be able to do this.\nIf we consider that the computer is not running, there is no correct answer other than C."
      },
      {
        "date": "2022-12-24T04:16:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/compute/docs/troubleshooting/troubleshooting-using-serial-console\nAnswer C"
      },
      {
        "date": "2022-12-16T06:18:00.000Z",
        "voteCount": 1,
        "content": "C is the answer.\n\nhttps://cloud.google.com/compute/docs/troubleshooting/troubleshooting-using-serial-console"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 164,
    "url": "https://www.examtopics.com/discussions/google/view/91582-exam-professional-cloud-developer-topic-1-question-164/",
    "body": "You are developing an application that needs to store files belonging to users in Cloud Storage. You want each user to have their own subdirectory in Cloud Storage. When a new user is created, the corresponding empty subdirectory should also be created. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an object with the name of the subdirectory ending with a trailing slash ('/') that is zero bytes in length.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an object with the name of the subdirectory, and then immediately delete the object within that subdirectory.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an object with the name of the subdirectory that is zero bytes in length and has WRITER access control list permission.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an object with the name of the subdirectory that is zero bytes in length. Set the Content-Type metadata to CLOUDSTORAGE_FOLDER."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 9,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-16T06:15:00.000Z",
        "voteCount": 7,
        "content": "A is the answer.\n\nhttps://cloud.google.com/storage/docs/folders\nIf you create an empty folder using the Google Cloud console, Cloud Storage creates a zero-byte object as a placeholder. For example, if you create a folder called folder in a bucket called my-bucket, a zero- byte object called gs://my-bucket/folder/ is created. This placeholder is discoverable by other tools when listing the objects in the bucket, for example when using the gsutil ls command."
      },
      {
        "date": "2023-09-22T23:35:00.000Z",
        "voteCount": 2,
        "content": "When you create an object with the name of the subdirectory ending with a trailing slash, Cloud Storage will treat the object as a subdirectory. This means that you can then store other objects in the subdirectory."
      },
      {
        "date": "2023-08-06T08:24:00.000Z",
        "voteCount": 1,
        "content": "I go with C.  WRITER permission and folder with zero bytes size is correct."
      },
      {
        "date": "2023-05-01T15:45:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/storage/docs/folders#overview\nAccording to the explanation upper URL, \" Cloud Storage operates with a flat namespace, which means that folders don't actually exist within Cloud Storage. \".\nThat's right. You can't create the state \"foo/\". \nThis is an actual experience using Cloud Storage.\nTherefore, I think the correct answer is C."
      },
      {
        "date": "2023-05-01T15:55:00.000Z",
        "voteCount": 1,
        "content": "Sorry. I was mistaken.\nThe correct answer is A is OK."
      },
      {
        "date": "2022-12-24T04:10:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/storage/docs/folders#overview\nAnswer A"
      },
      {
        "date": "2022-12-14T06:50:00.000Z",
        "voteCount": 1,
        "content": "Answer?"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 165,
    "url": "https://www.examtopics.com/discussions/google/view/91827-exam-professional-cloud-developer-topic-1-question-165/",
    "body": "Your company\u2019s corporate policy states that there must be a copyright comment at the very beginning of all source files. You want to write a custom step in Cloud Build that is triggered by each source commit. You need the trigger to validate that the source contains a copyright and add one for subsequent steps if not there. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBuild a new Docker container that examines the files in /workspace and then checks and adds a copyright for each source file. Changed files are explicitly committed back to the source repository.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBuild a new Docker container that examines the files in /workspace and then checks and adds a copyright for each source file. Changed files do not need to be committed back to the source repository.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBuild a new Docker container that examines the files in a Cloud Storage bucket and then checks and adds a copyright for each source file. Changed files are written back to the Cloud Storage bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBuild a new Docker container that examines the files in a Cloud Storage bucket and then checks and adds a copyright for each source file. Changed files are explicitly committed back to the source repository."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 7,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-08-06T08:29:00.000Z",
        "voteCount": 1,
        "content": "If a company policy states that every source code should have the copyright comment at the beginning of each file then for every build, we need to scan for each source code file and generate the copyright comments if not there, commit the updated files back to the repository.  This is like a prebuild check."
      },
      {
        "date": "2023-01-28T01:49:00.000Z",
        "voteCount": 2,
        "content": "A. Build a new Docker container that examines the files in /workspace and then checks and adds a copyright for each source file. Changed files are explicitly committed back to the source repository.\n\nThis option would allow you to create a custom step in Cloud Build that is triggered by each source commit, which would examine the source files in the /workspace directory, check for the presence of a copyright comment, and add one if not present. By committing the changed files back to the source repository, you ensure that the updated files with the added copyright comment are properly tracked and stored in the source control system."
      },
      {
        "date": "2023-01-08T10:44:00.000Z",
        "voteCount": 1,
        "content": "the code changes must be put back in the workplace folder or the sub other sub-step won't have the changes."
      },
      {
        "date": "2022-12-22T05:01:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/build/docs/automating-builds/create-manage-triggers\nAnswer A"
      },
      {
        "date": "2022-12-16T06:11:00.000Z",
        "voteCount": 3,
        "content": "A is the answer.\n\nhttps://cloud.google.com/build/docs/configuring-builds/pass-data-between-steps#passing_data_using_workspaces\nTo pass data between build steps, store the assets produced by the build step in /workspace and these assets will be available to any subsequent build steps."
      },
      {
        "date": "2022-12-16T06:10:00.000Z",
        "voteCount": 1,
        "content": "B is the answer.\n\nhttps://cloud.google.com/build/docs/configuring-builds/pass-data-between-steps#passing_data_using_workspaces\nTo pass data between build steps, store the assets produced by the build step in /workspace and these assets will be available to any subsequent build steps."
      },
      {
        "date": "2022-12-16T06:11:00.000Z",
        "voteCount": 2,
        "content": "Sorry answer should be A."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 166,
    "url": "https://www.examtopics.com/discussions/google/view/91692-exam-professional-cloud-developer-topic-1-question-166/",
    "body": "One of your deployed applications in Google Kubernetes Engine (GKE) is having intermittent performance issues. Your team uses a third-party logging solution. You want to install this solution on each node in your GKE cluster so you can view the logs. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the third-party solution as a DaemonSet\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify your container image to include the monitoring software",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse SSH to connect to the GKE node, and install the software manually",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the third-party solution using Terraform and deploy the logging Pod as a Kubernetes Deployment"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-09-22T23:44:00.000Z",
        "voteCount": 1,
        "content": "DaemonSet is correct choice here."
      },
      {
        "date": "2023-08-06T08:35:00.000Z",
        "voteCount": 1,
        "content": "A is best suitable than D here.  D is complicated with Terraform and so on.  Another solution would be to deploy the third party logging solution as a sidecar container with the main application."
      },
      {
        "date": "2023-01-08T13:13:00.000Z",
        "voteCount": 1,
        "content": "A is the answer, it's the use of daemonSet to ensures that a specific Pod is always running on all or some subset of the nodes"
      },
      {
        "date": "2022-12-22T04:53:00.000Z",
        "voteCount": 1,
        "content": "https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/\nAnswer A"
      },
      {
        "date": "2022-12-15T06:08:00.000Z",
        "voteCount": 2,
        "content": "A is the answer.\n\nhttps://cloud.google.com/kubernetes-engine/docs/concepts/daemonset#usage_patterns\nDaemonSets are useful for deploying ongoing background tasks that you need to run on all or certain nodes, and which do not require user intervention. Examples of such tasks include storage daemons like ceph, log collection daemons like fluent-bit, and node monitoring daemons like collectd."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 167,
    "url": "https://www.examtopics.com/discussions/google/view/92035-exam-professional-cloud-developer-topic-1-question-167/",
    "body": "Case study -<br><br>This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.<br><br>To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.<br><br>At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.<br><br><br>To start the case study -<br>To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.<br><br><br>Company Overview -<br>HipLocal is a community application designed to facilitate communication between people in close proximity. It is used for event planning and organizing sporting events, and for businesses to connect with their local communities. HipLocal launched recently in a few neighborhoods in Dallas and is rapidly growing into a global phenomenon. Its unique style of hyper-local community communication and business outreach is in demand around the world.<br><br><br>Executive Statement -<br>We are the number one local community app; it's time to take our local community services global. Our venture capital investors want to see rapid growth and the same great experience for new local and virtual communities that come online, whether their members are 10 or 10000 miles away from each other.<br><br><br>Solution Concept -<br>HipLocal wants to expand their existing service, with updated functionality, in new regions to better serve their global customers. They want to hire and train a new team to support these regions in their time zones. They will need to ensure that the application scales smoothly and provides clear uptime data, and that they analyze and respond to any issues that occur.<br><br><br>Existing Technical Environment -<br>HipLocal's environment is a mix of on-premises hardware and infrastructure running in Google Cloud Platform. The HipLocal team understands their application well, but has limited experience in global scale applications. Their existing technical environment is as follows:<br>\u2022 Existing APIs run on Compute Engine virtual machine instances hosted in GCP.<br>\u2022 State is stored in a single instance MySQL database in GCP.<br>\u2022 Release cycles include development freezes to allow for QA testing.<br>\u2022 The application has no logging.<br>\u2022 Applications are manually deployed by infrastructure engineers during periods of slow traffic on weekday evenings.<br>\u2022 There are basic indicators of uptime; alerts are frequently fired when the APIs are unresponsive.<br><br><br>Business Requirements -<br>HipLocal's investors want to expand their footprint and support the increase in demand they are seeing. Their requirements are:<br>\u2022 Expand availability of the application to new regions.<br>\u2022 Support 10x as many concurrent users.<br>\u2022 Ensure a consistent experience for users when they travel to different regions.<br>\u2022 Obtain user activity metrics to better understand how to monetize their product.<br>\u2022 Ensure compliance with regulations in the new regions (for example, GDPR).<br>\u2022 Reduce infrastructure management time and cost.<br>\u2022 Adopt the Google-recommended practices for cloud computing.<br>\u25cb Develop standardized workflows and processes around application lifecycle management.<br>\u25cb Define service level indicators (SLIs) and service level objectives (SLOs).<br><br><br>Technical Requirements -<br>\u2022 Provide secure communications between the on-premises data center and cloud-hosted applications and infrastructure.<br>\u2022 The application must provide usage metrics and monitoring.<br>\u2022 APIs require authentication and authorization.<br>\u2022 Implement faster and more accurate validation of new features.<br>\u2022 Logging and performance metrics must provide actionable information to be able to provide debugging information and alerts.<br>\u2022 Must scale to meet user demand.<br><br><br>For this question, refer to the HipLocal case study.<br><br>How should HipLocal redesign their architecture to ensure that the application scales to support a large increase in users?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Google Kubernetes Engine (GKE) to run the application as a microservice. Run the MySQL database on a dedicated GKE node.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse multiple Compute Engine instances to run MySQL to store state information. Use a Google Cloud-managed load balancer to distribute the load between instances. Use managed instance groups for scaling.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Memorystore to store session information and CloudSQL to store state information. Use a Google Cloud-managed load balancer to distribute the load between instances. Use managed instance groups for scaling.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a Cloud Storage bucket to serve the application as a static website, and use another Cloud Storage bucket to store user state information."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-09-22T23:59:00.000Z",
        "voteCount": 2,
        "content": "C is correct."
      },
      {
        "date": "2023-08-06T11:30:00.000Z",
        "voteCount": 3,
        "content": "HipLocal Standard Cloud Computing Scenario:\nMySQL to Cloud SQL definitely needed.\nManaged Instance group and load balancer are required.\nMemorystore (Redis) to store user's session data is required."
      },
      {
        "date": "2023-01-08T13:16:00.000Z",
        "voteCount": 1,
        "content": "C is the answer."
      },
      {
        "date": "2022-12-21T08:47:00.000Z",
        "voteCount": 1,
        "content": "C is the answer."
      },
      {
        "date": "2022-12-18T21:15:00.000Z",
        "voteCount": 3,
        "content": "A,B and D can be eliminated\n\nA. Because running MySQL inside GKE is not a GCP Best practice (there is CloudSQL)\nB. Running MySQL manually on CE instances is not best practice (there is CloudSQL)\nD. State information does not belong in cloud storage\n\nSo that leaves C as the only valid option."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 168,
    "url": "https://www.examtopics.com/discussions/google/view/92182-exam-professional-cloud-developer-topic-1-question-168/",
    "body": "Case study -<br><br>This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.<br><br>To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.<br><br>At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.<br><br><br>To start the case study -<br>To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.<br><br><br>Company Overview -<br>HipLocal is a community application designed to facilitate communication between people in close proximity. It is used for event planning and organizing sporting events, and for businesses to connect with their local communities. HipLocal launched recently in a few neighborhoods in Dallas and is rapidly growing into a global phenomenon. Its unique style of hyper-local community communication and business outreach is in demand around the world.<br><br><br>Executive Statement -<br>We are the number one local community app; it's time to take our local community services global. Our venture capital investors want to see rapid growth and the same great experience for new local and virtual communities that come online, whether their members are 10 or 10000 miles away from each other.<br><br><br>Solution Concept -<br>HipLocal wants to expand their existing service, with updated functionality, in new regions to better serve their global customers. They want to hire and train a new team to support these regions in their time zones. They will need to ensure that the application scales smoothly and provides clear uptime data, and that they analyze and respond to any issues that occur.<br><br><br>Existing Technical Environment -<br>HipLocal's environment is a mix of on-premises hardware and infrastructure running in Google Cloud Platform. The HipLocal team understands their application well, but has limited experience in global scale applications. Their existing technical environment is as follows:<br>\u2022 Existing APIs run on Compute Engine virtual machine instances hosted in GCP.<br>\u2022 State is stored in a single instance MySQL database in GCP.<br>\u2022 Release cycles include development freezes to allow for QA testing.<br>\u2022 The application has no logging.<br>\u2022 Applications are manually deployed by infrastructure engineers during periods of slow traffic on weekday evenings.<br>\u2022 There are basic indicators of uptime; alerts are frequently fired when the APIs are unresponsive.<br><br><br>Business Requirements -<br>HipLocal's investors want to expand their footprint and support the increase in demand they are seeing. Their requirements are:<br>\u2022 Expand availability of the application to new regions.<br>\u2022 Support 10x as many concurrent users.<br>\u2022 Ensure a consistent experience for users when they travel to different regions.<br>\u2022 Obtain user activity metrics to better understand how to monetize their product.<br>\u2022 Ensure compliance with regulations in the new regions (for example, GDPR).<br>\u2022 Reduce infrastructure management time and cost.<br>\u2022 Adopt the Google-recommended practices for cloud computing.<br>\u25cb Develop standardized workflows and processes around application lifecycle management.<br>\u25cb Define service level indicators (SLIs) and service level objectives (SLOs).<br><br><br>Technical Requirements -<br>\u2022 Provide secure communications between the on-premises data center and cloud-hosted applications and infrastructure.<br>\u2022 The application must provide usage metrics and monitoring.<br>\u2022 APIs require authentication and authorization.<br>\u2022 Implement faster and more accurate validation of new features.<br>\u2022 Logging and performance metrics must provide actionable information to be able to provide debugging information and alerts.<br>\u2022 Must scale to meet user demand.<br><br><br>For this question, refer to the HipLocal case study.<br><br>How should HipLocal increase their API development speed while continuing to provide the QA team with a stable testing environment that meets feature requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInclude unit tests in their code, and prevent deployments to QA until all tests have a passing status.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInclude performance tests in their code, and prevent deployments to QA until all tests have a passing status.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate health checks for the QA environment, and redeploy the APIs at a later time if the environment is unhealthy.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRedeploy the APIs to App Engine using Traffic Splitting. Do not move QA traffic to the new versions if errors are found."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-09-22T23:55:00.000Z",
        "voteCount": 1,
        "content": "A is correct."
      },
      {
        "date": "2023-08-06T11:38:00.000Z",
        "voteCount": 2,
        "content": "The answer must be A.  Dev team's responsibility is to make sure all unit tests are passed before saying code is ready for testing.\nNote: GCP PCD questions are really ridiculous.  There are many ways to ask this question in a very simple way.  I guess the questions are being prepared by a non-technical staff of GCP."
      },
      {
        "date": "2023-01-08T13:19:00.000Z",
        "voteCount": 1,
        "content": "Answer A"
      },
      {
        "date": "2022-12-22T04:49:00.000Z",
        "voteCount": 1,
        "content": "Answer A"
      },
      {
        "date": "2022-12-21T08:44:00.000Z",
        "voteCount": 1,
        "content": "A is the answer."
      },
      {
        "date": "2022-12-20T06:06:00.000Z",
        "voteCount": 2,
        "content": "A stable environment is one that works. Performance testing does not mean it works fine, unit testing will enable this."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 169,
    "url": "https://www.examtopics.com/discussions/google/view/92038-exam-professional-cloud-developer-topic-1-question-169/",
    "body": "Case study -<br><br>This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.<br><br>To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.<br><br>At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.<br><br><br>To start the case study -<br>To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.<br><br><br>Company Overview -<br>HipLocal is a community application designed to facilitate communication between people in close proximity. It is used for event planning and organizing sporting events, and for businesses to connect with their local communities. HipLocal launched recently in a few neighborhoods in Dallas and is rapidly growing into a global phenomenon. Its unique style of hyper-local community communication and business outreach is in demand around the world.<br><br><br>Executive Statement -<br>We are the number one local community app; it's time to take our local community services global. Our venture capital investors want to see rapid growth and the same great experience for new local and virtual communities that come online, whether their members are 10 or 10000 miles away from each other.<br><br><br>Solution Concept -<br>HipLocal wants to expand their existing service, with updated functionality, in new regions to better serve their global customers. They want to hire and train a new team to support these regions in their time zones. They will need to ensure that the application scales smoothly and provides clear uptime data, and that they analyze and respond to any issues that occur.<br><br><br>Existing Technical Environment -<br>HipLocal's environment is a mix of on-premises hardware and infrastructure running in Google Cloud Platform. The HipLocal team understands their application well, but has limited experience in global scale applications. Their existing technical environment is as follows:<br>\u2022 Existing APIs run on Compute Engine virtual machine instances hosted in GCP.<br>\u2022 State is stored in a single instance MySQL database in GCP.<br>\u2022 Release cycles include development freezes to allow for QA testing.<br>\u2022 The application has no logging.<br>\u2022 Applications are manually deployed by infrastructure engineers during periods of slow traffic on weekday evenings.<br>\u2022 There are basic indicators of uptime; alerts are frequently fired when the APIs are unresponsive.<br><br><br>Business Requirements -<br>HipLocal's investors want to expand their footprint and support the increase in demand they are seeing. Their requirements are:<br>\u2022 Expand availability of the application to new regions.<br>\u2022 Support 10x as many concurrent users.<br>\u2022 Ensure a consistent experience for users when they travel to different regions.<br>\u2022 Obtain user activity metrics to better understand how to monetize their product.<br>\u2022 Ensure compliance with regulations in the new regions (for example, GDPR).<br>\u2022 Reduce infrastructure management time and cost.<br>\u2022 Adopt the Google-recommended practices for cloud computing.<br>\u25cb Develop standardized workflows and processes around application lifecycle management.<br>\u25cb Define service level indicators (SLIs) and service level objectives (SLOs).<br><br><br>Technical Requirements -<br>\u2022 Provide secure communications between the on-premises data center and cloud-hosted applications and infrastructure.<br>\u2022 The application must provide usage metrics and monitoring.<br>\u2022 APIs require authentication and authorization.<br>\u2022 Implement faster and more accurate validation of new features.<br>\u2022 Logging and performance metrics must provide actionable information to be able to provide debugging information and alerts.<br>\u2022 Must scale to meet user demand.<br><br><br>For this question, refer to the HipLocal case study.<br><br>HipLocal's application uses Cloud Client Libraries to interact with Google Cloud. HipLocal needs to configure authentication and authorization in the Cloud Client Libraries to implement least privileged access for the application. What should they do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an API key. Use the API key to interact with Google Cloud.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the default compute service account to interact with Google Cloud.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a service account for the application. Export and deploy the private key for the application. Use the service account to interact with Google Cloud.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a service account for the application and for each Google Cloud API used by the application. Export and deploy the private keys used by the application. Use the service account with one Google Cloud API to interact with Google Cloud."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-03T12:35:00.000Z",
        "voteCount": 1,
        "content": "C is correct."
      },
      {
        "date": "2023-09-22T23:57:00.000Z",
        "voteCount": 1,
        "content": "C is correct."
      },
      {
        "date": "2023-08-06T11:45:00.000Z",
        "voteCount": 1,
        "content": "B is easily eliminated.\nA is not that much secure.  Provides only authorization and not authentication. There is no IAM here.\nD is more complex and not necessary to create service account for every API within the application."
      },
      {
        "date": "2023-01-28T01:54:00.000Z",
        "voteCount": 1,
        "content": "C. Create a service account for the application. Export and deploy the private key for the application. Use the service account to interact with Google Cloud.\nThis approach allows for least privileged access, as the service account will only have the necessary permissions to access the specific Google Cloud resources that the application needs. Option A, using an API key, would not provide the same level of granularity in terms of access permissions. Option B, using the default compute service account, would not provide the ability to restrict access to specific resources. Option D, creating a service account for each API, would be overly complex and may not be necessary if the permissions can be granted on a more general level."
      },
      {
        "date": "2023-01-08T13:21:00.000Z",
        "voteCount": 1,
        "content": "Answer C"
      },
      {
        "date": "2022-12-26T10:18:00.000Z",
        "voteCount": 1,
        "content": "Answer C"
      },
      {
        "date": "2022-12-21T08:42:00.000Z",
        "voteCount": 1,
        "content": "C is the answer."
      },
      {
        "date": "2022-12-18T21:45:00.000Z",
        "voteCount": 3,
        "content": "A,B,D can be eliminated:\n\nA. Cloud Client Libraries do not use API Keys to authenticate\nB. Compute engine default service account has too many privileges \nD. It does not make sense to create an SA for every API being access. The SA represents the Application itself, not the API\n\nSo that leaves C as the only valid option.\n\nStill, ideally you should not copy SA keys around. Most of the time, GCP gives you a way to associate a service account with a workload."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 170,
    "url": "https://www.examtopics.com/discussions/google/view/91691-exam-professional-cloud-developer-topic-1-question-170/",
    "body": "You are in the final stage of migrating an on-premises data center to Google Cloud. You are quickly approaching your deadline, and discover that a web API is running on a server slated for decommissioning. You need to recommend a solution to modernize this API while migrating to Google Cloud. The modernized web API must meet the following requirements:<br><br>\u2022 Autoscales during high traffic periods at the end of each month<br>\u2022 Written in Python 3.x<br>\u2022 Developers must be able to rapidly deploy new versions in response to frequent code changes<br><br>You want to minimize cost, effort, and operational overhead of this migration. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModernize and deploy the code on App Engine flexible environment.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModernize and deploy the code on App Engine standard environment.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the modernized application to an n1-standard-1 Compute Engine instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAsk the development team to re-write the application to run as a Docker container on Google Kubernetes Engine."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 7,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-09-23T00:03:00.000Z",
        "voteCount": 1,
        "content": "App Engine flexible environment is a fully managed platform for running Python 3.x applications. It autoscales during high traffic periods and can be rapidly deployed using the App Engine SDK or the App Engine gcloud command-line tool. Additionally, App Engine flexible environment is a cost-effective solution, as you only pay for the resources that you use."
      },
      {
        "date": "2023-12-06T10:08:00.000Z",
        "voteCount": 1,
        "content": "Keep in mind that we're also asked to minimize the cost here.\n\nThe GAE Standard is better as it supports Python 3.x already and it's a cheaper solution. GAE Flexible doesn't scale down to 0 and it will always have at least 1 instance running."
      },
      {
        "date": "2023-08-06T11:49:00.000Z",
        "voteCount": 1,
        "content": "B is a very straight forward option.\nClue: Python + Easy and fast scaling + Cost effective + frequest releases"
      },
      {
        "date": "2022-12-22T04:36:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/appengine/docs/standard#standard_environment_languages_and_runtimes\nAnswer B"
      },
      {
        "date": "2022-12-18T22:04:00.000Z",
        "voteCount": 2,
        "content": "A,C and D can be eliminated\n\nA. App engine flexible cannot scale down to 0, thus not minimizes the cost\nC. Deploying to a single VM will not allow autoscaling \nD. Running in a GKE cluster will not minimize the cost\n\nThat leaves B as the only valid option."
      },
      {
        "date": "2022-12-15T06:06:00.000Z",
        "voteCount": 4,
        "content": "B is the answer.\n\nhttps://cloud.google.com/appengine/docs/standard"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 171,
    "url": "https://www.examtopics.com/discussions/google/view/91690-exam-professional-cloud-developer-topic-1-question-171/",
    "body": "You are developing an application that consists of several microservices running in a Google Kubernetes Engine cluster. One microservice needs to connect to a third-party database running on-premises. You need to store credentials to the database and ensure that these credentials can be rotated while following security best practices. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the credentials in a sidecar container proxy, and use it to connect to the third-party database.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a service mesh to allow or restrict traffic from the Pods in your microservice to the database.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the credentials in an encrypted volume mount, and associate a Persistent Volume Claim with the client Pod.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the credentials as a Kubernetes Secret, and use the Cloud Key Management Service plugin to handle encryption and decryption.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-09-23T00:06:00.000Z",
        "voteCount": 1,
        "content": "D is correct."
      },
      {
        "date": "2023-08-06T11:53:00.000Z",
        "voteCount": 2,
        "content": "Storing credentials as a Kubernetes secret + KMS for encryption and decryption of the DB credentials are the best answer."
      },
      {
        "date": "2023-02-12T06:16:00.000Z",
        "voteCount": 1,
        "content": "Storing sensitive information such as database credentials in Kubernetes Secrets is a common and secure way to manage sensitive information in a cluster. The Cloud Key Management Service (KMS) can be used to further protect the secrets by encrypting and decrypting them, ensuring that they are protected both at rest and in transit. This combination of Kubernetes Secrets and Cloud KMS provides a secure way to manage and rotate credentials while following security best practices.\n\nOptions A and B are not recommended, as they do not provide a secure and centralized way to manage and rotate credentials. Option C is not recommended because storing secrets in an encrypted volume mount is not as secure as using a Key Management Service, as the encryption keys must still be managed and protected within the cluster."
      },
      {
        "date": "2022-12-22T04:31:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/kubernetes-engine/docs/how-to/encrypting-secrets#reencrypt-secrets\nAnswer D"
      },
      {
        "date": "2022-12-15T06:04:00.000Z",
        "voteCount": 3,
        "content": "D is the answer.\n\nhttps://cloud.google.com/kubernetes-engine/docs/how-to/encrypting-secrets\nBy default, Google Kubernetes Engine (GKE) encrypts customer content stored at rest, including Secrets. GKE handles and manages this default encryption for you without any additional action on your part.\n\nApplication-layer secrets encryption provides an additional layer of security for sensitive data, such as Secrets, stored in etcd. Using this functionality, you can use a key managed with Cloud KMS to encrypt data at the application layer. This encryption protects against attackers who gain access to an offline copy of etcd."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 172,
    "url": "https://www.examtopics.com/discussions/google/view/91689-exam-professional-cloud-developer-topic-1-question-172/",
    "body": "You manage your company's ecommerce platform's payment system, which runs on Google Cloud. Your company must retain user logs for 1 year for internal auditing purposes and for 3 years to meet compliance requirements. You need to store new user logs on Google Cloud to minimize on-premises storage usage and ensure that they are easily searchable. You want to minimize effort while ensuring that the logs are stored correctly. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the logs in a Cloud Storage bucket with bucket lock turned on.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the logs in a Cloud Storage bucket with a 3-year retention period.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the logs in Cloud Logging as custom logs with a custom retention period.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the logs in a Cloud Storage bucket with a 1-year retention period. After 1 year, move the logs to another bucket with a 2-year retention period."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-18T22:26:00.000Z",
        "voteCount": 9,
        "content": "The requirements say that the logs should be easily searchable. This is not easily achieved in Cloud Storage, so that eliminates A,B and D.\n\nThat leaves C and the valid option.\n\nNote, that it's possible to configure Cloud Logging with a custom retention period.\nhttps://cloud.google.com/logging/docs/buckets#custom-retention"
      },
      {
        "date": "2022-12-22T05:11:00.000Z",
        "voteCount": 1,
        "content": "Agree that Cloud Logging will be better for search."
      },
      {
        "date": "2023-09-23T00:13:00.000Z",
        "voteCount": 1,
        "content": "C is correct."
      },
      {
        "date": "2023-08-06T12:18:00.000Z",
        "voteCount": 1,
        "content": "Tricky question.\nEasily searchable is the key here.\nCloud logging supports retaining the logs between 1 to 3650 (10 years max) and we can set custom retention period on the cloud logs."
      },
      {
        "date": "2023-01-08T13:31:00.000Z",
        "voteCount": 1,
        "content": "C is the correct answer because Cloud Logging to retain logs between 1 day and 3650 days"
      },
      {
        "date": "2022-12-22T05:12:00.000Z",
        "voteCount": 2,
        "content": "C is the correct answer instead.\n\nhttps://cloud.google.com/logging/docs/routing/overview#logs-retention\nCloud Logging retains logs according to retention rules applying to the log bucket type where the logs are held.\n\nYou can configure Cloud Logging to retain logs between 1 day and 3650 days. Custom retention rules apply to all the logs in a bucket, regardless of the log type or whether that log has been copied from another location."
      },
      {
        "date": "2022-12-22T04:18:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/logging/docs/audit/best-practices#custom-retention\nhttps://cloud.google.com/logging/docs/central-log-storage\nLogs must be searchable as required by the question. point number 4 for the second link above supports that.\nAnswer C"
      },
      {
        "date": "2022-12-15T06:02:00.000Z",
        "voteCount": 1,
        "content": "B is the answer."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 173,
    "url": "https://www.examtopics.com/discussions/google/view/91688-exam-professional-cloud-developer-topic-1-question-173/",
    "body": "Your company has a new security initiative that requires all data stored in Google Cloud to be encrypted by customer-managed encryption keys. You plan to use Cloud Key Management Service (KMS) to configure access to the keys. You need to follow the \"separation of duties\" principle and Google-recommended best practices. What should you do? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tProvision Cloud KMS in its own project.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDo not assign an owner to the Cloud KMS project.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tProvision Cloud KMS in the project where the keys are being used.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGrant the roles/cloudkms.admin role to the owner of the project where the keys from Cloud KMS are being used.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGrant an owner role for the Cloud KMS project to a different user than the owner of the project where the keys from Cloud KMS are being used."
    ],
    "answer": "AB",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AB",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "AE",
        "count": 5,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-20T07:44:00.000Z",
        "voteCount": 5,
        "content": "AB should be correct instead.\n\nhttps://cloud.google.com/kms/docs/separation-of-duties#using_separate_project\nInstead, to allow for a separation of duties, you could run Cloud KMS in its own project, for example your-key-project. Then, depending on the strictness of your separation requirements, you could either:\n- (recommended) Create your-key-project without an owner at the project level, and designate an Organization Admin granted at the organization-level. Unlike an owner, an Organization Admin can't manage or use keys directly. They are restricted to setting IAM policies, which restrict who can manage and use keys. Using an organization-level node, you can further restrict permissions for projects in your organization."
      },
      {
        "date": "2023-09-23T00:16:00.000Z",
        "voteCount": 2,
        "content": "AB is correct."
      },
      {
        "date": "2023-09-23T00:20:00.000Z",
        "voteCount": 2,
        "content": "AE is correct as E provide separation of duty."
      },
      {
        "date": "2023-02-03T08:38:00.000Z",
        "voteCount": 3,
        "content": "To follow Google-recommended best practices, I would recommend choosing options A and E:\n\nA. Provision Cloud KMS in its own project - this helps to ensure that the management of encryption keys is isolated and separate from other projects in your Google Cloud organization.\n\nE. Grant an owner role for the Cloud KMS project to a different user than the owner of the project where the keys from Cloud KMS are being used - this follows the \"separation of duties\" principle and helps to ensure that the management of encryption keys is not tied to the project where the keys are being used."
      },
      {
        "date": "2022-12-26T10:17:00.000Z",
        "voteCount": 3,
        "content": "Answer A, B"
      },
      {
        "date": "2022-12-18T22:39:00.000Z",
        "voteCount": 4,
        "content": "As per the docs, https://cloud.google.com/kms/docs/separation-of-duties#using_separate_project\n\n\n1. The KMS should be in its own project\n2. Ideally, you should not assign an owner to the KMS project"
      },
      {
        "date": "2022-12-20T07:42:00.000Z",
        "voteCount": 4,
        "content": "After reading the documentation again, agree with you on AB.\n\nhttps://cloud.google.com/kms/docs/separation-of-duties#using_separate_project\n(recommended) Create your-key-project without an owner at the project level, and designate an Organization Admin granted at the organization-level. Unlike an owner, an Organization Admin can't manage or use keys directly. They are restricted to setting IAM policies, which restrict who can manage and use keys. Using an organization-level node, you can further restrict permissions for projects in your organization."
      },
      {
        "date": "2022-12-20T07:39:00.000Z",
        "voteCount": 1,
        "content": "For E, the owner of the KMS project is different from the project where keys from Cloud KMS is used."
      },
      {
        "date": "2022-12-15T06:01:00.000Z",
        "voteCount": 2,
        "content": "AE is the answer.\n\nhttps://cloud.google.com/kms/docs/separation-of-duties#using_separate_project\nCloud KMS could be run in an existing project, for example your-project, and this might be sensible if the data being encrypted with keys in Cloud KMS is stored in the same project.\n\nHowever, any user with owner access on that project is then also able to manage (and perform cryptographic operations with) keys in Cloud KMS in that project. This is because the keys themselves are owned by the project, of which the user is an owner.\n\nInstead, to allow for a separation of duties, you could run Cloud KMS in its own project, for example your-key-project."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 174,
    "url": "https://www.examtopics.com/discussions/google/view/91687-exam-professional-cloud-developer-topic-1-question-174/",
    "body": "You need to migrate a standalone Java application running in an on-premises Linux virtual machine (VM) to Google Cloud in a cost-effective manner. You decide not to take the lift-and-shift approach, and instead you plan to modernize the application by converting it to a container. How should you accomplish this task?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Migrate for Anthos to migrate the VM to your Google Kubernetes Engine (GKE) cluster as a container.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExport the VM as a raw disk and import it as an image. Create a Compute Engine instance from the Imported image.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Migrate for Compute Engine to migrate the VM to a Compute Engine instance, and use Cloud Build to convert it to a container.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Jib to build a Docker image from your source code, and upload it to Artifact Registry. Deploy the application in a GKE cluster, and test the application.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 7,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-10-09T08:52:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/java/getting-started/jib"
      },
      {
        "date": "2023-09-23T00:23:00.000Z",
        "voteCount": 2,
        "content": "Jib is a tool that builds Docker images from Java code without the need for a Dockerfile. This makes it easy to containerize Java applications, even if you don't have any experience with Docker."
      },
      {
        "date": "2023-08-06T12:33:00.000Z",
        "voteCount": 2,
        "content": "Going for D for the below reasons:\n- Cost effective way\n- Modernize the application by converting it to a container"
      },
      {
        "date": "2023-06-14T01:36:00.000Z",
        "voteCount": 1,
        "content": "This seems to be exactly the situation described in this tutorial:\nhttps://cloud.google.com/migrate/containers/docs/migrating-monolith-vm-overview-setup\nSo I think that option A is correct"
      },
      {
        "date": "2023-12-06T05:36:00.000Z",
        "voteCount": 1,
        "content": "Indeed, but I think Alt A doesn't address the whole picture. Migrate for Anthos itself would not automatically containerize the Java app running on the Linux VM. Migrate for Anthos can package the VM as a Kubernetes pod, but the application within the VM remains unchanged in its original form. To containerize this application and make it fully compatible with GKE, additional steps - those described in Alt D - would be required.\nAlternative D is correct."
      },
      {
        "date": "2023-01-09T02:34:00.000Z",
        "voteCount": 1,
        "content": "Answer D\nhttps://cloud.google.com/blog/products/application-development/introducing-jib-build-java-docker-images-better"
      },
      {
        "date": "2022-12-22T03:47:00.000Z",
        "voteCount": 1,
        "content": "Answer D\nhttps://cloud.google.com/blog/products/application-development/introducing-jib-build-java-docker-images-better"
      },
      {
        "date": "2022-12-15T05:54:00.000Z",
        "voteCount": 1,
        "content": "D is the answer.\n\nhttps://cloud.google.com/blog/products/application-development/introducing-jib-build-java-docker-images-better"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 175,
    "url": "https://www.examtopics.com/discussions/google/view/91686-exam-professional-cloud-developer-topic-1-question-175/",
    "body": "Your organization has recently begun an initiative to replatform their legacy applications onto Google Kubernetes Engine. You need to decompose a monolithic application into microservices. Multiple instances have read and write access to a configuration file, which is stored on a shared file system. You want to minimize the effort required to manage this transition, and you want to avoid rewriting the application code. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new Cloud Storage bucket, and mount it via FUSE in the container.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new persistent disk, and mount the volume as a shared PersistentVolume.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new Filestore instance, and mount the volume as an NFS PersistentVolume.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new ConfigMap and volumeMount to store the contents of the configuration file."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 20,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-23T11:05:00.000Z",
        "voteCount": 12,
        "content": "A is incorrect, because Cloud Storage FUSE does not support concurrency and file locking.\nB is incorrect, because a persistent disk PersistentVolume is not read-write-many. It can only be read-write once or read-many.\nC is correct, because it\u2019s the only managed, supported read-write-many storage option available for file-system access in Google Kubernetes Engine.\nD is incorrect, because the ConfigMap cannot be written to from the Pods.\n\nhttps://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes\nhttps://cloud.google.com/filestore/docs/accessing-fileshares\nhttps://cloud.google.com/storage/docs/gcs-fuse"
      },
      {
        "date": "2023-09-23T00:27:00.000Z",
        "voteCount": 1,
        "content": "ConfigMaps are Kubernetes objects that allow you to store configuration data in a key-value format. ConfigMaps are immutable, which means that they cannot be changed once they are created. This makes them ideal for storing configuration data that needs to be shared between multiple Pods."
      },
      {
        "date": "2024-06-08T09:19:00.000Z",
        "voteCount": 1,
        "content": "In this case the configuration file will be both read and written by the instances, so D is not the answer"
      },
      {
        "date": "2023-08-06T12:41:00.000Z",
        "voteCount": 1,
        "content": "ConfigMap is the usual way to store application configurations those runs under the cluster.  I donot understand why many of you are saying C.  The question is asking us to how we are going to manage configuration data in a GKE environment."
      },
      {
        "date": "2022-12-22T04:31:00.000Z",
        "voteCount": 2,
        "content": "C is the correct answer instead.\n\nhttps://kubernetes.io/docs/concepts/storage/volumes/#nfs\nAn nfs volume allows an existing NFS (Network File System) share to be mounted into a Pod. Unlike emptyDir, which is erased when a Pod is removed, the contents of an nfs volume are preserved and the volume is merely unmounted. This means that an NFS volume can be pre-populated with data, and that data can be shared between pods. NFS can be mounted by multiple writers simultaneously."
      },
      {
        "date": "2022-12-22T03:41:00.000Z",
        "voteCount": 3,
        "content": "Answer C \nAn nfs volume allows an existing NFS (Network File System) share to be mounted into a Pod. Unlike emptyDir, which is erased when a Pod is removed, the contents of an nfs volume are preserved and the volume is merely unmounted. This means that an NFS volume can be pre-populated with data, and that data can be shared between pods"
      },
      {
        "date": "2022-12-20T12:43:00.000Z",
        "voteCount": 3,
        "content": "Generally ConfigMaps (D) are the right choice to store pod config-files, but: They are read-only, which does not match what is asked for here. If, as stated in the question, the application-parts need to be able to also write to that Configfile that should be on a shared file system, the only valid choice is a NFS PV."
      },
      {
        "date": "2022-12-22T04:30:00.000Z",
        "voteCount": 1,
        "content": "Agree with C that a NFS will be required."
      },
      {
        "date": "2022-12-15T05:51:00.000Z",
        "voteCount": 2,
        "content": "D is the answer.\n\nhttps://cloud.google.com/kubernetes-engine/docs/concepts/configmap\nConfigMaps bind non-sensitive configuration artifacts such as configuration files, command-line arguments, and environment variables to your Pod containers and system components at runtime.\n\nA ConfigMap separates your configurations from your Pod and components, which helps keep your workloads portable. This makes their configurations easier to change and manage, and prevents hardcoding configuration data to Pod specifications."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 176,
    "url": "https://www.examtopics.com/discussions/google/view/91685-exam-professional-cloud-developer-topic-1-question-176/",
    "body": "Your development team has built several Cloud Functions using Java along with corresponding integration and service tests. You are building and deploying the functions and launching the tests using Cloud Build. Your Cloud Build job is reporting deployment failures immediately after successfully validating the code. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCheck the maximum number of Cloud Function instances.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tVerify that your Cloud Build trigger has the correct build parameters.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRetry the tests using the truncated exponential backoff polling strategy.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tVerify that the Cloud Build service account is assigned the Cloud Functions Developer role.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-09-23T00:31:00.000Z",
        "voteCount": 1,
        "content": "D is correct."
      },
      {
        "date": "2023-08-06T22:34:00.000Z",
        "voteCount": 1,
        "content": "Cloud Build service account must have the IAM developer role in order to deploy cloud functions."
      },
      {
        "date": "2023-02-26T08:29:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/build/docs/securing-builds/configure-access-for-cloud-build-service-account#granting_a_role_using_the_iam_page\n\nhttps://cloud.google.com/build/docs/troubleshooting#build_trigger_fails_due_to_missing_cloudbuildbuildscreate_permission"
      },
      {
        "date": "2023-01-09T02:40:00.000Z",
        "voteCount": 1,
        "content": "Answer D"
      },
      {
        "date": "2022-12-21T23:06:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/build/docs/troubleshooting#build_trigger_fails_due_to_missing_cloudbuildbuildscreate_permission\nAnswer D"
      },
      {
        "date": "2022-12-15T05:50:00.000Z",
        "voteCount": 1,
        "content": "D is the answer.\n\nhttps://cloud.google.com/build/docs/securing-builds/configure-access-for-cloud-build-service-account"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 177,
    "url": "https://www.examtopics.com/discussions/google/view/91684-exam-professional-cloud-developer-topic-1-question-177/",
    "body": "You manage a microservices application on Google Kubernetes Engine (GKE) using Istio. You secure the communication channels between your microservices by implementing an Istio AuthorizationPolicy, a Kubernetes NetworkPolicy, and mTLS on your GKE cluster. You discover that HTTP requests between two Pods to specific URLs fail, while other requests to other URLs succeed. What is the cause of the connection issue?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA Kubernetes NetworkPolicy resource is blocking HTTP traffic between the Pods.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe Pod initiating the HTTP requests is attempting to connect to the target Pod via an incorrect TCP port.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe Authorization Policy of your cluster is blocking HTTP requests for specific paths within your application.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe cluster has mTLS configured in permissive mode, but the Pod's sidecar proxy is sending unencrypted traffic in plain text."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-09-23T00:34:00.000Z",
        "voteCount": 1,
        "content": "C is correct."
      },
      {
        "date": "2023-08-06T22:43:00.000Z",
        "voteCount": 1,
        "content": "Key here is \"HTTP requests between two Pods to specific URLs fail\", this means no auth rule set for these urls in the Istio configuration."
      },
      {
        "date": "2023-01-13T05:54:00.000Z",
        "voteCount": 3,
        "content": "A is not correct because Kubernetes NetworkPolicy resources allow you to block HTTP traffic between groups of pods but not for selected paths. (https://kubernetes.io/docs/concepts/services-networking/network-policies/).\nB is not correct because if the client pod is using an incorrect port to communicate with the server, pod requests will time out for all URL paths.\nC is correct because an Istio Authorization policy allows you to block HTTP methods between pods for specific URL paths (https://istio.io/latest/docs/tasks/security/authorization/authz-http/).\nD is not correct because mTLS configuration using Istio should not cause HTTP requests to fail. In permissive mode (default configuration), a service can accept both plain text and mTLS encrypted traffic (https://istio.io/latest/docs/tasks/security/authentication/mtls-migration/)."
      },
      {
        "date": "2022-12-21T22:59:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/service-mesh/docs/troubleshooting/troubleshoot-security#authorization_policy_denial_logging\nAnswer C\nhttps://istio.io/latest/docs/ops/common-problems/network-issues/#sending-https-to-an-http-port"
      },
      {
        "date": "2022-12-15T05:45:00.000Z",
        "voteCount": 1,
        "content": "C is the answer."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 178,
    "url": "https://www.examtopics.com/discussions/google/view/91682-exam-professional-cloud-developer-topic-1-question-178/",
    "body": "You recently migrated an on-premises monolithic application to a microservices application on Google Kubernetes Engine (GKE). The application has dependencies on backend services on-premises, including a CRM system and a MySQL database that contains personally identifiable information (PII). The backend services must remain on-premises to meet regulatory requirements.<br><br>You established a Cloud VPN connection between your on-premises data center and Google Cloud. You notice that some requests from your microservices application on GKE to the backend services are failing due to latency issues caused by fluctuating bandwidth, which is causing the application to crash. How should you address the latency issues?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Memorystore to cache frequently accessed PII data from the on-premises MySQL database",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Istio to create a service mesh that includes the microservices on GKE and the on-premises services",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the number of Cloud VPN tunnels for the connection between Google Cloud and the on-premises services\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDecrease the network layer packet size by decreasing the Maximum Transmission Unit (MTU) value from its default value on Cloud VPN"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 9,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-09-23T00:40:00.000Z",
        "voteCount": 1,
        "content": "To use Istio to reduce latency in your microservices application, you would create a service mesh that includes the microservices on GKE and the on-premises services. Istio would then manage traffic between the microservices and the on-premises services, and would use its features to reduce latency."
      },
      {
        "date": "2023-08-06T23:44:00.000Z",
        "voteCount": 1,
        "content": "Cloud Interconnect would be better option than C, I guess. Increase the VPN tunnels would provide the required bandwidth for the GCP and On-Premises services to communicate."
      },
      {
        "date": "2023-06-26T17:39:00.000Z",
        "voteCount": 1,
        "content": "C for increase bandwidth"
      },
      {
        "date": "2023-01-28T02:20:00.000Z",
        "voteCount": 2,
        "content": "Istio can help to address the latency issues by creating a service mesh that allows you to control the flow of traffic between the microservices on GKE and the on-premises services. This can allow you to monitor and manage the traffic, as well as implement features such as load balancing and circuit breaking to help mitigate the impact of latency on the application. It also allows to increase the number of Cloud VPN tunnels for the connection between Google Cloud and the on-premises services, but it is not the best approach. Increasing the number of tunnels can help to increase the available bandwidth, but it does not address the underlying issues causing the latency. Decreasing the network layer packet size by decreasing the MTU value on Cloud VPN can cause fragmentation, which can increase latency, so it is not a good approach. Caching of PII data can be a good practice but it does not address the latency issues caused by fluctuating bandwidth."
      },
      {
        "date": "2023-01-27T04:42:00.000Z",
        "voteCount": 1,
        "content": "It could be D. If you decrease the MTU, the packets get priority in FIFO queue of the routers."
      },
      {
        "date": "2022-12-21T12:14:00.000Z",
        "voteCount": 2,
        "content": "Answer C. \nTake note the question is asking you to address a bandwidth issue, C is the most appropriate"
      },
      {
        "date": "2022-12-20T07:30:00.000Z",
        "voteCount": 3,
        "content": "C is the correct answer instead.\n\nhttps://cloud.google.com/network-connectivity/docs/vpn/concepts/topologies#more-bandwidth\nTo increase the bandwidth of your HA VPN gateways, add more HA VPN tunnels."
      },
      {
        "date": "2022-12-18T23:08:00.000Z",
        "voteCount": 4,
        "content": "A, B, and D can be eliminated\n\nA. Caching PII (Personally Identifiable Information) is never a good practice\nB. Using Istio is not going to improve the latency (the network hops remain the same)\nD. Reducing the packet size, has the effect of more packets being sent across which is counter productive \n\nThat leaves C as the valid option\n\nGCP does support having multiple VPN Tunnels on the same gateway\n\nhttps://cloud.google.com/network-connectivity/docs/vpn/concepts/choosing-networks-routing#route-alignment"
      },
      {
        "date": "2022-12-20T07:29:00.000Z",
        "voteCount": 1,
        "content": "Agree with you on C.\n\nhttps://cloud.google.com/network-connectivity/docs/vpn/concepts/topologies#more-bandwidth\nTo increase the bandwidth of your HA VPN gateways, add more HA VPN tunnels."
      },
      {
        "date": "2022-12-15T05:43:00.000Z",
        "voteCount": 1,
        "content": "A is the answer."
      },
      {
        "date": "2022-12-21T12:13:00.000Z",
        "voteCount": 1,
        "content": "Tjats not a bandwidth issue , you want to address the bandwidth issue here"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 179,
    "url": "https://www.examtopics.com/discussions/google/view/89945-exam-professional-cloud-developer-topic-1-question-179/",
    "body": "Your company has deployed a new API to a Compute Engine instance. During testing, the API is not behaving as expected. You want to monitor the application over 12 hours to diagnose the problem within the application code without redeploying the application. Which tool should you use?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Trace",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Monitoring",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Debugger logpoints\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Debugger snapshots"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-09-23T00:42:00.000Z",
        "voteCount": 1,
        "content": "C is correct."
      },
      {
        "date": "2023-08-06T23:48:00.000Z",
        "voteCount": 1,
        "content": "Cloud Debugger Logpoints:  Best suitable for Prod env where we no need to change the code and redeploy after adding log statements."
      },
      {
        "date": "2023-06-14T01:57:00.000Z",
        "voteCount": 4,
        "content": "Cloud debugger is deprecated since May 31 2023 so this question is no longer valid.\nhttps://cloud.google.com/stackdriver/docs/deprecations/debugger-deprecation"
      },
      {
        "date": "2023-02-20T08:31:00.000Z",
        "voteCount": 1,
        "content": "C but debugger is deprecated"
      },
      {
        "date": "2023-01-09T02:50:00.000Z",
        "voteCount": 1,
        "content": "Answer is C because within code and without changes on code so it eliminate the others choice."
      },
      {
        "date": "2022-12-22T04:38:00.000Z",
        "voteCount": 1,
        "content": "C is the answer.\n\nhttps://cloud.google.com/debugger/docs/using/logpoints\nAfter you have deployed or started your application, you can open Cloud Debugger in the Google Cloud console. Cloud Debugger allows you to inject logging into running services without restarting or interfering with the normal function of the service. This can be useful for debugging production issues without having to add log statements and redeploy."
      },
      {
        "date": "2022-12-21T12:10:00.000Z",
        "voteCount": 1,
        "content": "Debugger logpoints\nhttps://cloud.google.com/debugger/docs/using/logpoints"
      },
      {
        "date": "2022-12-04T03:48:00.000Z",
        "voteCount": 1,
        "content": "the answer is D: can add without redeploying or changing code https://cloud.google.com/debugger/docs/using/logpoints Logpoints allow you to inject logging into running services without restarting or interfering with the normal function of the service"
      },
      {
        "date": "2022-12-04T16:31:00.000Z",
        "voteCount": 2,
        "content": "answer is C\nD is typo only"
      },
      {
        "date": "2022-12-27T02:18:00.000Z",
        "voteCount": 2,
        "content": "what about this https://cloud.google.com/debugger/docs/deprecations#snapshot_debugger\nIsn't it the same as option D?"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 180,
    "url": "https://www.examtopics.com/discussions/google/view/91681-exam-professional-cloud-developer-topic-1-question-180/",
    "body": "You are designing an application that consists of several microservices. Each microservice has its own RESTful API and will be deployed as a separate Kubernetes Service. You want to ensure that the consumers of these APIs aren't impacted when there is a change to your API, and also ensure that third-party systems aren't interrupted when new versions of the API are released. How should you configure the connection to the application following Google-recommended best practices?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an Ingress that uses the API's URL to route requests to the appropriate backend.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLeverage a Service Discovery system, and connect to the backend specified by the request.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse multiple clusters, and use DNS entries to route requests to separate versioned backends.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCombine multiple versions in the same service, and then specify the API version in the POST request."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 8,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-09-23T00:45:00.000Z",
        "voteCount": 1,
        "content": "This approach is recommended by Google because it allows you to decouple the consumers of your APIs from the specific backend services that are providing those APIs. This makes it easier to scale your application and to make changes to your APIs without impacting the consumers."
      },
      {
        "date": "2023-09-23T00:47:00.000Z",
        "voteCount": 1,
        "content": "Answer will be A."
      },
      {
        "date": "2023-08-06T23:53:00.000Z",
        "voteCount": 2,
        "content": "Ingress or Nginx service that routes ( reverse proxy ) to the appropriate urls is a best solution."
      },
      {
        "date": "2022-12-21T12:07:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/kubernetes-engine/docs/concepts/ingress#deprecated_annotation\nhttps://cloud.google.com/kubernetes-engine/docs/concepts/ingress#features_of_https_load_balancing\nAnswer A"
      },
      {
        "date": "2022-12-18T23:50:00.000Z",
        "voteCount": 3,
        "content": "B,C, and D can be eliminated\n\nB. Service discovery only works within the cluster itself, so external clients can't use it\nC. Using multiple clusters is an overkill, you can deploy multiple version of the same service within a single cluster\nD. Passing the API version in the request body is not a REST best practice\n\nThe best practice is to pass the version of the API in the the URL path, e.g /v1/foo, /v2/foo\nUsing this approach, you can route requests to the appropriate backend service within the GKE cluster using an Ingress resource, which is option A."
      },
      {
        "date": "2022-12-15T05:32:00.000Z",
        "voteCount": 2,
        "content": "D is the answer."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 181,
    "url": "https://www.examtopics.com/discussions/google/view/91679-exam-professional-cloud-developer-topic-1-question-181/",
    "body": "Your team is building an application for a financial institution. The application's frontend runs on Compute Engine, and the data resides in Cloud SQL and one Cloud Storage bucket. The application will collect data containing PII, which will be stored in the Cloud SQL database and the Cloud Storage bucket. You need to secure the PII data. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create the relevant firewall rules to allow only the frontend to communicate with the Cloud SQL database<br>2. Using IAM, allow only the frontend service account to access the Cloud Storage bucket",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create the relevant firewall rules to allow only the frontend to communicate with the Cloud SQL database<br>2. Enable private access to allow the frontend to access the Cloud Storage bucket privately",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Configure a private IP address for Cloud SQL<br>2. Use VPC-SC to create a service perimeter<br>3. Add the Cloud SQL database and the Cloud Storage bucket to the same service perimeter\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Configure a private IP address for Cloud SQL<br>2. Use VPC-SC to create a service perimeter<br>3. Add the Cloud SQL database and the Cloud Storage bucket to different service perimeters"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 13,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-19T00:31:00.000Z",
        "voteCount": 5,
        "content": "Without using VPC-SC, the PII data is not secure from exfiltration. So that leaves only C, and D as possible valid responses. However, D can be eliminated because  both the Cloud SQL instance and and Cloud Storage bucket must be within the same perimeter, which leaves C and the valid answer."
      },
      {
        "date": "2023-09-23T00:52:00.000Z",
        "voteCount": 1,
        "content": "I would go with C"
      },
      {
        "date": "2023-08-06T23:58:00.000Z",
        "voteCount": 2,
        "content": "C is correct compared to other options."
      },
      {
        "date": "2023-01-07T20:55:00.000Z",
        "voteCount": 2,
        "content": "C should be the correct answer instead."
      },
      {
        "date": "2023-01-05T02:38:00.000Z",
        "voteCount": 3,
        "content": "Answer C"
      },
      {
        "date": "2022-12-15T05:30:00.000Z",
        "voteCount": 1,
        "content": "B is the answer."
      },
      {
        "date": "2023-01-06T05:00:00.000Z",
        "voteCount": 1,
        "content": "Answer C\nhttps://cloud.google.com/vpc-service-controls/docs/service-perimeters"
      },
      {
        "date": "2022-12-17T12:10:00.000Z",
        "voteCount": 1,
        "content": "Why do you think B is the answer? I was thinking about C since we secure the PII with the service perimeter that way."
      },
      {
        "date": "2023-01-07T20:55:00.000Z",
        "voteCount": 1,
        "content": "Agree C is the better answer. Passed my exam few weeks back, chose C as well."
      },
      {
        "date": "2023-01-12T02:18:00.000Z",
        "voteCount": 1,
        "content": "Congratulations"
      },
      {
        "date": "2023-01-12T02:18:00.000Z",
        "voteCount": 2,
        "content": "Congratulations"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 182,
    "url": "https://www.examtopics.com/discussions/google/view/89951-exam-professional-cloud-developer-topic-1-question-182/",
    "body": "You are designing a chat room application that will host multiple rooms and retain the message history for each room. You have selected Firestore as your database. How should you represent the data in Firestore?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a collection for the rooms. For each room, create a document that lists the contents of the messages<br><br><img src=\"https://img.examtopics.com/professional-cloud-developer/image2.png\"><br>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a collection for the rooms. For each room, create a collection that contains a document for each message<br><br><img src=\"https://img.examtopics.com/professional-cloud-developer/image3.png\"><br>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a collection for the rooms. For each room, create a document that contains a collection for documents, each of which contains a message.<br><br><img src=\"https://img.examtopics.com/professional-cloud-developer/image4.png\"><br>\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a collection for the rooms, and create a document for each room. Create a separate collection for messages, with one document per message. Each room\u2019s document contains a list of references to the messages.<br><br><img src=\"https://img.examtopics.com/professional-cloud-developer/image5.png\">"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-09-23T04:42:00.000Z",
        "voteCount": 1,
        "content": "Community answer C."
      },
      {
        "date": "2022-12-25T05:07:00.000Z",
        "voteCount": 1,
        "content": "Answer C"
      },
      {
        "date": "2022-12-15T05:26:00.000Z",
        "voteCount": 2,
        "content": "C is the answer.\n\nhttps://firebase.google.com/docs/firestore/data-model#hierarchical-data"
      },
      {
        "date": "2022-12-04T04:12:00.000Z",
        "voteCount": 1,
        "content": "Answer is C. \"The best way to store messages in this scenario is by using subcollections. A subcollection is a collection associated with a specific document.\"\n\nhttps://firebase.google.com/docs/firestore/data-model#subcollections"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 183,
    "url": "https://www.examtopics.com/discussions/google/view/91571-exam-professional-cloud-developer-topic-1-question-183/",
    "body": "You are developing an application that will handle requests from end users. You need to secure a Cloud Function called by the application to allow authorized end users to authenticate to the function via the application while restricting access to unauthorized users. You will integrate Google Sign-In as part of the solution and want to follow Google-recommended best practices. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy from a source code repository and grant users the roles/cloudfunctions.viewer role.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy from a source code repository and grant users the roles/cloudfunctions.invoker role\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy from your local machine using gcloud and grant users the roles/cloudfunctions.admin role",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy from your local machine using gcloud and grant users the roles/cloudfunctions.developer role"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-03T16:00:00.000Z",
        "voteCount": 1,
        "content": "B is the asnwer"
      },
      {
        "date": "2023-08-07T00:32:00.000Z",
        "voteCount": 1,
        "content": "The key here is \"secure a Cloud Function CALLED by the application to allow authorized end users to authenticate to the function via the application while restricting access to unauthorized users\""
      },
      {
        "date": "2022-12-25T05:06:00.000Z",
        "voteCount": 2,
        "content": "Answer B"
      },
      {
        "date": "2022-12-15T05:24:00.000Z",
        "voteCount": 1,
        "content": "B is the answer."
      },
      {
        "date": "2022-12-14T05:40:00.000Z",
        "voteCount": 1,
        "content": "Have the user account you are using to access Cloud Functions assigned a role that contains the cloudfunctions.functions.invoke permission. By default, the Cloud Functions Admin and Cloud Functions Developer roles have this permission. See Cloud Functions IAM Roles for the full list of roles and their associated permissions."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 184,
    "url": "https://www.examtopics.com/discussions/google/view/90814-exam-professional-cloud-developer-topic-1-question-184/",
    "body": "You are running a web application on Google Kubernetes Engine that you inherited. You want to determine whether the application is using libraries with known vulnerabilities or is vulnerable to XSS attacks. Which service should you use?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGoogle Cloud Armor",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDebugger",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWeb Security Scanner\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tError Reporting"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-12T01:10:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/security-command-center/docs/how-to-remediate-web-security-scanner-findings#xss"
      },
      {
        "date": "2024-02-03T16:03:00.000Z",
        "voteCount": 1,
        "content": "Answer is C"
      },
      {
        "date": "2023-08-07T01:01:00.000Z",
        "voteCount": 2,
        "content": "Web security scanner under gcp environment or we can use GRYPE for vulnerability scanning in on premise networks."
      },
      {
        "date": "2022-12-20T06:35:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/security-command-center/docs/how-to-remediate-web-security-scanner-findings\n\nAnswer C"
      },
      {
        "date": "2022-12-15T05:22:00.000Z",
        "voteCount": 2,
        "content": "C is the answer.\n\nhttps://cloud.google.com/security-command-center/docs/concepts-web-security-scanner-overview\nWeb Security Scanner identifies security vulnerabilities in your App Engine, Google Kubernetes Engine (GKE), and Compute Engine web applications. It crawls your application, following all links within the scope of your starting URLs, and attempts to exercise as many user inputs and event handlers as possible."
      },
      {
        "date": "2022-12-09T07:53:00.000Z",
        "voteCount": 1,
        "content": "C is correct \n\nhttps://cloud.google.com/security-command-center/docs/concepts-web-security-scanner-overview"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 185,
    "url": "https://www.examtopics.com/discussions/google/view/91572-exam-professional-cloud-developer-topic-1-question-185/",
    "body": "You are building a highly available and globally accessible application that will serve static content to users. You need to configure the storage and serving components. You want to minimize management overhead and latency while maximizing reliability for users. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a managed instance group. Replicate the static content across the virtual machines (VMs)<br>2. Create an external HTTP(S) load balancer.<br>3. Enable Cloud CDN, and send traffic to the managed instance group.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create an unmanaged instance group. Replicate the static content across the VMs.<br>2. Create an external HTTP(S) load balancer<br>3. Enable Cloud CDN, and send traffic to the unmanaged instance group.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a Standard storage class, regional Cloud Storage bucket. Put the static content in the bucket<br>2. Reserve an external IP address, and create an external HTTP(S) load balancer<br>3. Enable Cloud CDN, and send traffic to your backend bucket",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a Standard storage class, multi-regional Cloud Storage bucket. Put the static content in the bucket.<br>2. Reserve an external IP address, and create an external HTTP(S) load balancer.<br>3. Enable Cloud CDN, and send traffic to your backend bucket.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-12T01:21:00.000Z",
        "voteCount": 1,
        "content": "A &amp; B are out easily. \n\nwe might be tempted to got for C for lower cost, however, using a regional bucket could increase latency for users who are not located near the chosen region. A multi-regional bucket would be a better choice for a globally accessible application. So go with D"
      },
      {
        "date": "2024-02-03T16:05:00.000Z",
        "voteCount": 1,
        "content": "D is the answer."
      },
      {
        "date": "2023-09-23T04:50:00.000Z",
        "voteCount": 1,
        "content": "D is correct."
      },
      {
        "date": "2023-08-07T01:08:00.000Z",
        "voteCount": 4,
        "content": "Highly available = Multi-region Cloud Storage bucket\nGlobally accessible: Https load balancer\nApplication that will serve static content to users: Cloud CDN"
      },
      {
        "date": "2022-12-25T05:06:00.000Z",
        "voteCount": 2,
        "content": "Answer D"
      },
      {
        "date": "2022-12-15T05:20:00.000Z",
        "voteCount": 1,
        "content": "D is the answer."
      },
      {
        "date": "2022-12-14T05:43:00.000Z",
        "voteCount": 1,
        "content": "multi regional, less maintenance"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 186,
    "url": "https://www.examtopics.com/discussions/google/view/92051-exam-professional-cloud-developer-topic-1-question-186/",
    "body": "Case study -<br><br>This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.<br><br>To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.<br><br>At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.<br><br><br>To start the case study -<br>To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.<br><br><br>Company Overview -<br>HipLocal is a community application designed to facilitate communication between people in close proximity. It is used for event planning and organizing sporting events, and for businesses to connect with their local communities. HipLocal launched recently in a few neighborhoods in Dallas and is rapidly growing into a global phenomenon. Its unique style of hyper-local community communication and business outreach is in demand around the world.<br><br><br>Executive Statement -<br>We are the number one local community app; it's time to take our local community services global. Our venture capital investors want to see rapid growth and the same great experience for new local and virtual communities that come online, whether their members are 10 or 10000 miles away from each other.<br><br><br>Solution Concept -<br>HipLocal wants to expand their existing service, with updated functionality, in new regions to better serve their global customers. They want to hire and train a new team to support these regions in their time zones. They will need to ensure that the application scales smoothly and provides clear uptime data, and that they analyze and respond to any issues that occur.<br><br><br>Existing Technical Environment -<br>HipLocal's environment is a mix of on-premises hardware and infrastructure running in Google Cloud Platform. The HipLocal team understands their application well, but has limited experience in global scale applications. Their existing technical environment is as follows:<br>\u2022 Existing APIs run on Compute Engine virtual machine instances hosted in GCP.<br>\u2022 State is stored in a single instance MySQL database in GCP.<br>\u2022 Release cycles include development freezes to allow for QA testing.<br>\u2022 The application has no logging.<br>\u2022 Applications are manually deployed by infrastructure engineers during periods of slow traffic on weekday evenings.<br>\u2022 There are basic indicators of uptime; alerts are frequently fired when the APIs are unresponsive.<br><br><br>Business Requirements -<br>HipLocal's investors want to expand their footprint and support the increase in demand they are seeing. Their requirements are:<br>\u2022 Expand availability of the application to new regions.<br>\u2022 Support 10x as many concurrent users.<br>\u2022 Ensure a consistent experience for users when they travel to different regions.<br>\u2022 Obtain user activity metrics to better understand how to monetize their product.<br>\u2022 Ensure compliance with regulations in the new regions (for example, GDPR).<br>\u2022 Reduce infrastructure management time and cost.<br>\u2022 Adopt the Google-recommended practices for cloud computing.<br>\u25cb Develop standardized workflows and processes around application lifecycle management.<br>\u25cb Define service level indicators (SLIs) and service level objectives (SLOs).<br><br><br>Technical Requirements -<br>\u2022 Provide secure communications between the on-premises data center and cloud-hosted applications and infrastructure.<br>\u2022 The application must provide usage metrics and monitoring.<br>\u2022 APIs require authentication and authorization.<br>\u2022 Implement faster and more accurate validation of new features.<br>\u2022 Logging and performance metrics must provide actionable information to be able to provide debugging information and alerts.<br>\u2022 Must scale to meet user demand.<br><br><br>For this question refer to the HipLocal case study.<br><br>HipLocal wants to reduce the latency of their services for users in global locations. They have created read replicas of their database in locations where their users reside and configured their service to read traffic using those replicas. How should they further reduce latency for all database interactions with the least amount of effort?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the database to Bigtable and use it to serve all global user traffic.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the database to Cloud Spanner and use it to serve all global user traffic.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the database to Firestore in Datastore mode and use it to serve all global user traffic.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the services to Google Kubernetes Engine and use a load balancer service to better scale the application."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-03T16:09:00.000Z",
        "voteCount": 1,
        "content": "B is the asnwer."
      },
      {
        "date": "2023-09-23T04:52:00.000Z",
        "voteCount": 1,
        "content": "B is correct."
      },
      {
        "date": "2023-08-07T01:14:00.000Z",
        "voteCount": 1,
        "content": "Cloud spanner is the only best fit solution here."
      },
      {
        "date": "2022-12-21T08:37:00.000Z",
        "voteCount": 1,
        "content": "B is the answer."
      },
      {
        "date": "2022-12-20T06:19:00.000Z",
        "voteCount": 1,
        "content": "Answer B"
      },
      {
        "date": "2022-12-19T00:45:00.000Z",
        "voteCount": 4,
        "content": "While the question asks for \"least amount of effort\" ... all possible answers require a database migration. So it really boils down, to which database will be easiest to migrate to.\n\nHipLocal is using MySQL, which is a Relational database, so that rules out all options that are not relational ... leaving option B, Cloud Spanner as the only valid option.\n\nAlso, option D is completely unrelated. There is no point in migrated services to Kubernetes if you what you are after is improving the database latency. Maybe, if they put the services closer to the database it would help, but option D does not say anything about that."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 187,
    "url": "https://www.examtopics.com/discussions/google/view/89958-exam-professional-cloud-developer-topic-1-question-187/",
    "body": "Case study -<br><br>This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.<br><br>To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.<br><br>At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.<br><br><br>To start the case study -<br>To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.<br><br><br>Company Overview -<br>HipLocal is a community application designed to facilitate communication between people in close proximity. It is used for event planning and organizing sporting events, and for businesses to connect with their local communities. HipLocal launched recently in a few neighborhoods in Dallas and is rapidly growing into a global phenomenon. Its unique style of hyper-local community communication and business outreach is in demand around the world.<br><br><br>Executive Statement -<br>We are the number one local community app; it's time to take our local community services global. Our venture capital investors want to see rapid growth and the same great experience for new local and virtual communities that come online, whether their members are 10 or 10000 miles away from each other.<br><br><br>Solution Concept -<br>HipLocal wants to expand their existing service, with updated functionality, in new regions to better serve their global customers. They want to hire and train a new team to support these regions in their time zones. They will need to ensure that the application scales smoothly and provides clear uptime data, and that they analyze and respond to any issues that occur.<br><br><br>Existing Technical Environment -<br>HipLocal's environment is a mix of on-premises hardware and infrastructure running in Google Cloud Platform. The HipLocal team understands their application well, but has limited experience in global scale applications. Their existing technical environment is as follows:<br>\u2022 Existing APIs run on Compute Engine virtual machine instances hosted in GCP.<br>\u2022 State is stored in a single instance MySQL database in GCP.<br>\u2022 Release cycles include development freezes to allow for QA testing.<br>\u2022 The application has no logging.<br>\u2022 Applications are manually deployed by infrastructure engineers during periods of slow traffic on weekday evenings.<br>\u2022 There are basic indicators of uptime; alerts are frequently fired when the APIs are unresponsive.<br><br><br>Business Requirements -<br>HipLocal's investors want to expand their footprint and support the increase in demand they are seeing. Their requirements are:<br>\u2022 Expand availability of the application to new regions.<br>\u2022 Support 10x as many concurrent users.<br>\u2022 Ensure a consistent experience for users when they travel to different regions.<br>\u2022 Obtain user activity metrics to better understand how to monetize their product.<br>\u2022 Ensure compliance with regulations in the new regions (for example, GDPR).<br>\u2022 Reduce infrastructure management time and cost.<br>\u2022 Adopt the Google-recommended practices for cloud computing.<br>\u25cb Develop standardized workflows and processes around application lifecycle management.<br>\u25cb Define service level indicators (SLIs) and service level objectives (SLOs).<br><br><br>Technical Requirements -<br>\u2022 Provide secure communications between the on-premises data center and cloud-hosted applications and infrastructure.<br>\u2022 The application must provide usage metrics and monitoring.<br>\u2022 APIs require authentication and authorization.<br>\u2022 Implement faster and more accurate validation of new features.<br>\u2022 Logging and performance metrics must provide actionable information to be able to provide debugging information and alerts.<br>\u2022 Must scale to meet user demand.<br><br><br>For this question, refer to the HipLocal case study.<br><br>Which Google Cloud product addresses HipLocal\u2019s business requirements for service level indicators and objectives?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Profiler",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Monitoring\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Trace",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Logging"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-03T16:13:00.000Z",
        "voteCount": 1,
        "content": "B is the answer."
      },
      {
        "date": "2023-08-07T01:16:00.000Z",
        "voteCount": 1,
        "content": "B is correct.  Simple and straight forward."
      },
      {
        "date": "2023-01-10T00:52:00.000Z",
        "voteCount": 1,
        "content": "Answer B"
      },
      {
        "date": "2022-12-25T05:05:00.000Z",
        "voteCount": 1,
        "content": "Answer B"
      },
      {
        "date": "2022-12-21T08:34:00.000Z",
        "voteCount": 1,
        "content": "B is the answer.\n\nhttps://cloud.google.com/stackdriver/docs/solutions/slo-monitoring"
      },
      {
        "date": "2022-12-04T05:33:00.000Z",
        "voteCount": 1,
        "content": "Answer is B\nhttps://cloud.google.com/stackdriver/docs/solutions/slo-monitoring#defn-sli"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 188,
    "url": "https://www.examtopics.com/discussions/google/view/91573-exam-professional-cloud-developer-topic-1-question-188/",
    "body": "Case study -<br><br>This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.<br><br>To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.<br><br>At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.<br><br><br>To start the case study -<br>To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.<br><br><br>Company Overview -<br>HipLocal is a community application designed to facilitate communication between people in close proximity. It is used for event planning and organizing sporting events, and for businesses to connect with their local communities. HipLocal launched recently in a few neighborhoods in Dallas and is rapidly growing into a global phenomenon. Its unique style of hyper-local community communication and business outreach is in demand around the world.<br><br><br>Executive Statement -<br>We are the number one local community app; it's time to take our local community services global. Our venture capital investors want to see rapid growth and the same great experience for new local and virtual communities that come online, whether their members are 10 or 10000 miles away from each other.<br><br><br>Solution Concept -<br>HipLocal wants to expand their existing service, with updated functionality, in new regions to better serve their global customers. They want to hire and train a new team to support these regions in their time zones. They will need to ensure that the application scales smoothly and provides clear uptime data, and that they analyze and respond to any issues that occur.<br><br><br>Existing Technical Environment -<br>HipLocal's environment is a mix of on-premises hardware and infrastructure running in Google Cloud Platform. The HipLocal team understands their application well, but has limited experience in global scale applications. Their existing technical environment is as follows:<br>\u2022 Existing APIs run on Compute Engine virtual machine instances hosted in GCP.<br>\u2022 State is stored in a single instance MySQL database in GCP.<br>\u2022 Release cycles include development freezes to allow for QA testing.<br>\u2022 The application has no logging.<br>\u2022 Applications are manually deployed by infrastructure engineers during periods of slow traffic on weekday evenings.<br>\u2022 There are basic indicators of uptime; alerts are frequently fired when the APIs are unresponsive.<br><br><br>Business Requirements -<br>HipLocal's investors want to expand their footprint and support the increase in demand they are seeing. Their requirements are:<br>\u2022 Expand availability of the application to new regions.<br>\u2022 Support 10x as many concurrent users.<br>\u2022 Ensure a consistent experience for users when they travel to different regions.<br>\u2022 Obtain user activity metrics to better understand how to monetize their product.<br>\u2022 Ensure compliance with regulations in the new regions (for example, GDPR).<br>\u2022 Reduce infrastructure management time and cost.<br>\u2022 Adopt the Google-recommended practices for cloud computing.<br>\u25cb Develop standardized workflows and processes around application lifecycle management.<br>\u25cb Define service level indicators (SLIs) and service level objectives (SLOs).<br><br><br>Technical Requirements -<br>\u2022 Provide secure communications between the on-premises data center and cloud-hosted applications and infrastructure.<br>\u2022 The application must provide usage metrics and monitoring.<br>\u2022 APIs require authentication and authorization.<br>\u2022 Implement faster and more accurate validation of new features.<br>\u2022 Logging and performance metrics must provide actionable information to be able to provide debugging information and alerts.<br>\u2022 Must scale to meet user demand.<br><br><br>For this question, refer to the HipLocal case study.<br><br>A recent security audit discovers that HipLocal\u2019s database credentials for their Compute Engine-hosted MySQL databases are stored in plain text on persistent disks. HipLocal needs to reduce the risk of these credentials being stolen. What should they do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a service account and download its key. Use the key to authenticate to Cloud Key Management Service (KMS) to obtain the database credentials.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a service account and download its key. Use the key to authenticate to Cloud Key Management Service (KMS) to obtain a key used to decrypt the database credentials.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a service account and grant it the roles/iam.serviceAccountUser role. Impersonate as this account and authenticate using the Cloud SQL Proxy.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGrant the roles/secretmanager.secretAccessor role to the Compute Engine service account. Store and access the database credentials with the Secret Manager API.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-03T16:14:00.000Z",
        "voteCount": 1,
        "content": "D is the answer."
      },
      {
        "date": "2023-09-23T05:22:00.000Z",
        "voteCount": 1,
        "content": "Secret Manager is a service that helps you manage and protect your secrets. You can store secrets such as passwords, API keys, and SSH keys in Secret Manager. Secret Manager encrypts your secrets using Google-managed encryption keys, and it provides you with a number of features to help you manage and protect your secrets."
      },
      {
        "date": "2023-08-07T01:25:00.000Z",
        "voteCount": 1,
        "content": "D is the best option here."
      },
      {
        "date": "2022-12-21T08:31:00.000Z",
        "voteCount": 1,
        "content": "D is the answer."
      },
      {
        "date": "2022-12-20T06:16:00.000Z",
        "voteCount": 1,
        "content": "Answer D\nhttps://cloud.google.com/secret-manager/docs/best-practices"
      },
      {
        "date": "2022-12-19T00:52:00.000Z",
        "voteCount": 1,
        "content": "Both A, and B go against best practices that say you should try avoiding service account keys. Plus, these two answers would still store the service account key in the VM.\n\nOption C is completely irrelevant as it does not address the issue at hand, which is plain text credentials stored on disk.\n\nThis leaves option D as the valid option."
      },
      {
        "date": "2022-12-19T00:53:00.000Z",
        "voteCount": 1,
        "content": "Correction: \"avoid downloading service account keys\""
      },
      {
        "date": "2022-12-14T05:50:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/secret-manager/docs/overview"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 189,
    "url": "https://www.examtopics.com/discussions/google/view/92053-exam-professional-cloud-developer-topic-1-question-189/",
    "body": "Case study -<br><br>This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.<br><br>To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.<br><br>At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.<br><br><br>To start the case study -<br>To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.<br><br><br>Company Overview -<br>HipLocal is a community application designed to facilitate communication between people in close proximity. It is used for event planning and organizing sporting events, and for businesses to connect with their local communities. HipLocal launched recently in a few neighborhoods in Dallas and is rapidly growing into a global phenomenon. Its unique style of hyper-local community communication and business outreach is in demand around the world.<br><br><br>Executive Statement -<br>We are the number one local community app; it's time to take our local community services global. Our venture capital investors want to see rapid growth and the same great experience for new local and virtual communities that come online, whether their members are 10 or 10000 miles away from each other.<br><br><br>Solution Concept -<br>HipLocal wants to expand their existing service, with updated functionality, in new regions to better serve their global customers. They want to hire and train a new team to support these regions in their time zones. They will need to ensure that the application scales smoothly and provides clear uptime data, and that they analyze and respond to any issues that occur.<br><br><br>Existing Technical Environment -<br>HipLocal's environment is a mix of on-premises hardware and infrastructure running in Google Cloud Platform. The HipLocal team understands their application well, but has limited experience in global scale applications. Their existing technical environment is as follows:<br>\u2022 Existing APIs run on Compute Engine virtual machine instances hosted in GCP.<br>\u2022 State is stored in a single instance MySQL database in GCP.<br>\u2022 Release cycles include development freezes to allow for QA testing.<br>\u2022 The application has no logging.<br>\u2022 Applications are manually deployed by infrastructure engineers during periods of slow traffic on weekday evenings.<br>\u2022 There are basic indicators of uptime; alerts are frequently fired when the APIs are unresponsive.<br><br><br>Business Requirements -<br>HipLocal's investors want to expand their footprint and support the increase in demand they are seeing. Their requirements are:<br>\u2022 Expand availability of the application to new regions.<br>\u2022 Support 10x as many concurrent users.<br>\u2022 Ensure a consistent experience for users when they travel to different regions.<br>\u2022 Obtain user activity metrics to better understand how to monetize their product.<br>\u2022 Ensure compliance with regulations in the new regions (for example, GDPR).<br>\u2022 Reduce infrastructure management time and cost.<br>\u2022 Adopt the Google-recommended practices for cloud computing.<br>\u25cb Develop standardized workflows and processes around application lifecycle management.<br>\u25cb Define service level indicators (SLIs) and service level objectives (SLOs).<br><br><br>Technical Requirements -<br>\u2022 Provide secure communications between the on-premises data center and cloud-hosted applications and infrastructure.<br>\u2022 The application must provide usage metrics and monitoring.<br>\u2022 APIs require authentication and authorization.<br>\u2022 Implement faster and more accurate validation of new features.<br>\u2022 Logging and performance metrics must provide actionable information to be able to provide debugging information and alerts.<br>\u2022 Must scale to meet user demand.<br><br><br>For this question, refer to the HipLocal case study.<br><br>HipLocal is expanding into new locations. They must capture additional data each time the application is launched in a new European country. This is causing delays in the development process due to constant schema changes and a lack of environments for conducting testing on the application changes. How should they resolve the issue while meeting the business requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate new Cloud SQL instances in Europe and North America for testing and deployment. Provide developers with local MySQL instances to conduct testing on the application changes.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate data to Bigtable. Instruct the development teams to use the Cloud SDK to emulate a local Bigtable development environment.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMove from Cloud SQL to MySQL hosted on Compute Engine. Replicate hosts across regions in the Americas and Europe. Provide developers with local MySQL instances to conduct testing on the application changes.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate data to Firestore in Native mode and set up instances in Europe and North America. Instruct the development teams to use the Cloud SDK to emulate a local Firestore in Native mode development environment.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-08-07T01:29:00.000Z",
        "voteCount": 1,
        "content": "Schema changes -&gt; Firestore document database is a best fit."
      },
      {
        "date": "2022-12-21T08:29:00.000Z",
        "voteCount": 2,
        "content": "D is the answer."
      },
      {
        "date": "2022-12-19T01:28:00.000Z",
        "voteCount": 4,
        "content": "Looks like Option D satisfies the most requirements .\n\n1. It's a Document store, without strict schema enforcement so it's good for \"constant schema changes\"\n2. You can setup separate instances in NA, and EU to satisfy GDRP\n3. Dev teams can emulate Firestore locally for testing.\n4. It's a managed service, so reduces infra management time and cost\n\n\nOption C is a non-starter, as it's moving from managed service to non-managed, also it replicates data between EU and NA, which is against GDPR\n\nOption B could work, but Bigtable is an overkill for HipLocal, it costs more than Firestore.\n\nOption A does not reduce infrastructure management, as they need to provide local MySQL instances for developers."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 190,
    "url": "https://www.examtopics.com/discussions/google/view/91213-exam-professional-cloud-developer-topic-1-question-190/",
    "body": "You are writing from a Go application to a Cloud Spanner database. You want to optimize your application\u2019s performance using Google-recommended best practices. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite to Cloud Spanner using Cloud Client Libraries.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite to Cloud Spanner using Google API Client Libraries",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite to Cloud Spanner using a custom gRPC client library.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite to Cloud Spanner using a third-party HTTP client library."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 12,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-12T01:37:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/spanner/docs/reference/libraries#client-libraries-install-go:~:text=Although%20you%20can%20use%20Google%20Cloud%20APIs%20directly%20by%20making%20raw%20requests%20to%20the%20server%2C%20client%20libraries%20provide%20simplifications%20that%20significantly%20reduce%20the%20amount%20of%20code%20you%20need%20to%20write"
      },
      {
        "date": "2024-02-03T16:42:00.000Z",
        "voteCount": 1,
        "content": "Ais the answer."
      },
      {
        "date": "2023-11-11T03:00:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2023-09-23T05:31:00.000Z",
        "voteCount": 1,
        "content": "A is corrrect."
      },
      {
        "date": "2023-08-07T01:33:00.000Z",
        "voteCount": 2,
        "content": "We should know what cloud client libraries are.  Pl refer to https://cloud.google.com/apis/docs/cloud-client-libraries"
      },
      {
        "date": "2022-12-25T05:05:00.000Z",
        "voteCount": 1,
        "content": "Answer A"
      },
      {
        "date": "2022-12-19T01:31:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/apis/docs/cloud-client-libraries"
      },
      {
        "date": "2022-12-13T21:46:00.000Z",
        "voteCount": 1,
        "content": "option A"
      },
      {
        "date": "2022-12-13T05:27:00.000Z",
        "voteCount": 2,
        "content": "A is the answer.\n\nhttps://cloud.google.com/spanner/docs/reference/libraries"
      },
      {
        "date": "2022-12-12T07:09:00.000Z",
        "voteCount": 2,
        "content": "A is correct\nBC are part of A\nD idk\n\n\u201cCloud Client Libraries are the recommended option for accessing Cloud APIs programmatically, where available. Cloud Client Libraries use the latest client library models\u201d\n\nhttps://cloud.google.com/apis/docs/client-libraries-explained\nhttps://cloud.google.com/go/docs/reference"
      },
      {
        "date": "2023-05-01T19:28:00.000Z",
        "voteCount": 1,
        "content": "https://developers.google.com/api-client-library\nGoogle APIs give you programmatic access to Google Maps, Google Drive, YouTube, and many other Google products.\n\u2234B is wrong."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 191,
    "url": "https://www.examtopics.com/discussions/google/view/91409-exam-professional-cloud-developer-topic-1-question-191/",
    "body": "You have an application deployed in Google Kubernetes Engine (GKE). You need to update the application to make authorized requests to Google Cloud managed services. You want this to be a one-time setup, and you need to follow security best practices of auto-rotating your security keys and storing them in an encrypted store. You already created a service account with appropriate access to the Google Cloud service. What should you do next?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAssign the Google Cloud service account to your GKE Pod using Workload Identity.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExport the Google Cloud service account, and share it with the Pod as a Kubernetes Secret.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExport the Google Cloud service account, and embed it in the source code of the application.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExport the Google Cloud service account, and upload it to HashiCorp Vault to generate a dynamic service account for your application."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-09-23T05:33:00.000Z",
        "voteCount": 1,
        "content": "A is correct."
      },
      {
        "date": "2023-08-07T06:22:00.000Z",
        "voteCount": 2,
        "content": "Service account with proper IAM configurations already exists.\nWe should link the existing service account with GKE pod as workload identity.  This applies to all the pods running within a cluster."
      },
      {
        "date": "2023-05-01T19:37:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/iam/docs/best-practices-service-accounts#use-workload-identity"
      },
      {
        "date": "2023-02-26T08:15:00.000Z",
        "voteCount": 1,
        "content": "workload identity"
      },
      {
        "date": "2022-12-25T05:04:00.000Z",
        "voteCount": 1,
        "content": "Answer A"
      },
      {
        "date": "2022-12-13T21:47:00.000Z",
        "voteCount": 1,
        "content": "option A"
      },
      {
        "date": "2022-12-13T05:23:00.000Z",
        "voteCount": 1,
        "content": "A is the answer.\n\nhttps://cloud.google.com/kubernetes-engine/docs/concepts/workload-identity\nApplications running on GKE might need access to Google Cloud APIs such as Compute Engine API, BigQuery Storage API, or Machine Learning APIs.\n\nWorkload Identity allows a Kubernetes service account in your GKE cluster to act as an IAM service account. Pods that use the configured Kubernetes service account automatically authenticate as the IAM service account when accessing Google Cloud APIs. Using Workload Identity allows you to assign distinct, fine-grained identities and authorization for each application in your cluster."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 192,
    "url": "https://www.examtopics.com/discussions/google/view/91408-exam-professional-cloud-developer-topic-1-question-192/",
    "body": "You are planning to deploy hundreds of microservices in your Google Kubernetes Engine (GKE) cluster. How should you secure communication between the microservices on GKE using a managed service?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse global HTTP(S) Load Balancing with managed SSL certificates to protect your services",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy open source Istio in your GKE cluster, and enable mTLS in your Service Mesh",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall cert-manager on GKE to automatically renew the SSL certificates.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall Anthos Service Mesh, and enable mTLS in your Service Mesh.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 13,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-02-03T16:46:00.000Z",
        "voteCount": 2,
        "content": "Answer is D."
      },
      {
        "date": "2023-12-29T19:43:00.000Z",
        "voteCount": 1,
        "content": "Istio on GKE cluster has deprecated."
      },
      {
        "date": "2023-09-23T05:42:00.000Z",
        "voteCount": 1,
        "content": "I will go with B."
      },
      {
        "date": "2023-08-07T06:25:00.000Z",
        "voteCount": 1,
        "content": "Initially I thought B can be the option. Later realized that directly using Istio on GKE cluster is deprecated.  Hence going for D."
      },
      {
        "date": "2023-01-27T06:42:00.000Z",
        "voteCount": 1,
        "content": "Google Cloud provides a service called Istio on GKE, that simplifies the management, scaling and automatic upgrades of Istio on GKE clusters, giving you the flexibility of Istio with the ease of a managed service.\nAnthos Service Mesh is a service mesh built on top of Istio, and is designed to be used in conjunction with Google Cloud's Anthos platform. It provides many of the same features as Istio, but it also includes some additional features that are specific to Anthos, such as support for hybrid and multi-cloud environments."
      },
      {
        "date": "2023-02-05T15:10:00.000Z",
        "voteCount": 4,
        "content": "Warning: Istio on GKE is deprecated. After December 31, 2021, the UI no longer supports this feature during the creation of new clusters. After September 30, 2022, Istio on GKE will no longer be supported in existing clusters. You can migrate Istio on GKE to Anthos Service Mesh to continue using your service meshes. For more information, see the migration FAQ."
      },
      {
        "date": "2023-02-08T06:20:00.000Z",
        "voteCount": 3,
        "content": "Changing to the D option"
      },
      {
        "date": "2023-01-04T04:51:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/architecture/service-meshes-in-microservices-architecture#security_2\nhttps://cloud.google.com/architecture/service-meshes-in-microservices-architecture#security_2"
      },
      {
        "date": "2022-12-25T05:04:00.000Z",
        "voteCount": 2,
        "content": "Answer D"
      },
      {
        "date": "2022-12-13T21:50:00.000Z",
        "voteCount": 1,
        "content": "option D"
      },
      {
        "date": "2022-12-13T05:21:00.000Z",
        "voteCount": 3,
        "content": "D is the answer.\n\nhttps://cloud.google.com/service-mesh/docs/overview#security_benefits\n- Ensures encryption in transit. Using mTLS for authentication also ensures that all TCP communications are encrypted in transit."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 193,
    "url": "https://www.examtopics.com/discussions/google/view/91407-exam-professional-cloud-developer-topic-1-question-193/",
    "body": "You are developing an application that will store and access sensitive unstructured data objects in a Cloud Storage bucket. To comply with regulatory requirements, you need to ensure that all data objects are available for at least 7 years after their initial creation. Objects created more than 3 years ago are accessed very infrequently (less than once a year). You need to configure object storage while ensuring that storage cost is optimized. What should you do? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet a retention policy on the bucket with a period of 7 years.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse IAM Conditions to provide access to objects 7 years after the object creation date.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Object Versioning to prevent objects from being accidentally deleted for 7 years after object creation.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an object lifecycle policy on the bucket that moves objects from Standard Storage to Archive Storage after 3 years.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement a Cloud Function that checks the age of each object in the bucket and moves the objects older than 3 years to a second bucket with the Archive Storage class. Use Cloud Scheduler to trigger the Cloud Function on a daily schedule."
    ],
    "answer": "AD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AD",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-09-23T05:46:00.000Z",
        "voteCount": 1,
        "content": "AD is correct."
      },
      {
        "date": "2023-08-07T06:28:00.000Z",
        "voteCount": 1,
        "content": "Keys:\n1) all data objects are available for at least 7 years after their initial creation : A. Set a retention policy on the bucket with a period of 7 years\n2) Objects created more than 3 years ago are accessed very infrequently (less than once a year) : D. Create an object lifecycle policy on the bucket that moves objects from Standard Storage to Archive Storage after 3 years."
      },
      {
        "date": "2023-02-20T07:48:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/storage/docs/bucket-lock\nhttps://cloud.google.com/storage/docs/lifecycle"
      },
      {
        "date": "2023-01-04T03:48:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/storage/docs/bucket-lock\nhttps://cloud.google.com/storage/docs/lifecycle\nAnswer A,D"
      },
      {
        "date": "2022-12-15T08:26:00.000Z",
        "voteCount": 1,
        "content": "A is correct because Cloud Storage provides an option to configure a retention lifecycle rule.\nB is incorrect because it is not a recommended way to implement data retention requirements.\nC is incorrect because it does not guarantee that objects are not deleted within 7 years after object creation.\nD is correct because it\u2019s the easiest and recommended way to implement a storage lifecycle policy to move objects from Standard to Archive tier.\nE is incorrect because you do not require two buckets to store objects on two storage tiers."
      },
      {
        "date": "2022-12-13T21:51:00.000Z",
        "voteCount": 1,
        "content": "option AD"
      },
      {
        "date": "2022-12-13T05:18:00.000Z",
        "voteCount": 1,
        "content": "AD is the answer.\n\nhttps://cloud.google.com/storage/docs/bucket-lock\nThis page discusses the Bucket Lock feature, which allows you to configure a data retention policy for a Cloud Storage bucket that governs how long objects in the bucket must be retained. The feature also allows you to lock the data retention policy, permanently preventing the policy from being reduced or removed.\n\nhttps://cloud.google.com/storage/docs/storage-classes#archive\nArchive storage is the lowest-cost, highly durable storage service for data archiving, online backup, and disaster recovery. Unlike the \"coldest\" storage services offered by other Cloud providers, your data is available within milliseconds, not hours or days.\n\nArchive storage is the best choice for data that you plan to access less than once a year."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 194,
    "url": "https://www.examtopics.com/discussions/google/view/91406-exam-professional-cloud-developer-topic-1-question-194/",
    "body": "You are developing an application using different microservices that must remain internal to the cluster. You want the ability to configure each microservice with a specific number of replicas. You also want the ability to address a specific microservice from any other microservice in a uniform way, regardless of the number of replicas the microservice scales to. You plan to implement this solution on Google Kubernetes Engine. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy each microservice as a Deployment. Expose the Deployment in the cluster using a Service, and use the Service DNS name to address it from other microservices within the cluster.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy each microservice as a Deployment. Expose the Deployment in the cluster using an Ingress, and use the Ingress IP address to address the Deployment from other microservices within the cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy each microservice as a Pod. Expose the Pod in the cluster using a Service, and use the Service DNS name to address the microservice from other microservices within the cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy each microservice as a Pod. Expose the Pod in the cluster using an Ingress, and use the Ingress IP address to address the Pod from other microservices within the cluster."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 17,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-12T02:13:00.000Z",
        "voteCount": 2,
        "content": "\"configure each microservice with a specific number of replicas\"\n-- Deployment\n\n\"address a specific microservice from any other microservice in a uniform way, regardless of the number of replicas the microservice scales to\"\n-- Service"
      },
      {
        "date": "2024-02-03T16:56:00.000Z",
        "voteCount": 2,
        "content": "Answer is A."
      },
      {
        "date": "2023-10-23T05:07:00.000Z",
        "voteCount": 2,
        "content": "A is correct."
      },
      {
        "date": "2023-09-23T05:49:00.000Z",
        "voteCount": 1,
        "content": "A is correct."
      },
      {
        "date": "2023-08-07T06:33:00.000Z",
        "voteCount": 2,
        "content": "Option A is very simple and straight forward answer.\nWe need to have deployment descriptor with apiversion, kind, metadata, spec details.\nWe can mention the replica set inside spec.\nAll pods must communicate with each other using service DNS names -&gt; DNS is built in for GKE cluster."
      },
      {
        "date": "2023-01-13T05:52:00.000Z",
        "voteCount": 4,
        "content": "A Is correct because the Service will have a DNS entry inside the cluster that other microservices can use to address the pods of the Deployment that the Service is targetting.\nB Is not correct because an Ingress exposes a Service using an external or internal HTTP(s) load balancer, and it does not apply directly to a Deployment.\nC is not correct because a Pod is a single instance of the microservice, whereas a Deployment can be configured with a number of replicas.\nD is not correct because it combines the mistakes of options B and C."
      },
      {
        "date": "2023-01-04T03:45:00.000Z",
        "voteCount": 1,
        "content": "Answer A"
      },
      {
        "date": "2022-12-13T21:54:00.000Z",
        "voteCount": 1,
        "content": "option A"
      },
      {
        "date": "2022-12-13T05:15:00.000Z",
        "voteCount": 2,
        "content": "A is the answer."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 195,
    "url": "https://www.examtopics.com/discussions/google/view/91405-exam-professional-cloud-developer-topic-1-question-195/",
    "body": "You are building an application that uses a distributed microservices architecture. You want to measure the performance and system resource utilization in one of the microservices written in Java. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstrument the service with Cloud Profiler to measure CPU utilization and method-level execution times in the service.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstrument the service with Debugger to investigate service errors.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstrument the service with Cloud Trace to measure request latency.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstrument the service with OpenCensus to measure service latency, and write custom metrics to Cloud Monitoring."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-12T02:33:00.000Z",
        "voteCount": 1,
        "content": "Measure resource utilization? \n-- Cloud profiler comes to mind\nhttps://cloud.google.com/profiler/docs/profiling-java"
      },
      {
        "date": "2023-09-23T05:53:00.000Z",
        "voteCount": 1,
        "content": "A is correct."
      },
      {
        "date": "2023-08-07T06:35:00.000Z",
        "voteCount": 1,
        "content": "This is very similar to using JProfiler.  A is correct."
      },
      {
        "date": "2023-06-26T17:40:00.000Z",
        "voteCount": 1,
        "content": "A, use profiler"
      },
      {
        "date": "2023-01-04T03:41:00.000Z",
        "voteCount": 4,
        "content": "Answer A\nhttps://cloud.google.com/profiler/docs/profiling-java\nhttps://cloud.google.com/appengine/docs/legacy/standard/java/microservice-performance"
      },
      {
        "date": "2022-12-13T22:10:00.000Z",
        "voteCount": 1,
        "content": "A.\nhttps://cloud.google.com/profiler/docs"
      },
      {
        "date": "2022-12-13T05:09:00.000Z",
        "voteCount": 1,
        "content": "A is the answer.\n\nhttps://cloud.google.com/profiler/docs/profiling-java"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 196,
    "url": "https://www.examtopics.com/discussions/google/view/91109-exam-professional-cloud-developer-topic-1-question-196/",
    "body": "Your team is responsible for maintaining an application that aggregates news articles from many different sources. Your monitoring dashboard contains publicly accessible real-time reports and runs on a Compute Engine instance as a web application. External stakeholders and analysts need to access these reports via a secure channel without authentication. How should you configure this secure channel?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a public IP address to the instance. Use the service account key of the instance to encrypt the traffic.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Scheduler to trigger Cloud Build every hour to create an export from the reports. Store the reports in a public Cloud Storage bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd an HTTP(S) load balancer in front of the monitoring dashboard. Configure Identity-Aware Proxy to secure the communication channel.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd an HTTP(S) load balancer in front of the monitoring dashboard. Set up a Google-managed SSL certificate on the load balancer for traffic encryption.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 15,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-10-23T05:45:00.000Z",
        "voteCount": 2,
        "content": "D. Add an HTTP(S) load balancer in front of the monitoring dashboard. Set up a Google-managed SSL certificate on the load balancer for traffic encryption.\n\nThis option provides the most secure way to configure a publicly accessible channel for your monitoring dashboard without authentication. The HTTP(S) load balancer will distribute traffic to the backend instances of the dashboard, and the Google-managed SSL certificate will encrypt all traffic between the load balancer and the users."
      },
      {
        "date": "2023-09-23T06:08:00.000Z",
        "voteCount": 2,
        "content": "This approach is the most secure and reliable way to configure a secure channel for external stakeholders and analysts to access the publicly accessible real-time reports in your monitoring dashboard."
      },
      {
        "date": "2023-08-07T06:39:00.000Z",
        "voteCount": 3,
        "content": "SSL/TLS is must for data in transit encryption.  Since without authentication, D is more suitable option.  If authentication required, then we could have chosen C."
      },
      {
        "date": "2022-12-20T05:09:00.000Z",
        "voteCount": 1,
        "content": "Answer D\nhttps://cloud.google.com/load-balancing/docs/ssl-certificates/google-managed-certs"
      },
      {
        "date": "2022-12-15T08:18:00.000Z",
        "voteCount": 4,
        "content": "A is incorrect. A service account cannot be used to encrypt HTTPS traffic.\nB is incorrect. Periodical export would not meet the real-time requirement.\nC is incorrect. IAP is not securing the communication channel, it authenticates the user. Technically Cloud Load Balancing already secures the channel but without an appropriate certificate.\nD is correct. This provides an external HTTPS endpoint, and uses Google-managed services and a valid SSL certificate."
      },
      {
        "date": "2022-12-13T22:14:00.000Z",
        "voteCount": 1,
        "content": "option D"
      },
      {
        "date": "2022-12-13T05:07:00.000Z",
        "voteCount": 2,
        "content": "D is the answer."
      },
      {
        "date": "2022-12-11T21:41:00.000Z",
        "voteCount": 1,
        "content": "D is correct. This provides an external HTTPS endpoint, and uses Google-managed services and a valid SSL certificate.\n\nhttps://cloud.google.com/load-balancing/docs/ssl-certificates/google-managed-certs"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 197,
    "url": "https://www.examtopics.com/discussions/google/view/91404-exam-professional-cloud-developer-topic-1-question-197/",
    "body": "You are planning to add unit tests to your application. You need to be able to assert that published Pub/Sub messages are processed by your subscriber in order. You want the unit tests to be cost-effective and reliable. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement a mocking framework.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a topic and subscription for each tester.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a filter by tester to the subscription.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Pub/Sub emulator.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 9,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-09-23T06:16:00.000Z",
        "voteCount": 1,
        "content": "D is correct."
      },
      {
        "date": "2023-01-28T02:51:00.000Z",
        "voteCount": 2,
        "content": "Option B, creating a topic and subscription for each tester, would be costly and time-consuming as it would require creating and managing a large number of topics and subscriptions. Additionally, it would not ensure that messages are processed in order, as messages may be delivered out of order to different subscriptions.\n\nOption D, using the Pub/Sub emulator, would be cost-effective and reliable as it allows you to test your application's Pub/Sub functionality locally without incurring any costs. Additionally, the emulator allows you to easily assert that messages are processed in order by using the same topic and subscription for all unit tests."
      },
      {
        "date": "2023-01-11T01:25:00.000Z",
        "voteCount": 1,
        "content": "Agree with D. They want unit test cost-effective and reliable so need an emulator which will never have an issue to do that.\nB is not correct for me because the unit test using a real topic and subscription can have issue sometimes and it's not cost-effective to pay for each tester a subscription. The b is more a solution for integration test."
      },
      {
        "date": "2022-12-20T05:06:00.000Z",
        "voteCount": 1,
        "content": "https://brightsec.com/blog/unit-testing-best-practices/\nOne scenario per test"
      },
      {
        "date": "2022-12-19T01:50:00.000Z",
        "voteCount": 2,
        "content": "Agree with D, use the emulator for testing."
      },
      {
        "date": "2022-12-15T08:08:00.000Z",
        "voteCount": 3,
        "content": "The answer is D. See https://cloud.google.com/pubsub/docs/emulator, \"Testing apps locally with the emulator\"."
      },
      {
        "date": "2022-12-13T22:16:00.000Z",
        "voteCount": 1,
        "content": "option B"
      },
      {
        "date": "2022-12-13T05:04:00.000Z",
        "voteCount": 1,
        "content": "B is the answer."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 198,
    "url": "https://www.examtopics.com/discussions/google/view/91403-exam-professional-cloud-developer-topic-1-question-198/",
    "body": "You have an application deployed in Google Kubernetes Engine (GKE) that reads and processes Pub/Sub messages. Each Pod handles a fixed number of messages per minute. The rate at which messages are published to the Pub/Sub topic varies considerably throughout the day and week, including occasional large batches of messages published at a single moment.<br><br>You want to scale your GKE Deployment to be able to process messages in a timely manner. What GKE feature should you use to automatically adapt your workload?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tVertical Pod Autoscaler in Auto mode",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tVertical Pod Autoscaler in Recommendation mode",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tHorizontal Pod Autoscaler based on an external metric\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tHorizontal Pod Autoscaler based on resources utilization"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 11,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-04-13T01:19:00.000Z",
        "voteCount": 1,
        "content": "from documentation:\n\n\"If you need to scale your workload based on the performance of an application or service outside of Kubernetes, you can configure an external metric. For example, you might need to increase the capacity of your application to ingest messages from Pub/Sub if the number of undelivered messages is trending upward.\"\n\nhttps://cloud.google.com/kubernetes-engine/docs/concepts/custom-and-external-metrics#external-metrics"
      },
      {
        "date": "2024-02-03T17:26:00.000Z",
        "voteCount": 1,
        "content": "Answer is C"
      },
      {
        "date": "2023-08-07T06:47:00.000Z",
        "voteCount": 1,
        "content": "I go with C since we need to scale GKE Deployment to be able to process messages in a TIMELY manner.  External metrics is more suitable for this."
      },
      {
        "date": "2023-02-28T06:12:00.000Z",
        "voteCount": 2,
        "content": "C: https://cloud.google.com/kubernetes-engine/docs/tutorials/autoscaling-metrics#pubsub"
      },
      {
        "date": "2023-01-28T02:57:00.000Z",
        "voteCount": 1,
        "content": "Based on the requirement that the application reads and processes Pub/Sub messages, and that the rate at which messages are published to the Pub/Sub topic varies considerably throughout the day and week, including occasional large batches of messages published at a single moment, the best choice would be C: Horizontal Pod Autoscaler based on an external metric.\n\nBy using an external metric, the Horizontal Pod Autoscaler can monitor the number of messages in the Pub/Sub topic and adjust the number of replicas in the GKE Deployment accordingly. This allows the application to automatically adapt to changes in the rate at which messages are being published, ensuring that the pods are able to process messages in a timely manner.\n\nOn the other hand, Horizontal Pod Autoscaler based on resources utilization, it would not provide the needed functionality as it bases scaling on resource usage of the pods, not the number of messages in the Pub/Sub topic"
      },
      {
        "date": "2023-01-11T01:29:00.000Z",
        "voteCount": 1,
        "content": "The answer is C for me. \nThe need to scale is from the number of messages in the pub/sub. It's an external metrics that can be reported from pub/sub or cloud monitoring. So the scale will be better to use this metrics to add X pod (each pod is limited on the number of message per minutes so the system now how much pod you need to scale to answer at X new messages."
      },
      {
        "date": "2023-01-04T03:33:00.000Z",
        "voteCount": 4,
        "content": "Custom and external metrics allow workloads to adapt to conditions besides the workload itself. Consider an application that pulls tasks from a queue and completes them. \nAn external metric is reported from an application or service not running on your cluster, but whose performance impacts your Kubernetes application. For information, the metric could be reported from Cloud Monitoring or Pub/Sub. D isn't the answer, before selecting an answer , please do a thorough research and understand concepts and the key words in a question, D cant be the answer in this case.\nhttps://cloud.google.com/kubernetes-engine/docs/concepts/custom-and-external-metrics"
      },
      {
        "date": "2022-12-22T08:09:00.000Z",
        "voteCount": 2,
        "content": "The answer is C. Each pod will handle a fixed number of messages, fixed being the key word here. Now let's say this fixed number is \"1000\" messages per minute. Do you think a 1000 messages in a minute will cause the pod autoscaler to kick in based on resource utilisation?\n\nWe need to scale using external metrics here. When Pod 1 is handling the maximum of \"fixed number amount\" messages, we need to spin up pod 2 etc..."
      },
      {
        "date": "2022-12-13T22:20:00.000Z",
        "voteCount": 1,
        "content": "option D"
      },
      {
        "date": "2022-12-13T05:01:00.000Z",
        "voteCount": 1,
        "content": "D is the answer.\n\nhttps://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
      },
      {
        "date": "2023-01-04T03:34:00.000Z",
        "voteCount": 1,
        "content": "Wrong, answer C\nhttps://cloud.google.com/kubernetes-engine/docs/concepts/custom-and-external-metrics, the key words are external metrics and pub/sub, read the requirements of the question"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 199,
    "url": "https://www.examtopics.com/discussions/google/view/91110-exam-professional-cloud-developer-topic-1-question-199/",
    "body": "You are using Cloud Run to host a web application. You need to securely obtain the application project ID and region where the application is running and display this information to users. You want to use the most performant approach. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse HTTP requests to query the available metadata server at the http://metadata.google.internal/ endpoint with the Metadata-Flavor: Google header.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the Google Cloud console, navigate to the Project Dashboard and gather configuration details. Navigate to the Cloud Run \u201cVariables &amp; Secrets\u201d tab, and add the desired environment variables in Key:Value format.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the Google Cloud console, navigate to the Project Dashboard and gather configuration details. Write the application configuration information to Cloud Run's in-memory container filesystem.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMake an API call to the Cloud Asset Inventory API from the application and format the request to include instance metadata."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 9,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-04-13T01:22:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/run/docs/container-contract#metadata-server:~:text=Project%20ID%20of%20the%20project%20the%20Cloud%20Run%20service%20or%20job%20belongs%20to"
      },
      {
        "date": "2024-04-13T01:21:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/run/docs/container-contract#metadata-server:~:text=Project%20ID%20of%20the%20project%20the%20Cloud%20Run%20service%20or%20job%20belongs%20to"
      },
      {
        "date": "2023-09-23T06:31:00.000Z",
        "voteCount": 1,
        "content": "A is correct."
      },
      {
        "date": "2023-01-11T01:41:00.000Z",
        "voteCount": 2,
        "content": "Answer A\nhttps://cloud.google.com/run/docs/container-contract#metadata-server"
      },
      {
        "date": "2023-01-04T03:09:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/run/docs/container-contract#metadata-server\nAnswer A"
      },
      {
        "date": "2022-12-19T01:56:00.000Z",
        "voteCount": 2,
        "content": "Definitely A, it's clear in the docs.\n\nhttps://cloud.google.com/run/docs/container-contract#metadata-server"
      },
      {
        "date": "2022-12-13T22:24:00.000Z",
        "voteCount": 1,
        "content": "option A"
      },
      {
        "date": "2022-12-13T04:59:00.000Z",
        "voteCount": 1,
        "content": "A is the answer.\n\nhttps://cloud.google.com/run/docs/container-contract#metadata-server\nCloud Run container instances expose a metadata server that you can use to retrieve details about your container instance, such as the project ID, region, instance ID or service accounts.\n\nYou can access this data from the metadata server using simple HTTP requests to the http://metadata.google.internal/ endpoint with the Metadata-Flavor: Google header: no client libraries are required."
      },
      {
        "date": "2022-12-11T22:08:00.000Z",
        "voteCount": 1,
        "content": "voting B because \nis not A since thats compute metadata, if you can access  you can query project id but not the region necessarily from cloud run but gce\nis not D since you cannot query project id \nC is too manual and static\nso by discard I guess is B"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 200,
    "url": "https://www.examtopics.com/discussions/google/view/91207-exam-professional-cloud-developer-topic-1-question-200/",
    "body": "You need to deploy resources from your laptop to Google Cloud using Terraform. Resources in your Google Cloud environment must be created using a service account. Your Cloud Identity has the roles/iam.serviceAccountTokenCreator Identity and Access Management (IAM) role and the necessary permissions to deploy the resources using Terraform. You want to set up your development environment to deploy the desired resources following Google-recommended best practices. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Download the service account\u2019s key file in JSON format, and store it locally on your laptop.<br>2. Set the GOOGLE_APPLICATION_CREDENTIALS environment variable to the path of your downloaded key file.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Run the following command from a command line: gcloud config set auth/impersonate_service_account service-account-name@project.iam.gserviceacccount.com.<br>2. Set the GOOGLE_OAUTH_ACCESS_TOKEN environment variable to the value that is returned by the gcloud auth print-access-token command.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Run the following command from a command line: gcloud auth application-default login.<br>2. In the browser window that opens, authenticate using your personal credentials.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Store the service account's key file in JSON format in Hashicorp Vault.<br>2. Integrate Terraform with Vault to retrieve the key file dynamically, and authenticate to Vault using a short-lived access token."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 16,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-22T14:47:00.000Z",
        "voteCount": 5,
        "content": "A&amp;D assume that you download and store SA keys, which violates best practices, since you potentially loose control over what happens to those credentials and makes it impossible to track who actually uses the SA. D makes it even worse since it requires you to maintain you own secret management to minimize the risk.\n\nC does nothing that would give you the SA permissions you need.\n\nB follows best practices, since impersonation permissions can be managed transparently via IAM and via logs you can also see who impersonated/used the SA."
      },
      {
        "date": "2024-09-06T02:32:00.000Z",
        "voteCount": 1,
        "content": "Application Default Credentials (ADC): gcloud auth application-default login sets up the Application Default Credentials (ADC) which are a secure way to authenticate applications running on Google Cloud.\nMinimal Permissions: Your Cloud Identity already has the necessary permissions (roles/iam.serviceAccountTokenCreator) to create service account tokens and the required permissions to deploy resources using Terraform.\nSecurity Best Practices: Using ADC avoids storing the service account key file locally on your laptop, which minimizes the risk of exposure and adheres to security best practices."
      },
      {
        "date": "2024-04-13T01:26:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/docs/authentication/use-service-account-impersonation\nhttps://medium.com/bluetuple-ai/terraform-remote-state-on-gcp-d50e2f69b967"
      },
      {
        "date": "2024-03-18T02:23:00.000Z",
        "voteCount": 1,
        "content": "B is the correct Answer"
      },
      {
        "date": "2023-09-23T06:35:00.000Z",
        "voteCount": 1,
        "content": "B is correct."
      },
      {
        "date": "2023-08-07T07:10:00.000Z",
        "voteCount": 1,
        "content": "B is the best option here.\nD is more complicated.\nA &amp; C do not follow google best practices."
      },
      {
        "date": "2023-04-22T13:22:00.000Z",
        "voteCount": 1,
        "content": "B\n1. impersonation\n2. securely set up env variable that will be used by terraform to deploy"
      },
      {
        "date": "2023-04-02T10:48:00.000Z",
        "voteCount": 1,
        "content": "I think its A"
      },
      {
        "date": "2023-01-11T01:47:00.000Z",
        "voteCount": 1,
        "content": "Answer is B \nhttps://cloud.google.com/sdk/gcloud/reference/config/set#impersonate_service_account"
      },
      {
        "date": "2023-01-04T03:01:00.000Z",
        "voteCount": 4,
        "content": "https://cloud.google.com/blog/topics/developers-practitioners/using-google-cloud-service-account-impersonation-your-terraform-code\nAnswer B not D"
      },
      {
        "date": "2022-12-20T01:32:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/docs/terraform/best-practices-for-terraform#default-credhttps://cloud.google.com/docs/terraform/best-practices-for-terraform#storing-secrets\nAnswer D."
      },
      {
        "date": "2023-01-04T03:03:00.000Z",
        "voteCount": 1,
        "content": "Answer B not D"
      },
      {
        "date": "2022-12-19T02:02:00.000Z",
        "voteCount": 2,
        "content": "I think it's option B.\n\nThe question already says that you have the role for impersonating the service account.\n\nThis means that option B is a viable, as you can impersonate that service account, and get a token that has the required level of access to create resources."
      },
      {
        "date": "2022-12-12T06:45:00.000Z",
        "voteCount": 1,
        "content": "D is the answer.\n\nhttps://cloud.google.com/iam/docs/best-practices-for-managing-service-account-keys#file-system\nWhenever possible, avoid storing service account keys on a file system. If you can't avoid storing keys on disk, make sure to restrict access to the key file, configure file access auditing, and encrypt the underlying disk.\n\nhttps://cloud.google.com/iam/docs/best-practices-for-managing-service-account-keys#software-keystore\nIn situations where using a hardware-based key store isn't viable, use a software-based key store to manage service account keys. Similar to hardware-based options, a software-based key store lets users or applications use service account keys without revealing the private key. Software-based key store solutions can help you control key access in a fine-grained manner and can also ensure that each key access is logged."
      },
      {
        "date": "2023-01-06T05:09:00.000Z",
        "voteCount": 1,
        "content": "Answer B not D"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 201,
    "url": "https://www.examtopics.com/discussions/google/view/89886-exam-professional-cloud-developer-topic-1-question-201/",
    "body": "Your company uses Cloud Logging to manage large volumes of log data. You need to build a real-time log analysis architecture that pushes logs to a third-party application for processing. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Cloud Logging log export to Pub/Sub.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Cloud Logging log export to BigQuery.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Cloud Logging log export to Cloud Storage.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Cloud Function to read Cloud Logging log entries and send them to the third-party application."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 12,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 4,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-04-13T01:29:00.000Z",
        "voteCount": 1,
        "content": "A: Creating a Cloud Logging log export to Pub/Sub is the correct solution for this scenario. Pub/Sub is designed for real-time messaging and can push messages (in this case, log entries) to a third-party application for processing.\n\n\nB: While BigQuery is great for analyzing large volumes of data, it's not designed for real-time data pushing to third-party applications.\nD: Creating a Cloud Function to read log entries and send them to a third-party application could work, but it would add unnecessary complexity. Using Pub/Sub is a simpler and more efficient solution."
      },
      {
        "date": "2024-04-13T01:30:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/logging/docs/export/pubsub#integrate-thru-pubsub"
      },
      {
        "date": "2023-09-23T06:37:00.000Z",
        "voteCount": 1,
        "content": "A is correct."
      },
      {
        "date": "2023-08-07T07:17:00.000Z",
        "voteCount": 2,
        "content": "My answer is A.\nThird party service is the one responsible for analytics.\nFrom Google cloud we just need to push the log messages to a third party application for analytics that is the part of analytics architecture.\nReal time push means, I go with Pub-sub."
      },
      {
        "date": "2023-02-28T01:24:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/logging/docs/export/configure_export_v2#overview\nhttps://cloud.google.com/logging/docs/export/pubsub: \nThis document explains how you can find log entries that you routed from Cloud Logging to Pub/Sub topics, which occurs in near real-time. We recommend using Pub/Sub for integrating Cloud Logging logs with third-party software.\n\nWhen you route logs to a Pub/Sub topic, Logging publishes each log entry as a Pub/Sub message as soon as Logging receives that log entry. Routed logs are generally available within seconds of their arrival to Logging, with 99% of logs available in less than 60 seconds."
      },
      {
        "date": "2023-01-18T09:14:00.000Z",
        "voteCount": 1,
        "content": "The processing will be done in a third-party application so we need a solution to pass logs from gcp to thirs party in real time and no need for analytics. So the solution is pub/sub.\nExample on a case corresponding to the question by google:\nhttps://cloud.google.com/architecture/exporting-stackdriver-logging-for-splunk"
      },
      {
        "date": "2023-01-23T03:52:00.000Z",
        "voteCount": 1,
        "content": "no need for analytics??? the question says \"You need to build a real-time log analysis architecture that pushes logs to a third-party application for processing  \" its bigquery, it can connect to  others cloud providers..https://cloud.google.com/bigquery/docs/introduction#bigquery-analytics\nhttps://cloud.google.com/blog/products/data-analytics/bigquery-performance-powers-real-time-analytics"
      },
      {
        "date": "2023-02-08T08:20:00.000Z",
        "voteCount": 1,
        "content": "While BigQuery can be used for log analysis, it is not well suited for real-time log processing. BigQuery is designed for batch processing of large amounts of data and may not be able to provide the low latency and real-time processing capabilities required for real-time log analysis. Additionally, BigQuery may be more expensive than other options for real-time log analysis, as it charges for both storage and processing.\n\nTherefore, for real-time log analysis, it is more appropriate to use a solution like Cloud Pub/Sub, which is specifically designed for real-time streaming of data.\n\nMy understanding is that third-party application may not be a GCP solution.\n\nI would go for A"
      },
      {
        "date": "2023-01-04T01:53:00.000Z",
        "voteCount": 1,
        "content": "Answer B\nThird party transfers for BigQuery Data Transfer Service allow you to automatically schedule and manage recurring load jobs for external data sources such as Salesforce CRM, Adobe Analytics, and Facebook Ads.\n\nhttps://cloud.google.com/bigquery/docs/introduction#bigquery-analytics\nhttps://cloud.google.com/blog/products/data-analytics/bigquery-performance-powers-real-time-analytics\n\nPub/sub does real time streaming not analytics. analytics its biquery and dataflow those can do realtime analytics."
      },
      {
        "date": "2022-12-15T07:28:00.000Z",
        "voteCount": 2,
        "content": "A is the only option that meets all of these requirements:\n- Handles large volumes of log data\n- Sends messages (logs) to 3rd party applications in real time"
      },
      {
        "date": "2023-01-04T01:49:00.000Z",
        "voteCount": 1,
        "content": "Third party transfers for BigQuery Data Transfer Service allow you to automatically schedule and manage recurring load jobs for external data sources such as Salesforce CRM, Adobe Analytics, and Facebook Ads.\n\nto do thrid party transfers bigquery has this above mentioned capability"
      },
      {
        "date": "2023-01-04T01:48:00.000Z",
        "voteCount": 1,
        "content": "Can pub/sub analyse data?? Kindly revisit the documentation, the question says You need to build a real-time log analysis architecture, not real time streaming, pub/sub does realtime streaming not analysis, so its bigquery , i dnt know if you practically worked on gcp then you will know and understand these solutions"
      },
      {
        "date": "2022-12-13T22:29:00.000Z",
        "voteCount": 1,
        "content": "option B"
      },
      {
        "date": "2022-12-13T18:49:00.000Z",
        "voteCount": 4,
        "content": "A -&gt; real-time to a third party app . pubsub..\n!C-&gt; GCS not realtime\n!B -&gt; No third party"
      },
      {
        "date": "2023-01-04T01:50:00.000Z",
        "voteCount": 1,
        "content": "Third party transfers for BigQuery Data Transfer Service allow you to automatically schedule and manage recurring load jobs for external data sources such as Salesforce CRM, Adobe Analytics, and Facebook Ads.\n\nyou cant analyse data on pub/sub but you stream, so understand the difference, answer is Bigquery"
      },
      {
        "date": "2022-12-15T07:25:00.000Z",
        "voteCount": 1,
        "content": "Agree with this explanation."
      },
      {
        "date": "2022-12-12T05:45:00.000Z",
        "voteCount": 1,
        "content": "B is the answer."
      },
      {
        "date": "2022-12-11T06:11:00.000Z",
        "voteCount": 1,
        "content": "vote B"
      },
      {
        "date": "2022-12-09T02:53:00.000Z",
        "voteCount": 2,
        "content": "Answer B \nBigquery performs real time analysis"
      },
      {
        "date": "2022-12-03T08:54:00.000Z",
        "voteCount": 1,
        "content": "key is \u201clarge volume\u201d\nhttps://cloud.google.com/architecture/exporting-stackdriver-logging-for-compliance-requirements"
      },
      {
        "date": "2022-12-09T02:54:00.000Z",
        "voteCount": 2,
        "content": "B is the answer. \" real time log analysis csan be done by Bigquery, C isnt correct."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 202,
    "url": "https://www.examtopics.com/discussions/google/view/91398-exam-professional-cloud-developer-topic-1-question-202/",
    "body": "You are developing a new public-facing application that needs to retrieve specific properties in the metadata of users\u2019 objects in their respective Cloud Storage buckets. Due to privacy and data residency requirements, you must retrieve only the metadata and not the object data. You want to maximize the performance of the retrieval process. How should you retrieve the metadata?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the patch method.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the compose method.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the copy method.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the fields request parameter.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-13T01:33:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/storage/docs/json_api#partial-response\nhttps://cloud.google.com/storage/docs/json_api#partial-example"
      },
      {
        "date": "2023-08-07T10:02:00.000Z",
        "voteCount": 1,
        "content": "The requirement here is to access only the metadata.  The metadata is stored in key-value pairs and hence that should be retrieved as fields request parameter only."
      },
      {
        "date": "2023-02-26T23:17:00.000Z",
        "voteCount": 1,
        "content": "Did you guyz get all professional developer questions from here"
      },
      {
        "date": "2023-02-26T08:05:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/storage/docs/json_api/v1/objects/get\n\nalt: \n\nType of data to return. Defaults to json.\nAcceptable values are:\n\njson: Return object metadata.\nmedia: Return object data."
      },
      {
        "date": "2023-01-11T01:52:00.000Z",
        "voteCount": 1,
        "content": "Answer D"
      },
      {
        "date": "2022-12-20T00:52:00.000Z",
        "voteCount": 1,
        "content": "request or get method \nhttps://firebase.google.com/docs/storage/web/file-metadata#get_file_metadata"
      },
      {
        "date": "2022-12-20T00:52:00.000Z",
        "voteCount": 1,
        "content": "Answer D"
      },
      {
        "date": "2022-12-13T04:50:00.000Z",
        "voteCount": 1,
        "content": "D is the answer.\n\nhttps://cloud.google.com/storage/docs/json_api/v1/objects/get"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 203,
    "url": "https://www.examtopics.com/discussions/google/view/91204-exam-professional-cloud-developer-topic-1-question-203/",
    "body": "You are deploying a microservices application to Google Kubernetes Engine (GKE) that will broadcast livestreams. You expect unpredictable traffic patterns and large variations in the number of concurrent users. Your application must meet the following requirements:<br><br>\u2022\tScales automatically during popular events and maintains high availability<br>\u2022\tIs resilient in the event of hardware failures<br><br>How should you configure the deployment parameters? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDistribute your workload evenly using a multi-zonal node pool.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDistribute your workload evenly using multiple zonal node pools.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse cluster autoscaler to resize the number of nodes in the node pool, and use a Horizontal Pod Autoscaler to scale the workload.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a managed instance group for Compute Engine with the cluster nodes. Configure autoscaling rules for the managed instance group.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate alerting policies in Cloud Monitoring based on GKE CPU and memory utilization. Ask an on-duty engineer to scale the workload by executing a script when CPU and memory usage exceed predefined thresholds."
    ],
    "answer": "AC",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AC",
        "count": 11,
        "isMostVoted": true
      },
      {
        "answer": "BC",
        "count": 6,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-04-23T01:38:00.000Z",
        "voteCount": 5,
        "content": "https://cloud.google.com/kubernetes-engine/docs/concepts/planning-scalability#choosing_multi-zonal_or_single-zone_node_pools"
      },
      {
        "date": "2024-04-13T01:39:00.000Z",
        "voteCount": 1,
        "content": "A: https://cloud.google.com/kubernetes-engine/docs/concepts/planning-scalability#choosing_multi-zonal_or_single-zone_node_pools:~:text=To%20deploy%20a%20highly%20available%20application%2C%20distribute%20your%20workload%20across%20multiple%20compute%20zones%20in%20a%20region%20by%20using%20multi%2Dzonal%20node%20pools%20which%20distribute%20nodes%20uniformly%20across%20zones.\n\nC: https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-autoscaler#how_cluster_autoscaler_works\nhttps://cloud.google.com/kubernetes-engine/docs/concepts/horizontalpodautoscaler"
      },
      {
        "date": "2023-09-23T06:47:00.000Z",
        "voteCount": 1,
        "content": "AC is correct."
      },
      {
        "date": "2023-08-07T10:14:00.000Z",
        "voteCount": 1,
        "content": "1)  Is resilient in the event of hardware failures -&gt;  Is resilient in the event of hardware failures\n2) Scales automatically during popular events and maintains high availability -&gt; Cluster Autoscalar + Horizontal POD Autoscalar"
      },
      {
        "date": "2023-08-05T05:58:00.000Z",
        "voteCount": 1,
        "content": "A is for resiliency.\nC is for scalability"
      },
      {
        "date": "2023-02-20T07:30:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/kubernetes-engine/docs/concepts/planning-scalability#choosing_multi-zonal_or_single-zone_node_pools : To deploy a highly available application, distribute your workload across multiple compute zones in a region by using multi-zonal node pools which distribute nodes uniformly across zones."
      },
      {
        "date": "2023-12-03T14:06:00.000Z",
        "voteCount": 1,
        "content": "The paragraph you highlight above says \"by using MULTI-ZONAL node pools\", so why B?\nA and C are correct!"
      },
      {
        "date": "2023-01-04T01:35:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/kubernetes-engine/docs/concepts/planning-scalability#choosing_multi-zonal_or_single-zone_node_pools\nThw answer is B not A, so its BC"
      },
      {
        "date": "2022-12-20T00:25:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/blog/products/containers-kubernetes/best-practices-for-creating-a-highly-available\nhttps://cloud.google.com/kubernetes-engine/docs/concepts/types-of-clusters#multi-zonal_clusters\nAnswer A, C"
      },
      {
        "date": "2023-01-04T01:36:00.000Z",
        "voteCount": 1,
        "content": "To deploy a highly available application, distribute your workload across multiple compute zones in a region by using multi-zonal node pools which distribute nodes uniformly across zones.\n\nAnswer B not A"
      },
      {
        "date": "2023-01-05T06:57:00.000Z",
        "voteCount": 2,
        "content": "A says \"using multi-zonal node pools\" so I think it is not in contradiction with what you copied from the document. B is not reffering to multi-zonal pools at all.So I think it is A."
      },
      {
        "date": "2023-01-06T05:13:00.000Z",
        "voteCount": 1,
        "content": "i see yes A is correct"
      },
      {
        "date": "2023-01-08T06:07:00.000Z",
        "voteCount": 1,
        "content": "B is correct not A"
      },
      {
        "date": "2023-01-06T05:14:00.000Z",
        "voteCount": 1,
        "content": "pools is the key word, multi is from multiple.... so it should be pools not pool"
      },
      {
        "date": "2023-12-10T11:47:00.000Z",
        "voteCount": 1,
        "content": "Read the whole phrase, not just a single word. Option B suggests creating multiple ZONAL node pools, while A suggests a MULTI-ZONAL node pool.\n\nGCP documentation recommends option A: https://cloud.google.com/kubernetes-engine/docs/concepts/planning-scalability#choosing_multi-zonal_or_single-zone_node_pools"
      },
      {
        "date": "2022-12-13T22:34:00.000Z",
        "voteCount": 1,
        "content": "option A C"
      },
      {
        "date": "2022-12-13T18:46:00.000Z",
        "voteCount": 2,
        "content": "A  multi-zonal node pool. We dont have it on GKE ;)"
      },
      {
        "date": "2022-12-25T13:06:00.000Z",
        "voteCount": 2,
        "content": "I don't know if I understood it wrong, but it seems that a multizonal cluster automatically has multizonal node pools, isn't that right? Link: https://cloud.google.com/kubernetes-engine/docs/concepts/node-pools"
      },
      {
        "date": "2022-12-12T06:38:00.000Z",
        "voteCount": 1,
        "content": "AC is the answer."
      },
      {
        "date": "2023-01-04T01:39:00.000Z",
        "voteCount": 1,
        "content": "A is wrong, according to Best practices its \"To deploy a highly available application, distribute your workload across multiple compute zones in a region by using multi-zonal node pools which distribute nodes uniformly across zones.\""
      },
      {
        "date": "2023-01-06T05:12:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2023-01-06T05:15:00.000Z",
        "voteCount": 1,
        "content": "Actuallly the answer is B,C not AC"
      },
      {
        "date": "2023-01-07T03:14:00.000Z",
        "voteCount": 6,
        "content": "stick to one answer"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 204,
    "url": "https://www.examtopics.com/discussions/google/view/91200-exam-professional-cloud-developer-topic-1-question-204/",
    "body": "You work at a rapidly growing financial technology startup. You manage the payment processing application written in Go and hosted on Cloud Run in the Singapore region (asia-southeast1). The payment processing application processes data stored in a Cloud Storage bucket that is also located in the Singapore region.<br><br>The startup plans to expand further into the Asia Pacific region. You plan to deploy the Payment Gateway in Jakarta, Hong Kong, and Taiwan over the next six months. Each location has data residency requirements that require customer data to reside in the country where the transaction was made. You want to minimize the cost of these deployments. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Cloud Storage bucket in each region, and create a Cloud Run service of the payment processing application in each region.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Cloud Storage bucket in each region, and create three Cloud Run services of the payment processing application in the Singapore region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate three Cloud Storage buckets in the Asia multi-region, and create three Cloud Run services of the payment processing application in the Singapore region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate three Cloud Storage buckets in the Asia multi-region, and create three Cloud Run revisions of the payment processing application in the Singapore region."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-13T01:42:00.000Z",
        "voteCount": 1,
        "content": "Question says: \"Each location has data residency requirements \"\nThis means must have cloud bucket in each region and the Run service in each region."
      },
      {
        "date": "2023-09-23T06:54:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2023-08-07T10:18:00.000Z",
        "voteCount": 1,
        "content": "I go with A not complicating the requirments.\nCloud bucket in each of the region / multiple multi-region cloud buckets with Cloud run in each region."
      },
      {
        "date": "2023-06-21T10:09:00.000Z",
        "voteCount": 1,
        "content": "A is ok"
      },
      {
        "date": "2023-04-06T17:35:00.000Z",
        "voteCount": 1,
        "content": "Answer is D\nBy creating three Cloud Storage buckets in the Asia multi-region (which includes Jakarta, Hong Kong, and Taiwan), the startup can ensure that customer data resides in the respective countries where the transactions are made, as required by the data residency requirements. The Cloud Run revisions of the payment processing application can be deployed in the Singapore region, which is the closest region to the Asia Pacific region with low-latency connectivity. This way, the application can process data from the Cloud Storage buckets in the Asia multi-region without incurring additional data transfer costs between regions."
      },
      {
        "date": "2023-01-11T01:58:00.000Z",
        "voteCount": 1,
        "content": "Answer is A"
      },
      {
        "date": "2022-12-20T00:02:00.000Z",
        "voteCount": 1,
        "content": "Answer A"
      },
      {
        "date": "2022-12-12T06:33:00.000Z",
        "voteCount": 2,
        "content": "A is the answer."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 205,
    "url": "https://www.examtopics.com/discussions/google/view/91198-exam-professional-cloud-developer-topic-1-question-205/",
    "body": "You recently joined a new team that has a Cloud Spanner database instance running in production. Your manager has asked you to optimize the Spanner instance to reduce cost while maintaining high reliability and availability of the database. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Logging to check for error logs, and reduce Spanner processing units by small increments until you find the minimum capacity required.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Trace to monitor the requests per sec of incoming requests to Spanner, and reduce Spanner processing units by small increments until you find the minimum capacity required.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Monitoring to monitor the CPU utilization, and reduce Spanner processing units by small increments until you find the minimum capacity required.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Snapshot Debugger to check for application errors, and reduce Spanner processing units by small increments until you find the minimum capacity required."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-13T02:13:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/spanner/docs/compute-capacity#increasing_and_decreasing_compute_capacity:~:text=In%20the%20latter,in%20Cloud%20Monitoring"
      },
      {
        "date": "2023-09-23T06:56:00.000Z",
        "voteCount": 1,
        "content": "C is correct."
      },
      {
        "date": "2023-08-07T10:30:00.000Z",
        "voteCount": 2,
        "content": "Monitoring allows the behavior and requests per sec to Cloud spanner.  By observing these parameter, we can optimize Spanner processing units in small increments until we find the minimum capacity required.  After these, we can fine tune Cloud spanner parameter so that costs and resource utilization will be within the limit.\nThe key here is observe and improve."
      },
      {
        "date": "2022-12-20T00:00:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/spanner/docs/compute-capacity#increasing_and_decreasing_compute_capacity\nAnswer C"
      },
      {
        "date": "2022-12-13T22:39:00.000Z",
        "voteCount": 1,
        "content": "option C"
      },
      {
        "date": "2022-12-13T18:44:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2022-12-12T06:32:00.000Z",
        "voteCount": 3,
        "content": "C is the answer.\n\nhttps://cloud.google.com/spanner/docs/compute-capacity#increasing_and_decreasing_compute_capacity"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 206,
    "url": "https://www.examtopics.com/discussions/google/view/89888-exam-professional-cloud-developer-topic-1-question-206/",
    "body": "You recently deployed a Go application on Google Kubernetes Engine (GKE). The operations team has noticed that the application's CPU usage is high even when there is low production traffic. The operations team has asked you to optimize your application's CPU resource consumption. You want to determine which Go functions consume the largest amount of CPU. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy a Fluent Bit daemonset on the GKE cluster to log data in Cloud Logging. Analyze the logs to get insights into your application code\u2019s performance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a custom dashboard in Cloud Monitoring to evaluate the CPU performance metrics of your application.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConnect to your GKE nodes using SSH. Run the top command on the shell to extract the CPU utilization of your application.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify your Go application to capture profiling data. Analyze the CPU metrics of your application in flame graphs in Profiler.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-13T02:15:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/profiler/docs/profiling-go\nhttps://cloud.google.com/profiler/docs/interacting-flame-graph"
      },
      {
        "date": "2023-09-23T06:57:00.000Z",
        "voteCount": 1,
        "content": "D is correct."
      },
      {
        "date": "2023-08-07T10:50:00.000Z",
        "voteCount": 1,
        "content": "Key here is flame graphs from the profiler.  So using cloud profiler is the right choice."
      },
      {
        "date": "2023-02-26T07:56:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/profiler/docs\n\nCloud Profiler is a statistical, low-overhead profiler that continuously gathers CPU usage and memory-allocation information from your production applications. It attributes that information to the application's source code, helping you identify the parts of the application consuming the most resources, and otherwise illuminating the performance characteristics of the code"
      },
      {
        "date": "2022-12-19T23:50:00.000Z",
        "voteCount": 1,
        "content": "Answer D"
      },
      {
        "date": "2022-12-13T22:39:00.000Z",
        "voteCount": 1,
        "content": "option D"
      },
      {
        "date": "2022-12-12T05:43:00.000Z",
        "voteCount": 2,
        "content": "D is the answer.\n\nhttps://cloud.google.com/profiler/docs/about-profiler\nCloud Profiler is a statistical, low-overhead profiler that continuously gathers CPU usage and memory-allocation information from your production applications. It attributes that information to the source code that generated it, helping you identify the parts of your application that are consuming the most resources, and otherwise illuminating your applications performance characteristics."
      },
      {
        "date": "2022-12-03T09:17:00.000Z",
        "voteCount": 1,
        "content": "D is correct\nhttps://cloud.google.com/profiler/docs"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 207,
    "url": "https://www.examtopics.com/discussions/google/view/91193-exam-professional-cloud-developer-topic-1-question-207/",
    "body": "Your team manages a Google Kubernetes Engine (GKE) cluster where an application is running. A different team is planning to integrate with this application. Before they start the integration, you need to ensure that the other team cannot make changes to your application, but they can deploy the integration on GKE. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUsing Identity and Access Management (IAM), grant the Viewer IAM role on the cluster project to the other team.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new GKE cluster. Using Identity and Access Management (IAM), grant the Editor role on the cluster project to the other team.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new namespace in the existing cluster. Using Identity and Access Management (IAM), grant the Editor role on the cluster project to the other team.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new namespace in the existing cluster. Using Kubernetes role-based access control (RBAC), grant the Admin role on the new namespace to the other team.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-04T19:39:00.000Z",
        "voteCount": 1,
        "content": "D is the answer."
      },
      {
        "date": "2023-02-20T07:16:00.000Z",
        "voteCount": 2,
        "content": "D: You define permissions within a Role or ClusterRole object. A Role defines access to resources within a single Namespace, while a ClusterRole defines access to resources in the entire cluster.\n\nhttps://cloud.google.com/kubernetes-engine/docs/how-to/role-based-access-control"
      },
      {
        "date": "2022-12-19T23:49:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/kubernetes-engine/docs/concepts/access-control#rbac\nhttps://cloud.google.com/kubernetes-engine/docs/how-to/role-based-access-control\nAnswer D"
      },
      {
        "date": "2022-12-12T06:26:00.000Z",
        "voteCount": 2,
        "content": "D is the answer."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 208,
    "url": "https://www.examtopics.com/discussions/google/view/91191-exam-professional-cloud-developer-topic-1-question-208/",
    "body": "You have recently instrumented a new application with OpenTelemetry, and you want to check the latency of your application requests in Trace. You want to ensure that a specific request is always traced. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWait 10 minutes, then verify that Trace captures those types of requests automatically.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite a custom script that sends this type of request repeatedly from your dev project.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Trace API to apply custom attributes to the trace.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd the X-Cloud-Trace-Context header to the request with the appropriate parameters.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-09-23T07:03:00.000Z",
        "voteCount": 1,
        "content": "D is correct."
      },
      {
        "date": "2023-08-07T11:29:00.000Z",
        "voteCount": 1,
        "content": "To identify specific request is from setting the request header with specific key value pairs.\nKey is X-Cloud-Trace-Context, Value is True."
      },
      {
        "date": "2023-07-04T07:19:00.000Z",
        "voteCount": 1,
        "content": "According to the Professional Google Cloud Developer documentation, to ensure that a specific request is always traced, the X-Cloud-Trace-Context header must be added to the request with the appropriate parameters. This header ensures that all requests made to the application are traced and added to the Trace list. Additionally, the documentation explains that the Trace ID and Span ID must be included in the header to ensure that the request is correctly attributed to the trace. By using this method, developers can easily monitor and analyze the latency and performance of their applications using Trace."
      },
      {
        "date": "2022-12-19T23:31:00.000Z",
        "voteCount": 1,
        "content": "To force a specific reqhttps://cloud.google.com/trace/docs/setup#force-trace request to be traced, add an X-Cloud-Trace-Context header to the request."
      },
      {
        "date": "2022-12-12T06:22:00.000Z",
        "voteCount": 2,
        "content": "D is the answer.\n\nhttps://cloud.google.com/trace/docs/setup#force-trace\nCloud Trace doesn't sample every request.\n\nTo force a specific request to be traced, add an X-Cloud-Trace-Context header to the request."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 209,
    "url": "https://www.examtopics.com/discussions/google/view/91396-exam-professional-cloud-developer-topic-1-question-209/",
    "body": "You are trying to connect to your Google Kubernetes Engine (GKE) cluster using kubectl from Cloud Shell. You have deployed your GKE cluster with a public endpoint. From Cloud Shell, you run the following command:<br><br><img src=\"https://img.examtopics.com/professional-cloud-developer/image6.png\"><br><br>You notice that the kubectl commands time out without returning an error message. What is the most likely cause of this issue?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYour user account does not have privileges to interact with the cluster using kubectl.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYour Cloud Shell external IP address is not part of the authorized networks of the cluster.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe Cloud Shell is not part of the same VPC as the GKE cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA VPC firewall is blocking access to the cluster\u2019s endpoint."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-04-13T02:30:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/kubernetes-engine/docs/troubleshooting#kubectl-times-out\n\n\"If the cluster is a private GKE cluster, then ensure that the outgoing IP of the machine you are attempting to connect from is included in the list of existing authorized networks.\""
      },
      {
        "date": "2023-09-23T07:06:00.000Z",
        "voteCount": 1,
        "content": "B is correct."
      },
      {
        "date": "2023-08-07T11:35:00.000Z",
        "voteCount": 1,
        "content": "Cloud shells public IP is not present in the authorized networks/IP list of the GKE cluster."
      },
      {
        "date": "2023-01-27T10:13:00.000Z",
        "voteCount": 1,
        "content": "Where is the info that this is a private cluster?"
      },
      {
        "date": "2023-01-27T10:28:00.000Z",
        "voteCount": 2,
        "content": "Ok, it is B"
      },
      {
        "date": "2022-12-28T01:17:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/kubernetes-engine/docs/troubleshooting#connection_refused"
      },
      {
        "date": "2022-12-27T05:47:00.000Z",
        "voteCount": 1,
        "content": "Answer B\nhttps://cloud.google.com/kubernetes-engine/docs/how-to/private-clusters#cloud_shell"
      },
      {
        "date": "2022-12-13T04:43:00.000Z",
        "voteCount": 2,
        "content": "B is the answer.\n\nhttps://cloud.google.com/kubernetes-engine/docs/how-to/private-clusters#cloud_shell\nIf you want to use Cloud Shell to access the cluster, you must add the public IP address of your Cloud Shell to the cluster's list of authorized networks."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 210,
    "url": "https://www.examtopics.com/discussions/google/view/90014-exam-professional-cloud-developer-topic-1-question-210/",
    "body": "You are developing a web application that contains private images and videos stored in a Cloud Storage bucket. Your users are anonymous and do not have Google Accounts. You want to use your application-specific logic to control access to the images and videos. How should you configure access?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCache each web application user's IP address to create a named IP table using Google Cloud Armor. Create a Google Cloud Armor security policy that allows users to access the backend bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGrant the Storage Object Viewer IAM role to allUsers. Allow users to access the bucket after authenticating through your web application.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Identity-Aware Proxy (IAP) to authenticate users into the web application. Allow users to access the bucket after authenticating through IAP.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGenerate a signed URL that grants read access to the bucket. Allow users to access the URL after authenticating through your web application.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-09-23T07:09:00.000Z",
        "voteCount": 1,
        "content": "D is correct."
      },
      {
        "date": "2023-08-07T11:42:00.000Z",
        "voteCount": 1,
        "content": "The key here is \"application-specific logic to control access to the images and videos\".  Signed Url with Read only permission with limited access time is the right choice."
      },
      {
        "date": "2023-06-21T10:27:00.000Z",
        "voteCount": 1,
        "content": "D is ok \nhttps://cloud.google.com/storage/docs/access-control/signed-urls#should-you-use"
      },
      {
        "date": "2022-12-12T05:41:00.000Z",
        "voteCount": 2,
        "content": "D is the answer.\n\nhttps://cloud.google.com/storage/docs/access-control/signed-urls#should-you-use\nIn some scenarios, you might not want to require your users to have a Google account in order to access Cloud Storage, but you still want to control access using your application-specific logic. The typical way to address this use case is to provide a signed URL to a user, which gives the user read, write, or delete access to that resource for a limited time. You specify an expiration time when you create the signed URL. Anyone who knows the URL can access the resource until the expiration time for the URL is reached or the key used to sign the URL is rotated."
      },
      {
        "date": "2022-12-04T15:26:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/storage/docs/access-control/signed-urls#should-you-use"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 211,
    "url": "https://www.examtopics.com/discussions/google/view/90011-exam-professional-cloud-developer-topic-1-question-211/",
    "body": "You need to configure a Deployment on Google Kubernetes Engine (GKE). You want to include a check that verifies that the containers can connect to the database. If the Pod is failing to connect, you want a script on the container to run to complete a graceful shutdown. How should you configure the Deployment?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate two jobs: one that checks whether the container can connect to the database, and another that runs the shutdown script if the Pod is failing.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate the Deployment with a livenessProbe for the container that will fail if the container can't connect to the database. Configure a Prestop lifecycle handler that runs the shutdown script if the container is failing.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate the Deployment with a PostStart lifecycle handler that checks the service availability. Configure a PreStop lifecycle handler that runs the shutdown script if the container is failing.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate the Deployment with an initContainer that checks the service availability. Configure a Prestop lifecycle handler that runs the shutdown script if the Pod is failing."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 12,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-04T15:12:00.000Z",
        "voteCount": 6,
        "content": "https://cloud.google.com/architecture/best-practices-for-running-cost-effective-kubernetes-applications-on-gke#make_sure_your_applications_are_shutting_down_in_accordance_with_kubernetes_expectations"
      },
      {
        "date": "2024-04-13T04:02:00.000Z",
        "voteCount": 1,
        "content": "\"Most programs don't stop accepting requests right away. However, if you're using third-party code or are managing a system that you don't have control over, such as nginx, the preStop hook is a good option for triggering a graceful shutdown without modifying the application. One common strategy is to execute, in the preStop hook, a sleep of a few seconds to postpone the SIGTERM. This gives Kubernetes extra time to finish the Pod deletion process, and reduces connection errors on the client side.\"\n\nhttps://cloud.google.com/architecture/best-practices-for-running-cost-effective-kubernetes-applications-on-gke#:~:text=If%20your%20application%20doesn%27t%20follow%20the%20preceding%20practice%2C%20use%20the%20preStop%20hook\n\nhttps://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks"
      },
      {
        "date": "2023-08-07T11:48:00.000Z",
        "voteCount": 1,
        "content": "I go with B, that is liveness probe and if failed for max retries then call prestop hook to gracefully shutdown the container.  D is also very close, but it used init container to check for the database connectivity first.  I am not sure whether we can prestop hook if initContainer fails to starts."
      },
      {
        "date": "2023-02-20T06:50:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/architecture/best-practices-for-running-cost-effective-kubernetes-applications-on-gke#make_sure_your_applications_are_shutting_down_in_accordance_with_kubernetes_expectations -&gt;  the preStop hook is a good option for triggering a graceful shutdown without modifying the application.\nhttps://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#hook-details -&gt;\nThis hook is called immediately before a container is terminated due to an API request or management event such as a liveness/startup probe failure, preemption, resource contention and others. A call to the PreStop hook fails if the container is already in a terminated or completed state and the hook must complete before the TERM signal to stop the container can be sent. The Pod's termination grace period countdown begins before the PreStop hook is executed, so regardless of the outcome of the handler, the container will eventually terminate within the Pod's termination grace period. No parameters are passed to the handler."
      },
      {
        "date": "2023-01-11T02:37:00.000Z",
        "voteCount": 1,
        "content": "Answer B"
      },
      {
        "date": "2022-12-21T07:26:00.000Z",
        "voteCount": 2,
        "content": "B is the answer."
      },
      {
        "date": "2022-12-04T13:31:00.000Z",
        "voteCount": 2,
        "content": "D could be the right answer"
      },
      {
        "date": "2022-12-22T07:28:00.000Z",
        "voteCount": 1,
        "content": "The answer is definitely D. Anytime you need to do some work before a container can be considered ready, you use init containers. With a liveness probe we would need to add an endpoint that checks whether we can connect to the database, with init containers we can separate this logic."
      },
      {
        "date": "2022-12-22T07:31:00.000Z",
        "voteCount": 1,
        "content": "Liveness probe also supports other methods to do work beside http, but I hope you understand my message. Here's more on init containers btw: https://kubernetes.io/docs/concepts/workloads/pods/init-containers/"
      },
      {
        "date": "2022-12-22T16:32:00.000Z",
        "voteCount": 1,
        "content": "initContainer requires separate container to run in addition to the application container, but the question asked for script to be run in the same container."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 212,
    "url": "https://www.examtopics.com/discussions/google/view/91107-exam-professional-cloud-developer-topic-1-question-212/",
    "body": "You are responsible for deploying a new API. That API will have three different URL paths:<br><br>\u2022\thttps://yourcompany.com/students<br>\u2022\thttps://yourcompany.com/teachers<br>\u2022\thttps://yourcompany.com/classes<br><br>You need to configure each API URL path to invoke a different function in your code. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate one Cloud Function as a backend service exposed using an HTTPS load balancer.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate three Cloud Functions exposed directly.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate one Cloud Function exposed directly.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate three Cloud Functions as three backend services exposed using an HTTPS load balancer.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 9,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 7,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-03-15T17:02:00.000Z",
        "voteCount": 1,
        "content": "There's no purpose for a Load Balancer here as you are not balancing traffic across multiple backend server instances. You need 3 different Cloud Functions each with their own Endpoint that's all.  See this example: https://cloud.google.com/functions/docs/create-deploy-gcloud-1st-gen"
      },
      {
        "date": "2024-02-22T18:00:00.000Z",
        "voteCount": 3,
        "content": "option D. \nWhen users query \"yourcompany.com,\" they receive an IP address and access the load balancer. Consequently, the load balancer then executes path-based routing.\nB is Wrong. It is not possible to deploy three cloud functions with the same domain name."
      },
      {
        "date": "2023-12-25T09:44:00.000Z",
        "voteCount": 1,
        "content": "i go for B because i don't understand the necessity of LB in this case"
      },
      {
        "date": "2023-08-07T12:05:00.000Z",
        "voteCount": 1,
        "content": "I go with B.  Exposed using an HTTPS load balancer is not required.  Those three are different end points of the service. We no need to setup load balancer in case of Cloud functions, it is serverless."
      },
      {
        "date": "2023-06-10T20:33:00.000Z",
        "voteCount": 2,
        "content": "D. The differences between B and D are:\n1. Cost: 3 Cloud Function exposed directly (B) will create 3 endpoints / load balancers, whereas D only exposed one load balancer.\n2. Scalability: exposing directly with endpoint or instance itself would cause scalability problem - can't upscale the endpoint instance fast enough.\n3. Handling service-to-service call: In B, all services rely on external DNS resolution, which is slower. In D, it has chance that cross-service call can be resolved internally."
      },
      {
        "date": "2023-04-03T05:56:00.000Z",
        "voteCount": 1,
        "content": "Option B (Create three Cloud Functions exposed directly) is the best choice in this scenario, as it allows you to create a separate Cloud Function for each API URL path and configure each one to invoke a different function in your code.\n\nOption A (Create one Cloud Function as a backend service exposed using an HTTPS load balancer) and Option D (Create three Cloud Functions as three backend services exposed using an HTTPS load balancer) both involve using an HTTPS load balancer, which adds additional complexity and configuration overhead. These options may be appropriate for more complex scenarios, but in this case, they are not necessary.\n\nOption C (Create one Cloud Function exposed directly) would require all three API URL paths to invoke the same function in your code, which does not meet the requirement of invoking different functions for each URL path."
      },
      {
        "date": "2023-04-23T02:23:00.000Z",
        "voteCount": 1,
        "content": "B is wrong, in API context you need to map each external url to cloud function url, to do that you need LB"
      },
      {
        "date": "2023-02-20T06:53:00.000Z",
        "voteCount": 1,
        "content": "i choose D"
      },
      {
        "date": "2023-02-28T01:15:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/load-balancing/docs/https/setting-up-https-serverless\nhttps://cloud.google.com/load-balancing/docs/negs/serverless-neg-concepts"
      },
      {
        "date": "2023-02-03T12:58:00.000Z",
        "voteCount": 3,
        "content": "Each function is defined as an HTTP trigger, which allows it to be triggered by incoming HTTP requests. The endpoint for each function is defined in the function name (e.g. \"students\", \"teachers\", \"classes\").\n\nThis means that the APIs would be accessible at the following endpoints:\n\n\u2022 https://yourcompany.com/students\n\u2022 https://yourcompany.com/teachers\n\u2022 https://yourcompany.com/classes\n\nNote that you would need to configure \"yourcompany.com\" DNS registry.\n\nIn this case, option B, \"Create three Cloud Functions exposed directly\", would be correct."
      },
      {
        "date": "2023-02-08T08:44:00.000Z",
        "voteCount": 1,
        "content": "Using option D, where you create three Cloud Functions as backend services exposed through an HTTPS load balancer, is not necessary in this scenario. An HTTPS load balancer would be useful in scenarios where you need to balance incoming traffic across multiple instances of a backend service to distribute the workload, ensure high availability, and provide failover protection. However, in this case, you only need to map each API URL path to a different function, which can be achieved by creating three separate Cloud Functions, each exposed directly. This would be a simpler and more straightforward solution for this specific use case."
      },
      {
        "date": "2023-01-27T10:42:00.000Z",
        "voteCount": 1,
        "content": "Why there is the need of LB?"
      },
      {
        "date": "2022-12-13T23:05:00.000Z",
        "voteCount": 1,
        "content": "option D"
      },
      {
        "date": "2022-12-12T05:32:00.000Z",
        "voteCount": 1,
        "content": "D is the answer."
      },
      {
        "date": "2022-12-12T07:42:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/load-balancing/docs/https/setup-global-ext-https-serverless"
      },
      {
        "date": "2022-12-11T20:51:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 213,
    "url": "https://www.examtopics.com/discussions/google/view/89806-exam-professional-cloud-developer-topic-1-question-213/",
    "body": "You are deploying a microservices application to Google Kubernetes Engine (GKE). The application will receive daily updates. You expect to deploy a large number of distinct containers that will run on the Linux operating system (OS). You want to be alerted to any known OS vulnerabilities in the new containers. You want to follow Google-recommended best practices. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the gcloud CLI to call Container Analysis to scan new container images. Review the vulnerability results before each deployment.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Container Analysis, and upload new container images to Artifact Registry. Review the vulnerability results before each deployment.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Container Analysis, and upload new container images to Artifact Registry. Review the critical vulnerability results before each deployment.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Container Analysis REST API to call Container Analysis to scan new container images. Review the vulnerability results before each deployment."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 9,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-06-22T20:03:00.000Z",
        "voteCount": 2,
        "content": "B. Actually the tricky part for this question is: Is the Container Analysis enabled by default? Can Container Analysis be called on-demand via REST without specifically enabled it? By default GCP does not enable Container Analysis; that's why D is out."
      },
      {
        "date": "2023-02-20T06:55:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/artifact-registry/docs/analysis\n\nVulnerability scanning can occur automatically or on-demand:\n\nWhen automatic scanning is enabled, scanning triggers automatically every time you push a new image to Artifact Registry or Container Registry. Vulnerability information is continuously updated when new vulnerabilities are discovered.\n\nWhen On-Demand Scanning is enabled, you must run a command to scan a local image or an image in Artifact Registry or Container Registry. On-Demand Scanning gives you more flexibility around when you scan containers. For example, you can scan a locally-built image and remediate vulnerabilities before storing it in a registry.\n\nScanning results are available for up to 48 hours after the scan is completed, and vulnerability information is not updated after the scan."
      },
      {
        "date": "2023-01-03T00:52:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/blog/products/application-development/understanding-artifact-registry-vs-container-registry"
      },
      {
        "date": "2022-12-27T05:05:00.000Z",
        "voteCount": 1,
        "content": "Container Analysis is a service that provides vulnerability scanning and metadata storage for containers. The scanning service performs vulnerability scans on images in Container Registry and Artifact Registry, then stores the resulting metadata and makes it available for consumption through an API. Metadata storage allows storing information from different sources, including vulnerability scanning, other Google Cloud services, and third-party providers.\n\nhttps://cloud.google.com/container-analysis/docs/container-analysis"
      },
      {
        "date": "2022-12-13T23:11:00.000Z",
        "voteCount": 1,
        "content": "option B"
      },
      {
        "date": "2022-12-10T19:51:00.000Z",
        "voteCount": 1,
        "content": "B is the answer.\n\nhttps://cloud.google.com/container-analysis/docs/automated-scanning-howto"
      },
      {
        "date": "2022-12-04T14:30:00.000Z",
        "voteCount": 1,
        "content": "Answer B\nIf you have done Devops you will understand"
      },
      {
        "date": "2022-12-04T09:52:00.000Z",
        "voteCount": 1,
        "content": "Answer B"
      },
      {
        "date": "2022-12-04T03:29:00.000Z",
        "voteCount": 1,
        "content": "\"Container Analysis REST API\" doesn't exist. \n\nhttps://cloud.google.com/container-analysis/docs/os-overview says: \nThe Container Scanning API allows you to automate OS vulnerability detection, scanning each time you push an image to Container Registry or Artifact Registry. Enabling this API also triggers language package scans for Go and Java vulnerabilities (Preview)."
      },
      {
        "date": "2022-12-04T03:33:00.000Z",
        "voteCount": 2,
        "content": "After reviewing the document again, I changed my answer to D."
      },
      {
        "date": "2022-12-27T05:03:00.000Z",
        "voteCount": 1,
        "content": "It cant  be D,thats not how the Container analysis API works"
      },
      {
        "date": "2022-12-27T05:02:00.000Z",
        "voteCount": 1,
        "content": "Do not confuse yourself, there is Container analysis API, it exists. check what the question requires, ok\nhttps://cloud.google.com/container-analysis/docs/reference/rest"
      },
      {
        "date": "2022-12-03T17:18:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/container-analysis/docs/enable-container-scanning"
      },
      {
        "date": "2022-12-02T12:31:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/container-analysis/docs/os-overview"
      },
      {
        "date": "2022-12-04T07:18:00.000Z",
        "voteCount": 2,
        "content": "Answer is B\nhttps://cloud.google.com/container-analysis/docs/automated-scanning-howto"
      },
      {
        "date": "2022-12-11T20:55:00.000Z",
        "voteCount": 1,
        "content": "after re review i think B is correct too but im still not sure"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 214,
    "url": "https://www.examtopics.com/discussions/google/view/91108-exam-professional-cloud-developer-topic-1-question-214/",
    "body": "You are a developer at a large organization. You have an application written in Go running in a production Google Kubernetes Engine (GKE) cluster. You need to add a new feature that requires access to BigQuery. You want to grant BigQuery access to your GKE cluster following Google-recommended best practices. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Google service account with BigQuery access. Add the JSON key to Secret Manager, and use the Go client library to access the JSON key.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Google service account with BigQuery access. Add the Google service account JSON key as a Kubernetes secret, and configure the application to use this secret.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Google service account with BigQuery access. Add the Google service account JSON key to Secret Manager, and use an init container to access the secret for the application to use.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Google service account and a Kubernetes service account. Configure Workload Identity on the GKE cluster, and reference the Kubernetes service account on the application Deployment.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 7,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-04-13T04:12:00.000Z",
        "voteCount": 1,
        "content": "\"Applications running on GKE might need access to Google Cloud APIs such as Compute Engine API, BigQuery Storage API, or Machine Learning APIs.\"\n\nhttps://cloud.google.com/kubernetes-engine/docs/concepts/workload-identity#what_is"
      },
      {
        "date": "2023-02-20T06:58:00.000Z",
        "voteCount": 1,
        "content": "Workload Identity allows a Kubernetes service account in your GKE cluster to act as an IAM service account. Pods that use the configured Kubernetes service account automatically authenticate as the IAM service account when accessing Google Cloud APIs. Using Workload Identity allows you to assign distinct, fine-grained identities and authorization for each application in your cluster.\n\nhttps://cloud.google.com/kubernetes-engine/docs/concepts/workload-identity#what_is"
      },
      {
        "date": "2023-01-11T02:43:00.000Z",
        "voteCount": 1,
        "content": "The answer is D because the best pratice is to use workload identity\nhttps://cloud.google.com/kubernetes-engine/docs/concepts/workload-identity#what_is"
      },
      {
        "date": "2023-01-04T00:44:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/kubernetes-engine/docs/quickstarts/deploy-app-container-image#deploying_to_gke"
      },
      {
        "date": "2022-12-27T04:58:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/kubernetes-engine/docs/concepts/workload-identity#what_is\nAnswer D"
      },
      {
        "date": "2022-12-13T23:13:00.000Z",
        "voteCount": 1,
        "content": "option D"
      },
      {
        "date": "2022-12-13T18:42:00.000Z",
        "voteCount": 1,
        "content": "a go???? no!!  D is correct"
      },
      {
        "date": "2022-12-12T06:15:00.000Z",
        "voteCount": 1,
        "content": "D is the answer.\n\nhttps://cloud.google.com/kubernetes-engine/docs/concepts/workload-identity#what_is\nApplications running on GKE might need access to Google Cloud APIs such as Compute Engine API, BigQuery Storage API, or Machine Learning APIs.\n\nWorkload Identity allows a Kubernetes service account in your GKE cluster to act as an IAM service account. Pods that use the configured Kubernetes service account automatically authenticate as the IAM service account when accessing Google Cloud APIs. Using Workload Identity allows you to assign distinct, fine-grained identities and authorization for each application in your cluster."
      },
      {
        "date": "2022-12-11T21:20:00.000Z",
        "voteCount": 1,
        "content": "vote A because the type of auth supported by bq and the recommended way of auth which is use go libraries \n\nhttps://cloud.google.com/bigquery/docs/authorization\nhttps://pkg.go.dev/golang.org/x/oauth2/google?utm_source=cloud.google.com&amp;utm_medium=referral#JWTAccessTokenSourceFromJSON"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 215,
    "url": "https://www.examtopics.com/discussions/google/view/89954-exam-professional-cloud-developer-topic-1-question-215/",
    "body": "You have an application written in Python running in production on Cloud Run. Your application needs to read/write data stored in a Cloud Storage bucket in the same project. You want to grant access to your application following the principle of least privilege. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a user-managed service account with a custom Identity and Access Management (IAM) role.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a user-managed service account with the Storage Admin Identity and Access Management (IAM) role.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a user-managed service account with the Project Editor Identity and Access Management (IAM) role.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the default service account linked to the Cloud Run revision in production."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-13T06:26:00.000Z",
        "voteCount": 1,
        "content": "A. Create a user-managed service account with a custom Identity and Access Management (IAM) role."
      },
      {
        "date": "2023-09-24T04:14:00.000Z",
        "voteCount": 1,
        "content": "A is correct."
      },
      {
        "date": "2023-08-07T12:28:00.000Z",
        "voteCount": 1,
        "content": "principle of least privilege -&gt; custom Identity and Access Management (IAM) role"
      },
      {
        "date": "2023-01-11T02:45:00.000Z",
        "voteCount": 2,
        "content": "Answer is A\nThe others give too many acess"
      },
      {
        "date": "2022-12-10T19:42:00.000Z",
        "voteCount": 2,
        "content": "A is the answer."
      },
      {
        "date": "2022-12-04T14:29:00.000Z",
        "voteCount": 1,
        "content": "Answer A"
      },
      {
        "date": "2022-12-04T04:22:00.000Z",
        "voteCount": 1,
        "content": "A - assign the needed permissions, following the least privilege rule\n\nNot B - https://cloud.google.com/iam/docs/understanding-roles#storage.admin\nC and D gives too many access"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 216,
    "url": "https://www.examtopics.com/discussions/google/view/89985-exam-professional-cloud-developer-topic-1-question-216/",
    "body": "Your team is developing unit tests for Cloud Function code. The code is stored in a Cloud Source Repositories repository. You are responsible for implementing the tests. Only a specific service account has the necessary permissions to deploy the code to Cloud Functions. You want to ensure that the code cannot be deployed without first passing the tests. How should you configure the unit testing process?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Cloud Build to deploy the Cloud Function. If the code passes the tests, a deployment approval is sent to you.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Cloud Build to deploy the Cloud Function, using the specific service account as the build agent. Run the unit tests after successful deployment.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Cloud Build to run the unit tests. If the code passes the tests, the developer deploys the Cloud Function.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Cloud Build to run the unit tests, using the specific service account as the build agent. If the code passes the tests, Cloud Build deploys the Cloud Function.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 9,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-09-24T04:29:00.000Z",
        "voteCount": 1,
        "content": "D is correct."
      },
      {
        "date": "2023-08-07T15:15:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is D. First run unit tests and if all pass then deploy as Cloud Func."
      },
      {
        "date": "2023-02-26T07:44:00.000Z",
        "voteCount": 1,
        "content": "i made a midtake, it's d"
      },
      {
        "date": "2023-02-20T06:31:00.000Z",
        "voteCount": 1,
        "content": "b) first run test and then deploy"
      },
      {
        "date": "2023-02-26T07:43:00.000Z",
        "voteCount": 1,
        "content": "typo, D"
      },
      {
        "date": "2023-01-27T08:47:00.000Z",
        "voteCount": 1,
        "content": "D. Configure Cloud Build to run the unit tests, using the specific service account as the build agent. If the code passes the tests, Cloud Build deploys the Cloud Function.\n\nThis ensures that only the specific service account, which has the necessary permissions, is able to deploy the code after it has passed the unit tests. The developer does not need to worry about deploying the code, and the code cannot be deployed without passing the tests."
      },
      {
        "date": "2023-01-11T02:46:00.000Z",
        "voteCount": 1,
        "content": "Answer D"
      },
      {
        "date": "2022-12-13T23:16:00.000Z",
        "voteCount": 1,
        "content": "option D"
      },
      {
        "date": "2022-12-10T19:40:00.000Z",
        "voteCount": 1,
        "content": "D is the answer."
      },
      {
        "date": "2022-12-04T09:51:00.000Z",
        "voteCount": 2,
        "content": "Answer D"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 217,
    "url": "https://www.examtopics.com/discussions/google/view/89812-exam-professional-cloud-developer-topic-1-question-217/",
    "body": "Your team detected a spike of errors in an application running on Cloud Run in your production project. The application is configured to read messages from Pub/Sub topic A, process the messages, and write the messages to topic B. You want to conduct tests to identify the cause of the errors. You can use a set of mock messages for testing. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the Pub/Sub and Cloud Run emulators on your local machine. Deploy the application locally, and change the logging level in the application to DEBUG or INFO. Write mock messages to topic A, and then analyze the logs.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the gcloud CLI to write mock messages to topic A. Change the logging level in the application to DEBUG or INFO, and then analyze the logs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the Pub/Sub emulator on your local machine. Point the production application to your local Pub/Sub topics. Write mock messages to topic A, and then analyze the logs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Google Cloud console to write mock messages to topic A. Change the logging level in the application to DEBUG or INFO, and then analyze the logs."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 9,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-09-24T04:31:00.000Z",
        "voteCount": 1,
        "content": "A is correct."
      },
      {
        "date": "2023-08-07T15:19:00.000Z",
        "voteCount": 1,
        "content": "A is right. Pub-Sub and cloud run emulator to run under local env with mock msgs publish to a topic, INFO and DEBUG logs enabled to see detailed log info."
      },
      {
        "date": "2023-02-20T06:34:00.000Z",
        "voteCount": 1,
        "content": "I choose A. C is against all practices."
      },
      {
        "date": "2022-12-14T05:15:00.000Z",
        "voteCount": 1,
        "content": "A is the answer."
      },
      {
        "date": "2022-12-11T21:29:00.000Z",
        "voteCount": 2,
        "content": "going with A because it mentions the 2 points of posible failure and gives you a full scenario to analyse"
      },
      {
        "date": "2022-12-04T03:42:00.000Z",
        "voteCount": 3,
        "content": "Run all locally"
      },
      {
        "date": "2022-12-02T15:56:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/pubsub/docs/emulator"
      },
      {
        "date": "2022-12-03T16:35:00.000Z",
        "voteCount": 4,
        "content": "i think that is better the option A, because can you run all locally and check the logs."
      },
      {
        "date": "2022-12-20T06:34:00.000Z",
        "voteCount": 2,
        "content": "If production is pointing to your local emulator...your users are in trouble lol...Answer is A"
      },
      {
        "date": "2023-01-04T00:26:00.000Z",
        "voteCount": 1,
        "content": "You want to conduct tests to identify the cause of the errors...this is the core of the question....."
      },
      {
        "date": "2023-01-03T00:36:00.000Z",
        "voteCount": 1,
        "content": "when emulatimg you are testing, you have to test, i think you need to read the document first then you will understand what pub/sub emulation is and how it is done. yeah the answer might not be correct because it doesnt include cloud run emulation.... you run all locally to test then you can deploy to production"
      },
      {
        "date": "2023-05-01T23:42:00.000Z",
        "voteCount": 1,
        "content": "You may not have experience with this, but wouldn't the main application be developed in a local environment?\nIn the same way, it is better to develop and debug everything locally.\nThere are various troublesome parts, and in reality, the details (Functions, Pub/Sub, etc.) are checked in the development environment, but ideally, it would be convenient if everything could be developed and debugged in the local environment.\nIn an ideal case, it would be convenient if everything could be developed and debugged in a local environment."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 218,
    "url": "https://www.examtopics.com/discussions/google/view/89944-exam-professional-cloud-developer-topic-1-question-218/",
    "body": "You are developing a Java Web Server that needs to interact with Google Cloud services via the Google Cloud API on the user's behalf. Users should be able to authenticate to the Google Cloud API using their Google Cloud identities. Which workflow should you implement in your web application?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. When a user arrives at your application, prompt them for their Google username and password.<br>2. Store an SHA password hash in your application's database along with the user's username.<br>3. The application authenticates to the Google Cloud API using HTTPs requests with the user's username and password hash in the Authorization request header.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. When a user arrives at your application, prompt them for their Google username and password.<br>2. Forward the user's username and password in an HTTPS request to the Google Cloud authorization server, and request an access token.<br>3. The Google server validates the user's credentials and returns an access token to the application.<br>4. The application uses the access token to call the Google Cloud API.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. When a user arrives at your application, route them to a Google Cloud consent screen with a list of requested permissions that prompts the user to sign in with SSO to their Google Account.<br>2. After the user signs in and provides consent, your application receives an authorization code from a Google server.<br>3. The Google server returns the authorization code to the user, which is stored in the browser's cookies.<br>4. The user authenticates to the Google Cloud API using the authorization code in the cookie.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. When a user arrives at your application, route them to a Google Cloud consent screen with a list of requested permissions that prompts the user to sign in with SSO to their Google Account.<br>2. After the user signs in and provides consent, your application receives an authorization code from a Google server.<br>3. The application requests a Google Server to exchange the authorization code with an access token.<br>4. The Google server responds with the access token that is used by the application to call the Google Cloud API.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-09-24T04:37:00.000Z",
        "voteCount": 1,
        "content": "D is correct."
      },
      {
        "date": "2023-08-07T15:22:00.000Z",
        "voteCount": 1,
        "content": "D is right. OAuth 2.0 authorization code grant flow is the technique to use Google APIs to access resources servers."
      },
      {
        "date": "2023-02-20T06:36:00.000Z",
        "voteCount": 1,
        "content": "https://developers.google.com/identity/protocols/oauth2"
      },
      {
        "date": "2023-01-11T02:52:00.000Z",
        "voteCount": 2,
        "content": "D is the anwser\nYou need to use OAUTH of google so A and B are eliminated.\nThe C is using the authorization code in the cookie, it's not how it's works.\n\nSo D is the correct to exchange the code for an access token."
      },
      {
        "date": "2022-12-10T19:31:00.000Z",
        "voteCount": 1,
        "content": "D is the answer.\n\nhttps://developers.google.com/identity/protocols/oauth2#webserver\nThe Google OAuth 2.0 endpoint supports web server applications that use languages and frameworks such as PHP, Java, Python, Ruby, and ASP.NET.\n\nThe authorization sequence begins when your application redirects a browser to a Google URL; the URL includes query parameters that indicate the type of access being requested. Google handles the user authentication, session selection, and user consent. The result is an authorization code, which the application can exchange for an access token and a refresh token."
      },
      {
        "date": "2022-12-04T14:27:00.000Z",
        "voteCount": 1,
        "content": "Yes D is the answer"
      },
      {
        "date": "2022-12-04T09:46:00.000Z",
        "voteCount": 1,
        "content": "I do agree with D"
      },
      {
        "date": "2022-12-04T03:45:00.000Z",
        "voteCount": 3,
        "content": "https://developers.google.com/identity/protocols/oauth2"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 219,
    "url": "https://www.examtopics.com/discussions/google/view/89811-exam-professional-cloud-developer-topic-1-question-219/",
    "body": "You recently developed a new application. You want to deploy the application on Cloud Run without a Dockerfile. Your organization requires that all container images are pushed to a centrally managed container repository. How should you build your container using Google Cloud services? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPush your source code to Artifact Registry.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSubmit a Cloud Build job to push the image.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the pack build command with pack CLI.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInclude the --source flag with the gcloud run deploy CLI command.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInclude the --platform=kubernetes flag with the gcloud run deploy CLI command."
    ],
    "answer": "CD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "CD",
        "count": 10,
        "isMostVoted": true
      },
      {
        "answer": "AB",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "BC",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "AC",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-12-12T16:39:00.000Z",
        "voteCount": 2,
        "content": "C: \u201cFor example, use buildpacks to build the source code of your Cloud Run service into a container image.\u201d - https://cloud.google.com/docs/buildpacks/build-application\n\nAlso, https://cloud.google.com/blog/products/containers-kubernetes/google-cloud-now-supports-buildpacks\n\n\nD: \u201cIf no Dockerfile is present in the source code directory, Google Cloud's buildpacks automatically detects the language you are using and fetches the dependencies of the code to make a production-ready container image, using a secure base image managed by Google.\u201d - https://cloud.google.com/run/docs/deploying-source-code"
      },
      {
        "date": "2023-12-12T16:41:00.000Z",
        "voteCount": 1,
        "content": "Not A, as it's redundant when using the option D: \u201cYou can also deploy directly from source to Cloud Run, which includes automatically creating a container image for your built source and storing the image in Artifact Registry.\u201d - https://cloud.google.com/artifact-registry/docs/integrate-cloud-run\n\nB &amp; E are irrelevant."
      },
      {
        "date": "2023-12-03T16:45:00.000Z",
        "voteCount": 1,
        "content": "The actual question is \"How should you build your container using Google Cloud services?\", so it doesn't mention how to deploy it.\n\nAlso, if we exclude B, how is the image build in C ending up at the central container repository?"
      },
      {
        "date": "2023-08-07T15:27:00.000Z",
        "voteCount": 1,
        "content": "C is packeto build pack to create an image. This is a very efficient way of creating images explicitly. \nD is through gcloud run command. Not sure what framework cloud build uses to create an image."
      },
      {
        "date": "2023-02-26T07:49:00.000Z",
        "voteCount": 1,
        "content": "i choose cd"
      },
      {
        "date": "2023-01-30T05:17:00.000Z",
        "voteCount": 1,
        "content": "pack build [IMAGE-NAME] --builder [BUILDER-IMAGE] --path [APPLICATION-DIRECTORY]\ngcloud run deploy [SERVICE-NAME] --image [IMAGE-NAME] --source [APPLICATION-DIRECTORY]"
      },
      {
        "date": "2023-01-30T04:41:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/run/docs/deploying-source-code"
      },
      {
        "date": "2023-01-27T08:07:00.000Z",
        "voteCount": 3,
        "content": "A and B are the correct options because they both involve using Cloud Build to build the container image.\n\nOption A, Push your source code to Artifact Registry, allows you to store the source code of your application in a central location, making it easier to manage and version control.\n\nOption B, Submit a Cloud Build job to push the image, allows you to use Cloud Build to build the container image, which is a recommended method for building container images in a production environment. It allows you to automate the build process, test the image, and push it to a container registry."
      },
      {
        "date": "2023-01-27T08:07:00.000Z",
        "voteCount": 3,
        "content": "Option C, Use the pack build command with pack CLI, is not correct because Cloud Run does not support the use of the pack CLI.\n\nOption D, Include the --source flag with the gcloud run deploy CLI command, is not correct because this flag is used to specify the source code location when deploying the application, not building the container.\n\nOption E, Include the --platform=kubernetes flag with the gcloud run deploy CLI command, is not correct because this flag is used to specify the platform when deploying the application on Kubernetes and not Cloud Run."
      },
      {
        "date": "2023-01-11T02:56:00.000Z",
        "voteCount": 1,
        "content": "The Cloud run use the buildpacks to automatically build container images from source code but you need to use source code flag so you need to add the --source flag to your command gcloud run deploy --source=/PATH/\n\nAnswer C &amp; D"
      },
      {
        "date": "2022-12-14T04:51:00.000Z",
        "voteCount": 1,
        "content": "CD is the answer.\n\nhttps://cloud.google.com/run/docs/deploying-source-code\nThis page describes how to deploy new services and new revisions to Cloud Run directly from source code using a single gcloud CLI command, gcloud run deploy with the --source flag.\n\nBehind the scenes, this command uses Google Cloud's buildpacks and Cloud Build to automatically build container images from your source code without having to install Docker on your machine or set up buildpacks or Cloud Build."
      },
      {
        "date": "2022-12-13T18:40:00.000Z",
        "voteCount": 2,
        "content": "C &amp; D are correct"
      },
      {
        "date": "2022-12-12T11:39:00.000Z",
        "voteCount": 4,
        "content": "C and D.\n\nC: Google Cloud for buildpacks\u2014an open-source technology that makes it fast and easy for you to create secure, production-ready container images from source code and without a Dockerfile.\nhttps://cloud.google.com/blog/products/containers-kubernetes/google-cloud-now-supports-buildpacks (also mentioned by TNT87)\n\nD: Deploying from source code. \"This page describes how to deploy new services and new revisions to Cloud Run directly from source code using a single gcloud CLI command, gcloud run deploy with the --source flag.\"\nhttps://cloud.google.com/run/docs/deploying-source-code\n\nA is incorrect because Artifact Registry is for container images, not source code.\nB is incorrect because only the built image is needed to be deployed to Cloud Run. \"A centrally managed container repository\" can be somewhere outside of Google, so as the build tool. It doesn't necessary to be built on Cloud Build.\nE is irrelevant in this case, as K8S is not involved in this question."
      },
      {
        "date": "2022-12-12T11:52:00.000Z",
        "voteCount": 1,
        "content": "Finally find something that excludes E as an answer: also in the buildpacks blog post, \"these buildpacks produce container images that follow best practices and are suitable for running on all of our container platforms: Cloud Run (fully managed), Anthos, and Google Kubernetes Engine (GKE)\"\n\nIf it deploys to Cloud Run, it needs to be fully managed. Then the platform cannot be \"kubernetes\" - use the default value \"managed\" instead. See https://cloud.google.com/sdk/gcloud/reference/run/deploy#--platform"
      },
      {
        "date": "2022-12-14T01:05:00.000Z",
        "voteCount": 1,
        "content": "Artifact registry, A"
      },
      {
        "date": "2022-12-14T01:07:00.000Z",
        "voteCount": 1,
        "content": "Artifact Registry does not support Docker chunked uploads. Some tools support uploading large images with either chunked uploads or a single monolithic upload."
      },
      {
        "date": "2022-12-14T01:09:00.000Z",
        "voteCount": 1,
        "content": "Re-read the question, we simply need a method  to deploy the application on Cloud Run without a Dockerfile.thats all"
      },
      {
        "date": "2022-12-04T03:50:00.000Z",
        "voteCount": 1,
        "content": "https://dev.to/alvardev/gcp-cloud-run-containers-without-dockerfile-2jh3"
      },
      {
        "date": "2022-12-04T09:46:00.000Z",
        "voteCount": 1,
        "content": "B is wrong sir"
      },
      {
        "date": "2022-12-04T14:26:00.000Z",
        "voteCount": 1,
        "content": "B, cant be."
      },
      {
        "date": "2022-12-02T15:39:00.000Z",
        "voteCount": 1,
        "content": "Answer A\nhttps://cloud.google.com/run/docs/deploying#images\n\nAnswer C\nhttps://cloud.google.com/blog/products/containers-kubernetes/google-cloud-now-supports-buildpacks"
      },
      {
        "date": "2022-12-16T17:20:00.000Z",
        "voteCount": 2,
        "content": "\"Push your source code to Artifact Registry\" -&gt; GAR is not used for source code"
      },
      {
        "date": "2023-01-03T00:28:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/artifact-registry/docs/integrate-cloud-run"
      },
      {
        "date": "2022-12-19T00:25:00.000Z",
        "voteCount": 1,
        "content": "How should you build your container using Google Cloud services?\nIts simply A, C"
      },
      {
        "date": "2022-12-20T07:34:00.000Z",
        "voteCount": 2,
        "content": "You cannot push source code to Artifact Registry. Cloud Source Repositories stores source code, while GAR stores build artifacts and dependencies."
      },
      {
        "date": "2022-12-27T03:24:00.000Z",
        "voteCount": 1,
        "content": "Note that source deployments use Artifact Registry to store built containers. If your project doesn't already have an Artifact Registry repository with the name cloud-run-source-deploy in the region you are deploying to, this feature automatically creates an Artifact Registry repository with the name cloud-run-source-deploy.\n\nRed this to understand, i can tell you have never done DevOps at all\nhttps://cloud.google.com/run/docs/deploying-source-code"
      },
      {
        "date": "2022-12-19T00:28:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/run/docs/deploying#images\nThis will help you and if you have done Devops you will understand this"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 220,
    "url": "https://www.examtopics.com/discussions/google/view/91188-exam-professional-cloud-developer-topic-1-question-220/",
    "body": "You work for an organization that manages an online ecommerce website. Your company plans to expand across the world; however, the estore currently serves one specific region. You need to select a SQL database and configure a schema that will scale as your organization grows. You want to create a table that stores all customer transactions and ensure that the customer (CustomerId) and the transaction (TransactionId) are unique. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Cloud SQL table that has TransactionId and CustomerId configured as primary keys. Use an incremental number for the TransactionId.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Cloud SQL table that has TransactionId and CustomerId configured as primary keys. Use a random string (UUID) for the Transactionid.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Cloud Spanner table that has TransactionId and CustomerId configured as primary keys. Use a random string (UUID) for the TransactionId.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Cloud Spanner table that has TransactionId and CustomerId configured as primary keys. Use an incremental number for the TransactionId."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-09-24T05:58:00.000Z",
        "voteCount": 1,
        "content": "C is correct."
      },
      {
        "date": "2023-08-07T15:31:00.000Z",
        "voteCount": 1,
        "content": "Requirement is to scale globally. Cloud spanner is the best fit. UUID as a transaction ID is good for security purpose...avoids guessing of next transaction ID."
      },
      {
        "date": "2023-02-20T06:44:00.000Z",
        "voteCount": 2,
        "content": "across the world -&gt; global/multi-region -&gt; spanner\nuuid for primary key"
      },
      {
        "date": "2023-01-27T07:58:00.000Z",
        "voteCount": 1,
        "content": "C. Create a Cloud Spanner table that has TransactionId and CustomerId configured as primary keys. Use a random string (UUID) for the TransactionId. This will ensure that the combination of CustomerId and TransactionId is unique, even as your organization grows and expands across the world. Cloud Spanner is a highly scalable and globally-distributed SQL database, making it well-suited for this use case. Using a UUID for the TransactionId will ensure that it is unique across all regions and customers."
      },
      {
        "date": "2023-01-11T03:14:00.000Z",
        "voteCount": 1,
        "content": "Answer C, cloud spanner for multi-region and uui primary key to be sure to be unique"
      },
      {
        "date": "2022-12-27T03:20:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/spanner/docs/schema-design#uuid_primary_key\nAnswer C"
      },
      {
        "date": "2022-12-13T23:31:00.000Z",
        "voteCount": 1,
        "content": "option C"
      },
      {
        "date": "2022-12-12T11:28:00.000Z",
        "voteCount": 1,
        "content": "Globally available --&gt; Cloud Spanner (multi-region). Cloud SQL is a regional service."
      },
      {
        "date": "2022-12-12T06:11:00.000Z",
        "voteCount": 1,
        "content": "C is the answer."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 221,
    "url": "https://www.examtopics.com/discussions/google/view/91187-exam-professional-cloud-developer-topic-1-question-221/",
    "body": "You are monitoring a web application that is written in Go and deployed in Google Kubernetes Engine. You notice an increase in CPU and memory utilization. You need to determine which source code is consuming the most CPU and memory resources. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDownload, install, and start the Snapshot Debugger agent in your VM. Take debug snapshots of the functions that take the longest time. Review the call stack frame, and identify the local variables at that level in the stack.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImport the Cloud Profiler package into your application, and initialize the Profiler agent. Review the generated flame graph in the Google Cloud console to identify time-intensive functions.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImport OpenTelemetry and Trace export packages into your application, and create the trace provider.<br>Review the latency data for your application on the Trace overview page, and identify where bottlenecks are occurring.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Cloud Logging query that gathers the web application's logs. Write a Python script that calculates the difference between the timestamps from the beginning and the end of the application's longest functions to identity time-intensive functions."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-09-24T06:00:00.000Z",
        "voteCount": 1,
        "content": "B is correct."
      },
      {
        "date": "2023-08-07T15:48:00.000Z",
        "voteCount": 1,
        "content": "Focus is to find which function is more CPU and Memory intensive. Flame graphs highlights the memory intensive functions in a graphical way. B is the best answer."
      },
      {
        "date": "2023-02-20T06:19:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/profiler/docs"
      },
      {
        "date": "2023-01-27T07:49:00.000Z",
        "voteCount": 1,
        "content": "B. Import the Cloud Profiler package into your application, and initialize the Profiler agent. Review the generated flame graph in the Google Cloud console to identify time-intensive functions.\n\nOption B is the best solution because it involves importing the Cloud Profiler package into the application, initializing the Profiler agent, and reviewing the generated flame graph in the Google Cloud console. This will allow you to identify time-intensive functions and determine which source code is consuming the most CPU and memory resources. The flame graph is a visualization of the call stack and it can be used to identify bottlenecks in the application.\nOption A and C are also related to profiling but they don't exactly focus on identifying time-intensive functions. Option D is not the best option because it would be more complex and less efficient than using a profiler."
      },
      {
        "date": "2022-12-27T03:18:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/profiler/docs/about-profiler#profiling_agent\nhttps://cloud.google.com/profiler/docs/about-profiler#environment_and_languages\nAnswer B"
      },
      {
        "date": "2022-12-13T23:32:00.000Z",
        "voteCount": 1,
        "content": "option B"
      },
      {
        "date": "2022-12-12T06:08:00.000Z",
        "voteCount": 1,
        "content": "B is the answer.\n\nhttps://cloud.google.com/profiler/docs/about-profiler\nCloud Profiler is a statistical, low-overhead profiler that continuously gathers CPU usage and memory-allocation information from your production applications. It attributes that information to the source code that generated it, helping you identify the parts of your application that are consuming the most resources, and otherwise illuminating your applications performance characteristics."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 222,
    "url": "https://www.examtopics.com/discussions/google/view/89759-exam-professional-cloud-developer-topic-1-question-222/",
    "body": "You have a container deployed on Google Kubernetes Engine. The container can sometimes be slow to launch, so you have implemented a liveness probe. You notice that the liveness probe occasionally fails on launch. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a startup probe.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the initial delay for the liveness probe.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the CPU limit for the container.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a readiness probe."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 12,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 8,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-04-16T09:08:00.000Z",
        "voteCount": 6,
        "content": "https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#define-startup-probes\nThe kubelet uses startup probes to know when a container application has started. If such a probe is configured, it disables liveness and readiness checks until it succeeds, making sure those probes don't interfere with the application startup. This can be used to adopt liveness checks on slow starting containers, avoiding them getting killed by the kubelet before they are up and running."
      },
      {
        "date": "2023-12-25T11:46:00.000Z",
        "voteCount": 2,
        "content": "\"Sometimes, you have to deal with legacy applications that might require an additional startup time on their first initialization. In such cases, it can be tricky to set up liveness probe parameters without compromising the fast response to deadlocks that motivated such a probe. The trick is to set up a startup probe with the same command\"\nhttps://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#define-startup-probes"
      },
      {
        "date": "2023-09-24T06:04:00.000Z",
        "voteCount": 1,
        "content": "A startup probe is a probe that Kubernetes uses to determine if a container has started successfully. If the startup probe fails, Kubernetes will restart the container."
      },
      {
        "date": "2023-08-07T16:52:00.000Z",
        "voteCount": 1,
        "content": "Readiness probe is the right answer. Likeness probe fails if it tries to probe a container not yet ready to serve the traffic. So we need to add readiness probe. There is no such thing called startup probe in kubernetes."
      },
      {
        "date": "2023-08-10T11:37:00.000Z",
        "voteCount": 1,
        "content": "Typo: Likeness probe... -&gt; Liveness probe"
      },
      {
        "date": "2023-10-11T11:17:00.000Z",
        "voteCount": 1,
        "content": "Startup probes have been enabled by default since v1.19."
      },
      {
        "date": "2023-02-20T06:20:00.000Z",
        "voteCount": 1,
        "content": "https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/\n\nCaution: Liveness probes do not wait for readiness probes to succeed. If you want to wait before executing a liveness probe you should use initialDelaySeconds or a startupProbe."
      },
      {
        "date": "2023-02-04T00:55:00.000Z",
        "voteCount": 1,
        "content": "A liveness probe checks if the container is running as expected, and if not, it restarts it. If the container is slow to launch, it may take some time for it to fully start up and be able to respond to the liveness probe. Increasing the initial delay for the liveness probe can help mitigate this issue by giving the container more time to start up before the probe begins checking its status. This can help reduce the likelihood of false-positive failures during launch."
      },
      {
        "date": "2023-01-27T07:45:00.000Z",
        "voteCount": 3,
        "content": "A. Adding a startup probe is useful for determining when a container has started, but it won't help with the problem of the liveness probe occasionally failing on launch.\n\nB. Increasing the initial delay for the liveness probe might help if the container is taking longer than the delay to start, but it's not a guaranteed solution.\n\nC. Increasing the CPU limit for the container may help if the container is running out of resources, but it may not be necessary if the issue is related to the container's initialization process.\n\nD. A readiness probe can help determine when a container is ready to receive traffic, but it won't help with the problem of the liveness probe occasionally failing on launch."
      },
      {
        "date": "2023-01-20T01:11:00.000Z",
        "voteCount": 1,
        "content": "To the people voting B:\nThe question specifically says that the problem occurs sometimes on launch, so how is the solution not a readiness probe?"
      },
      {
        "date": "2023-01-23T03:43:00.000Z",
        "voteCount": 1,
        "content": "provide a link to cement your argument"
      },
      {
        "date": "2023-04-16T09:02:00.000Z",
        "voteCount": 1,
        "content": "Should be A definitely\nhttps://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#define-startup-probes\nSometimes, you have to deal with legacy applications that might require an additional startup time on their first initialization. In such cases, it can be tricky to set up liveness probe parameters without compromising the fast response to deadlocks that motivated such a probe. The trick is to set up a startup probe with the same command, HTTP or TCP check, with a failureThreshold * periodSeconds long enough to cover the worse case startup time"
      },
      {
        "date": "2023-04-16T09:03:00.000Z",
        "voteCount": 1,
        "content": "Thanks to the startup probe, the application will have a maximum of 5 minutes (30 * 10 = 300s) to finish its startup. Once the startup probe has succeeded once, the liveness probe takes over to provide a fast response to container deadlocks. If the startup probe never succeeds, the container is killed after 300s and subject to the pod's restartPolicy."
      },
      {
        "date": "2023-01-21T09:50:00.000Z",
        "voteCount": 2,
        "content": "Changing to A:\n\nThe problem is that the liveness probes fires too early, so we need a startup probe to determine when liveness (and potential readiness) probe are valid.\n\nhttps://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/"
      },
      {
        "date": "2023-01-30T00:52:00.000Z",
        "voteCount": 1,
        "content": "https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/\nAnswer will remain B"
      },
      {
        "date": "2022-12-20T08:12:00.000Z",
        "voteCount": 2,
        "content": "B is the answer.\n\nhttps://cloud.google.com/blog/products/containers-kubernetes/kubernetes-best-practices-setting-up-health-checks-with-readiness-and-liveness-probes"
      },
      {
        "date": "2022-12-13T23:37:00.000Z",
        "voteCount": 1,
        "content": "option B \nhttps://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#configure-probes"
      },
      {
        "date": "2022-12-02T02:02:00.000Z",
        "voteCount": 3,
        "content": "Increase the Timeout of the Liveness Probe"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 223,
    "url": "https://www.examtopics.com/discussions/google/view/89809-exam-professional-cloud-developer-topic-1-question-223/",
    "body": "You work for an organization that manages an ecommerce site. Your application is deployed behind a global HTTP(S) load balancer. You need to test a new product recommendation algorithm. You plan to use A/B testing to determine the new algorithm\u2019s effect on sales in a randomized way. How should you test this feature?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSplit traffic between versions using weights.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable the new recommendation feature flag on a single instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMirror traffic to the new version of your application.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse HTTP header-based routing."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 13,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-09-24T06:08:00.000Z",
        "voteCount": 1,
        "content": "Splitting traffic between versions using weights is a common way to implement A/B testing. To do this, you would create two versions of your application, one with the new recommendation algorithm and one without. You would then configure the load balancer to split traffic between the two versions using weights. For example, you could configure the load balancer to send 50% of traffic to the new version and 50% of traffic to the old version."
      },
      {
        "date": "2023-08-07T16:55:00.000Z",
        "voteCount": 1,
        "content": "Split traffic is the right answer."
      },
      {
        "date": "2023-04-23T02:48:00.000Z",
        "voteCount": 1,
        "content": "in a randomized way - so its A, D otherwise"
      },
      {
        "date": "2023-02-20T06:22:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/traffic-director/docs/advanced-traffic-management#weight-based_traffic_splitting_for_safer_deployments"
      },
      {
        "date": "2023-02-20T06:22:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/traffic-director/docs/advanced-traffic-management#weight-based_traffic_splitting_for_safer_deployments"
      },
      {
        "date": "2022-12-13T23:37:00.000Z",
        "voteCount": 1,
        "content": "option A"
      },
      {
        "date": "2022-12-10T19:18:00.000Z",
        "voteCount": 1,
        "content": "A is the answer.\n\nhttps://cloud.google.com/load-balancing/docs/https/traffic-management-global#traffic_actions_weight-based_traffic_splitting\nDeploying a new version of an existing production service generally incurs some risk. Even if your tests pass in staging, you probably don't want to subject 100% of your users to the new version immediately. With traffic management, you can define percentage-based traffic splits across multiple backend services.\n\nFor example, you can send 95% of the traffic to the previous version of your service and 5% to the new version of your service. After you've validated that the new production version works as expected, you can gradually shift the percentages until 100% of the traffic reaches the new version of your service. Traffic splitting is typically used for deploying new versions, A/B testing, service migration, and similar processes."
      },
      {
        "date": "2022-12-04T03:59:00.000Z",
        "voteCount": 1,
        "content": "A is the answer"
      },
      {
        "date": "2022-12-02T14:54:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/traffic-director/docs/advanced-traffic-management#weight-based_traffic_splitting_for_safer_deployments"
      },
      {
        "date": "2022-12-02T14:50:00.000Z",
        "voteCount": 2,
        "content": "A is the recommended way to test A/B\n\nhttps://cloud.google.com/load-balancing/docs/https/traffic-management-global"
      },
      {
        "date": "2022-12-02T14:48:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/architecture/implementing-deployment-and-testing-strategies-on-gke#split_the_traffic_2\nhttps://cloud.google.com/load-balancing/docs/https/traffic-management-global#traffic_actions_weight-based_traffic_splitting"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 224,
    "url": "https://www.examtopics.com/discussions/google/view/89950-exam-professional-cloud-developer-topic-1-question-224/",
    "body": "You plan to deploy a new application revision with a Deployment resource to Google Kubernetes Engine (GKE) in production. The container might not work correctly. You want to minimize risk in case there are issues after deploying the revision. You want to follow Google-recommended best practices. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPerform a rolling update with a PodDisruptionBudget of 80%.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPerform a rolling update with a HorizontalPodAutoscaler scale-down policy value of 0.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConvert the Deployment to a StatefulSet, and perform a rolling update with a PodDisruptionBudget of 80%.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConvert the Deployment to a StatefulSet, and perform a rolling update with a HorizontalPodAutoscaler scale-down policy value of 0."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-09-24T06:16:00.000Z",
        "voteCount": 2,
        "content": "By performing a rolling update with a PDB of 80%, you can ensure that at least 80% of the Pods are always available during the deployment. This will minimize the risk of downtime in case there are issues with the new revision."
      },
      {
        "date": "2023-02-20T06:27:00.000Z",
        "voteCount": 2,
        "content": "https://kubernetes.io/docs/tasks/run-application/configure-pdb/#identify-an-application-to-protect"
      },
      {
        "date": "2023-02-04T01:03:00.000Z",
        "voteCount": 1,
        "content": "A rolling update with a PodDisruptionBudget (PDB) of 80% helps to minimize the risk of issues after deploying a new revision to a production environment in GKE. The PDB specifies the number of pods in a deployment that must remain available during an update, ensuring that there is sufficient capacity to handle any increase in traffic or demand. By setting a PDB of 80%, you ensure that at least 80% of the pods are available during the update, reducing the risk of disruption to your application. This is a recommended best practice by Google for deploying updates to production environments in GKE."
      },
      {
        "date": "2022-12-27T02:58:00.000Z",
        "voteCount": 1,
        "content": "https://kubernetes.io/docs/tutorials/kubernetes-basics/update/update-intro/\nhttps://cloud.google.com/blog/products/containers-kubernetes/ensuring-reliability-and-uptime-for-your-gke-cluster\nAnswer A"
      },
      {
        "date": "2022-12-08T06:08:00.000Z",
        "voteCount": 1,
        "content": "A is the answer.\n\nhttps://cloud.google.com/blog/products/containers-kubernetes/ensuring-reliability-and-uptime-for-your-gke-cluster\nSetting PodDisruptionBudget ensures that your workloads have a sufficient number of replicas, even during maintenance. Using the PDB, you can define a number (or percentage) of pods that can be terminated, even if terminating them brings the current replica count below the desired value. With PDB configured, Kubernetes will drain a node following the configured disruption schedule. New pods will be deployed on other available nodes. This approach ensures Kubernetes schedules workloads in an optimal way while controlling the disruption based on the PDB configuration."
      },
      {
        "date": "2022-12-04T04:05:00.000Z",
        "voteCount": 1,
        "content": "https://blog.knoldus.com/how-to-avoid-outages-in-your-kubernetes-cluster-using-pdb/"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 225,
    "url": "https://www.examtopics.com/discussions/google/view/89756-exam-professional-cloud-developer-topic-1-question-225/",
    "body": "Before promoting your new application code to production, you want to conduct testing across a variety of different users. Although this plan is risky, you want to test the new version of the application with production users and you want to control which users are forwarded to the new version of the application based on their operating system. If bugs are discovered in the new version, you want to roll back the newly deployed version of the application as quickly as possible.<br><br>What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy your application on Cloud Run. Use traffic splitting to direct a subset of user traffic to the new version based on the revision tag.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy your application on Google Kubernetes Engine with Anthos Service Mesh. Use traffic splitting to direct a subset of user traffic to the new version based on the user-agent header.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy your application on App Engine. Use traffic splitting to direct a subset of user traffic to the new version based on the IP address.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy your application on Compute Engine. Use Traffic Director to direct a subset of user traffic to the new version based on predefined weights."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 11,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-09-24T06:22:00.000Z",
        "voteCount": 2,
        "content": "Anthos Service Mesh is a fully managed service that provides a wide range of features for managing microservices, including traffic splitting. Traffic splitting allows you to distribute traffic between different versions of your application based on a variety of factors, such as the user-agent header."
      },
      {
        "date": "2023-08-07T17:06:00.000Z",
        "voteCount": 1,
        "content": "B is perfect. Key is split traffic based on type of OS. So that info can be retrieved with user-agent header."
      },
      {
        "date": "2022-12-14T05:13:00.000Z",
        "voteCount": 1,
        "content": "B is the answer."
      },
      {
        "date": "2022-12-12T08:47:00.000Z",
        "voteCount": 4,
        "content": "The key point for this question is the last two word of this statement \"you want to control which users are forwarded to the new version of the application based on their operating system\". Operating system. Where could the developers find the OS for a certain user? That's the User-Agent header. Example of a header: Mozilla/5.0 (Macintosh; Intel Mac OS X x.y; rv:42.0) Gecko/20100101 Firefox/42.0.\n\n- More info about the User-Agent header: https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/User-Agent"
      },
      {
        "date": "2022-12-04T04:11:00.000Z",
        "voteCount": 3,
        "content": "The requirement is \"you want to control which users are forwarded to the new version of the application based on their operating system\".\nhttps://cloud.google.com/traffic-director/docs/ingress-traffic#sending-traffic"
      },
      {
        "date": "2022-12-02T00:46:00.000Z",
        "voteCount": 1,
        "content": "You can use traffic splitting to specify a percentage distribution of traffic across two or more of the versions within a service. Splitting traffic allows you to conduct A/B testing between your versions and provides control over the pace when rolling out features.\n\nhttps://cloud.google.com/appengine/docs/legacy/standard/python/splitting-traffic#ip_address_splitting Answer C"
      },
      {
        "date": "2022-12-16T17:29:00.000Z",
        "voteCount": 4,
        "content": "C is based on IP address and not OS, so it cannot be the answer."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 226,
    "url": "https://www.examtopics.com/discussions/google/view/89755-exam-professional-cloud-developer-topic-1-question-226/",
    "body": "Your team is writing a backend application to implement the business logic for an interactive voice response (IVR) system that will support a payroll application. The IVR system has the following technical characteristics:<br><br>\u2022\tEach customer phone call is associated with a unique IVR session.<br>\u2022\tThe IVR system creates a separate persistent gRPC connection to the backend for each session.<br>\u2022\tIf the connection is interrupted, the IVR system establishes a new connection, causing a slight latency for that call.<br><br>You need to determine which compute environment should be used to deploy the backend application. Using current call data, you determine that:<br><br>\u2022\tCall duration ranges from 1 to 30 minutes.<br>\u2022\tCalls are typically made during business hours.<br>\u2022\tThere are significant spikes of calls around certain known dates (e.g., pay days), or when large payroll changes occur.<br><br>You want to minimize cost, effort, and operational overhead. Where should you deploy the backend application?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCompute Engine",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGoogle Kubernetes Engine cluster in Standard mode",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Functions",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Run\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-09-24T06:26:00.000Z",
        "voteCount": 1,
        "content": "D is correct."
      },
      {
        "date": "2023-08-07T23:20:00.000Z",
        "voteCount": 1,
        "content": "Cloud Run is more suitable for gRPC communication between micro services.\nThe key here is \"gRPC connection to the backend for each session\"."
      },
      {
        "date": "2023-01-11T03:33:00.000Z",
        "voteCount": 1,
        "content": "Answer D"
      },
      {
        "date": "2022-12-08T05:59:00.000Z",
        "voteCount": 2,
        "content": "D is the answer."
      },
      {
        "date": "2022-12-02T00:23:00.000Z",
        "voteCount": 2,
        "content": "Answer D\nThis page shows Cloud Run-specific details for developers who want to use gRPC to connect a Cloud Run service with other services, for example, to provide simple, high performance communication between internal microservices. You can use all gRPC types, streaming or unary, with Cloud Run.\n\nPossible use cases include:\n\nCommunication between internal microservices.\nHigh loads of data (gRPC uses protocol buffers, which are up to seven times faster than REST calls).\nOnly a simple service definition is needed, you don't want to write a full client library.\nUse streaming gRPCs in your gRPC server to build more responsive applications and APIs.\nhttps://cloud.google.com/run/docs/tutorials/secure-services#:~:text=The%20backend%20service%20is%20private,Google%20Cloud%20except%20where%20necessary."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 227,
    "url": "https://www.examtopics.com/discussions/google/view/89753-exam-professional-cloud-developer-topic-1-question-227/",
    "body": "You are developing an application hosted on Google Cloud that uses a MySQL relational database schema. The application will have a large volume of reads and writes to the database and will require backups and ongoing capacity planning. Your team does not have time to fully manage the database but can take on small administrative tasks. How should you host the database?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Cloud SQL to host the database, and import the schema into Cloud SQL.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy MySQL from the Google Cloud Marketplace to the database using a client, and import the schema.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Bigtable to host the database, and import the data into Bigtable.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Cloud Spanner to host the database, and import the schema into Cloud Spanner.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Firestore to host the database, and import the data into Firestore."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 15,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-09-24T06:34:00.000Z",
        "voteCount": 1,
        "content": "It is a good choice for applications that require a high volume of reads and writes, as well as regular backups and capacity planning."
      },
      {
        "date": "2023-08-07T23:22:00.000Z",
        "voteCount": 1,
        "content": "I go with A, since it is a Cloud SQL is a fully managed service that involves less operational overheads."
      },
      {
        "date": "2023-04-16T10:07:00.000Z",
        "voteCount": 3,
        "content": "A or D\ncloud sql ideal for heavy reads and not ideal for heavy writes\nspanner ideal for both reads/writes but more about global\nanyway both are extremely fast - then go for A"
      },
      {
        "date": "2023-02-19T11:40:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/sql/docs/mysql -&gt; no time to manage DB therefore use a manage service"
      },
      {
        "date": "2023-01-27T07:23:00.000Z",
        "voteCount": 1,
        "content": "The best option to host the MySQL relational database schema on Google Cloud while minimizing management overhead and maximizing the ability to handle a large volume of reads and writes, backups and ongoing capacity planning would be:\n\nA. Configure Cloud SQL to host the database, and import the schema into Cloud SQL.\n\nCloud SQL is a fully-managed service that makes it easy to set up, maintain, manage, and administer your relational databases on Google Cloud. It is specifically designed for MySQL, so it is a good fit for this use case. With Cloud SQL, you can automatically backup your data, and perform capacity planning, so you don't have to worry about managing the infrastructure. Additionally, Cloud SQL provides high availability, automatic failover and easy scaling.\n\nOption D and E are not correct since Cloud Spanner is a NoSQL database and Firestore is a document-based database and not suitable for Relational Database."
      },
      {
        "date": "2023-01-27T07:23:00.000Z",
        "voteCount": 1,
        "content": "Option C is not the best option to host the MySQL relational database schema because Bigtable is not a relational database. It is a NoSQL, wide-column store database that is designed to handle large amounts of data with low latency. It is not a good fit for this use case because it does not provide the same level of support for relational data structures and SQL queries that a relational database like MySQL would. Additionally, Bigtable is not designed for handling large volumes of reads and writes, backups and ongoing capacity planning."
      },
      {
        "date": "2023-01-11T03:38:00.000Z",
        "voteCount": 1,
        "content": "A, cloud SQL is easy to put in place from an other relational database"
      },
      {
        "date": "2022-12-12T08:23:00.000Z",
        "voteCount": 2,
        "content": "The answer A is more likely to be the correct one. Although Cloud Spanner is also a relational DB service (and has certain advantages over Cloud SQL), migrating from MySQL to Cloud Spanner is not as trivial as \"import the schema\" (stating by the answer D). If D has been excluded, the only relational DB option in the answers is A: Cloud SQL.\nhttps://cloud.google.com/spanner/docs/migrating-mysql-to-spanner#migration-process"
      },
      {
        "date": "2022-12-08T05:58:00.000Z",
        "voteCount": 2,
        "content": "A is the answer.\n\nhttps://cloud.google.com/sql/docs/mysql\nCloud SQL for MySQL is a fully-managed database service that helps you set up, maintain, manage, and administer your MySQL relational databases on Google Cloud Platform."
      },
      {
        "date": "2022-12-02T00:02:00.000Z",
        "voteCount": 3,
        "content": "Cloud SQL: Cloud SQL is a web service that allows you to create, configure, and use relational databases that live in Google's cloud. It is a fully-managed service that maintains, manages, and administers your databases, allowing you to focus on your applications and services.\n\nAnswer A"
      },
      {
        "date": "2022-12-01T23:48:00.000Z",
        "voteCount": 1,
        "content": "a large volume of reads and writes to the database and will require backups and ongoing capacity planning. Thats Bigtable. Changing my answer to C"
      },
      {
        "date": "2022-12-01T23:52:00.000Z",
        "voteCount": 1,
        "content": "But Bigtable doesnt use MySql.... aiiii will stick to A"
      },
      {
        "date": "2022-12-01T23:46:00.000Z",
        "voteCount": 1,
        "content": "Answer A"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 228,
    "url": "https://www.examtopics.com/discussions/google/view/89750-exam-professional-cloud-developer-topic-1-question-228/",
    "body": "You are developing a new web application using Cloud Run and committing code to Cloud Source Repositories. You want to deploy new code in the most efficient way possible. You have already created a Cloud Build YAML file that builds a container and runs the following command: gcloud run deploy. What should you do next?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Pub/Sub topic to be notified when code is pushed to the repository. Create a Pub/Sub trigger that runs the build file when an event is published to the topic.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a build trigger that runs the build file in response to a repository code being pushed to the development branch.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a webhook build trigger that runs the build file in response to HTTP POST calls to the webhook URL.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Cron job that runs the following command every 24 hours: gcloud builds submit."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-13T23:46:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/build/docs/automating-builds/create-manage-triggers"
      },
      {
        "date": "2023-09-24T06:38:00.000Z",
        "voteCount": 1,
        "content": "B is correct."
      },
      {
        "date": "2023-08-07T23:30:00.000Z",
        "voteCount": 1,
        "content": "I go with B.\nCode commit to the repository should trigger the build process.\nC is complicated because of webhook POST Url"
      },
      {
        "date": "2023-02-19T11:40:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/build/docs/triggers"
      },
      {
        "date": "2022-12-08T05:56:00.000Z",
        "voteCount": 1,
        "content": "B is the answer.\n\nhttps://cloud.google.com/build/docs/triggers\nCloud Build uses build triggers to enable CI/CD automation. You can configure triggers to listen for incoming events, such as when a new commit is pushed to a repository or when a pull request is initiated, and then automatically execute a build when new events come in. You can also configure triggers to build code on any changes to your source repository or only on changes that match certain criteria."
      },
      {
        "date": "2022-12-01T23:36:00.000Z",
        "voteCount": 1,
        "content": "Cloud Build enables you to build the container image, store the built image in Container Registry, and then deploy the image to Cloud Run."
      },
      {
        "date": "2022-12-01T23:16:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/build/docs/automating-builds/create-manage-triggers#connect_repo\nAnswer B"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 229,
    "url": "https://www.examtopics.com/discussions/google/view/97619-exam-professional-cloud-developer-topic-1-question-229/",
    "body": "You are a developer at a large organization. You are deploying a web application to Google Kubernetes Engine (GKE). The DevOps team has built a CI/CD pipeline that uses Cloud Deploy to deploy the application to Dev, Test, and Prod clusters in GKE. After Cloud Deploy successfully deploys the application to the Dev cluster, you want to automatically promote it to the Test cluster. How should you configure this process following Google-recommended best practices?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a Cloud Build trigger that listens for SUCCEEDED Pub/Sub messages from the clouddeploy-operations topic.<br>2. Configure Cloud Build to include a step that promotes the application to the Test cluster.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a Cloud Function that calls the Google Cloud Deploy API to promote the application to the Test cluster.<br>2. Configure this function to be triggered by SUCCEEDED Pub/Sub messages from the cloud-builds topic.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a Cloud Function that calls the Google Cloud Deploy API to promote the application to the Test cluster.<br>2. Configure this function to be triggered by SUCCEEDED Pub/Sub messages from the clouddeploy-operations topic.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a Cloud Build pipeline that uses the gke-deploy builder.<br>2. Create a Cloud Build trigger that listens for SUCCEEDED Pub/Sub messages from the cloud-builds topic.<br>3. Configure this pipeline to run a deployment step to the Test cluster."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 8,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 7,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-08-28T06:27:00.000Z",
        "voteCount": 1,
        "content": "Again, question from googles exam practice questions. A is correct."
      },
      {
        "date": "2024-07-13T11:37:00.000Z",
        "voteCount": 2,
        "content": "Answer from google's exam practice questions: Correct answer\nA. 1. Create a Cloud Build trigger that listens for SUCCEEDED Pub/Sub messages from the clouddeploy-operations topic. 2. Configure Cloud Build to include a step that promotes the application to the Test cluster."
      },
      {
        "date": "2023-12-30T12:43:00.000Z",
        "voteCount": 3,
        "content": "C in answer. A cannot be the answer as it mention \"cloud build\" and the question talk about cloud deploy."
      },
      {
        "date": "2023-09-24T06:44:00.000Z",
        "voteCount": 1,
        "content": "I think it should be C."
      },
      {
        "date": "2023-08-07T23:36:00.000Z",
        "voteCount": 1,
        "content": "Its either A or D.  A is better since the topic is clouddeploy-operations.  Once the msg is being published to this topic that means deployment has been done successfully to the environment, so the next step is to deploy the containers in the test cluster.  So I go with option A."
      },
      {
        "date": "2023-04-16T10:32:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/functions/docs/calling/pubsub\nhttps://cloud.google.com/deploy/docs/integrating#integrating_with_automated_testing\ncloud deploy sends message\ncloud build reads this message"
      },
      {
        "date": "2023-02-26T07:33:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/build/docs/automate-builds-pubsub-events#console_2"
      },
      {
        "date": "2023-02-28T00:54:00.000Z",
        "voteCount": 1,
        "content": "Cloud Build Pub/Sub triggers enable you to execute builds in response to Google Cloud events published over Pub/Sub. You can use information from a Pub/Sub event to parameterize your build and to decide if a build should execute in response to the event. Pub/Sub triggers can be configured to listen to any Pub/Sub topic."
      },
      {
        "date": "2023-02-05T13:59:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/functions/docs/calling/pubsub"
      },
      {
        "date": "2023-02-04T01:20:00.000Z",
        "voteCount": 2,
        "content": "This option (C) is recommended because it follows the best practice of using a serverless function, specifically Cloud Functions, for triggering automated tasks in response to events. In this case, the function will be triggered by a SUCCEEDED message from the clouddeploy-operations topic, indicating that the deployment to the Dev cluster has completed successfully. The function will then use the Google Cloud Deploy API to promote the application to the Test cluster.\n\nUsing a Cloud Function in this way allows for a scalable, event-driven architecture and reduces the amount of infrastructure required to manage the deployment process."
      },
      {
        "date": "2023-02-04T01:30:00.000Z",
        "voteCount": 3,
        "content": "Option A is not better than Option C because it involves using Cloud Build instead of Cloud Functions for the deployment promotion process. While Cloud Build is a powerful tool for building and testing applications, it is generally not recommended for triggering automated tasks in response to events like the successful deployment of an application to a cluster.\n\nIn Option A, the Cloud Build trigger listens for SUCCEEDED Pub/Sub messages from the clouddeploy-operations topic, and then promotes the application to the Test cluster as part of the Cloud Build pipeline. This approach involves using a more complex and less scalable infrastructure than using a serverless function like Cloud Functions.\n\nOn the other hand, Option C uses a Cloud Function to promote the application, which is a more streamlined, scalable, and event-driven solution. Cloud Functions are designed specifically for triggering automated tasks in response to events, making them a better choice for this type of use case."
      },
      {
        "date": "2023-02-05T13:52:00.000Z",
        "voteCount": 2,
        "content": "Your explanation is not from the documentation sir, kindly provide a link to your answer. Do you also understtand what and when to use cloud build??"
      },
      {
        "date": "2023-02-01T23:18:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/deploy/docs/integrating#integrating_with_automated_testing\nhttps://cloud.google.com/deploy/docs/integrating#before_you_begin"
      },
      {
        "date": "2023-02-05T13:58:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/functions/docs/calling/pubsub\nmhh i see why it could be C"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 230,
    "url": "https://www.examtopics.com/discussions/google/view/97626-exam-professional-cloud-developer-topic-1-question-230/",
    "body": "Your application is running as a container in a Google Kubernetes Engine cluster. You need to add a secret to your application using a secure approach. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Kubernetes Secret, and pass the Secret as an environment variable to the container.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Application-layer Secret Encryption on the cluster using a Cloud Key Management Service (KMS) key.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the credential in Cloud KMS. Create a Google service account (GSA) to read the credential from Cloud KMS. Export the GSA as a .json file, and pass the .json file to the container as a volume which can read the credential from Cloud KMS.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the credential in Secret Manager. Create a Google service account (GSA) to read the credential from Secret Manager. Create a Kubernetes service account (KSA) to run the container. Use Workload Identity to configure your KSA to act as a GSA.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 7,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-12-05T08:10:00.000Z",
        "voteCount": 1,
        "content": "Alternative D is correct.\nProblem I see with alternative A is that storing secrets in Kubernetes Secrets in plain text format is not aligned with best practices, as such secrets are base64 encoded but not encrypted at rest. If a malicious agent gains access to the cluster, secrets can be easily decodes and captured."
      },
      {
        "date": "2023-09-24T06:52:00.000Z",
        "voteCount": 1,
        "content": "D is correct."
      },
      {
        "date": "2023-08-07T23:44:00.000Z",
        "voteCount": 1,
        "content": "What I have seen till now and done till now is option A.  So I go with option A.  What is the best secure approach between A and D, I am not sure.  So, very doubtfully I go with A."
      },
      {
        "date": "2023-02-19T11:49:00.000Z",
        "voteCount": 2,
        "content": "https://kubernetes.io/docs/concepts/configuration/secret/"
      },
      {
        "date": "2023-02-12T05:39:00.000Z",
        "voteCount": 4,
        "content": "A is not correct because a Kubernetes Secret only encodes the string, and anyone who can read the secret will be able to decode it."
      },
      {
        "date": "2023-02-06T08:49:00.000Z",
        "voteCount": 2,
        "content": "D is best answer:  You should not store secrets in k8s secrets:\nhttps://kubernetes.io/docs/concepts/configuration/secret/\nThey are for environment variables."
      },
      {
        "date": "2023-02-04T01:34:00.000Z",
        "voteCount": 2,
        "content": "Using D would also be a secure approach. Option D uses a combination of Google Secret Manager, Google Service Account, and Workload Identity to store and retrieve secrets securely. The Workload Identity enables the Kubernetes Service Account to act as the Google Service Account, which has the required permissions to read the secrets from Secret Manager.\n\nBoth options A and D are secure ways to store and retrieve secrets in a Kubernetes cluster, but option A is simpler and requires fewer steps. It may be more appropriate for smaller or less complex environments, while option D provides more advanced security and management features and is more suitable for larger and more complex environments."
      },
      {
        "date": "2023-02-11T06:37:00.000Z",
        "voteCount": 1,
        "content": "It is the D option\nhttps://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity\n\nhttps://cloud.google.com/secret-manager/docs/overview"
      },
      {
        "date": "2023-02-12T05:39:00.000Z",
        "voteCount": 1,
        "content": "A is not correct because a Kubernetes Secret only encodes the string, and anyone who can read the secret will be able to decode it."
      },
      {
        "date": "2023-02-02T00:13:00.000Z",
        "voteCount": 1,
        "content": "Secrets can be mounted as data volumes or exposed as environment variables to be used by a container in a Pod. Secrets can also be used by ...\nhttps://cloud.google.com/secret-manager/docs/best-practices\nhttps://kubernetes.io/docs/concepts/security/secrets-good-practices/"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 231,
    "url": "https://www.examtopics.com/discussions/google/view/97535-exam-professional-cloud-developer-topic-1-question-231/",
    "body": "You are a developer at a financial institution. You use Cloud Shell to interact with Google Cloud services. User data is currently stored on an ephemeral disk; however, a recently passed regulation mandates that you can no longer store sensitive information on an ephemeral disk. You need to implement a new storage solution for your user data. You want to minimize code changes. Where should you store your user data?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore user data on a Cloud Shell home disk, and log in at least every 120 days to prevent its deletion.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore user data on a persistent disk in a Compute Engine instance.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore user data in a Cloud Storage bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore user data in BigQuery tables."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-08-07T23:53:00.000Z",
        "voteCount": 1,
        "content": "Persistent disk is the right option to store sensitive info in this case.  ( Obviously in the general sense, we should store user data in the data store )\nKey points:\n1) You use Cloud Shell to interact with Google Cloud services\n2) You want to minimize code changes"
      },
      {
        "date": "2023-02-06T08:52:00.000Z",
        "voteCount": 3,
        "content": "C is best answer:\nUsing gsfuse: https://github.com/GoogleCloudPlatform/gcsfuse you can have Cloud Shell interact with Cloud Storage directly as a drive.   You don't need to redesign or recode or move your app from Cloud Shell.  This is the same approach that dataproc uses to leverage GCS as a storage solution."
      },
      {
        "date": "2023-02-11T01:17:00.000Z",
        "voteCount": 1,
        "content": "No , kindly read the documentation about cloud shell and what it is them you will know why B is the answer"
      },
      {
        "date": "2023-02-04T01:48:00.000Z",
        "voteCount": 4,
        "content": "Store user data in a Cloud Storage bucket is a good option for storing large amounts of data, but if you need to minimize code changes, using a persistent disk in a Compute Engine instance may be a better fit as it provides a more direct replacement for an ephemeral disk with similar access patterns, which will likely require fewer changes to your existing code. Storing user data in a Cloud Storage bucket would likely require more significant changes to how your application interacts with the data."
      },
      {
        "date": "2023-02-01T05:29:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/shell/docs/how-cloud-shell-works#persistent_disk_storage\nHow Cloud Shell works\n\nbookmark_border\nCloud Shell provisions a Compute Engine virtual machine running a Debian-based Linux operating system for your temporary use. This virtual machine is owned and managed by Google Cloud, so will not appear within any of your GCP projects.\nhttps://cloud.google.com/shell/docs/how-cloud-shell-works"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 232,
    "url": "https://www.examtopics.com/discussions/google/view/97532-exam-professional-cloud-developer-topic-1-question-232/",
    "body": "You recently developed a web application to transfer log data to a Cloud Storage bucket daily. Authenticated users will regularly review logs from the prior two weeks for critical events. After that, logs will be reviewed once annually by an external auditor. Data must be stored for a period of no less than 7 years. You want to propose a storage solution that meets these requirements and minimizes costs. What should you do? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Bucket Lock feature to set the retention policy on the data.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun a scheduled job to set the storage class to Coldline for objects older than 14 days.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a JSON Web Token (JWT) for users needing access to the Coldline storage buckets.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a lifecycle management policy to set the storage class to Coldline for objects older than 14 days.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a lifecycle management policy to set the storage class to Nearline for objects older than 14 days."
    ],
    "answer": "AD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AD",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-08-08T00:10:00.000Z",
        "voteCount": 1,
        "content": "A -&gt; Data must be stored for a period of no less than 7 years.\nD -&gt; Authenticated users will regularly review logs from the prior two weeks for critical events. After that, logs will be reviewed once annually by an external auditor."
      },
      {
        "date": "2023-08-08T00:12:00.000Z",
        "voteCount": 1,
        "content": "D. Create a lifecycle management policy to set the storage class to Coldline for objects older than 14 days \nThis should be like setting the storage class to Archival for objects older than 14 days, since logs will be reviewed once annually by an external auditor.  But the close answer is Coldline in Option D."
      },
      {
        "date": "2023-02-19T11:26:00.000Z",
        "voteCount": 1,
        "content": "lock to avoid deletion before 7 years\nlifecycle policy to change to coldline (since it will be accessed anually) after 14 days."
      },
      {
        "date": "2023-02-04T01:53:00.000Z",
        "voteCount": 3,
        "content": "The requirement of storing data for a period of no less than 7 years can be met by setting the retention policy for the data in the Cloud Storage bucket. This can be done using the Bucket Lock feature (A) or a lifecycle management policy (D), which can be set to retain the objects for the required period of 7 years."
      },
      {
        "date": "2023-02-01T05:17:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/storage/docs/bucket-lock\nhttps://cloud.google.com/storage/docs/lifecycle"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 233,
    "url": "https://www.examtopics.com/discussions/google/view/97524-exam-professional-cloud-developer-topic-1-question-233/",
    "body": "Your team is developing a Cloud Function triggered by Cloud Storage events. You want to accelerate testing and development of your Cloud Function while following Google-recommended best practices. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new Cloud Function that is triggered when Cloud Audit Logs detects the cloudfunctions.functions.sourceCodeSet operation in the original Cloud Function. Send mock requests to the new function to evaluate the functionality.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMake a copy of the Cloud Function, and rewrite the code to be HTTP-triggered. Edit and test the new version by triggering the HTTP endpoint. Send mock requests to the new function to evaluate the functionality.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall the Functions Frameworks library, and configure the Cloud Function on localhost. Make a copy of the function, and make edits to the new version. Test the new version using curl.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMake a copy of the Cloud Function in the Google Cloud console. Use the Cloud console's in-line editor to make source code changes to the new function. Modify your web application to call the new function, and test the new version in production"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 4,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-11-20T15:01:00.000Z",
        "voteCount": 1,
        "content": "Making a copy of the function for edits ensures that your changes do not affect the original function in production. It provides a controlled environment for development and testing.\ncurl Testing:\n\nTesting the new version using curl is a simple and effective way to send mock requests and evaluate the functionality of your Cloud Function locally.\nUsing the Functions Frameworks library and local testing provides a development environment that is both efficient and aligned with Google-recommended best practices for Cloud Functions development."
      },
      {
        "date": "2023-08-08T00:26:00.000Z",
        "voteCount": 1,
        "content": "Option C is well suited for testing cloud functions in the local environment.\nhttps://cloud.google.com/functions/docs/running/function-frameworks"
      },
      {
        "date": "2023-07-31T06:52:00.000Z",
        "voteCount": 1,
        "content": "Not B because \n\"Local testing\nMany development paradigms depend on being able to test your code relatively quickly.\n\nBecause testing code on Cloud Functions itself involves waiting for deployed code and log entries to become available, running and testing your function on your development machine can make the testing process (and, in turn, the development process) significantly faster.\"\nC because: https://cloud.google.com/functions/docs/running/function-frameworks"
      },
      {
        "date": "2023-04-16T10:48:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/functions/docs/running/calling#cloudevent_functions"
      },
      {
        "date": "2023-02-28T00:35:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/functions/docs/running/overview#choosing_an_abstraction_layer\nhttps://cloud.google.com/functions/docs/running/function-frameworks\nhttps://cloud.google.com/functions/docs/running/calling#cloudevent-function-curl-tabs-storage"
      },
      {
        "date": "2023-02-01T01:58:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/functions/docs/writing/write-event-driven-functions\nhttps://cloud.google.com/functions/docs/calling/storage"
      },
      {
        "date": "2023-02-01T02:01:00.000Z",
        "voteCount": 1,
        "content": "https://firebase.google.com/docs/functions/gcp-storage-events"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 234,
    "url": "https://www.examtopics.com/discussions/google/view/97523-exam-professional-cloud-developer-topic-1-question-234/",
    "body": "Your team is setting up a build pipeline for an application that will run in Google Kubernetes Engine (GKE). For security reasons, you only want images produced by the pipeline to be deployed to your GKE cluster. Which combination of Google Cloud services should you use?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Build, Cloud Storage, and Binary Authorization",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGoogle Cloud Deploy, Cloud Storage, and Google Cloud Armor",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGoogle Cloud Deploy, Artifact Registry, and Google Cloud Armor",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Build, Artifact Registry, and Binary Authorization\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-08T20:54:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/architecture/app-development-and-delivery-with-cloud-code-gcb-cd-and-gke#architecture"
      },
      {
        "date": "2023-09-24T07:05:00.000Z",
        "voteCount": 1,
        "content": "D is correct."
      },
      {
        "date": "2023-08-08T00:31:00.000Z",
        "voteCount": 1,
        "content": "D is the right option.\nCloud Build: To build code and push image into Artifactory\nArtifact Registry: A store for built images\nBinary Authorization: Approval for deployment"
      },
      {
        "date": "2023-02-23T02:30:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/binary-authorization/docs/cloud-build"
      },
      {
        "date": "2023-02-19T11:33:00.000Z",
        "voteCount": 1,
        "content": "i choose D"
      },
      {
        "date": "2023-02-01T01:30:00.000Z",
        "voteCount": 1,
        "content": "I'd go with D\nhttps://cloud.google.com/architecture/app-development-and-delivery-with-cloud-code-gcb-cd-and-gke#objectives"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 235,
    "url": "https://www.examtopics.com/discussions/google/view/97521-exam-professional-cloud-developer-topic-1-question-235/",
    "body": "You are supporting a business-critical application in production deployed on Cloud Run. The application is reporting HTTP 500 errors that are affecting the usability of the application. You want to be alerted when the number of errors exceeds 15% of the requests within a specific time window. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Cloud Function that consumes the Cloud Monitoring API. Use Cloud Scheduler to trigger the Cloud Function daily and alert you if the number of errors is above the defined threshold.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNavigate to the Cloud Run page in the Google Cloud console, and select the service from the services list. Use the Metrics tab to visualize the number of errors for that revision, and refresh the page daily.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an alerting policy in Cloud Monitoring that alerts you if the number of errors is above the defined threshold.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Cloud Function that consumes the Cloud Monitoring API. Use Cloud Composer to trigger the Cloud Function daily and alert you if the number of errors is above the defined threshold."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 7,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-07T06:29:00.000Z",
        "voteCount": 2,
        "content": "C is 100% correct answer, practically used"
      },
      {
        "date": "2023-09-24T07:13:00.000Z",
        "voteCount": 1,
        "content": "C is correct."
      },
      {
        "date": "2023-08-08T00:43:00.000Z",
        "voteCount": 2,
        "content": "C is a right answer.\nCreate an alert policy that alerts you if the number of errors exceeds 15% of the requests within a specific time window. Simple and straight forward."
      },
      {
        "date": "2023-02-19T11:36:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/run/docs/monitoring#custom-metrics \nhttps://cloud.google.com/run/docs/monitoring#add_alerts\nhttps://cloud.google.com/monitoring/alerts"
      },
      {
        "date": "2023-02-04T02:13:00.000Z",
        "voteCount": 3,
        "content": "Option A involves creating a Cloud Function that is triggered by Cloud Scheduler, but this option does not fully address the requirement of being alerted if the number of errors exceeds a specific threshold. Option A requires manual checking of the error count, whereas option C provides a more automated solution by setting up an alerting policy in Cloud Monitoring that sends an alert if the number of errors exceeds the defined threshold."
      },
      {
        "date": "2023-02-05T13:48:00.000Z",
        "voteCount": 1,
        "content": "How is A involving manual checking cause this is all automated processes???"
      },
      {
        "date": "2023-02-08T10:40:00.000Z",
        "voteCount": 2,
        "content": "Option A involves setting up multiple Google Cloud Platform (GCP) services, including Cloud Function, Cloud Scheduler, and the Cloud Monitoring API, to monitor the error count and send alerts. This setup process is manual, and the error count must be manually checked after the Cloud Function is triggered by Cloud Scheduler. This approach may not be ideal for business-critical applications that require real-time monitoring and automated alerting."
      },
      {
        "date": "2023-02-01T01:17:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/monitoring/alerts/policies-in-api#metric-polices\nA has all 3 requirements , C is also good , but i will go with A"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 236,
    "url": "https://www.examtopics.com/discussions/google/view/97519-exam-professional-cloud-developer-topic-1-question-236/",
    "body": "You need to build a public API that authenticates, enforces quotas, and reports metrics for API callers. Which tool should you use to complete this architecture?<br><br><img src=\"https://img.examtopics.com/professional-cloud-developer/image7.png\">",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tApp Engine",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Endpoints\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIdentity-Aware Proxy",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGKE Ingress for HTTP(S) Load Balancing"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-11-12T21:17:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2023-08-08T04:27:00.000Z",
        "voteCount": 1,
        "content": "Cloud endpoints is the right answer.\ncloud.google.com/endpoints/docs/frameworks/quotas-configure"
      },
      {
        "date": "2023-02-19T11:02:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/endpoints"
      },
      {
        "date": "2023-02-01T01:04:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/endpoints/docs/openapi/quotas-overview"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 237,
    "url": "https://www.examtopics.com/discussions/google/view/97518-exam-professional-cloud-developer-topic-1-question-237/",
    "body": "You noticed that your application was forcefully shut down during a Deployment update in Google Kubernetes Engine. Your application didn\u2019t close the database connection before it was terminated. You want to update your application to make sure that it completes a graceful shutdown. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate your code to process a received SIGTERM signal to gracefully disconnect from the database.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a PodDisruptionBudget to prevent the Pod from being forcefully shut down.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the terminationGracePeriodSeconds for your application.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a PreStop hook to shut down your application."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 8,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-09-24T07:45:00.000Z",
        "voteCount": 1,
        "content": "This is the most direct and effective way to ensure that your application completes a graceful shutdown. When your application receives a SIGTERM signal, it should use this signal as a trigger to disconnect from the database and complete any other necessary tasks before terminating."
      },
      {
        "date": "2023-08-08T04:32:00.000Z",
        "voteCount": 1,
        "content": "A is right.  After caching the SIGTERM event that raised by Pod shutdown, we need to release the DB connection."
      },
      {
        "date": "2023-04-23T03:44:00.000Z",
        "voteCount": 2,
        "content": "A is a best practice\nhttps://cloud.google.com/blog/products/containers-kubernetes/kubernetes-best-practices-terminating-with-grace"
      },
      {
        "date": "2023-02-28T00:18:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/blog/products/containers-kubernetes/kubernetes-best-practices-terminating-with-grace"
      },
      {
        "date": "2023-02-19T11:09:00.000Z",
        "voteCount": 1,
        "content": "i would choose A"
      },
      {
        "date": "2023-02-04T02:40:00.000Z",
        "voteCount": 2,
        "content": "While a PodDisruptionBudget can help protect a Pod from being forcibly terminated during a deployment update, it does not ensure a graceful shutdown of the application. Option A, updating the code to handle SIGTERM signals, is the recommended way to ensure a graceful shutdown in the event of a termination."
      },
      {
        "date": "2023-02-08T10:44:00.000Z",
        "voteCount": 1,
        "content": "Here's a link to the official Kubernetes documentation on the SIGTERM signal:\n\nhttps://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks\n\nAnd here's a link to the official Kubernetes documentation on how to handle the SIGTERM signal in your application:\n\nhttps://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#define-a-graceful-shutdown-for-your-application"
      },
      {
        "date": "2023-02-01T01:03:00.000Z",
        "voteCount": 1,
        "content": "https://kubernetes.io/docs/concepts/workloads/pods/disruptions/#pod-disruption-budgets\n\nhttps://cloud.google.com/blog/products/containers-kubernetes/kubernetes-best-practices-terminating-with-grace"
      },
      {
        "date": "2023-02-08T10:45:00.000Z",
        "voteCount": 3,
        "content": "Option B, Configuring a PodDisruptionBudget, is used to control the number of replicas of a pod that can be down simultaneously. It prevents voluntary disruption of the pods, but it does not prevent forced termination of the pods. When a pod is terminated forcefully, for example, during a node failure, the PodDisruptionBudget does not come into play. In this scenario, you need to handle the termination gracefully in your application code, as described in option A."
      },
      {
        "date": "2023-02-11T01:14:00.000Z",
        "voteCount": 1,
        "content": "As i said i passed my exam already"
      },
      {
        "date": "2023-02-12T05:47:00.000Z",
        "voteCount": 7,
        "content": "And you don't want other people passing the exam? Because this option B seems to be wrong to me"
      },
      {
        "date": "2023-02-15T03:50:00.000Z",
        "voteCount": 2,
        "content": "According to you and who are you?? stay away from baseless arguments they dnt benefit you with anything"
      },
      {
        "date": "2023-12-16T13:19:00.000Z",
        "voteCount": 1,
        "content": "Have you passed the exam scoring 100% and it has had this particular question? Cause if not, you're just talking smack - GCP exams do not show where exactly you were wrong, so even if you passed it successfully it doesn't mean that you're right in this exact question."
      },
      {
        "date": "2023-05-03T07:47:00.000Z",
        "voteCount": 3,
        "content": "I think Mr. TNT87 is very exceed person, because I know he answered very exellent answer in other questions.\nBut this question's demand is\n\"You want to update your application to make sure that it completes a graceful shutdown.\".\nIt's not about avoiding shutdown.\nI think the answer is what you should do in occuring shutdown.\nSo me too, I think \"to prevent the Pod from being forcefully shut dow n.\" doesn't anser for demand.\nI think also right answer is A."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 238,
    "url": "https://www.examtopics.com/discussions/google/view/97517-exam-professional-cloud-developer-topic-1-question-238/",
    "body": "You are a lead developer working on a new retail system that runs on Cloud Run and Firestore in Datastore mode. A web UI requirement is for the system to display a list of available products when users access the system and for the user to be able to browse through all products. You have implemented this requirement in the minimum viable product (MVP) phase by returning a list of all available products stored in Firestore.<br><br>A few months after go-live, you notice that Cloud Run instances are terminated with HTTP 500: Container instances are exceeding memory limits errors during busy times. This error coincides with spikes in the number of Datastore entity reads. You need to prevent Cloud Run from crashing and decrease the number of Datastore entity reads. You want to use a solution that optimizes system performance. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the query that returns the product list using integer offsets.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the query that returns the product list using limits.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the Cloud Run configuration to increase the memory limits.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the query that returns the product list using cursors.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 9,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-12-16T13:37:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/datastore/docs/concepts/queries#cursors_limits_and_offsets"
      },
      {
        "date": "2023-09-24T07:48:00.000Z",
        "voteCount": 1,
        "content": "Cursors allow you to paginate through the results of a Firestore query. This can be useful for queries that return a large number of results, such as the query that returns the list of all available products."
      },
      {
        "date": "2023-08-08T04:42:00.000Z",
        "voteCount": 1,
        "content": "D is correct.\nUse pagination and return only results in batch/limits when querying for the list of products.  This is called lazy loading."
      },
      {
        "date": "2023-02-23T03:15:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/datastore/docs/best-practices#queries"
      },
      {
        "date": "2023-02-19T11:13:00.000Z",
        "voteCount": 1,
        "content": "Note: To conserve memory and improve performance, a query should, whenever possible, specify a limit on the number of results returned.\n\nhttps://cloud.google.com/datastore/docs/concepts/queries#cursors_limits_and_offsets"
      },
      {
        "date": "2023-02-28T00:20:00.000Z",
        "voteCount": 1,
        "content": "it's D, not a, wrongly selected"
      },
      {
        "date": "2023-02-28T00:21:00.000Z",
        "voteCount": 1,
        "content": "Although Datastore mode databases support integer offsets, you should avoid using them. Instead, use cursors. Using an offset only avoids returning the skipped entities to your application, but these entities are still retrieved internally. The skipped entities do affect the latency of the query, and your application is billed for the read operations required to retrieve them. Using cursors instead of offsets lets you avoid all these costs"
      },
      {
        "date": "2023-02-04T02:50:00.000Z",
        "voteCount": 4,
        "content": "While increasing the memory limits of Cloud Run instances could help alleviate the issue temporarily, it would not address the root cause of the problem, which is the high number of Datastore entity reads during busy times. Over time, as more products are added to the system, this problem would only become more severe, and you would have to continually increase the memory limits to prevent Cloud Run from crashing.\n\nUsing cursors to paginate the results and retrieve a limited number of products at a time is a more sustainable solution as it reduces the amount of data that needs to be read from Datastore and decreases the memory usage of your Cloud Run instances. This way, you can maintain the performance of the system and prevent it from crashing, even as more products are added over time."
      },
      {
        "date": "2023-02-01T00:40:00.000Z",
        "voteCount": 1,
        "content": "The issue is with the memory limits. \nhttps://cloud.google.com/run/docs/configuring/memory-limits#optimizing\nhttps://cloud.google.com/run/docs/configuring/memory-limits#optimizing"
      },
      {
        "date": "2023-02-08T10:50:00.000Z",
        "voteCount": 4,
        "content": "https://cloud.google.com/datastore/docs/concepts/queries#cursors\n\nCursors allow you to paginate through query results efficiently, which can help reduce the number of Datastore entity reads and prevent Cloud Run instances from crashing due to exceeding memory limits. By using cursors, you can retrieve only a portion of the query results at a time, instead of retrieving all results in one go, which can help optimize system performance."
      },
      {
        "date": "2023-02-08T10:50:00.000Z",
        "voteCount": 3,
        "content": "Option C, increasing the memory limits of Cloud Run, may provide a temporary solution to the issue, but it does not address the root cause of the problem. The root cause is that too many Datastore entities are being read in one go, which is causing Cloud Run instances to exceed their memory limits and crash. Increasing the memory limits simply allows the instances to handle more data in memory, but it does not address the issue of retrieving too much data in one go.\n\nUsing cursors to paginate through query results, as in option D, is a better solution because it allows you to retrieve only the necessary data at a time, which can help reduce the number of Datastore entity reads and prevent Cloud Run instances from crashing."
      },
      {
        "date": "2023-05-03T08:15:00.000Z",
        "voteCount": 2,
        "content": "I agree with Mr. mrvergara\nIf I would image I'm actually creating the code for this program code, I would  think current program isn't very good.\nI'll think I should improve this program better than now firstly. If it's possible, I wouldn't like to add resources."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 239,
    "url": "https://www.examtopics.com/discussions/google/view/97516-exam-professional-cloud-developer-topic-1-question-239/",
    "body": "You need to deploy an internet-facing microservices application to Google Kubernetes Engine (GKE). You want to validate new features using the A/B testing method. You have the following requirements for deploying new container image releases:<br>\u2022\tThere is no downtime when new container images are deployed.<br>\u2022\tNew production releases are tested and verified using a subset of production users.<br><br>What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Configure your CI/CD pipeline to update the Deployment manifest file by replacing the container version with the latest version.<br>2. Recreate the Pods in your cluster by applying the Deployment manifest file.<br>3. Validate the application's performance by comparing its functionality with the previous release version, and roll back if an issue arises.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a second namespace on GKE for the new release version.<br>2. Create a Deployment configuration for the second namespace with the desired number of Pods.<br>3. Deploy new container versions in the second namespace.<br>4. Update the Ingress configuration to route traffic to the namespace with the new container versions.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Install the Anthos Service Mesh on your GKE cluster.<br>2. Create two Deployments on the GKE cluster, and label them with different version names.<br>3. Implement an Istio routing rule to send a small percentage of traffic to the Deployment that references the new version of the application.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Implement a rolling update pattern by replacing the Pods gradually with the new release version.<br>2. Validate the application's performance for the new subset of users during the rollout, and roll back if an issue arises."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 11,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-02-28T00:25:00.000Z",
        "voteCount": 6,
        "content": "https://cloud.google.com/architecture/implementing-deployment-and-testing-strategies-on-gke#perform_an_ab_test \n\ni would say C: \nTo try this pattern, you perform the following steps:\n\nDeploy the current version of the application (app:current) on the GKE cluster.\nDeploy a new version of the application (app:new) alongside the current version.\nUse Istio to route incoming requests that have the username test in the request's cookie to app:new. All other requests are routed to app:current."
      },
      {
        "date": "2023-09-24T07:50:00.000Z",
        "voteCount": 1,
        "content": "C is correct."
      },
      {
        "date": "2023-08-08T06:44:00.000Z",
        "voteCount": 1,
        "content": "C looks good since \"send a small percentage of traffic to the Deployment that references the new version of the application\" for A/B testing.\nD is close but not perfect for the said requirements."
      },
      {
        "date": "2023-06-10T13:48:00.000Z",
        "voteCount": 2,
        "content": "C. The keywords, \"A/B testing\", \"verified using a subset of production users\", mean we need canary deployment. \nA: No. In-place deployment.\nB: No. This is Blue/Green deployment, but Ingress config (=manifest) does not have way to specify subset of traffic routing to different namespace.\nC: Yes. \nD: No, there's no mechanism on Ingress / Services manifests that can specify a subset of users, plus this is rolling update (=in-place deployment)"
      },
      {
        "date": "2023-05-03T08:29:00.000Z",
        "voteCount": 1,
        "content": "I couldn't find the wrong point in Option C.\nAnd it's cool way.\nI think in option B, some accidents possibly occur in the case the communication occurred between some microservices includeing new container."
      },
      {
        "date": "2023-02-05T13:35:00.000Z",
        "voteCount": 1,
        "content": "Actually according to this link , its B\nhttps://cloud.google.com/kubernetes-engine/docs/tutorials/hello-app#deploying_a_new_version_of_the_sample_app"
      },
      {
        "date": "2023-02-04T03:08:00.000Z",
        "voteCount": 4,
        "content": "This approach allows you to deploy new container images without downtime, as the traffic is only being redirected to the new namespace once the Deployment is ready. This also allows you to test and verify the new production release using a subset of production users by routing only a portion of the traffic to the new namespace."
      },
      {
        "date": "2023-02-04T03:08:00.000Z",
        "voteCount": 2,
        "content": "Option D, which implements a rolling update pattern, can result in some downtime as Pods are gradually replaced with the new release version. While this approach can minimize the impact of any issues with the new release, it does not meet the requirement of \"no downtime when new container images are deployed.\" Option D would be a suitable approach for situations where downtime is acceptable and can be managed, but it does not meet the requirements specified in this scenario."
      },
      {
        "date": "2023-02-01T00:32:00.000Z",
        "voteCount": 1,
        "content": "https://auth0.com/blog/deployment-strategies-in-kubernetes/\n Rolling updates are ideal because they allow you to deploy an application slowly with minimal overhead, minimal performance impact, and minimal"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 240,
    "url": "https://www.examtopics.com/discussions/google/view/97515-exam-professional-cloud-developer-topic-1-question-240/",
    "body": "Your team manages a large Google Kubernetes Engine (GKE) cluster. Several application teams currently use the same namespace to develop microservices for the cluster. Your organization plans to onboard additional teams to create microservices. You need to configure multiple environments while ensuring the security and optimal performance of each team\u2019s work. You want to minimize cost and follow Google-recommended best practices. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate new role-based access controls (RBAC) for each team in the existing cluster, and define resource quotas.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new namespace for each environment in the existing cluster, and define resource quotas.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new GKE cluster for each team.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new namespace for each team in the existing cluster, and define resource quotas.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 13,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 6,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-07-12T09:52:00.000Z",
        "voteCount": 2,
        "content": "correct answer D"
      },
      {
        "date": "2024-06-28T23:23:00.000Z",
        "voteCount": 2,
        "content": "key word- \"optimal performance of each team\u2019s work\""
      },
      {
        "date": "2024-04-08T21:54:00.000Z",
        "voteCount": 2,
        "content": "D: Creating a new namespace for each team within the existing cluster and defining resource quotas is a good way to provide isolation, manage resources, and maintain security without incurring the cost of additional clusters.\n\nRejected:\nA: While RBAC can help manage access control, it doesn't provide the same level of resource isolation and management as using namespaces.\nB: Creating a namespace for each environment doesn't account for multiple teams working in the same environment.\nC: Creating a new GKE cluster for each team could lead to higher costs and complexity. It's more efficient to use namespaces within a single cluster for team isolation."
      },
      {
        "date": "2024-02-14T02:00:00.000Z",
        "voteCount": 3,
        "content": "I'd like to say A, but namespacing is too important to be left aside.\nI say D."
      },
      {
        "date": "2023-09-24T08:14:00.000Z",
        "voteCount": 2,
        "content": "I will go with D."
      },
      {
        "date": "2023-09-03T04:07:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2023-08-08T06:52:00.000Z",
        "voteCount": 1,
        "content": "I go with A. Because of Security, low cost and Google-recommended best practices.  I hope there is no need to create additional namespaces since several application teams are already use the same namespace to develop microservices for the cluster."
      },
      {
        "date": "2023-06-14T05:39:00.000Z",
        "voteCount": 4,
        "content": "Option B is the only one which addresses the part of the question that says 'You need to configure multiple environments'"
      },
      {
        "date": "2023-09-03T04:06:00.000Z",
        "voteCount": 1,
        "content": "This is the correct answer as its the only one which addresses the question: \"You need to configure multiple environments\""
      },
      {
        "date": "2023-05-03T09:15:00.000Z",
        "voteCount": 1,
        "content": "I worried A or D.\nI judged these teams are creating a microservice for each function on a learge same application by the explain of \"to develop microservices for the cluster\" .\nIf it's true, you don't need to separate using namespace.\nI think the thing you should protect is resources, for example the spanner for develop environment, the spanner for release environment and forbidden other team's the spanner access.\nIn the case I think like that, I think this Q's answer is A."
      },
      {
        "date": "2023-04-24T11:21:00.000Z",
        "voteCount": 1,
        "content": "security"
      },
      {
        "date": "2023-03-24T16:02:00.000Z",
        "voteCount": 2,
        "content": "for each team, hence need namespaces and quota"
      },
      {
        "date": "2023-05-03T09:46:00.000Z",
        "voteCount": 1,
        "content": "You could give the Role to user or user group."
      },
      {
        "date": "2023-02-19T11:22:00.000Z",
        "voteCount": 2,
        "content": "To configure more granular access to Kubernetes resources at the cluster level or within Kubernetes namespaces, you use Role-Based Access Control (RBAC). RBAC allows you to create detailed policies that define which operations and resources you allow users and service accounts to access. With RBAC, you can control access for Google Accounts, Google Cloud service accounts, and Kubernetes service accounts. T"
      },
      {
        "date": "2023-02-01T00:07:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/kubernetes-engine/docs/best-practices/rbac"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 241,
    "url": "https://www.examtopics.com/discussions/google/view/97514-exam-professional-cloud-developer-topic-1-question-241/",
    "body": "You have deployed a Java application to Cloud Run. Your application requires access to a database hosted on Cloud SQL. Due to regulatory requirements, your connection to the Cloud SQL instance must use its internal IP address. How should you configure the connectivity while following Google-recommended best practices?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure your Cloud Run service with a Cloud SQL connection.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure your Cloud Run service to use a Serverless VPC Access connector.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure your application to use the Cloud SQL Java connector.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure your application to connect to an instance of the Cloud SQL Auth proxy."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 6,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-04-08T22:03:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/sql/docs/mysql/connect-run#private-ip"
      },
      {
        "date": "2023-09-24T08:16:00.000Z",
        "voteCount": 1,
        "content": "B is correct."
      },
      {
        "date": "2023-02-05T03:49:00.000Z",
        "voteCount": 3,
        "content": "It should be B, EI faced this exact challenge in one of my projects"
      },
      {
        "date": "2023-02-04T03:15:00.000Z",
        "voteCount": 1,
        "content": "Option B, using a Serverless VPC Access connector, is the recommended best practice for accessing a Cloud SQL instance from Cloud Run because it provides a secure and scalable way to connect to your internal resources.\n\nWith this option, you can connect your Cloud Run service to your internal VPC network, allowing it to access resources such as Cloud SQL instances that have internal IP addresses. This eliminates the need for a public IP address or a public network connection to your database, which can increase security and regulatory compliance."
      },
      {
        "date": "2023-02-04T03:15:00.000Z",
        "voteCount": 1,
        "content": "Option A, configuring a Cloud SQL connection, is not possible because Cloud Run does not support direct connections to Cloud SQL instances.\n\nOption C, using the Cloud SQL Java connector, is a valid way to connect to a Cloud SQL instance but does not provide the secure and scalable VPC connectivity that is recommended by Google.\n\nOption D, connecting to an instance of the Cloud SQL Auth proxy, is a valid way to connect to a Cloud SQL instance, but it requires additional setup and maintenance, and may not be the most secure or scalable option, especially for large-scale deployments."
      },
      {
        "date": "2023-02-01T00:00:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/sql/docs/mysql/connect-connectors#setup-and-usage\n If your application is written in Java you can skip this step, since you do this in the Java Cloud SQL Connector"
      },
      {
        "date": "2023-02-08T11:01:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/sql/docs/mysql/connect-run#vpc-access\n\nIn this documentation, Google recommends using a Serverless VPC Access connector to connect to the internal IP address of a Cloud SQL instance, which is a secure and scalable way to access resources in a VPC network."
      },
      {
        "date": "2023-02-08T11:01:00.000Z",
        "voteCount": 3,
        "content": "Option C, \"Configure your application to use the Cloud SQL Java connector,\" is a valid option, but it is not recommended by Google as the best practice. The Cloud SQL Java connector is designed to work with external IP addresses, and using it with an internal IP address can result in increased latency and potential security vulnerabilities.\n\nUsing a Serverless VPC Access connector to connect to the internal IP address, as suggested by option B, provides a more secure and performant solution. This method allows you to access the internal IP address of your Cloud SQL instance from a private network, bypassing the public internet, and avoiding exposure to security threats."
      },
      {
        "date": "2023-02-25T04:19:00.000Z",
        "voteCount": 1,
        "content": "According doc tnt87 sent - SQL connectors can't provide a network path to a Cloud SQL instance if one is not already present."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 242,
    "url": "https://www.examtopics.com/discussions/google/view/97513-exam-professional-cloud-developer-topic-1-question-242/",
    "body": "Your application stores customers\u2019 content in a Cloud Storage bucket, with each object being encrypted with the customer's encryption key. The key for each object in Cloud Storage is entered into your application by the customer. You discover that your application is receiving an HTTP 4xx error when reading the object from Cloud Storage. What is a possible cause of this error?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYou attempted the read operation on the object with the customer's base64-encoded key.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYou attempted the read operation without the base64-encoded SHA256 hash of the encryption key.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYou entered the same encryption algorithm specified by the customer when attempting the read operation.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYou attempted the read operation on the object with the base64-encoded SHA256 hash of the customer's key."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 12,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 4,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-02-27T07:51:00.000Z",
        "voteCount": 5,
        "content": "According to the documentation the SHA256 is needed in the REST API -&gt; B\nhttps://cloud.google.com/storage/docs/encryption/using-customer-supplied-keys#rest-csek-download-object"
      },
      {
        "date": "2024-09-08T01:24:00.000Z",
        "voteCount": 1,
        "content": "Base64-encoded SHA256 hash vs. Raw Encryption Key:\n\nThe Google Cloud Storage documentation you linked mentions two approaches for customer-managed encryption keys:\n\nBase64-encoded SHA256 hash: This is primarily used for verification purposes and access control. not for reading \nOption A: Correct. Using the base64-encoded encryption key instead of the raw key bytes for reading will likely lead to a 4xx error.\nOption B: Incorrect. You don't directly use the base64-encoded SHA256 hash for reading the object, but it might be required for authentication purposes.\nOption C: Incorrect. Entering the correct encryption algorithm shouldn't lead to a 4xx error if everything else is configured correctly.\nOption D: Incorrect. Similar to option B, using the base64-encoded SHA256 hash for reading the object is not the correct approach."
      },
      {
        "date": "2023-12-25T13:38:00.000Z",
        "voteCount": 1,
        "content": "as some guys said, in the link https://cloud.google.com/storage/docs/encryption/customer-supplied-keys#response we understand why B is correct"
      },
      {
        "date": "2023-12-24T08:21:00.000Z",
        "voteCount": 1,
        "content": "B is correct."
      },
      {
        "date": "2023-09-24T08:19:00.000Z",
        "voteCount": 1,
        "content": "B is correct."
      },
      {
        "date": "2023-08-08T07:10:00.000Z",
        "voteCount": 2,
        "content": "4xx is for Bad request, resource forbidden, not found and many more.\nIf we want to read the object of Cloud storage bucket programmatically, then we need to pass the same customer key that was used for encrypting the object. \n\nThe request we need to send with Base64Encode ( SHA256 Hash (customer-key ) )\nThe key set for object is SHA256 Hash (customer-key ) and while reading the Base64decode of the key will happen and comparing the Hash of the keys. If Hash are equal, then read access is permitted."
      },
      {
        "date": "2023-03-02T01:44:00.000Z",
        "voteCount": 2,
        "content": "took my exam yesterday (01-03-2023) and this question was there"
      },
      {
        "date": "2023-06-06T06:01:00.000Z",
        "voteCount": 1,
        "content": "what was the answer? did you pass?"
      },
      {
        "date": "2023-06-06T06:01:00.000Z",
        "voteCount": 1,
        "content": "I think its A"
      },
      {
        "date": "2023-02-06T18:06:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/storage/docs/encryption/customer-supplied-keys"
      },
      {
        "date": "2023-02-04T03:29:00.000Z",
        "voteCount": 2,
        "content": "Option D is a possible cause of an HTTP 4xx error when reading an object from Cloud Storage because it is incorrect to use the base64-encoded SHA256 hash of the customer's encryption key to read an encrypted object. To read an encrypted object, you need to use the original encryption key, not its hash. The HTTP 4xx error could be a result of an incorrect or unsupported key format, or a key mismatch. On the other hand, using the base64-encoded key (Option A), the encryption algorithm (Option C), or the base64-encoded SHA256 hash of the encryption key (Option B) without the original encryption key would not allow the object to be decrypted and read."
      },
      {
        "date": "2023-02-08T11:08:00.000Z",
        "voteCount": 1,
        "content": "The Google Cloud Storage documentation explains how to access objects in a bucket, including the use of an encryption key. The encryption key must be base64-encoded, and it is recommended to use the base64-encoded SHA256 hash of the encryption key for secure access to the objects.\n\nHere's the link to the Google Cloud Storage documentation: https://cloud.google.com/storage/docs/access-control/using-encryption-keys#using-base64-encoded-sha256-hashes-to-authenticate"
      },
      {
        "date": "2023-02-26T06:57:00.000Z",
        "voteCount": 1,
        "content": "link do not exists :/"
      },
      {
        "date": "2023-02-01T05:09:00.000Z",
        "voteCount": 2,
        "content": "Answer B, made a mistsake"
      },
      {
        "date": "2023-01-31T23:47:00.000Z",
        "voteCount": 4,
        "content": "You receive an HTTP 400 error in the following cases:\n\n1.You upload an object using a customer-supplied encryption key, and you attempt to perform another operation on the object (other than requesting or updating most metadata or deleting the object) without providing the key.\n2.You upload an object using a customer-supplied encryption key, and you attempt to perform another operation on the object with an incorrect key.\n3.You upload an object without providing a customer-supplied encryption key, and you attempt to perform another operation on the object with a customer-supplied encryption key.\n4.You specify an encryption algorithm, key, or SHA256 hash that is not valid.\nPoint number 2 has the answer \nhttps://cloud.google.com/storage/docs/encryption/customer-supplied-keys#response"
      },
      {
        "date": "2023-02-01T05:08:00.000Z",
        "voteCount": 1,
        "content": "typo , its B not C"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 243,
    "url": "https://www.examtopics.com/discussions/google/view/97512-exam-professional-cloud-developer-topic-1-question-243/",
    "body": "You have two Google Cloud projects, named Project A and Project B. You need to create a Cloud Function in Project A that saves the output in a Cloud Storage bucket in Project B. You want to follow the principle of least privilege. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a Google service account in Project B.<br>2. Deploy the Cloud Function with the service account in Project A.<br>3. Assign this service account the roles/storage.objectCreator role on the storage bucket residing in Project B.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a Google service account in Project A<br>2. Deploy the Cloud Function with the service account in Project A.<br>3. Assign this service account the roles/storage.objectCreator role on the storage bucket residing in Project B.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Determine the default App Engine service account (PROJECT_ID@appspot.gserviceaccount.com) in Project A.<br>2. Deploy the Cloud Function with the default App Engine service account in Project A.<br>3. Assign the default App Engine service account the roles/storage.objectCreator role on the storage bucket residing in Project B.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Determine the default App Engine service account (PROJECT_ID@appspot.gserviceaccount.com) in Project B.<br>2. Deploy the Cloud Function with the default App Engine service account in Project A.<br>3. Assign the default App Engine service account the roles/storage.objectCreator role on the storage bucket residing in Project B."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 9,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-04-08T23:11:00.000Z",
        "voteCount": 1,
        "content": "quite straightforward"
      },
      {
        "date": "2023-09-25T03:05:00.000Z",
        "voteCount": 1,
        "content": "B is correct."
      },
      {
        "date": "2023-09-24T08:21:00.000Z",
        "voteCount": 1,
        "content": "B is correct."
      },
      {
        "date": "2023-08-08T07:15:00.000Z",
        "voteCount": 1,
        "content": "B is correct. Simple and straight forward.\nCreate SA in Project A, Assign SA the role of object creator to push objects to Cloud bucket in Project B."
      },
      {
        "date": "2023-03-02T01:44:00.000Z",
        "voteCount": 4,
        "content": "took my exam yesterday (01-03-2023) and this question was there"
      },
      {
        "date": "2023-02-26T07:06:00.000Z",
        "voteCount": 2,
        "content": "it's B.\n\nhttps://articles.wesionary.team/multi-project-account-service-account-in-gcp-ba8f8821347e"
      },
      {
        "date": "2023-02-11T06:41:00.000Z",
        "voteCount": 3,
        "content": "A is not correct because you cannot run a Cloud Function with a service account that is not in the same Google Cloud project.\nB is correct because it follows the least privilege principle and for a Cloud Function, the service account must be created in the same project where the function is getting executed."
      },
      {
        "date": "2023-02-06T18:16:00.000Z",
        "voteCount": 2,
        "content": "option B is right. We have permissions to object creation  in project for the SA created in proejct A. https://www.youtube.com/watch?v=ctACCk80H-w"
      },
      {
        "date": "2023-02-04T04:43:00.000Z",
        "voteCount": 2,
        "content": "In option B, a service account is created in Project A, but this service account would have access to all the resources within Project A, which is more than is necessary for the task of saving output to a storage bucket in Project B.\n\nOptions C and D use the default App Engine service account, which would have more permissions than necessary, as it would have access to all App Engine resources within Project A or B, rather than just the permissions needed for the task of saving output to a storage bucket in Project B."
      },
      {
        "date": "2023-02-05T13:19:00.000Z",
        "voteCount": 1,
        "content": "No it cant be A, check the link provided below please. it cant be A, there is no way"
      },
      {
        "date": "2023-02-08T12:03:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/docs/authentication/production#providing_credentials_to_your_application\n\nIn this guide, it explains the best practice for providing authentication credentials to your application. By creating a separate Google service account in the project that owns the resource you want to access (in this case, Project B), and then using that service account to perform actions on the resource (writing to the Cloud Storage bucket in Project B), you are following the principle of least privilege. This means that you are granting the minimum permissions necessary to perform the desired action."
      },
      {
        "date": "2023-02-11T01:11:00.000Z",
        "voteCount": 1,
        "content": "Anyway i passed my exam last week"
      },
      {
        "date": "2023-02-12T06:00:00.000Z",
        "voteCount": 1,
        "content": "Congrats, this time you are right. The answer is option B"
      },
      {
        "date": "2023-02-11T06:42:00.000Z",
        "voteCount": 1,
        "content": "It is the B option"
      },
      {
        "date": "2023-01-31T23:31:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/functions/docs/concepts/iam#runtime_service_accounts"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 244,
    "url": "https://www.examtopics.com/discussions/google/view/97946-exam-professional-cloud-developer-topic-1-question-244/",
    "body": "A governmental regulation was recently passed that affects your application. For compliance purposes, you are now required to send a duplicate of specific application logs from your application\u2019s project to a project that is restricted to the security team. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate user-defined log buckets in the security team\u2019s project. Configure a Cloud Logging sink to route your application\u2019s logs to log buckets in the security team\u2019s project.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a job that copies the logs from the _Required log bucket into the security team\u2019s log bucket in their project.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the _Default log bucket sink rules to reroute the logs into the security team\u2019s log bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a job that copies the System Event logs from the _Required log bucket into the security team\u2019s log bucket in their project."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-09-24T08:22:00.000Z",
        "voteCount": 1,
        "content": "A is correct."
      },
      {
        "date": "2023-08-08T07:21:00.000Z",
        "voteCount": 1,
        "content": "I go with A.\nThis question is to test Cloud Logging Sink feature."
      },
      {
        "date": "2023-02-19T10:58:00.000Z",
        "voteCount": 2,
        "content": "i also choose A. https://cloud.google.com/architecture/security-log-analytics"
      },
      {
        "date": "2023-02-04T04:48:00.000Z",
        "voteCount": 3,
        "content": "I choose option A because it provides a direct and automated solution for duplicating the specific application logs and sending them to the security team's project. This method uses Cloud Logging's sink feature, which is a powerful tool for routing logs to other destinations, such as log buckets or Pub/Sub topics. By using a sink, you can ensure that the duplication of logs is performed in real-time and automatically, which would minimize manual intervention and minimize the risk of errors."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 245,
    "url": "https://www.examtopics.com/discussions/google/view/97506-exam-professional-cloud-developer-topic-1-question-245/",
    "body": "You plan to deploy a new Go application to Cloud Run. The source code is stored in Cloud Source Repositories. You need to configure a fully managed, automated, continuous deployment pipeline that runs when a source code commit is made. You want to use the simplest deployment solution. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a cron job on your workstations to periodically run gcloud run deploy --source in the working directory.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a Jenkins trigger to run the container build and deploy process for each source code commit to Cloud Source Repositories.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure continuous deployment of new revisions from a source repository for Cloud Run using buildpacks.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Build with a trigger configured to run the container build and deploy process for each source code commit to Cloud Source Repositories.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 5,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-09-24T08:24:00.000Z",
        "voteCount": 1,
        "content": "D is correct."
      },
      {
        "date": "2023-08-08T07:31:00.000Z",
        "voteCount": 1,
        "content": "D is more suitable since we need \"simplest deployment solution\".\nC is close but it is limited to only building the image and pushing that into artifact registry.  It donot take part in deployment.\nB is also close but it takes more work than D."
      },
      {
        "date": "2023-07-22T21:41:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is C, which is simpler than D.\nIn option C, there is no need to create a Dockerfile, making it a more straightforward solution compared to option D."
      },
      {
        "date": "2023-06-25T03:41:00.000Z",
        "voteCount": 1,
        "content": "C. Configure continuous deployment of new revisions from a source repository for Cloud Run using buildpacks.\n\nThis is because Google Cloud Run offers the ability to automate the deployment of new revisions directly from a source repository using buildpacks. This is an extremely simple and managed way to set up a continuous deployment pipeline.\n\nOption D, while a valid method for automating deployments, is not as simple as using Cloud Run's integrated deployment feature, as it involves the additional service of Cloud Build."
      },
      {
        "date": "2023-09-16T03:28:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/blog/products/serverless/build-and-deploy-an-app-to-cloud-run-with-a-single-command"
      },
      {
        "date": "2023-02-26T07:08:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/run/docs/continuous-deployment-with-cloud-build\nCloud Build is a fully managed, scalable, and efficient service provided by Google Cloud that allows you to automate your software delivery pipeline, including building, testing, and deploying applications. By using a trigger with Cloud Build, you can automatically build and deploy your Go application to Cloud Run whenever a source code commit is made in Cloud Source Repositories. This provides a simple, fully managed solution for continuous deployment, and eliminates the need for manual processes or external tools like Jenkins."
      },
      {
        "date": "2023-02-04T04:51:00.000Z",
        "voteCount": 1,
        "content": "Cloud Build is a fully managed, scalable, and efficient service provided by Google Cloud that allows you to automate your software delivery pipeline, including building, testing, and deploying applications. By using a trigger with Cloud Build, you can automatically build and deploy your Go application to Cloud Run whenever a source code commit is made in Cloud Source Repositories. This provides a simple, fully managed solution for continuous deployment, and eliminates the need for manual processes or external tools like Jenkins."
      },
      {
        "date": "2023-01-31T17:42:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/run/docs/continuous-deployment-with-cloud-build"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 246,
    "url": "https://www.examtopics.com/discussions/google/view/97504-exam-professional-cloud-developer-topic-1-question-246/",
    "body": "Your team has created an application that is hosted on a Google Kubernetes Engine (GKE) cluster. You need to connect the application to a legacy REST service that is deployed in two GKE clusters in two different regions. You want to connect your application to the target service in a way that is resilient. You also want to be able to run health checks on the legacy service on a separate port. How should you set up the connection? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Traffic Director with a sidecar proxy to connect the application to the service.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a proxyless Traffic Director configuration to connect the application to the service.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the legacy service's firewall to allow health checks originating from the proxy.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the legacy service's firewall to allow health checks originating from the application.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the legacy service's firewall to allow health checks originating from the Traffic Director control plane."
    ],
    "answer": "AC",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AC",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-09-24T08:27:00.000Z",
        "voteCount": 1,
        "content": "AC are correct."
      },
      {
        "date": "2023-03-02T01:38:00.000Z",
        "voteCount": 3,
        "content": "took my exam yesterday (01-03-2023) and this question was there"
      },
      {
        "date": "2023-02-26T06:46:00.000Z",
        "voteCount": 1,
        "content": "i agree, AC"
      },
      {
        "date": "2023-02-04T04:56:00.000Z",
        "voteCount": 3,
        "content": "A. Using Traffic Director with a sidecar proxy can provide resilience for your application by allowing for failover to the secondary region in the event of an outage. The sidecar proxy can route traffic to the legacy service in either of the two GKE clusters, ensuring high availability.\n\nC. Configuring the legacy service's firewall to allow health checks originating from the proxy allows the proxy to periodically check the health of the legacy service and ensure that it is functioning properly. This helps to ensure that traffic is only routed to healthy instances of the legacy service, further improving the resilience of the setup."
      },
      {
        "date": "2023-01-31T17:36:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/load-balancing/docs/health-checks#health_check_categories_protocols_and_ports"
      },
      {
        "date": "2023-01-31T17:33:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/traffic-director/docs/advanced-setup#routing-rule-maps\nhttps://cloud.google.com/traffic-director/docs/advanced-setup"
      },
      {
        "date": "2023-02-03T06:51:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/load-balancing/docs/health-check-concepts"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 247,
    "url": "https://www.examtopics.com/discussions/google/view/97503-exam-professional-cloud-developer-topic-1-question-247/",
    "body": "You have an application running in a production Google Kubernetes Engine (GKE) cluster. You use Cloud Deploy to automatically deploy your application to your production GKE cluster. As part of your development process, you are planning to make frequent changes to the application\u2019s source code and need to select the tools to test the changes before pushing them to your remote source code repository. Your toolset must meet the following requirements:<br>\u2022\tTest frequent local changes automatically.<br>\u2022\tLocal deployment emulates production deployment.<br><br>Which tools should you use to test building and running a container on your laptop using minimal resources?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDocker Compose and dockerd",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTerraform and kubeadm",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMinikube and Skaffold\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tkaniko and Tekton"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-11-28T08:21:00.000Z",
        "voteCount": 2,
        "content": "How is this even related to GCP"
      },
      {
        "date": "2023-09-24T08:29:00.000Z",
        "voteCount": 1,
        "content": "Minikube is a tool for running Kubernetes locally on your laptop. Skaffold is a tool for scaffolding, building, and deploying Kubernetes applications."
      },
      {
        "date": "2023-08-08T13:07:00.000Z",
        "voteCount": 1,
        "content": "C is the correct choice. Since GKE local environment is required, Minikube and scaffold are right choices."
      },
      {
        "date": "2023-08-10T14:26:00.000Z",
        "voteCount": 1,
        "content": "Tilt and Octant are also very good local development tools to test K8S applications."
      },
      {
        "date": "2023-03-02T01:39:00.000Z",
        "voteCount": 4,
        "content": "took my exam yesterday (01-03-2023) and this question was there"
      },
      {
        "date": "2023-02-04T05:14:00.000Z",
        "voteCount": 3,
        "content": "Minikube is a tool that runs a single-node Kubernetes cluster locally on your laptop, allowing you to test and run your application on a simulated production environment. Skaffold is a command line tool that automates the process of building and deploying your application to a local or remote Kubernetes cluster.\n\nTogether, Minikube and Skaffold allow you to test your frequent changes locally, with a deployment that emulates a production environment, using minimal resources. Minikube provides the simulated production environment, while Skaffold takes care of building and deploying your application, making the development process smoother and more efficient."
      },
      {
        "date": "2023-01-31T17:18:00.000Z",
        "voteCount": 1,
        "content": "Answer C\nMinikube is a lightweight Kubernetes implementation that creates a VM on your local machine and deploys a simple cluster containing only one node. Minikube is available for Linux, macOS, and Windows systems.\n\nSkaffold is a tool that handles the workflow for building, pushing and deploying your application. You can use Skaffold to easily configure a local development workspace, streamline your inner development loop, and integrate with other tools such as Kustomize and Helm to help manage your Kubernetes manifests"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 248,
    "url": "https://www.examtopics.com/discussions/google/view/97502-exam-professional-cloud-developer-topic-1-question-248/",
    "body": "You are deploying a Python application to Cloud Run using Cloud Source Repositories and Cloud Build. The Cloud Build pipeline is shown below:<br><br><img src=\"https://img.examtopics.com/professional-cloud-developer/image8.png\"><br><br>You want to optimize deployment times and avoid unnecessary steps. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRemove the step that pushes the container to Artifact Registry.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy a new Docker registry in a VPC, and use Cloud Build worker pools inside the VPC to run the build pipeline.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore image artifacts in a Cloud Storage bucket in the same region as the Cloud Run instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd the --cache-from argument to the Docker build step in your build config file.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-09-24T08:30:00.000Z",
        "voteCount": 1,
        "content": "D is correct."
      },
      {
        "date": "2023-08-08T13:14:00.000Z",
        "voteCount": 1,
        "content": "Use local cached image to save build time."
      },
      {
        "date": "2023-02-28T00:01:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/build/docs/optimize-builds/speeding-up-builds#using_a_cached_docker_image"
      },
      {
        "date": "2023-02-04T05:34:00.000Z",
        "voteCount": 1,
        "content": "Option D, adding the --cache-from argument to the Docker build step in the build config file, would be the best option to optimize deployment times.\n\nThe --cache-from argument allows you to specify a list of images that Docker should use as a cache source when building the image. If a layer in the current build matches a layer in one of the cache source images, Docker uses the cached layer instead of building it again, reducing the build time.\n\nOptions A and C may not have a significant impact on deployment times, and option B would likely add complexity and increase deployment times, as it would require deploying and managing a new Docker registry and using a VPC-based Cloud Build worker pool."
      },
      {
        "date": "2023-01-31T17:12:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/build/docs/optimize-builds/speeding-up-builds#using_a_cached_docker_image"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 249,
    "url": "https://www.examtopics.com/discussions/google/view/97500-exam-professional-cloud-developer-topic-1-question-249/",
    "body": "You are developing an event-driven application. You have created a topic to receive messages sent to Pub/Sub. You want those messages to be processed in real time. You need the application to be independent from any other system and only incur costs when new messages arrive. How should you configure the architecture?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the application on Compute Engine. Use a Pub/Sub push subscription to process new messages in the topic.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy your code on Cloud Functions. Use a Pub/Sub trigger to invoke the Cloud Function. Use the Pub/Sub API to create a pull subscription to the Pub/Sub topic and read messages from it.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the application on Google Kubernetes Engine. Use the Pub/Sub API to create a pull subscription to the Pub/Sub topic and read messages from it.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy your code on Cloud Functions. Use a Pub/Sub trigger to handle new messages in the topic.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 10,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 8,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-07-12T10:16:00.000Z",
        "voteCount": 1,
        "content": "This option is the most suitable. Cloud Functions are fully managed and serverless, meaning you only incur costs when your code is executed in response to incoming messages. Using a Pub/Sub trigger ensures that the Cloud Function is invoked in real-time when new messages arrive in the topic, perfectly aligning with the requirements."
      },
      {
        "date": "2024-04-09T18:46:00.000Z",
        "voteCount": 2,
        "content": "D: Deploying your code on Cloud Functions and using a Pub/Sub trigger to handle new messages in the topic allows for a real-time, event-driven architecture. Cloud Functions only incur costs when invoked, which aligns with the requirement to only incur costs when new messages arrive.\n\n\nB: With Cloud Functions, there's no need to manually create a pull subscription. The Pub/Sub trigger handles the message retrieval."
      },
      {
        "date": "2024-02-04T22:27:00.000Z",
        "voteCount": 1,
        "content": "I will go for D."
      },
      {
        "date": "2023-12-03T12:34:00.000Z",
        "voteCount": 1,
        "content": "Alternative D is correct.\nA \"push subscription\" (not \"pull\"!) is more suitable when messages must be processed in real-time. Message ingested in the Pub/Sub topic, message pushed and retried recurrently until acknowledged."
      },
      {
        "date": "2023-10-07T02:34:00.000Z",
        "voteCount": 1,
        "content": "Not sure why we are complicating!! D is the right option"
      },
      {
        "date": "2023-09-24T08:34:00.000Z",
        "voteCount": 1,
        "content": "I would go with D."
      },
      {
        "date": "2023-08-08T13:22:00.000Z",
        "voteCount": 2,
        "content": "B is a very detailed answer and it is a right choice.\nD is missing info like cloud function to subscribe pub sub topic to handle new messages."
      },
      {
        "date": "2023-04-30T17:00:00.000Z",
        "voteCount": 4,
        "content": "Selected Answer:D\nhttps://cloud.google.com/functions/docs/calling/pubsub\nWe selected D based on our experience with Cloud Functions and the material at the URL above.\nSince messages can be obtained from Cloud Functions arguments, we are not aware of the description of Subscription.\n\"only incur costs when new messages arrive.\" so it's OK to process on the trigger.\nI don't think real time means so strictly.\nFor the life of me, I can't find any reason why D is wrong, and it seems to me that B is an error because of the extra processing."
      },
      {
        "date": "2023-04-30T16:49:00.000Z",
        "voteCount": 2,
        "content": "Selected Answer:D\nhttps://cloud.google.com/functions/docs/calling/pubsub\nWe selected D based on our experience with Cloud Functions and the material at the URL above.\nSince messages can be obtained from Cloud Functions arguments, we are not aware of the description of Subscription."
      },
      {
        "date": "2023-03-02T01:39:00.000Z",
        "voteCount": 3,
        "content": "took my exam yesterday (01-03-2023) and this question was there"
      },
      {
        "date": "2023-12-01T12:54:00.000Z",
        "voteCount": 1,
        "content": "and what is the answer? option D?"
      },
      {
        "date": "2023-02-04T05:47:00.000Z",
        "voteCount": 4,
        "content": "Option D is not ideal because using a Pub/Sub trigger to handle new messages in a topic is not the most efficient way to process messages in real time. In a trigger-based architecture, Cloud Functions are invoked only when new messages are available, so there is a possibility of delays in processing.\n\nOn the other hand, Option B provides a more efficient architecture for real-time processing. A Cloud Function is invoked for each message received in the Pub/Sub topic, providing immediate processing as messages arrive. This way, the application is independent from any other system and incurs costs only when new messages arrive, fulfilling the requirements stated in the question."
      },
      {
        "date": "2023-01-31T17:05:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/solutions/event-driven-architecture-pubsub"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 250,
    "url": "https://www.examtopics.com/discussions/google/view/97499-exam-professional-cloud-developer-topic-1-question-250/",
    "body": "You have an application running on Google Kubernetes Engine (GKE). The application is currently using a logging library and is outputting to standard output. You need to export the logs to Cloud Logging, and you need the logs to include metadata about each request. You want to use the simplest method to accomplish this. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange your application\u2019s logging library to the Cloud Logging library, and configure your application to export logs to Cloud Logging.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate your application to output logs in JSON format, and add the necessary metadata to the JSON.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate your application to output logs in CSV format, and add the necessary metadata to the CSV.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall the Fluent Bit agent on each of your GKE nodes, and have the agent export all logs from /var/log."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 10,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 9,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-04-17T13:36:00.000Z",
        "voteCount": 12,
        "content": "The answer is B since GKE is integrated with Cloud Logging by default.\n\n\"By default, GKE clusters are natively integrated with Cloud Logging (and Monitoring). When you create a GKE cluster, both Monitoring and Cloud Logging are enabled by default.\"\n\"GKE deploys a per-node logging agent that reads container logs, adds helpful metadata, and then sends the logs to the logs router, which sends the logs to Cloud Logging and any of the Logging sink destinations that you have configured. Cloud Logging stores logs for the duration that you specify or 30 days by default. Because Cloud Logging automatically collects standard output and error logs for containerized processes, you can start viewing your logs as soon as your application is deployed.\"\n\nSource: https://cloud.google.com/blog/products/management-tools/using-logging-your-apps-running-kubernetes-engine"
      },
      {
        "date": "2024-08-28T06:20:00.000Z",
        "voteCount": 1,
        "content": "This question is from the Google Official Example Questions, B is the correct answer because it is the easiest way."
      },
      {
        "date": "2024-07-13T11:46:00.000Z",
        "voteCount": 3,
        "content": "B is correct because it\u2019s the easiest way to get a rich format into Cloud Logging. GKE automatically forwards logs sent to stdout to Cloud Logging. As long as it has the right JSON format, Cloud Logging will ingest the rich message"
      },
      {
        "date": "2024-07-13T11:47:00.000Z",
        "voteCount": 1,
        "content": "answer from google practice exam"
      },
      {
        "date": "2024-07-13T11:49:00.000Z",
        "voteCount": 1,
        "content": "Feedback\nB is correct because it\u2019s the easiest way to get a rich format into Cloud Logging. GKE automatically forwards logs sent to stdout to Cloud Logging. As long as it has the right JSON format, Cloud Logging will ingest the rich message\nA is not correct because it would require a lot of extra work to replace the library, and replicate the extra information (such as pod name) that the GKE logs exporter automatically provides.\nD is not correct because this is only needed for pods (normally special/privileged pods) that write directly to the GKE file system.\nC is not correct because Cloud Logging doesn\u2019t support ingesting CSV."
      },
      {
        "date": "2024-06-11T01:54:00.000Z",
        "voteCount": 1,
        "content": "Enhanced Metadata: The Cloud Logging library can automatically include valuable metadata about each request, such as request ID, user agent, and resource information.\n\nwhy not B. Update your application to output logs in JSON format, and add the necessary metadata to the JSON: While this is possible, it requires more manual effort to format logs and ensure all the necessary metadata is included."
      },
      {
        "date": "2024-02-04T22:39:00.000Z",
        "voteCount": 3,
        "content": "I will go for B.\n\n In Google Kubernetes Engine (GKE), the standard output (stdout) of containers is automatically sent to Cloud Logging. This means that if your application in GKE prints logs to standard output, these logs will be captured and can be viewed in Cloud Logging without additional configuration.\n\nAnd if the logs are in JSON is better for processing.\n\nOption A is irrelevant because the logs have been sent already to cloud logging via standar output."
      },
      {
        "date": "2023-12-25T14:24:00.000Z",
        "voteCount": 2,
        "content": "since we need to update the application, the best solution between A, B and C is A."
      },
      {
        "date": "2023-12-03T17:13:00.000Z",
        "voteCount": 2,
        "content": "B for me: we're looking for the simplest method and I feel it's easier to configure the existing library to output JSON and include some context metadata rather than changing every log statement to use Cloud Logging library."
      },
      {
        "date": "2023-08-08T13:31:00.000Z",
        "voteCount": 1,
        "content": "The key here is \"use the simplest method to accomplish this...\", using Cloud logging library is a very simple and straight forward solution.  If the app is running on the single and multiple VMs in a instance group, then installing the cloud logging agent must be the correct answer.  This environment is GKE cluster and separate some normal VM workflow."
      },
      {
        "date": "2023-12-03T17:12:00.000Z",
        "voteCount": 1,
        "content": "How is it simpler to change every logging statement to use a different library rather than configuring the existing one to ouput in JSON and automatically append context metadata? I'd go for B"
      },
      {
        "date": "2023-04-16T12:53:00.000Z",
        "voteCount": 1,
        "content": "to log request metadata\nhttps://cloud.google.com/logging/docs/reference/libraries#write_request_logs"
      },
      {
        "date": "2023-03-02T01:39:00.000Z",
        "voteCount": 2,
        "content": "took my exam yesterday (01-03-2023) and this question was there"
      },
      {
        "date": "2023-02-19T10:36:00.000Z",
        "voteCount": 1,
        "content": "When you write logs from your service or job, they will be picked up automatically by Cloud Logging so long as the logs are written to any of these locations:\n\nStandard output (stdout) or standard error (stderr) streams\nAny files under the /var/log directory\nsyslog (/dev/log)\nLogs written using Cloud Logging client libraries, which are available for many popular languages\n\nhttps://cloud.google.com/run/docs/logging#container-logs"
      },
      {
        "date": "2023-02-28T00:04:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/stackdriver/docs/solutions/gke/managing-logs#what_logs"
      },
      {
        "date": "2023-02-04T05:53:00.000Z",
        "voteCount": 4,
        "content": "Option D, installing the Fluent Bit agent on each of your GKE nodes, is not the most straightforward method for exporting logs to Cloud Logging, as it requires manual configuration and management of the Fluent Bit agent. While Fluent Bit can be used to collect and forward logs to Cloud Logging, it is typically used for more complex logging scenarios where custom log processing is required.\n\nUsing the Cloud Logging library, as described in Option A, is a simpler and more direct method for exporting logs to Cloud Logging, as it eliminates the need to manage an additional log agent and provides a more integrated solution for logging in a GKE environment."
      },
      {
        "date": "2023-02-01T04:47:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/blog/products/management-tools/using-logging-your-apps-running-kubernetes-engine"
      },
      {
        "date": "2023-01-31T16:59:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/run/docs/logging#container-logs"
      },
      {
        "date": "2023-02-05T13:16:00.000Z",
        "voteCount": 2,
        "content": "Answer A not D"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 251,
    "url": "https://www.examtopics.com/discussions/google/view/97498-exam-professional-cloud-developer-topic-1-question-251/",
    "body": "You are working on a new application that is deployed on Cloud Run and uses Cloud Functions. Each time new features are added, new Cloud Functions and Cloud Run services are deployed. You use ENV variables to keep track of the services and enable interservice communication, but the maintenance of the ENV variables has become difficult. You want to implement dynamic discovery in a scalable way. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure your microservices to use the Cloud Run Admin and Cloud Functions APIs to query for deployed Cloud Run services and Cloud Functions in the Google Cloud project.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Service Directory namespace. Use API calls to register the services during deployment, and query during runtime.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRename the Cloud Functions and Cloud Run services endpoint is using a well-documented naming convention.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy Hashicorp Consul on a single Compute Engine instance. Register the services with Consul during deployment, and query during runtime."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-09-24T21:16:00.000Z",
        "voteCount": 1,
        "content": "B is correct."
      },
      {
        "date": "2023-08-08T13:35:00.000Z",
        "voteCount": 1,
        "content": "Creating a service registry is a right choice.  B is correct.\nOne more way is to write the new service urls to the config server registry so that other application/services can fetch those urls dynamically as and when required."
      },
      {
        "date": "2023-08-10T14:50:00.000Z",
        "voteCount": 1,
        "content": "One example of creating service directory to use in subsequent MS calls is seen in Microservice Orchestration Design principles and patterns."
      },
      {
        "date": "2023-03-02T01:38:00.000Z",
        "voteCount": 2,
        "content": "took my exam yesterday (01-03-2023) and this question was there"
      },
      {
        "date": "2023-02-26T06:32:00.000Z",
        "voteCount": 2,
        "content": "service directory for egistration and discovery of services"
      },
      {
        "date": "2023-02-04T06:19:00.000Z",
        "voteCount": 2,
        "content": "Service Directory provides a scalable way to manage the registration and discovery of services. By creating a namespace, you can use API calls to register your Cloud Run and Cloud Functions services, and query them during runtime. This allows for dynamic discovery and eliminates the need for manually updating environment variables. Service Directory also provides features such as service health checks and metadata, which can be used to further improve the reliability and scalability of your application."
      },
      {
        "date": "2023-01-31T16:47:00.000Z",
        "voteCount": 2,
        "content": "https://medium.com/google-cloud/fine-grained-cloud-dns-iam-via-service-directory-446058b4362e\nhttps://cloud.google.com/service-directory/docs/overview"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 252,
    "url": "https://www.examtopics.com/discussions/google/view/97495-exam-professional-cloud-developer-topic-1-question-252/",
    "body": "You work for a financial services company that has a container-first approach. Your team develops microservices applications. A Cloud Build pipeline creates the container image, runs regression tests, and publishes the image to Artifact Registry. You need to ensure that only containers that have passed the regression tests are deployed to Google Kubernetes Engine (GKE) clusters. You have already enabled Binary Authorization on the GKE clusters. What should you do next?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an attestor and a policy. After a container image has successfully passed the regression tests, use Cloud Build to run Kritis Signer to create an attestation for the container image.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy Voucher Server and Voucher Client components. After a container image has successfully passed the regression tests, run Voucher Client as a step in the Cloud Build pipeline.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the Pod Security Standard level to Restricted for the relevant namespaces. Use Cloud Build to digitally sign the container images that have passed the regression tests.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an attestor and a policy. Create an attestation for the container images that have passed the regression tests as a step in the Cloud Build pipeline.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 8,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 7,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-09-08T01:39:00.000Z",
        "voteCount": 1,
        "content": "simpler thatn using Kritis"
      },
      {
        "date": "2024-06-05T08:47:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/binary-authorization/docs/cloud-build\n\nAttestation Creation: The key difference is that you don't necessarily need to use Kritis Signer within Cloud Build to create the attestation. The Cloud Build documentation shows how you can use the gcloud beta container binauthz attestations create command directly within your Cloud Build steps to generate the attestation."
      },
      {
        "date": "2024-02-04T23:18:00.000Z",
        "voteCount": 4,
        "content": "I will go for D.\n\nD: The next step, after enable Binary Auth, is creating an attestor and a policy and then configure the attestation step in the cloud build pipeline.\n\nNot A because when you use kritis to sign an image you must provide the private key file from the attestor. And for that you must save the private key when you create the attestor for it later use. Its more complicated.\n\nNot C because pod security standard level to restricted don't enforce the use of signed images."
      },
      {
        "date": "2024-02-04T23:20:00.000Z",
        "voteCount": 1,
        "content": "With option D the cloud build step could looks like:\n\n  - name: 'gcr.io/cloud-builders/gcloud'\n    entrypoint: 'bash'\n    args: [ '-c', 'gcloud container binauthz create-signature --artifact-url gcr.io/&lt;PROJECT_ID&gt;/&lt;IMAGE_NAME&gt;:signed --attestor &lt;ATTESTOR_NAME&gt; --keyversion &lt;KEY_VERSION&gt; --project &lt;PROJECT_ID&gt;' ]"
      },
      {
        "date": "2023-09-24T21:19:00.000Z",
        "voteCount": 1,
        "content": "A is correct."
      },
      {
        "date": "2023-08-08T13:43:00.000Z",
        "voteCount": 1,
        "content": "I go with A since it is detailed and more specific about Kritis digital signature."
      },
      {
        "date": "2023-06-11T11:55:00.000Z",
        "voteCount": 1,
        "content": "A. For folks wonder what differences between Kritis Signer and Voucher Server Voucher Client, I asked Google Bard about it. Bard stated Kritis Signer is a command-line tools, whereas Voucher Server Voucher Client is a web-based tool. I then tried to verify that with Google search and Google image search (search \"voucher server voucher client\" then click Images). It seems Bard report correctly. Someone even wrote a Kritis Signer integrated pipeline with terraform (https://xebia.com/blog/how-to-automate-the-kritis-signer-on-google-cloud-platform/) . \nAlso, yes, both Kritis Signer and Voucher Server Voucher Client have Google official documentations. However, if you look carefully on Voucher Server Voucher Client Google official doc, they use curl to the Voucher Server address, which indirectly prove Vouch Server Vouch Client is a web-based tool."
      },
      {
        "date": "2023-04-16T13:31:00.000Z",
        "voteCount": 1,
        "content": "question is not about checking vulnerabilities.\nits not A. The Kritis Signer is a command-line utility to check whether an image violates the policy on security vulnerabilities.\nits not a voucher too."
      },
      {
        "date": "2023-04-25T11:55:00.000Z",
        "voteCount": 2,
        "content": "its D definitely\nhttps://cloud.google.com/binary-authorization/docs/cloud-build"
      },
      {
        "date": "2023-03-02T01:38:00.000Z",
        "voteCount": 3,
        "content": "took my exam yesterday (01-03-2023) and this question was there"
      },
      {
        "date": "2023-02-26T06:39:00.000Z",
        "voteCount": 2,
        "content": "info on voucher server: https://cloud.google.com/binary-authorization/docs/creating-attestations-voucher"
      },
      {
        "date": "2023-02-26T06:37:00.000Z",
        "voteCount": 1,
        "content": "Kritis Signer is an open source command-line tool that can create Binary Authorization attestations based on a policy that you configure. You can also use Kritis Signer to create attestations after checking an image for vulnerabilities identified by Container Analysis.\n\nhttps://cloud.google.com/binary-authorization/docs/creating-attestations-kritis"
      },
      {
        "date": "2023-02-04T06:26:00.000Z",
        "voteCount": 1,
        "content": "Binary Authorization in GKE provides a way to enforce that only verified container images are deployed in a cluster. In this scenario, to ensure that only containers that have passed the regression tests are deployed, you would create an attestor and a policy in Binary Authorization, and use Kritis Signer to create an attestation for the container image after it has passed the tests. The attestation verifies that the image meets the policy's criteria and is authorized to be deployed. This provides a secure and automated way to enforce that only containers that have passed the required tests are deployed in the cluster."
      },
      {
        "date": "2023-02-04T06:26:00.000Z",
        "voteCount": 2,
        "content": "Kritis Signer is a component of the Kritis project, which is an open-source implementation of Binary Authorization for Kubernetes. Kritis Signer is used to sign container images and create attestations, which verify that the image meets the criteria specified in a Binary Authorization policy. These attestations can be used to enforce that only authorized containers are deployed in a cluster, providing an additional layer of security for your containerized applications."
      },
      {
        "date": "2023-01-31T16:39:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/binary-authorization/docs/creating-attestations-kritis"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 253,
    "url": "https://www.examtopics.com/discussions/google/view/97492-exam-professional-cloud-developer-topic-1-question-253/",
    "body": "You are reviewing and updating your Cloud Build steps to adhere to best practices. Currently, your build steps include:<br><br>1. Pull the source code from a source repository.<br>2. Build a container image<br>3. Upload the built image to Artifact Registry.<br><br>You need to add a step to perform a vulnerability scan of the built container image, and you want the results of the scan to be available to your deployment pipeline running in Google Cloud. You want to minimize changes that could disrupt other teams\u2019 processes. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Binary Authorization, and configure it to attest that no vulnerabilities exist in a container image.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpload the built container images to your Docker Hub instance, and scan them for vulnerabilities.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable the Container Scanning API in Artifact Registry, and scan the built container images for vulnerabilities.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd Artifact Registry to your Aqua Security instance, and scan the built container images for vulnerabilities."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 7,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-12-18T01:54:00.000Z",
        "voteCount": 1,
        "content": "I'm not so sure about C because the task is to add a STEP to our Cloud Build pipeline to perform the vulnerability scan, whereas C implies more doing the job via Cloud Console. Why would we enable the Container Scanning API in Artifact Registry every time we run the pipeline?\n\nThis scenario is similar to what we have in question #252. I'd go with A:\nhttps://cloud.google.com/binary-authorization/docs/creating-attestations-kritis"
      },
      {
        "date": "2023-09-24T22:14:00.000Z",
        "voteCount": 1,
        "content": "C is correct."
      },
      {
        "date": "2023-08-08T13:46:00.000Z",
        "voteCount": 1,
        "content": "C is right.  Requirement is to perform a vulnerability scan of the built container image.\nC states Enable the Container Scanning API in Artifact Registry, and scan the built container images for vulnerabilities.  Further steps for better security would be to follow option A."
      },
      {
        "date": "2023-02-26T06:41:00.000Z",
        "voteCount": 2,
        "content": "i choose C"
      },
      {
        "date": "2023-02-04T06:30:00.000Z",
        "voteCount": 2,
        "content": "Enabling the Container Scanning API in Artifact Registry and scanning the built container images is a best practice because it allows you to perform security scans within the same environment where the built images are stored. This helps minimize the changes that could disrupt other teams' processes, as the images are already in Artifact Registry, and the scanning results can be easily accessed by the deployment pipeline in Google Cloud. Additionally, the Container Scanning API integrates with Google Cloud security and governance tools, allowing you to enforce security policies and manage vulnerabilities in a centralized and automated way."
      },
      {
        "date": "2023-01-31T16:28:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/container-analysis/docs/automated-scanning-howto#view_the_image_vulnerabilities"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 254,
    "url": "https://www.examtopics.com/discussions/google/view/97547-exam-professional-cloud-developer-topic-1-question-254/",
    "body": "You are developing an online gaming platform as a microservices application on Google Kubernetes Engine (GKE). Users on social media are complaining about long loading times for certain URL requests to the application. You need to investigate performance bottlenecks in the application and identify which HTTP requests have a significantly high latency span in user requests. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure GKE workload metrics using kubectl. Select all Pods to send their metrics to Cloud Monitoring. Create a custom dashboard of application metrics in Cloud Monitoring to determine performance bottlenecks of your GKE cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate your microservices to log HTTP request methods and URL paths to STDOUT. Use the logs router to send container logs to Cloud Logging. Create filters in Cloud Logging to evaluate the latency of user requests across different methods and URL paths.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstrument your microservices by installing the OpenTelemetry tracing package. Update your application code to send traces to Trace for inspection and analysis. Create an analysis report on Trace to analyze user requests.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall tcpdump on your GKE nodes. Run tcpdump to capture network traffic over an extended period of time to collect data. Analyze the data files using Wireshark to determine the cause of high latency."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 9,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-04-09T20:04:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/trace/docs/setup/python-ot"
      },
      {
        "date": "2023-09-24T22:35:00.000Z",
        "voteCount": 1,
        "content": "This approach allows you to update your application code to send traces to Trace for inspection and analysis. You can then create an analysis report on Trace to analyze user requests. This will help you identify which HTTP requests have a significantly high latency span in user requests, which seems to be the main concern according to the complaints from users on social media."
      },
      {
        "date": "2023-08-08T13:58:00.000Z",
        "voteCount": 1,
        "content": "There is no best fit other than C here."
      },
      {
        "date": "2023-04-26T06:30:00.000Z",
        "voteCount": 3,
        "content": "This is the best way to investigate performance bottlenecks in a microservices application. By using OpenTelemetry, you can collect traces from all of your microservices and analyze them in Trace. This will allow you to identify which requests are taking the longest and where the bottlenecks are occurring."
      },
      {
        "date": "2023-03-02T01:38:00.000Z",
        "voteCount": 2,
        "content": "took my exam yesterday (01-03-2023) and this question was there"
      },
      {
        "date": "2023-02-27T23:55:00.000Z",
        "voteCount": 1,
        "content": "correcting my choice"
      },
      {
        "date": "2023-02-19T03:54:00.000Z",
        "voteCount": 1,
        "content": "question clearly says: performance botlenecks and which step is having latency ---&gt; Cloud Trace\nhttps://cloud.google.com/trace/docs/overview"
      },
      {
        "date": "2023-02-27T23:55:00.000Z",
        "voteCount": 1,
        "content": "i selected the wrong, it's C"
      },
      {
        "date": "2023-02-05T14:13:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/trace/docs/setup#when-to-instrument"
      },
      {
        "date": "2023-02-04T06:37:00.000Z",
        "voteCount": 1,
        "content": "Instrumenting your microservices with the OpenTelemetry tracing package, updating your application code to send traces to Trace for inspection and analysis, and creating an analysis report on Trace would be the recommended solution for investigating performance bottlenecks in the application and identifying HTTP requests with high latency. This would allow you to visualize and analyze the complete request-response cycle and identify specific parts of the application that might be contributing to long loading times."
      },
      {
        "date": "2023-02-01T06:14:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/stackdriver/docs/solutions/gke/workload-metrics"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 255,
    "url": "https://www.examtopics.com/discussions/google/view/97491-exam-professional-cloud-developer-topic-1-question-255/",
    "body": "You need to load-test a set of REST API endpoints that are deployed to Cloud Run. The API responds to HTTP POST requests. Your load tests must meet the following requirements:<br>\u2022\tLoad is initiated from multiple parallel threads.<br>\u2022\tUser traffic to the API originates from multiple source IP addresses.<br>\u2022\tLoad can be scaled up using additional test instances.<br><br>You want to follow Google-recommended best practices. How should you configure the load testing?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an image that has cURL installed, and configure cURL to run a test plan. Deploy the image in a managed instance group, and run one instance of the image for each VM.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an image that has cURL installed, and configure cURL to run a test plan. Deploy the image in an unmanaged instance group, and run one instance of the image for each VM.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy a distributed load testing framework on a private Google Kubernetes Engine cluster. Deploy additional Pods as needed to initiate more traffic and support the number of concurrent users.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDownload the container image of a distributed load testing framework on Cloud Shell. Sequentially start several instances of the container on Cloud Shell to increase the load on the API."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 6,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-02-04T23:28:00.000Z",
        "voteCount": 1,
        "content": "I will go for C."
      },
      {
        "date": "2023-08-08T14:31:00.000Z",
        "voteCount": 1,
        "content": "Requirements are very clear.  Load testing with concurrent users/threads + Multiple source origins/IP address, C is the best choice."
      },
      {
        "date": "2023-08-08T14:33:00.000Z",
        "voteCount": 1,
        "content": "Option D is closer but we cannot instantiate the mutliple requests with different IP addresses."
      },
      {
        "date": "2023-05-06T05:46:00.000Z",
        "voteCount": 1,
        "content": "It's normal to launch some compute engine from cloud shell.\nI think we can increase right load by increasing Compute Engine which do load-test from Cloud Shell step by step.\nCan the test from GKE cover the condition which is \"from multiple source IP addresses.\"."
      },
      {
        "date": "2023-05-06T05:51:00.000Z",
        "voteCount": 1,
        "content": "I feel that load testing from the Compute Engine is more accurate than load testing from the Pod.\nI'm not sure what to think.\nIs it possible that GKE's engress is a bottleneck and load testing is not possible?"
      },
      {
        "date": "2023-02-04T06:52:00.000Z",
        "voteCount": 3,
        "content": "Option D, which involves starting several instances of a load testing framework container on Cloud Shell, may not be a recommended approach for several reasons:\n\nCloud Shell is a shell environment for managing resources hosted on Google Cloud and does not provide a scalable infrastructure for running load tests.\n\nStarting several instances of a container on Cloud Shell is not a highly available or scalable solution for load testing, and may not provide sufficient parallelism or control over the source IP addresses of the traffic.\n\nUsing a private Google Kubernetes Engine cluster to deploy a distributed load testing framework allows for scaling up the load testing by deploying additional Pods, which can provide more control over the number of concurrent users and the source IP addresses of the traffic, and can provide a more robust and scalable infrastructure for load testing."
      },
      {
        "date": "2023-05-06T05:45:00.000Z",
        "voteCount": 1,
        "content": "\"Cloud Shell is a shell environment for managing resources hosted on Google Cloud and does not provide a scalable infrastructure for running load tests.\"\nI agree with this explanation.\nBut\n\"Starting several instances of a container on Cloud Shell is not a highly available or scalable solution for load testing, and may not provide sufficient parallelism or control over the source IP addresses of the traffic.\"\nI can't agree with this explanation.\nPlease teach me where this explanation is written.\nIt's normal to launch some compute engine from cloud shell.\nI think we can increase right load by increasing Compute Engine which do load-test from Cloud Shell step by step.\nCan the test from GKE cover the condition which is \"from multiple source IP addresses.\".\nI think this question's answer is D."
      },
      {
        "date": "2023-02-01T04:30:00.000Z",
        "voteCount": 1,
        "content": "nope Answer is D....not  C"
      },
      {
        "date": "2023-02-01T06:16:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/run/docs/about-load-testing"
      },
      {
        "date": "2023-01-31T16:04:00.000Z",
        "voteCount": 1,
        "content": "To deploy the load testing tasks, you do the following:\nDeploy a load testing master.\nDeploy a group of load testing workers. With these load testing workers, you can create a substantial amount of traffic for testing purposes.\nhttps://cloud.google.com/run/docs/about-load-testing\nhttps://cloud.google.com/architecture/distributed-load-testing-using-gke#build_the_container_image\nAnswer"
      },
      {
        "date": "2023-02-01T23:15:00.000Z",
        "voteCount": 1,
        "content": "This tutorial explains how to use Google Kubernetes Engine (GKE) to deploy a distributed load testing framework that uses multiple containers to create traffic for a simple REST-based API. This tutorial load-tests a web application deployed to App Engine that exposes REST-style endpoints to respond to incoming HTTP POST requests."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 256,
    "url": "https://www.examtopics.com/discussions/google/view/107095-exam-professional-cloud-developer-topic-1-question-256/",
    "body": "Your team is creating a serverless web application on Cloud Run. The application needs to access images stored in a private Cloud Storage bucket. You want to give the application Identity and Access Management (IAM) permission to access the images in the bucket, while also securing the services using Google-recommended best practices. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnforce signed URLs for the desired bucket. Grant the Storage Object Viewer IAM role on the bucket to the Compute Engine default service account.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnforce public access prevention for the desired bucket. Grant the Storage Object Viewer IAM role on the bucket to the Compute Engine default service account.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnforce signed URLs for the desired bucket. Create and update the Cloud Run service to use a user-managed service account. Grant the Storage Object Viewer IAM role on the bucket to the service account.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnforce public access prevention for the desired bucket. Create and update the Cloud Run service to use a user-managed service account. Grant the Storage Object Viewer IAM role on the bucket to the service account.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-04T23:37:00.000Z",
        "voteCount": 1,
        "content": "I will go for D.\n\nOption C sounds good, but as the service account only have the Storage Object Viewer role it can't generate signed URLs for files from bucket because need storage.object.get, then  it's incorrect."
      },
      {
        "date": "2023-09-24T22:48:00.000Z",
        "voteCount": 1,
        "content": "This approach allows you to secure your Cloud Storage bucket by enforcing public access prevention, which prevents data from being accidentally shared with the public. By creating and updating the Cloud Run service to use a user-managed service account, you can ensure that only this service has access to the bucket. Granting the Storage Object Viewer IAM role on the bucket to the service account allows the service to read objects stored in the bucket."
      },
      {
        "date": "2023-08-08T14:40:00.000Z",
        "voteCount": 2,
        "content": "D is right.\n1) Create service account with role of viewing the objects under Cloud storage bucket\n2) Create policies to prevent public access to the bucket.\n\nA and C: Neither of these are close to the solution.\nB is somewhat closer but the statement \"Grant the Storage Object Viewer IAM role on the bucket to the Compute Engine default service account\" is wrong since we need to create service account for the application and not for VM."
      },
      {
        "date": "2023-04-26T06:23:00.000Z",
        "voteCount": 2,
        "content": "most secure and efficient way to give the application Identity and Access Management (IAM) permission to access the images in the bucket."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 257,
    "url": "https://www.examtopics.com/discussions/google/view/107582-exam-professional-cloud-developer-topic-1-question-257/",
    "body": "You are using Cloud Run to host a global ecommerce web application. Your company\u2019s design team is creating a new color scheme for the web app. You have been tasked with determining whether the new color scheme will increase sales. You want to conduct testing on live production traffic. How should you design the study?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an external HTTP(S) load balancer to route a predetermined percentage of traffic to two different color schemes of your application. Analyze the results to determine whether there is a statistically significant difference in sales.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an external HTTP(S) load balancer to route traffic to the original color scheme while the new deployment is created and tested. After testing is complete, reroute all traffic to the new color scheme. Analyze the results to determine whether there is a statistically significant difference in sales.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an external HTTP(S) load balancer to mirror traffic to the new version of your application. Analyze the results to determine whether there is a statistically significant difference in sales.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable a feature flag that displays the new color scheme to half of all users. Monitor sales to see whether they increase for this group of users."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 9,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 5,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-05-07T05:08:00.000Z",
        "voteCount": 6,
        "content": "Correct answer is A. This is classic A/B testing. Since you already have a new version, built into an image, all you need do is to use the load balancer to split traffic going to old version and new version. See: https://cloud.google.com/load-balancing/docs/l7-internal/traffic-management#traffic_actions_weight-based_traffic_splitting. Note that global load balancers can route to serverless services. https://cloud.google.com/load-balancing/docs/https/setting-up-https-serverless"
      },
      {
        "date": "2024-07-07T15:58:00.000Z",
        "voteCount": 2,
        "content": "Besides Cloud Run alone doesn't provide Feature Flag solution that requires Firebase Remote Config feature. For an A/B test you have to fix what users are using what version.\nOption A doesn't not provide a complete solution for something that is crucial.\nSo, besides both seems incomplete, if that request was made to me, I'd implement as a Feature Flag using Firebase Remote Config"
      },
      {
        "date": "2023-12-02T19:50:00.000Z",
        "voteCount": 1,
        "content": "Considering the importance of traffic analysis and the need for precise control over traffic distribution for a global ecommerce web application, Option A is likely the better choice. This option allows for detailed monitoring and analysis of user interactions with different color schemes, offering clear insights into which version performs better in terms of sales. The use of an external HTTP(S) load balancer for traffic routing provides a more controlled environment for conducting such a study."
      },
      {
        "date": "2023-09-24T22:51:00.000Z",
        "voteCount": 1,
        "content": "A is correct."
      },
      {
        "date": "2023-08-08T14:44:00.000Z",
        "voteCount": 1,
        "content": "A is right.  D is specifying 50% of the users which is not correct.  In really the traffic split is 80-20 or 75-25 ratio.  This is a specialized version of canary depolyments."
      },
      {
        "date": "2023-04-26T06:17:00.000Z",
        "voteCount": 3,
        "content": "This is the best way to test the new color scheme on live production traffic. By enabling a feature flag, you can display the new color scheme to a subset of users while keeping the old color scheme for the rest of the users. This will allow you to compare sales between the two groups of users and determine whether the new color scheme has a statistically significant impact on sales."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 258,
    "url": "https://www.examtopics.com/discussions/google/view/107579-exam-professional-cloud-developer-topic-1-question-258/",
    "body": "You are a developer at a large corporation. You manage three Google Kubernetes Engine clusters on Google Cloud. Your team\u2019s developers need to switch from one cluster to another regularly without losing access to their preferred development tools. You want to configure access to these multiple clusters while following Google-recommended best practices. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAsk the developers to use Cloud Shell and run gcloud container clusters get-credential to switch to another cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn a configuration file, define the clusters, users, and contexts. Share the file with the developers and ask them to use kubect1 contig to add cluster, user, and context details.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAsk the developers to install the gcloud CLI on their workstation and run gcloud container clusters get-credentials to switch to another cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAsk the developers to open three terminals on their workstation and use kubect1 config to configure access to each cluster."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 8,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 6,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-10-10T19:27:00.000Z",
        "voteCount": 1,
        "content": "C. Ask the developers to install the gcloud CLI on their workstation and run gcloud container clusters get-credentials to switch to another cluster.\n\nThis approach allows developers to easily switch between clusters while maintaining access to their preferred development tools. The gcloud container clusters get-credentials command updates the kubeconfig file, enabling seamless access to the specified cluster. It is a widely used method that aligns with best practices for managing Kubernetes contexts and simplifies the workflow for developers."
      },
      {
        "date": "2024-09-04T11:36:00.000Z",
        "voteCount": 1,
        "content": "The other options are incorrect because:\nA: Using Cloud Shell is not a scalable solution for multiple developers who need to switch between clusters regularly.\nB: While defining clusters, users, and contexts in a configuration file can be helpful, it's not the most efficient or secure way to manage access to multiple clusters. Sharing the configuration file with developers can also pose security risks.\nD: Opening three terminals for each cluster is cumbersome and not recommended. It's better to use a single tool like the gcloud CLI to manage multiple clusters."
      },
      {
        "date": "2024-07-07T15:52:00.000Z",
        "voteCount": 1,
        "content": "We are talking about an easy way and GKE. Not K8s self-managed.\nA question detail is that users don't want to change the tolls they use. So C is the right."
      },
      {
        "date": "2023-12-18T11:54:00.000Z",
        "voteCount": 2,
        "content": "B 100%\n\nhttps://kubernetes.io/docs/tasks/access-application-cluster/configure-access-multiple-clusters/"
      },
      {
        "date": "2023-12-18T11:53:00.000Z",
        "voteCount": 1,
        "content": "D 100%\n\nhttps://kubernetes.io/docs/tasks/access-application-cluster/configure-access-multiple-clusters/"
      },
      {
        "date": "2023-12-18T11:54:00.000Z",
        "voteCount": 1,
        "content": "Typo - B*"
      },
      {
        "date": "2023-09-24T22:54:00.000Z",
        "voteCount": 3,
        "content": "This approach allows developers to switch between different Google Kubernetes Engine clusters directly from their local workstation1. The gcloud container clusters get-credentials command configures kubectl with the credentials of the specified cluster1, making it easy for developers to switch contexts and interact with different clusters."
      },
      {
        "date": "2023-08-08T14:51:00.000Z",
        "voteCount": 3,
        "content": "https://kubernetes.io/docs/tasks/access-application-cluster/configure-access-multiple-clusters/\n\nCommand used: kubectl config use-context"
      },
      {
        "date": "2023-06-14T06:36:00.000Z",
        "voteCount": 1,
        "content": "https://kubernetes.io/docs/tasks/access-application-cluster/configure-access-multiple-clusters/"
      },
      {
        "date": "2023-04-26T06:13:00.000Z",
        "voteCount": 2,
        "content": "Option B is the best solution because it is secure, convenient, and time-efficient. By using a configuration file, you can define the clusters, users, and contexts that you want to use. You can then share the file with the developers, who can use it to add the cluster, user, and context details to their kubeconfig file. Once the developers have added the cluster, user, and context details to their kubeconfig file, they can switch to another cluster by using the following command: kubectl config use &lt;context-name&gt;"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 259,
    "url": "https://www.examtopics.com/discussions/google/view/107575-exam-professional-cloud-developer-topic-1-question-259/",
    "body": "You are a lead developer working on a new retail system that runs on Cloud Run and Firestore. A web UI requirement is for the user to be able to browse through all products. A few months after go-live, you notice that Cloud Run instances are terminated with HTTP 500: Container instances are exceeding memory limits errors during busy times. This error coincides with spikes in the number of Firestore queries.<br><br>You need to prevent Cloud Run from crashing and decrease the number of Firestore queries. You want to use a solution that optimizes system performance. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the query that returns the product list using cursors with limits.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a custom index over the products.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the query that returns the product list using integer offsets.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the Cloud Run configuration to increase the memory limits."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-09-24T22:56:00.000Z",
        "voteCount": 1,
        "content": "A is correct."
      },
      {
        "date": "2023-08-08T14:53:00.000Z",
        "voteCount": 1,
        "content": "A is Best.  Using pagination with limit on the size.  This is called Lazy loading of data."
      },
      {
        "date": "2023-04-26T06:02:00.000Z",
        "voteCount": 4,
        "content": "A cursor is a pointer to a specific location in a Firestore database. By using cursors with limits, you can control the number of documents that are returned in a query. This can help to reduce the number of Firestore queries that are made, which can improve performance and prevent Cloud Run from crashing."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 260,
    "url": "https://www.examtopics.com/discussions/google/view/107486-exam-professional-cloud-developer-topic-1-question-260/",
    "body": "You are a developer at a large organization. Your team uses Git for source code management (SCM). You want to ensure that your team follows Google-recommended best practices to manage code to drive higher rates of software delivery. Which SCM process should your team use?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEach developer commits their code to the main branch before each product release, conducts testing, and rolls back if integration issues are detected.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEach group of developers copies the repository, commits their changes to their repository, and merges their code into the main repository before each product release.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEach developer creates a branch for their own work, commits their changes to their branch, and merges their code into the main branch daily.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEach group of developers creates a feature branch from the main branch for their work, commits their changes to their branch, and merges their code into the main branch after the change advisory board approves it.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 7,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-12-18T12:00:00.000Z",
        "voteCount": 1,
        "content": "It's D"
      },
      {
        "date": "2023-12-11T00:32:00.000Z",
        "voteCount": 1,
        "content": "I don't get it why everybody here said D?\nIMHO Option D is FALSE bc - Creating feature branches for groups of developers and waiting for a change advisory board's approval can slow down the development process and might not align with the principles of agile and continuous delivery."
      },
      {
        "date": "2023-12-18T11:59:00.000Z",
        "voteCount": 1,
        "content": "As they say, better safe than sorry. When you rush, you're more prone to make a mistake and it often takes more time to fix a mistake than to prevent it. Also, check out the 4-eyes principle."
      },
      {
        "date": "2023-09-24T22:59:00.000Z",
        "voteCount": 1,
        "content": "D is correct."
      },
      {
        "date": "2023-08-07T11:27:00.000Z",
        "voteCount": 1,
        "content": "The defacto SCM process is described in option D.\nAdvisory board approves it means approving the PR raised by developer before merging into develop/master branch."
      },
      {
        "date": "2023-06-08T23:31:00.000Z",
        "voteCount": 1,
        "content": "B. Put all commits inside a single repo would place lots of burden on branching and commit history. This not only makes checkout slower but also hard to manage - think about a small-mid size team (10 developers) creating 200 commits per day, roughly equals to 156,000 for 3 months. The maintainers of a repo would have a hard time to chase down and merge 156,000 commits. Not to mention the repo permission management would be a nightmare if the repo extends to enterprise level (200 - 500 developers / SRE / qa / SA) .Forking repo as opt B would be a better choice. Each developer deals with his fork. PR merge and squash once ready. In fact, all famous OSS software, such as Kubernetes, Grafana, Prometheus, etc., use this approach."
      },
      {
        "date": "2023-04-26T05:58:00.000Z",
        "voteCount": 2,
        "content": "Use a centralized repository. A centralized repository is a single location where all of your team's code is stored. This makes it easy for everyone to access the latest code, and it also helps to prevent conflicts.\nUse branches. Branches are a way to create a separate version of the code for development purposes. This allows developers to work on new features or bug fixes without affecting the main branch of the code."
      },
      {
        "date": "2023-04-25T12:57:00.000Z",
        "voteCount": 2,
        "content": "You are a developer at a large organization"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 261,
    "url": "https://www.examtopics.com/discussions/google/view/107487-exam-professional-cloud-developer-topic-1-question-261/",
    "body": "You have a web application that publishes messages to Pub/Sub. You plan to build new versions of the application locally and want to quickly test Pub/Sub integration for each new build. How should you configure local testing?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall Cloud Code on the integrated development environment (IDE). Navigate to Cloud APIs, and enable Pub/Sub against a valid Google Project ID. When developing locally, configure your application to call pubsub.googleapis.com.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall the Pub/Sub emulator using gcloud, and start the emulator with a valid Google Project ID. When developing locally, configure your application to use the local emulator with ${gcloud beta emulators pubsub env-init}.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the Google Cloud console, navigate to the API Library, and enable the Pub/Sub API. When developing locally, configure your application to call pubsub.googleapis.com.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall the Pub/Sub emulator using gcloud, and start the emulator with a valid Google Project IWhen developing locally, configure your application to use the local emulator by exporting the PUBSUB_EMULATOR_HOST variable."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 6,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-04-09T22:36:00.000Z",
        "voteCount": 1,
        "content": "Here we have only single machine, so just using the terminal command is OK, no need to export ENV variables\nhttps://cloud.google.com/pubsub/docs/emulator#automatically_setting_the_variables"
      },
      {
        "date": "2023-12-03T12:24:00.000Z",
        "voteCount": 2,
        "content": "Alternative B is correct. Link: https://cloud.google.com/pubsub/docs/emulator#env\n\nExecuting \"gcloud beta emulators pubsub env-init\" is required for local testing when the application and the emulator run either on the same machine or on different machines. The export of the PUBSUB_EMULATOR_HOST variable is an additional step required only in the latter case (when the application and the emulator run on different machines)."
      },
      {
        "date": "2023-12-02T18:46:00.000Z",
        "voteCount": 1,
        "content": "Based on the common steps for implementing the Pub/Sub emulator, the best choice for configuring local testing of your web application's Pub/Sub integration would be:\n\nOption B: Install the Pub/Sub emulator using gcloud, and start the emulator with a valid Google Project ID. When developing locally, configure your application to use the local emulator with ${gcloud beta emulators pubsub env-init}.\n\nThis option covers the essential steps for both scenarios (same machine or different machines) and provides a clear path for setting up and utilizing the Pub/Sub emulator effectively for local development and testing."
      },
      {
        "date": "2023-09-25T01:51:00.000Z",
        "voteCount": 1,
        "content": "This approach allows you to test your application\u2019s integration with Pub/Sub without making actual calls to the Pub/Sub service, which can be time-consuming and may incur costs. Instead, your application interacts with the local emulator, which mimics the behavior of the actual Pub/Sub service. This makes it a fast and cost-effective solution for local testing. Remember to set the PUBSUB_EMULATOR_HOST environment variable to point your application to the local emulator."
      },
      {
        "date": "2023-06-10T06:38:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/pubsub/docs/emulator#automatically_setting_the_variables\n\n\nIf your application and the emulator run on the same machine, you can set the environment variables automatically with:\n1) $(gcloud beta emulators pubsub env-init) \n\n\nIf your application and the emulator run on different machines, set the environment variables manually with:\n1) Run the env-init command:  gcloud beta emulators pubsub env-init\n2) On the machine that runs your application, set the PUBSUB_EMULATOR_HOST"
      },
      {
        "date": "2023-05-21T22:52:00.000Z",
        "voteCount": 1,
        "content": "I agree with B as the correct answer.\nhttps://cloud.google.com/pubsub/docs/emulator#automatically_setting_the_variables\nYou can run gcloud beta emulators pubsub env-init in your own localhost within if your application and the emulator run on the same machine"
      },
      {
        "date": "2023-04-26T05:57:00.000Z",
        "voteCount": 1,
        "content": "Answer: D"
      },
      {
        "date": "2023-04-26T05:53:00.000Z",
        "voteCount": 1,
        "content": "Answer: D"
      },
      {
        "date": "2023-04-25T13:13:00.000Z",
        "voteCount": 1,
        "content": "gcloud beta emulators pubsub start --project=my-project-id\nexport PUBSUB_EMULATOR_HOST=localhost:8085\nexport GOOGLE_CLOUD_PROJECT=my-project-id"
      },
      {
        "date": "2023-04-25T13:21:00.000Z",
        "voteCount": 4,
        "content": "its B\nhttps://cloud.google.com/pubsub/docs/emulator#automatically_setting_the_variables"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 262,
    "url": "https://www.examtopics.com/discussions/google/view/108088-exam-professional-cloud-developer-topic-1-question-262/",
    "body": "Your ecommerce application receives external requests and forwards them to third-party API services for credit card processing, shipping, and inventory management as shown in the diagram.<br><br><img src=\"https://img.examtopics.com/professional-cloud-developer/image9.png\"><br><br>Your customers are reporting that your application is running slowly at unpredictable times. The application doesn\u2019t report any metrics. You need to determine the cause of the inconsistent performance. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall the OpenTelemetry library for your respective language, and instrument your application.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall the Ops Agent inside your container and configure it to gather application metrics.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify your application to read and forward the X-Cloud-Trace-Context header when it calls the downstream services.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Managed Service for Prometheus on the Google Kubernetes Engine cluster to gather application metrics."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 4,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-12-02T18:54:00.000Z",
        "voteCount": 1,
        "content": "The key part of the question prompting the use of OpenTelemetry over Prometheus is: \"Your customers are reporting that your application is running slowly at unpredictable times. The application doesn\u2019t report any metrics.\" This indicates a need for detailed instrumentation to trace and diagnose performance issues across various external service interactions. OpenTelemetry, with its comprehensive APIs and SDKs for collecting a wide range of telemetry data (traces, metrics, logs), is well-suited for this task. It allows for tracing the application's workflow and identifying bottlenecks, which is essential for understanding the root cause of the inconsistent performance."
      },
      {
        "date": "2023-09-25T03:03:00.000Z",
        "voteCount": 1,
        "content": "OpenTelemetry provides a single set of APIs, libraries, agents, and collector services to capture distributed traces and metrics from your application. You can analyze them using Prometheus, Jaeger, and other observability tools. This will help you understand the performance of your application and identify any bottlenecks or issues causing the slowdown. It\u2019s a comprehensive tool for observability, making it a suitable choice for this scenario."
      },
      {
        "date": "2023-08-07T11:10:00.000Z",
        "voteCount": 1,
        "content": "I go with A. Opentelelmetry standard supports tracing, metrics, logging. With tracing we can find out what is causing the performance issue.  Zipkin is almost obsolete now."
      },
      {
        "date": "2023-07-23T11:12:00.000Z",
        "voteCount": 2,
        "content": "I should be A."
      },
      {
        "date": "2023-06-22T17:57:00.000Z",
        "voteCount": 1,
        "content": "OpenTelemetry is a set of APIs, libraries, and agents that help you collect telemetry data (such as traces, metrics, and logs) from your applications. By instrumenting your application with OpenTelemetry, you can gather performance metrics, trace requests across different components, and identify potential bottlenecks or issues causing inconsistent performance."
      },
      {
        "date": "2023-06-14T06:53:00.000Z",
        "voteCount": 1,
        "content": "Managed Service for Prometheus collects metrics from Prometheus exporters and lets you query the data globally using PromQL, meaning that you can keep using any existing Grafana dashboards, PromQL-based alerts, and workflows. It is hybrid- and multi-cloud compatible, can monitor both Kubernetes and VM workloads, retains data for 24 months, and maintains portability by staying compatible with upstream Prometheus. \n\nhttps://cloud.google.com/stackdriver/docs/managed-prometheus"
      },
      {
        "date": "2023-05-01T02:46:00.000Z",
        "voteCount": 4,
        "content": "It should be A.\nhttps://cloud.google.com/trace/docs/trace-app-latency"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 263,
    "url": "https://www.examtopics.com/discussions/google/view/125908-exam-professional-cloud-developer-topic-1-question-263/",
    "body": "You are developing a new application. You want the application to be triggered only when a given file is updated in your Cloud Storage bucket. Your trigger might change, so your process must support different types of triggers. You want the configuration to be simple so that multiple team members can update the triggers in the future. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Cloud Storage events to be sent to Pub/Sub, and use Pub/Sub events to trigger a Cloud Build job that executes your application.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Eventarc trigger that monitors your Cloud Storage bucket for a specific filename, and set the target as Cloud Run.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a Cloud Function that executes your application and is triggered when an object is updated in Cloud Storage.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a Firebase function that executes your application and is triggered when an object is updated in Cloud Storage."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 6,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-06-29T00:20:00.000Z",
        "voteCount": 2,
        "content": "B- is correct because Eventarc supports exible lters and you can create triggers based\non lename patterns.\n\nQuestion is from Sample exam available on official GCP site.\n\nhttps://cloud.google.com/eventarc/docs/event-driven-architectures\nhttps://cloud.google.com/functions/docs/calling/storage\nhttps://cloud.google.com/storage/docs/reporting-changes\nhttps://cloud.google.com/build/docs/automating-builds/create-pubsub-triggers#gcs_build_trigger\nhttps://cloud.google.com/eventarc/docs/overview\nhttps://firebase.google.com/docs/functions/gcp-storage-events\nhttps://cloud.google.com/eventarc/docs/run/create-trigger#trigger-setup"
      },
      {
        "date": "2024-04-09T22:48:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/functions/docs/calling/storage"
      },
      {
        "date": "2023-12-04T02:10:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/functions/docs/calling"
      },
      {
        "date": "2023-12-02T19:11:00.000Z",
        "voteCount": 1,
        "content": "I change my answer to C, the choice of Option C (\"Configure a Cloud Function that executes your application and is triggered when an object is updated in Cloud Storage\") is strongly supported by the sentence \"You want the configuration to be simple so that multiple team members can update the triggers in the future.\" Cloud Functions provide a more straightforward and user-friendly approach for setting up and managing triggers, making it easier for various team members to work with and update the configuration as needed. This simplicity aligns well with the requirement for an easily manageable and modifiable trigger process."
      },
      {
        "date": "2023-12-02T19:04:00.000Z",
        "voteCount": 2,
        "content": "Given the requirement in the question for a process that supports different types of triggers and allows for easy updating by multiple team members, Option B (Create an Eventarc trigger that monitors your Cloud Storage bucket for a specific filename, and set the target as Cloud Run) seems more relevant.\n\nThis option offers greater flexibility for handling various types of triggers with Eventarc, which can be crucial if the application's triggering requirements change over time. Eventarc's ability to integrate with multiple Google Cloud services and route events to Cloud Run provides a robust and scalable solution for diverse event handling. This aligns well with the need for a versatile and easily updatable trigger mechanism."
      },
      {
        "date": "2023-12-18T13:11:00.000Z",
        "voteCount": 1,
        "content": "Another requirement is to keep it as simple as possible. C is correct"
      },
      {
        "date": "2023-11-26T14:25:00.000Z",
        "voteCount": 1,
        "content": "i think the right response is C.\nhttps://cloud.google.com/functions/docs/calling/storage\nhttps://cloud.google.com/workflows/docs/calling-run-functions#:~:text=Calling%20or%20invoking%20a%20Google,the%20call%20field%20to%20http."
      },
      {
        "date": "2023-11-12T23:23:00.000Z",
        "voteCount": 2,
        "content": "select B"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 264,
    "url": "https://www.examtopics.com/discussions/google/view/125909-exam-professional-cloud-developer-topic-1-question-264/",
    "body": "You are defining your system tests for an application running in Cloud Run in a Google Cloud project. You need to create a testing environment that is isolated from the production environment. You want to fully automate the creation of the testing environment with the least amount of effort and execute automated tests. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUsing Cloud Build, execute Terraform scripts to create a new Google Cloud project and a Cloud Run instance of your application in the Google Cloud project.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUsing Cloud Build, execute a Terraform script to deploy a new Cloud Run revision in the existing Google Cloud project. Use traffic splitting to send traffic to your test environment.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUsing Cloud Build, execute gcloud commands to create a new Google Cloud project and a Cloud Run instance of your application in the Google Cloud project.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUsing Cloud Build, execute gcloud commands to deploy a new Cloud Run revision in the existing Google Cloud project. Use traffic splitting to send traffic to your test environment."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 7,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-06-06T06:42:00.000Z",
        "voteCount": 1,
        "content": "Why other options are not as efficient:\n\nA and C (Terraform): While Terraform is a powerful tool, it introduces additional complexity compared to using gcloud commands directly. Terraform is better suited for managing large-scale infrastructure, not just deploying a single Cloud Run revision.\n\nB (Terraform with traffic splitting):  Similar to options A and C, using Terraform for this task adds unnecessary complexity. Traffic splitting can be easily configured using gcloud commands."
      },
      {
        "date": "2023-12-27T21:47:00.000Z",
        "voteCount": 4,
        "content": "fully automate == terraform.\nand new separate env == new project \ntherefore i vote A."
      },
      {
        "date": "2024-06-06T06:46:00.000Z",
        "voteCount": 1,
        "content": "I meant C"
      },
      {
        "date": "2024-06-06T06:43:00.000Z",
        "voteCount": 1,
        "content": "I would say D: A and C (Terraform): While Terraform is a powerful tool, it introduces additional complexity compared to using gcloud commands directly. Terraform is better suited for managing large-scale infrastructure, not just deploying a single Cloud Run revision.\n\nAdding gcloud commands to a Cloud Build script is much easier and faster than using terraform"
      },
      {
        "date": "2023-12-26T01:45:00.000Z",
        "voteCount": 1,
        "content": "C is simpler thant A!"
      },
      {
        "date": "2023-12-27T21:48:00.000Z",
        "voteCount": 4,
        "content": "you right, but the part of the requirements is \"You want to *fully automate* the creation of the testing environment\""
      },
      {
        "date": "2023-12-30T14:19:00.000Z",
        "voteCount": 1,
        "content": "oh you right, i didn't make attention for this detail."
      },
      {
        "date": "2023-12-18T13:21:00.000Z",
        "voteCount": 1,
        "content": "A is correct\nhttps://cloud.google.com/docs/terraform/resource-management/managing-infrastructure-as-code"
      },
      {
        "date": "2023-12-04T02:30:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/docs/terraform/best-practices-for-terraform"
      },
      {
        "date": "2023-11-17T02:54:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2023-11-12T23:26:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 265,
    "url": "https://www.examtopics.com/discussions/google/view/126362-exam-professional-cloud-developer-topic-1-question-265/",
    "body": "You are a cluster administrator for Google Kubernetes Engine (GKE). Your organization\u2019s clusters are enrolled in a release channel. You need to be informed of relevant events that affect your GKE clusters, such as available upgrades and security bulletins. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure cluster notifications to be sent to a Pub/Sub topic.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExecute a scheduled query against the google_cloud_release_notes BigQuery dataset.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tQuery the GKE API for available versions.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an RSS subscription to receive a daily summary of the GKE release notes."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 5,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-12-18T13:25:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-notifications"
      },
      {
        "date": "2023-12-11T00:00:00.000Z",
        "voteCount": 1,
        "content": "After i read \nhttps://cloud.google.com/kubernetes-engine/docs/concepts/cluster-notifications\nand \nhttps://cloud.google.com/kubernetes-engine/docs/concepts/release-channels#channels\ni choose A, the two of the answer is correct, but i think how wrote this question according the text, mean to option A. \nthe question and what write here - is literally same words as here:\nhttps://cloud.google.com/kubernetes-engine/docs/concepts/cluster-notifications"
      },
      {
        "date": "2023-11-29T03:01:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-notifications"
      },
      {
        "date": "2023-12-04T09:31:00.000Z",
        "voteCount": 1,
        "content": "Agree with A."
      },
      {
        "date": "2023-11-16T06:32:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/kubernetes-engine/docs/concepts/release-channels#channels"
      },
      {
        "date": "2023-12-18T13:25:00.000Z",
        "voteCount": 1,
        "content": "In this link it's said that \"you can subscribe to upgrade notifications to be informed of newly available versions\".\n\nThe correct answer is A: https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-notifications"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 266,
    "url": "https://www.examtopics.com/discussions/google/view/125910-exam-professional-cloud-developer-topic-1-question-266/",
    "body": "You are tasked with using C++ to build and deploy a microservice for an application hosted on Google Cloud. The code needs to be containerized and use several custom software libraries that your team has built. You do not want to maintain the underlying infrastructure of the application. How should you deploy the microservice?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Functions to deploy the microservice.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Build to create the container, and deploy it on Cloud Run.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Shell to containerize your microservice, and deploy it on a Container-Optimized OS Compute Engine instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Shell to containerize your microservice, and deploy it on standard Google Kubernetes Engine."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-05T11:15:00.000Z",
        "voteCount": 1,
        "content": "I will go for B. Fully managed and well suited for c++"
      },
      {
        "date": "2023-12-03T13:34:00.000Z",
        "voteCount": 1,
        "content": "Can you provide some documentation that prove B is the right choice?"
      },
      {
        "date": "2023-12-10T23:48:00.000Z",
        "voteCount": 5,
        "content": "Option A, using Cloud Functions, is not suitable for your scenario because Cloud Functions primarily supports Node.js, Python, Go, Java, .NET, Ruby, and PHP runtimes. It does not natively support C++ applications. Therefore, for a microservice written in C++, Cloud Functions would not be an appropriate choice as it would require significant workarounds or a complete rewrite of your application in a supported language.\nand C ,D is OS and GKE so you need to setup the infra. \ntherefore the answer is B."
      },
      {
        "date": "2023-11-17T02:57:00.000Z",
        "voteCount": 2,
        "content": "It's B"
      },
      {
        "date": "2023-11-12T23:32:00.000Z",
        "voteCount": 3,
        "content": "For me is B"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 267,
    "url": "https://www.examtopics.com/discussions/google/view/125970-exam-professional-cloud-developer-topic-1-question-267/",
    "body": "You need to containerize a web application that will be hosted on Google Cloud behind a global load balancer with SSL certificates. You don\u2019t have the time to develop authentication at the application level, and you want to offload SSL encryption and management from your application. You want to configure the architecture using managed services where possible. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tHost the application on Google Kubernetes Engine, and deploy an NGINX Ingress Controller to handle authentication.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tHost the application on Google Kubernetes Engine, and deploy cert-manager to manage SSL certificates.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tHost the application on Compute Engine, and configure Cloud Endpoints for your application.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tHost the application on Google Kubernetes Engine, and use Identity-Aware Proxy (IAP) with Cloud Load Balancing and Google-managed certificates.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 4,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-08-15T09:16:00.000Z",
        "voteCount": 1,
        "content": "IAP provides authentication and access control for your web application without requiring you to implement authentication at the application level. It integrates with Cloud Load Balancing and can protect your application by requiring users to authenticate via Google accounts."
      },
      {
        "date": "2024-05-13T07:52:00.000Z",
        "voteCount": 1,
        "content": "of course D"
      },
      {
        "date": "2023-12-02T16:06:00.000Z",
        "voteCount": 2,
        "content": "IAP provides a way to control access to applications running on GCP without the need for traditional VPNs. It works by verifying a user\u2019s identity and determining if that user should be allowed access to the application. This is especially useful since you do not have the time to develop authentication at the application level. IAP can handle this for you."
      },
      {
        "date": "2023-11-15T10:53:00.000Z",
        "voteCount": 1,
        "content": "It should be D"
      },
      {
        "date": "2023-11-13T12:10:00.000Z",
        "voteCount": 1,
        "content": "D. provides a comprehensive, managed solution that fulfills all the requirements: containerized application hosting on GKE, simplified authentication with IAP, SSL offloading and management with Cloud Load Balancing and Google-managed certificates."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 268,
    "url": "https://www.examtopics.com/discussions/google/view/125971-exam-professional-cloud-developer-topic-1-question-268/",
    "body": "You manage a system that runs on stateless Compute Engine VMs and Cloud Run instances. Cloud Run is connected to a VPC, and the ingress setting is set to Internal. You want to schedule tasks on Cloud Run. You create a service account and grant it the roles/run.invoker Identity and Access Management (IAM) role. When you create a schedule and test it, a 403 Permission Denied error is returned in Cloud Logging. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGrant the service account the roles/run.developer IAM role.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a cron job on the Compute Engine VMs to trigger Cloud Run on schedule.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the Cloud Run ingress setting to 'Internal and Cloud Load Balancing.'",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Scheduler with Pub/Sub to invoke Cloud Run.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-12-02T16:20:00.000Z",
        "voteCount": 4,
        "content": "Cloud Scheduler can trigger Cloud Run services, but in this case, where the ingress is set to 'Internal', direct invocation might not work. Instead, you can use Cloud Scheduler in combination with Pub/Sub. Cloud Scheduler can create a Pub/Sub message on a schedule, and this Pub/Sub message can then trigger the Cloud Run service. This approach is commonly used for invoking services with restricted network access."
      },
      {
        "date": "2023-12-02T16:19:00.000Z",
        "voteCount": 1,
        "content": "Cloud Scheduler can trigger Cloud Run services, but in this case, where the ingress is set to 'Internal', direct invocation might not work. Instead, you can use Cloud Scheduler in combination with Pub/Sub. Cloud Scheduler can create a Pub/Sub message on a schedule, and this Pub/Sub message can then trigger the Cloud Run service. This approach is commonly used for invoking services with restricted network access."
      },
      {
        "date": "2023-11-15T11:00:00.000Z",
        "voteCount": 1,
        "content": "D. When setting PubSub subscription, use type push and use the service account with the invoker role as authentication. A. no need more permissions. B. it could work id the vms are in the same VPC, but it is not best practice. C. That setting is only for connecting to load balancer"
      },
      {
        "date": "2023-11-13T12:12:00.000Z",
        "voteCount": 1,
        "content": "D. is the best solution because it effectively circumvents the limitation of the Internal ingress setting of Cloud Run. This setting restricts external access, which is likely causing the 403 error. By using Cloud Scheduler to trigger a Pub/Sub topic, and then having Pub/Sub trigger the Cloud Run service, you maintain internal access security while enabling external scheduling. This method is both secure and adheres to the internal-only access requirements, leveraging managed services for scalability and reliability."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 269,
    "url": "https://www.examtopics.com/discussions/google/view/125972-exam-professional-cloud-developer-topic-1-question-269/",
    "body": "You work on an application that relies on Cloud Spanner as its main datastore. New application features have occasionally caused performance regressions. You want to prevent performance issues by running an automated performance test with Cloud Build for each commit made. If multiple commits are made at the same time, the tests might run concurrently. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new project with a random name for every build. Load the required data. Delete the project after the test is run.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new Cloud Spanner instance for every build. Load the required data. Delete the Cloud Spanner instance after the test is run.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a project with a Cloud Spanner instance and the required data. Adjust the Cloud Build build file to automatically restore the data to its previous state after the test is run.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStart the Cloud Spanner emulator locally. Load the required data. Shut down the emulator after the test is run."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 7,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-12-19T10:39:00.000Z",
        "voteCount": 2,
        "content": "I'd go with B. Sure it may be costly but this option is far more adequate than A or C.\n\nNot D because performance and scalability for the Cloud Spanner emulator is not comparable to the real service:\nhttps://cloud.google.com/spanner/docs/emulator#limitations"
      },
      {
        "date": "2023-12-02T16:45:00.000Z",
        "voteCount": 2,
        "content": "Since the testing needs to accommodate scenarios where multiple commits are made simultaneously, and hence multiple tests might run concurrently, the testing environment should support isolated and independent testing instances to avoid interference among tests.\n\nGiven these requirements, using the Cloud Spanner emulator would not be the best choice for this scenario. The emulator is primarily suited for local development, unit, and integration testing, and is not built for production-scale performance testing. It may not accurately replicate performance characteristics at scale or under load, which are crucial aspects in this case."
      },
      {
        "date": "2023-11-26T14:49:00.000Z",
        "voteCount": 1,
        "content": "B is the right choice in my opinion"
      },
      {
        "date": "2023-11-26T20:08:00.000Z",
        "voteCount": 1,
        "content": "in case of multiple tests running concurrently, won't this choice be time and resource consuming?"
      },
      {
        "date": "2023-11-15T11:09:00.000Z",
        "voteCount": 2,
        "content": "D. https://cloud.google.com/sdk/gcloud/reference/beta/emulators/spanner Use an emulator if possible when testing. B. It could work, but in real world, spanning an cluster of cloud spanner takes a lot of time, and also money"
      },
      {
        "date": "2023-11-26T14:47:00.000Z",
        "voteCount": 1,
        "content": "the question say \"You want to prevent performance issues by running an automated performance test with Cloud Build for each commit made\". Where is the automated test is respone D ?"
      },
      {
        "date": "2023-11-13T12:14:00.000Z",
        "voteCount": 2,
        "content": "Isolation of Test Environments: Creating a new Cloud Spanner instance for each build ensures complete isolation of test environments. This is crucial for accurate performance testing, as it avoids any interference from other concurrent test runs.\n\nConsistency and Accuracy of Data: By loading the required data into each new instance, you ensure that every test starts with a consistent dataset, which is essential for reliable and repeatable performance testing.\n\nResource Management: Automatically deleting the Cloud Spanner instance after each test helps manage resources and costs effectively. It ensures that you are only using and paying for resources during the duration of the test.\n\nParallel Testing: This approach supports concurrent testing for multiple commits, as each test run has its own dedicated Cloud Spanner instance."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 270,
    "url": "https://www.examtopics.com/discussions/google/view/125973-exam-professional-cloud-developer-topic-1-question-270/",
    "body": "Your company's security team uses Identity and Access Management (IAM) to track which users have access to which resources. You need to create a version control system that can integrate with your security team's processes. You want your solution to support fast release cycles and frequent merges to your main branch to minimize merge conflicts. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Cloud Source Repositories repository, and use trunk-based development.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Cloud Source Repositories repository, and use feature-based development.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a GitHub repository, mirror it to a Cloud Source Repositories repository, and use trunk-based development.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a GitHub repository, mirror it to a Cloud Source Repositories repository, and use feature-based development."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 5,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-06-06T07:16:00.000Z",
        "voteCount": 1,
        "content": "C and D (GitHub repository, mirrored to CSR): While GitHub offers feature-based development workflows, mirroring to CSR introduces an additional step and potential delay in the development process. It also doesn't provide the same level of native IAM integration as using CSR directly."
      },
      {
        "date": "2024-02-05T12:47:00.000Z",
        "voteCount": 1,
        "content": "I will go for A instead of C because it's more simple and straightforward.\n\nThere isn't a requirement to have PR's or something like that to choose to have GitHub."
      },
      {
        "date": "2023-12-19T11:08:00.000Z",
        "voteCount": 3,
        "content": "I choose A.\n\nWhere does the GitHub repository requirement come from? \n\nThe security team uses IAM for user accesses and we only need to create a version control system that can integrate with their processes. IAM can't control who is pushing stuff in GitHub and with options C or D it will end up in CSR regardless.\n\nFast release cycles and frequent merges to the main branch to minimize merge conflicts -&gt; trunk-based development."
      },
      {
        "date": "2023-12-02T16:57:00.000Z",
        "voteCount": 1,
        "content": "\"You want your solution to support fast release cycles and frequent merges to your main branch to minimize merge conflicts.\"\n\nThe requirement for fast release cycles and frequent merges with minimal merge conflicts aligns well with trunk-based development. In trunk-based development, developers work in short-lived branches and merge their changes frequently into the main branch, which helps in reducing merge conflicts and supports a more rapid and continuous release cycle."
      },
      {
        "date": "2023-11-15T11:20:00.000Z",
        "voteCount": 1,
        "content": "C. using an external git repository as Github or Bitbucket synced with Cloud Repositories is a best practice, as it provides with a lot more features as PRs and branch permissions. Trunk or master development is okay in this case as is a small project with fast development. For more large projects, larger teams is better to use a feature development as GitFlow"
      },
      {
        "date": "2023-11-13T12:17:00.000Z",
        "voteCount": 2,
        "content": "GitHub Repository: GitHub is a popular and powerful platform for version control. It supports a wide range of development workflows and integrates well with various CI/CD tools, making it suitable for fast release cycles.\n\nMirroring to Cloud Source Repositories: By mirroring the GitHub repository to Google Cloud Source Repositories, you can leverage Google Cloud's IAM features for access control and security. This integration allows your security team to track and manage user access effectively within the Google Cloud environment.\n\nTrunk-Based Development: This development methodology involves developers merging their changes into the main branch frequently. It's well-suited for fast-paced development environments, as it minimizes the duration of branches and reduces the likelihood of significant merge conflicts."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 271,
    "url": "https://www.examtopics.com/discussions/google/view/125974-exam-professional-cloud-developer-topic-1-question-271/",
    "body": "You recently developed an application that monitors a large number of stock prices. You need to configure Pub/Sub to receive messages and update the current stock price in an in-memory database. A downstream service needs the most up-to-date prices in the in-memory database to perform stock trading transactions. Each message contains three pieces or information:<br><br>\u2022 Stock symbol<br>\u2022 Stock price<br>\u2022 Timestamp for the update<br><br>How should you set up your Pub/Sub subscription?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a push subscription with exactly-once delivery enabled.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a pull subscription with both ordering and exactly-once delivery turned off.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a pull subscription with ordering enabled, using the stock symbol as the ordering key.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a push subscription with both ordering and exactly-once delivery turned off."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 5,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-09-08T04:29:00.000Z",
        "voteCount": 1,
        "content": "Ordering enabled: In this scenario, it's crucial that updates for the same stock symbol arrive in the correct order because the latest price update must reflect the actual current stock price. Enabling ordering ensures that messages with the same ordering key (e.g., stock symbol) are delivered in the order they were published.\n\nPull subscription: This gives your application control over when and how messages are processed, which is often beneficial for managing concurrency and ensuring that updates are handled properly.\n\nStock symbol as the ordering key: This ensures that updates for the same stock are processed in sequence, preventing issues where an older price update could overwrite a newer one."
      },
      {
        "date": "2024-09-08T02:48:00.000Z",
        "voteCount": 1,
        "content": "I'll go with D, push subscription because of large amount of data, exactly once is not possible for push according to the documentation, and it's not important here to have exactly once. If rate is not changing, the event shouldn't be published anyway"
      },
      {
        "date": "2024-09-08T02:47:00.000Z",
        "voteCount": 1,
        "content": "I'll go with D, push subscription because of large amount of data, exactly once is not possible for push according to the documentation, and it's not important here to have exactly once. If rate is not changing, the event shouldn't be published anyway"
      },
      {
        "date": "2024-04-11T03:37:00.000Z",
        "voteCount": 1,
        "content": "iven the requirement that the downstream service needs the most up-to-date prices, you don't need to order the messages. In this case, a push subscription would be suitable as it delivers messages as they arrive, ensuring the downstream service receives the latest stock prices promptly. Exactly-once delivery can prevent the same message from being processed multiple times, which could be beneficial in this scenario."
      },
      {
        "date": "2024-02-05T13:09:00.000Z",
        "voteCount": 1,
        "content": "I will go for A.\n\nB and C aren't good because you need to receive the prices in real time as they come.\n\nBetween A and D:\n\nD with exactly-once delivery turned off you can process the same message many times and it isn't good for financial systems."
      },
      {
        "date": "2024-02-05T16:19:00.000Z",
        "voteCount": 2,
        "content": "I change my mind.\n\nI will go for D, because the question says \u201cA downstream service needs the most up-to-date prices\u201d.\nThe pull subscriptions introduce the possibility of latency between the time a message is published and when it's pulled by the app.\nThen B and C are discarded.\n\nBetween A and D, A is not correct because the exactly-once delivery feature is only for pull subscriptions.\n\nAlso, this questions is a duplicate from Question #271."
      },
      {
        "date": "2023-12-03T11:24:00.000Z",
        "voteCount": 4,
        "content": "A is correct (I am not a contributor, so unable to vote). Rationale:\n\n1. The downstream application needs only the most up-to-date value for a stock price. There's no need of historical values from a time series, so \"ordering\" does not make any sense in this scenario. This eliminates alternatives B, C and D. In addition, in alternative C, \"using the stock symbol as the ordering key\" has no practical effect, once \"ordering\" is not necessary.\n\n2. About \"push\" and \"pull\": in a \"push\" subscription, whenever the topic is fed with a new value, it will keep pushing it to the application until an acknowledgement is received. Latency is lower in this case. In a \"pull\" subscription, there's an additional burden on the application to keep pulling from the topic. This increases latency. A \"push\" subscription is recommended in such scenarios."
      },
      {
        "date": "2023-11-13T12:21:00.000Z",
        "voteCount": 4,
        "content": "Pull Subscription for Controlled Processing: A pull subscription gives you control over when and how messages are processed. This can be particularly important for maintaining the integrity of the in-memory database, as it allows for more deliberate handling of message backlogs and peak loads.\n\nMessage Ordering Is Crucial: The ordering of stock price updates is critical. Using the stock symbol as the ordering key ensures that updates for a specific stock are processed in the order they were sent. This is vital to ensure the accuracy of stock price data, as prices must be updated in the sequence they were received to reflect the true market conditions.\n\nNo Need for Exactly-Once Delivery: In most financial data scenarios, the latest data supersedes the old. If a message is delivered more than once, the last update for a given timestamp will leave the database in the correct state. Therefore, exactly-once delivery, which can add complexity and overhead, might not be necessary."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 272,
    "url": "https://www.examtopics.com/discussions/google/view/128264-exam-professional-cloud-developer-topic-1-question-272/",
    "body": "You are a developer at a social media company. The company runs their social media website on-premises and uses MySQL as a backend to store user profiles and user posts. Your company plans to migrate to Google Cloud, and your learn will migrate user profile information to Firestore. You are tasked with designing the Firestore collections. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate one root collection for user profiles, and create one root collection for user posts.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate one root collection for user profiles, and create one subcollection for each user's posts.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate one root collection for user profiles, and store each user's post as a nested list in the user profile document.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate one root collection for user posts, and create one subcollection for each user's profile."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-11T03:42:00.000Z",
        "voteCount": 1,
        "content": "B: The best practice for this scenario would be to create one root collection for user profiles and one subcollection for each user's posts. This allows for easy retrieval of all posts for a given user and aligns well with the hierarchical data model of Firestore. It also provides good isolation of data as each user's posts are stored separately."
      },
      {
        "date": "2024-02-05T13:11:00.000Z",
        "voteCount": 1,
        "content": "I will go for B."
      },
      {
        "date": "2023-12-26T02:53:00.000Z",
        "voteCount": 1,
        "content": "B is more appropriate"
      },
      {
        "date": "2023-12-10T23:24:00.000Z",
        "voteCount": 1,
        "content": "For migrating user profile information to Firestore in your social media company, the best approach is:\n\nB. Create one root collection for user profiles, and create one subcollection for each user's posts.\n\nThis structure offers better scalability, efficient data retrieval, and clearer organization, while also simplifying access control and data modeling. Options A, C, and D are less optimal due to potential performance issues, complex querying, and counterintuitive data relationships."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 273,
    "url": "https://www.examtopics.com/discussions/google/view/125968-exam-professional-cloud-developer-topic-1-question-273/",
    "body": "Your team recently deployed an application on Google Kubernetes Engine (GKE). You are monitoring your application and want to be alerted when the average memory consumption of your containers is under 20% or above 80%. How should you configure the alerts?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Cloud Function that consumes the Monitoring API. Create a schedule to trigger the Cloud Function hourly and alert you if the average memory consumption is outside the defined range.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn Cloud Monitoring, create an alerting policy to notify you if the average memory consumption is outside the defined range.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Cloud Function that runs on a schedule, executes kubectl top on all the workloads on the cluster, and sends an email alert if the average memory consumption is outside the defined range.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite a script that pulls the memory consumption of the instance at the OS level and sends an email alert if the average memory consumption is outside the defined range."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-05T13:12:00.000Z",
        "voteCount": 1,
        "content": "I will go for B."
      },
      {
        "date": "2023-12-13T21:32:00.000Z",
        "voteCount": 1,
        "content": "To monitor average memory consumption of containers in Google Kubernetes Engine (GKE), the best approach is to use Cloud Monitoring. You can create an alerting policy in Cloud Monitoring to notify you if the average memory consumption is outside the defined range. This method leverages Google Cloud's built-in capabilities for efficient and accurate monitoring.\n\nFor detailed instructions, please refer to:\nGoogle Cloud Documentation: https://cloud.google.com/monitoring"
      },
      {
        "date": "2023-12-10T02:25:00.000Z",
        "voteCount": 1,
        "content": "No need to use CloudFunction in this case and the custom script is overengineering. Answer B fits the needs."
      },
      {
        "date": "2023-12-02T01:54:00.000Z",
        "voteCount": 1,
        "content": "Cloud Monitoring provides a user-friendly interface to create complex alerting policies. You can set up thresholds for specific metrics, like memory consumption, and receive notifications if these thresholds are exceeded or undercut. This feature negates the need for custom scripts or functions to monitor these metrics."
      },
      {
        "date": "2023-11-23T14:28:00.000Z",
        "voteCount": 1,
        "content": "B is the right answer, alert in the cloud monitoring"
      },
      {
        "date": "2023-11-13T12:05:00.000Z",
        "voteCount": 2,
        "content": "B. using Cloud Monitoring to create an alerting policy is the most efficient, reliable, and straightforward method to monitor and be alerted about the memory consumption of your containers in GKE."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 274,
    "url": "https://www.examtopics.com/discussions/google/view/125969-exam-professional-cloud-developer-topic-1-question-274/",
    "body": "You manage a microservice-based ecommerce platform on Google Cloud that sends confirmation emails to a third-party email service provider using a Cloud Function. Your company just launched a marketing campaign, and some customers are reporting that they have not received order confirmation emails. You discover that the services triggering the Cloud Function are receiving HTTP 500 errors. You need to change the way emails are handled to minimize email loss. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the Cloud Function's timeout to nine minutes.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the sender application to publish the outgoing emails in a message to a Pub/Sub topic. Update the Cloud Function configuration to consume the Pub/Sub queue.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the sender application to write emails to Memorystore and then trigger the Cloud Function. When the function is triggered, it reads the email details from Memorystore and sends them to the email service.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the sender application to retry the execution of the Cloud Function every one second if a request fails."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-12-02T03:33:00.000Z",
        "voteCount": 3,
        "content": "This is a robust and scalable approach. By decoupling the email sending process using Pub/Sub, you introduce a queueing mechanism. This ensures that even if the Cloud Function encounters an issue, the email messages are not lost but remain in the queue. Additionally, Pub/Sub can handle high throughput and provides retry mechanisms."
      },
      {
        "date": "2023-11-23T14:30:00.000Z",
        "voteCount": 1,
        "content": "B is the right answer"
      },
      {
        "date": "2023-11-15T11:34:00.000Z",
        "voteCount": 1,
        "content": "B. With pub sub you can scale the load of sending emails to the Cloud Function. Also can configure exponential backoff if errors arise in the third party service and ensure the email is delivered"
      },
      {
        "date": "2023-11-13T12:08:00.000Z",
        "voteCount": 1,
        "content": "B. the most effective solution would be B. - using Pub/Sub to queue email messages and having the Cloud Function process these messages is a robust, scalable, and reliable way to handle email sending in your ecommerce platform, especially during high load conditions."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 275,
    "url": "https://www.examtopics.com/discussions/google/view/127703-exam-professional-cloud-developer-topic-1-question-275/",
    "body": "You have a web application that publishes messages to Pub/Sub. You plan to build new versions of the application locally and need to quickly test Pub/Sub integration for each new build. How should you configure local testing?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the Google Cloud console, navigate to the API Library, and enable the Pub/Sub API. When developing locally configure your application to call pubsub.googleapis.com.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall the Pub/Sub emulator using gcloud, and start the emulator with a valid Google Project ID. When developing locally, configure your applicat.cn to use the local emulator by exporting the PUBSUB_EMULATOR_HOST variable.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun the gcloud config set api_endpoint_overrides/pubsub https://pubsubemulator.googleapis.com.com/ command to change the Pub/Sub endpoint prior to starting the application.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall Cloud Code on the integrated development environment (IDE). Navigate to Cloud APIs, and enable Pub/Sub against a valid Google Project IWhen developing locally, configure your application to call pubsub.googleapis.com."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-12-13T21:39:00.000Z",
        "voteCount": 2,
        "content": "For local testing of Pub/Sub integration in a web application, installing and using the Pub/Sub emulator is the most efficient approach. The emulator can be installed via gcloud and started with a valid Google Project ID. Configuring your application to use the emulator locally is done by setting the PUBSUB_EMULATOR_HOST environment variable.\n\nFor more details on setting up and using the Pub/Sub emulator, visit:\nGoogle Cloud Documentation: https://cloud.google.com/pubsub/docs/emulator"
      },
      {
        "date": "2023-12-03T09:43:00.000Z",
        "voteCount": 3,
        "content": "For local testing of Pub/Sub integration, the most suitable option is B. You\u2019d install the Pub/Sub emulator via gcloud, initiate it with a valid Google Project ID, and configure your application to utilize the local emulator by setting the PUBSUB_EMULATOR_HOST variable. This method replicates the Pub/Sub environment locally for efficient testing."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 276,
    "url": "https://www.examtopics.com/discussions/google/view/127582-exam-professional-cloud-developer-topic-1-question-276/",
    "body": "You recently developed an application that monitors a large number of stock prices. You need to configure Pub/Sub to receive a high volume messages and update the current stock price in a single large in-memory database. A downstream service needs the most up-to-date prices in the in-memory database to perform stock trading transactions. Each message contains three pieces or information:<br>\u2022\tStock symbol<br>\u2022\tStock price<br>\u2022\tTimestamp for the update<br><br>How should you set up your Pub/Sub subscription?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a pull subscription with exactly-once delivery enabled.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a push subscription with both ordering and exactly-once delivery turned off.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a push subscription with exactly-once delivery enabled.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a pull subscription with both ordering and exactly-once delivery turned off."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 4,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 4,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-12-19T09:58:00.000Z",
        "voteCount": 8,
        "content": "https://cloud.google.com/pubsub/docs/exactly-once-delivery\n\"This page explains how to receive and acknowledge messages using exactly-once semantics. Only the pull subscription type supports exactly-once delivery, including subscribers that use the StreamingPull API.\n\nPush and export subscriptions don't support exactly-once delivery.\""
      },
      {
        "date": "2024-10-11T10:40:00.000Z",
        "voteCount": 1,
        "content": "Exactly-once delivery is not supported for push subscriptions in Google Cloud Pub/Sub."
      },
      {
        "date": "2024-09-08T04:42:00.000Z",
        "voteCount": 1,
        "content": "pull subscription with both ordering and exactly-once delivery turned off"
      },
      {
        "date": "2024-09-05T02:08:00.000Z",
        "voteCount": 2,
        "content": "ordering &amp; pull seems to be the most valid values for downstream service, don't U think ?"
      },
      {
        "date": "2024-09-02T04:30:00.000Z",
        "voteCount": 1,
        "content": "In financial services we have to ensure that msg has been consumed ( ACK ) that's for sure, and push is faster, seems that C is the best option"
      },
      {
        "date": "2023-12-26T03:40:00.000Z",
        "voteCount": 2,
        "content": "i go for C : push subscription + exactly once delivery so ordering is guaranteed"
      },
      {
        "date": "2023-12-10T23:12:00.000Z",
        "voteCount": 1,
        "content": "duplicate from  Question #271.\nIMHO the answer is C. need push, and fast ASAP without spend resource on ordering and so on."
      },
      {
        "date": "2023-12-03T11:18:00.000Z",
        "voteCount": 3,
        "content": "C is correct (I am not a contributor, so unable to vote). Rationale:\n\n1. The downstream application needs only the most up-to-date value for a stock price. There's no need of historical values from a time series, so \"ordering\" does not make any sense in this scenario. This eliminates alternatives B and D.\n\n2. Next choice is between \"push\" and \"pull\". In a \"push\" subscription, whenever the topic is fed with a new value, it will keep pushing it to the application until an acknowledgement is received. Latency is lower in this case. In a \"pull\" subscription, there's an additional burden on the application to keep pulling from the topic. This increases latency. A \"push\" subscription is recommended in such scenarios."
      },
      {
        "date": "2023-12-01T22:58:00.000Z",
        "voteCount": 3,
        "content": "This setup provides the necessary guarantees for message delivery without duplication, and the pull model allows your service to manage message consumption at a pace that ensures the integrity and timeliness of updates in your in-memory database."
      },
      {
        "date": "2023-12-10T23:13:00.000Z",
        "voteCount": 1,
        "content": "i disagree bc this is STOCK! you have to know the fast as possible the the pricing of the stocks. so the answer is probably PUSH and not PULL."
      },
      {
        "date": "2023-12-07T04:53:00.000Z",
        "voteCount": 1,
        "content": "do you have any link with documentation for this scenario please?"
      },
      {
        "date": "2023-12-01T04:33:00.000Z",
        "voteCount": 1,
        "content": "C is the correct answer"
      },
      {
        "date": "2023-12-01T04:31:00.000Z",
        "voteCount": 1,
        "content": "Given the requirement for maintaining the most up-to-date stock prices in the in-memory database for trading transactions, the suitable setup for the Pub/Sub subscription would be option C: Create a push subscription with exactly-once delivery enabled. This ensures that messages are reliably delivered and processed exactly once, avoiding duplicates and ensuring that the downstream service receives the most recent stock price updates for accurate trading decisions."
      },
      {
        "date": "2023-12-07T04:52:00.000Z",
        "voteCount": 1,
        "content": "do you have any link with documentation for this scenario please?"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 277,
    "url": "https://www.examtopics.com/discussions/google/view/127583-exam-professional-cloud-developer-topic-1-question-277/",
    "body": "Your team has created an application that is hosted on a Google Kubemetes Engine (GKE) cluster. You need to connect the application to a legacy REST service that is deployed in two GKE clusters in two different regions. You want to connect your application to the legacy service in a way that is resilient and requires the fewest number of steps. You also want to be able to run probe-based health checks on the legacy service on a separate port. How should you set up the connection? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Traffic Director with a sidecar proxy to connect the application to the service.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up a proxyless Traffic Director configuration for the application.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the legacy service's firewall to allow health checks originating from the sidecar proxy.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the legacy service's firewall to allow health checks originating from the application.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the legacy service's firewall to allow health checks originating from the Traffic Director control plane."
    ],
    "answer": "AC",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AC",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-11T07:04:00.000Z",
        "voteCount": 1,
        "content": "repeat of question 246"
      },
      {
        "date": "2024-02-05T16:22:00.000Z",
        "voteCount": 1,
        "content": "I will go for AC."
      },
      {
        "date": "2023-12-26T03:56:00.000Z",
        "voteCount": 2,
        "content": "Answer AC\nQuestion is duplicated : 246"
      },
      {
        "date": "2023-12-13T20:17:00.000Z",
        "voteCount": 1,
        "content": "For connecting a GKE-hosted application to a legacy REST service across two regions, using Traffic Director with a sidecar proxy and configuring the legacy service's firewall for health checks offers a resilient and efficient solution. More details can be found at: https://thenewstack.io/google-traffic-director-and-the-l7-internal-load-balancer-intermingles-cloud-native-and-legacy-workloads/"
      },
      {
        "date": "2023-12-01T04:38:00.000Z",
        "voteCount": 2,
        "content": "1.Use Traffic Director with a sidecar proxy (Option A):This enables reliable communication between your application and the legacy service. The sidecar proxy can manage traffic routing, load balancing, and resilience.\n\n2. Configure the legacy service's firewall to allow health checks originating from the sidecar proxy (Option C):By allowing health checks from the sidecar proxy, you ensure that the health checks, which are necessary for ensuring service availability, are permitted by the firewall."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 278,
    "url": "https://www.examtopics.com/discussions/google/view/128532-exam-professional-cloud-developer-topic-1-question-278/",
    "body": "You are monitoring a web application that is written in Go and deployed in Google Kubernetes Engine. You notice an increase in CPU and memory utilization. You need to determine which function is consuming the most CPU and memory resources. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd print commands to the application source code to log when each function is called, and redeploy the application.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Cloud Logging query that gathers the web application s logs. Write a Python script that calculates the difference between the timestamps from the beginning and the end of the application's longest functions to identify time-intensive functions.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImport OpenTelemetry and Trace export packages into your application, and create the trace provider. Review the latency data for your application on the Trace overview page, and identify which functions cause the most latency.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImport the Cloud Profiler package into your application, and initialize the Profiler agent. Review the generated flame graph in the Google Cloud console to identify time-intensive functions.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 3,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-11T07:07:00.000Z",
        "voteCount": 1,
        "content": "we want to know which function is consuming the most CPU and memory resources; cloud profiler comes to mind automatically.  Importing the Cloud Profiler package into your application and initializing the Profiler agent would help you identify which functions are consuming the most CPU and memory resources. The generated flame graph in the Google Cloud Console provides a visualization of the call stack and resource usage, making it easy to identify time- and resource-intensive functions."
      },
      {
        "date": "2024-02-05T16:23:00.000Z",
        "voteCount": 1,
        "content": "I will go for D. Cloud Profiler is the good one to gather metrics and information about CPU and memory."
      },
      {
        "date": "2023-12-13T20:09:00.000Z",
        "voteCount": 1,
        "content": "Google Cloud Profiler efficiently analyzes cloud-based applications, like those on GKE, using minimal code changes and flame graphs for in-depth insights into CPU and memory usage, thus identifying performance bottlenecks with ease. For more information, visit: https://dev.to/billcchung/profiling-go-grpc-service-with-google-cloud-profiler-1k8l and https://www.cncf.io/blog/2020/03/27/continuous-profiling-go-application-running-in-kubernetes/"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 279,
    "url": "https://www.examtopics.com/discussions/google/view/128533-exam-professional-cloud-developer-topic-1-question-279/",
    "body": "You are developing a flower ordering application. Currently you have three microservices:<br>\u2022\tOrder Service (receives the orders)<br>\u2022\tOrder Fulfillment Service (processes the orders)<br>\u2022\tNotification Service (notifies the customer when the order is filled)<br><br>You need to determine how the services will communicate with each other. You want incoming orders to be processed quickly and you need to collect order information for fulfillment. You also want to make sure orders are not lost between your services and are able to communicate asynchronously. How should the requests be processed?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<img src=\"https://img.examtopics.com/professional-cloud-developer/image10.png\"><br>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<img src=\"https://img.examtopics.com/professional-cloud-developer/image11.png\"><br>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<img src=\"https://img.examtopics.com/professional-cloud-developer/image12.png\"><br>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<img src=\"https://img.examtopics.com/professional-cloud-developer/image13.png\">\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-11T07:12:00.000Z",
        "voteCount": 1,
        "content": "Pub/Sub must be involved in this kind of requirements, so that orders are not lost if some microservice is down.\n\nIs this the last questions as of today????"
      },
      {
        "date": "2024-02-05T16:26:00.000Z",
        "voteCount": 1,
        "content": "Key is asynchronous and make sure orders are not lost between your services -&gt; D"
      },
      {
        "date": "2023-12-27T23:29:00.000Z",
        "voteCount": 2,
        "content": "key to find answer is asynchronous communication. Answer is D"
      },
      {
        "date": "2023-12-13T20:22:00.000Z",
        "voteCount": 3,
        "content": "For efficient and reliable communication in a flower ordering application with microservices, the best approach is \"Order request -&gt; Order Service -&gt; Pub/Sub queue -&gt; Order Fulfillment Service -&gt; Firestore Database -&gt; Pub/Sub queue -&gt; Notification Service,\" ensuring quick processing and asynchronous communication without data loss.\n\nMore information can be found at:\nGoogle Cloud Documentation: https://cloud.google.com/pubsub/docs/overview"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 280,
    "url": "https://www.examtopics.com/discussions/google/view/149091-exam-professional-cloud-developer-topic-1-question-280/",
    "body": "You recently deployed an application to GKE where Pods are writing files to a Compute Engine persistent disk. You have created a PersistentVolumeClaim (PVC) and a PersistentVolume (PV) object on Kubernetes for the disk, and you reference the PVC in the deployment manifest file.<br><br>You recently expanded the size of the persistent disk because the application has used up almost all of the disk space. You have logged on to one of the Pods, and you notice that the disk expansion is not visible in the container file system. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the spec.capacity.storage value of the PV object to match the size of the persistent disk. Apply the updated configuration by using kubectl.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRecreate the application Pods by running the kubectl delete deployment DEPLOYMENT_NAME &amp;&amp; kubectl apply deployment.yaml command, where the DEPLOYMENT_NAME parameter is the name of your deployment and deployment.yaml is its manifest file.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the spec.resources.requests.storage value of the PVC object to match the size of the persistent disk. Apply the updated configuration by using kubectl.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the Pod, resize the disk partition to the maximum value by using the fdisk or parted utility."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-10-11T10:54:00.000Z",
        "voteCount": 1,
        "content": "To extend storage space in GKE, you need to update the PersistentVolumeClaim (PVC) to request a larger volume size; this will trigger the creation of a new PersistentVolume (PV) with the desired capacity if your StorageClass is configured for dynamic provisioning"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 281,
    "url": "https://www.examtopics.com/discussions/google/view/149100-exam-professional-cloud-developer-topic-1-question-281/",
    "body": "You work for an ecommerce company. You are designing a new Orders API that will be exposed through Apigee. In your Apigee organization, you created two new environments named orders-test and orders-prod. You plan to use unique URLs named test.lnk-42.com/api/v1/orders and Ink-42.com/api/v1/orders for each environment. You need to ensure that each environment only uses the assigned URL. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Attach orders-test and orders-prod to the orders environment group.<br>2. Add each hostname to the appropriate environment.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Attach orders-test and orders-prod to the orders environment group.<br>2. Add each hostname to the orders environment group.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Attach orders-test to the test environment group, and attach orders-prod to the production environment group.<br>2. Add each hostname to the appropriate environment.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Attach orders-test to the test environment group, and attach orders-prod to the production environment group.<br>2. Add each hostname to the appropriate environment group."
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-10-11T19:23:00.000Z",
        "voteCount": 1,
        "content": "environment group can have multiple environments attached to it, but the environments will only respond to requests using the hostnames specified for that group.\nAttaching environments to environment groups ensures that only those environments use the corresponding hostnames.\nBy attaching orders-test to the test environment group and orders-prod to the production environment group, you are ensuring that each environment is restricted to its appropriate URL.\n"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 282,
    "url": "https://www.examtopics.com/discussions/google/view/149103-exam-professional-cloud-developer-topic-1-question-282/",
    "body": "You are developing an application that uses microservices architecture that includes Cloud Run, Bigtable, and Pub/Sub. You want to conduct the testing and debugging process as quickly as possible to create a minimally viable product with minimal cost. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Shell Editor and Cloud Shell to deploy the application, and test the functionality by using the Google Cloud console in the project.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse emulators to test the functionality of cloud resources locally, and deploy the code to your Google Cloud project.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Build to create a pipeline, and add the unit testing stage and the manual approval stage. Deploy the code to your Google Cloud project.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Code to develop, deploy, and test microservices resources. Use Cloud Logging to review the resource logs."
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-10-11T20:19:00.000Z",
        "voteCount": 1,
        "content": "Emulators are great for quickly testing and debugging cloud resources locally without incurring costs associated with deploying the resources on Google Cloud. For example, you can use the Bigtable and Pub/Sub emulators locally to simulate their behavior."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 283,
    "url": "https://www.examtopics.com/discussions/google/view/149105-exam-professional-cloud-developer-topic-1-question-283/",
    "body": "You are a lead developer at an organization that recently integrated several Google Cloud services. These services are located within Virtual Private Cloud (VPC) environments that are secured with VPC Service Controls and Private Service Connect endpoints. Developers across your organization use different operating systems, development frameworks, and integrated development environments (IDEs). You need to recommend a developer environment that will ensure consistency in the developer process and improve the overall developer experience. You want this solution to:<br><br>\u2022 Enforce consistent security controls.<br>\u2022 Have access to Google Cloud resources and applications within the VPC.<br>\u2022 Allow the installation of custom tools and utilities on the development environments.<br><br>What solution should you recommend?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Workstations, and allow developers to create their own custom images.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Workstations with preconfigured base images. For custom tools and utilities, use custom images that are rebuilt weekly.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Cloud Code extension with the IDEs that are used across the organization. Configure Cloud VPN to enable VPC access.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Cloud Code extension with the IDEs that are used across the organization. Use Identity-Aware Proxy to enable access to the services in the VPC."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-10-11T20:22:00.000Z",
        "voteCount": 1,
        "content": "Cloud Workstations is a fully managed development environment designed for consistency, security, and access to cloud resources. It ensures that all developers have a uniform environment, meeting the requirement for consistent security controls.\nBy using preconfigured base images, you can ensure that all developers start with the same tools and settings. Allowing custom tools and utilities through custom images that are rebuilt regularly (e.g., weekly) ensures developers have the flexibility to use the tools they need without compromising security.\nAccess to Google Cloud resources within the VPC is supported with Cloud Workstations, and because it's managed on Google Cloud, it fits into VPC Service Controls and Private Service Connect security measures."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 284,
    "url": "https://www.examtopics.com/discussions/google/view/149106-exam-professional-cloud-developer-topic-1-question-284/",
    "body": "You are preparing to conduct a load test on your Cloud Run service by using JMeter. You need to orchestrate the steps and services to use for an effective load test and analysis. You want to follow Google-recommended practices. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall JMeter on your local machine, create a log sink to BigQuery, and use Looker to analyze the results.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up a Compute Engine instance, install JMeter on the instance, create a log sink to a Cloud Storage bucket, and use Looker Studio to analyze the results.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up a Compute Engine instance, install JMeter on the instance, create a log sink to a Cloud Storage bucket, and use Looker to analyze the results.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up a Compute Engine instance, install JMeter on the instance, create a log sink to BigQuery, and use Looker Studio to analyze the results."
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-10-11T20:24:00.000Z",
        "voteCount": 1,
        "content": "Set up a Compute Engine instance, install JMeter on the instance, create a log sink to BigQuery, and use Looker Studio to analyze the results."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 285,
    "url": "https://www.examtopics.com/discussions/google/view/149108-exam-professional-cloud-developer-topic-1-question-285/",
    "body": "You are designing a Node.js-based mobile news feed application that stores data on Google Cloud. You need to select the application's database. You want the database to have zonal resiliency out of the box, low latency responses, ACID compliance, an optional middle tier, semi-structured data storage, and network-partition-tolerant and offline-mode client libraries. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Firestore and use the Firestore client library in the app.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Bigtable and use the Bigtable client in the app.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Cloud SQL and use the Google Client Library for Cloud SQL in the app.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure BigQuery and use the BigQuery REST API in the app."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-10-11T20:26:00.000Z",
        "voteCount": 1,
        "content": "Other options like Bigtable (B) and BigQuery (D) do not meet the ACID compliance requirement, while Cloud SQL (C) is not optimized for semi-structured data and mobile offline capabilities."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 286,
    "url": "https://www.examtopics.com/discussions/google/view/149109-exam-professional-cloud-developer-topic-1-question-286/",
    "body": "You are developing an application component to capture user behavior data and stream the data to BigQuery. You plan to use the BigQuery Storage Write API. You need to ensure that the data that arrives in BigQuery does not have any duplicates. You want to use the simplest operational method to achieve this. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a write stream in the default type.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a write stream in the committed type.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a Kafka cluster. Use a primary universally unique identifier (UUID) for duplicate messages.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a Pub/Sub topic. Use Cloud Functions to subscribe to the topic and remove any duplicates."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-10-11T20:28:00.000Z",
        "voteCount": 1,
        "content": "BigQuery Storage Write API supports different stream types, and the committed stream type is designed for ensuring data consistency. Data written using committed streams is deduplicated by BigQuery, which guarantees that no duplicates will be present."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 287,
    "url": "https://www.examtopics.com/discussions/google/view/149111-exam-professional-cloud-developer-topic-1-question-287/",
    "body": "You maintain a popular mobile game deployed on Google Cloud services that include Firebase, Firestore, and Cloud Functions. Recently, the game experienced a surge in usage, and the application encountered HTTP 429 RESOURCE_EXHAUSTED errors when accessing the Firestore API. The application has now stabilized. You want to quickly fix this issue because your company has a marketing campaign next week and you expect another surge in usage. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRequest a quota increase, and modify the application code to retry the Firestore API call with fixed backoff.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRequest a quota increase, and modify the application code to retry the Firestore API call with exponential backoff.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOptimize database queries to reduce read/write operations, and modify the application code to retry the Firestore API call with fixed backoff.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOptimize database queries to reduce read/write operations, and modify the application code to retry the Firestore API call with exponential backoff."
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-10-11T20:33:00.000Z",
        "voteCount": 1,
        "content": "Optimizing database queries: Reducing the number of read/write operations can prevent resource exhaustion and help Firestore handle higher traffic more efficiently.\nExponential backoff: This is a best practice when handling rate-limited or resource-exhausted errors (like HTTP 429). It gradually increases the delay between retry attempts, reducing the load on the system and giving it time to recover."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 288,
    "url": "https://www.examtopics.com/discussions/google/view/149113-exam-professional-cloud-developer-topic-1-question-288/",
    "body": "You are developing a mobile application that allows users to create and manage to-do lists. Your application has the following requirements:<br><br>\u2022 Store and synchronize data between different mobile devices.<br>\u2022 Support offline access.<br>\u2022 Provide real-time updates on each user's device.<br><br>You need to implement a database solution while minimizing operational effort. Which approach should you use?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Cloud SQL for MySQL instance. Implement a data model to store to-do list information. Create indexes for the most heavily and frequently used queries.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Bigtable instance. Design a database schema to avoid hotspots when writing data. Use a Bigtable change stream to capture data changes.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Firestore as the database. Configure Firestore offline persistence to cache a copy of the Firestore data. Listen to document changes to update applications whenever there are document changes.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement a SQLite database on each user's device. Use a scheduled job to synchronize each device database with a copy stored in Cloud Storage."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-10-11T20:55:00.000Z",
        "voteCount": 1,
        "content": "Firestore is a fully managed NoSQL database that provides real-time synchronization and offline access out of the box, making it ideal for mobile applications."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 289,
    "url": "https://www.examtopics.com/discussions/google/view/149114-exam-professional-cloud-developer-topic-1-question-289/",
    "body": "You manage an application deployed on GKE clusters across multiple environments. You are using Cloud Build to run user acceptance testing (UAT) tests. You have integrated Cloud Build with Artifact Analysis, and enabled the Binary Authorization API in all Google Cloud projects hosting your environments. You want only container images that have passed certain automated UAT tests to be deployed to the production environment. You have already created an attestor. What should you do next?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAfter the UAT phase, sign the attestation with a key stored as a Kubernetes secret. Add a GKE cluster-specific rule in Binary Authorization for the UAT Google Cloud project.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAfter the UAT phase, sign the attestation with a key stored as a Kubernetes secret. Add a GKE cluster-specific rule in Binary Authorization for the production Google Cloud project policy.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAfter the UAT phase, sign the attestation with a key stored in Cloud Key Management Service (KMS). Add a default rule in Binary Authorization for the UAT Google Cloud project.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAfter the UAT phase, sign the attestation with a key stored in Cloud Key Management Service (KMS). Add a GKE cluster-specific rule in Binary Authorization for the production Google Cloud project policy."
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-10-11T20:59:00.000Z",
        "voteCount": 1,
        "content": "After the UAT phase, sign the attestation with a key stored in Cloud Key Management Service (KMS). Add a GKE cluster-specific rule in Binary Authorization for the production Google Cloud project policy"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 290,
    "url": "https://www.examtopics.com/discussions/google/view/149115-exam-professional-cloud-developer-topic-1-question-290/",
    "body": "You work for a company that operates an ecommerce website. You are developing a new integration that will manage all order fulfillment steps after orders are placed. You have created multiple Cloud Functions to process each order. You need to orchestrate the execution of the functions, using the output of each function to determine the flow. You want to minimize the latency of this process. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Workflows to call the functions, and use callbacks to handle the execution logic.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Workflows to call the functions, and use conditional jumps to handle the execution logic.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Composer to call the functions, and use an Apache Airflow HTTP operator to handle the execution logic.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Composer to call the functions, and use an Apache Airflow operator to handle the execution logic."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-10-11T21:02:00.000Z",
        "voteCount": 1,
        "content": "Workflows is a service in Google Cloud that allows you to orchestrate serverless functions and services in a sequence or in parallel. It is specifically designed for orchestrating tasks like this in a simple and efficient manner.\nUsing conditional jumps within Workflows allows you to dynamically control the flow based on the outputs of the functions. This means you can easily determine which function to call next based on the results of the previous steps."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 291,
    "url": "https://www.examtopics.com/discussions/google/view/149116-exam-professional-cloud-developer-topic-1-question-291/",
    "body": "You are currently pushing container images to Artifact Registry and deploying a containerized microservices application to GKE. After deploying the application, you notice that the services do not behave as expected. You use the kubectl get pods command to inspect the state of the application Pods, and discover that one of the Pods has a state of CrashLoopBackoff. How should you troubleshoot the Pod?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConnect to the problematic Pod by running the kubectl exec -it POD_NAME - /bin/bash command where the POD_NAME parameter is the name of the problematic Pod. Inspect the logs in the /var/log/messages folder to determine the root cause.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExecute the gcloud projects get-iam-policy PROJECT_ID command where the PROJECT_ID parameter is the name of the project where your Artifact Registry resides. Inspect the IAM bindings of the node pool s service account. Validate if the service account has the roles/artifactregistry.reader role.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun the kubectl logs POD_NAME command where the POD_NAME parameter is the name of the problematic Pod. Analyze the logs of the Pod from previous runs to determine the root cause of failed start attempts of the Pod.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the Google Cloud console, navigate to Cloud Logging in the project of the cluster\u2019s VPC. Enter a filter to show denied egress traffic to the Private Google Access CIDR range. Validate if egress traffic is denied from your GKE cluster to the Private Google Access CIDR range."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-10-11T21:05:00.000Z",
        "voteCount": 1,
        "content": "container in crashloopbackoff , check logs first"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 292,
    "url": "https://www.examtopics.com/discussions/google/view/149117-exam-professional-cloud-developer-topic-1-question-292/",
    "body": "You use Cloud Build to build and test container images prior to deploying them to Cloud Run. Your images are stored in Artifact Registry. You need to ensure that only container images that have passed testing are deployed. You want to minimize operational overhead. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy a new revision to a Cloud Run service. Assign a tag that allows access to the revision at a specific URL without serving traffic. Test that revision again. Migrate the traffic to the Cloud Run service after you confirm that the new revision is performing as expected.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Binary Authorization on your Cloud Run service. Create an attestation if the container image has passed all tests. Configure Binary Authorization to allow only images with appropriate attestation to be deployed to the Cloud Run service.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a GKE cluster. Verify that all tests have passed, and then deploy the image to the GKE cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure build provenance on your Cloud Build pipeline. Verify that all the tests have passed, and then deploy the image to a Cloud Run service."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-10-11T21:06:00.000Z",
        "voteCount": 1,
        "content": "Enable Binary Authorization on your Cloud Run service. Create an attestation if the container image has passed all tests. Configure Binary Authorization to allow only images with appropriate attestation to be deployed to the Cloud Run service."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 293,
    "url": "https://www.examtopics.com/discussions/google/view/149118-exam-professional-cloud-developer-topic-1-question-293/",
    "body": "You are developing a scalable web application for internal users. Your organization uses Google Workspace. You need to set up authentication to the application for the users, and then deploy the application on Google Cloud. You plan to use cloud-native features, and you want to minimize infrastructure management effort. What should you do? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Compute Engine VM, configure a web server, and deploy the application in a VPC.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tContainerize the application, and deploy it as a Cloud Run service.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Cloud SQL database with a table containing the users and password hashes. Add an authentication screen to ensure that only internal users can access the application.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Identity Aware Proxy, and grant the roles/iap.httpsResourceAccessor IAM role to the users that need to access the application.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Identity Aware Proxy, and grant the roles/iap.tunnelResourceAccessor IAM role to the users that need to access the application."
    ],
    "answer": "BD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BD",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-10-11T21:43:00.000Z",
        "voteCount": 1,
        "content": "B. Containerize the application, and deploy it as a Cloud Run service.\n\nCloud Run is a fully managed service that allows you to run containerized applications without managing the underlying infrastructure. This aligns with your goal of minimizing infrastructure management effort and provides scalability for your application.\nD. Configure Identity Aware Proxy, and grant the roles/iap.httpsResourceAccessor IAM role to the users that need to access the application.\n\nIdentity Aware Proxy (IAP) secures access to your application by authenticating users using their Google Workspace credentials. By granting the roles/iap.httpsResourceAccessor IAM role to the internal users, you ensure that only authenticated users can access the application. This method leverages existing Google Workspace accounts, simplifying user management."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 294,
    "url": "https://www.examtopics.com/discussions/google/view/149119-exam-professional-cloud-developer-topic-1-question-294/",
    "body": "You work for an ecommerce company, and you are responsible for deploying and managing multiple APIs. The operations team wants to review the traffic patterns in the orders-prod and users-prod environments. These are the only environments in the store-prod environment group. You want to follow Google-recommended practices. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAssign the Apigee Analytics Viewer IAM role to the operations team for both environments. Use Cloud Monitoring to review traffic patterns.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAssign the Apigee Analytics Viewer IAM role to the operations team for both environments. Use Apigee API Analytics to review traffic patterns.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAssign the Apigee API Reader IAM role to each user of the operations team for both environments. Use Cloud Monitoring to review traffic patterns.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAssign the Apigee API Reader IAM role to each user of the operations team for both environments. Use Apigee API Analytics to review traffic patterns."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-10-11T21:46:00.000Z",
        "voteCount": 1,
        "content": "Apigee API Analytics is specifically designed for monitoring and analyzing API traffic. It provides insights into traffic patterns, response times, errors, and other metrics related to API usage, making it the most suitable tool for this purpose.\nAssigning the Apigee Analytics Viewer IAM role gives the operations team the necessary permissions to view analytics data without allowing them to make changes to the APIs, adhering to the principle of least privilege."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 295,
    "url": "https://www.examtopics.com/discussions/google/view/149120-exam-professional-cloud-developer-topic-1-question-295/",
    "body": "You are migrating a containerized application to Cloud Run. You plan to use Cloud Build to build your container image and push it to Artifact Registry, and you plan to use Cloud Deploy to deploy the image to production. You need to ensure that only secure images are deployed to production. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Armor in front of Cloud Run to protect the container image from threats.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Artifact Analysis to scan the image for vulnerabilities. Use Cloud Key Management Service to encrypt the image to be deployed to production.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Secret Manager to store the encrypted image. Deploy this image to production.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Binary Authorization to enforce a policy that only allows images that have been signed with a trusted key to be deployed to production."
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-10-11T21:50:00.000Z",
        "voteCount": 1,
        "content": "Binary Authorization is a security feature that helps ensure only trusted container images are deployed to production environments. By enforcing a policy that requires images to be signed by a trusted key, you can prevent the deployment of unverified or vulnerable images, significantly enhancing the security of your application."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 296,
    "url": "https://www.examtopics.com/discussions/google/view/149121-exam-professional-cloud-developer-topic-1-question-296/",
    "body": "Your team uses Cloud Storage for a video and image application that was recently migrated to Google Cloud. Following a viral surge, users are reporting application instability, coinciding with a 10x increase in HTTP 429 error codes from Cloud Storage APIs. You need to resolve the errors and establish a long-term solution. You want to ensure that the application remains stable if the load increases again in the future. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOptimize the application code to reduce unnecessary calls to Cloud Storage APIs to prevent HTTP 429 errors.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCompress the video and images files to reduce their size, and minimize storage costs and bandwidth usage. Implement a custom throttling mechanism in the application that limits the number of concurrent API calls.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate all image and video data to Firestore. Replace the Cloud Storage APIs in the application code with the new Firestore database.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement a retry strategy with exponential backoff for requests that encounter HTTP 429 errors."
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-10-11T22:24:00.000Z",
        "voteCount": 1,
        "content": "Classic example of exponential backoff strategy"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 297,
    "url": "https://www.examtopics.com/discussions/google/view/149124-exam-professional-cloud-developer-topic-1-question-297/",
    "body": "You are developing a container build pipeline for an application hosted on GKE. You have the following requirements:<br><br>\u2022 Only images that are created using your build pipeline should be deployed on your GKE cluster.<br>\u2022 All code and build artifacts should remain within your environment and protected from data exfiltration.<br><br>How should you build the pipeline?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a build pipeline by using Cloud Build with the default worker pool.<br>2. Deploy container images to a private container registry in your VPC.<br>3. Create a VPC firewall policy in your project that denies all egress and ingress traffic to public networks.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a build pipeline by using Cloud Build with a private worker pool.<br>2. Use VPC Service Controls to place all components and services in your CI/CD pipeline inside a security perimeter.<br>3. Configure your GKE cluster to only allow container images signed by Binary Authorization.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a build pipeline by using Cloud Build with a private worker pool.<br>2. Configure the CI/CD pipeline to build container images and store them in Artifact Registry.<br>3. Configure Artifact Registry to encrypt container images by using customer-managed encryption keys (CMEK).",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a build pipeline by using Cloud Build with the default worker pool.<br>2. Configure the CI/CD pipeline to build container images and store them in Artifact Registry.<br>3. Configure your GKE cluster to only allow container images signed by Binary Authorization."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-10-11T23:55:00.000Z",
        "voteCount": 1,
        "content": "VPC Service control for data exfiltration"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 298,
    "url": "https://www.examtopics.com/discussions/google/view/149125-exam-professional-cloud-developer-topic-1-question-298/",
    "body": "You are a developer at a company that operates an ecommerce website. The website stores the customer order data in a Cloud SQL for PostgreSQL database. Data scientists on the marketing team access this data to run their reports. Every time they run these reports, the website's performance is negatively affected. You want to provide access to up-to-date customer order datasets without affecting your website. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Cloud Scheduler to run an hourly Cloud Function that exports the data from the Cloud SQL database into CSV format and sends the data to a Cloud Storage bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up a Bigtable table for the data science team. Configure the application to perform dual writes to both Cloud SQL and Bigtable simultaneously.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up a BigQuery dataset for the data science team. Configure Datastream to replicate the relevant Cloud SQL tables in BigQuery.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a clone of the PostgreSQL database instance for the data science team. Schedule a job to create a new clone every 15 minutes."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-10-11T23:57:00.000Z",
        "voteCount": 1,
        "content": "BigQuery for Analytics: BigQuery is a serverless data warehouse designed for large-scale analytics. It can handle complex queries and large datasets without affecting the performance of your operational database (Cloud SQL).\n\nDatastream for Replication: Using Datastream allows you to continuously replicate the relevant tables from Cloud SQL to BigQuery. This ensures that the data scientists have access to up-to-date customer order datasets without putting any load on the Cloud SQL database. They can run their reports and analytics directly on BigQuery, which is optimized for such workloads."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 299,
    "url": "https://www.examtopics.com/discussions/google/view/149126-exam-professional-cloud-developer-topic-1-question-299/",
    "body": "You are developing a web application by using Cloud Run and Cloud Storage. You are notified of a production issue that you need to troubleshoot immediately. You need to implement a workaround that requires you to execute a script on a Git repository. Your corporate laptop is unavailable but you have your personal computer. You can use your corporate credentials to access the required Git repository and Google Cloud resources. You want to fix the issue as quickly and efficiently as possible while minimizing additional cost. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate and launch a workstation with Cloud Workstations on your personal computer. Authenticate and set up API access in the workstation. Clone the Git repository and execute the workaround script. Ensure that the issue has been fixed.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall VS Code and the extension Cloud Code for VS Code on your personal computer. Check the Cloud Run logs in Cloud Code to confirm the error. Execute the workaround script. Ensure that the issue has been fixed.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConnect to the Google Cloud console and open Cloud Shell on your personal computer. Clone the Git repository and execute the workaround script. Ensure that the issue has been fixed.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDownload and install the gcloud CLI on your personal computer. Authenticate and set up API access. Clone the Git repository and execute the workaround script. Ensure that the issue has been fixed."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-10-12T00:00:00.000Z",
        "voteCount": 1,
        "content": "Using cloud shell is more easy approach"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 300,
    "url": "https://www.examtopics.com/discussions/google/view/149127-exam-professional-cloud-developer-topic-1-question-300/",
    "body": "You are using App Engine and Cloud SQL for PostgreSQL to develop an application. You want to test your application code locally before deploying new application versions to the development environment that is shared with other developers. You need to set up your App Engine local development environment to test your application while keeping all traffic to Cloud SQL instances encrypted and authenticated to Cloud IAM and PostgreSQL. What should you do before starting the local development server?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall PostgreSQL on your local workstation. Run a local PostgreSQL database on your workstation. Configure the application to connect to a PostgreSQL instance on localhost.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDownload and install the Cloud SQL Auth Proxy to your local development environment. Configure the Cloud SQL Auth Proxy to connect to the Cloud SQL instance and run the proxy. Configure the application to connect to a PostgreSQL instance on localhost.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy a Compute Engine instance, and install HAProxy on the instance. Configure Cloud SQL Auth Proxy on the instance, and use the instance\u2019s service account to authenticate to Cloud SQL. Configure the application to connect to the Compute Engine instance's IP address.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure your local development server to connect to the private IP address of the Cloud SQL instance. Encrypt database entries with a cryptographic library before submitting them to the database. Store the decryption key as an environment variable in App Engine."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-10-12T00:01:00.000Z",
        "voteCount": 1,
        "content": "The Cloud SQL Auth Proxy provides a secure way to connect to your Cloud SQL instance without having to manage SSL certificates or IAM permissions manually. It handles the authentication and encryption of traffic to the Cloud SQL instance, ensuring that all data transmitted is secure."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 301,
    "url": "https://www.examtopics.com/discussions/google/view/149128-exam-professional-cloud-developer-topic-1-question-301/",
    "body": "You are developing a public web application on Cloud Run. You expose the Cloud Run service directly with its public IP address. You are now running a load test to ensure that your application is resilient against high traffic loads. You notice that your application performs as expected when you initiate light traffic. However, when you generate high loads, your web server runs slowly and returns error messages. How should you troubleshoot this issue?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCheck the network traffic to Cloud Run in Cloud Monitoring to validate whether a traffic spike occurred. If necessary, enable traffic splitting on the Cloud Run instance to route some of the traffic to a previous instance revision.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCheck the min-instances value for your Cloud Run service. If necessary, increase the min-instances value to match the maximum number of virtual users in your load test.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCheck whether Cloud Armor is detecting distributed denial of service (DDoS) attacks and is blocking traffic before the traffic is routed to your Cloud Run service. If necessary, disable any Cloud Armor policies in your project.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCheck whether the Cloud Run service has scaled to a number of instances that equals the max-instances value. If necessary, increase the max-instances value."
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-10-12T00:30:00.000Z",
        "voteCount": 1,
        "content": "D. Check whether the Cloud Run service has scaled to a number of instances that equals the max-instances value. If necessary, increase the max-instances value."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 302,
    "url": "https://www.examtopics.com/discussions/google/view/149129-exam-professional-cloud-developer-topic-1-question-302/",
    "body": "You are developing a new image processing application that needs to handle various tasks, such as resizing, cropping, and watermarking images. You also need to monitor the workflow and ensure that it scales efficiently when there are large volumes of images. You want to automate the image processing tasks and workflow monitoring with the least effort. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEmploy Cloud Composer to manage the image processing workflows. Use Dataproc for workflow monitoring and analytics.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Run to deploy the image processing functions. Use Apigee to expose the API. Use Cloud Logging for workflow monitoring.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement Workflows to orchestrate the image processing tasks. Use Cloud Logging for workflow monitoring.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Build to trigger Cloud Functions for the image processing tasks. Use Cloud Monitoring for workflow monitoring."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-10-12T00:33:00.000Z",
        "voteCount": 1,
        "content": "Google Cloud Workflows is ideal for orchestrating tasks across multiple services, like image processing tasks (resizing, cropping, watermarking). It allows you to manage and automate complex workflows with minimal effort."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 303,
    "url": "https://www.examtopics.com/discussions/google/view/149130-exam-professional-cloud-developer-topic-1-question-303/",
    "body": "You are developing a web application that will be deployed to production on Cloud Run. The application consists of multiple microservices, some of which will be publicly accessible and others that will only be accessible after authentication by Google identities. You need to ensure that only authenticated users can access the restricted services, while allowing unrestricted access to the public services of the application. You want to use the most secure approach while minimizing management overhead and complexity. How should you configure access?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Identity-Aware Proxy (IAP) for all microservices. Develop a new microservice that checks the authentication requirements for each application and controls access to the respective services.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Identity-Aware Proxy (IAP) for all microservices. Manage access control lists (ACLs) for the restricted services, and configure allAuthenticatedUsers access to the public services.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Endpoints with Firebase Authentication for all microservices. Configure Firebase rules to manage access control lists (ACLs) for each service, allowing access to the public services.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure separate Cloud Run services for the public and restricted microservices. Enable Identity-Aware Proxy (IAP) only for the restricted services, and configure the Cloud Run ingress settings to \u2018Internal and Cloud Load Balancing\u2019."
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-10-12T00:34:00.000Z",
        "voteCount": 1,
        "content": "Configure separate Cloud Run services for the public and restricted microservices. Enable Identity-Aware Proxy (IAP) only for the restricted services, and configure the Cloud Run ingress settings to \u2018Internal and Cloud Load Balancing\u2019."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 304,
    "url": "https://www.examtopics.com/discussions/google/view/149131-exam-professional-cloud-developer-topic-1-question-304/",
    "body": "You are the lead developer for a company that provides a financial risk calculation API. The API is built on Cloud Run and has a gRPC interface. You frequently develop optimizations to the risk calculators. You want to enable these optimizations for select customers who registered to try out the optimizations prior to rolling out the optimization to all customers. Your CI/CD pipeline has built a new image and stored it in the Artifact Registry.<br><br>Which rollout strategy should you use?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the traffic to the new service by setting Cloud Run\u2019s traffic split based on the percentage of registered customers.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the traffic to the new service by using a blue/green deployment approach.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the traffic to the new service by using a feature flag for registered customers.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the traffic to the new service and enable session affinity for Cloud Run."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-10-12T00:36:00.000Z",
        "voteCount": 1,
        "content": "Using feature flags allows you to selectively enable the new optimizations only for registered customers who have signed up to try them. This provides more control and flexibility over which customers get access to the new features without the need to route traffic based on percentages or splitting across revisions."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 305,
    "url": "https://www.examtopics.com/discussions/google/view/149133-exam-professional-cloud-developer-topic-1-question-305/",
    "body": "Your ecommerce application has a rapidly growing user base, and it is experiencing performance issues due to excessive requests to your backend API. Your team develops and manages this API. The Cloud SQL backend database is struggling to handle the high demand, leading to latency and timeouts. You need to implement a solution that optimizes API performance and improves user experience. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Apigee to expose your API. Use Memorystore for Redis to cache frequently accessed data. Implement exponential backoff in the application to retry failed requests.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Apigee to expose your API. Implement rate limiting and access control policies in Apigee to control API traffic. Use Pub/Sub to queue requests to prevent database overload.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Load Balancing to expose your API. Use Cloud CDN in front of the load balancer to cache responses. Implement exponential backoff to retry failed requests.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Load Balancing to expose your API. Increase the memory for the database instances to handle more concurrent requests. Implement a custom rate-limiting mechanism in your application code to control API requests."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-10-12T00:39:00.000Z",
        "voteCount": 1,
        "content": "Apigee: Using Apigee to expose your API helps manage, secure, and scale your API. Apigee provides robust traffic management features like rate limiting, analytics, and security, making it ideal for handling a growing user base.\nMemorystore for Redis: Caching frequently accessed data in Memorystore (a managed Redis service) offloads repetitive read requests from your Cloud SQL backend, reducing database load and improving response times.\nExponential Backoff: Implementing exponential backoff in the application ensures smoother handling of failed or timed-out requests by retrying them with increasing delays, preventing system overload during high traffic periods."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 306,
    "url": "https://www.examtopics.com/discussions/google/view/149134-exam-professional-cloud-developer-topic-1-question-306/",
    "body": "You need to deploy a new feature into production on Cloud Run. Your company\u2019s SRE team mandates gradual deployments to avoid large downtimes caused by code change errors. You want to configure this deployment with minimal effort. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the application\u2019s frontend load balancer to toggle between the new and old revisions.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the application code to send a small percentage of users to the newly deployed revision.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the feature with \u201cServe this revision immediately\u201d unchecked, and configure the new revision to serve a small percentage of traffic. Check for errors, and increase traffic to the revision as appropriate.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the feature with \u201cServe this revision immediately\u201d checked. Check for errors, roll back to the previous revision, and repeat the process until you have verified that the deployment is bug-free."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-10-12T00:41:00.000Z",
        "voteCount": 1,
        "content": "By deploying the new feature with \u201cServe this revision immediately\u201d unchecked, you can control how much traffic the new revision receives without fully switching over to it.\nConfiguring a small percentage of traffic to go to the new revision allows for gradual testing in production with real users, minimizing the risk of downtime or large-scale errors.\nIf no issues are encountered, you can gradually increase the traffic to the new revision until it fully replaces the old one."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 307,
    "url": "https://www.examtopics.com/discussions/google/view/149135-exam-professional-cloud-developer-topic-1-question-307/",
    "body": "You are developing an external-facing application on GKE that provides a streaming API to users. You want to offer two subscription tiers, \u201cbasic\" and \u201cpremium\", to users based on the number of API requests that each client application is allowed to make each day. You want to design the application architecture to provide subscription tiers to users while following Google-recommended practices. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Configure the service on GKE as a backend to an Apigee proxy.<br>2. Provide API keys to users to identify client applications.<br>3. Configure a Quota policy in Apigee for API keys based on the subscription tier.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Configure the service on GKE as a backend to an Apigee proxy.<br>2. Provide API keys to users to identify client applications.<br>3. Configure a SpikeArrest policy in Apigee for API keys based on the subscription tier.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Configure the service on GKE as a backend to two new projects, each with a separate Application Load Balancer.<br>2. Configure the quota \"Queries per second (QPS) per region per network\u201d for each project individually.<br>3. Provide users with API endpoints based on the subscription tier.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Deploy the application to two GKE clusters, one for each subscription tier. Configure each cluster to have a separate Ingress.<br>2. Configure each cluster as a backend to an Apigee proxy.<br>3. Provide API keys to users to identify client applications.<br>4. Configure separate rate limits for client applications based on the subscription tier."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-10-12T00:45:00.000Z",
        "voteCount": 1,
        "content": "Apigee Proxy: Apigee acts as an API gateway, handling security, rate limiting, and traffic management. Using it ensures scalability and the ability to manage subscription-based API access efficiently.\nAPI Keys: Issuing API keys to identify client applications allows for tracking and controlling usage.\nQuota Policy: Apigee's quota policies allow you to set request limits (e.g., daily or per-minute quotas) based on the subscription tier. This is an ideal solution for managing different subscription levels, as you can define different API request limits for \"basic\" and \"premium\" users."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 308,
    "url": "https://www.examtopics.com/discussions/google/view/149136-exam-professional-cloud-developer-topic-1-question-308/",
    "body": "Your organization has users and groups configured in an external identity provider (IdP). You want to leverage the same external IdP to allow Google Cloud console access to all employees. You also want to personalize the sign-in experience by displaying the user's name and photo when users access the Google Cloud console. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure workforce identity federation with the external IdP, and set up attribute mapping.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a service account for each individual by using the user name and photo, and grant permissions for each user to impersonate their respective service accounts.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure workload identity federation to get the external IdP tokens, and use these tokens to sign in to the Google Cloud console.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Google group that includes organization email IDs for all users. Ask users to use the same name, work email ID, and password to register and sign in."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-10-12T00:50:00.000Z",
        "voteCount": 1,
        "content": "Workforce Identity Federation allows organizations to authenticate and manage access for users from external IdPs (such as Azure AD or Okta) without creating and managing separate Google Cloud accounts.\nAttribute Mapping enables the personalization of the user experience by mapping attributes such as the user's name and photo from the external IdP, ensuring that the user's details are displayed correctly when accessing the Google Cloud console."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 309,
    "url": "https://www.examtopics.com/discussions/google/view/149137-exam-professional-cloud-developer-topic-1-question-309/",
    "body": "You are developing a new API that creates requests on an asynchronous message service. Requests will be consumed by different services. You need to expose the API by using a gRPC interface while minimizing infrastructure management overhead. How should you deploy the API?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy your API to App Engine. Create a Pub/Sub topic, and configure your API to push messages to the topic.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy your API as a Cloud Run service. Create a Pub/Sub topic, and configure your API to push messages to the topic.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy your API to a GKE cluster. Create a Kafka cluster, and configure your API to write messages to the cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy your API on a Compute Engine instance. Create a Kafka cluster, and configure your API to write messages to the cluster."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-10-12T00:53:00.000Z",
        "voteCount": 1,
        "content": "Cloud Run is best suitable for gRPC interface"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 310,
    "url": "https://www.examtopics.com/discussions/google/view/149139-exam-professional-cloud-developer-topic-1-question-310/",
    "body": "You are about to deploy an application hosted on a Compute Engine instance with Windows OS and Cloud SQL. You plan to use the Cloud SQL Auth Proxy for connectivity to the Cloud SQL instance. You plan to follow Google-recommended practices and the principle of least privilege. You have already created a custom service account. What should you do next?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate and assign a custom role with the cloudsql.instances.connect permission to the custom service account. Adjust the Cloud SQL Auth Proxy start command to specify your instance connection name.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGrant the custom service account the roles/cloudsql.client role. Adjust the Cloud SQL Auth Proxy start command to use the --unix-socket CLI option.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGrant the custom service account the roles/cloudsql.editor role.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGrant the custom service account the roles/cloudsql.viewer role. Adjust the Cloud SQL Auth Proxy start command to specify your instance connection name."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-10-12T00:57:00.000Z",
        "voteCount": 1,
        "content": "B"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 311,
    "url": "https://www.examtopics.com/discussions/google/view/149140-exam-professional-cloud-developer-topic-1-question-311/",
    "body": "You are developing a secure document sharing platform. The platform allows users to share documents with other users who may be external to their organization. Access to these documents should be revoked after a configurable time period. The documents are stored in Cloud Storage. How should you configure Cloud Storage to support this functionality?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate signed policy documents on the Cloud Storage bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tApply access control list (ACL) permissions to the Cloud Storage bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGenerate a signed URL for each document the user wants to share.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGrant the Storage Object Viewer IAM role to all authenticated users."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-10-12T01:03:00.000Z",
        "voteCount": 1,
        "content": "easy way : signed URL"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 312,
    "url": "https://www.examtopics.com/discussions/google/view/149142-exam-professional-cloud-developer-topic-1-question-312/",
    "body": "You work for an environmental agency in a large city. You are developing a new monitoring platform that will capture air quality readings from thousands of locations in the city. You want the air quality reading devices to send and receive their data payload to the newly created RESTful backend systems every minute by using a curl command. The backend systems are running in a single cloud region and are using Premium Tier networking. You need to connect the devices to the backend while minimizing the daily average latency, measured by using Time to First Byte (TTFB). How should you build this service?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Run the air quality devices\u2019 backends on Compute Engine VMs.<br>2. Create a weighted round robin routing policy on Cloud DNS.<br>3. Configure the air quality devices to connect by using this DNS.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Run the air quality devices\u2019 backends on Compute Engine VMs.<br>2. Create a round robin routing policy on Cloud DNS for these Compute Engine VMs.<br>3. Configure the air quality devices to connect by using this DNS.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Run the air quality devices' backends in a managed instance group.<br>2. Create an external passthrough Network Load Balancer to connect to the managed instance group.<br>3. Configure a connection between the air quality devices and the Network Load Balancer.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1 Run the air quality devices' backends in a managed instance group.<br>2. Create an external Application Load Balancer, and connect it to the managed instance group.<br>3. Configure a connection between the air quality devices and the Application Load Balancer."
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-10-12T01:06:00.000Z",
        "voteCount": 1,
        "content": "Managed Instance Group: Using a managed instance group allows for automatic scaling based on demand, which is essential when dealing with a high volume of data coming from thousands of devices.\nExternal Application Load Balancer: This type of load balancer is designed to distribute incoming traffic across multiple instances in the managed instance group, helping to ensure low latency and high availability. The Application Load Balancer also supports HTTP/HTTPS traffic, which is ideal for RESTful APIs."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 313,
    "url": "https://www.examtopics.com/discussions/google/view/149143-exam-professional-cloud-developer-topic-1-question-313/",
    "body": "Your infrastructure team is responsible for creating and managing Compute Engine VMs. Your team uses the Google Cloud console and gcloud CLI to provision resources for the development environment. You need to ensure that all Compute Engine VMs are labeled correctly for compliance reasons. In case of missing labels, you need to implement corrective actions so the labels are configured accordingly without changing the current deployment process. You want to use the most scalable approach. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a Cloud Audit Logs trigger to invoke a Cloud Function when a Compute Engine VM is created. Check for missing labels and assign them if necessary.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy resources with Terraform. Use the gcloud terraform vet command with a policy to ensure that every Compute Engine VM that is provisioned by Terraform has labels set.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite a script to check all Compute Engine VMs for missing labels regularly by using Cloud Scheduler. Use the script to assign the labels.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCheck all Compute Engine VMs for missing labels regularly. Use the console to assign the labels."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-10-12T01:12:00.000Z",
        "voteCount": 1,
        "content": "Use a Cloud Audit Logs trigger to invoke a Cloud Function when a Compute Engine VM is created. Check for missing labels and assign them if necessary."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 314,
    "url": "https://www.examtopics.com/discussions/google/view/149144-exam-professional-cloud-developer-topic-1-question-314/",
    "body": "You are developing a discussion portal that is built on Cloud Run. Incoming external requests are routed through a set of microservices before a response is sent. Some of these microservices connect to databases. You need to run a load test to identify any bottlenecks in the application when it is under load. You want to follow Google-recommended practices. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the response to include a time series that shows elapsed time per service. Use Log Analytics in Cloud Logging to create a heatmap that exposes any service that could be a bottleneck.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Cloud Trace to capture the requests from the load testing clients. Review the timings in Cloud Trace.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExpose the latency metrics per service for each request. Configure Google Cloud Managed Service for Prometheus, and use it to scrape and analyze the metrics.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd log statements that capture elapsed time. Analyze the logs and metrics by using BigQuery."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-10-12T01:18:00.000Z",
        "voteCount": 1,
        "content": "Cloud Trace is designed to analyze the latency of applications, especially those built using microservices. It automatically collects latency data for each request, allowing you to see how long each service takes to respond"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 315,
    "url": "https://www.examtopics.com/discussions/google/view/149145-exam-professional-cloud-developer-topic-1-question-315/",
    "body": "Your team currently uses Bigtable as their database backend. In your application's app profile, you notice that the connection to the Bigtable cluster is specified as single-cluster routing, and the cluster\u2019s connection logic is configured to conduct manual failover when the cluster is unavailable. You want to optimize the application code to have more efficient and highly available Bigtable connectivity. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up Memcached so that queries hit the cache layer first and automatically get data from Bigtable in the event of a cache miss.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the Bigtable client\u2019s connection pool size.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a Dataflow template, and use a Beam connector to stream data changes.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the app profile to use multi-cluster routing."
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-10-12T01:19:00.000Z",
        "voteCount": 1,
        "content": "Configure the app profile to use multi-cluster routing."
      },
      {
        "date": "2024-10-12T01:20:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/bigtable/docs/replication-settings#high-availability"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 316,
    "url": "https://www.examtopics.com/discussions/google/view/149146-exam-professional-cloud-developer-topic-1-question-316/",
    "body": "You work for an ecommerce company. Your company is migrating multiple applications to Google Cloud, and you are assisting with the migration of one of the applications. The application is currently deployed on a VM without any OS dependencies. You have created a Dockerfile and used it to upload a new image to Artifact Registry. You want to minimize the infrastructure and operational complexity. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the image to Cloud Run.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the image to a GKE Autopilot cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the image to a GKE Standard cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the image to a Compute Engine instance."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-10-12T01:22:00.000Z",
        "voteCount": 1,
        "content": "Cloud run"
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 317,
    "url": "https://www.examtopics.com/discussions/google/view/149147-exam-professional-cloud-developer-topic-1-question-317/",
    "body": "You recently deployed an Apigee API proxy to your organization across two regions. Both regions are configured with a separate backend that is hosting the API. You need to configure Apigee to route traffic to the appropriate local region backend. What should you do?<br><br><img src=\"https://img.examtopics.com/professional-cloud-developer/image14.png\">",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a TargetEndpoint with a weighted load balancing algorithm. Configure the API proxy to use the same weights for each region's backend.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a regional internal Application Load Balancer in each region, and use health checks to verify that each backend is active. Create a DNS A record that contains the IP addresses of both regions' load balancers. Configure a Targetserver for each region that uses this DNS name.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a global external Application Load Balancer and configure each region\u2019s backend with a different regional backend service. Each region communicates to this single global external Application Load Balancer as its TargetServer.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a TargetServer for each region's backend host names. Configure the API proxy to choose the TargetServer based on the system.region.name flow variable."
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-10-12T01:24:00.000Z",
        "voteCount": 1,
        "content": "Utilizing the system.region.name flow variable allows Apigee to dynamically determine which backend to use based on the region from which the request originates. This enables regional traffic management and helps reduce latency by ensuring that users are routed to the nearest backend."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 318,
    "url": "https://www.examtopics.com/discussions/google/view/149148-exam-professional-cloud-developer-topic-1-question-318/",
    "body": "You are a developer that works for a local concert venue. Customers use your company\u2019s website to purchase tickets for events. You need to provide customers with immediate confirmation when a selected seat has been reserved. How should you design the ticket ordering process?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd the seat reservation to a Cloud Tasks queue, which triggers Workflows to process the seat reservation.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPublish the seat reservation to a Pub/Sub topic. Configure the backend service to use Eventarc to process the seat reservation on GKE.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpload the seat reservation to a Cloud Storage bucket, which triggers an event to a Cloud Run service that processes the orders.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSubmit the seat reservation in an HTTP POST request to an Application Load Balancer. Configure the Application Load Balancer to distribute the request to a Compute Engine managed instance group that processes the reservation."
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-10-12T01:30:00.000Z",
        "voteCount": 1,
        "content": "Asynchronous Processing:\n\nBy using Pub/Sub, you can publish the seat reservation message immediately, allowing for quick acknowledgment to the customer without waiting for the backend processing to complete. This ensures that users receive confirmation right away, improving the user experience.\nEventarc Integration:\n\nEventarc can be configured to route events from Pub/Sub to your backend service running on GKE. This allows for flexibility and scalability, as your service can automatically handle incoming requests based on events published to the topic."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 319,
    "url": "https://www.examtopics.com/discussions/google/view/149149-exam-professional-cloud-developer-topic-1-question-319/",
    "body": "You work for a financial services company that has a container-first approach. Your team develops microservices applications. You have a Cloud Build pipeline that creates a container image, runs regression tests, and publishes the image to Artifact Registry. You need to ensure that only containers that have passed the regression tests are deployed to GKE clusters. You have already enabled Binary Authorization on the GKE clusters. What should you do next?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy Voucher Server and Voucher Client components. After a container image has passed the regression tests, run Voucher Client as a step in the Cloud Build pipeline.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an attestor and a policy. Run a vulnerability scan to create an attestation for the container image as a step in the Cloud Build pipeline.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an attestor and a policy. Create an attestation for the container images that have passed the regression tests as a step in the Cloud Build pipeline.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the Pod Security Standard level to Restricted for the relevant namespaces. Digitally sign the container images that have passed the regression tests as a step in the Cloud Build pipeline."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-10-12T01:34:00.000Z",
        "voteCount": 1,
        "content": "You need to create an attestor that can attest to the validity of the container images. The attestor will verify that the images meet the defined policy conditions, such as having passed all regression tests."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 320,
    "url": "https://www.examtopics.com/discussions/google/view/149150-exam-professional-cloud-developer-topic-1-question-320/",
    "body": "You have an application running in production on Cloud Run. Your team needs to change one of the application\u2019s services to return a new field. You want to test the new revision on 10% of your clients using the least amount of effort. You also need to keep your service backward compatible.<br><br>What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReplace the current service with the new revision. Deploy the new revision with no traffic allocated. After the deployment, split the traffic between the previous service and the new revision.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the current service with the new changes. Deploy the new revision. After the deployment, split the traffic between the current service and the new revision.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the current service with the new changes. Deploy the new revision with no traffic allocated. Split the traffic between the current service and the new revision.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReplace the current service with the new revision. Deploy the new revision. Create a load balancer to split the traffic between the previous service and the new revision."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-10-12T01:37:00.000Z",
        "voteCount": 1,
        "content": "Backward Compatibility:\n\nBy deploying the new revision with no traffic allocated initially, you ensure that existing clients continue to receive responses from the current service without any disruptions. This approach preserves backward compatibility, allowing clients that rely on the previous response structure to continue functioning.\nTraffic Splitting:\n\nAfter deploying the new revision, you can then proceed to split the traffic between the current service and the new revision. In this case, directing 10% of the traffic to the new revision allows you to test the changes with a subset of users while monitoring for any issues."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 321,
    "url": "https://www.examtopics.com/discussions/google/view/149162-exam-professional-cloud-developer-topic-1-question-321/",
    "body": "Your team plans to use AlloyDB as their database backend for an upcoming application release. Your application is currently hosted in a different project and network than the AlloyDB instances. You need to securely connect your application to the AlloyDB instance while keeping the projects isolated. You want to minimize additional operations and follow Google-recommended practices. How should you configure the network for database connectivity?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tProvision a Shared VPC project where both the application project and the AlloyDB project are service projects.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AlloyDB Auth Proxy and configure the application project\u2019s firewall to allow connections to port 5433.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tProvision a service account from the AlloyDB project. Use this service account\u2019s JSON key file as the --credentials-file to connect to the AlloyDB instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAsk the database team to provision AlloyDB databases in the same project and network as the application."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-10-12T06:43:00.000Z",
        "voteCount": 1,
        "content": "he best option for securely connecting your application to the AlloyDB instance while keeping the projects isolated, following Google-recommended practices, and minimizing additional operations is Option B: Use AlloyDB Auth Proxy and configure the application project\u2019s firewall to allow connections to port 5433."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 322,
    "url": "https://www.examtopics.com/discussions/google/view/149165-exam-professional-cloud-developer-topic-1-question-322/",
    "body": "You have an on-premises containerized service written in the current stable version of Python 3 that is available only to users in the United States. The service has high traffic during the day and no traffic at night. You need to migrate this application to Google Cloud and track error logs after the migration in Error Reporting. You want to minimize the cost and effort of these tasks. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the code on Cloud Run. Configure your code to write errors to standard error.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the code on Cloud Run. Configure your code to stream errors to a Cloud Storage bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the code on a GKE Autopilot cluster. Configure your code to write error logs to standard error.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the code on a GKE Autopilot cluster. Configure your code to write error logs to a Cloud Storage bucket."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-10-12T07:33:00.000Z",
        "voteCount": 1,
        "content": "A. Deploy the code on Cloud Run. Configure your code to write errors to standard error."
      }
    ],
    "examNameCode": "professional-cloud-developer",
    "topicNumber": "1"
  }
]