[
  {
    "topic": 1,
    "index": 1,
    "url": "https://www.examtopics.com/discussions/google/view/7083-exam-professional-cloud-architect-topic-1-question-1/",
    "body": "Your company has decided to make a major revision of their API in order to create better experiences for their developers. They need to keep the old version of the API available and deployable, while allowing new customers and testers to try out the new API. They want to keep the same SSL and DNS records in place to serve both APIs.<br>What should they do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a new load balancer for the new version of the API",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReconfigure old clients to use a new endpoint for the new API",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tHave the old API forward traffic to the new API based on the path",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse separate backend pools for each API path behind the load balancer\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 40,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-09-20T11:04:00.000Z",
        "voteCount": 101,
        "content": "D is the answer because HTTP(S) load balancer can direct traffic reaching a single IP to different backends based on the incoming URL. A is not correct because configuring a new load balancer would require a new or different SSL and DNS records which conflicts with the requirements to keep the same SSL and DNS records. B is not correct because it goes against the requirements. The company wants to keep the old API available while new customers and testers try the new API. C is not correct because it is not a requirement to decommission the implementation behind the old API. Moreover, it introduces unnecessary risk in case bugs or incompatibilities are discovered in the new API."
      },
      {
        "date": "2022-10-16T13:34:00.000Z",
        "voteCount": 3,
        "content": "D is right"
      },
      {
        "date": "2019-11-19T07:38:00.000Z",
        "voteCount": 19,
        "content": "agreed, The answer is D"
      },
      {
        "date": "2024-09-21T02:29:00.000Z",
        "voteCount": 2,
        "content": "You have a Compute Engine managed instance group that adds and removes Compute Engine instances from the group in response to the load on your application.The instances have a shutdown script that removes REDIS database entries associated with the instance.You see that many database entries have not been removed, and  you suspect that the shutdown script is the problem.You need to ensure that the commands in the shutdown script are run reliably  every time an instance is shut down. You create a Cloud Function to remove the database entries.What should you do next?\nA.\nModify the shutdown script to wait for 30 seconds and then publish a message to a Pub/Sub queue.\nB.\nModify the shutdown script to wait for 30 seconds before triggering the Cloud Function.\nC.\nSet up a Cloud Monitoring sink that triggers the Cloud Function after an instance removal log message arrives in Cloud logging\nD.\nDo not use the Cloud Function. Modify the shutdown script  to restart if it has not completed in 30 seconds. \n\nwhat is your answer?"
      },
      {
        "date": "2022-03-31T06:08:00.000Z",
        "voteCount": 1,
        "content": "c it's correct"
      },
      {
        "date": "2024-09-21T02:29:00.000Z",
        "voteCount": 1,
        "content": "Your company and one of its partners each have a Google cloud project in separate organizations.Your complny's project(prj-a) runs in virtual Private Cloud (vpc-a). The partner's project (prj-b) runs in vpc-b. There are two instances running on vpc-a and one instance running on vpc-b \nSubnets defined in both VPCs are not overlapping.You need to ensure that all instances communicate with each other via internal IPs \nminimizing latency and maximizing throughput.What should you do \nA.Set up a network peering between vpc-a and vpc-b\nB.Set up a VPN between vpc-a and vpc-b using cloud VPN \nC.Configure IAP TCP forwarding on the instance in vpc-b, and then launch the following gcloud command from one of the instances \nvpc-a gcloud:\ngcloud compute start-iap-tunnel INSTANCE_NAME_VPC_\n--local-host-port=localhost:22\nD.\n1.Create an additional instance in vpc-a. \n2.Create an additional instance in vpc-b. \n3.Install OpenVPN in newly created instances \n4.Configure a VPN tunnel between vpc-a and vpc-b and b with the help of OpenVPN.\n\nwhat is your answer?"
      },
      {
        "date": "2022-06-06T17:48:00.000Z",
        "voteCount": 2,
        "content": "A. VPC peering"
      },
      {
        "date": "2024-09-21T02:29:00.000Z",
        "voteCount": 1,
        "content": "Your company has an application running on App engine that allows users to upload music files and share therm with other people .You want to allow users to upload files directly into Cloud storage from their browser session.The payload should not be passed through the  backend. What should you do?\nA.\nSet a CORS configuration in the target Cloud storage bucket where the base URL of the App Engine application is an allowed origin.Use the Cloud Storage signed URL feature to generate a POST URL.\nB.\nSet a CORS configuration in the target cloud storage bucket where the base URL of the App Engine application is an allowed origin. Assign the Cloud Storage WRITER role to users who upload files.\nC.\nUse the Cloud Storage Signed URL feature to generate a  POST URL.Use App Engine default credentials to sign requests against Cloud Storage.\nD.\nAssign the Cloud Storage WRITER role to users who upload files; use App Engine default credentials to sign requests against Cloud Storage.\n\nwhat is your answer\uff1f"
      },
      {
        "date": "2022-06-02T09:01:00.000Z",
        "voteCount": 1,
        "content": "Must be A"
      },
      {
        "date": "2022-08-15T04:03:00.000Z",
        "voteCount": 1,
        "content": "C.\nUse the Cloud Storage Signed URL feature to generate a POST URL"
      },
      {
        "date": "2022-11-12T08:19:00.000Z",
        "voteCount": 1,
        "content": "A, The Cross Origin Resource Sharing (CORS) spec was developed by the World Wide Web Consortium (W3C) to remove the limit of Same Origin Policy. Cloud Storage supports this specification by allowing you to configure your buckets to support CORS. \n https://cloud.google.com/storage/docs/cross-origin\n\nThen for authentication I'll go with signed URL feature, it gives more security"
      },
      {
        "date": "2024-09-21T02:29:00.000Z",
        "voteCount": 1,
        "content": "You are deploying an application to Google Cloud. The of a system.The application in Google Cloud must communicate to private network with applications in a non-Google cloud environment \nThe expected average throughput is 200 kbps.The business require \n\u00b7as close to 100% system availability as possible\n\u00b7cost optimization\nYou need to design the connectivity between the business requirements. What should you provision? \nA.\nAn HA Cloud VPN gateway connected with two tunnels to an on-premises VPN gateway.\nB.\nA single Cloud VPN gateway connected to an on-premises VPN gateway.\nC.Two Classic Cloud VPN gateways connected to two on-premises VPN gateways. Configure each classic cloud VPN gateway to have two tunnels,each connected to different on-premises VPN gateways.\nD.\nTwo HA Cloud VPN gateways connected to two on-premises VPN gateways. Configure each HA Cloud VPN gateway to have two tunnels,each connected to different on-premises VPN gateways.\n\nwhat is your answer?"
      },
      {
        "date": "2022-04-02T17:06:00.000Z",
        "voteCount": 2,
        "content": "Option D seems to be the right one..\ncloud.google.com/network-connectivity/docs/vpn/concepts/topologies#to_peer_vpn_gateways"
      },
      {
        "date": "2022-08-15T03:38:00.000Z",
        "voteCount": 1,
        "content": "I think it should be A. refer to the documentation, to have HA 99.99%, it needs at least 2 tunnels. And one HA VPN with one peer VPN could have 2 tunnels. In Google Cloud, the REDUNDANCY_TYPE for this configuration takes the value SINGLE_IP_INTERNALLY_REDUNDANT.\nD. not cost optimized\nB. didn't mention 2 tunnels\nC. not simple"
      },
      {
        "date": "2024-09-21T02:28:00.000Z",
        "voteCount": 2,
        "content": "D because This approach allows you to keep the same SSL and DNS records while directing traffic based on the API path. The load balancer can be configured to route requests to different backend pools depending on whether the request is for the old or new API version. This ensures that both versions are accessible under the same domain, providing a seamless transition for both old and new users."
      },
      {
        "date": "2024-07-03T07:46:00.000Z",
        "voteCount": 1,
        "content": "D is right"
      },
      {
        "date": "2024-05-09T23:06:00.000Z",
        "voteCount": 1,
        "content": "D is the correct"
      },
      {
        "date": "2024-04-21T14:34:00.000Z",
        "voteCount": 1,
        "content": "D is the answer"
      },
      {
        "date": "2024-01-24T00:16:00.000Z",
        "voteCount": 2,
        "content": "D is ok"
      },
      {
        "date": "2024-01-09T05:20:00.000Z",
        "voteCount": 1,
        "content": "D answer is correct"
      },
      {
        "date": "2023-12-26T03:33:00.000Z",
        "voteCount": 1,
        "content": "Agreed D is the answer"
      },
      {
        "date": "2023-12-10T11:56:00.000Z",
        "voteCount": 1,
        "content": "D is the right answer."
      },
      {
        "date": "2023-09-17T23:51:00.000Z",
        "voteCount": 1,
        "content": "Obvious.... split the feed"
      },
      {
        "date": "2023-08-23T06:01:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is D as visually explained in the HTTPS Load Balancer architecture infographic (figure 5.5) at page 225 in the \"GCP Professional Cloud Network Engineer Certification Companion\" book."
      },
      {
        "date": "2023-08-15T02:02:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 2,
    "url": "https://www.examtopics.com/discussions/google/view/7080-exam-professional-cloud-architect-topic-1-question-2/",
    "body": "Your company plans to migrate a multi-petabyte data set to the cloud. The data set must be available 24hrs a day. Your business analysts have experience only with using a SQL interface.<br>How should you store the data to optimize it for ease of analysis?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLoad data into Google BigQuery\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInsert data into Google Cloud SQL",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPut flat files into Google Cloud Storage",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStream data into Google Cloud Datastore"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 26,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-09-20T11:06:00.000Z",
        "voteCount": 35,
        "content": "This question could go either way for A or B. But Big Query was designed with this in mind, according to numerous Google presentation and videos. Cloud Datastore is a NoSQL database (https://cloud.google.com/datastore/docs/concepts/overview)\nCloud Storage does not have an SQL interface. The previous two sentences eliminate options C and D. So I'd pick \"A\"."
      },
      {
        "date": "2020-08-05T02:57:00.000Z",
        "voteCount": 16,
        "content": "A is ok"
      },
      {
        "date": "2022-10-19T02:03:00.000Z",
        "voteCount": 1,
        "content": "IMHO, it should be A only. The reason is that they want to perform analysis on the data and BigQuery excels in that over Cloud SQL. You can run SQL queries in both but I BigQuery has better analytical tools. It can do ad-hoc analysis like Cloud SQL using Cloud Standard SQL and it can do geo-spatial and ML analysis via its Cloud Standard SQL interface."
      },
      {
        "date": "2022-10-19T02:05:00.000Z",
        "voteCount": 4,
        "content": "Also the question does not say whether the data is relational or not. So we cannot assume it is only relational. Therefore, for maximum flexibility BQ is the correct option also. Note that Cloud SQL storage capacity is now at 64TB"
      },
      {
        "date": "2022-10-16T19:32:00.000Z",
        "voteCount": 4,
        "content": "Cloud SQL does not scale to that magnitude also Cloud SQL is not meant for OLAP\nAnswer is BigQuery"
      },
      {
        "date": "2022-01-16T00:49:00.000Z",
        "voteCount": 13,
        "content": "B is not correct because Cloud SQL storage limit doesn't fit the requirement."
      },
      {
        "date": "2020-05-09T14:59:00.000Z",
        "voteCount": 14,
        "content": "I'll go with A because BQ (and BT) are usually meant for analytics.\nB isn't correct because Cloud SQL does not scale to that volume.\nC isn't correct because Cloud Storage does not provide a standard SQL mechanism.\nD could be right but it sounds off because of the analytics requirement."
      },
      {
        "date": "2024-09-24T22:19:00.000Z",
        "voteCount": 1,
        "content": "A, BigQuery is more suitable for analysis. While Cloud SQL can work, it's more generic"
      },
      {
        "date": "2024-09-21T02:30:00.000Z",
        "voteCount": 2,
        "content": "Let's go with option elimination\nA. Load data into Google BigQuery\n&gt;&gt;Big Query = Analytic + SQL (Ease of using SQL) Storage hence the solution\nB. Insert data into Google Cloud SQL\n&gt;&gt; Yes you can SQL query with your own applicaiton console compared to BigQuery SQL console, and 24 hrs avalablity but you won't have 1-2 sec response on petabytes of data, as you can do in GCP BigQuery partitioned and clustered tables.\nC. Put flat files into Google Cloud Storage\n&gt;&gt;The requirement is for analytics and SQL querying of data. You can store it in the flat file but will need to use GCP BigQuery to do that\nD. Stream data into Google Cloud Datastore\n&gt;&gt; Only dealing with storage problems does not address analytics and SQL querying"
      },
      {
        "date": "2021-09-06T22:40:00.000Z",
        "voteCount": 2,
        "content": "Hence Option A"
      },
      {
        "date": "2024-09-21T02:30:00.000Z",
        "voteCount": 2,
        "content": "A. Load data into Google BigQuery\n\nBigQuery is a fully managed, cloud-native data warehousing solution that makes it easy to analyze large and complex datasets. It is optimized for analyzing large amounts of data quickly, and can handle petabyte-scale datasets with ease. It also has a SQL-like interface that is familiar to business analysts, making it easy for them to query and analyze the data. Additionally, BigQuery is highly scalable and can handle high query concurrency, making it a good choice for storing data that must be available 24/7.\n\nOption B, inserting data into Google Cloud SQL, is not a good choice for a multi-petabyte dataset because Cloud SQL is not designed to handle such large volumes of data. Option C, putting flat files into Cloud Storage, is also not a good choice because it is not optimized for querying and analyzing data. Option D, streaming data into Cloud Datastore, is not a good choice because Cloud Datastore is a NoSQL database and does not have a SQL-like interface."
      },
      {
        "date": "2024-09-21T02:30:00.000Z",
        "voteCount": 2,
        "content": "A. Load data into Google BigQuery\n\nTo optimize the storage of the multi-petabyte data set for ease of analysis by business analysts who have experience only with using a SQL interface, you should load the data into Google BigQuery. BigQuery is a fully-managed, cloud-native data warehouse that allows you to perform fast SQL queries on large amounts of data. By loading the data into BigQuery, you can provide your business analysts with a familiar SQL interface for querying the data, making it easier for them to analyze the data set.\n\nOther options, such as inserting data into Google Cloud SQL, putting flat files into Google Cloud Storage, or streaming data into Google Cloud Datastore, may not provide the necessary SQL interface or query performance for efficient analysis of the data set."
      },
      {
        "date": "2024-04-21T14:34:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2024-02-27T00:20:00.000Z",
        "voteCount": 1,
        "content": "A is ok"
      },
      {
        "date": "2024-02-14T19:44:00.000Z",
        "voteCount": 1,
        "content": "A BigQuery formula is similar to SQL.\nB Google Cloud SQL cannot handle multiple petabyte data.\nD Google Cloud Datastore is NoSQL."
      },
      {
        "date": "2024-02-01T05:08:00.000Z",
        "voteCount": 1,
        "content": "A is right answer!\n\nGet Up-to-date: https://www.pinterest.com/pin/937522847419094382"
      },
      {
        "date": "2024-01-09T05:23:00.000Z",
        "voteCount": 2,
        "content": "BigQuery"
      },
      {
        "date": "2023-12-26T01:14:00.000Z",
        "voteCount": 2,
        "content": "B doesn\u2019t fit the bill as cloud SQL is good for data up to 30 TB. I would go with option A."
      },
      {
        "date": "2023-12-08T12:13:00.000Z",
        "voteCount": 1,
        "content": "I got with A\nBigQuery is a serverless, highly scalable data warehouse designed for analytics:\n\nHigh-performance querying: BigQuery allows large datasets to be queried quickly and efficiently, making it ideal for business analysts who need to analyze data frequently.\nSQL compatibility: BigQuery uses a standard SQL interface, allowing business analysts to leverage their existing SQL skills without needing to learn new tools or languages.\n24/7 availability: BigQuery offers 99.95% availability, ensuring that your data is accessible to your business analysts whenever they need it."
      },
      {
        "date": "2023-09-17T23:52:00.000Z",
        "voteCount": 2,
        "content": "BQ the correct tool"
      },
      {
        "date": "2023-08-15T02:04:00.000Z",
        "voteCount": 2,
        "content": "Multi petabyte and SQL interface =&gt; BigQuery"
      },
      {
        "date": "2023-02-22T20:49:00.000Z",
        "voteCount": 1,
        "content": "A is the correct one"
      },
      {
        "date": "2023-01-27T07:43:00.000Z",
        "voteCount": 2,
        "content": "A - The question states that the data set must be available 24hrs a day and that your business analysts have experience only with using a SQL interface.\nLoading data into Google BigQuery will allow your business analysts to access the data using a SQL interface. It will also allow the data to be available 24hrs a day."
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 3,
    "url": "https://www.examtopics.com/discussions/google/view/54378-exam-professional-cloud-architect-topic-1-question-3/",
    "body": "The operations manager asks you for a list of recommended practices that she should consider when migrating a J2EE application to the cloud.<br>Which three practices should you recommend? (Choose three.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPort the application code to run on Google App Engine",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIntegrate Cloud Dataflow into the application to capture real-time metrics",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstrument the application with a monitoring tool like Stackdriver Debugger\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSelect an automation framework to reliably provision the cloud infrastructure\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy a continuous integration tool with automated testing in a staging environment\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate from MySQL to a managed NoSQL database like Google Cloud Datastore or Bigtable"
    ],
    "answer": "CDE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "CDE",
        "count": 68,
        "isMostVoted": true
      },
      {
        "answer": "ADE",
        "count": 42,
        "isMostVoted": false
      },
      {
        "answer": "ACE",
        "count": 33,
        "isMostVoted": false
      },
      {
        "answer": "DEF",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-09-20T11:08:00.000Z",
        "voteCount": 60,
        "content": "This is talking about the APPLICATION not the infrastructure, therefore I believe we should focus on the APP-side of things:\n1. port the app to app engine for content delivery\n2. add monitoring for troubleshooting\n3. use a CI/CD workflow for continuous delivery w/testing for a stable application\n\nso, for me: A, C and E should be the answers"
      },
      {
        "date": "2023-03-16T02:10:00.000Z",
        "voteCount": 3,
        "content": "the person who asking you recommendation is operation manager, it can be related to infrastructure"
      },
      {
        "date": "2021-08-23T23:04:00.000Z",
        "voteCount": 12,
        "content": "Let's go with option elimination\nA. Port the application code to run on Google App Engine\n&gt;&gt; PaaS serverless managed service, so all my infra provisioning is taken care by GCP.\nB. Integrate Cloud Dataflow into the application to capture real-time metrics\n&gt;&gt; Good to have \nC. Instrument the application with a monitoring tool like Stackdriver Debugger\n&gt;&gt; Is a must for debugging issues and monitoring application logs this is now GCP Cloud monitoring and logging.\nD. Select an automation framework to reliably provision the cloud infrastructure\n&gt;&gt; App Engine is a PaaS so the infrastructure is taken care of by App Engine, I would select this if I have not selected A, hence will eliminate this option for now\nE. Deploy a continuous integration tool with automated testing in a staging environment\n&gt;&gt; Good to have \nF. Migrate from MySQL to a managed NoSQL database like Google Cloud Datastore or Bigtable\n&gt;&gt; There is no requirement for DB enhancement hence will elimination this option\n\nA and C are must-have \nB and E are Good to have, but E has more importance than Big\n\nHence will go with ACE"
      },
      {
        "date": "2024-09-21T02:34:00.000Z",
        "voteCount": 3,
        "content": "I chose ACE, but ADE make sense.\n\nA. Port the application code to run on Google App Engine.\nOk. It's a good practice use managed services when possible, we shouldn't worry about infrastructure.\n\nB. Integrate Cloud Dataflow into the application to capture real-time metrics.\nNo Ok. It's just a J2EE application, the question says nothin about a batch or stream pipeline or real-time in insight.\n\nC. Instrument the application with a monitoring tool like Stackdriver Debugger.\nNo Ok. App Engine already have natively logging and monitoring, we only have to enable debugger to fix some problem.\n\nD. Select an automation framework to reliably provision the cloud infrastructure.\nOk. It's a good practice use IaC (infrastructure as code).\n\nE. Deploy a continuous integration tool with automated testing in a staging environment.\nOk. It's a good practice use CI/CD and tests.\n\nF. Migrate from MySQL to a managed NoSQL database like Google Cloud Datastore or Bigtable.\nNo Ok. The question says nothin about Database."
      },
      {
        "date": "2024-09-21T02:34:00.000Z",
        "voteCount": 2,
        "content": "Absolutely different\uff1a\nB No need to use DataFlow \nF No need to use NOSQL.We should use CloudSQL.\nAbsolutely Correct:A\u3001E\nA First Step.\nI'm at a loss:C,D,E\nC It is microservices app best practice.App Engine is microservices app.\nAndIt is also written on this page.(Configuring your App with app.yaml)\nD This is Correct, but App Engine does it automatically.\nE Automatically test is a Java best practice."
      },
      {
        "date": "2024-09-21T02:32:00.000Z",
        "voteCount": 3,
        "content": "I voted ADE.\nA - App Engine could be used for J2EE application\nB - Dataflow is not used for real-time metrics\nC - Debugger is not used for monitoring\nD - Good practice\nE - Good practice\nF - Depends on the situation but I don't think Datastore or BigTable is a good choice for MySQL migration"
      },
      {
        "date": "2024-09-21T02:32:00.000Z",
        "voteCount": 3,
        "content": "I voted ADE.\nA - App Engine could be used for J2EE application\nB - Dataflow is not used for real-time metrics\nC - Debugger is not used for monitoring\nD - Good practice\nE - Good practice\nF - Depends on the situation but I don't think Datastore or BigTable is a good choice for MySQL migration"
      },
      {
        "date": "2024-09-21T02:32:00.000Z",
        "voteCount": 1,
        "content": "I think it is ACE\nA. Port the application code to run on Google App Engine - Correct - Best Practice to migrate J2EE app to APP engine.\nB. Integrate Cloud Dataflow into the application to capture real-time metrics - Incorrect - Cloud Data flow in not relevant in migrating existing J2EE app to cloud\nC. Instrument the application with a monitoring tool like Stackdriver Debugger - Correct - Only because it is relevant in this use case of migrating J2EE app to GAE have to 3 best answers. otherwise App engine is already enabled with Debugger by default, Nothing to do extra.\nD. Select an automation framework to reliably provision the cloud infrastructure - Incorrect - As APP enigne is managed service no requirement to automate provision of infrastructure.\nE. Deploy a continuous integration tool with automated testing in a staging environment - Correct - This is a best practice for using a cloud native CI/CD"
      },
      {
        "date": "2022-11-25T05:57:00.000Z",
        "voteCount": 1,
        "content": "C is not correct as Stackdriver Debugger is not the monitoring tool"
      },
      {
        "date": "2024-09-21T02:32:00.000Z",
        "voteCount": 5,
        "content": "D: Automation frameworks can help you reliably provision the necessary cloud infrastructure for your application, ensuring that the migration process is smooth and consistent.\n\nE: Continuous integration tools can help you automate the testing process, ensuring that your application is properly tested before it is deployed to the cloud. A staging environment can provide a separate testing environment that is isolated from the production environment, allowing you to test your application before it goes live.\n\nC: Monitoring tools like Stackdriver Debugger can help you identify and troubleshoot issues with your application after it is migrated to the cloud. This can help ensure that your application is running smoothly and efficiently in the cloud."
      },
      {
        "date": "2022-12-21T00:24:00.000Z",
        "voteCount": 1,
        "content": "Other practices, such as porting the application code to run on Google App Engine, integrating Cloud Dataflow into the application to capture real-time metrics, or migrating from MySQL to a managed NoSQL database like Google Cloud Datastore or Bigtable, may not be necessary for all J2EE applications and may depend on the specific requirements and goals of the migration."
      },
      {
        "date": "2024-09-21T02:31:00.000Z",
        "voteCount": 2,
        "content": "CDE\n\nThe three recommended practices you should recommend to the operations manager are:\n\nC. Instrument the application with a monitoring tool like Stackdriver Debugger: Monitoring the application's performance and health is crucial for identifying and resolving issues quickly. Stackdriver Debugger provides detailed insights into the application's behavior, helping diagnose performance bottlenecks and debug errors.\n\nD. Select an automation framework to reliably provision the cloud infrastructure: Automating infrastructure provisioning reduces manual effort and ensures consistent configuration across environments. This can be achieved using tools like Terraform or Ansible.\n\nE. Deploy a continuous integration tool with automated testing in a staging environment: Continuous integration and continuous delivery (CI/CD) pipelines automate the build, test, and deployment process, ensuring code changes are delivered reliably and with minimal downtime. Automated testing in a staging environment helps identify and fix regressions before they impact production."
      },
      {
        "date": "2024-09-20T11:08:00.000Z",
        "voteCount": 1,
        "content": "CDE\nC: Monitoring tools like Stackdriver (now Google Cloud Operations) help track the application's performance\nD: Automation frameworks (such as Terraform or Google Deployment Manager) for your environment is reproducible and can be scaled or modified with minimal manual intervention.\nE : is crucial to ensure that the application functions correctly after migration and during subsequent updates"
      },
      {
        "date": "2024-08-07T22:55:00.000Z",
        "voteCount": 1,
        "content": "Will choose CDE"
      },
      {
        "date": "2024-07-30T04:00:00.000Z",
        "voteCount": 1,
        "content": "App Engine for app deployment\nStackdriver for monitoring\nCI-CD for continuous integration"
      },
      {
        "date": "2024-06-27T05:17:00.000Z",
        "voteCount": 1,
        "content": "CDE is correct"
      },
      {
        "date": "2024-06-21T00:01:00.000Z",
        "voteCount": 1,
        "content": "Question: wouldn't porting/running the application on App engine take care of monitoring and health checks?\n\nCDE is good but\n\nC is manual process considering that monitoring and health checks can be taken care by APP engine itself\n\nSo maybe ADE\nAlso I searched \"J2EE application on google\" and all I could see was trail of App engine recommendations links."
      },
      {
        "date": "2024-05-19T00:13:00.000Z",
        "voteCount": 1,
        "content": "The answer is CDE"
      },
      {
        "date": "2024-05-14T00:21:00.000Z",
        "voteCount": 1,
        "content": "* 'Deploying a Java App' https://cloud.google.com/appengine/docs/legacy/standard/java/tools/uploadinganapp\n\n* 'Getting Started: Cloud SQL' https://cloud.google.com/appengine/docs/legacy/standard/java/building-app/cloud-sql"
      },
      {
        "date": "2024-04-28T06:53:00.000Z",
        "voteCount": 1,
        "content": "Definitely CDE"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 4,
    "url": "https://www.examtopics.com/discussions/google/view/7085-exam-professional-cloud-architect-topic-1-question-4/",
    "body": "A news feed web service has the following code running on Google App Engine. During peak load, users report that they can see news articles they already viewed.<br>What is the most likely cause of this problem?<br><img src=\"/assets/media/exam-media/04339/0007100001.png\" class=\"in-exam-image\"><br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe session variable is local to just a single instance\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe session variable is being overwritten in Cloud Datastore",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe URL of the API needs to be modified to prevent caching",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe HTTP Expires header needs to be set to -1 stop caching"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-09-20T11:11:00.000Z",
        "voteCount": 112,
        "content": "It's A. AppEngine spins up new containers automatically according to the load. During peak traffic, HTTP requests originated by the same user could be served by different containers. Given that the variable `sessions` is recreated for each container, it might store different data.\nThe problem here is that this Flask app is stateful. The `sessions` variable is the state of this app. And stateful variables in AppEngine / Cloud Run / Cloud Functions are problematic.\nA solution would be to store the session in some database (e.g. Firestore, Memorystore) and retrieve it from there. This way the app would fetch the session from a single place and would be stateless."
      },
      {
        "date": "2022-12-12T23:07:00.000Z",
        "voteCount": 14,
        "content": "Very well stated, jack. I just wanted to point, GAE is a webserver platform anyway, so making application stateless or stateful is up to the developer and has nothing to do with GAE. The issue is about session consistency. GAE spin new container if there's a need, and based on the code, the session is stored locally, this means, there's no consistency between container, and there's no grantee that the same container might serve the same user. Thank you Jack, very good explanation"
      },
      {
        "date": "2019-11-14T07:39:00.000Z",
        "voteCount": 29,
        "content": "A is correct"
      },
      {
        "date": "2024-09-21T02:34:00.000Z",
        "voteCount": 4,
        "content": "A\nThe most likely cause of the issue described in the code is that the session variable is local to just a single instance. In this code, the session variable is defined as a local dictionary within the Flask application. This means that it is not shared across different instances of the application and will not be persisted between requests. As a result, when the application is running on multiple instances, each instance will have its own local copy of the session variable, and users may see news articles that they have already viewed on other instances."
      },
      {
        "date": "2022-12-21T00:36:00.000Z",
        "voteCount": 2,
        "content": "To fix this issue, you could consider using a persistent storage solution, such as Cloud Datastore or Cloud SQL, to store the session data in a way that is shared across all instances of the application. This would allow you to maintain a consistent view of the session data for each user across all instances of the application.\n\nOther potential causes for this issue, such as modifying the URL of the API to prevent caching or setting the HTTP Expires header to -1 to stop caching, are not related to the issue described in the code and would not likely address the problem."
      },
      {
        "date": "2024-09-21T02:34:00.000Z",
        "voteCount": 8,
        "content": "The most likely cause of the reported issue is that the session variable is local to just a single instance.\n\nIn the code provided, the sessions variable is a dictionary that stores the viewed news articles for each user. However, this variable is only stored in memory on the instance that handles the request, and it is not shared between instances. Therefore, when a new request is handled by a different instance, it will not have access to the same session data, and the user may see previously viewed news articles.\n\nTo solve this problem, a shared session management system should be used that can be accessed by all instances. Google App Engine provides a few options for session management, such as using Memcache or Cloud Datastore to store the session data. By using a shared session management system, all instances can access the same session data, and users will not see previously viewed news articles."
      },
      {
        "date": "2024-09-15T22:21:00.000Z",
        "voteCount": 1,
        "content": "A.The sessions dictionary is used to store user-specific data, such as which news articles have been viewed. This dictionary is created as an in-memory variable within the Flask app.\nIn a cloud environment, like Google App Engine, the application may be running on multiple instances, especially during peak loads. Since the sessions dictionary is stored in memory, it is local to each instance.\nThis means that if a user is routed to a different instance (due to load balancing), their session data will not be available on that new instance, causing the application to serve news articles they\u2019ve already seen."
      },
      {
        "date": "2023-01-27T07:38:00.000Z",
        "voteCount": 1,
        "content": "A is the right answer"
      },
      {
        "date": "2022-12-30T19:50:00.000Z",
        "voteCount": 3,
        "content": "Where in the code does it show that the session variable is local to just a single instance?"
      },
      {
        "date": "2022-12-20T06:34:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2022-11-26T04:42:00.000Z",
        "voteCount": 1,
        "content": "stateful variable should be in firestore (redis)."
      },
      {
        "date": "2022-11-12T09:08:00.000Z",
        "voteCount": 1,
        "content": "I agree with ackdbd"
      },
      {
        "date": "2022-10-22T09:11:00.000Z",
        "voteCount": 1,
        "content": "A is the correct answer app becoming stateful and it should not be sin case of app engine, cloud run and functions"
      },
      {
        "date": "2022-10-12T06:28:00.000Z",
        "voteCount": 1,
        "content": "A. The session variable is local to just a single instance\nThe others are not relevant"
      },
      {
        "date": "2022-07-30T13:35:00.000Z",
        "voteCount": 2,
        "content": "Thank you that was nice explanation"
      },
      {
        "date": "2022-07-23T10:55:00.000Z",
        "voteCount": 2,
        "content": "Where was this presented in the GCP Architecture training &amp; labs?"
      },
      {
        "date": "2022-07-22T18:42:00.000Z",
        "voteCount": 2,
        "content": "vote A\n- rule out C,D not thing to do with problem\n- rule out B, Q is not mention datastore"
      },
      {
        "date": "2022-06-27T01:45:00.000Z",
        "voteCount": 1,
        "content": "A of course, it's just a code pb here"
      },
      {
        "date": "2022-05-27T14:00:00.000Z",
        "voteCount": 1,
        "content": "A seems correct"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 5,
    "url": "https://www.examtopics.com/discussions/google/view/6837-exam-professional-cloud-architect-topic-1-question-5/",
    "body": "An application development team believes their current logging tool will not meet their needs for their new cloud-based product. They want a better tool to capture errors and help them analyze their historical log data. You want to help them find a solution that meets their needs.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDirect them to download and install the Google StackDriver logging agent",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSend them a list of online resources about logging best practices",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tHelp them define their requirements and assess viable logging tools\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tHelp them upgrade their current tool to take advantage of any new features"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 54,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 45,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-04-26T23:34:00.000Z",
        "voteCount": 110,
        "content": "A. This is GCP exam. They will always promote their services. Not a third party solution."
      },
      {
        "date": "2020-06-01T09:30:00.000Z",
        "voteCount": 11,
        "content": "Remember that agent is only required for non cloud based resources. The question is saying their cloud based...  feel C meets this need"
      },
      {
        "date": "2022-01-18T09:06:00.000Z",
        "voteCount": 4,
        "content": "logging agent is required  for compute engine too."
      },
      {
        "date": "2021-06-22T22:05:00.000Z",
        "voteCount": 4,
        "content": "It's given as 'cloud based resource' but didn't mention if it is 'GCP'. It could be any cloud provider. So Stackdriver might be the answer."
      },
      {
        "date": "2024-04-09T13:07:00.000Z",
        "voteCount": 4,
        "content": "Is an Architect exam... the team THINKS that the tool WILL NOT MEET requirements... you should help to undesntend what they need... Is C... it not says that you will offert third party solution"
      },
      {
        "date": "2020-12-05T12:55:00.000Z",
        "voteCount": 6,
        "content": "Totally Agree. offering Stackdriver Logging is what they want from a GCA. answer is A."
      },
      {
        "date": "2022-10-16T13:48:00.000Z",
        "voteCount": 1,
        "content": "Agreed"
      },
      {
        "date": "2019-10-20T04:19:00.000Z",
        "voteCount": 58,
        "content": "C should be the correct answer here"
      },
      {
        "date": "2024-09-21T02:36:00.000Z",
        "voteCount": 4,
        "content": "Review the logging requirements and use existing logging utility. is not right.\n\nYou know the requirements as you have found the current logging tool will not meet the needs for the new cloud-based product.\n\nInstall Google Cloud logging agent on all VMs. is the right answer.\n\nThe Logging agent streams logs from common third-party applications and system software to Logging. It is a best practice to run the Logging agent on all your VM instances. The agent runs under both Linux and Windows. The Logging agent is compatible with both GCP Compute Engine instances as well as AWS EC2 instances. Google, through its partners, provides logging services for on-premise and hybrid cloud platforms consistently and predictably.\n\nRef: https://cloud.google.com/logging/docs/agent"
      },
      {
        "date": "2020-12-11T08:37:00.000Z",
        "voteCount": 1,
        "content": "Why do you think they are using VM on GCE?"
      },
      {
        "date": "2021-09-19T08:02:00.000Z",
        "voteCount": 4,
        "content": "This is wrong. This certification is meant to prepare you to be a cloud architect (focusing on GCP). This does not mean that you are going to recommend everything that has google on its name. You need to understand client requirements first."
      },
      {
        "date": "2022-02-03T02:55:00.000Z",
        "voteCount": 5,
        "content": "Actually it does. A lot of questions guide you to the, in best case, fully managed GCP services. They should rename the certificate to \"Professional Google Cloud Sales Person\"."
      },
      {
        "date": "2020-08-05T03:27:00.000Z",
        "voteCount": 10,
        "content": "C is ok"
      },
      {
        "date": "2020-08-14T01:47:00.000Z",
        "voteCount": 15,
        "content": "I would love to choose B, but need to keep my job.."
      },
      {
        "date": "2021-03-19T07:20:00.000Z",
        "voteCount": 2,
        "content": "What all viable logging you would suggest in this scenario! Cloud operations suite has everything.. cloud logging helps with many thing..In my mind this question is not meant for being a perfectionist but what would mostly work while option C is an approach that we will take questions ask about tool. Hence selecting A make sense to me. Thoughts!"
      },
      {
        "date": "2021-03-24T05:14:00.000Z",
        "voteCount": 7,
        "content": "I'm surprised, @MeasService.\nI guess you had created the question an sugested ans. A. \nThen you wrote \"C should be the correct answer here\".\nDo you change your mind ?"
      },
      {
        "date": "2022-12-23T13:22:00.000Z",
        "voteCount": 1,
        "content": "The Stackderiver agent currently known as Ops agent is the primary agent for collecting telemetry from your Compute Engine instances. It needs to be installed on GCP services such as GCE instances in order to collect logs from those instances and send them to cloud logging and monitoring. https://cloud.google.com/stackdriver/docs/solutions/agents/ops-agent/installation."
      },
      {
        "date": "2024-09-21T02:40:00.000Z",
        "voteCount": 2,
        "content": "The key to the answer is this: \"You want to help them find a solution that meets their needs.\"\n\nYou only start to find solution after you know the needs.\nHence, this implied that you have already done the \"Help them define their requirements and assess viable logging tools\" (This rules out C)\n\nB is a lazy way do things and did not address the needs. (You are suppose to do the work, not ask them to do it themselves)\n\nD - \"Help them upgrade their current tool to take advantage of any new features\", well, this is a solution, but it is costly and it is not a business functions/features that worth the resources (you would go for COTS) \n\nSo, left with A...."
      },
      {
        "date": "2024-09-21T02:37:00.000Z",
        "voteCount": 2,
        "content": "I've seen a lot of folks chosen C with a good argument of \"requirements before solution\". While I do agree with that argument. My choice would still be A.\n \nReason being: \n(c.f: https://cloud.google.com/products/operations?hl=en)\nNote: Stackdriver was the former name, it's now Google Cloud's Operation Suite, and it works both \"inside and outside\" of Google Cloud.\n\nTherefore, if this new \"cloud-based product\" is on AWS or some other cloud provider, it would still work. \nFinally I saw a comment where someone said Google \"will always promote their services\". This is not true, especially for the Architect Exam."
      },
      {
        "date": "2024-09-21T02:37:00.000Z",
        "voteCount": 1,
        "content": "C. Help them define their requirements and assess viable logging tools.\n\nThis approach is strategic because it starts with understanding the specific needs of the development team, which can vary widely depending on the nature of the cloud-based product they are working on. Here\u2019s why this is the most suitable option:\n\nRequirement Analysis: By defining the exact requirements, you ensure that the selected logging tool will have the necessary features to capture errors and analyze historical log data effectively.\n\nTool Assessment: Once the requirements are clear, you can perform an informed assessment of available logging tools that meet these criteria, which could include Google StackDriver, other cloud-native tools, or third-party solutions.\n\nLong-term Solution: Choosing the right tool based on precise requirements is likely to offer a long-term solution rather than a temporary fix."
      },
      {
        "date": "2024-06-21T09:57:00.000Z",
        "voteCount": 1,
        "content": "C. Help them define their requirements and assess viable logging tools\n\nHere's why the other options are less suitable:\n\nA. Downloading Stackdriver Logging Agent: While Stackdriver Logging Agent is a component of Cloud Logging, it's just the agent for sending logs. They need a broader solution that addresses their needs and evaluates alternatives.\nB. Logging Best Practices: Best practices are helpful, but they don't directly address finding the right tool for their specific needs.\nD. Upgrading Current Tool: Upgrading might be an option, but it doesn't consider if a better tool exists that could be a better fit.\nBy helping them define their requirements (error capturing, historical data analysis) and assess various logging tools (including Cloud Logging with Stackdriver), you provide a more comprehensive solution."
      },
      {
        "date": "2024-06-09T12:38:00.000Z",
        "voteCount": 1,
        "content": "All who says C, better not go to GCP. \n\nIt is A, stackdriver agent is GCP Agent. Are you studying for GCP or what?"
      },
      {
        "date": "2024-06-03T05:21:00.000Z",
        "voteCount": 1,
        "content": "Maybe, at the end of a requirement analysis, the solution might be A, but a tool hardly represents a solution to a business concern."
      },
      {
        "date": "2024-05-19T00:21:00.000Z",
        "voteCount": 2,
        "content": "It's C. Option A makes assumptions, that is not what an architect does. DOn't get baited by the that it's a GCP exam and therefore needs to be a GCP product. But most importantly, an agent is for deploying on compute instances. The solution might not even use VMs so A is 100% the wrong answer based on the info we have."
      },
      {
        "date": "2024-04-09T13:07:00.000Z",
        "voteCount": 1,
        "content": "first we need to know tghe needs"
      },
      {
        "date": "2024-02-17T00:58:00.000Z",
        "voteCount": 1,
        "content": "Solution should meet their needs"
      },
      {
        "date": "2024-02-16T23:34:00.000Z",
        "voteCount": 1,
        "content": "There is cost of IOPS ,  data storage and compute cost of querying of historical data  , if I were an organization that decided to implement an open source solution in GCP I would incur these costs anyways plus developer cost for implementation. I would rather go with whats already there . Of course writing data to a cheaper cloud provider on regular intervals via a message Q could be another option but then again that would  incur some extra network costs as well. Since they require an immediate solution I would go with A."
      },
      {
        "date": "2024-02-01T05:13:00.000Z",
        "voteCount": 1,
        "content": "C is right answer!\n\nGet Up-to-date: https://www.pinterest.com/pin/937522847419094382"
      },
      {
        "date": "2024-01-20T09:20:00.000Z",
        "voteCount": 1,
        "content": "It is not mentioned that Team is using GCP but some cloud provider. Therefore without understanding requirements and current gaps in tooling we cannot define the solution. I feel correct answer is C"
      },
      {
        "date": "2024-01-11T01:19:00.000Z",
        "voteCount": 1,
        "content": "c is the best response here"
      },
      {
        "date": "2024-01-01T09:43:00.000Z",
        "voteCount": 1,
        "content": "A is too specific (for GCE) considering limit solution details provided, so for an 'architect' exam C makes more sense."
      },
      {
        "date": "2023-12-25T20:09:00.000Z",
        "voteCount": 2,
        "content": "Only reason A cannot be the answer is because of the need for Historical Log Data. The logs are captured by stackdriver agent from the point of installation and the  requirement clearly says it needs historical logs. Also stackdriver retains logs for 3 months. You need another BQ based solution to retain historical logs older than 3 months. \n\nRequirements are need to be analyzed to gather answers for above. So \"C\" is the answer."
      },
      {
        "date": "2024-04-26T11:47:00.000Z",
        "voteCount": 1,
        "content": "their NEW cloud-based product."
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 6,
    "url": "https://www.examtopics.com/discussions/google/view/54383-exam-professional-cloud-architect-topic-1-question-6/",
    "body": "You need to reduce the number of unplanned rollbacks of erroneous production deployments in your company's web hosting platform. Improvement to the QA/<br>Test processes accomplished an 80% reduction.<br>Which additional two approaches can you take to further reduce the rollbacks? (Choose two.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIntroduce a green-blue deployment model\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReplace the QA environment with canary releases",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFragment the monolithic platform into microservices\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReduce the platform's dependency on relational database systems",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReplace the platform's relational database systems with a NoSQL database"
    ],
    "answer": "AC",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AC",
        "count": 34,
        "isMostVoted": true
      },
      {
        "answer": "BC",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-09-04T00:31:00.000Z",
        "voteCount": 64,
        "content": "D) and E) are pointless in this context.\nC) is certainly a good practice.\nNow between A) and B)\nA) Blue green deployment is an application release model that gradually transfers user traffic from a previous version of an app or microservice to a nearly identical new release\u2014both of which are running in production. \nc) In software, a canary process is usually the first instance that receives live production traffic about a new configuration update, either a binary or configuration rollout. The new release only goes to the canary at first. The fact that the canary handles real user traffic is key: if it breaks, real users get affected, so canarying should be the first step in your deployment process, as opposed to the last step in testing in production. \"\nWhile both green-blue and canary releases are useful, B) suggests \"replacing QA\" with canary releases - which is not good. QA got the issue down by 80%. Hence A) and C)"
      },
      {
        "date": "2021-06-04T04:42:00.000Z",
        "voteCount": 34,
        "content": "A &amp; C for me"
      },
      {
        "date": "2024-06-19T19:19:00.000Z",
        "voteCount": 1,
        "content": "B &amp; C see should be the correct answer.  There is no green-blue deployment but rather a blue green."
      },
      {
        "date": "2024-06-11T10:54:00.000Z",
        "voteCount": 1,
        "content": "C is also dumb even if it is a good answer but the question never specifies whether it is a monolithic platform or not. I hate tests because of this kind of incomplete context."
      },
      {
        "date": "2024-06-03T05:27:00.000Z",
        "voteCount": 1,
        "content": "B. Replace the QA environment with canary releases\ncanary release is not a replacement for a QA environment.\n    D. Reduce the platform's dependency on relational database systems\n    E. Replace the platform's relational database systems with a NoSQL database\na relational database system is not, as it is, an obstacle to improving the deployment success of the application.\nThen, in my opinion, AC is the correct answer."
      },
      {
        "date": "2024-02-20T04:59:00.000Z",
        "voteCount": 2,
        "content": "A &amp; C is the correct answer"
      },
      {
        "date": "2024-01-20T04:56:00.000Z",
        "voteCount": 1,
        "content": "A and C, the description given by JustJack21 is all you need."
      },
      {
        "date": "2024-01-11T01:26:00.000Z",
        "voteCount": 2,
        "content": "A and C"
      },
      {
        "date": "2023-12-23T05:15:00.000Z",
        "voteCount": 1,
        "content": "D &amp; F for me"
      },
      {
        "date": "2023-12-02T14:23:00.000Z",
        "voteCount": 1,
        "content": "A is the only answer.\nC is a general improvement but does not guarantee the reduction of rollbacks due to quality failures if programming errors remain.\nB, canary only makes sense in PRO. The statement is ambiguous. In any case, if what we want is to reduce the current situation, it does not seem convenient to remove what now helps."
      },
      {
        "date": "2023-10-03T21:16:00.000Z",
        "voteCount": 3,
        "content": "B. Replace the QA environment with canary releases\nC. Fragment the monolithic platform into microservices\n\nAs splitting monolithinc application in Microservices means that code and dependencies are bundled together and DEV, TEST, QA and PROD will have same docker image. Replacing QA environment with Canary will ensure testing the final code with sub-set of users before Go-Live. There is no RollBack and no downtime. Even if testing with sub-set users fails that previous PROD deployment will continue to serve traffic. However in case of Blue-Green deployment, you will have Current PROD code and new prod code, In case new code fails post deployment, it has to be rolled-back to working code.\n\nand the ask is to reduce or eliminate Rollback."
      },
      {
        "date": "2023-10-03T19:07:00.000Z",
        "voteCount": 1,
        "content": "B. Replace the QA environment with canary releases\nC. Fragment the monolithic platform into microservices\n\nAs splitting monolithinc application in Microservices means that code and dependencies are bundled together and DEV, TEST, QA and PROD will have same docker image. Also canary release will ensure testing the final code with sub-set of users before Go-Live. Which will reduce your rollbacks. (in Blue-Green deployment, we are actually making ourself ready for rollback in case things go wrong)."
      },
      {
        "date": "2023-09-23T08:52:00.000Z",
        "voteCount": 1,
        "content": "Use Blue-Green to reduce rollback. Check this blog (https://circleci.com/blog/canary-vs-blue-green-downtime/#:~:text=In%20blue%2Dgreen%20deployment%20you,first%2C%20before%20finishing%20the%20others.): Using your load balancers to direct traffic keeps your blue environment running seamlessly for production users while you test and deploy to your green environment. When your deployment and testing are successful, you can switch your load balancer to target your green environment with no perceptible change for your users.\nWhen testing in Green environment, you don't perform rollback if test failed in Green."
      },
      {
        "date": "2023-09-18T00:01:00.000Z",
        "voteCount": 2,
        "content": "A to validate your deployment and C to ensure that errors do not cascade across the process"
      },
      {
        "date": "2023-08-22T09:25:00.000Z",
        "voteCount": 1,
        "content": "A&amp;C is correct!"
      },
      {
        "date": "2023-03-20T03:18:00.000Z",
        "voteCount": 1,
        "content": "A &amp; C , because canery is usefull while doing the testing and once satisfied then only roll out otherwise roll back."
      },
      {
        "date": "2023-01-16T21:45:00.000Z",
        "voteCount": 1,
        "content": "A and C; I don't know what canary release is. Canary is also kind of deployment method, and what do you mean canary release. This methodology will help you accerlerate your deploying many many small new features. Blue/Green would drop old deployment and create new ones, and it costs much more than other deployments. I think all the stackholder  would take care of it much seriously.  Microservice is for seperating a whole function to many small ones. Every team just takes care of the small ones, I think it will help make much less deployment rollback."
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 7,
    "url": "https://www.examtopics.com/discussions/google/view/54304-exam-professional-cloud-architect-topic-1-question-7/",
    "body": "To reduce costs, the Director of Engineering has required all developers to move their development infrastructure resources from on-premises virtual machines<br>(VMs) to Google Cloud Platform. These resources go through multiple start/stop events during the day and require state to persist. You have been asked to design the process of running a development environment in Google Cloud while providing cost visibility to the finance department.<br>Which two steps should you take? (Choose two.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the - -no-auto-delete flag on all persistent disks and stop the VM\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the - -auto-delete flag on all persistent disks and terminate the VM",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tApply VM CPU utilization label and include it in the BigQuery billing export",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Google BigQuery billing export and labels to associate cost to groups\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore all state into local SSD, snapshot the persistent disks, and terminate the VM",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore all state in Google Cloud Storage, snapshot the persistent disks, and terminate the VM"
    ],
    "answer": "AD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AD",
        "count": 21,
        "isMostVoted": true
      },
      {
        "answer": "DF",
        "count": 8,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "DE",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "BD",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-09-20T11:14:00.000Z",
        "voteCount": 74,
        "content": "I spent all morning researching this question. I just popped over and took the GCP Practice exam on Google's website and guess what... this question was on it word for word, but it had slightly different answers, but not by much here is what I learned.  The correct answer is 100% A / D and here is why.  On the sample question, the \"F\" option is gone.   \"A\" is there but slightly reworked, it now says: \"Use persistent disks to store the state. Start and stop the VM as needed\" which makes much more sense.  The practice exam says A and D are correct. Given the wording of this question, if A and B, where there then both would be correct because of the word \"persistent\" and not because of the flag. The \"no-auto-delete\" makes A slightly safer than B, but it is the \"persistent disk\" that makes them right, not the flag.  Hope that helps! F is not right because that is a complex way of solving the issue that by choosing Persistent Disk solves it up front. HTH"
      },
      {
        "date": "2022-08-11T09:41:00.000Z",
        "voteCount": 4,
        "content": "(A) is not sense because the flag is to preserve disk when the istances was deleted, when the istances was stopped the data on persistend disk are not deleted. So good to  know that the response was reworked\n(B) is wrong because only on AWS you can  terminate istances. On GCP the \"terminate\" action do not exist ."
      },
      {
        "date": "2021-06-03T02:29:00.000Z",
        "voteCount": 22,
        "content": "A and D looks correct as per https://cloud.google.com/sdk/gcloud/reference/compute/instances/set-disk-auto-delete#--auto-delete ;\nhttps://cloud.google.com/billing/docs/how-to/export-data-bigquery"
      },
      {
        "date": "2021-07-09T19:39:00.000Z",
        "voteCount": 3,
        "content": "-no-auto-delete flag does not have effect on the state of the application. I believe D and F are correct ANS, https://cloud.google.com/compute/docs/instances/stop-start-instance"
      },
      {
        "date": "2024-08-07T23:16:00.000Z",
        "voteCount": 1,
        "content": "Choose A and D"
      },
      {
        "date": "2024-06-11T11:17:00.000Z",
        "voteCount": 1,
        "content": "Another confusing question because I took the \"these machines go through multiple stop/starts during the day\" as a part of the migration, not as a part of daily functionality, so none of the answers other than D made much sense to me. People need to word the questions better on tests and give more than enough context or people like me are going to get confused, second guess our answers, and fail."
      },
      {
        "date": "2024-05-10T13:49:00.000Z",
        "voteCount": 1,
        "content": "F is too complex solution to solve this problem.\nE local SSD does not persist on termination of vm so this is also a wrong option\nA, B suggest persistent disk but I think A makes better sense.\n\nSo my answer is A and D"
      },
      {
        "date": "2024-04-21T02:27:00.000Z",
        "voteCount": 1,
        "content": "E wrong:\nScenarios where Compute Engine does not persist Local SSD data\n\nData on Local SSD disks does not persist through the following events:\n\nIf you shut down the guest operating system and force the VM to stop."
      },
      {
        "date": "2024-03-19T21:55:00.000Z",
        "voteCount": 1,
        "content": "Hi all, i am trying to view questions from 135.But i cannot access the page as it is asking for contributer access.Is it same for everyone?"
      },
      {
        "date": "2024-02-02T01:38:00.000Z",
        "voteCount": 2,
        "content": "From the GCP Practice exam... A and D"
      },
      {
        "date": "2024-01-20T09:54:00.000Z",
        "voteCount": 1,
        "content": "Based on documentation A is correct https://cloud.google.com/sdk/gcloud/reference/compute/instances/set-disk-auto-delete\nAlso the documentation clearly states that this flag will be help retain the state when VM is started/stopped.\nhttps://cloud.google.com/blog/products/storage-data-transfer/save-money-by-stopping-and-starting-compute-engine-instances-on-schedule\nFor cost visibility option D is correct."
      },
      {
        "date": "2024-01-11T01:40:00.000Z",
        "voteCount": 1,
        "content": "D. Use Google BigQuery billing export and labels to associate cost to groups: This offers granular cost visibility across various development teams or projects through BigQuery data analysis. Combining labels with billing export allows you to associate resource consumption with specific groups, enabling chargeback mechanisms and fostering cost accountability.\nE. Store all state into local SSD, snapshot the persistent disks, and terminate the VM: This option minimizes ongoing cost by utilizing low-cost local SSD for active state during runtime and terminating the VM when development isn't ongoing. Snapshots offer quick restoration back to the latest state without incurring persistent disk charges during downtime."
      },
      {
        "date": "2023-12-31T09:58:00.000Z",
        "voteCount": 1,
        "content": "DF. This is the question in google practice test."
      },
      {
        "date": "2023-12-23T05:16:00.000Z",
        "voteCount": 1,
        "content": "D &amp; F for me"
      },
      {
        "date": "2023-10-08T20:36:00.000Z",
        "voteCount": 1,
        "content": "How can D ever be right coz it's Bigquery Billing Export and question is about VM billing"
      },
      {
        "date": "2023-11-13T04:32:00.000Z",
        "voteCount": 1,
        "content": "This means you can export the billing to the BQ and then do analysis."
      },
      {
        "date": "2023-10-03T21:33:00.000Z",
        "voteCount": 4,
        "content": "A is correct, Use of persistent disk mean the data is preserved even after restart. -np-auto-delete on persistent disk means pesistent disk won't be deleted when VM is deleted.\n\nD is correct, becuase second part of question asks for billing report to finanace department and Label and BQ helps in cost analysis."
      },
      {
        "date": "2023-08-06T19:57:00.000Z",
        "voteCount": 1,
        "content": "B and D. A is wrong because,  (--no-auto-delete) would lead to extra storage costs for the disks even when VMs are not running."
      },
      {
        "date": "2023-03-28T00:47:00.000Z",
        "voteCount": 3,
        "content": "same question, official option:\nA. Use persistent disks to store the state. Start and stop the VM as needed.\nB. Use the \"gcloud --auto-delete\" flag on all persistent disks before stopping the VM.\nC. Apply VM CPU utilization label and include it in the BigQuery billing export.\nD. Use BigQuery billing export and labels to relate cost to groups.\nE. Store all state in a Local SSD, snapshot the persistent disks, and terminate the VM.\n\nThis question will not be tested, no need to read"
      },
      {
        "date": "2023-03-24T00:45:00.000Z",
        "voteCount": 3,
        "content": "AD \nUse the flag -no-auto-delete with this flag, the disk won't be delete when the VM is terminated.\n\nBilling export to BigQuery enables you to export your daily usage and cost estimates automatically throughout the day to a BigQuery dataset you specify. You can then access your billing data from BigQuery."
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 8,
    "url": "https://www.examtopics.com/discussions/google/view/7126-exam-professional-cloud-architect-topic-1-question-8/",
    "body": "Your company wants to track whether someone is present in a meeting room reserved for a scheduled meeting. There are 1000 meeting rooms across 5 offices on 3 continents. Each room is equipped with a motion sensor that reports its status every second. The data from the motion detector includes only a sensor ID and several different discrete items of information. Analysts will use this data, together with information about account owners and office locations.<br>Which database type should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFlat file",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNoSQL\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRelational",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBlobstore"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 15,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-05-09T16:03:00.000Z",
        "voteCount": 34,
        "content": "I'll go with B.\n\nThis is time series data.  We also have no idea what kinds of data are being captured so it doesn't appear structurd.\n\nA does not seem reasonable because a flat file is not easy to query and analyze.\nB seems reasonable because this accommodates unstructured data.\nC seems unreasonable because we have no idea on the structure of the data.\nD seems unreasonable beacause there is no such Google database type."
      },
      {
        "date": "2024-01-20T05:15:00.000Z",
        "voteCount": 2,
        "content": "With frequencies of data (per second), the best case would be using pub/sub and NoSQL. Relational DB/BlobStore/FlatFile are not good for Near realtime data."
      },
      {
        "date": "2024-01-11T02:11:00.000Z",
        "voteCount": 1,
        "content": "C. Relational database.\n\nHere's why:\n\nScalability: A relational database can handle the data volume from 1000 sensors reporting every second effectively.\nStructure: It provides a well-defined schema for organizing data like sensor ID, timestamp, motion status, account owner, and office location, making it easily queryable and understandable for analysts.\nRelationships: It allows establishing relationships between tables, such as linking sensor data to specific meeting rooms and their corresponding owners and locations. This facilitates analyses involving multiple data sources.\nFlexibility: Relational databases offer flexibility for expanding data collection beyond motion sensors in the future to include other sensor types or meeting room details."
      },
      {
        "date": "2023-10-15T22:30:00.000Z",
        "voteCount": 1,
        "content": "B, It is"
      },
      {
        "date": "2023-10-03T21:35:00.000Z",
        "voteCount": 1,
        "content": "Unstructured realtime aata"
      },
      {
        "date": "2023-09-18T00:06:00.000Z",
        "voteCount": 1,
        "content": "b [ You need seperate fields and keys -- you do not need to relate them"
      },
      {
        "date": "2023-08-22T09:30:00.000Z",
        "voteCount": 1,
        "content": "NOSQL DB's are meant for these kind of workloads"
      },
      {
        "date": "2023-06-21T09:55:00.000Z",
        "voteCount": 2,
        "content": "The requirement to join the data to other data sets implies RDBMS.\nBigQuery can handle 1GB/s when streaming inserts, I doubt these 1000 sensors will send that much data.\n\nBigtable seems over the top and not able to fulfil all the requirements."
      },
      {
        "date": "2023-02-23T10:58:00.000Z",
        "voteCount": 3,
        "content": "This will be time-series data. The best DB would be a Big Table (also sensorID can be used in the row key for faster retrieval of data)"
      },
      {
        "date": "2022-12-31T01:20:00.000Z",
        "voteCount": 1,
        "content": "B for me"
      },
      {
        "date": "2022-12-24T22:24:00.000Z",
        "voteCount": 1,
        "content": "B is right"
      },
      {
        "date": "2022-11-26T06:10:00.000Z",
        "voteCount": 1,
        "content": "B (No SQL should be the right answer)"
      },
      {
        "date": "2022-10-16T20:17:00.000Z",
        "voteCount": 2,
        "content": "surprised by the options given, this is a great use case of Bigtable so NoSQL"
      },
      {
        "date": "2022-10-16T13:55:00.000Z",
        "voteCount": 1,
        "content": "B is right"
      },
      {
        "date": "2022-10-12T07:20:00.000Z",
        "voteCount": 1,
        "content": "B. NoSQL - unstructured data"
      },
      {
        "date": "2022-06-17T03:36:00.000Z",
        "voteCount": 1,
        "content": "Option B - NO SQL (Unstructured)"
      },
      {
        "date": "2022-04-19T07:50:00.000Z",
        "voteCount": 1,
        "content": "the data is not structed. or at least in a too simple way, (1 table). So No SQL is a good option."
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 9,
    "url": "https://www.examtopics.com/discussions/google/view/7130-exam-professional-cloud-architect-topic-1-question-9/",
    "body": "You set up an autoscaling instance group to serve web traffic for an upcoming launch. After configuring the instance group as a backend service to an HTTP(S) load balancer, you notice that virtual machine (VM) instances are being terminated and re-launched every minute. The instances do not have a public IP address.<br>You have verified the appropriate web response is coming from each instance using the curl command. You want to ensure the backend is configured correctly.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure that a firewall rules exists to allow source traffic on HTTP/HTTPS to reach the load balancer.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAssign a public IP to each instance and configure a firewall rule to allow the load balancer to reach the instance public IP.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure that a firewall rule exists to allow load balancer health checks to reach the instances in the instance group.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a tag on each instance with the name of the load balancer. Configure a firewall rule with the name of the load balancer as the source and the instance tag as the destination."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-09-20T11:14:00.000Z",
        "voteCount": 35,
        "content": "\"A\" and \"B\" wouldn't turn the VMs on or off, it would jsut prevent traffic. \"C\" would turn them off if the health check is configured to terminate the VM is it fails. \"D\" is the start of a pseudo health check without any logic, so it also isn't an answer because it is like \"A\" and \"B\". Correct Answer: \"C\""
      },
      {
        "date": "2020-08-05T03:47:00.000Z",
        "voteCount": 14,
        "content": "C is ok"
      },
      {
        "date": "2021-03-03T23:42:00.000Z",
        "voteCount": 6,
        "content": "C because terminated and relaunch.... something wrong with HC."
      },
      {
        "date": "2022-10-16T13:58:00.000Z",
        "voteCount": 2,
        "content": "agreed and C is right"
      },
      {
        "date": "2021-11-18T18:30:00.000Z",
        "voteCount": 7,
        "content": "A. Ensure that a firewall rules exists to allow source traffic on HTTP/HTTPS to reach the load balancer. &gt;&gt; not correct, load balancer is not the issue here.\nB. Assign a public IP to each instance and configure a firewall rule to allow the load balancer to reach the instance public IP. &gt;&gt;  defeats the purpose of getting load balancers , not correct\nC. Ensure that a firewall rule exists to allow load balancer health checks to reach the instances in the instance group.&gt;&gt; Correct. if using different port then appropriate FW rule need to be setup to ensure LB can reach backend instances for healthcheck. if healthcheck traffic is blcked, instances will be marked unhealthy and will be restarted.\nD. Create a tag on each instance with the name of the load balancer. Configure a firewall rule with the name of the load balancer as the source and the instance tag as the destination.&gt;&gt; tagging is not useful here as the instance is not the source of traffic, just the port need to be opened on FW."
      },
      {
        "date": "2024-03-09T08:43:00.000Z",
        "voteCount": 1,
        "content": "c is right"
      },
      {
        "date": "2024-01-11T08:37:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2023-12-26T02:56:00.000Z",
        "voteCount": 1,
        "content": "Most likely the problem of instances terminating is with the threshold settings on the health check. It\u2019s thinking too sooner that some VMs can be terminated due to less load."
      },
      {
        "date": "2023-10-09T21:02:00.000Z",
        "voteCount": 1,
        "content": "One thing that i couldn't understand is - How VMs getting terminated and relaunched for not setting health-checks in the load balancer ? how will that affect VM's uptime ?"
      },
      {
        "date": "2023-10-03T21:37:00.000Z",
        "voteCount": 1,
        "content": "If the healthcheck is not successful, it will keep on re-creating the instances in MIG."
      },
      {
        "date": "2022-11-26T06:19:00.000Z",
        "voteCount": 2,
        "content": "C (LB Health check should be taken care of)"
      },
      {
        "date": "2022-10-22T10:09:00.000Z",
        "voteCount": 2,
        "content": "C is the correct answer"
      },
      {
        "date": "2022-10-12T07:47:00.000Z",
        "voteCount": 3,
        "content": "C. Ensure that a firewall rule exists to allow load balancer health checks to reach the instances in the instance group."
      },
      {
        "date": "2022-04-23T03:51:00.000Z",
        "voteCount": 2,
        "content": "answer C:\nhttps://cloud.google.com/load-balancing/docs/health-check-concepts#ip-ranges"
      },
      {
        "date": "2022-01-25T06:57:00.000Z",
        "voteCount": 2,
        "content": "C is corect."
      },
      {
        "date": "2021-12-29T00:16:00.000Z",
        "voteCount": 1,
        "content": "C is corect."
      },
      {
        "date": "2021-12-29T00:21:00.000Z",
        "voteCount": 2,
        "content": "' (VM) instances are being terminated and re-launched every minute. '\nIsn't it because the health check is failing.\n\nA &amp; D Maybe aleady there.curl command passed.\nB What are you doing. Absolutely no."
      },
      {
        "date": "2021-12-03T09:19:00.000Z",
        "voteCount": 4,
        "content": "Go for C.\nThis questions is in sample quesitons of Google\nhttps://docs.google.com/forms/d/e/1FAIpQLSdvf8Xq6m0kvyIoysdr8WZYCG32WHENStftiHTSdtW4ad2-0w/viewform"
      },
      {
        "date": "2021-11-09T00:16:00.000Z",
        "voteCount": 2,
        "content": "C is the correct answer."
      },
      {
        "date": "2021-10-09T04:58:00.000Z",
        "voteCount": 2,
        "content": "Option C\nIf curl command is working then traffic exists.. So we need to check why health checks are failing.. so firewall issues for health check done by Google probers"
      },
      {
        "date": "2021-08-24T02:17:00.000Z",
        "voteCount": 3,
        "content": "Let's go with option elimination\nA. Ensure that firewall rules exist to allow source traffic on HTTP/HTTPS to reach the load balancer.\n&gt;&gt; We don't need a firewall rule to reach LB but VM in the VPN - eliminate the option\nB. Assign a public IP to each instance and configure a firewall rule to allow the load balancer to reach the instance public IP.\n&gt;&gt; LB don't need a public IP to reach to VM.\nC. Ensure that a firewall rule exists to allow load balancer health checks to reach the instances in the instance group.\n&gt;&gt; Correct\nD. Create a tag on each instance with the name of the load balancer. Configure a firewall rule with the name of the load balancer as the source and the instance tag as the destination.\n&gt;&gt; N/w tagging not needed just port opening needed to reach to VM from LB. This is when you want to separate some traffic to reach to particular VM than other https://cloud.google.com/vpc/docs/add-remove-network-tags"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 10,
    "url": "https://www.examtopics.com/discussions/google/view/7041-exam-professional-cloud-architect-topic-1-question-10/",
    "body": "You write a Python script to connect to Google BigQuery from a Google Compute Engine virtual machine. The script is printing errors that it cannot connect to<br>BigQuery.<br>What should you do to fix the script?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall the latest BigQuery API client library for Python",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun your script on a new virtual machine with the BigQuery access scope enabled",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new service account with BigQuery access and execute your script with that user\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall the bq component for gcloud with the command gcloud components install bq."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 49,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 9,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-09-20T11:15:00.000Z",
        "voteCount": 97,
        "content": "A - If client library was not installed, the python scripts won't run - since the question states the script reports \"cannot connect\" - the client library must have been installed. so it's B or C.\n\nB - https://cloud.google.com/bigquery/docs/authorization an access scope is how your client application retrieve access_token with access permission in OAuth when you want to access services via API call - in this case, it is possible that the python script use an API call instead of library, if this is true, then access scope is required. client library requires no access scope (as it does not go through OAuth)\n\nC - service account is Google Cloud's best practice\nSo prefer C."
      },
      {
        "date": "2021-10-09T04:00:00.000Z",
        "voteCount": 11,
        "content": "Access scopes are the legacy method of specifying permissions for your instance. read from &gt; https://cloud.google.com/compute/docs/access/service-accounts . So , I would go with C"
      },
      {
        "date": "2021-02-22T22:52:00.000Z",
        "voteCount": 4,
        "content": "agreed to comment here . C seems like a good option"
      },
      {
        "date": "2022-08-21T09:10:00.000Z",
        "voteCount": 2,
        "content": "agree\naccess scope is enabled by default\nhttps://cloud.google.com/bigquery/docs/authorization#authenticate_with_oauth_20\n\n If you use the BigQuery client libraries, you do not need this information, as this is done for you automatically."
      },
      {
        "date": "2020-06-13T06:13:00.000Z",
        "voteCount": 4,
        "content": "Might be an old version"
      },
      {
        "date": "2019-10-23T00:00:00.000Z",
        "voteCount": 13,
        "content": "Why not B? It looks better for me."
      },
      {
        "date": "2020-08-05T03:55:00.000Z",
        "voteCount": 11,
        "content": "C is ok"
      },
      {
        "date": "2020-08-14T01:54:00.000Z",
        "voteCount": 15,
        "content": "Sorry, B is ok. You can create service account, add user to service account, and grant the user role as Service Account User. You still need to enable BigQuery scope to make the Python script running the instance to access BigQuery."
      },
      {
        "date": "2020-08-14T10:39:00.000Z",
        "voteCount": 35,
        "content": "Stop confusing people, B) doesn't make any sense. Why would you use or create a whole new VM just because of a permission issue? If anything, just stop the instance and edit the scope of the default Compute Service Account and grant it the role through IAM. C) is the most appropriate answer since you can only set scopes of the default Compute Service Account, if you're using any other, there's no scope option - its access is dictated strictly by IAM in such scenario. So C) is the answer: Stop the VM, change the Service Account with the appropriate permissions and done. B) would still need to have permission the set through IAM &amp; Admin, the scope isn't enough with the default Compute Service Account."
      },
      {
        "date": "2020-11-14T02:26:00.000Z",
        "voteCount": 4,
        "content": "cloud guy1, relax. tartar is the hero for google cloud and if you read his answer, he explains the service account user's role granting on this one as that is the best practice"
      },
      {
        "date": "2020-11-28T04:47:00.000Z",
        "voteCount": 5,
        "content": "Configure the Python API to use a service account with relevant BigQuery access enabled. is the right answer.\n\nIt is likely that this service account this script is running under does not have the permissions to connect to BigQuery and that could be causing issues. You can prevent these by using a service account that has the necessary roles to access BigQuery.\n\nRef: https://cloud.google.com/bigquery/docs/reference/libraries#cloud-console\n\nA service account is a special kind of account used by an application or a virtual machine (VM) instance, not a person.\n\nRef: https://cloud.google.com/iam/docs/service-accounts"
      },
      {
        "date": "2021-03-03T23:43:00.000Z",
        "voteCount": 6,
        "content": "C, no brainer. You need SA for using API period. Thats where your start your troubleshooting."
      },
      {
        "date": "2021-03-03T23:48:00.000Z",
        "voteCount": 2,
        "content": "I stand corrected, B you need to have scope. It is union of Scope + Service Account. If scope is not there, you are screwed anyways."
      },
      {
        "date": "2022-12-26T08:07:00.000Z",
        "voteCount": 2,
        "content": "Create a new service account with BigQuery access and execute your script with that user: If you want to run the script on an existing virtual machine, you can create a new service account with the necessary permissions to access BigQuery and then execute the script using that service account. This will allow the script to connect to BigQuery and access the data it needs."
      },
      {
        "date": "2024-08-07T23:26:00.000Z",
        "voteCount": 1,
        "content": "Choose C"
      },
      {
        "date": "2024-06-08T09:52:00.000Z",
        "voteCount": 1,
        "content": "I suppose all of them are correct, but we should choose the least effort, B is correct.."
      },
      {
        "date": "2024-06-08T09:53:00.000Z",
        "voteCount": 1,
        "content": "run script on a new vm, not create a new vm.."
      },
      {
        "date": "2024-06-07T22:59:00.000Z",
        "voteCount": 2,
        "content": "Tricky question.\nHowever, as you can read in gcloud compute instances create documentation:\n--scopes=[SCOPE,\u2026]\nIf not provided, the instance will be assigned the default scopes, described below. However, if neither --scopes nor --no-scopes are specified and the project has no default service account, then the instance will be created with no scopes. Note that the level of access that a service account has is determined by a combination of access scopes and IAM roles so you must configure both access scopes and IAM roles for the service account to work properly.\n\nSo, probably, B is the right one, as for the \"new vm\", I guess that this is because you don't want to stop the current one before having the working one ready..."
      },
      {
        "date": "2024-05-24T12:46:00.000Z",
        "voteCount": 1,
        "content": "C - service account is Google Cloud's best practice"
      },
      {
        "date": "2024-04-24T10:48:00.000Z",
        "voteCount": 2,
        "content": "You don't need to create a new VM to have different access scopes:\nhttps://cloud.google.com/compute/docs/access/service-accounts#accesscopesiam\nThis weakens answer B.\nWhen a user-managed service account is attached to the instance, the access scope defaults to cloud-platform:\nhttps://cloud.google.com/compute/docs/access/service-accounts#scopes_best_practice\nSee Step 6 in: https://cloud.google.com/compute/docs/instances/change-service-account#changeserviceaccountandscopes\nThese facts leave C as the valid answer."
      },
      {
        "date": "2024-03-09T02:55:00.000Z",
        "voteCount": 3,
        "content": "C. Create a new service account with BigQuery access and execute your script with that user.\nService accounts are used for server-to-server interactions, such as those between a virtual machine and BigQuery. You would need to create a service account that has the necessary permissions to access BigQuery, then download the service account key in JSON format. Once you have the key, you can set an environment variable (GOOGLE_APPLICATION_CREDENTIALS) to the path of the JSON key file before running your script, which will authenticate your requests to BigQuery."
      },
      {
        "date": "2024-03-12T08:54:00.000Z",
        "voteCount": 1,
        "content": "better than creating and downloading a service account key would be to impersonate the service account"
      },
      {
        "date": "2024-02-04T13:54:00.000Z",
        "voteCount": 1,
        "content": "The answer is C\n\nhttps://cloud.google.com/bigquery/docs/authentication\nFor most services, you must attach the service account when you create the resource that will run your code; you cannot add or replace the service account later. Compute Engine is an exception\u2014it lets you attach a service account to a VM instance at any time."
      },
      {
        "date": "2024-01-11T08:40:00.000Z",
        "voteCount": 1,
        "content": "Connecting to BigQuery from a script requires proper authorization. Service accounts provide a secure way to grant access without sharing user credentials."
      },
      {
        "date": "2023-12-28T15:39:00.000Z",
        "voteCount": 1,
        "content": "It should be B,\nScript cannot be run by user and user cannot be assigned with Service account, SA can be assigned to a VM"
      },
      {
        "date": "2023-11-10T07:26:00.000Z",
        "voteCount": 1,
        "content": "C\nBest practice is that SA with least privilege from a CE should access BQ."
      },
      {
        "date": "2023-11-06T06:10:00.000Z",
        "voteCount": 1,
        "content": "B is silly because there's no need to create a new VM just to change the access scope.  You can edit the existing VM's access scope, although you do have to stop it first."
      },
      {
        "date": "2023-09-23T09:12:00.000Z",
        "voteCount": 1,
        "content": "Closest is C. https://cloud.google.com/compute/docs/access/create-enable-service-accounts-for-instances#gcloud\nThe confusion part is that it should never use the word user to represent service account"
      },
      {
        "date": "2023-09-08T10:28:00.000Z",
        "voteCount": 2,
        "content": "Recomended best practice"
      },
      {
        "date": "2023-08-22T09:33:00.000Z",
        "voteCount": 1,
        "content": "you need service account to access any service in Google"
      },
      {
        "date": "2023-08-06T19:59:00.000Z",
        "voteCount": 4,
        "content": "Service accounts provide a way to authenticate your application to Google Cloud services. When you create a service account, you can assign it specific roles that dictate what resources the service account can interact with, and how it can interact with them. In this case, you would assign the BigQuery access role to the service account, which would then be used to authenticate your script to BigQuery."
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 11,
    "url": "https://www.examtopics.com/discussions/google/view/7133-exam-professional-cloud-architect-topic-1-question-11/",
    "body": "Your customer is moving an existing corporate application to Google Cloud Platform from an on-premises data center. The business owners require minimal user disruption. There are strict security team requirements for storing passwords.<br>What authentication strategy should they use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse G Suite Password Sync to replicate passwords into Google",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFederate authentication via SAML 2.0 to the existing Identity Provider\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tProvision users in Google using the Google Cloud Directory Sync tool",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAsk users to set their Google password to match their corporate password"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 59,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 17,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-05-04T14:28:00.000Z",
        "voteCount": 77,
        "content": "The correct answer is B. \nGCDS tool only copies the usernames, not the passwords. And more over strict security requirements for the passwords. Not allowed to copy them onto Google, I think. \n\nFederation technique help resolve this issue. Please correct me if I am wrong."
      },
      {
        "date": "2023-10-31T05:56:00.000Z",
        "voteCount": 10,
        "content": "B is the answer. Why ?\n\nGCDS syncs passwords - Ok but which passwords? Clients need to provide a new password for accessing Google Cloud after GCDS sync.\nGoogle recognizes the user because GCDS populated the user list. The user is\nredirected to a standard Google sign-in screen where they enter their standard username and Google Cloud-specific password. \nThe issue here is the two sets of passwords. Even if a user manually sets them both to the same value, they aren\u2019t managed in a single place. If you need to update your password, you\u2019d have to do that in AD and then again in Google Cloud Identity. In some cases, this approach can allow for better separation between your on-premises environment and Google Cloud, but it\u2019s also one more password to manage for your users."
      },
      {
        "date": "2024-05-24T12:51:00.000Z",
        "voteCount": 3,
        "content": "This should be the top comment. It explains in detail the proccess"
      },
      {
        "date": "2022-08-28T09:42:00.000Z",
        "voteCount": 8,
        "content": "Passwords are also synchronized:\nhttps://support.google.com/a/answer/6120130?hl=en&amp;ref_topic=2679497"
      },
      {
        "date": "2021-06-10T10:35:00.000Z",
        "voteCount": 11,
        "content": "GCDS synchronises password as well and that is the reason why B is the correct answer. Only in B the password doesn't get copied to GCP."
      },
      {
        "date": "2022-10-16T20:32:00.000Z",
        "voteCount": 3,
        "content": "C is the answer"
      },
      {
        "date": "2019-10-24T06:52:00.000Z",
        "voteCount": 22,
        "content": "\"A\" will syncronise passwords between on pre-mise and the GCP, this duplicates the existing strategy plus Google's \"built-in\" encryption of\nall the data. \"B\" does not support the moving to GCP. \"C\" The directory sync tool copies the filesystem settings between servers, UNIX filesystems\nhave permission settings built in and passwords to log into the permission groups, syncing these would set GCP up the same way their on-premises\nis, plus Google's \"built-in\" encryption. \"D\" disrupts the users, so this is not correct. The debate should be between \"A\" and \"C\", \"C\" includes\n\"A\" according to (https://cloud.google.com/solutions/migrating-consumer-accounts-to-cloud-identity-or-g-suite-best-practices-federation) so\nchoose \"C\""
      },
      {
        "date": "2020-07-05T02:06:00.000Z",
        "voteCount": 4,
        "content": "B is supported read https://cloud.google.com/architecture/identity/federating-gcp-with-active-directory-configuring-single-sign-on"
      },
      {
        "date": "2021-03-04T16:50:00.000Z",
        "voteCount": 3,
        "content": "There is no mention SSO is needed."
      },
      {
        "date": "2020-08-05T20:08:00.000Z",
        "voteCount": 5,
        "content": "B is ok."
      },
      {
        "date": "2020-08-14T01:55:00.000Z",
        "voteCount": 11,
        "content": "miss typed.. C is ok"
      },
      {
        "date": "2021-03-03T23:49:00.000Z",
        "voteCount": 3,
        "content": "B, you dont want to store password as per security guidelines provided in question."
      },
      {
        "date": "2020-06-26T02:07:00.000Z",
        "voteCount": 16,
        "content": "GCDS syncs user accounts and some other LDAP attributes but not the passwords, with hybrid connectivity to GCP, SAML (or federation) is the preferred method.\n\nAnswer should be \"B\"\n\nhttps://cloud.google.com/solutions/patterns-for-authenticating-corporate-users-in-a-hybrid-environment\nhttps://cloud.google.com/architecture/identity/federating-gcp-with-active-directory-synchronizing-user-accounts#deciding_what_to_provision"
      },
      {
        "date": "2022-01-21T08:49:00.000Z",
        "voteCount": 1,
        "content": "This is the best answer so far."
      },
      {
        "date": "2020-10-03T00:21:00.000Z",
        "voteCount": 5,
        "content": "GCDS does sync passwords. Please refer - https://support.google.com/a/answer/6120130. Since the question says client wants to move to GCP , C should be the answer."
      },
      {
        "date": "2023-06-21T10:50:00.000Z",
        "voteCount": 1,
        "content": "The article implies that ADFS is best but suggests you also need the GCDS.  This makes sense, you need the users in Google to allocate permissions but you don't want to copy the passwords across hence ADFS."
      },
      {
        "date": "2024-10-17T08:06:00.000Z",
        "voteCount": 1,
        "content": "cross-domain SSO can be achieved by SAML"
      },
      {
        "date": "2024-09-06T22:40:00.000Z",
        "voteCount": 1,
        "content": "I think B is correct"
      },
      {
        "date": "2024-09-03T01:14:00.000Z",
        "voteCount": 1,
        "content": "Minimal user disruption: By federating authentication via SAML 2.0, users can continue using their existing corporate credentials without having to manage or remember new passwords.\nSecurity requirements: SAML 2.0 federation allows your organization to maintain control over user authentication and password management within the existing Identity Provider (IdP). Passwords do not need to be stored in Google\u2019s systems, which aligns with strict security requirements."
      },
      {
        "date": "2024-08-13T12:28:00.000Z",
        "voteCount": 1,
        "content": "B is right one . Because C  While Google Cloud Directory Syc (GCDS) helps sync users between an on-premises directory and Google, it does not address the password management aspect. Users may still face disruptions as this method might not handle existing passwords securely."
      },
      {
        "date": "2024-08-07T23:50:00.000Z",
        "voteCount": 1,
        "content": "Choose B"
      },
      {
        "date": "2024-06-09T21:43:00.000Z",
        "voteCount": 1,
        "content": "the most convenient way is B, but the principle of this kind of exam is use cloud provider's native tools, so the C is correct.. this principle is also used on aws"
      },
      {
        "date": "2024-03-09T03:05:00.000Z",
        "voteCount": 5,
        "content": "B. Federate authentication via SAML 2.0 to the existing Identity Provider.\n\nHere's why:\n\nSecurity: SAML 2.0 allows for secure single sign-on (SSO) without storing passwords on Google's side. It ensures that authentication happens against the corporate Identity Provider (IdP), which maintains control over the user credentials.\n\nMinimal Disruption: Users can continue to use their existing corporate credentials to access the application on GCP without having to remember a new set of credentials or go through a password change process.\n\nCompliance: It satisfies the security team's requirements for password storage by ensuring that passwords remain within the corporate boundary.\n\nIntegration: SAML is widely supported and can be integrated with many IdPs, allowing for a seamless transition to cloud-based resources while leveraging existing identity management infrastructure."
      },
      {
        "date": "2024-02-18T20:31:00.000Z",
        "voteCount": 2,
        "content": "The correct answer is C.\nGoogle Cloud Directory Sync will provide federated authentications.\nB is wrong because SAML is used for Single sign-on. It also doesn't mention how the cloud can be authenticated to the existing Identity Provider. SAML by itself is not enough to do the job."
      },
      {
        "date": "2024-02-18T20:24:00.000Z",
        "voteCount": 2,
        "content": "Federating authentication aligns with strict security team requirements for password storage, as it avoids the need to store or sync passwords outside the corporate environment."
      },
      {
        "date": "2024-01-11T09:12:00.000Z",
        "voteCount": 1,
        "content": "Minimal User Disruption:\n\nUsers continue using their existing corporate credentials for both on-premises and GCP applications, avoiding password resets or new account creations.\nSecurity Team Requirements:\n\nGCP doesn't store or manage corporate passwords; authentication relies on the existing Identity Provider (IdP), meeting strict password storage requirements."
      },
      {
        "date": "2023-11-21T15:26:00.000Z",
        "voteCount": 2,
        "content": "B is a preferred solution nowadays, that's why:\n\nhttps://cloud.google.com/architecture/framework/security/identity-access#use_a_single_identity_provider"
      },
      {
        "date": "2023-11-13T21:38:00.000Z",
        "voteCount": 1,
        "content": "GCDS is better as it is a corporate application. The requirements for storing password can be met by GCP. As GCP has many security features\nFor SAML, the corporate needs to have Identity provider service such as the one provided by Google, Facebook"
      },
      {
        "date": "2023-11-13T21:49:00.000Z",
        "voteCount": 1,
        "content": "Also the application needs to be modified to use identity provider service, if they are going by choice B"
      },
      {
        "date": "2023-11-05T13:26:00.000Z",
        "voteCount": 1,
        "content": "main reason for B are strict storage requirements."
      },
      {
        "date": "2023-10-09T21:24:00.000Z",
        "voteCount": 1,
        "content": "B is the correct answer"
      },
      {
        "date": "2023-09-23T20:59:00.000Z",
        "voteCount": 2,
        "content": "I think it's B because they want minimal user disruption, and only this option focuses on using the same password. Plus, they want to move ONE existing corporate application, not all their infrastructure. \n\nA. I don't think this meets a strict security requirement, and if they eventually need to change the password, I think this would not be synced or may have issues syncing both passwords. \nC. We don't want to provision new users; we want to keep users with minimal disruption and doing what they do already taking the least steps possible. \nD. Probably a terrible security practice; if anything, we would like them to use one password and sign in from there. \n\nB seems to me the most fitting."
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 12,
    "url": "https://www.examtopics.com/discussions/google/view/7134-exam-professional-cloud-architect-topic-1-question-12/",
    "body": "Your company has successfully migrated to the cloud and wants to analyze their data stream to optimize operations. They do not have any existing code for this analysis, so they are exploring all their options. These options include a mix of batch and stream processing, as they are running some hourly jobs and live- processing some data as it comes in.<br>Which technology should they use for this?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGoogle Cloud Dataproc",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGoogle Cloud Dataflow\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGoogle Container Engine with Bigtable",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGoogle Compute Engine with Google BigQuery"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 17,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-09-20T11:18:00.000Z",
        "voteCount": 35,
        "content": "All four options can accomplish what the question asks, in regards to batching and streaming processes. \"A\" is for Apache Spark and Hadoop, a juggernaut in speed of data processing. \"B\" is Google's best attempt at TIBCO, Ab Initio, and other processing technology, built explicity for visualizing batch operations and streams without through various labeled circuit boards. \"C\" and \"D\" are used within \"A\" and \"B\" and would require more work and higher risk. I'd guess Google wants you to select \"B\""
      },
      {
        "date": "2020-01-30T06:08:00.000Z",
        "voteCount": 6,
        "content": "answer: B"
      },
      {
        "date": "2024-09-24T04:54:00.000Z",
        "voteCount": 1,
        "content": "B. Google Cloud Dataflow\n\nExplanation:\nUnified Processing:\n\nGoogle Cloud Dataflow is designed to handle both batch and stream processing in a unified manner. This means you can process data as it arrives (stream processing) and also perform scheduled batch jobs efficiently.\nServerless and Scalable:\n\nDataflow is serverless, which means you don\u2019t have to worry about managing the underlying infrastructure. It automatically scales to handle varying workloads, making it ideal for optimizing operations based on live data streams and scheduled jobs.\nIntegration with Other Google Cloud Services:\n\nDataflow integrates well with other Google Cloud services, such as Google Cloud Storage, BigQuery, and Pub/Sub. This makes it easier to build a comprehensive data pipeline that can analyze data streams effectively.\nFlexible SDKs:\n\nDataflow supports popular programming languages like Java and Python, allowing your team to write custom processing logic as needed."
      },
      {
        "date": "2024-08-07T23:53:00.000Z",
        "voteCount": 1,
        "content": "Choose B"
      },
      {
        "date": "2024-01-11T09:21:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2023-12-20T16:51:00.000Z",
        "voteCount": 1,
        "content": "chatGPT answers:\nB. Google Cloud Dataflow\nGoogle Cloud Dataflow is a fully managed service for stream and batch data processing. It is built on Apache Beam and provides a unified programming model, making it an ideal choice for scenarios where both batch and stream data processing are required. Dataflow simplifies the complexities of data parallel processing, allowing for easy development and maintenance of data processing pipelines. It integrates well with other Google Cloud services, like BigQuery for analytics and Cloud Storage for storing data, providing a comprehensive solution for real-time and batch data processing needs."
      },
      {
        "date": "2023-06-22T09:39:00.000Z",
        "voteCount": 5,
        "content": "The word analysis throws me off.  Wonder if the question is just written incorrectly here?  I'd say Dataflow is a key tool to enable the processing of the data to be able to do the analysis but feels like the final analysis should be in a database."
      },
      {
        "date": "2023-03-22T14:27:00.000Z",
        "voteCount": 2,
        "content": "B is the answer"
      },
      {
        "date": "2023-02-23T13:06:00.000Z",
        "voteCount": 2,
        "content": "A is a managed Hadoop and Spark service. C and D are mostly for petabyte kinds of data. So remains B (suitable for ETL jobs)"
      },
      {
        "date": "2022-12-20T01:16:00.000Z",
        "voteCount": 3,
        "content": "To analyze a data stream and optimize operations, your company could consider using Google Cloud Dataflow, which is a fully-managed, cloud-native data processing service that can handle both batch and stream processing.\n\nGoogle Cloud Dataflow is designed to handle large volumes of data and can scale up or down automatically to meet the needs of the workload. It provides a number of pre-built connectors and integrations that make it easy to ingest data from a variety of sources, and it offers a range of processing options, including batch processing and stream processing, that can be used to analyze the data in real-time.\n\nOption A: Google Cloud Dataproc, option C: Google Container Engine with Bigtable, and option D: Google Compute Engine with Google BigQuery, while potentially useful for certain types of data processing, would not necessarily be well-suited to handle both batch and stream processing in the way that Google Cloud Dataflow can"
      },
      {
        "date": "2022-12-16T07:39:00.000Z",
        "voteCount": 2,
        "content": "answer is D for me the question is which tool for analyse data. Dataflow does not analyse data"
      },
      {
        "date": "2022-11-05T01:43:00.000Z",
        "voteCount": 1,
        "content": "ok for B"
      },
      {
        "date": "2022-10-22T10:54:00.000Z",
        "voteCount": 1,
        "content": "B is the right answer"
      },
      {
        "date": "2022-10-16T14:04:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2022-10-12T22:55:00.000Z",
        "voteCount": 1,
        "content": "B. Google Cloud Dataflow"
      },
      {
        "date": "2022-09-20T06:45:00.000Z",
        "voteCount": 1,
        "content": "correct is B use data flow for stream and batch process"
      },
      {
        "date": "2022-09-04T00:12:00.000Z",
        "voteCount": 2,
        "content": "Answer seems to be B in most other websites as well.\n\nhttps://cloud.google.com/solutions/authenticating-corporate-users-in-a-hybrid-environment"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 13,
    "url": "https://www.examtopics.com/discussions/google/view/7137-exam-professional-cloud-architect-topic-1-question-13/",
    "body": "Your customer is receiving reports that their recently updated Google App Engine application is taking approximately 30 seconds to load for some of their users.<br>This behavior was not reported before the update.<br>What strategy should you take?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWork with your ISP to diagnose the problem",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOpen a support ticket to ask for network capture and flow data to diagnose the problem, then roll back your application",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRoll back to an earlier known good release initially, then use Stackdriver Trace and Logging to diagnose the problem in a development/test/staging environment\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRoll back to an earlier known good release, then push the release again at a quieter period to investigate. Then use Stackdriver Trace and Logging to diagnose the problem"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 21,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2019-11-22T12:19:00.000Z",
        "voteCount": 27,
        "content": "C is the answer"
      },
      {
        "date": "2024-09-20T11:19:00.000Z",
        "voteCount": 17,
        "content": "Key word: This behavior was not reported before the update\nA - Not Correct as it was working before with same ISP\nB - New code update caused an issue- why to open support ticket\nC - I agree with C\nD - This requires downtime and live prod affected too"
      },
      {
        "date": "2019-12-26T01:17:00.000Z",
        "voteCount": 1,
        "content": "\"then use Stackdriver Trace and Logging to diagnose the problem in a development/test/staging environment\" they are NOT asking us to setup Dev/Text/Stage.. meaning the environment already exist and we have to use it"
      },
      {
        "date": "2020-06-14T23:54:00.000Z",
        "voteCount": 1,
        "content": "\"then use Stackdriver Trace and Logging to diagnose the problem in a development/test/staging environment\" this is not asking for set environment either, it just says to diagnose problem in other environment so C it is"
      },
      {
        "date": "2024-01-11T09:47:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2023-10-03T21:50:00.000Z",
        "voteCount": 1,
        "content": "Your customer is receiving reports that their recently updated Google App Engine application is taking approximately 30 seconds to load for some of their users.\nThis behavior was not reported before the update.\nWhat strategy should you take?\n\n\nHere the application (our code) is updated and only some users are facing lantecy (Cloud Trace) issue.\n\nThe issue is not with ISP (A), Not an issue with Google (B).\nRollback must be done as mitigation, but testing should be done in Non-Prod environments (C), not on prod environment (D).\n\nHence C is correct answer."
      },
      {
        "date": "2023-09-23T21:21:00.000Z",
        "voteCount": 2,
        "content": "I'm going for C. While D may be \"better\" in case this is an issue that only occurs in production, I think that keeping the disruption at minimum would be the best practice, which D would not really do. Plus, if the problem is load related, having this released at a quieter period may not surface the problem either."
      },
      {
        "date": "2023-06-27T21:43:00.000Z",
        "voteCount": 2,
        "content": "Although it sounds like the right answer to do network tracing in stg again, this may be a network pass-through related issue and it is felt that the problem may not be reproduced if not checked in a prod environment."
      },
      {
        "date": "2023-06-27T21:42:00.000Z",
        "voteCount": 1,
        "content": "Although it sounds like the right answer to do network tracing in stg again, this may be a network pass-through related issue and it is felt that the problem may not be reproduced if not checked in a prod environment."
      },
      {
        "date": "2023-05-06T05:38:00.000Z",
        "voteCount": 2,
        "content": "should be C"
      },
      {
        "date": "2023-03-22T14:25:00.000Z",
        "voteCount": 2,
        "content": "C is the answer"
      },
      {
        "date": "2023-03-13T00:25:00.000Z",
        "voteCount": 3,
        "content": "Option C is also a valid strategy in this scenario. Rolling back to an earlier known good release initially and using Stackdriver Trace and Logging to diagnose the problem in a development/test/staging environment can help diagnose the issue without impacting production users.\n\nHowever, the reason why option D may be a better approach is that it allows for investigation during a quieter period, which can reduce the impact of any issues that may occur during the investigation. Rolling back to a known good release and then pushing the release again at a quieter period can help to ensure that users are not impacted during the investigation."
      },
      {
        "date": "2022-11-05T01:39:00.000Z",
        "voteCount": 1,
        "content": "ok for C"
      },
      {
        "date": "2022-10-22T11:07:00.000Z",
        "voteCount": 1,
        "content": "C is the correct answer"
      },
      {
        "date": "2022-10-16T14:05:00.000Z",
        "voteCount": 1,
        "content": "C is perfect to troubleshoot latency issues with app"
      },
      {
        "date": "2022-10-12T23:04:00.000Z",
        "voteCount": 3,
        "content": "C. Roll back to an earlier known good release initially, then use Stackdriver Trace and Logging to diagnose the problem in a development/test/staging environment\n\nA and B are not relevant\nD - no IT manager will ever allow re-deployment of erroneous code in production, even in a quiet period...!"
      },
      {
        "date": "2023-05-01T13:49:00.000Z",
        "voteCount": 2,
        "content": "I agree why not D, but in the past I faced issues only reproducible in prd, at that situation D was a possibility but usually yep C is for sure"
      },
      {
        "date": "2022-09-20T06:47:00.000Z",
        "voteCount": 1,
        "content": "correct answer is C use the standard practise"
      },
      {
        "date": "2022-09-10T07:26:00.000Z",
        "voteCount": 1,
        "content": "How come everyone is agreeing to C!! In option C after rollback, the investigation will happen only on the earlier good release. Whereas in option D, all the troubleshooting will happen on current/problematic build. Option D should be the right option as it resolves the issue in short term and provides room for further investigation without downtime."
      },
      {
        "date": "2022-10-16T20:38:00.000Z",
        "voteCount": 1,
        "content": "you want to minimize the business loose, best option is to rollback and use stack-driver to diagnose the issue"
      },
      {
        "date": "2022-09-25T01:41:00.000Z",
        "voteCount": 3,
        "content": "Option C is investigating the bad build in test.  The problem with option D is it is user impacting.  Always best to attempt to find the problem in a test environment first.  D could end-up being an option of last resort if all attempts to diagnose in test fail but I doubt any business person would be happy with D as it impacts service."
      },
      {
        "date": "2022-06-23T00:55:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is c."
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 14,
    "url": "https://www.examtopics.com/discussions/google/view/7142-exam-professional-cloud-architect-topic-1-question-14/",
    "body": "A production database virtual machine on Google Compute Engine has an ext4-formatted persistent disk for data files. The database is about to run out of storage space.<br>How can you remediate the problem with the least amount of downtime?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the Cloud Platform Console, increase the size of the persistent disk and use the resize2fs command in Linux.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tShut down the virtual machine, use the Cloud Platform Console to increase the persistent disk size, then restart the virtual machine",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the Cloud Platform Console, increase the size of the persistent disk and verify the new space is ready to use with the fdisk command in Linux",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the Cloud Platform Console, create a new persistent disk attached to the virtual machine, format and mount it, and configure the database service to move the files to the new disk",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the Cloud Platform Console, create a snapshot of the persistent disk restore the snapshot to a new larger disk, unmount the old disk, mount the new disk and restart the database service"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 22,
        "isMostVoted": true
      },
      {
        "answer": "E",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2019-11-22T12:22:00.000Z",
        "voteCount": 28,
        "content": "A is the correct answer because the question says \"with minimum downtime\""
      },
      {
        "date": "2019-12-17T13:14:00.000Z",
        "voteCount": 14,
        "content": "least amount of downtime? is the sugar word. You miss that you miss all. Everything there is correct but I believe its only A that fits that requirement"
      },
      {
        "date": "2021-04-07T04:43:00.000Z",
        "voteCount": 1,
        "content": "but in option A, nowhere it is mentioned to shut down the VM."
      },
      {
        "date": "2021-07-15T07:02:00.000Z",
        "voteCount": 1,
        "content": "No need to reboot."
      },
      {
        "date": "2023-11-07T08:37:00.000Z",
        "voteCount": 1,
        "content": "A is the right answer.You should resize the disk, take a snapshot, then resize the filesystem and partitions (eg.) ext4, xfs etc."
      },
      {
        "date": "2023-10-03T21:58:00.000Z",
        "voteCount": 8,
        "content": "Unlike Azure, in google you can dynamically resize the persistent disk while VM is running. This narrows down the option to A or C.  Since the question says \"ext4-formatted persistent disk\", we need to choose correct command (resize2fs or fdisk ) for Linux for resizing ext4 file format disk. To resize an ext4 file system in Linux, you can use the resize2fs command. FDISK to manipulate partition tables in Linux."
      },
      {
        "date": "2023-09-05T21:48:00.000Z",
        "voteCount": 4,
        "content": "According to the right url, A is the right answer. https://cloud.google.com/compute/docs/disks/resize-persistent-disk"
      },
      {
        "date": "2023-09-05T21:46:00.000Z",
        "voteCount": 1,
        "content": "The right URL for the oficial document is, https://cloud.google.com/compute/docs/disks/resize-persistent-disk"
      },
      {
        "date": "2023-08-22T09:41:00.000Z",
        "voteCount": 1,
        "content": "A is no brainer"
      },
      {
        "date": "2023-08-12T03:33:00.000Z",
        "voteCount": 2,
        "content": "E is the correct because is true you need minimum downtime but in Production a backup is a must."
      },
      {
        "date": "2023-08-12T03:32:00.000Z",
        "voteCount": 1,
        "content": "E because you are in Production, and you need a backup"
      },
      {
        "date": "2023-03-22T14:31:00.000Z",
        "voteCount": 3,
        "content": "A is correct, resize disk don't required reboot or downtime\nhttps://cloud.google.com/compute/docs/disks/resize-persistent-disk"
      },
      {
        "date": "2022-12-20T01:25:00.000Z",
        "voteCount": 1,
        "content": "A: Increasing the size of the persistent disk in the Cloud Platform Console and using the resize2fs command in Linux.\n\nIncreasing the size of the persistent disk can be done without requiring the virtual machine to be shut down, and the resize2fs command can be used to resize the ext4 filesystem on the disk to take advantage of the additional space. This will allow you to add more storage space to the virtual machine without disrupting the database service."
      },
      {
        "date": "2022-11-10T02:42:00.000Z",
        "voteCount": 2,
        "content": "A is ok"
      },
      {
        "date": "2022-10-22T11:12:00.000Z",
        "voteCount": 1,
        "content": "A https://cloud.google.com/compute/docs/disks/resize-persistent-disk?_ga=2.233866652.-3622898.1631303718"
      },
      {
        "date": "2022-10-12T23:23:00.000Z",
        "voteCount": 1,
        "content": "A. In the Cloud Platform Console, increase the size of the persistent disk and use the resize2fs command in Linux."
      },
      {
        "date": "2022-10-16T14:07:00.000Z",
        "voteCount": 1,
        "content": "yes, A is right"
      },
      {
        "date": "2022-09-20T06:48:00.000Z",
        "voteCount": 1,
        "content": "A resize the disk standard command"
      },
      {
        "date": "2022-09-14T18:00:00.000Z",
        "voteCount": 1,
        "content": "In the Cloud Platform Console, increase the size of the persistent disk and use the resize2fs command in Linux."
      },
      {
        "date": "2022-08-24T04:57:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 15,
    "url": "https://www.examtopics.com/discussions/google/view/7147-exam-professional-cloud-architect-topic-1-question-15/",
    "body": "Your application needs to process credit card transactions. You want the smallest scope of Payment Card Industry (PCI) compliance without compromising the ability to analyze transactional data and trends relating to which payment methods are used.<br>How should you design your architecture?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a tokenizer service and store only tokenized data\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate separate projects that only process credit card data",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate separate subnetworks and isolate the components that process credit card data",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStreamline the audit discovery phase by labeling all of the virtual machines (VMs) that process PCI data",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Logging export to Google BigQuery and use ACLs and views to scope the data shared with the auditor"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-05-27T23:58:00.000Z",
        "voteCount": 46,
        "content": "Final Decision to go with Option A. I have done PCI DSS Audit for my project and thats the best suited case. 100% sure to use tokenised data instead of actual card number"
      },
      {
        "date": "2020-06-13T09:14:00.000Z",
        "voteCount": 4,
        "content": "But with A you cannot extract statistics. That is the second r4equirement."
      },
      {
        "date": "2021-01-06T05:03:00.000Z",
        "voteCount": 5,
        "content": "Analyzing Transaction does not require Credit Card number I guess. Only amount of transaction or balance what is needed. We also perform something similar with transactional data with tokenized PII information. So CC can be tokenized. So answer should be A."
      },
      {
        "date": "2020-07-31T04:14:00.000Z",
        "voteCount": 2,
        "content": "Thinking about that better, I think you can because you are only tokenizing the sensitive data, not the transaction type."
      },
      {
        "date": "2022-08-17T00:47:00.000Z",
        "voteCount": 2,
        "content": "You can as the generated token for a given credit card would be same(generally but there are approaches which can give you different token for the same sensitive data input). Only thing that you won't know is the actual card number which is not required for the trend analysis. \n \nWhen the trend analysis involves referential integrity then tokenization process becomes challenging but still once data is tokenized correctly you should be able to perform any kind of the analysis."
      },
      {
        "date": "2022-10-16T14:09:00.000Z",
        "voteCount": 3,
        "content": "I agree. A is the best option"
      },
      {
        "date": "2022-12-20T01:29:00.000Z",
        "voteCount": 27,
        "content": "To minimize the scope of Payment Card Industry (PCI) compliance while still allowing for the analysis of transactional data and trends related to payment methods, you should consider using a tokenizer service and storing only tokenized data, as described in option A.\n\nTokenization is a process of replacing sensitive data, such as credit card numbers, with unique, randomly-generated tokens that cannot be used for fraudulent purposes. By using a tokenizer service and storing only tokenized data, you can reduce the scope of PCI compliance to only the tokenization service, rather than the entire application. This can help minimize the amount of sensitive data that needs to be protected and reduce the overall compliance burden."
      },
      {
        "date": "2023-01-21T15:50:00.000Z",
        "voteCount": 5,
        "content": "man, this is an amazing answer. props"
      },
      {
        "date": "2024-06-09T12:17:00.000Z",
        "voteCount": 1,
        "content": "thanks to chagpt, are you serious saying that?"
      },
      {
        "date": "2023-12-29T04:47:00.000Z",
        "voteCount": 1,
        "content": "Nicely explained, thanks."
      },
      {
        "date": "2024-01-12T08:41:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2023-09-01T10:12:00.000Z",
        "voteCount": 2,
        "content": "Not sure why 100% agree on A. To limit PCI DSS scope, the data handling should be done in a separate project with very limited access. Only in this project should tokenization be done and made available for analytics. The first requirement however, is isolating the payment and tokenization code in a separate project. Answer should be B."
      },
      {
        "date": "2023-08-22T09:42:00.000Z",
        "voteCount": 1,
        "content": "Tokenizing is the best way to protect PCI information."
      },
      {
        "date": "2023-07-26T05:24:00.000Z",
        "voteCount": 1,
        "content": "A is my best option"
      },
      {
        "date": "2022-11-09T22:52:00.000Z",
        "voteCount": 4,
        "content": "https://cloud.google.com/architecture/tokenizing-sensitive-cardholder-data-for-pci-dss"
      },
      {
        "date": "2022-11-05T01:32:00.000Z",
        "voteCount": 1,
        "content": "ok for A"
      },
      {
        "date": "2022-10-13T00:22:00.000Z",
        "voteCount": 1,
        "content": "A. Create a tokenizer service and store only tokenized data"
      },
      {
        "date": "2022-09-25T02:06:00.000Z",
        "voteCount": 1,
        "content": "B appears the most thorough but the question asks to comply with the smallest scope, network segmentation is not a must.  Tokenization is simpler.  C is similar to B, more than required.  D &amp; E do not address the problem."
      },
      {
        "date": "2022-09-20T06:49:00.000Z",
        "voteCount": 2,
        "content": "correct answer is A use tokenize"
      },
      {
        "date": "2022-09-14T18:01:00.000Z",
        "voteCount": 1,
        "content": "Correct answer A"
      },
      {
        "date": "2022-04-20T00:25:00.000Z",
        "voteCount": 1,
        "content": "The mandatory stage in PCI is having a encryption/ description system. Data must not be stored as is with PAN.  So A IS A MUST. The rest are nice to have."
      },
      {
        "date": "2022-04-03T00:58:00.000Z",
        "voteCount": 1,
        "content": "I think it should be A and C  - the paper states clearly, a proper network segmentation is still required to disparate the vault and token servers from the rest of the flat network."
      },
      {
        "date": "2022-01-15T15:30:00.000Z",
        "voteCount": 1,
        "content": "I chose A. But why C is not good?"
      },
      {
        "date": "2021-12-24T04:11:00.000Z",
        "voteCount": 3,
        "content": "A is the correct answer\nhttps://cloud.google.com/architecture/tokenizing-sensitive-cardholder-data-for-pci-dss#a_service_for_handling_sensitive_information"
      },
      {
        "date": "2021-12-03T09:36:00.000Z",
        "voteCount": 2,
        "content": "Go for A"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 16,
    "url": "https://www.examtopics.com/discussions/google/view/20238-exam-professional-cloud-architect-topic-1-question-16/",
    "body": "You have been asked to select the storage system for the click-data of your company's large portfolio of websites. This data is streamed in from a custom website analytics package at a typical rate of 6,000 clicks per minute. With bursts of up to 8,500 clicks per second. It must have been stored for future analysis by your data science and user experience teams.<br>Which storage infrastructure should you choose?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGoogle Cloud SQL",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGoogle Cloud Bigtable\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGoogle Cloud Storage",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGoogle Cloud Datastore"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 16,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-07-04T07:21:00.000Z",
        "voteCount": 12,
        "content": "B. Google Cloud Bigtable"
      },
      {
        "date": "2021-04-10T19:57:00.000Z",
        "voteCount": 10,
        "content": "B, Bigtable due to the IoT like requirements"
      },
      {
        "date": "2024-06-19T08:17:00.000Z",
        "voteCount": 1,
        "content": "Big table if it needs real time but here the need for analysis is not urgent =&gt; Google cloud storage."
      },
      {
        "date": "2023-11-30T23:02:00.000Z",
        "voteCount": 1,
        "content": "The right answer is BigTable for a such large volume of data."
      },
      {
        "date": "2023-11-07T09:54:00.000Z",
        "voteCount": 1,
        "content": "B. Google cloud Bigtable is most suitable as it is for analysis and streamed data."
      },
      {
        "date": "2023-08-15T07:05:00.000Z",
        "voteCount": 1,
        "content": "Note: DataStore is now rename as FireStore."
      },
      {
        "date": "2023-04-07T05:09:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2023-03-22T14:40:00.000Z",
        "voteCount": 3,
        "content": "Bigtable is a high-perf NoSQL db service that handle large volumes of structured data with low latency"
      },
      {
        "date": "2023-01-27T11:12:00.000Z",
        "voteCount": 1,
        "content": "The Google Cloud Bigtable  goes together with the BigQuery. The question itself gives away a bit."
      },
      {
        "date": "2022-12-20T01:32:00.000Z",
        "voteCount": 4,
        "content": "For storing click-data that is streamed in at a rate of 6,000 clicks per minute, with bursts of up to 8,500 clicks per second, and that needs to be stored for future analysis by your data science and user experience teams, you should consider using a scalable, high-performance, and low-latency NoSQL database such as Google Cloud Bigtable, option B.\n\nGoogle Cloud Bigtable is a fully managed, high-performance NoSQL database service that is designed to handle large volumes of structured data with low latency. It is well-suited for storing high-velocity data streams and can scale to handle millions of reads and writes per second.\n\nOption A: Google Cloud SQL, option C: Google Cloud Storage, and option D: Google Cloud Datastore, would not be suitable for this use case, as they are not designed to handle high-velocity data streams at this scale."
      },
      {
        "date": "2022-12-16T23:37:00.000Z",
        "voteCount": 2,
        "content": "B. Google Cloud Bigtable\n\nGoogle Cloud Bigtable is a scalable, high-performance NoSQL database that is well-suited for storing large amounts of data with low latency. It is designed for high-throughput workloads such as streaming data, and is able to handle bursts of up to millions of reads and writes per second.\n\nGiven the high volume of click data that needs to be stored and the requirement for low latency, Google Cloud Bigtable would be a good choice for storing the data. It is able to handle the high rate of incoming data and provide fast access to the data for analysis by the data science and user experience teams.\n\nGoogle Cloud SQL is a fully-managed relational database service, and may not be the best choice for storing high-volume streaming data. Google Cloud Storage is an object storage service, and may not provide the necessary performance for storing and querying large amounts of data in real-time. Google Cloud Datastore is a NoSQL document database, and while it may be suitable for storing large amounts of data, it may not provide the necessary performance for handling high volumes of streaming data."
      },
      {
        "date": "2022-12-05T21:24:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/bigtable#section-9"
      },
      {
        "date": "2022-11-25T23:04:00.000Z",
        "voteCount": 1,
        "content": "B. Google Cloud Bigtable"
      },
      {
        "date": "2022-11-19T02:15:00.000Z",
        "voteCount": 1,
        "content": "Cloud Bigtable has all the features to fulfill the requirements mentioned in the question"
      },
      {
        "date": "2022-10-22T11:24:00.000Z",
        "voteCount": 1,
        "content": "D is right answer"
      },
      {
        "date": "2022-10-16T20:49:00.000Z",
        "voteCount": 3,
        "content": "Bigtable for a high volume of writes. 8500 clicks  per sec"
      },
      {
        "date": "2022-10-13T01:06:00.000Z",
        "voteCount": 1,
        "content": "B. Google Cloud Bigtable"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 17,
    "url": "https://www.examtopics.com/discussions/google/view/7150-exam-professional-cloud-architect-topic-1-question-17/",
    "body": "You are creating a solution to remove backup files older than 90 days from your backup Cloud Storage bucket. You want to optimize ongoing Cloud Storage spend.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite a lifecycle management rule in XML and push it to the bucket with gsutil",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite a lifecycle management rule in JSON and push it to the bucket with gsutil\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSchedule a cron script using gsutil ls \u05d2\u20ac\"lr gs://backups/** to find and remove items older than 90 days",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSchedule a cron script using gsutil ls \u05d2\u20ac\"l gs://backups/** to find and remove items older than 90 days and schedule it with cron"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 13,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2019-10-24T10:02:00.000Z",
        "voteCount": 34,
        "content": "All four are correct answers. Google has built in cron job schduling with Cloud Schedule, so that would place \"D\" behind \"C\" in Google's perspective. Google also has it's own lifecycle management command line prompt gcloud lifecycle so \"A\" or \"B\" could be used. JSON is slightly faster than XML because of the \"{\" verse \"&lt;c&gt;\" distinguisher, with a Trie tree used for alphanumeric parsing. So between \"A\" and \"B\", choose \"B\". Between \"B\" and \"A\", \"B\" is slightly more efficient from the GCP operator perspective. So choose \"B\"."
      },
      {
        "date": "2020-01-14T01:03:00.000Z",
        "voteCount": 30,
        "content": "gsutil command takes only json as input for lifecycle management. In case of API, both XML and json can be used.\nhttps://cloud.google.com/storage/docs/gsutil/commands/lifecycle\nhttps://cloud.google.com/storage/docs/xml-api/put-bucket-lifecycle\nhttps://cloud.google.com/storage/docs/json_api/v1/buckets/update"
      },
      {
        "date": "2020-08-05T21:52:00.000Z",
        "voteCount": 8,
        "content": "B is ok"
      },
      {
        "date": "2021-03-04T07:49:00.000Z",
        "voteCount": 10,
        "content": "B is correct. Policy = JSON format. No matter if its AWS or GCP."
      },
      {
        "date": "2020-05-09T18:49:00.000Z",
        "voteCount": 15,
        "content": "I'll go with B.\nA is not reasonable because life cycle policies are not written in XML.\nB is reasonable and is cloud native.\nC requires a cron script which needs something to run the script and is a non-cloud native approach.\nD requires a cron script which needs something to run the script and is a non-cloud native approach."
      },
      {
        "date": "2024-08-08T00:07:00.000Z",
        "voteCount": 1,
        "content": "choose B"
      },
      {
        "date": "2023-11-04T07:12:00.000Z",
        "voteCount": 1,
        "content": "The correct available answer is B. But in real life, we use Terraform tfvars file."
      },
      {
        "date": "2023-07-26T05:09:00.000Z",
        "voteCount": 2,
        "content": "Cloud Storage has lifecycle management rules and could be applied with gsutil and gcloud storage buckets. It is common to use JSON for transferring data."
      },
      {
        "date": "2023-03-22T14:43:00.000Z",
        "voteCount": 1,
        "content": "B, gsutil can set policy using json file\nhttps://cloud.google.com/storage/docs/gsutil/commands/lifecycle#examples"
      },
      {
        "date": "2022-12-20T01:38:00.000Z",
        "voteCount": 3,
        "content": "To remove backup files older than 90 days from a Cloud Storage bucket and optimize ongoing Cloud Storage spend, you should consider writing a lifecycle management rule in JSON and pushing it to the bucket with gsutil, as described in option B.\n\nLifecycle management rules allow you to automatically delete objects from a Cloud Storage bucket based on age or other criteria, such as the object's storage class. By writing a rule in JSON and pushing it to the bucket with gsutil, you can specify that objects older than 90 days should be deleted, ensuring that the bucket only contains current backup files and minimizing Cloud Storage spend.\n\nOption A, C and D would not be suitable for this use case, as they do not allow you to specify lifecycle management rules that delete objects based on age."
      },
      {
        "date": "2022-12-16T23:39:00.000Z",
        "voteCount": 1,
        "content": "B. Write a lifecycle management rule in JSON and push it to the bucket with gsutil\n\nTo remove backup files older than 90 days from a Cloud Storage bucket, you can use the lifecycle management feature in Cloud Storage. This feature allows you to specify rules to automatically delete objects based on their age."
      },
      {
        "date": "2022-12-05T21:26:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/storage/docs/gsutil/commands/lifecycle"
      },
      {
        "date": "2022-11-25T23:06:00.000Z",
        "voteCount": 1,
        "content": "B. Life cycle management using JSON."
      },
      {
        "date": "2022-11-19T02:19:00.000Z",
        "voteCount": 1,
        "content": "B with JSON option is correct"
      },
      {
        "date": "2022-11-05T02:26:00.000Z",
        "voteCount": 1,
        "content": "B is ok"
      },
      {
        "date": "2022-10-22T11:25:00.000Z",
        "voteCount": 1,
        "content": "B is the right answer"
      },
      {
        "date": "2022-10-16T20:53:00.000Z",
        "voteCount": 1,
        "content": "life cycle management is the answer written in JSON format. JSON is easier to write and read compared to XML which you can not use in commands"
      },
      {
        "date": "2022-10-16T14:12:00.000Z",
        "voteCount": 1,
        "content": "Lifecycle management with JSON is right .. I will go with B"
      },
      {
        "date": "2022-10-13T01:09:00.000Z",
        "voteCount": 1,
        "content": "B. Write a lifecycle management rule in JSON and push it to the bucket with gsutil"
      },
      {
        "date": "2022-09-20T06:53:00.000Z",
        "voteCount": 1,
        "content": "D looks correct schedule cron"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 18,
    "url": "https://www.examtopics.com/discussions/google/view/7154-exam-professional-cloud-architect-topic-1-question-18/",
    "body": "Your company is forecasting a sharp increase in the number and size of Apache Spark and Hadoop jobs being run on your local datacenter. You want to utilize the cloud to help you scale this upcoming demand with the least amount of operations work and code change.<br>Which product should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGoogle Cloud Dataflow",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGoogle Cloud Dataproc\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGoogle Compute Engine",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGoogle Kubernetes Engine"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 13,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-01-11T12:24:00.000Z",
        "voteCount": 19,
        "content": "\"B. Google Cloud Dataproc\" is the answer"
      },
      {
        "date": "2021-01-13T18:18:00.000Z",
        "voteCount": 12,
        "content": "Dataproc is a managed Spark and Hadoop service that lets you take advantage of open source data tools for batch processing, querying, streaming, and machine learning. Dataproc automation helps you create clusters quickly, manage them easily, and save money by turning clusters off when you don't need them. With less time and money spent on administration, you can focus on your jobs and your data.\nhttps://cloud.google.com/dataproc/docs/concepts/overview#:~:text=Dataproc%20is%20a%20managed%20Spark,%2C%20streaming%2C%20and%20machine%20learning.&amp;text=With%20less%20time%20and%20money,your%20jobs%20and%20your%20data."
      },
      {
        "date": "2022-10-16T14:14:00.000Z",
        "voteCount": 1,
        "content": "Agreed"
      },
      {
        "date": "2024-09-06T23:39:00.000Z",
        "voteCount": 1,
        "content": "B is correct because Dataproc is uded for Apache Hadoop and Spark"
      },
      {
        "date": "2023-07-26T05:13:00.000Z",
        "voteCount": 3,
        "content": "Dataflow for data stream and batch.\nDataproc for data process with Apache Spark and Hadoop.\nCompute Engine for VM.\nKubernetes Engine for Kubernetes Cluster with Compute Engine under the hood."
      },
      {
        "date": "2023-03-22T14:45:00.000Z",
        "voteCount": 2,
        "content": "B, Dataproc is Hadoop/Spark managed  service in GCP"
      },
      {
        "date": "2022-12-24T04:56:00.000Z",
        "voteCount": 1,
        "content": "Dataproc is a fully managed and highly scalable service for running Apache Hadoop, Apache Spark, Apache Flink, Presto, and 30+ open source tools and frameworks. Use Dataproc for data lake modernization, ETL, and secure data science, at scale, integrated with Google Cloud, at a fraction of the cost.\n\nhttps://cloud.google.com/dataproc"
      },
      {
        "date": "2022-12-20T01:40:00.000Z",
        "voteCount": 2,
        "content": "To scale the number and size of Apache Spark and Hadoop jobs being run on a local datacenter with the least amount of operations work and code change, you should consider using Google Cloud Dataproc, option B. Google Cloud Dataproc is a fully-managed service that makes it easy to run Apache Spark and Hadoop workloads in the cloud. It is designed to simplify the process of setting up and managing clusters for data processing, and allows you to scale quickly and easily as demand increases.\n\nWith Cloud Dataproc, you can create and delete clusters in just a few minutes, and you can use the familiar Apache Spark and Hadoop APIs and tools to process data. This means that you can utilize the cloud to scale your workloads with minimal changes to your code and operations work.\n\nOption A: Google Cloud Dataflow, option C: Google Compute Engine, and option D: Google Kubernetes Engine, would not be suitable for this use case, as they do not provide the same level of support for running Apache Spark and Hadoop workloads as Cloud Dataproc."
      },
      {
        "date": "2022-11-19T02:20:00.000Z",
        "voteCount": 1,
        "content": "B. Dataproc is managed Apache Spark and Hadoop in GCP"
      },
      {
        "date": "2022-10-16T20:54:00.000Z",
        "voteCount": 1,
        "content": "Dataproc for Hadoop and spark ecosystem"
      },
      {
        "date": "2022-10-13T01:17:00.000Z",
        "voteCount": 1,
        "content": "B. Google Cloud Dataproc"
      },
      {
        "date": "2022-09-20T06:55:00.000Z",
        "voteCount": 1,
        "content": "B data proc for hadoop and spark"
      },
      {
        "date": "2022-06-17T04:06:00.000Z",
        "voteCount": 1,
        "content": "Keyword - Apache Spark and Hadoop jobs - Go with Dataproc"
      },
      {
        "date": "2022-05-31T04:44:00.000Z",
        "voteCount": 1,
        "content": "dataproc"
      },
      {
        "date": "2022-04-20T01:22:00.000Z",
        "voteCount": 2,
        "content": "Google Cloud Dataproc == managed Spark and Hadoop service"
      },
      {
        "date": "2022-04-17T23:09:00.000Z",
        "voteCount": 1,
        "content": "B - Dataproc Lift&amp;Shift of Apache Spark and Hadoop jobs"
      },
      {
        "date": "2022-01-25T05:56:00.000Z",
        "voteCount": 1,
        "content": "B is ok."
      },
      {
        "date": "2021-12-29T02:10:00.000Z",
        "voteCount": 1,
        "content": "B Cloud Data Proc\u3000is correct.\nCloud Data Proc can easly migration form hadoop , spark."
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 19,
    "url": "https://www.examtopics.com/discussions/google/view/7161-exam-professional-cloud-architect-topic-1-question-19/",
    "body": "The database administration team has asked you to help them improve the performance of their new database server running on Google Compute Engine. The database is for importing and normalizing their performance statistics and is built with MySQL running on Debian Linux. They have an n1-standard-8 virtual machine with 80 GB of SSD persistent disk.<br>What should they change to get better performance from this system?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the virtual machine's memory to 64 GB",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new virtual machine running PostgreSQL",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDynamically resize the SSD persistent disk to 500 GB\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate their performance metrics warehouse to BigQuery",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify all of their batch jobs to use bulk inserts into the database"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 32,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 6,
        "isMostVoted": false
      },
      {
        "answer": "E",
        "count": 4,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2019-11-26T11:01:00.000Z",
        "voteCount": 67,
        "content": "Answer is C because persistent disk performance is based on the total persistent disk capacity attached to an instance and the number of vCPUs that the instance has. Incrementing the persistent disk capacity will increment its throughput and IOPS, which in turn improve the performance of MySQL."
      },
      {
        "date": "2019-10-24T14:47:00.000Z",
        "voteCount": 34,
        "content": "Assuming that the database is approaching its hardware limits... both options A and C would improve performance, A would increase number of CPUs and memory, but C would increase memory by more. If it a software problem, it is likly it is a hashing problem (the search and sort algorithms are not specific enough to search within the database). This problem would not be fixed just by migrating to PostgreSQL or BigQuery but modifying the inserts would help the situation because it would entail specifications of data lookups. However, it wouldn't help with search performance just inserts and it doesn't help in normalization. So B, D, and E are eliminated. Since statistics is based on sets, the larger the number of sets the better the predictions. This means that the largest amount of memory would not only increase computer performance but also knowledge enhancements. So C beats A."
      },
      {
        "date": "2021-03-04T07:53:00.000Z",
        "voteCount": 8,
        "content": "C.  universal truth - OLTP D/B performance is depended on IOPs. SSD is the best solution for higher IOPs. In GCP bigger the disk size higher the IOPs."
      },
      {
        "date": "2020-12-06T04:47:00.000Z",
        "voteCount": 6,
        "content": "Also, if you increased the memory size, it would not be a n1-standard-8 anymore. You should eventually change machine type, not simply increase memory."
      },
      {
        "date": "2020-08-05T21:55:00.000Z",
        "voteCount": 8,
        "content": "C is ok."
      },
      {
        "date": "2021-12-03T10:06:00.000Z",
        "voteCount": 2,
        "content": "When you increase the memory yo need to shutdown the machine, but when you increase the disk, it is not necessary. Answer is B."
      },
      {
        "date": "2024-08-19T09:15:00.000Z",
        "voteCount": 1,
        "content": "Repectfully, this isn't accurate.  On Google Compute Engine, you can often increase the memory of a running virtual machine without needing to shut it down. This is known as live migration or memory hot-add."
      },
      {
        "date": "2024-06-14T02:01:00.000Z",
        "voteCount": 1,
        "content": "Since its using SQL, so there will be a Maintenance Window, so this change can be implemented during the downtime( also there is no mention that the system should be always avaliable)"
      },
      {
        "date": "2022-05-17T09:26:00.000Z",
        "voteCount": 1,
        "content": "there isn't \"without downtime\""
      },
      {
        "date": "2024-08-08T00:11:00.000Z",
        "voteCount": 1,
        "content": "Choose C"
      },
      {
        "date": "2024-07-09T05:13:00.000Z",
        "voteCount": 2,
        "content": "increase size will not increase performance, it either increase RAM or serverless.  A or D.  if no cost concern will pick D"
      },
      {
        "date": "2024-01-21T06:26:00.000Z",
        "voteCount": 1,
        "content": "I was looking for CloudSQL in options, since it is not there, C is best"
      },
      {
        "date": "2024-01-13T07:53:00.000Z",
        "voteCount": 1,
        "content": "The fact that the database is used for importing and normalizing performance statistics suggests frequent data insertions. Optimizing this process through bulk inserts directly addresses a likely performance bottleneck."
      },
      {
        "date": "2024-01-09T10:30:00.000Z",
        "voteCount": 1,
        "content": "The answer according to Google is A. This question is part of the Google's sample questions for the certification."
      },
      {
        "date": "2024-06-09T12:02:00.000Z",
        "voteCount": 1,
        "content": "Yes, it is, and says it is C."
      },
      {
        "date": "2023-10-08T04:03:00.000Z",
        "voteCount": 3,
        "content": "I see most of the people here replying C, but I do not think that the size of the disk we bring much gains in performance. D, yes, seems to me that will bring much improvements in performance, management, operations and cost. So B,  Migrate their performance metrics warehouse to BigQuery"
      },
      {
        "date": "2023-08-18T03:35:00.000Z",
        "voteCount": 1,
        "content": "I would go with option E because Bulk Insert improves performance drastically unless it is been implemented already."
      },
      {
        "date": "2023-07-26T05:20:00.000Z",
        "voteCount": 4,
        "content": "Increasing disk size will also increase its performance.\n\nhttps://cloud.google.com/compute/docs/disks/performance#optimize_disk_performance"
      },
      {
        "date": "2023-04-18T01:44:00.000Z",
        "voteCount": 3,
        "content": "C. Dynamically resize the SSD persistent disk to 500 GB\n\nBy increasing the size of the SSD persistent disk, the database server can achieve better performance. A larger SSD persistent disk provides higher IOPS (input/output operations per second) and throughput, allowing for faster read and write operations. This can help improve the performance of the MySQL database server running on the Google Compute Engine instance."
      },
      {
        "date": "2023-03-24T22:34:00.000Z",
        "voteCount": 1,
        "content": "On another website I found the question with the hint \"you are not allowed to reboot the VM before next maintenance window\". That makes it more clear --&gt; C."
      },
      {
        "date": "2023-03-13T01:47:00.000Z",
        "voteCount": 3,
        "content": "E. Modify all of their batch jobs to use bulk inserts into the database: This can be a very effective solution for improving performance. Bulk inserts can greatly reduce the number of round-trips to the database, which can help to minimize latency and improve overall throughput.\n\nTherefore, option E is the best choice for improving performance in this scenario."
      },
      {
        "date": "2022-12-10T03:28:00.000Z",
        "voteCount": 3,
        "content": "in option C - even increasing disc can gain performance - that will take few months to face new limits. mySQL is not desiged for OLAP/analytics - but OLTP.\nso  I vote on D"
      },
      {
        "date": "2022-11-19T02:27:00.000Z",
        "voteCount": 2,
        "content": "Correct answer is C. Increased disk capacity improved I/O and direct impacts the performance"
      },
      {
        "date": "2022-10-22T10:22:00.000Z",
        "voteCount": 11,
        "content": "C, because...\nN1 8cpu max IOPS = 15,000 https://cloud.google.com/compute/docs/disks/performance#n1_vms\n\nSSD persistent disks can reach up to 30 IOPS per GB of disk. https://cloud.google.com/compute/docs/disks/performance#example\n80 GB X 30 IOPS = 2,400 IOPS\n500 GB  (answer C) X 30 IOPS = 15,000 IOPS  = N1 8 cpu max IOPS"
      },
      {
        "date": "2022-10-16T20:58:00.000Z",
        "voteCount": 5,
        "content": "adding memory to VM will need to be shut down which means the business will be impacted, not good for any option \nremember this for your exam"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 20,
    "url": "https://www.examtopics.com/discussions/google/view/22386-exam-professional-cloud-architect-topic-1-question-20/",
    "body": "You want to optimize the performance of an accurate, real-time, weather-charting application. The data comes from 50,000 sensors sending 10 readings a second, in the format of a timestamp and sensor reading.<br>Where should you store the data?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGoogle BigQuery",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGoogle Cloud SQL",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGoogle Cloud Bigtable\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGoogle Cloud Storage"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-07-04T07:28:00.000Z",
        "voteCount": 12,
        "content": "C. Google Cloud Bigtable"
      },
      {
        "date": "2022-09-10T01:33:00.000Z",
        "voteCount": 4,
        "content": "I too got this question in 10-09-22 exam with similar option and result is pass"
      },
      {
        "date": "2024-02-21T12:23:00.000Z",
        "voteCount": 2,
        "content": "Bigtable - NoSQL, high-throughput, low-latency, making it suitable for storing time-series data from sensors"
      },
      {
        "date": "2023-03-22T15:02:00.000Z",
        "voteCount": 2,
        "content": "BigTable is NoSQL for IoT"
      },
      {
        "date": "2023-03-02T20:58:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2022-12-20T01:51:00.000Z",
        "voteCount": 3,
        "content": "To optimize the performance of an accurate, real-time, weather-charting application that receives data from 50,000 sensors sending 10 readings per second, it would be most appropriate to store the data in a distributed, horizontally scalable, NoSQL database such as Google Cloud Bigtable\nOther options, such as Google BigQuery, Google Cloud SQL, and Google Cloud Storage, may not be as well-suited for handling high volumes of real-time data and may not provide the same level of performance and scalability as Google Cloud Bigtable."
      },
      {
        "date": "2022-11-25T23:12:00.000Z",
        "voteCount": 1,
        "content": "C. Bigtable, IoT data."
      },
      {
        "date": "2022-11-19T02:31:00.000Z",
        "voteCount": 1,
        "content": "C Bigtable"
      },
      {
        "date": "2022-10-22T11:26:00.000Z",
        "voteCount": 1,
        "content": "C bigtable right answer"
      },
      {
        "date": "2022-10-16T21:00:00.000Z",
        "voteCount": 4,
        "content": "real-time, IoT, time series and huge writes are some of the keywords to look after for Bigtable"
      },
      {
        "date": "2022-10-13T01:28:00.000Z",
        "voteCount": 1,
        "content": "C. Google Cloud Bigtable"
      },
      {
        "date": "2022-09-20T06:57:00.000Z",
        "voteCount": 1,
        "content": "C big table for IOT data"
      },
      {
        "date": "2022-08-25T20:17:00.000Z",
        "voteCount": 1,
        "content": "Google Cloud Bigtable"
      },
      {
        "date": "2022-06-17T04:11:00.000Z",
        "voteCount": 2,
        "content": "Keyword - Timestamp - Big table"
      },
      {
        "date": "2022-06-06T00:49:00.000Z",
        "voteCount": 1,
        "content": "Go for c"
      },
      {
        "date": "2022-04-20T01:47:00.000Z",
        "voteCount": 1,
        "content": "C. Google Cloud Bigtable is the Best Practice option"
      },
      {
        "date": "2022-02-19T20:36:00.000Z",
        "voteCount": 2,
        "content": "Ans c - when ever thier is input from IOT devices across and time series data which is huge go for big table in gcp"
      },
      {
        "date": "2022-10-16T14:17:00.000Z",
        "voteCount": 1,
        "content": "Big Table is right choice, hence C is correct"
      },
      {
        "date": "2022-01-19T23:18:00.000Z",
        "voteCount": 1,
        "content": "Google Cloud Big Table is best for the use-case to store the time-series data, so C is correct"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 21,
    "url": "https://www.examtopics.com/discussions/google/view/7128-exam-professional-cloud-architect-topic-1-question-21/",
    "body": "Your company's user-feedback portal comprises a standard LAMP stack replicated across two zones. It is deployed in the us-central1 region and uses autoscaled managed instance groups on all layers, except the database. Currently, only a small group of select customers have access to the portal. The portal meets a<br>99,99% availability SLA under these conditions. However next quarter, your company will be making the portal available to all users, including unauthenticated users. You need to develop a resiliency testing strategy to ensure the system maintains the SLA once they introduce additional user load.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCapture existing users input, and replay captured user load until autoscale is triggered on all layers. At the same time, terminate all resources in one of the zones",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate synthetic random user input, replay synthetic load until autoscale logic is triggered on at least one layer, and introduce \u05d2\u20acchaos\u05d2\u20ac to the system by terminating random resources on both zones\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExpose the new system to a larger group of users, and increase group size each day until autoscale logic is triggered on all layers. At the same time, terminate random resources on both zones",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCapture existing users input, and replay captured user load until resource utilization crosses 80%. Also, derive estimated number of users based on existing user's usage of the app, and deploy enough resources to handle 200% of expected load"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 42,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 9,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2019-10-26T12:44:00.000Z",
        "voteCount": 90,
        "content": "resilience test is not about load, is about terminate resources and service not affected. Think it's B. The best for resilience in to introduce chaos in the infraestructure"
      },
      {
        "date": "2020-01-19T08:13:00.000Z",
        "voteCount": 20,
        "content": "I agree with @jcmoranp, B) is correct for more info - https://cloud.google.com/solutions/scalable-and-resilient-apps#test_your_resilience"
      },
      {
        "date": "2021-11-12T11:56:00.000Z",
        "voteCount": 8,
        "content": "Isn't A superior in one way.  It will demonstrate that the app is regionally redundant by demonstrating it can survive the loss of an entire zone.  B only demonstrates the app is zonally redundant and can lose a random instance here and there within individual zones which is not that resilient.  Thoughts?"
      },
      {
        "date": "2022-10-19T06:30:00.000Z",
        "voteCount": 6,
        "content": "No. It is only terminating the service in ONE zone. B caters for terminating the service in both zones randomly. You want to be able to test resiliency when either zone has an outage."
      },
      {
        "date": "2020-11-22T12:26:00.000Z",
        "voteCount": 60,
        "content": "Will go with A. Reason: \n1. SLA in question is about the Availability (The portal meets a\n99,99% availability SLA under these conditions.) therefore maintaining SLA means Availability.\n2. Its a user-feedback portal and type of user input is going to be similar or same (A is capturing the user input and replaying it).\n\nWhy not B: \nThe infrastructure is using MIG (Instances created using templates) most likely to be used with Health Check and killing random VMs cannot test the availability (neither affect the availability as health check will immediately kill the effected Instances and create the other one.)\nWhy not D:\nSLA is about Availability not reliability or scaling. (As all of it does work hand to hand but still major focus should be on availability.)\n\n--- IF AGREE PLEASE UP VOTE TO MAKE IT CLEAR FOR THE OTHERS --- Thank you."
      },
      {
        "date": "2022-08-17T02:45:00.000Z",
        "voteCount": 10,
        "content": "Only problem with A is that it says \"replay captured user load\". We are not testing for the incoming unpredictable load due to the inclusion of unauthenticated users and something that we haven't captured earlier.\n\nOption B covers breadth and depth for the desired SLA."
      },
      {
        "date": "2022-08-22T11:18:00.000Z",
        "voteCount": 3,
        "content": "What does \"replay captured user load\" mean?"
      },
      {
        "date": "2021-01-03T12:02:00.000Z",
        "voteCount": 1,
        "content": "valuable input in terms of 'availability'. did you select this answer in exam too?"
      },
      {
        "date": "2021-09-07T02:32:00.000Z",
        "voteCount": 1,
        "content": "We are talking about resilience testing where as SLA is an argument of the system."
      },
      {
        "date": "2021-09-07T02:33:00.000Z",
        "voteCount": 1,
        "content": "And resilience means the capacity to recover from failure."
      },
      {
        "date": "2021-11-15T11:06:00.000Z",
        "voteCount": 1,
        "content": "A ensures the app can withstand the loss of a whole Zone which I think is important as well."
      },
      {
        "date": "2024-08-26T01:29:00.000Z",
        "voteCount": 1,
        "content": "A: requirements states \"all layers\" and \"resiliency testing\""
      },
      {
        "date": "2024-05-25T12:19:00.000Z",
        "voteCount": 1,
        "content": "I would go with A.\nThis solution test autoscale policy of each layer (not only one as option B refers).\nAlso, it propose a regional shutdown. This is a very good test commonly requested if your application is geo-redundant. In crontast, option B propose random termination of resources, not a bad practice but a little bit vague that can be implemented terrible wrong (for example you do not kill the interesting services or you kill the same service in both regions, thus generating a blackout)"
      },
      {
        "date": "2024-02-21T13:13:00.000Z",
        "voteCount": 3,
        "content": "We need to do 1. load testing and 2. reliability test ( failover redundency )\n B does both\nA only tests one zone\nC impacts real user experience\nD 200% not necessary"
      },
      {
        "date": "2023-10-04T04:08:00.000Z",
        "voteCount": 1,
        "content": "You need to develop a resiliency testing strategy to ensure the system maintains the SLA once they introduce additional user load.\n\nNeed to maintain SLA of 99.9% means multiple zones, resilience means fault tolerance. Teminating all resources in one zone is also creating a chaos."
      },
      {
        "date": "2023-08-22T09:53:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2023-06-13T21:01:00.000Z",
        "voteCount": 2,
        "content": "Option D is the best resiliency testing strategy in this scenario as it ensures that the system is tested with actual user data, takes into account the expected increase in user load, and ensures that the system is adequately scaled to handle the anticipated load."
      },
      {
        "date": "2023-08-18T23:17:00.000Z",
        "voteCount": 1,
        "content": "Do not agree. B is 100% correct"
      },
      {
        "date": "2023-09-24T14:25:00.000Z",
        "voteCount": 1,
        "content": "But this would assume that the user load will not change; plus, the current application is visible only to a small group of select customers - this is the current production setup. The deployment should be prepared for all existing users plus unauthenticated users, and the load increase is unknown, so testing for 200% of \"expected load\" is very ambiguous."
      },
      {
        "date": "2023-04-18T01:46:00.000Z",
        "voteCount": 5,
        "content": "B. Create synthetic random user input, replay synthetic load until autoscale logic is triggered on at least one layer, and introduce \u05d2\u20acchaos\u05d2\u20ac to the system by terminating random resources on both zones.\n\nBy creating synthetic random user input and replaying the load, you can simulate the expected increased user traffic and trigger the autoscale logic on different layers of the application. Introducing chaos to the system by terminating random resources in both zones helps test the resiliency and redundancy of the system under stress. This strategy will help ensure that the system can maintain the 99.99% availability SLA when subjected to additional user load."
      },
      {
        "date": "2023-03-24T01:56:00.000Z",
        "voteCount": 2,
        "content": "chaos == test resilience for google"
      },
      {
        "date": "2023-02-23T18:51:00.000Z",
        "voteCount": 2,
        "content": "This is chaos engineering used by Netflix. https://netflixtechblog.com/tagged/chaos-engineering"
      },
      {
        "date": "2023-01-09T10:41:00.000Z",
        "voteCount": 4,
        "content": "chaos == checking resilience"
      },
      {
        "date": "2022-12-16T03:58:00.000Z",
        "voteCount": 1,
        "content": "right thought ,"
      },
      {
        "date": "2022-11-19T02:47:00.000Z",
        "voteCount": 2,
        "content": "B is correct; Using synthetic/random input is recommended. Chaos Engineering/Symian Army from Netflix is one of the proven mechanism to test the resilience of the application."
      },
      {
        "date": "2022-11-10T03:47:00.000Z",
        "voteCount": 2,
        "content": "B is ok"
      },
      {
        "date": "2022-10-19T06:31:00.000Z",
        "voteCount": 3,
        "content": "B caters for terminating the service in both zones randomly. You want to be able to test resiliency when either zone has an outage."
      },
      {
        "date": "2022-10-16T21:09:00.000Z",
        "voteCount": 1,
        "content": "chaos engineering is the buzzword to look after \nAnswer is B"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 22,
    "url": "https://www.examtopics.com/discussions/google/view/54406-exam-professional-cloud-architect-topic-1-question-22/",
    "body": "One of the developers on your team deployed their application in Google Container Engine with the Dockerfile below. They report that their application deployments are taking too long.<br><img src=\"/assets/media/exam-media/04339/0008300001.png\" class=\"in-exam-image\"><br>You want to optimize this Dockerfile for faster deployment times without adversely affecting the app's functionality.<br>Which two actions should you take? (Choose two.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRemove Python after running pip",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRemove dependencies from requirements.txt",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a slimmed-down base image like Alpine Linux\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse larger machine types for your Google Container Engine node pools",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCopy the source after he package dependencies (Python and pip) are installed\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "CE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "CE",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-06-26T03:49:00.000Z",
        "voteCount": 55,
        "content": "C &amp; E:\nC: Smaller the base image with minimum dependency faster the container will start\nE:  Docker image build uses caching. Docker Instructions sequence matter because \napplication\u2019s dependencies change less frequently than the Python code which will help to reuse the cached layer of dependency and only add new layer for code change for Python Source code."
      },
      {
        "date": "2021-11-21T19:48:00.000Z",
        "voteCount": 12,
        "content": "C &amp; E are the correct answers. \nKindly refer - https://www.docker.com/blog/intro-guide-to-dockerfile-best-practices/"
      },
      {
        "date": "2024-04-23T05:52:00.000Z",
        "voteCount": 1,
        "content": "Hi everyone, I have a doubt. The question talks about \"deployment\" not \"build\". CE are more correct, in my opinion, to accelerate the build phase (and application management) rather than the simple deployment (with docker swarm, simple docker, kubernetes etc etc)"
      },
      {
        "date": "2023-12-22T03:24:00.000Z",
        "voteCount": 3,
        "content": "C&amp;E\nC - Use small images.\nE - \"Try to make expensive steps appear near the beginning of the Dockerfile. Steps that change often should appear near the end of the Dockerfile\" \nhttps://docs.docker.com/build/cache/"
      },
      {
        "date": "2023-09-24T18:03:00.000Z",
        "voteCount": 1,
        "content": "C is an obvious choice, as an optimized image will be much better for a container than the one in the file. For E, I found an explanation here about COPY that helped me confirm that's the other solution - https://test-dockerrr.readthedocs.io/en/latest/userguide/eng-image/dockerfile_best-practices/#:~:text=required%20files%20change.-,For%20example%3A,-COPY%20requirements.txt."
      },
      {
        "date": "2023-07-06T17:48:00.000Z",
        "voteCount": 4,
        "content": "I know C &amp; E makes the most sense, but the Dockerfile is has a bug. If you don't copy requirements.txt from source, you have nothing to pip install. The line after FROM should be: \n\nCOPY requirements.txt requirements.txt"
      },
      {
        "date": "2023-03-29T15:41:00.000Z",
        "voteCount": 1,
        "content": "The two actions that should be taken to optimize the Dockerfile for faster deployment times without adversely affecting the app's functionality are:\n\nB. Remove dependencies from requirements.txt: The requirements.txt file should only contain necessary dependencies to reduce the number of packages to be installed.\n\nC. Use a slimmed-down base image like Alpine Linux: The Alpine Linux image is smaller than Ubuntu and has a smaller attack surface, which reduces the container's build time and image size.\n\nTherefore, options A, D, and E are not correct as they do not directly address the issue of slow deployment times caused by a bloated Dockerfile."
      },
      {
        "date": "2023-09-24T17:26:00.000Z",
        "voteCount": 1,
        "content": "But how do you know that requirements.txt has other dependencies? We don't have the file to confirm if there are unnecessary dependencies. Assuming that we only have the necessary ones, removing dependencies would break the deployment."
      },
      {
        "date": "2023-03-22T15:14:00.000Z",
        "voteCount": 2,
        "content": "C: use smaller image decrease pull time\nE:  optimize build time using previous cache layer image. generate new layer only for a different app code and requirements\n\nA:  can't remove python\nB:  the developer choose right deps\nD: changing istance type don't directly reduce deploy time"
      },
      {
        "date": "2022-12-20T23:30:00.000Z",
        "voteCount": 6,
        "content": "C &amp; E\nUsing a slimmed-down base image like Alpine Linux can help reduce the size of your Docker image, which can lead to faster deployment times. Alpine Linux is a lightweight Linux distribution that is often used as a base image for Docker images because of its small size.\n\nAdditionally, copying the source code after installing the package dependencies can help reduce the image build time because the dependencies will only need to be installed once, rather than every time the source code is changed. This can lead to faster deployment times because the image build process will be faster."
      },
      {
        "date": "2022-12-20T23:30:00.000Z",
        "voteCount": 1,
        "content": "It is not recommended to remove dependencies from the requirements.txt file or remove Python after running pip, as this could adversely affect the functionality of the application. Similarly, using larger machine types for your Google Container Engine node pools may not directly affect the deployment times of your application, as the deployment times are primarily dependent on the size and complexity of the Docker image being deployed."
      },
      {
        "date": "2022-10-21T02:44:00.000Z",
        "voteCount": 2,
        "content": "B &amp; E\nbase image is already cached - so no improvement in build time\nB is about removing unnecessary dependencies and not about all of them\ni remember saw this question on google's site - BE are correct"
      },
      {
        "date": "2022-10-16T14:20:00.000Z",
        "voteCount": 1,
        "content": "CE and is perfect"
      },
      {
        "date": "2022-10-13T02:00:00.000Z",
        "voteCount": 1,
        "content": "C. Use a slimmed-down base image like Alpine Linux\nE. Copy the source after he package dependencies (Python and pip) are installed"
      },
      {
        "date": "2022-07-24T00:17:00.000Z",
        "voteCount": 1,
        "content": "vote C, E\nhttps://cloud.google.com/architecture/best-practices-for-building-containers"
      },
      {
        "date": "2022-06-30T07:16:00.000Z",
        "voteCount": 2,
        "content": "As far as I know, it is necessary to copy the requirements.txt first in order to run pip, it is not possible to run pip without first copying the requirements.txt. if they copy requirements.txt first then E would work."
      },
      {
        "date": "2022-05-07T07:31:00.000Z",
        "voteCount": 3,
        "content": "By means of elimination A  B, dont make sence.D . is optimizing the mackie not script. Hence we are left with C&amp; E"
      },
      {
        "date": "2022-05-01T01:41:00.000Z",
        "voteCount": 1,
        "content": "C.Alpine is a ligweight Linux distro, with smaller image E. Pushing often changing files down the Dockerfile helps reducing image layers variations. Both help faster image pull operations"
      },
      {
        "date": "2022-02-19T20:48:00.000Z",
        "voteCount": 4,
        "content": "A -invalid\nB-dependencies are required\nC-it will help as it is one of the best practices to make use of lighter image if possible\nD-Not helpful\nE-is the best practice to do the steps that changes more frequently at the end . so copy . should be performed at last as it will be changing more frequently and we can make use of docker caching \nhence Answer E is most then C"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 23,
    "url": "https://www.examtopics.com/discussions/google/view/7167-exam-professional-cloud-architect-topic-1-question-23/",
    "body": "Your solution is producing performance bugs in production that you did not see in staging and test environments. You want to adjust your test and deployment procedures to avoid this problem in the future.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy fewer changes to production",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy smaller changes to production",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the load on your test and staging environments\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy changes to a small subset of users before rolling out to production"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 64,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 37,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-01-14T01:35:00.000Z",
        "voteCount": 80,
        "content": "Question Statement: You want to adjust your test and deployment procedures to avoid this problem in the future\n\nSo based on this, I think the option \"C\" is correct, since it is the only one talking about doing changes in the test environment."
      },
      {
        "date": "2024-06-12T08:55:00.000Z",
        "voteCount": 2,
        "content": "There is no indication given anywhere that the load is the problem or that the bugs are a result of load and not some other issue encountered when using a specific feature."
      },
      {
        "date": "2020-09-25T23:17:00.000Z",
        "voteCount": 20,
        "content": "C. Increase the load on your test and staging environments.\n\nAs you have pointed out in \"Question Statement\", I do not see C covering \"deployment procedures\". Test and Staging environment is more on testing, but not about deployment procedure to production.\n\nSo, the only option that cover test and deployment is D. (Yes, kind of unacceptable to have the users to do \"testing\", but we make it \"ok\" by calling it \"canary deployment\")"
      },
      {
        "date": "2023-10-15T06:05:00.000Z",
        "voteCount": 9,
        "content": "With canary deployment we expose the new version to a small portion of users. With this approach maybe we don't see performance bugs in the canary release, since we don't have the 100% of traffic on the canary. But when we migrate the 100% of traffic to the new release (previous canary) we can see performance bugs."
      },
      {
        "date": "2021-12-06T15:26:00.000Z",
        "voteCount": 9,
        "content": "The answer is D"
      },
      {
        "date": "2021-12-30T03:40:00.000Z",
        "voteCount": 18,
        "content": "\"Your solution is producing performance bugs in production...\" - I don't see how \"D\" would help to detect performance bugs. \n- \"C\" looks more adequate."
      },
      {
        "date": "2019-10-24T15:44:00.000Z",
        "voteCount": 38,
        "content": "A wouldn't prevent the bugs, it would just avoid them. B would help with root-cause analysis because it'd be a smaller change to review. C would test the performance of the system at its peak processing rates, so this assumes the bugs in production only occur because of usage. D would allow you to test the new code against smaller user sets to see if it occurs then, and if it still does you know it is not because of more user responses. So it's a tossup between C and D, D would be the cheaper/quicker answer so I'd choose D first then C if it's because of usage."
      },
      {
        "date": "2021-03-04T10:00:00.000Z",
        "voteCount": 7,
        "content": "D, canary rollout"
      },
      {
        "date": "2022-08-12T23:12:00.000Z",
        "voteCount": 1,
        "content": "It has nothing to do with the \"performance bugs\""
      },
      {
        "date": "2022-08-12T23:08:00.000Z",
        "voteCount": 2,
        "content": "According to the question, [Your solution is producing \"performance\" bugs in production], so I think it is about the load. Plus canary test will not reproduce the bugs related to high load, I  vote for C"
      },
      {
        "date": "2020-08-06T01:08:00.000Z",
        "voteCount": 18,
        "content": "The question is about the performance of the existing Code that they did not detect in Test environments . This is not about new API release . In order to test the performance they should increase the load in test environment and hence answer C."
      },
      {
        "date": "2022-10-16T14:23:00.000Z",
        "voteCount": 2,
        "content": "C is the best"
      },
      {
        "date": "2024-10-13T04:11:00.000Z",
        "voteCount": 1,
        "content": "It's D"
      },
      {
        "date": "2024-09-03T02:47:00.000Z",
        "voteCount": 2,
        "content": "C because The performance issues in production might not have been seen in staging or test environments because the load (number of users, transactions, data volume, etc.) in those environments is not representative of the load in production. By increasing the load on your test and staging environments to match or exceed production levels, you can better simulate real-world conditions and catch performance issues before deployment"
      },
      {
        "date": "2024-08-08T00:25:00.000Z",
        "voteCount": 1,
        "content": "I will choose D. \nC: The question does not say the error caused by the load."
      },
      {
        "date": "2024-06-24T07:00:00.000Z",
        "voteCount": 3,
        "content": "Without overthinking the wording, canary (and similar) deployment methodologies are often recommended in Google documentation, whereas increasing load in dev environments aren't. (My $0.02...)"
      },
      {
        "date": "2024-06-04T21:56:00.000Z",
        "voteCount": 1,
        "content": "Me too! I can't see how a \"performance bug\" might be mitigated via a canary deployment.\nHowever, I see that C doesn't cover the \"deployment\" part of the question, then I deduce that the question is ambiguously formulated."
      },
      {
        "date": "2024-05-25T12:27:00.000Z",
        "voteCount": 1,
        "content": "Although all answers can be good practices, I think only option C address the problem described."
      },
      {
        "date": "2024-03-12T12:40:00.000Z",
        "voteCount": 1,
        "content": "D. Deploy changes to a small subset of users before rolling out to production.\n\nThis approach, known as canary releasing or canary deployment, involves rolling out changes to a small group of users before deploying them to the entire user base. It is a very effective way to catch performance issues that might not have been apparent during testing.\nC. Increase the load on your test and staging environments: This is definitely a good practice, as it can help simulate production-like conditions more closely. However, it may still not capture all real-world scenarios and user behaviors that can lead to performance issues."
      },
      {
        "date": "2023-12-11T10:59:00.000Z",
        "voteCount": 2,
        "content": "C looks good to me"
      },
      {
        "date": "2023-12-06T18:03:00.000Z",
        "voteCount": 5,
        "content": "Canary deployment is perfect to test new feature but to do stress testing, I do development for 25 years, when we want to resolve performance and scalability issues we do stress and load testing in pre prod environment, something you can't do by exposing the new feature to subset of users."
      },
      {
        "date": "2023-12-05T07:55:00.000Z",
        "voteCount": 1,
        "content": "as for C: synthetic load may not cover all scenarios. For D obviously we need to have monitoring in place to see if e.g. system load or response times increased after canary deployment"
      },
      {
        "date": "2023-10-03T00:13:00.000Z",
        "voteCount": 1,
        "content": "It talks about test and deployment Procedures NOT Environment. Answer is D"
      },
      {
        "date": "2023-09-25T17:57:00.000Z",
        "voteCount": 1,
        "content": "It is really about Canary deployment"
      },
      {
        "date": "2023-09-24T18:12:00.000Z",
        "voteCount": 3,
        "content": "I'm going for D. According to ChatGPT:\n\nD. The best approach to avoid performance bugs in production that weren't detected in staging and test environments is to gradually roll out changes to a small subset of users before deploying them to production. This way, you can identify and address any issues that may arise in a controlled environment before affecting all users. \n\nWhy not C: Increasing the load on your test and staging environments, as suggested in option C, can be a valuable strategy for detecting certain types of performance issues related to scalability and load handling. However, it may not address all types of performance bugs or issues that are specific to the production environment.\n\nA and B seem a bit obviously wrong. A is incorrect because it would be incompatible with CI/CD, and deploying fewer changes may still introduce bugs regardless of the low frequency. B would help root causing, but may also introduce new bugs that are exclusive to production."
      },
      {
        "date": "2023-09-24T02:28:00.000Z",
        "voteCount": 2,
        "content": "From the question we don't know anything about reasons of performance problems in production environemnt. We also don't know anything about tests that were performed in test in staging environments. There is no reason to believe that load in test environment is not sufficient. It is always possible that some performance problems occurs only in production environment. It is also not economically reasonable to reproduce full production load in test environemnt. Taking all of this into account I am incling to answer D."
      },
      {
        "date": "2023-09-08T10:47:00.000Z",
        "voteCount": 1,
        "content": "C if you test with a subset group of users you cant test the performance properly"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 24,
    "url": "https://www.examtopics.com/discussions/google/view/10898-exam-professional-cloud-architect-topic-1-question-24/",
    "body": "A small number of API requests to your microservices-based application take a very long time. You know that each request to the API can traverse many services.<br>You want to know which service takes the longest in those cases.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet timeouts on your application so that you can fail requests faster",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSend custom metrics for each of your requests to Stackdriver Monitoring",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Stackdriver Monitoring to look for insights that show when your API latencies are high",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstrument your application with Stackdriver Trace in order to break down the request latencies at each microservice\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 16,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2019-12-24T23:18:00.000Z",
        "voteCount": 23,
        "content": "D is correct !"
      },
      {
        "date": "2020-08-05T22:25:00.000Z",
        "voteCount": 9,
        "content": "D is ok"
      },
      {
        "date": "2021-03-04T10:01:00.000Z",
        "voteCount": 4,
        "content": "D, trace is just for latency testing."
      },
      {
        "date": "2022-12-20T23:35:00.000Z",
        "voteCount": 8,
        "content": "D. Instrument your application with Stackdriver Trace in order to break down the request latencies at each microservice\n\nStackdriver Trace is a distributed tracing system that allows you to understand the relationships between requests and the various microservices that they touch as they pass through your application. By instrumenting your application with Stackdriver Trace, you can get a detailed breakdown of the latencies at each microservice, which can help you identify which service is taking the longest in those cases where a small number of API requests take a very long time.\n\nSetting timeouts on your application or sending custom metrics to Stackdriver Monitoring may not provide the level of detail that you need to identify the specific service that is causing the latency issues. Looking for insights in Stackdriver Monitoring may also not provide the necessary level of detail, as it may not show the individual latencies at each microservice."
      },
      {
        "date": "2024-08-08T00:31:00.000Z",
        "voteCount": 1,
        "content": "Choose D"
      },
      {
        "date": "2024-05-25T12:35:00.000Z",
        "voteCount": 4,
        "content": "This question is not updated. It will be refered as Cloud Trace as part of Google Cloud Operation suite"
      },
      {
        "date": "2024-05-25T12:32:00.000Z",
        "voteCount": 1,
        "content": "This question is not updated. Stackdriver is now called \"Google Cloud operations\""
      },
      {
        "date": "2023-07-26T07:32:00.000Z",
        "voteCount": 1,
        "content": "Trace is a signal that can be used to follow the program's data and flow, including the duration of the program's components."
      },
      {
        "date": "2023-06-01T10:48:00.000Z",
        "voteCount": 2,
        "content": "D should be correct, the headline in GC trace documentation says it all: \"Cloud Trace is a distributed tracing system for Google Cloud that collects latency data from applications and displays it in near real-time in the Google Cloud Console.\""
      },
      {
        "date": "2023-06-01T10:49:00.000Z",
        "voteCount": 1,
        "content": "even got confused with the C ... after reading the conversation and reference doc D make sense to me."
      },
      {
        "date": "2023-03-22T15:17:00.000Z",
        "voteCount": 1,
        "content": "D is the correct answer"
      },
      {
        "date": "2023-03-03T03:36:00.000Z",
        "voteCount": 1,
        "content": "Why not C?"
      },
      {
        "date": "2022-12-19T06:03:00.000Z",
        "voteCount": 1,
        "content": "D is the correct answer"
      },
      {
        "date": "2022-11-19T03:33:00.000Z",
        "voteCount": 1,
        "content": "Stacdrive Trace would trace the APIs and helps to identify the bottleneck"
      },
      {
        "date": "2022-11-18T00:33:00.000Z",
        "voteCount": 1,
        "content": "D is correct trace would report to the latency"
      },
      {
        "date": "2022-10-22T11:56:00.000Z",
        "voteCount": 1,
        "content": "D is correct trace would report to the latency"
      },
      {
        "date": "2022-10-13T02:17:00.000Z",
        "voteCount": 2,
        "content": "D. Instrument your application with Stackdriver Trace in order to break down the request latencies at each microservice"
      },
      {
        "date": "2022-10-16T14:25:00.000Z",
        "voteCount": 1,
        "content": "this is the best option to find more details"
      },
      {
        "date": "2022-08-23T20:16:00.000Z",
        "voteCount": 2,
        "content": "Instrument your application with Stackdriver Trace in order to break down the request latencies at each microservice"
      },
      {
        "date": "2022-08-17T01:03:00.000Z",
        "voteCount": 1,
        "content": "D is correct !"
      },
      {
        "date": "2022-05-07T07:43:00.000Z",
        "voteCount": 1,
        "content": "Latency issue in your applicant. trace loy is way to go. Hence D"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 25,
    "url": "https://www.examtopics.com/discussions/google/view/7118-exam-professional-cloud-architect-topic-1-question-25/",
    "body": "During a high traffic portion of the day, one of your relational databases crashes, but the replica is never promoted to a master. You want to avoid this in the future.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a different database",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChoose larger instances for your database",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate snapshots of your database more regularly",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement routinely scheduled failovers of your databases\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 57,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 30,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2019-11-26T16:26:00.000Z",
        "voteCount": 95,
        "content": "Answer is D"
      },
      {
        "date": "2019-12-13T07:58:00.000Z",
        "voteCount": 20,
        "content": "Yep, +1 for D"
      },
      {
        "date": "2019-10-25T06:35:00.000Z",
        "voteCount": 44,
        "content": "@chiar, I agree the question i s not clear. In GCP larger instances have larger number of CPUs, Memory and come with their own private network. So increases the instance size would help prevent the need for failover during high traffic times. However, routinely scheduled failovers would allow the team to test the failover when it is not requried. This would make sure it is working when it is required."
      },
      {
        "date": "2019-12-02T18:19:00.000Z",
        "voteCount": 9,
        "content": "exactly how do you know the optimal size. it will be a guess. answer should be D"
      },
      {
        "date": "2024-05-28T05:22:00.000Z",
        "voteCount": 2,
        "content": "- **A. Use a different database**: Simply switching to a different database does not inherently solve the problem of failover and promotion mechanisms. The issue is more about the setup and management of failover strategies rather than the specific database technology used.\n\n- **B. Choose larger instances for your database**: While using larger instances might improve performance and potentially reduce the risk of a crash due to resource constraints, it does not address the failover mechanism. The key problem is the replica not being promoted, which larger instances alone won't fix.\n\n- **C. Create snapshots of your database more regularly**: Regular snapshots are useful for backups and recovery but do not help with automatic failover and high availability. Snapshots do not ensure that a replica will be promoted to a master if the primary fails."
      },
      {
        "date": "2024-05-22T06:52:00.000Z",
        "voteCount": 1,
        "content": "I think the answer is B, as the most important thing is customer experience. We can NOT expect database fails as a normal event which have direct customer and business impacts (if current data base fail because of load then replica database may fail as well).\n\nOf course we need to setup the failover process to work, but the more important task will be to increase database load first, thus I choose B."
      },
      {
        "date": "2024-03-06T17:58:00.000Z",
        "voteCount": 1,
        "content": "I think the answer is D because we can not assume the crash was due to load."
      },
      {
        "date": "2024-03-06T18:00:00.000Z",
        "voteCount": 2,
        "content": "Further the issue isn't that a crash occurred, the issue is the contingency that was in place didn't kick in. Hence D as my choice."
      },
      {
        "date": "2024-01-22T07:47:00.000Z",
        "voteCount": 2,
        "content": "D is correct with given choice, because what if larger instance size is also failed. How ever question ignores logging and networking completely, the best way is to use logging why the DB is crashed, failover is just a remedy to solve the issues at current, not solving the problem itself."
      },
      {
        "date": "2024-01-04T20:04:00.000Z",
        "voteCount": 2,
        "content": "It is important to identify key words, the issue is replica not being promoted when primary instance fails. Regular testing would identify issue"
      },
      {
        "date": "2024-01-04T16:52:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is D\nThe question is asking how to avoid the situation of not failing over. The answer is to test the failover procedure. The question does *NOT*ask about what happens in the future and whether the secondary node is large enough."
      },
      {
        "date": "2023-11-30T23:11:00.000Z",
        "voteCount": 1,
        "content": "I would say D. Beacuse also B is correct, but you have a replica here, that is never  promoted. So we need failover strategy."
      },
      {
        "date": "2023-11-16T21:34:00.000Z",
        "voteCount": 2,
        "content": "My Answer is B\nWhy not D ?\nI think\n- Each day we cannot know. What times of the day are peak usage times? Implementing routinely scheduled failovers won't solve the problem.\n- if Implement routinely scheduled failovers of your databases but  replica database server is a same spec with main database server ,  replica database server will crashes by high traffic portion same a main database server"
      },
      {
        "date": "2023-11-07T14:28:00.000Z",
        "voteCount": 1,
        "content": "Answer is D. Implement routinely scheduled failovers of your databases\n\nThis option is most aligned with addressing the issue. Routine failovers can help ensure that the failover process is working correctly and that the system is resilient to crashes. It can be part of a disaster recovery plan, where you routinely test the failover to the replica to ensure that it can handle being promoted to a master if needed. For the above reasons, i believe D is correct."
      },
      {
        "date": "2023-09-24T12:18:00.000Z",
        "voteCount": 1,
        "content": "Why not B? \n\nUsing a large instance during low traffic hours means incurring more cost than benefit except the instance is elastic. Therefore using a larger instance is not cost effective and the answer is D."
      },
      {
        "date": "2023-08-20T22:51:00.000Z",
        "voteCount": 1,
        "content": "Answer must be D i.e. Implement routinely scheduled failovers of your database because the main issue was the replica couldnt get created. So, its wise to ensure that failovers are checked on routine basis."
      },
      {
        "date": "2023-08-09T05:57:00.000Z",
        "voteCount": 1,
        "content": "Answer is D"
      },
      {
        "date": "2023-06-09T14:04:00.000Z",
        "voteCount": 2,
        "content": "Answer is D"
      },
      {
        "date": "2023-06-05T22:26:00.000Z",
        "voteCount": 2,
        "content": "D is ok"
      },
      {
        "date": "2023-05-28T22:53:00.000Z",
        "voteCount": 2,
        "content": "For me, the correct answer is D.\n\nB is not plausible. Certainly by creating larger instances the db will crash less, but in any case we cannot know whether this will not happen again for other reasons, even with the largest instance in the world. \nIn this case, we should have a ready-to-use DB that can be started up quickly on its own.\n\nThis is a good architectural design (which the PCA certification aims to give you)."
      },
      {
        "date": "2023-06-01T10:51:00.000Z",
        "voteCount": 1,
        "content": "Agree with you"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 26,
    "url": "https://www.examtopics.com/discussions/google/view/7172-exam-professional-cloud-architect-topic-1-question-26/",
    "body": "Your organization requires that metrics from all applications be retained for 5 years for future analysis in possible legal proceedings.<br>Which approach should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGrant the security team access to the logs in each Project",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Stackdriver Monitoring for all Projects, and export to BigQuery",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Stackdriver Monitoring for all Projects with the default retention policies",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Stackdriver Monitoring for all Projects, and export to Google Cloud Storage\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 77,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 26,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2019-11-14T07:56:00.000Z",
        "voteCount": 147,
        "content": "D is correct and best practice for long term log storage"
      },
      {
        "date": "2024-06-24T06:26:00.000Z",
        "voteCount": 1,
        "content": "same for me. The best approach for the long time log is to export it from monitoring to Cloud Storage Archival type"
      },
      {
        "date": "2021-03-17T06:51:00.000Z",
        "voteCount": 22,
        "content": "+1. For archival purposes, Customer should use Cloud Storage. BigQuery is a datawarehouse, and could eventually import data from Cloud Storage if necessary."
      },
      {
        "date": "2022-01-06T22:41:00.000Z",
        "voteCount": 14,
        "content": "+1 Due to long term storage, cloud storage is better answer than BigQuery"
      },
      {
        "date": "2020-01-03T01:18:00.000Z",
        "voteCount": 68,
        "content": "A and C can be quickly ruled out because none of them is solution for the requirements \"retained for 5 years\"\n\nBetween B and D, the different is where to store, BigQuery or Cloud Storage. Since the main concern is extended storing period, D (Correct Answer) is better choice, and the \"retained for 5 years for future analysis\" further qualifies it, for example, using Coldline storage class.\n\nWith regards of BigQuery, while it is also a low-cost storage, but the main purpose is for analysis. Also, logs stored in Cloud Storage is easy to transport to BigQuery or do query directly against the files saved in Cloud Storage if and whenever needed."
      },
      {
        "date": "2020-01-04T04:14:00.000Z",
        "voteCount": 2,
        "content": "point : organization requires that metrics from all applications be retained for 5 years"
      },
      {
        "date": "2020-01-04T04:15:00.000Z",
        "voteCount": 4,
        "content": "I mean answer : D"
      },
      {
        "date": "2020-10-04T13:25:00.000Z",
        "voteCount": 5,
        "content": "If you have 2 viable solutions (B&amp;D), then always chose the one that is cost optimised - I chose D"
      },
      {
        "date": "2022-03-19T10:52:00.000Z",
        "voteCount": 11,
        "content": "Bigquery long term storage cost: $0.020 per GB\nCloud Storage archive cost: $0,0012 per GB\nOnly if metrics need less than 10 GB (free service part on Bigquery) then the correct solution will be B... But all metrics for all applications during more than 5 years... I think never will be the case :D"
      },
      {
        "date": "2021-02-23T23:30:00.000Z",
        "voteCount": 1,
        "content": "second that! like the way u explained.."
      },
      {
        "date": "2020-12-06T05:40:00.000Z",
        "voteCount": 16,
        "content": "The question is about metrics, not logs. I'd go for B.\nSee https://cloud.google.com/solutions/stackdriver-monitoring-metric-export"
      },
      {
        "date": "2021-01-24T14:02:00.000Z",
        "voteCount": 5,
        "content": "This is a good example. thanks.\nBut, we can easily change that implementation to dump the metrics to buckets to save lots of money. And, when talking about legal purpose, 1 hour interval may not be enough. You may have to keep more frequent metrics. So, only cold line or archive work for that purpose."
      },
      {
        "date": "2024-09-02T19:59:00.000Z",
        "voteCount": 1,
        "content": "I go for B, as the question is about 5 years worth of data \"for future analysis in possible legal proceedings\", and the \"future\" can be next day, based on when the legal proceeding happen.\nIt is not about long term log storage.\nEven the argument of \"future\" means 100 years later, the Cold Storage Archival still does not fulfill the \"analysis\" portion of the requirements.\nYou will need to move the data from Cold Storage to BigQuery for the analysis.\nSo the ideal answer should be combination of D and B, but we do not have such option, hence the answer can meet all requirements is B."
      },
      {
        "date": "2024-08-08T00:50:00.000Z",
        "voteCount": 1,
        "content": "D is answer. Monitoring has only 24 months retention."
      },
      {
        "date": "2024-08-06T05:36:00.000Z",
        "voteCount": 1,
        "content": "for storing the logs you need cloud storage."
      },
      {
        "date": "2024-07-25T23:59:00.000Z",
        "voteCount": 1,
        "content": "B should be correct. How can Cloud Storage analyze the data?"
      },
      {
        "date": "2024-07-30T16:38:00.000Z",
        "voteCount": 1,
        "content": "The statement is not about the actual analysis of the data, but 'where' to store the data for future analysis.  Who know when that will be??? So GCS is the best answer. When need be, it can be import into BQ"
      },
      {
        "date": "2024-07-30T16:39:00.000Z",
        "voteCount": 1,
        "content": "Also it said retained for 5 years for future analysis... you don't store in BQ"
      },
      {
        "date": "2024-06-05T12:02:00.000Z",
        "voteCount": 1,
        "content": "I mean, \"for possible future legal proceedings\", I think that immutable storage that grants data integrity is the best option here, what's more, it's also the cheapest one..."
      },
      {
        "date": "2024-05-28T05:27:00.000Z",
        "voteCount": 1,
        "content": "Google Cloud Storage for 5 years storing legally."
      },
      {
        "date": "2024-04-11T06:15:00.000Z",
        "voteCount": 1,
        "content": "best practice for long term log storage"
      },
      {
        "date": "2024-02-04T15:33:00.000Z",
        "voteCount": 1,
        "content": "I think B is the right answer because transferring the data could be a basis for discrediting the data for legal use. Since BIg Query can store the data and retain it with all the metadata intact, I will go for it."
      },
      {
        "date": "2024-01-29T03:10:00.000Z",
        "voteCount": 1,
        "content": "\"organization requires that metrics from all applications be retained for 5 years\" means cloud storage"
      },
      {
        "date": "2024-01-20T11:39:00.000Z",
        "voteCount": 1,
        "content": "Cloud storage is appropriate for long term storage"
      },
      {
        "date": "2024-01-18T05:02:00.000Z",
        "voteCount": 1,
        "content": "This should be D. The tool lists option B as correct option which doesn't sound right. Bigquery export is fine for analysis but not for long term storage for 5 yrs."
      },
      {
        "date": "2024-01-14T08:08:00.000Z",
        "voteCount": 1,
        "content": "D - Correct\nB. Configure Stackdriver Monitoring for all Projects, and export to BigQuery: This approach can work, but it may be expensive since BigQuery charges for the storage and processing of data. Additionally, you may need to configure a specific retention policy to retain the data for 5 years, which can further increase the cost.\n\nD. Configure Stackdriver Monitoring for all Projects, and export to Google Cloud Storage: This approach is the most appropriate because it allows for the configuration of retention policies that meet the requirement of retaining metrics for 5 years. Also, Google Cloud Storage is a cost-effective solution for long-term data storage. Exporting the logs to Google Cloud Storage can be automated and scheduled for regular intervals, reducing the manual effort required to ensure compliance with the retention policy. The exported logs can then be analyzed using various tools, including BigQuery, if needed."
      },
      {
        "date": "2023-11-24T06:37:00.000Z",
        "voteCount": 2,
        "content": "D is correct"
      },
      {
        "date": "2023-11-18T16:38:00.000Z",
        "voteCount": 2,
        "content": "The keyword here is \"long term log storage\" so it is D. If keywords like \"log analysis and storage\" were  used I would say BQ. But for this question answer is \"D\""
      },
      {
        "date": "2023-10-23T02:34:00.000Z",
        "voteCount": 2,
        "content": "Definitely D, as long as you've seen it's stored for 5 years and Cloud Storage would be the best candidate."
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 27,
    "url": "https://www.examtopics.com/discussions/google/view/11781-exam-professional-cloud-architect-topic-1-question-27/",
    "body": "Your company has decided to build a backup replica of their on-premises user authentication PostgreSQL database on Google Cloud Platform. The database is 4<br>TB, and large updates are frequent. Replication requires private address space communication.<br>Which networking approach should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGoogle Cloud Dedicated Interconnect\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGoogle Cloud VPN connected to the data center network",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA NAT and TLS translation gateway installed on-premises",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA Google Compute Engine instance with a VPN server installed connected to the data center network"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 7,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-01-11T12:54:00.000Z",
        "voteCount": 26,
        "content": "A is the one"
      },
      {
        "date": "2020-08-05T22:41:00.000Z",
        "voteCount": 8,
        "content": "A is ok"
      },
      {
        "date": "2021-03-04T10:05:00.000Z",
        "voteCount": 3,
        "content": "A, direct connect is private. VPN not enough for 4 TB with huge frequent changes."
      },
      {
        "date": "2021-08-25T00:00:00.000Z",
        "voteCount": 16,
        "content": "Let's go with option elimination\nA. Google Cloud Dedicated Interconnect\n&gt;&gt; Secured, fast connection, hence the choice. This will allow private connection from GCP to the data centre with a fast connection. Cost is not mentioned in the requirement to eliminate this option.\nB. Google Cloud VPN connected to the data centre network\n&gt;&gt; We have to think about data flowing on the internet and the requirement talks about private connect. Also not sure how well you connect VPN with Data Center until you use the hybrid option. https://cloud.google.com/network-connectivity/docs/vpn/concepts/overview hence eliminate\nC. A NAT and TLS translation gateway installed on-premises\n&gt;&gt;This is a VM option to reach outside won't for this requirement hence eliminate\nD. A Google Compute Engine instance with a VPN server installed connected to the data centre network\n&gt;&gt;This is a slow option hence eliminate\n\nHence A"
      },
      {
        "date": "2024-05-25T12:43:00.000Z",
        "voteCount": 2,
        "content": "Option B\nInterconnect is incredibly expensive and the usecase do not justify it.\nProperly configured, a VPN provides similar features. If the question included an inusally high SLA, I will go with Interconnect. If not, VPN is a great option."
      },
      {
        "date": "2024-06-05T10:12:00.000Z",
        "voteCount": 1,
        "content": "A simple VPN may not provide enough bandwidth for replication if the DB is busy. We know that the auth DB is 4TB, I'd say this must be a quite big company, possibly they can offer an interconnect? But it surely is expensive"
      },
      {
        "date": "2024-04-30T01:03:00.000Z",
        "voteCount": 3,
        "content": "I'll go for VPN.\nFirst, the database is only for authentication and updates will be on this part, small portion of data needs to be replicated between on-premise and cloud. so no need for high bandwidth. the first migration will needs bandwidth but not toom much (5T) can be migrated using VPN.\nVPN permits to use private networking and it's secure.\nVPN not expensive as direct connect.\nAs architect you should also evaluate the cost over the requirement, at the end you need convice business with solution. Paying 5K will kick you out the project for such small requirement."
      },
      {
        "date": "2024-05-25T12:43:00.000Z",
        "voteCount": 1,
        "content": "I think this fella hit the righ nail. Interconnect is incredibly expensive and the usecase do not justify it.\nProperly configured, a VPN provides similar features. If the question included an inusally high SLA, I will go with Interconnect. If not, VPN is a great option."
      },
      {
        "date": "2024-03-06T18:10:00.000Z",
        "voteCount": 1,
        "content": "If you tried to sell me on Interconnect when all I needed was a VPN (meets bandwidth req, private address space, encryption of traffic possible), I would reach out to AWS for a quote..."
      },
      {
        "date": "2023-09-25T17:38:00.000Z",
        "voteCount": 3,
        "content": "GoogleVPN throughput is 3Gbps. It supports private IP connection and cheaper than DIrect Connection.\n\nDirect connect supports 8 * 10Gbps or 2*100Gbps.  But too expensive for this"
      },
      {
        "date": "2023-07-26T18:40:00.000Z",
        "voteCount": 2,
        "content": "Connect to private space with high-speed bandwidth will go to A."
      },
      {
        "date": "2023-05-12T14:22:00.000Z",
        "voteCount": 1,
        "content": "B: Dedicated Interconnect would be a major overkill here and a quite expensive one as well. Requirements mention private _address space_, not private connection. Data over VPN is just as secure. Also there is no mention that a Google PoP would be available.\nhttps://cloud.google.com/network-connectivity/docs/how-to/choose-product"
      },
      {
        "date": "2022-11-25T22:03:00.000Z",
        "voteCount": 1,
        "content": "A is the correct answer"
      },
      {
        "date": "2022-10-22T12:24:00.000Z",
        "voteCount": 1,
        "content": "A is great but expensive for just a database DR but what can we do about that"
      },
      {
        "date": "2022-10-16T21:31:00.000Z",
        "voteCount": 1,
        "content": "VPN is not private, it is public but encrypted. Also, VPN is not suitable for large updates that happen frequently"
      },
      {
        "date": "2022-10-16T14:31:00.000Z",
        "voteCount": 1,
        "content": "without any second thought A is right"
      },
      {
        "date": "2022-10-13T03:06:00.000Z",
        "voteCount": 1,
        "content": "A. Google Cloud Dedicated Interconnect - large updates and better security, however may not be the most cost effective choice"
      },
      {
        "date": "2022-05-13T23:46:00.000Z",
        "voteCount": 1,
        "content": "A is the one"
      },
      {
        "date": "2022-04-20T20:33:00.000Z",
        "voteCount": 1,
        "content": "Direct connect."
      },
      {
        "date": "2021-12-16T12:23:00.000Z",
        "voteCount": 5,
        "content": "Challenge me but this is answer B. I have 4TB DB, frequent update would be what ? 50% daily change means 2TB daily means ~25Mbps. With VPN I can easily achieved that. It is typical ingress to cloud free ....It would be madness to pay 5k montly only for Directo Connect..."
      },
      {
        "date": "2022-05-13T06:32:00.000Z",
        "voteCount": 2,
        "content": "Key points of quesiton - \n1) Huge data and \n2) on-premises user authentication PostgreSQL - which means security  - vpn uses public internet .. so B is not option.\nA - should be correct answer"
      },
      {
        "date": "2021-12-04T01:35:00.000Z",
        "voteCount": 2,
        "content": "Go for A"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 28,
    "url": "https://www.examtopics.com/discussions/google/view/6884-exam-professional-cloud-architect-topic-1-question-28/",
    "body": "Auditors visit your teams every 12 months and ask to review all the Google Cloud Identity and Access Management (Cloud IAM) policy changes in the previous 12 months. You want to streamline and expedite the analysis and audit process.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate custom Google Stackdriver alerts and send them to the auditor",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Logging export to Google BigQuery and use ACLs and views to scope the data shared with the auditor\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse cloud functions to transfer log entries to Google Cloud SQL and use ACLs and views to limit an auditor's view",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Google Cloud Storage (GCS) log export to audit logs into a GCS bucket and delegate access to the bucket"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 47,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 26,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-01-14T01:56:00.000Z",
        "voteCount": 93,
        "content": "B. https://cloud.google.com/iam/docs/roles-audit-logging#scenario_external_auditors"
      },
      {
        "date": "2020-01-19T08:25:00.000Z",
        "voteCount": 3,
        "content": "b) seems correct"
      },
      {
        "date": "2020-04-03T00:36:00.000Z",
        "voteCount": 5,
        "content": "I agree. Answer B"
      },
      {
        "date": "2021-08-26T02:36:00.000Z",
        "voteCount": 1,
        "content": "The article references either gcs or bq.  I think this q is referring to gcs"
      },
      {
        "date": "2021-11-21T18:34:00.000Z",
        "voteCount": 4,
        "content": "B makes more sense after reading it. thx"
      },
      {
        "date": "2019-10-21T06:10:00.000Z",
        "voteCount": 36,
        "content": "Think B is better. Export to Bigquery and restrict access to queries with ACLs to auditors"
      },
      {
        "date": "2020-12-06T05:53:00.000Z",
        "voteCount": 4,
        "content": "I think D is better. B implies too much data manipulation to make it suitable for an audit."
      },
      {
        "date": "2020-08-05T22:48:00.000Z",
        "voteCount": 7,
        "content": "D is ok."
      },
      {
        "date": "2020-08-14T02:22:00.000Z",
        "voteCount": 14,
        "content": "Sorry, changed my view. B is the recommended practice"
      },
      {
        "date": "2021-01-18T19:27:00.000Z",
        "voteCount": 4,
        "content": "don't change your view, D was right :)"
      },
      {
        "date": "2021-07-10T08:55:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2021-03-04T15:08:00.000Z",
        "voteCount": 3,
        "content": "D, rest all options are no good."
      },
      {
        "date": "2022-05-14T07:43:00.000Z",
        "voteCount": 3,
        "content": "Please check the keywords in question -- \"streamline and expedite\" -- Bigquery is suitable not storage bucket. so it should be (B)"
      },
      {
        "date": "2019-12-17T13:40:00.000Z",
        "voteCount": 5,
        "content": "I thought same as well. I would go with B"
      },
      {
        "date": "2024-09-03T04:36:00.000Z",
        "voteCount": 1,
        "content": "Option B is the best approach. Enable Logging export to Google BigQuery and use ACLs and views to scope the data shared with the auditor. This method provides robust querying capabilities, ensures that historical IAM policy changes can be analyzed effectively, and allows you to control access securely."
      },
      {
        "date": "2024-08-06T05:38:00.000Z",
        "voteCount": 1,
        "content": "b is correct because it is easier to implement compared to D"
      },
      {
        "date": "2024-07-14T13:51:00.000Z",
        "voteCount": 4,
        "content": "READ THIS, ACL is not available in BIG QUERY , thereforeD. Enable Google Cloud Storage (GCS) log export to audit logs into a GCS bucket and delegate access to the bucket"
      },
      {
        "date": "2024-03-06T18:19:00.000Z",
        "voteCount": 1,
        "content": "ACLs would provide year-round access to the data which is more privileges than necessary. Logs will need to be retained for a full year because hypothetically, January logs could be looked at in December. Cloud Storage offers signed URLs, and less expensive storage options."
      },
      {
        "date": "2024-02-22T19:47:00.000Z",
        "voteCount": 1,
        "content": "Both B and D are ok.\n\nUsing cloud storage requires additional setup for auditors, pulling data to BQ.\nUsing BQ would satisfy \"streamline and expedite the analysis and audit process\""
      },
      {
        "date": "2024-01-20T11:46:00.000Z",
        "voteCount": 3,
        "content": "Based on google documentation B is the correct answer.\nhttps://cloud.google.com/iam/docs/job-functions/auditing#scenario_external_auditors\nDashboard is available in BigQuery to review historic logs and in case anamoly is found elevated access is provided. Access is revoked after audit activities are done."
      },
      {
        "date": "2024-01-14T08:30:00.000Z",
        "voteCount": 2,
        "content": "D - Correct\nB - his option requires additional work to set up the ACLs and views to limit an auditor's view of the data. This could be time-consuming and complex to implement. Furthermore, BigQuery may not be the ideal tool for auditors who are only interested in reviewing Cloud IAM policy changes."
      },
      {
        "date": "2023-11-20T02:36:00.000Z",
        "voteCount": 1,
        "content": "That\u2018s the only logical one also Bard is confirming this one"
      },
      {
        "date": "2023-11-11T07:22:00.000Z",
        "voteCount": 3,
        "content": "D\nI will not go with B, as the requirement is once for 12 months. Push the data in Coldline for 12 months and retrieve it during audit is enough. Save costs."
      },
      {
        "date": "2023-11-11T07:23:00.000Z",
        "voteCount": 2,
        "content": "Coldline / Archive"
      },
      {
        "date": "2024-01-07T09:06:00.000Z",
        "voteCount": 2,
        "content": "Streamline and expedite analysis is the goal. Costs are never brought up."
      },
      {
        "date": "2023-10-24T01:59:00.000Z",
        "voteCount": 2,
        "content": "Reading from Cloud Storage raw audit logs (without filtering applied) is everything but streamlined. Imagine the auditor fetching all audit logs, then write some script to analyze them..."
      },
      {
        "date": "2023-10-08T20:57:00.000Z",
        "voteCount": 4,
        "content": "B talks about ACL in BigQuery and ACL is not associated with BigQuery but with GCS."
      },
      {
        "date": "2023-10-04T21:30:00.000Z",
        "voteCount": 1,
        "content": "You want to streamline and expedite the analysis and audit process.\n\nBig Query, as the data retention is mentioned, and data is related to Cloud IAM policy changes, it is safe to assume long term retention with annual audit."
      },
      {
        "date": "2023-10-01T19:40:00.000Z",
        "voteCount": 1,
        "content": "``To comply with this requirement, a dashboard is available that provides access to the historic logs stored in BigQuery, and on request, to the Cloud Logging Admin Activity logs.\n\nThe organization creates a Google group for these external auditors and adds the current auditor to the group. This group is monitored and is typically granted access to the dashboard application.\n\nDuring normal access, the auditors' Google group is only granted access to view the historic logs stored in BigQuery. If any anomalies are discovered, the group is granted permission to view the actual Cloud Logging Admin Activity logs via the dashboard's elevated access mode. At the end of each audit period, the group's access is then revoked.''\n\nhttps://cloud.google.com/iam/docs/job-functions/auditing#scenario_external_auditors"
      },
      {
        "date": "2023-09-13T13:43:00.000Z",
        "voteCount": 1,
        "content": "Auditors don't have why know how make BigQuery queries. They usually ask for evidence files."
      },
      {
        "date": "2023-09-13T09:10:00.000Z",
        "voteCount": 1,
        "content": "Sorry B is the Correct Answer"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 29,
    "url": "https://www.examtopics.com/discussions/google/view/7200-exam-professional-cloud-architect-topic-1-question-29/",
    "body": "You are designing a large distributed application with 30 microservices. Each of your distributed microservices needs to connect to a database back-end. You want to store the credentials securely.<br>Where should you store the credentials?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the source code",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn an environment variable",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn a secret management system\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn a config file that has restricted access through ACLs"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2019-10-25T06:54:00.000Z",
        "voteCount": 33,
        "content": "Google Secret Management was designed explicitly for this purpose."
      },
      {
        "date": "2020-08-05T22:49:00.000Z",
        "voteCount": 7,
        "content": "C is ok"
      },
      {
        "date": "2021-03-04T15:09:00.000Z",
        "voteCount": 6,
        "content": "C, microservices = GKE = Kubernetes = secrets."
      },
      {
        "date": "2019-11-26T11:13:00.000Z",
        "voteCount": 12,
        "content": "C is the answer, since key management systems generate, use, rotate, encrypt, and destroy cryptographic keys and manage permissions to those keys.\n\nA is incorrect because storing credentials in source code and source control is discoverable, in plain text, by anyone with access to the source code. This also introduces the requirement to update code and do a deployment each time the credentials are rotated. B is not correct because consistently populating environment variables would require the credentials to be available, in plain text, when the session is started. D is incorrect because instead of managing access to the config file and updating manually as keys are rotated, it would be better to leverage a key management system. Additionally, there is increased risk if the config file contains the credentials in plain text."
      },
      {
        "date": "2024-01-22T12:14:00.000Z",
        "voteCount": 1,
        "content": "Other options are not best practices."
      },
      {
        "date": "2024-01-20T11:47:00.000Z",
        "voteCount": 1,
        "content": "Need to use key management system for this usecase since other options are not secure."
      },
      {
        "date": "2023-04-07T05:50:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2022-12-20T23:50:00.000Z",
        "voteCount": 3,
        "content": "C. In a secret management system\n\nIt is important to store the credentials for your database back-end securely in order to protect them from unauthorized access. One way to do this is by using a secret management system, such as Google Cloud's Secret Manager. Secret Manager is a secure and convenient storage system for API keys, passwords, and other sensitive data that is designed to protect against unauthorized access. By storing the credentials in Secret Manager, you can ensure that they are kept secure and can be easily accessed by your microservices as needed.\n\nStoring the credentials in the source code, an environment variable, or a config file that has restricted access through ACLs may not provide the same level of security as a dedicated secret management system. It is important to ensure that your credentials are stored in a secure and controlled manner to protect against unauthorized access."
      },
      {
        "date": "2022-11-19T04:03:00.000Z",
        "voteCount": 1,
        "content": "C is correct; If credential then always use secret manager."
      },
      {
        "date": "2022-11-05T02:36:00.000Z",
        "voteCount": 1,
        "content": "C is ok"
      },
      {
        "date": "2022-10-16T21:39:00.000Z",
        "voteCount": 1,
        "content": "secret manager is the answer"
      },
      {
        "date": "2022-10-16T14:35:00.000Z",
        "voteCount": 1,
        "content": "C is right"
      },
      {
        "date": "2022-09-13T05:02:00.000Z",
        "voteCount": 1,
        "content": "answer is C"
      },
      {
        "date": "2022-03-03T05:15:00.000Z",
        "voteCount": 1,
        "content": "Use Google Secret Manager"
      },
      {
        "date": "2022-01-17T11:35:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2021-12-04T01:47:00.000Z",
        "voteCount": 1,
        "content": "Go for C"
      },
      {
        "date": "2021-11-27T06:30:00.000Z",
        "voteCount": 1,
        "content": "C is the right answer"
      },
      {
        "date": "2021-10-09T03:29:00.000Z",
        "voteCount": 1,
        "content": "Google Practice exam question with option C :  In a key management system\nC is correct because key management systems generate, use, rotate, encrypt, and destroy cryptographic keys and manage permissions to those keys.\nhttps://cloud.google.com/kms/\nFor this question, refer to the Mountkirk Games case study."
      },
      {
        "date": "2021-10-09T03:17:00.000Z",
        "voteCount": 1,
        "content": "Google Practice exam question with option C :  In a key management system\nHere also C is correct because key management systems generate, use, rotate, encrypt, and destroy cryptographic keys and manage permissions to those keys.\nhttps://cloud.google.com/kms/\nFor this question, refer to the Mountkirk Games case study."
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 30,
    "url": "https://www.examtopics.com/discussions/google/view/54125-exam-professional-cloud-architect-topic-1-question-30/",
    "body": "A lead engineer wrote a custom tool that deploys virtual machines in the legacy data center. He wants to migrate the custom tool to the new cloud environment.<br>You want to advocate for the adoption of Google Cloud Deployment Manager.<br>What are two business risks of migrating to Cloud Deployment Manager? (Choose two.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Deployment Manager uses Python",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Deployment Manager APIs could be deprecated in the future",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Deployment Manager is unfamiliar to the company's engineers\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Deployment Manager requires a Google APIs service account to run",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Deployment Manager can be used to permanently delete cloud resources",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Deployment Manager only supports automation of Google Cloud resources\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "CF",
    "answerDescription": "",
    "votes": [
      {
        "answer": "CF",
        "count": 44,
        "isMostVoted": true
      },
      {
        "answer": "CE",
        "count": 20,
        "isMostVoted": false
      },
      {
        "answer": "EF",
        "count": 20,
        "isMostVoted": false
      },
      {
        "answer": "BC",
        "count": 9,
        "isMostVoted": false
      },
      {
        "answer": "BF",
        "count": 8,
        "isMostVoted": false
      },
      {
        "answer": "DF",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-07-04T07:51:00.000Z",
        "voteCount": 79,
        "content": "E. Cloud Deployment Manager can be used to permanently delete cloud resources\nF. Cloud Deployment Manager only supports automation of Google Cloud resources"
      },
      {
        "date": "2023-09-25T11:24:00.000Z",
        "voteCount": 5,
        "content": "F is not a risk, it is a limitation of solution. Risk is something that is not known for sure and is manageable (risk can be mitigated, avoided). You cannot manage the limitation of solution. You can use it with this limitation or not and you know it in advance."
      },
      {
        "date": "2023-12-25T23:56:00.000Z",
        "voteCount": 3,
        "content": "Advocating to adopt Google Cloud Deployment Manager will become a risk if the lead engineer or other business need ask for use other cloud platform."
      },
      {
        "date": "2021-07-25T15:25:00.000Z",
        "voteCount": 13,
        "content": "Yup, E + F. In GCP documentation it states as a warning note that deletion made through Deployment Manager scripts cannot be undone, if devs are not well trained a human errors can impact Business"
      },
      {
        "date": "2021-06-15T01:55:00.000Z",
        "voteCount": 42,
        "content": "C and F- make sense to me"
      },
      {
        "date": "2021-11-20T01:44:00.000Z",
        "voteCount": 3,
        "content": "I think this is right. the key of the question is \"business risks\"."
      },
      {
        "date": "2022-10-16T14:37:00.000Z",
        "voteCount": 3,
        "content": "yes, C and F right"
      },
      {
        "date": "2023-04-02T00:04:00.000Z",
        "voteCount": 5,
        "content": "C - Makes sense, because company engineer may take longer to develop, so more cost and more 'time-to-market'\n\nReg F:\nCan I pls ask how does business care whether you are Google Cloud Resources or legacy data center tools, as long as it servs business requirement? \n\nSo I'm leaning towards E, as the engineers are still in the process of learning CDM and may accidently delete VMs bringing down the entire application."
      },
      {
        "date": "2023-04-02T00:16:00.000Z",
        "voteCount": 2,
        "content": "Forgot to mention, once determined as \"risks\", the mitigation actions below can be followed:\nC: Train the existing resources, Hire an experienced personnel\nE: Peer Reviews, QA, thorough testing etc."
      },
      {
        "date": "2024-10-10T05:59:00.000Z",
        "voteCount": 1,
        "content": "BF are correct"
      },
      {
        "date": "2024-09-25T07:41:00.000Z",
        "voteCount": 1,
        "content": "E and F"
      },
      {
        "date": "2024-09-03T04:40:00.000Z",
        "voteCount": 2,
        "content": "B. Cloud Deployment Manager APIs could be deprecated in the future: There's always a risk that APIs and tools can be deprecated or replaced with new versions. This could impact the long-term stability of your deployment process if the APIs used by Cloud Deployment Manager are deprecated and require migration to new APIs.\n\nC. Cloud Deployment Manager is unfamiliar to the company's engineers: If the company's engineers are not familiar with Cloud Deployment Manager, there could be a learning curve and potential delays during the migration process. Training and adaptation time could affect productivity and introduce risks associated with potential mistakes or inefficiencies during the transition."
      },
      {
        "date": "2024-08-15T03:52:00.000Z",
        "voteCount": 2,
        "content": "B. Cloud deployment manager is being deprecated.\nF. Cloud Deployment Manager only supports automation of Google Cloud resources\n\nThe question is about business risks - it's not about technical risks \nA C D E options are technical aspects of the Deployment manager"
      },
      {
        "date": "2024-08-06T03:35:00.000Z",
        "voteCount": 3,
        "content": "Should be B &amp; C"
      },
      {
        "date": "2024-06-28T06:40:00.000Z",
        "voteCount": 4,
        "content": "My answer is BC.\nI think the correct answers should be related to the engineer's custom tool.\nThe choices should answer the question \"Why should we migrate Lead Engineer's tool to the new cloud environment\".\nIt's tried and tested Custom Tool versus the new Deployment Manager.\nB, because it can be deprecated, custom tool has been used for a long time.\nC, because their engineers don't know it yet."
      },
      {
        "date": "2024-06-12T13:13:00.000Z",
        "voteCount": 2,
        "content": "Chat GPT tells me B and C and it gets it right maybe 98% of the time so far. It seems nobody on this discussion can agree on an answer but if we're going to look at keywords of the questions and over-scrutinize them, then yes, risk is a key word and I don't see F as a risk. E is a risk but not even remotely unique to this situation. C is a mild risk but really the only one that makes sense in light of eliminating the others. B is definitely a risk... but then it also isn't unique to this service so we're back at the same issue with E. This question is garbage."
      },
      {
        "date": "2024-06-09T09:11:00.000Z",
        "voteCount": 2,
        "content": "for me C and F"
      },
      {
        "date": "2024-05-27T04:38:00.000Z",
        "voteCount": 1,
        "content": "B: Cloud deployment manager is being deprecated.\nF: Mentions on-premise resources and managing GCP which will make cloud deployment manager not the best tool for this."
      },
      {
        "date": "2024-05-15T13:55:00.000Z",
        "voteCount": 2,
        "content": "All solutions can be deprecated, even GCP. So we are talking about current business risks, so for me work C and F"
      },
      {
        "date": "2024-05-01T08:13:00.000Z",
        "voteCount": 1,
        "content": "It's about business risk."
      },
      {
        "date": "2024-01-20T11:59:00.000Z",
        "voteCount": 2,
        "content": "Since business risks are mentioned. C. Training resources will be a cost and untrained resources might cause unnecessary issues costing the business.\nF. Since the current tool is developed in house  and used on premise its not clear if its used for GCP only.  GCP deployment manager is only for GCP resources."
      },
      {
        "date": "2024-01-14T13:10:00.000Z",
        "voteCount": 3,
        "content": "According to Google Bard AI:\nThe two business risks of migrating to Cloud Deployment Manager are B. Cloud Deployment Manager APIs could be deprecated in the future and C. Cloud Deployment Manager is unfamiliar to the company's engineers.\n\nB --&gt; This could make it difficult to maintain and update the tool, and could also prevent the company from taking advantage of new features that are added to Cloud Deployment Manager.\n\nC --&gt; a business risk because it will require the company to invest time and resources in training its engineers how to use the tool. This could slow down the migration process, and could also make it more difficult for the company to adapt to new changes in the tool."
      },
      {
        "date": "2024-01-14T08:41:00.000Z",
        "voteCount": 3,
        "content": "CF - Correct\n\nhttps://encore.dev/resources/google-cloud-deployment-manager\nDrawbacks\nLearning Curve: Can be challenging for new users, particularly those not familiar with YAML.\nVendor Lock-in: Confined exclusively to GCP, hindering a multi-cloud strategy.\nLess Community Support: May lack extensive community support compared to some alternatives."
      },
      {
        "date": "2023-12-14T04:26:00.000Z",
        "voteCount": 1,
        "content": "It can't be C because if Cloud Deployment Manager is unfamiliar to the company's engineers, they have to learn it."
      },
      {
        "date": "2024-01-07T09:09:00.000Z",
        "voteCount": 3,
        "content": "Well that's why it's a risk. C is definitely one of the answers."
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 31,
    "url": "https://www.examtopics.com/discussions/google/view/54389-exam-professional-cloud-architect-topic-1-question-31/",
    "body": "A development manager is building a new application. He asks you to review his requirements and identify what cloud technologies he can use to meet them. The application must:<br>1. Be based on open-source technology for cloud portability<br>2. Dynamically scale compute capacity based on demand<br>3. Support continuous software delivery<br>4. Run multiple segregated copies of the same application stack<br>5. Deploy application bundles using dynamic templates<br>6. Route network traffic to specific services based on URL<br>Which combination of technologies will meet all of his requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGoogle Kubernetes Engine, Jenkins, and Helm\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGoogle Kubernetes Engine and Cloud Load Balancing",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGoogle Kubernetes Engine and Cloud Deployment Manager",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGoogle Kubernetes Engine, Jenkins, and Cloud Load Balancing"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 37,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 24,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-06-10T02:28:00.000Z",
        "voteCount": 60,
        "content": "it should be A .. helm is needed for \"Deploy application bundles using dynamic templates\"\n\nLoad Balancing should be part of GKE Already"
      },
      {
        "date": "2021-08-30T18:15:00.000Z",
        "voteCount": 4,
        "content": "Kubernetes Engine offers integrated support for two types of Cloud Load Balancing (Ingress and External Network Load Balancing) , hence Option A\nReference : https://cloud.google.com/kubernetes-engine/docs/tutorials/http-balancer"
      },
      {
        "date": "2022-10-16T14:40:00.000Z",
        "voteCount": 2,
        "content": "A should be fine"
      },
      {
        "date": "2023-10-03T00:30:00.000Z",
        "voteCount": 2,
        "content": "Load balancing is not a part of GKE untill it's created explicitly"
      },
      {
        "date": "2021-07-25T15:29:00.000Z",
        "voteCount": 10,
        "content": "Not for \"based on URL\", that is the difference."
      },
      {
        "date": "2021-10-10T11:06:00.000Z",
        "voteCount": 15,
        "content": "https://cloud.google.com/kubernetes-engine/docs/tutorials/http-balancer#optional_serving_multiple_applications_on_a_load_balancer\n\nAs per the above document and given example of \"fanout-ingress.yaml\"  in above document and also in GKE  sample repository below\nhttps://github.com/GoogleCloudPlatform/kubernetes-engine-samples/tree/master/load-balancing\n\nit's clear that GKE LB can handle \"6. Route network traffic to specific services based on URL\"\nSo NO need for Cloud Load balancing.\n\nHelm satisfy \"5. Deploy application bundles using dynamic templates\" \nand no other option satisfies this point #5.\n\nSo correct answer should be: \nA"
      },
      {
        "date": "2021-07-04T07:48:00.000Z",
        "voteCount": 40,
        "content": "D. Google Kubernetes Engine, Jenkins, and Cloud Load Balancing"
      },
      {
        "date": "2024-09-02T21:49:00.000Z",
        "voteCount": 1,
        "content": "Based on: \"Be based on open-source technology for cloud portability\", I go for A, because it is more portable, as compare to D."
      },
      {
        "date": "2024-08-08T01:09:00.000Z",
        "voteCount": 2,
        "content": "Choose A"
      },
      {
        "date": "2024-07-31T14:54:00.000Z",
        "voteCount": 1,
        "content": "Yes, Kubernetes can route network traffic to specific services based on URL using Ingress and Ingress controllers\n\nAnswer is A"
      },
      {
        "date": "2024-06-28T08:14:00.000Z",
        "voteCount": 1,
        "content": "Load Balancer is required as per point 6.\nJenkins satisfies point 3.\nD is correct"
      },
      {
        "date": "2024-07-20T12:14:00.000Z",
        "voteCount": 3,
        "content": "Helm is needed for the dynamic templates requirement.  The question is vague in whether the application is internally or externally facing which would clarify things a lot more for us.  However, in its ambiguity option A has the techonologies neede to the requirments and thus deduce or infer that the application is internally facing and the ingress controllers will handle the routing of traffic.  It's ambiguous on purpose which I hate in these exams. More of a test of how well we can read and interpret the questions vs our knowledge of material."
      },
      {
        "date": "2024-03-18T03:22:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2024-03-11T03:04:00.000Z",
        "voteCount": 4,
        "content": "Option A (GKE, Jenkins, Helm) meets most requirements except for explicit URL-based routing, though Kubernetes Ingress (which can be managed through Helm charts) implicitly covers this.\n\nOption D (GKE, Jenkins, Cloud Load Balancing) directly meets every requirement, including URL-based routing without needing to infer capabilities or integrate additional tools beyond the scope of what's listed. Jenkins supports continuous delivery, GKE supports dynamic scaling, segregated application stacks, and cloud portability. Cloud Load Balancing directly addresses the URL-based routing requirement."
      },
      {
        "date": "2024-06-13T07:02:00.000Z",
        "voteCount": 1,
        "content": "Except none of that meets the needs for deployment templates."
      },
      {
        "date": "2024-07-20T12:17:00.000Z",
        "voteCount": 1,
        "content": "My point exactly and my response to Chris_21. By process of elimination, you need Helm for the dynamic templates and you need Jenkins. Thus, you have to assume the application is internally facing and the ingress controllers will handle the traffic just fine."
      },
      {
        "date": "2024-02-20T23:21:00.000Z",
        "voteCount": 1,
        "content": "D is OK"
      },
      {
        "date": "2024-01-22T12:31:00.000Z",
        "voteCount": 5,
        "content": "1. Be based on open-source technology for cloud portability: GKE\n2. Dynamically scale compute capacity based on demand: GKE\n3. Support continuous software delivery: Jenkins\n4. Run multiple segregated copies of the same application stack: GKE\n5. Deploy application bundles using dynamic templates -&gt; Jenkins\n6. Route network traffic to specific services based on URL -&gt; Only HTTPs load balancer can meet this requirement: Next best is Cloud balancing (that can be either Network or HTTPs), \nSo D makes sense to me."
      },
      {
        "date": "2024-01-14T08:47:00.000Z",
        "voteCount": 1,
        "content": "D - Correct"
      },
      {
        "date": "2024-01-03T03:37:00.000Z",
        "voteCount": 3,
        "content": "I thought that answer A was correct but after researching HELM  I think now the option D is correct.\nHelm is not a cloud service on its own, and it is not built into Google Kubernetes Engine (GKE). Helm is an open-source package manager for Kubernetes that simplifies the deployment and management of applications on Kubernetes clusters.\n\nHere's a brief overview:\n\nHelm:\n\nHelm allows you to define, install, and upgrade even the most complex Kubernetes applications using packages called charts. A Helm chart includes pre-configured Kubernetes resources that define the structure of an application. Helm provides a convenient way to package, version, and deploy applications on Kubernetes."
      },
      {
        "date": "2024-06-13T07:04:00.000Z",
        "voteCount": 1,
        "content": "A is the real-world answer, D is the \"Google (TM)\" answer I think. It's obnoxious how bad these questions are."
      },
      {
        "date": "2023-12-25T13:54:00.000Z",
        "voteCount": 1,
        "content": "Kubernetes natively supports routing to different services based on URLs via the ingress gateway regardless of wether a LB is used..."
      },
      {
        "date": "2023-12-19T01:46:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is A, you can route network traffic to specific services based on URL using Google Kubernetes Engine (GKE) using Ingress"
      },
      {
        "date": "2023-10-16T04:20:00.000Z",
        "voteCount": 1,
        "content": "Correct should be D, Focus on this part of the question \"what cloud technologies he can use to meet them\", And Now coming to options ,B,C are self explanatory now coming to  A  Because Helm is not a Cloud Technology first, its just a package Manager for Kubernetes, and Even if you say Loadbalncing is part of GKE , actually whatever ingress you create for path based , It does create a load Balancer , and ultimately LOad Balancer is Definitely important, now coming to Templating I Feel there can be many tools we can use ex: Kustomize can be one of them but that is completly something User can decide and even create some customised way to templatize the deployment using YAML or shell or any other language ."
      },
      {
        "date": "2024-06-13T07:05:00.000Z",
        "voteCount": 1,
        "content": "I hate tests and this question is why I hate them. There is too much confusion in the question. I would say A is the real world answer, D is the Google (tm) answer."
      },
      {
        "date": "2023-10-11T02:04:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is D - Don't get confused with this option \"Deploy application bundles using dynamic templates\". In this option, they have mentioned as \"Deploy applications\". In GKE app deployment, can be configured using YAML files. Helm is not a highly preferred tool and that won't be a recommedation from Google. Answer is D"
      },
      {
        "date": "2023-10-04T22:02:00.000Z",
        "voteCount": 1,
        "content": "Dynamic bundling - GKE\nCI/ CD - Jenkins\nRouting based on URL - Cloud B"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 32,
    "url": "https://www.examtopics.com/discussions/google/view/7202-exam-professional-cloud-architect-topic-1-question-32/",
    "body": "You have created several pre-emptible Linux virtual machine instances using Google Compute Engine. You want to properly shut down your application before the virtual machines are preempted.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a shutdown script named k99.shutdown in the /etc/rc.6.d/ directory",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a shutdown script registered as a xinetd service in Linux and configure a Stackdriver endpoint check to call the service",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a shutdown script and use it as the value for a new metadata entry with the key shutdown-script in the Cloud Platform Console when you create the new virtual machine instance\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a shutdown script, registered as a xinetd service in Linux, and use the gcloud compute instances add-metadata command to specify the service URL as the value for a new metadata entry with the key shutdown-script-url"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 29,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 17,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2019-10-25T07:11:00.000Z",
        "voteCount": 38,
        "content": "https://cloud.google.com/compute/docs/shutdownscript ... So C"
      },
      {
        "date": "2021-03-04T15:13:00.000Z",
        "voteCount": 4,
        "content": "C, statup/shutdown script = metadata"
      },
      {
        "date": "2021-07-30T01:50:00.000Z",
        "voteCount": 11,
        "content": "Since the instance is already created Option C gets eliminated. \"gcloud compute instances addmetadata\u201d\ncommand can be used to add or update the metadata of a virtual machine instance\""
      },
      {
        "date": "2020-05-11T01:01:00.000Z",
        "voteCount": 27,
        "content": "I have doubts with the answer C because the question states that \"You have created the instances\" so C works too but the solution cannot apply to the already created instances. D seems correct to me...\n\nReference:\nhttps://cloud.google.com/compute/docs/shutdownscript#apply_a_shutdown_script_to_running_instances"
      },
      {
        "date": "2023-12-25T01:12:00.000Z",
        "voteCount": 2,
        "content": "I think C should be correct over D, because https://cloud.google.com/compute/docs/shutdownscript#apply_a_shutdown_script_to_running_instances"
      },
      {
        "date": "2022-06-17T22:39:00.000Z",
        "voteCount": 2,
        "content": "I also feel so because the virtual machines are already created."
      },
      {
        "date": "2023-04-13T09:48:00.000Z",
        "voteCount": 5,
        "content": "xinetd. Xinet makes the D answer be nonsense"
      },
      {
        "date": "2020-11-08T15:37:00.000Z",
        "voteCount": 4,
        "content": "Yes. The correct answer should be D.\n\nTo add a shutdown script to a running instance, follow the instructions in the Applying a startup script to running instances documentation but replace the metadata keys with one of the following keys:\n\nshutdown-script: Supply the shutdown script contents directly with this key. Using the gcloud command-line tool, you can provide the path to a shutdown script file, using the --metadata-from-file flag and the shutdown-script metadata key.\nshutdown-script-url: Supply a Cloud Storage URL to the shutdown script file with this key."
      },
      {
        "date": "2020-11-08T15:41:00.000Z",
        "voteCount": 6,
        "content": "changed my mind. preemptible vms can be stopped and started anytime. with that flexibility, C is ok."
      },
      {
        "date": "2024-10-16T11:08:00.000Z",
        "voteCount": 1,
        "content": "It's not C.  C says \"when you create the new virtual machine instance\".  But in the question, the instances have already been created.  Thus, D.  In D, \"service URL\" obviously means cloud storage."
      },
      {
        "date": "2024-08-12T08:18:00.000Z",
        "voteCount": 1,
        "content": "Among Option C &amp; D, option D uses shutdown-script-url but shutdown-script-url should be a Cloud storage url and not a linux service URL. \n\nThat brings us to option C"
      },
      {
        "date": "2024-08-08T01:12:00.000Z",
        "voteCount": 1,
        "content": "Choose C"
      },
      {
        "date": "2024-06-28T07:53:00.000Z",
        "voteCount": 1,
        "content": "C is correct since xinetd does not make sense."
      },
      {
        "date": "2024-02-23T19:33:00.000Z",
        "voteCount": 2,
        "content": "Every virtual machine instance in GCP has access to a metadata server, which provides information about the instance and allows you to configure various settings, including startup and shutdown scripts.\n\nStartup and shutdown scripts are specified using special metadata keys in the metadata server. \nshutdown-script specifies the shutdown script that should be executed when the instance is being shut down."
      },
      {
        "date": "2024-01-22T12:46:00.000Z",
        "voteCount": 1,
        "content": "All other options are related to play with Linux files or services. With Preemptible VMs ,these operations are overhead. Hence it makes sense to Automate such tasks."
      },
      {
        "date": "2024-01-14T13:33:00.000Z",
        "voteCount": 2,
        "content": "The answer is either C or D\n\nI exclude D because shutdown-script-url only works with a shutdown script hosted on a cloud storage. Option D wants you to use shutdown-script-url for a locally hosted shutdown script, thus it's not the correct answer."
      },
      {
        "date": "2024-01-14T08:49:00.000Z",
        "voteCount": 1,
        "content": "C\nhttps://cloud.google.com/compute/docs/shutdownscript"
      },
      {
        "date": "2023-12-26T01:08:00.000Z",
        "voteCount": 2,
        "content": "The answer should be C. reference to https://cloud.google.com/compute/docs/shutdownscript#apply_a_shutdown_script_to_running_instances\nRegarding the answer D, it is not the option becasue no need to touch xinetd servie inside Linux."
      },
      {
        "date": "2023-11-02T08:34:00.000Z",
        "voteCount": 1,
        "content": "I believe the answer should be D since the VMs are already created"
      },
      {
        "date": "2023-10-11T02:07:00.000Z",
        "voteCount": 1,
        "content": "C is the right answer. See, there is one tip. In GCP, things like these are given to the customers as a solution - like give a shutdown script. GCP won't trouble the users to know all those geeky linux stuffs. So the answer is simply C"
      },
      {
        "date": "2023-10-10T22:05:00.000Z",
        "voteCount": 1,
        "content": "Option D: Creating a shutdown script, registered as a xinetd service in Linux, and using the gcloud compute instances add-metadata command to specify the service URL as the value for a new metadata entry with the key shutdown-script-url is not as reliable as option C because it requires the gcloud command-line tool to be installed and configured on the virtual machine instance."
      },
      {
        "date": "2023-10-09T08:47:00.000Z",
        "voteCount": 1,
        "content": "Reference:\nhttps://cloud.google.com/compute/docs/shutdownscript#apply_a_shutdown_script_to_running_instances"
      },
      {
        "date": "2023-10-04T22:15:00.000Z",
        "voteCount": 1,
        "content": "I agreed with Eroc for Option C\nhttps://cloud.google.com/compute/docs/shutdownscript"
      },
      {
        "date": "2023-08-22T12:18:00.000Z",
        "voteCount": 1,
        "content": "C seems to be more reasonable answer....i am also leaning towards D but C is definitely correct in my opinion."
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 33,
    "url": "https://www.examtopics.com/discussions/google/view/9033-exam-professional-cloud-architect-topic-1-question-33/",
    "body": "Your organization has a 3-tier web application deployed in the same network on Google Cloud Platform. Each tier (web, API, and database) scales independently of the others. Network traffic should flow through the web to the API tier and then on to the database tier. Traffic should not flow between the web and the database tier.<br>How should you configure the network?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd each tier to a different subnetwork",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up software based firewalls on individual VMs",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd tags to each tier and set up routes to allow the desired traffic flow",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd tags to each tier and set up firewall rules to allow the desired traffic flow\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2019-11-24T17:05:00.000Z",
        "voteCount": 36,
        "content": "D. refer to target filtering. https://cloud.google.com/solutions/best-practices-vpc-design"
      },
      {
        "date": "2020-08-14T02:27:00.000Z",
        "voteCount": 8,
        "content": "D is ok"
      },
      {
        "date": "2020-11-08T15:53:00.000Z",
        "voteCount": 8,
        "content": "Thank you for the link. \nPrecisely:\nhttps://cloud.google.com/solutions/best-practices-vpc-design#target_filtering"
      },
      {
        "date": "2023-01-03T14:09:00.000Z",
        "voteCount": 2,
        "content": "perfect! the example in that section is the exact question statement"
      },
      {
        "date": "2021-03-04T15:15:00.000Z",
        "voteCount": 4,
        "content": "D, firewalls can be done on ip or network tags or service accounts in GCE."
      },
      {
        "date": "2022-10-16T14:43:00.000Z",
        "voteCount": 1,
        "content": "D is right"
      },
      {
        "date": "2021-08-26T21:28:00.000Z",
        "voteCount": 10,
        "content": "Let's go with option elimination\n\nA. Add each tier to a different subnetwork\n&gt;&gt; Adding tiers to different subnets does not prevent or block them from accessing each other. Until specific firewall rules on VM or subnet allow access traffic on a specific port in the rule.\n\nB. Set up software-based firewalls on individual VMs\n&gt;&gt; Not a recommended practice will have to enable firewall anyway.\n\nC. Add tags to each tier and set up routes to allow the desired traffic flow\n&gt;&gt; Can be done but. \n\nD. Add tags to each tier and set up firewall rules to allow the desired traffic flow\n&gt;&gt; Recommended way\n\nHence D"
      },
      {
        "date": "2024-10-16T02:26:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2024-02-23T19:47:00.000Z",
        "voteCount": 1,
        "content": "Routes are typically used for directing traffic between networks rather than within the same network. While tags can be used for identifying resources, they are typically used in conjunction with firewall rules for controlling traffic flow."
      },
      {
        "date": "2023-10-04T22:18:00.000Z",
        "voteCount": 1,
        "content": "Why to implement anything else when Firewall is built-in within VPC and works based on Tags associated with resources."
      },
      {
        "date": "2023-08-22T12:20:00.000Z",
        "voteCount": 1,
        "content": "separate vnet is ruled out as they are on same network."
      },
      {
        "date": "2023-05-28T23:13:00.000Z",
        "voteCount": 1,
        "content": "For me most suitable answer is D"
      },
      {
        "date": "2022-12-21T00:07:00.000Z",
        "voteCount": 5,
        "content": "It's D\nTo configure the network so that traffic flows through the web to the API tier and then on to the database tier, but does not flow between the web and the database tier, you can add tags to each tier and set up firewall rules to allow the desired traffic flow. By adding tags to each tier, you can identify the VMs that belong to each tier and create firewall rules that allow traffic between the tiers as needed. For example, you can create a firewall rule that allows traffic from the web tier to the API tier, and another rule that allows traffic from the API tier to the database tier. This will ensure that traffic flows through the desired path and is not allowed between the web and database tiers.\n\nOther options, such as adding each tier to a different subnetwork or setting up software-based firewalls on individual VMs, may not provide the necessary level of control over the traffic flow between the tiers. Setting up routes to allow the desired traffic flow may not be sufficient to prevent traffic between the web and database tiers."
      },
      {
        "date": "2022-11-05T02:56:00.000Z",
        "voteCount": 1,
        "content": "D is ok"
      },
      {
        "date": "2022-10-22T14:00:00.000Z",
        "voteCount": 1,
        "content": "D is right answer"
      },
      {
        "date": "2022-10-13T05:46:00.000Z",
        "voteCount": 1,
        "content": "Having 3-tier web application deployed in the same network is wrong to begin with. However, even in different subnets you will need to apply firewall rules to prevent traffic between selected subnets. In this case they will probably be better of with D."
      },
      {
        "date": "2022-10-16T22:06:00.000Z",
        "voteCount": 1,
        "content": "Did this appear in the exam?"
      },
      {
        "date": "2022-09-21T21:55:00.000Z",
        "voteCount": 1,
        "content": "use firewall rules"
      },
      {
        "date": "2022-05-09T09:33:00.000Z",
        "voteCount": 1,
        "content": "A per my comment below ."
      },
      {
        "date": "2021-12-25T04:20:00.000Z",
        "voteCount": 3,
        "content": "D is the correct answer"
      },
      {
        "date": "2021-12-04T02:08:00.000Z",
        "voteCount": 2,
        "content": "Go for D"
      },
      {
        "date": "2021-10-09T03:01:00.000Z",
        "voteCount": 2,
        "content": "From Google practice exam question : \nD is correct because as instances scale, they will all have the same tag to identify the tier. These tags can then be leveraged in firewall rules to allow and restrict traffic as required, because tags can be used for both the target and source.\nhttps://cloud.google.com/vpc/docs/using-vpc\nhttps://cloud.google.com/vpc/docs/routes\nhttps://cloud.google.com/vpc/docs/add-remove-network-tags"
      },
      {
        "date": "2021-06-24T10:03:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer: D\nThe web tier can communicate with end users and the app tier, and the app tier can communicate with the database tier, but no other communication between tiers is allowed. The instances running the web tier have a network tag of web, the instances running the app tier have a network tag of app, and the instances running the database tier have a network tag of db.\nhttps://cloud.google.com/architecture/best-practices-vpc-design#target_filtering"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 34,
    "url": "https://www.examtopics.com/discussions/google/view/54535-exam-professional-cloud-architect-topic-1-question-34/",
    "body": "Your development team has installed a new Linux kernel module on the batch servers in Google Compute Engine (GCE) virtual machines (VMs) to speed up the nightly batch process. Two days after the installation, 50% of the batch servers failed the nightly batch run. You want to collect details on the failure to pass back to the development team.<br>Which three actions should you take? (Choose three.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Stackdriver Logging to search for the module log entries\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRead the debug GCE Activity log using the API or Cloud Console",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse gcloud or Cloud Console to connect to the serial console and observe the logs\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIdentify whether a live migration event of the failed server occurred, using in the activity log",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdjust the Google Stackdriver timeline to match the failure time, and observe the batch server metrics\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExport a debug VM into an image, and run the image on a local server where kernel log messages will be displayed on the native screen"
    ],
    "answer": "ACE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ACE",
        "count": 8,
        "isMostVoted": true
      },
      {
        "answer": "ABE",
        "count": 7,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-06-04T09:20:00.000Z",
        "voteCount": 45,
        "content": "ACE \nA. Use Stackdriver Logging to search for the module log entries = Check logs \nC. Use gcloud or Cloud Console to connect to the serial console and observe the logs = Check grub messages, remember new kernel module was installed. \nE. Adjust the Google Stackdriver timeline to match the failure time, and observe the batch server metrics = Zoom into the time window when problem happened."
      },
      {
        "date": "2021-06-14T01:11:00.000Z",
        "voteCount": 3,
        "content": "But the assumption you made is that stack driver was already installed on the vms.  What if it was not there? Would there be any scope to install later and retrieve the logs?"
      },
      {
        "date": "2023-01-03T20:01:00.000Z",
        "voteCount": 1,
        "content": "But isn't it the same with B? it is talking about 'reading' the logs."
      },
      {
        "date": "2022-05-20T22:01:00.000Z",
        "voteCount": 1,
        "content": "A, B, E\nC - doesn't look correct as it ends with \"observe the logs\" - question is on sharing the details to development team, not to look for cause"
      },
      {
        "date": "2024-06-05T10:56:00.000Z",
        "voteCount": 1,
        "content": "E observe the logs too. One of my problem is that several of the options go on and do some observation instead of just delivering."
      },
      {
        "date": "2021-12-04T02:14:00.000Z",
        "voteCount": 11,
        "content": "Go for A,B,E.\nC is when the VM is running , but in this case the sentence says \u201crecollect\u201d. It means that \u201cerror ever\u201d already happened."
      },
      {
        "date": "2022-01-08T14:02:00.000Z",
        "voteCount": 1,
        "content": "and how will activity log help?"
      },
      {
        "date": "2024-08-08T01:20:00.000Z",
        "voteCount": 1,
        "content": "Vote ACE"
      },
      {
        "date": "2024-05-01T08:28:00.000Z",
        "voteCount": 2,
        "content": "I'm not sure but I vote for ABE"
      },
      {
        "date": "2024-01-22T12:57:00.000Z",
        "voteCount": 1,
        "content": "ACE makes sense. \nA and E dont have any doubts. Questions is that if it is B, C or F.  \nWhats' use of serial console if team does not use it for logging especially kernal related updates.   that makes sense to choose C."
      },
      {
        "date": "2023-11-20T11:33:00.000Z",
        "voteCount": 3,
        "content": "Most automated way, with C, collecting from VMs involves a lot of manual efforts"
      },
      {
        "date": "2023-08-03T10:43:00.000Z",
        "voteCount": 2,
        "content": "ABE is correct"
      },
      {
        "date": "2023-01-03T05:06:00.000Z",
        "voteCount": 7,
        "content": "I'm really not sure here. A and E are just OK, and for me the final point is between B o C. many ppl is saying C, but, the question says that the VM's already failed and you're investigating what happened in the past. \n\nAnyway, there are 2 ways to interpret this, from my point of view:\n1) The failure happened and it's going to happen again. In this case ACE would be maybe the best option\nBUt\n2)  The failure happened and you want to investigate this failure, which happened in the past. Therefore ABE would be the right one, as you are \"splunking\" in the logs of the past, not having a review of the logs as they happen.\n\nfrom my personal interpretation I'd go with ABE"
      },
      {
        "date": "2022-12-21T00:12:00.000Z",
        "voteCount": 3,
        "content": "ACE\nTo collect details on the failure of the batch servers in GCE VMs, you can take the following actions:\n\nA: Stackdriver Logging can help you identify any issues related to the new Linux kernel module by searching for log entries related to the module.\n\nC: Connecting to the serial console allows you to view the logs in real-time as the batch servers are running. This can help you identify any issues related to the new kernel module.\n\nE: By adjusting the timeline in Stackdriver to match the failure time, you can view the batch server metrics during the time when the failures occurred. This can help you identify any issues related to the new kernel module.\n\nOther options, such as reading the debug GCE Activity log using the API or Cloud Console, identifying whether a live migration event of the failed server occurred, or exporting a debug VM into an image and running the image on a local server, may not provide the necessary information to understand"
      },
      {
        "date": "2022-10-13T05:52:00.000Z",
        "voteCount": 2,
        "content": "ACE by eliminating the incorrect answers"
      },
      {
        "date": "2022-09-21T21:56:00.000Z",
        "voteCount": 1,
        "content": "ADE seems correct"
      },
      {
        "date": "2022-09-19T06:46:00.000Z",
        "voteCount": 1,
        "content": "I vote for."
      },
      {
        "date": "2021-12-27T07:38:00.000Z",
        "voteCount": 4,
        "content": "This question is very poorly asked.\n\nThere is no place saying if the live migration is enabled. If a VM is not set to live migrate, the VM is terminated during host system events.\n\nThere is no place saying if you run into problems accessing your instance through SSH or need to troubleshoot an instance that is not fully booted, so you can enable interactive access to the serial console.\n\nC: https://cloud.google.com/compute/docs/troubleshooting/troubleshooting-using-serial-console\nD: https://cloud.google.com/compute/docs/instances/live-migration"
      },
      {
        "date": "2021-12-10T21:07:00.000Z",
        "voteCount": 2,
        "content": "ACE are the correct choices"
      },
      {
        "date": "2021-11-28T16:38:00.000Z",
        "voteCount": 3,
        "content": "vote ACE"
      },
      {
        "date": "2021-10-24T10:56:00.000Z",
        "voteCount": 6,
        "content": "I would say that Q26 = ABE\nA - since it investigates logs of Linux kernel module installed recently. For that Log Agent should be installed on VMs and Linux syslog is streamed by default to Stackdriver Logging via agent. So, this answer is relevant to Q's context, it checks if new Linux kernel runs OK.\nB - investigates \"app-level\" issues on GCE, logs API called from this VM, system events, etc.\nC - review of Serial Log is useful only for HW/OS crashes, and only during short period of time (since only last 1MB of logs are stored there, if more logs needed then they are streamed to Stackdriver logging). So, this option doesn't fit 2 days period and also serves different failure types.\nD - live migration event is irrelevant to this Q (transferring hot/running context of one VM to another transparently, so original VM can be maintained \u2013 BIOS/HW updates). Even if that happens, then GCE activity logs in B should cover this.\nE - monitoring of metrics at the time of failure makes sense for troubleshooting.\nF - smth long and ridiculous."
      },
      {
        "date": "2021-10-08T11:30:00.000Z",
        "voteCount": 1,
        "content": "If I'm reading \"F\" correctly, it is to export a VM and move it back to a \"local\" server which I'm reading as \"on-prem\", your laptop, or your local datacenter.(aka NOT GCP)  So if I'm reading that correctly, that is a very ineffective idea. Keep it in GCP and use the powerful GCP tools.  If somebody feels that I'm reading that wrong, I would love to see from a different POV. If you are reading that the same as me, then rule out F."
      },
      {
        "date": "2021-10-10T11:30:00.000Z",
        "voteCount": 1,
        "content": "F is practically impossible."
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 35,
    "url": "https://www.examtopics.com/discussions/google/view/54534-exam-professional-cloud-architect-topic-1-question-35/",
    "body": "Your company wants to try out the cloud with low risk. They want to archive approximately 100 TB of their log data to the cloud and test the analytics features available to them there, while also retaining that data as a long-term disaster recovery backup.<br>Which two steps should you take? (Choose two.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLoad logs into Google BigQuery\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLoad logs into Google Cloud SQL",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImport logs into Google Stackdriver",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInsert logs into Google Cloud Bigtable",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpload log files into Google Cloud Storage\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "AE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AE",
        "count": 10,
        "isMostVoted": true
      },
      {
        "answer": "CE",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-06-04T08:16:00.000Z",
        "voteCount": 33,
        "content": "Answer is A as they want to load logs for analytics and E for storing data in buckets for long term."
      },
      {
        "date": "2022-12-21T00:16:00.000Z",
        "voteCount": 11,
        "content": "AE\nTo archive approximately 100 TB of log data to the cloud and test the analytics features available while also retaining the data as a long-term disaster recovery backup, you can take the following steps:\n\nE: Upload log files into Google Cloud Storage: Google Cloud Storage is a scalable, durable, and fully-managed cloud storage service that can be used to store large amounts of data. You can upload your log files to Cloud Storage to archive them in the cloud.\n\nA: Load logs into Google BigQuery: Google BigQuery is a fully-managed, cloud-native data warehouse that can be used to analyze large amounts of data quickly and efficiently. You can load your log data into BigQuery to perform analytics on it and test the available analytics features.\n\nOther options, such as loading logs into Google Cloud SQL, importing logs into Google Stackdriver, or inserting logs into Google Cloud Bigtable, may not provide the necessary functionality for archiving and analyzing the log data."
      },
      {
        "date": "2023-04-03T05:36:00.000Z",
        "voteCount": 2,
        "content": "This looks right."
      },
      {
        "date": "2022-11-15T01:42:00.000Z",
        "voteCount": 3,
        "content": "If you want to analize those logs its recommended Big Query. For storing and backup Cloud Storage is your option, so AE"
      },
      {
        "date": "2022-10-16T14:46:00.000Z",
        "voteCount": 2,
        "content": "A and E can do the required task"
      },
      {
        "date": "2022-10-13T06:06:00.000Z",
        "voteCount": 1,
        "content": "AE are the only options for analysis and archiving"
      },
      {
        "date": "2022-09-21T21:59:00.000Z",
        "voteCount": 1,
        "content": "A load in Big query for analytics and E for cloud storage"
      },
      {
        "date": "2022-08-19T02:42:00.000Z",
        "voteCount": 1,
        "content": "The key word is 'Analytics' here the main reason for moving logs to GCP is to perform Analytics on the data. BigQuery is the best suite for it. For long term storage it wiould be GC"
      },
      {
        "date": "2022-08-08T08:01:00.000Z",
        "voteCount": 2,
        "content": "Answer is C and E\n\nKey features\nReal-time log management and analysis\nCloud Logging is a fully managed service that performs at scale and can ingest application and platform log data, as well as custom log data from GKE environments, VMs, and other services inside and outside of Google Cloud. Get advanced performance, troubleshooting, security, and business insights with Log Analytics, integrating the power of BigQuery into Cloud Logging.  -  https://cloud.google.com/products/operations"
      },
      {
        "date": "2022-08-12T19:52:00.000Z",
        "voteCount": 6,
        "content": "you don't do realtime log management on 10 TB data.\nYou only perform analytics on it. \nSo A for Analytics\nE for storage."
      },
      {
        "date": "2024-06-05T11:00:00.000Z",
        "voteCount": 1,
        "content": "Is it even possible to import 10TB of logs into StackDriver?"
      },
      {
        "date": "2022-08-19T02:42:00.000Z",
        "voteCount": 1,
        "content": "The key word is 'Analytics' here the main reason for moving logs to GCP is to perform Analytics on the data. BigQuery is the best suite for it. For long term storage it wiould be GCS"
      },
      {
        "date": "2022-08-04T03:52:00.000Z",
        "voteCount": 3,
        "content": "Big Query for analytics, Cloud Storage for long term archive"
      },
      {
        "date": "2022-06-18T01:16:00.000Z",
        "voteCount": 1,
        "content": "For Storage GCS is the best option and for analyzing the data BIg Query makes sense"
      },
      {
        "date": "2022-04-25T08:30:00.000Z",
        "voteCount": 2,
        "content": "A E are ok"
      },
      {
        "date": "2021-12-25T04:28:00.000Z",
        "voteCount": 3,
        "content": "AE are the correct answers"
      },
      {
        "date": "2021-12-16T19:41:00.000Z",
        "voteCount": 4,
        "content": "Answers: A is correct because BigQuery is the fully managed cloud data warehouse for analytics and supports the analytics requirement.\nE is correct because Cloud Storage provides the Coldline storage class to support long-term storage with infrequent access, which would support the long-term disaster recovery backup requirement.\nhttps://cloud.google.com/bigquery/\nhttps://cloud.google.com/stackdriver/\nhttps://cloud.google.com/storage/docs/storage-classes#coldline\nhttps://cloud.google.com/sql/\nhttps://cloud.google.com/bigtable/"
      },
      {
        "date": "2022-06-03T04:20:00.000Z",
        "voteCount": 1,
        "content": "But BigQuery is for SQL DATA, the logs are nosql ?\nWhy not choose stackdriver?"
      },
      {
        "date": "2021-12-04T02:15:00.000Z",
        "voteCount": 3,
        "content": "Go for A,E"
      },
      {
        "date": "2021-11-29T05:20:00.000Z",
        "voteCount": 1,
        "content": "A and E"
      },
      {
        "date": "2021-10-13T12:08:00.000Z",
        "voteCount": 2,
        "content": "A E is the right answer"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 36,
    "url": "https://www.examtopics.com/discussions/google/view/10522-exam-professional-cloud-architect-topic-1-question-36/",
    "body": "You created a pipeline that can deploy your source code changes to your infrastructure in instance groups for self-healing. One of the changes negatively affects your key performance indicator. You are not sure how to fix it, and investigation could take up to a week.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLog in to a server, and iterate on the fox locally",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRevert the source code change, and rerun the deployment pipeline\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLog into the servers with the bad code change, and swap in the previous code",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the instance group template to the previous one, and delete all instances"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 48,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 29,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-08-29T03:45:00.000Z",
        "voteCount": 74,
        "content": "Let's go with option elimination\n\nA. Log in to a server, and iterate on the fix locally\n&gt;&gt; Long step, hence eliminate\n\nB. Revert the source code change and rerun the deployment pipeline\n&gt;&gt; This revert will be logged in the source repo. Will go with this way although D also is correct.\n\nC. login to the servers with the bad code change, and swap in the previous code\n&gt;&gt; C is manually doing what can be automatically done by B and C, hence eliminate.\n\nD. Change the instance group template to the previous one and delete all instances\n&gt;&gt; This is similar to B but why manually do something which is automated. Hence eliminate. But is also correct. But B is better from code lifecycle perspective.\n\nHence B"
      },
      {
        "date": "2024-01-23T05:44:00.000Z",
        "voteCount": 3,
        "content": "The question itself looks the madeup. Not a real scenario ...\"You created a pipeline that can deploy your source code changes to your infrastructure in instance groups for self-healing. One of the changes negatively affects your key performance indicator.  \"  How a self healing code is affecting KPI.  What was KPI, we dont know. Was the self healing done? we dont know. Dont know who make this questions.  Even if we go whatever they try to ask, with  options available, B is safest. However this option is just answer to any troubleshooting step. I m not convinced for the person who wrote this question"
      },
      {
        "date": "2020-08-06T11:57:00.000Z",
        "voteCount": 62,
        "content": "Too many responses saying B is the answer - I wonder if GCP pays people to provide the wrong answers on this website. It's clearly D, MIG templates support versioning, they were created to solve this exact problem. You simply select the previous template version, set that as the new deployment, and it will roll back the KPI depriving deployment and roll out the previous working deployment. The only part of D I don't like is the \"terminate all instances\" since you should engage in a rolling deployment, but if it's not a live website I suppose that would be fine.\nhttps://cloud.google.com/compute/docs/instance-groups/rolling-out-updates-to-managed-instance-groups"
      },
      {
        "date": "2020-11-01T05:53:00.000Z",
        "voteCount": 21,
        "content": "If you can deploy your source code changes to the infrastructure in instance group for self-healing, it means you're not using Manage Instance Groups. Otherwise you would be creating a new template with the code changes. Further more, you would not delete instances on a MIG, you would be rolling out the previous template again in a controlled manner using maxsurge, maxunavailable, etc. For those reasons I'll choose B."
      },
      {
        "date": "2022-05-25T07:21:00.000Z",
        "voteCount": 2,
        "content": "B. keyword is \"self-healing\" not \"auto-healing\" - which means MIG not used. So correct answer is B"
      },
      {
        "date": "2021-10-18T07:21:00.000Z",
        "voteCount": 4,
        "content": "seems with approach, there will be a mismatch in pipeline"
      },
      {
        "date": "2022-01-18T11:07:00.000Z",
        "voteCount": 7,
        "content": "If you change manually the template.. why are using pipelines? B is the best answer because is automated!!! Why Google will be interested to vote the wrong answers??? They want more professionals with GCP certifications!!!!"
      },
      {
        "date": "2022-03-21T06:23:00.000Z",
        "voteCount": 1,
        "content": "\"....One of the changes has impacted negatively your PKI\". Why is the question about pipeline? It is about how to do investigations and keep your PKI at the proper SLA/SLO."
      },
      {
        "date": "2024-04-04T00:49:00.000Z",
        "voteCount": 1,
        "content": "Agree with B"
      },
      {
        "date": "2024-02-28T20:01:00.000Z",
        "voteCount": 3,
        "content": "D is infrastructure change. \nB is application change. \nSo B is correct."
      },
      {
        "date": "2024-01-14T09:15:00.000Z",
        "voteCount": 1,
        "content": "D\nThe question is talking abt MIG and you can revert Inst Template same as B. Since this is about MIG's I will choose D"
      },
      {
        "date": "2023-12-25T14:00:00.000Z",
        "voteCount": 1,
        "content": "D. This is a question about instance groups and so modifying templates should be what we're looking for."
      },
      {
        "date": "2023-12-19T01:53:00.000Z",
        "voteCount": 1,
        "content": "This a B for sure"
      },
      {
        "date": "2023-10-05T07:35:00.000Z",
        "voteCount": 2,
        "content": "The popped up with source code changes, hence reverting the change and deployment will solve the issue.\n\nB. Revert the source code change, and rerun the deployment pipeline"
      },
      {
        "date": "2023-09-26T17:49:00.000Z",
        "voteCount": 1,
        "content": "D is incorrect:\n - MIG instance group template is immutable, there is no version concept. The context never mentioned that team created multiple instance group templates.\n - Software code change might not all ended up in the instance group templates, it depends on how the deployment pipeline is configured. \n\nRegardless, B is a best practices, ensure that your infrastructure is synced with your source control system."
      },
      {
        "date": "2023-09-15T00:48:00.000Z",
        "voteCount": 2,
        "content": "You don't need to touch your code, just deploy and older version and fix the code, then deploy the fixed version"
      },
      {
        "date": "2023-08-14T04:44:00.000Z",
        "voteCount": 2,
        "content": "The most secure option is D\nRevert a source code change could be complex (if change was made in various components)"
      },
      {
        "date": "2023-06-22T00:12:00.000Z",
        "voteCount": 2,
        "content": "Because the question starting from source code"
      },
      {
        "date": "2023-06-08T04:44:00.000Z",
        "voteCount": 1,
        "content": "by bard:\nThe correct answer is B. Revert the source code change, and rerun the deployment pipeline."
      },
      {
        "date": "2023-06-02T09:31:00.000Z",
        "voteCount": 2,
        "content": "B. \nThe keyword here is source code not VM configuration. If it was the later then instance group templates is the answer. But in this case simply rollback your source code change and rerun to last workable version. Simples!"
      },
      {
        "date": "2023-05-28T23:30:00.000Z",
        "voteCount": 1,
        "content": "B is ok for me"
      },
      {
        "date": "2023-05-03T05:05:00.000Z",
        "voteCount": 1,
        "content": "I would go with D, but it depends on many details.\nif it is an app KPI and there is a real issue to the business I would opt to make the app work perfectly again ASAP and then I would address the issue in the code base.\nBut I can understand who would say that they would go with the pipeline approach."
      },
      {
        "date": "2023-03-26T00:34:00.000Z",
        "voteCount": 2,
        "content": "If a change negatively affects your key performance indicator, it's best to revert the source code change to a known good state and rerun the deployment pipeline. This ensures that your infrastructure is restored to a stable state while you investigate and fix the issue. Reverting the change and redeploying the code will allow your instance groups to continue functioning with the previous stable version, minimizing the impact on your application and users."
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 37,
    "url": "https://www.examtopics.com/discussions/google/view/7208-exam-professional-cloud-architect-topic-1-question-37/",
    "body": "Your organization wants to control IAM policies for different departments independently, but centrally.<br>Which approach should you take?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMultiple Organizations with multiple Folders",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMultiple Organizations, one for each department",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA single Organization with Folders for each department\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA single Organization with multiple projects, each with a central owner"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2019-11-20T07:22:00.000Z",
        "voteCount": 26,
        "content": "https://cloud.google.com/docs/enterprise/best-practices-for-enterprise-organizations\n\nI will stick with C"
      },
      {
        "date": "2022-10-16T14:51:00.000Z",
        "voteCount": 2,
        "content": "C is recommended approach"
      },
      {
        "date": "2022-09-28T10:57:00.000Z",
        "voteCount": 5,
        "content": "The reason it isn't D is that a Dept modelled as a project puts a massive constraint on the dept that they can only have a single project, it's likely a department will want many projects."
      },
      {
        "date": "2024-10-10T23:58:00.000Z",
        "voteCount": 1,
        "content": "B\n"
      },
      {
        "date": "2023-07-14T19:52:00.000Z",
        "voteCount": 1,
        "content": "Clearly C"
      },
      {
        "date": "2023-05-28T23:32:00.000Z",
        "voteCount": 1,
        "content": "Is obviously C"
      },
      {
        "date": "2022-12-21T00:46:00.000Z",
        "voteCount": 2,
        "content": "C. A single Organization with Folders for each department\n\nTo control IAM policies for different departments independently but centrally, you should create a single organization and use folders to organize the policies for each department. This approach allows you to centralize the management of IAM policies for all departments within a single organization, while also allowing you to set up different policies for each department as needed.\n\nOption A, multiple organizations with multiple folders, would not be an effective solution because it would create unnecessary complexity and make it more difficult to centralize the management of IAM policies. Option B, multiple organizations, one for each department, would also not be an effective solution because it would create unnecessary complexity and make it more difficult to centralize the management of IAM policies. Option D, a single organization with multiple projects, each with a central owner, would not be an effective solution because it would not allow you to set up different policies for each department as needed."
      },
      {
        "date": "2022-10-20T12:25:00.000Z",
        "voteCount": 3,
        "content": "C. It's a best practice and I've done this with my previous and current company :)"
      },
      {
        "date": "2022-09-30T00:19:00.000Z",
        "voteCount": 1,
        "content": "............."
      },
      {
        "date": "2022-09-22T02:08:00.000Z",
        "voteCount": 1,
        "content": "C single org and multiple folders"
      },
      {
        "date": "2022-04-01T12:42:00.000Z",
        "voteCount": 1,
        "content": "C is the right answer"
      },
      {
        "date": "2022-03-28T12:45:00.000Z",
        "voteCount": 1,
        "content": "Will go for C"
      },
      {
        "date": "2021-12-04T02:47:00.000Z",
        "voteCount": 2,
        "content": "Go for C"
      },
      {
        "date": "2021-11-27T09:30:00.000Z",
        "voteCount": 1,
        "content": "C is the right answer"
      },
      {
        "date": "2021-09-24T22:55:00.000Z",
        "voteCount": 1,
        "content": "C shall be the correct answer"
      },
      {
        "date": "2021-08-18T16:45:00.000Z",
        "voteCount": 1,
        "content": "C. Seems to be best practice (cf AWS56). And I believe that D should be excluded because it says \"Project owner\" - it is not best practice since it's a basic role + it's not even stated as a requisite"
      },
      {
        "date": "2021-05-18T23:14:00.000Z",
        "voteCount": 3,
        "content": "C. A single Organization with Folders for each department"
      },
      {
        "date": "2021-05-11T10:38:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 38,
    "url": "https://www.examtopics.com/discussions/google/view/7209-exam-professional-cloud-architect-topic-1-question-38/",
    "body": "You deploy your custom Java application to Google App Engine. It fails to deploy and gives you the following stack trace.<br>What should you do?<br><img src=\"/assets/media/exam-media/04339/0009300001.png\" class=\"in-exam-image\"><br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpload missing JAR files and redeploy your application.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDigitally sign all of your JAR files and redeploy your application\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRecompile the CLoakedServlet class using and MD5 hash instead of SHA1"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 12,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 5,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2019-10-25T11:02:00.000Z",
        "voteCount": 23,
        "content": "Signing the JAR files grants it permissions. (https://docs.oracle.com/javase/tutorial/deployment/jar/signindex.html)"
      },
      {
        "date": "2020-08-06T00:47:00.000Z",
        "voteCount": 9,
        "content": "B is ok"
      },
      {
        "date": "2021-10-12T19:41:00.000Z",
        "voteCount": 2,
        "content": "Where do you go? when we need you for other questions. Plz ans other q's if you have time"
      },
      {
        "date": "2021-03-04T15:28:00.000Z",
        "voteCount": 18,
        "content": "B, SHA1 Digest error in the first line in the error code. With Java errors, always focus on the first line in the error code, rest of the lines are garbage **mostly**."
      },
      {
        "date": "2022-12-21T00:55:00.000Z",
        "voteCount": 12,
        "content": "The most likely cause of the error is that one of the JAR files in your application has been tampered with or is corrupt. The SHA1 digest error indicates that the JAR file's signature does not match the expected value, which could be due to tampering or corruption.\n\nTo fix the issue, you should try uploading missing JAR files and redeploying your application. If the issue persists, you may need to digitally sign all of your JAR files and redeploy your application to ensure that the signatures are valid. You should not try to recompile the Cloaked"
      },
      {
        "date": "2024-02-28T20:29:00.000Z",
        "voteCount": 3,
        "content": "A missing JAR (Java ARchive) file indicates a problem with the code to be deployed.\nB Digitally signing all of your JAR files indicates a problem with the signature.\nA is better"
      },
      {
        "date": "2023-10-05T07:54:00.000Z",
        "voteCount": 1,
        "content": "\"JavaVerifier.Java.428\" is the key here"
      },
      {
        "date": "2023-08-22T07:38:00.000Z",
        "voteCount": 1,
        "content": "B is the correct answer."
      },
      {
        "date": "2023-03-15T01:10:00.000Z",
        "voteCount": 2,
        "content": "A. Upload missing JAR files and redeploy your application.\n\nThe error message indicates that there is a problem with the SHA1 digest for the \"com/altostrat/cloakedservlet.class\" file. This can be caused by a corrupted or incomplete JAR file. Therefore, the best course of action is to upload any missing JAR files and redeploy the application."
      },
      {
        "date": "2024-06-05T11:11:00.000Z",
        "voteCount": 1,
        "content": "Corrupted or incomplete. But not missing."
      },
      {
        "date": "2022-11-12T11:49:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2022-10-13T07:01:00.000Z",
        "voteCount": 7,
        "content": "Ok B but how is this question related to a GCP exam? I guess a google search will be faster than reading the theory around Java (unless you are a developer)."
      },
      {
        "date": "2022-10-16T22:25:00.000Z",
        "voteCount": 5,
        "content": "have you done any Azure exams? you will thank Google"
      },
      {
        "date": "2022-10-16T14:54:00.000Z",
        "voteCount": 3,
        "content": "nothing to do with GCP however basic troubleshooting skills required as a DevOps or Architect, B is fine"
      },
      {
        "date": "2023-01-03T20:16:00.000Z",
        "voteCount": 1,
        "content": "I see your point, but for basic troubleshooting of apps, i will usually have access to google (aka stackoverflow homepage). This could have been a cloud developer question that they repurposed."
      },
      {
        "date": "2022-09-22T02:12:00.000Z",
        "voteCount": 1,
        "content": "B is the right answer"
      },
      {
        "date": "2022-05-14T01:14:00.000Z",
        "voteCount": 1,
        "content": "B is Correct!"
      },
      {
        "date": "2022-05-02T11:12:00.000Z",
        "voteCount": 1,
        "content": "B is the right answer"
      },
      {
        "date": "2021-12-25T04:47:00.000Z",
        "voteCount": 2,
        "content": "B is the correct answer"
      },
      {
        "date": "2021-11-28T16:43:00.000Z",
        "voteCount": 1,
        "content": "vote B"
      },
      {
        "date": "2021-11-19T03:21:00.000Z",
        "voteCount": 1,
        "content": "B is correct answer"
      },
      {
        "date": "2021-07-08T17:36:00.000Z",
        "voteCount": 1,
        "content": "Option B. Digitally sign all of your JAR files and redeploy your application"
      },
      {
        "date": "2021-07-08T04:44:00.000Z",
        "voteCount": 1,
        "content": "Answer is B"
      },
      {
        "date": "2021-05-19T00:25:00.000Z",
        "voteCount": 3,
        "content": "B. Digitally sign all of your JAR files and redeploy your application"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 39,
    "url": "https://www.examtopics.com/discussions/google/view/6844-exam-professional-cloud-architect-topic-1-question-39/",
    "body": "You are designing a mobile chat application. You want to ensure people cannot spoof chat messages, by providing a message were sent by a specific user.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTag messages client side with the originating user identifier and the destination user.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEncrypt the message client side using block-based encryption with a shared key.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse public key infrastructure (PKI) to encrypt the message client side using the originating user's private key.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a trusted certificate authority to enable SSL connectivity between the client application and the server."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 28,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 8,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2019-10-23T04:55:00.000Z",
        "voteCount": 37,
        "content": "I am not sure about this one. D works if SSL client authentication is enabled.\nC works as well if client encrypts message with private key and server decrypt with public key.\nI prefer C."
      },
      {
        "date": "2019-11-14T08:05:00.000Z",
        "voteCount": 5,
        "content": "Agree with C"
      },
      {
        "date": "2020-01-14T16:39:00.000Z",
        "voteCount": 5,
        "content": "I agree with C on this one."
      },
      {
        "date": "2021-02-18T03:24:00.000Z",
        "voteCount": 13,
        "content": "Encrypting each block and tagging each message at the client side is an overhead on the application. Best method which has been adopted since years is contacting SSL provider and use public certificate to encrypt the traffic between client and server. \n\nD is correct"
      },
      {
        "date": "2022-01-18T11:18:00.000Z",
        "voteCount": 1,
        "content": "If you use server public key you aren't meeting the goal. Don't miss the \"for specific user\" in the statement"
      },
      {
        "date": "2021-02-25T01:53:00.000Z",
        "voteCount": 6,
        "content": "If you use the server's public certificate to encrypt your data you only ensure the right server is the only one to read you. \n\nBut anyone can use the same encryption key as you did and pretend to be you. Hence it does not solve our authentication problematic"
      },
      {
        "date": "2021-03-02T03:38:00.000Z",
        "voteCount": 2,
        "content": "thanks for your insight! C is correct."
      },
      {
        "date": "2023-11-27T08:48:00.000Z",
        "voteCount": 1,
        "content": "Can you explain why you think this?"
      },
      {
        "date": "2021-04-01T02:26:00.000Z",
        "voteCount": 1,
        "content": "I cannot agree with you. Before one be able to pretend to be someone else, he should know his (someone's) password on the Chat Server..."
      },
      {
        "date": "2021-08-02T02:30:00.000Z",
        "voteCount": 4,
        "content": "SSL doesn't use server's public key to encrypt data. This is definitely wrong. Please read SSL specs.  SSL uses a separate session key for message encryption.  This session key is temporary and will be rotated for every single session."
      },
      {
        "date": "2024-06-04T12:06:00.000Z",
        "voteCount": 3,
        "content": "The \"C\" answer is either messed up on purpose, or somebody dumped it wrong. \n\nWhen you use PKI (Public Key Infrastructure), you encrypt using a _public_ key of the recipient, and the recipient decrypts using their _private_ key. Sample reference that this is correct: https://www.keyfactor.com/education-center/what-is-pki/\n\nOn the contrary, when a messages is _digitally signed_, the originator is using their _private_ key to sign the message, and the recipient is verifying it using _public key_ of the _originator_.\n\nI still don't know which answer would I choose on the actual exam."
      },
      {
        "date": "2024-02-01T08:14:00.000Z",
        "voteCount": 1,
        "content": "Option A is not secure because anyone who intercepts the message could modify the user identifiers. Option B does not provide a way to verify the sender\u2019s identity. Option D is important for securing the connection between the client and server, but it does not prevent message spoofing by itself.\nHence C."
      },
      {
        "date": "2023-11-23T13:54:00.000Z",
        "voteCount": 1,
        "content": "I was considering D, but nope, C:\nhttps://support.google.com/messages/answer/10262381#:~:text=your%20device%20and%20the%20device%20you%20message"
      },
      {
        "date": "2023-10-26T20:59:00.000Z",
        "voteCount": 1,
        "content": "Answer : D\nLet me clarify , what PKI is saying i think first the answer is D , reason , Just understand what it says , i.e. option c - Using PKI to encrypt messages using the originating user's private key, now couple people are saying that it is good and then the server will decrypt the msg using public key, but can't you see anyone in the whole world will be able to see the messages as public key is available  publicly. ideally what should have been the solution, Using the public key of receiver the messages should have been encrypted then the receiver would have decrypted  using his private key, which absolutely makes sense, Talking about ssl i  think its one of the widely used secure tech for communication between client and server"
      },
      {
        "date": "2023-11-13T09:06:00.000Z",
        "voteCount": 1,
        "content": "Thanks for your explanation, so I choose C....."
      },
      {
        "date": "2023-11-27T08:49:00.000Z",
        "voteCount": 1,
        "content": "Why? Can you explain that?"
      },
      {
        "date": "2023-10-05T07:59:00.000Z",
        "voteCount": 4,
        "content": "Option C - Use public key infrastructure (PKI) to encrypt the message client-side using the originating user's private key: Using PKI to encrypt messages using the originating user's private key provides end-to-end encryption, which means only the intended recipient can decrypt the message. This option also ensures that the message's authenticity is protected. If a malicious user changes the sender's name, the recipient will not be able to decrypt the message since it was not encrypted using the correct private key. This option is a strong method for securing chat messages."
      },
      {
        "date": "2023-07-21T08:35:00.000Z",
        "voteCount": 3,
        "content": "C is wrong because private can only be used for signing, not encrypting. Public key is used for encrypting, private key is used for decrypting."
      },
      {
        "date": "2023-11-27T08:50:00.000Z",
        "voteCount": 1,
        "content": "But still, spoofing is not about reading data in clear but impersonating someone else. \nUsing D you can sign the message by confirming your identity."
      },
      {
        "date": "2023-07-06T02:28:00.000Z",
        "voteCount": 3,
        "content": "As question is about ensuring a specific user sent a message, answer could not be D, which would ensure secure message transmission, but not message origin (which can only be done by using asymmetric key)"
      },
      {
        "date": "2023-11-27T08:52:00.000Z",
        "voteCount": 1,
        "content": "So the sender is signing the message while encrypting it with the private key?"
      },
      {
        "date": "2023-06-08T04:51:00.000Z",
        "voteCount": 1,
        "content": "bard say D.\nand ChatGTP say C.."
      },
      {
        "date": "2023-04-03T05:42:00.000Z",
        "voteCount": 1,
        "content": "I noticed this question in other tests and the suggested answer was C."
      },
      {
        "date": "2023-03-15T01:14:00.000Z",
        "voteCount": 4,
        "content": "To prevent message spoofing, it is important to ensure that messages cannot be altered or forged by anyone other than the originating user. One way to accomplish this is by using public key infrastructure (PKI) to encrypt messages using the originating user's private key."
      },
      {
        "date": "2022-12-21T00:57:00.000Z",
        "voteCount": 2,
        "content": "To ensure that chat messages cannot be spoofed and that the messages are truly sent by a specific user, the best option would be to use public key infrastructure (PKI) to encrypt the message client side using the originating user's private key. This would allow the recipient to verify the authenticity of the message by using the originating user's public key to decrypt the message.\n\nOption A, tagging the message with the originating user identifier and the destination user, would not ensure the authenticity of the message, as it could potentially be forged by an attacker."
      },
      {
        "date": "2022-12-21T00:57:00.000Z",
        "voteCount": 1,
        "content": "Option B, encrypting the message using block-based encryption with a shared key, would also not ensure the authenticity of the message, as the shared key could potentially be compromised by an attacker.\n\nOption D, using a trusted certificate authority to enable SSL connectivity between the client application and the server, would help to secure the communication channel between the client and the server, but it would not necessarily ensure the authenticity of the chat messages themselves.\n\nOverall, using PKI and the originating user's private key to encrypt the message would be the most effective way to ensure the authenticity of the chat messages in your mobile chat application."
      },
      {
        "date": "2022-11-29T22:21:00.000Z",
        "voteCount": 3,
        "content": "C is the answer, The requirement is the integrity of messages sent in CIA security (Confidentiality, Integrity, and Availability). For Confidentiality, using PublicKey of receiver, for Integrity, using PrivateKey of sender. D works in case of SSL client authentication."
      },
      {
        "date": "2022-11-12T01:52:00.000Z",
        "voteCount": 1,
        "content": "C is ok"
      },
      {
        "date": "2022-11-04T03:59:00.000Z",
        "voteCount": 1,
        "content": "PKI uses X.509 certificates and Public Keys, where the key is used for end-to-end encrypted communication, so that both parties can trust each other and test their authenticity. PKI is mostly used in TLS/SSL to secure connections between the user and the server, while the user tests the server\u2019s authenticity to make sure it\u2019s not spoofed"
      },
      {
        "date": "2022-09-28T11:02:00.000Z",
        "voteCount": 5,
        "content": "C is the best\nA: Can be spoofed by amending the tags\nB: Shared key so can be spoofed\nC: Protects from start to end\nD: Encrypts the data in transit to the server.  Attack possible on server"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 40,
    "url": "https://www.examtopics.com/discussions/google/view/7211-exam-professional-cloud-architect-topic-1-question-40/",
    "body": "As part of implementing their disaster recovery plan, your company is trying to replicate their production MySQL database from their private data center to their<br>GCP project using a Google Cloud VPN connection. They are experiencing latency issues and a small amount of packet loss that is disrupting the replication.<br>What should they do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure their replication to use UDP.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a Google Cloud Dedicated Interconnect.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRestore their database daily using Google Cloud SQL.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd additional VPN connections and load balance them.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSend the replicated transaction to Google Cloud Pub/Sub."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 12,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 6,
        "isMostVoted": false
      },
      {
        "answer": "E",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-02-18T17:00:00.000Z",
        "voteCount": 33,
        "content": "It's latency issues. That won't be solved by adding another VPN tunnel. If it was just a throughput issue then VPN would do, however to improve latency you need to go layer 2. Answer is B"
      },
      {
        "date": "2019-11-15T04:31:00.000Z",
        "voteCount": 30,
        "content": "I think B is correct. I think it is more reliable."
      },
      {
        "date": "2024-05-19T01:26:00.000Z",
        "voteCount": 2,
        "content": "It's B, Interconnect.\nD is wrong in this case: While this might help distribute traffic, it won't solve the underlying issue of latency and packet loss caused by the inherent limitations of VPNs."
      },
      {
        "date": "2024-01-08T17:27:00.000Z",
        "voteCount": 2,
        "content": "Dedicated interconnect is the answer. A second VPN will give you an HA solution, not going to resolve the latency."
      },
      {
        "date": "2024-01-04T00:51:00.000Z",
        "voteCount": 2,
        "content": "Have you mind the budget you'll need to improve network infrastructure - whether it's dedicated interconnect or duplicating VPN connection? Mega IT corps may can afford, but I won't approve the work just for disaster recovery plan, if I were the authority. It's definatedly overkill.\nMain problem here is a latency issue and/or packet loss, yet the reason hasn't clearly configured. Whether it's occational, or repeatetive, and/or by DB engine or by network, mostly unknown. But you don't have to solve the problem if there's better bypass. Simply retry and test it. There C also can be a solution (which likely being placed already), but it doesn't have significant feature for logging a transaction success/fail. If you take advance Pub/Sub, you can track each transaction processes. Ordering, can adding another issue, but it worth a try - cost effectively."
      },
      {
        "date": "2023-12-19T02:15:00.000Z",
        "voteCount": 2,
        "content": "Correct answer is B as its a latency issue."
      },
      {
        "date": "2023-12-12T12:38:00.000Z",
        "voteCount": 2,
        "content": "I go for B.\nLatency won't be solved by adding new VPN tunnels."
      },
      {
        "date": "2023-09-13T09:36:00.000Z",
        "voteCount": 6,
        "content": "We have something called as HAVPN which uses 2 VPN Connections at a time. As this Question is old. we dont have this Option called use HAVPN. Now its Updated so the answer will be D. As its just a replication for disaster recovery and they are facing very minimal challenges"
      },
      {
        "date": "2023-03-04T22:07:00.000Z",
        "voteCount": 3,
        "content": "I think the best option is E, using PubSub. In question the main issue is \" a small amount of packet loss\". As per google PubSub documentation, data replication among databases is one of the common use case of PubSub. The asynchronously communication of PubSub can overcome small latency issues. Setting up dedicated interconnect would be very costly and required many pre-requisites."
      },
      {
        "date": "2023-03-02T18:30:00.000Z",
        "voteCount": 1,
        "content": "You could technically use your private key to encrypt a message, but it would not be secure because anyone who has your public key could decrypt the message. The recommended practice is to use your private key only for decryption and to use the recipient's public key for encryption.\n\nI vote for D"
      },
      {
        "date": "2023-03-02T09:32:00.000Z",
        "voteCount": 1,
        "content": "Configuring a Google Cloud Dedicated Interconnect  requirement for company is a proximity to colocation facility and meeting condition to have dedicated interconnect. Had this was possible why did they used Cloud VPN in the first place ?  I think answer should be D."
      },
      {
        "date": "2023-01-12T02:24:00.000Z",
        "voteCount": 2,
        "content": "If i face this issue, I will give a try with additional VPN, decision using VPN, maybe because they need encryption as well. With switching to dedicated interconnect, you have to implement your own VPN solution or application encryption. so it need more anaylsis to just skip VPN and use dedicated interconnect solution."
      },
      {
        "date": "2022-12-21T01:01:00.000Z",
        "voteCount": 2,
        "content": "The company should consider configuring a Google Cloud Dedicated Interconnect. A Google Cloud Dedicated Interconnect provides a private connection between the company's on-premises data center and GCP, which can help to reduce latency and improve the reliability of the connection. This can be particularly useful for replicating large amounts of data or for applications that require low-latency connectivity.\n\nOption A, configuring the replication to use UDP, would not necessarily improve the reliability of the connection, as UDP is a connectionless protocol that does not guarantee delivery of packets.\n\nOption C, restoring the database daily using Google Cloud SQL, would not address the underlying issues with the replication process."
      },
      {
        "date": "2022-12-21T01:01:00.000Z",
        "voteCount": 1,
        "content": "Option D, adding additional VPN connections and load balancing them, may help to improve the reliability of the connection by providing redundancy, but it may not necessarily address latency issues.\n\nOption E, sending the replicated transaction to Google Cloud Pub/Sub, could potentially help to improve the reliability of the replication process by allowing the company to handle failures and retries in a more structured way, but it would not necessarily address latency issues.\n\nOverall, configuring a Google Cloud Dedicated Interconnect is likely to be the most effective solution for addressing latency issues and packet loss in the replication process."
      },
      {
        "date": "2023-01-31T14:13:00.000Z",
        "voteCount": 3,
        "content": "Why focus on latency when the solution is for disaster recovery? The main issue is packet loss. While this can be solved with Dedicated Interconnect or Cloud Pub/Sub, PubSub seems like a cheaper alternative that prevents data loss and achieves reliability.  I wouldn't care about latency as the backup is only for DA.. so how does it matter if it goes slowly?"
      },
      {
        "date": "2023-10-07T20:33:00.000Z",
        "voteCount": 2,
        "content": "Because latency and packet loss are probably coming from traffic going over public internet. This is the Cloud VPN definition from the public documentation:\n\n\"Cloud VPN securely extends your peer network to Google's network through an IPsec VPN tunnel. Traffic is encrypted and travels between the two networks over the public internet. Cloud VPN is useful for low-volume data connections.\"\n\n\nThere are documents showing what happens when public internet is used, but basically there's no way to prevent information from going through multiple hops over the internet, which is why Dedicated Interconnect should work. Plus, VPN is recommended for low-volume data connections, and I highly doubt that replicating a database is considered a low-volume operation. \n\nI'm going with B: using Cloud Dedicated Interconnect."
      },
      {
        "date": "2022-11-13T06:38:00.000Z",
        "voteCount": 3,
        "content": "so just to  solve this  issue we are going over a Dedicated Interconnect imagine saying this to a your project head."
      },
      {
        "date": "2022-11-05T08:27:00.000Z",
        "voteCount": 2,
        "content": "ok for B"
      },
      {
        "date": "2022-10-16T14:58:00.000Z",
        "voteCount": 2,
        "content": "This is straight forward question, B is right. Dedicated line solves latency issues"
      },
      {
        "date": "2022-06-18T01:25:00.000Z",
        "voteCount": 1,
        "content": "Latency issue - Dedicated Interconnect solves the issue"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 41,
    "url": "https://www.examtopics.com/discussions/google/view/11803-exam-professional-cloud-architect-topic-1-question-41/",
    "body": "Your customer support tool logs all email and chat conversations to Cloud Bigtable for retention and analysis. What is the recommended approach for sanitizing this data of personally identifiable information or payment card information before initial storage?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tHash all data using SHA256",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEncrypt all data using elliptic curve cryptography",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDe-identify the data with the Cloud Data Loss Prevention API\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse regular expressions to find and redact phone numbers, email addresses, and credit card numbers"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-01-12T05:02:00.000Z",
        "voteCount": 23,
        "content": "C is the answer"
      },
      {
        "date": "2021-03-04T15:34:00.000Z",
        "voteCount": 8,
        "content": "C, data sanitization = DLP"
      },
      {
        "date": "2020-08-14T02:52:00.000Z",
        "voteCount": 8,
        "content": "C is ok"
      },
      {
        "date": "2022-12-21T01:58:00.000Z",
        "voteCount": 11,
        "content": "The recommended approach for sanitizing data of personally identifiable information or payment card information before storing it in Cloud Bigtable is option C: De-identify the data with the Cloud Data Loss Prevention API.\n\nThe Cloud Data Loss Prevention (DLP) API is a powerful tool that allows you to automatically discover, classify, and redact sensitive data in your organization. It uses advanced machine learning techniques to accurately identify and protect a wide range of sensitive data types, including personal information such as names, addresses, phone numbers, and payment card information.\n\nUsing the DLP API to de-identify your data before storing it in Cloud Bigtable is the most effective way to ensure that sensitive information is protected and not accessible to unauthorized users."
      },
      {
        "date": "2022-12-21T01:58:00.000Z",
        "voteCount": 3,
        "content": "Option A: Hashing data using SHA256 is not sufficient for protecting sensitive information, as hashes can be reversed using various techniques.\n\nOption B: Encrypting data using elliptic curve cryptography is a good option for protecting data, but it requires that you have a secure way to store and manage the encryption keys. If the keys are lost or compromised, the data will be inaccessible.\n\nOption D: Using regular expressions to find and redact phone numbers, email addresses, and credit card numbers can be effective in some cases, but it requires that you have a complete and up-to-date list of all the data patterns that you want to protect. It is also prone to errors and may not be able to detect all instances of sensitive data."
      },
      {
        "date": "2023-05-03T07:22:00.000Z",
        "voteCount": 1,
        "content": "About D, usually is recommended that you don\u00b4t reinvent the wheel specially when talking about security ."
      },
      {
        "date": "2023-09-12T03:57:00.000Z",
        "voteCount": 1,
        "content": "Without any doubt C is the right answer as the DLP API is a flexible and robust tool that helps identify sensitive data like credit card numbers, social security numbers, names and other forms of personally identifiable information (PII)."
      },
      {
        "date": "2023-04-01T09:54:00.000Z",
        "voteCount": 1,
        "content": "Cloud Data Loss Prevention API provides obfuscation and de-identification methods like masking and tokenization. Especially for credit card transactions, the card numbers are supposed to be tokenized. Therefore, this API is helpful."
      },
      {
        "date": "2022-11-21T08:47:00.000Z",
        "voteCount": 1,
        "content": "C for sure !"
      },
      {
        "date": "2022-11-19T05:57:00.000Z",
        "voteCount": 1,
        "content": "C is correct, DLP is the solution"
      },
      {
        "date": "2022-11-06T08:08:00.000Z",
        "voteCount": 1,
        "content": "ok for C"
      },
      {
        "date": "2021-12-25T04:52:00.000Z",
        "voteCount": 2,
        "content": "C is the correct answer"
      },
      {
        "date": "2021-12-05T04:06:00.000Z",
        "voteCount": 3,
        "content": "Go for C"
      },
      {
        "date": "2021-12-05T04:07:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/dlp"
      },
      {
        "date": "2021-07-08T05:02:00.000Z",
        "voteCount": 1,
        "content": "Answer is C"
      },
      {
        "date": "2021-05-19T00:23:00.000Z",
        "voteCount": 3,
        "content": "C. De-identify the data with the Cloud Data Loss Prevention API"
      },
      {
        "date": "2021-05-12T01:14:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2021-04-21T02:19:00.000Z",
        "voteCount": 3,
        "content": "Effectively reduce data risk with de-identification methods like masking and tokenization\nhttps://cloud.google.com/dlp"
      },
      {
        "date": "2021-03-30T22:47:00.000Z",
        "voteCount": 1,
        "content": "Answer is C"
      },
      {
        "date": "2021-03-30T07:03:00.000Z",
        "voteCount": 1,
        "content": "C is ok"
      },
      {
        "date": "2021-01-25T20:22:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/dlp\nEffectively reduce data risk with de-identification methods like masking and tokenization.\nC is right."
      },
      {
        "date": "2020-09-26T03:26:00.000Z",
        "voteCount": 1,
        "content": "I LOL on answer A.... *clap* *clap* *clap*.... hash all data..."
      },
      {
        "date": "2020-10-07T16:40:00.000Z",
        "voteCount": 2,
        "content": "correct answer is C - https://cloud.google.com/solutions/de-identification-re-identification-pii-using-cloud-dlp"
      },
      {
        "date": "2022-10-16T15:00:00.000Z",
        "voteCount": 1,
        "content": "yes. thank you for sharing the link"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 42,
    "url": "https://www.examtopics.com/discussions/google/view/7212-exam-professional-cloud-architect-topic-1-question-42/",
    "body": "You are using Cloud Shell and need to install a custom utility for use in a few weeks. Where can you store the file so it is in the default execution path and persists across sessions?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t~/bin\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Storage",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t/google/scripts",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t/usr/local/bin"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 7,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2019-10-29T22:21:00.000Z",
        "voteCount": 72,
        "content": "A  is correct\nhttps://cloud.google.com/shell/docs/how-cloud-shell-works\nCloud Shell provisions 5 GB of free persistent disk storage mounted as your $HOME directory on the virtual machine instance. This storage is on a per-user basis and is available across projects. Unlike the instance itself, this storage does not time out on inactivity. All files you store in your home directory, including installed software, scripts and user configuration files like .bashrc and .vimrc, persist between sessions. Your $HOME directory is private to you and cannot be accessed by other users."
      },
      {
        "date": "2021-04-12T06:25:00.000Z",
        "voteCount": 17,
        "content": "Maybe also to mention is that ~/bin is located in the $HOME directory"
      },
      {
        "date": "2021-09-14T07:01:00.000Z",
        "voteCount": 9,
        "content": "cd ~/   is egual at   cd $HOME\n~/bin  is egual a  cd $HOME/bin\nthe persistent disk in cloud shell is for $HOME"
      },
      {
        "date": "2022-10-16T15:01:00.000Z",
        "voteCount": 1,
        "content": "Agree. A is right"
      },
      {
        "date": "2020-11-18T14:32:00.000Z",
        "voteCount": 1,
        "content": "$HOME is not ~/bin. So 'C' is the answer."
      },
      {
        "date": "2021-09-14T07:01:00.000Z",
        "voteCount": 10,
        "content": "cd ~/ is egual at cd $HOME\n~/bin is egual a cd $HOME/bin\nthe persistent disk in cloud shell is for $HOME"
      },
      {
        "date": "2020-05-07T06:39:00.000Z",
        "voteCount": 2,
        "content": "Won\u2019t the persistent disk be auto-delete enabled by default, whereby the work maybe lost. Would that not be sufficient reason to consider Cloud storage instead. Thanks"
      },
      {
        "date": "2020-06-23T02:18:00.000Z",
        "voteCount": 3,
        "content": "The virtual machine instance that backs your Cloud Shell session is not permanently allocated to a Cloud Shell session and terminates if the session is inactive for an hour. After the instance is terminated, any modifications that you made to it outside your $HOME are lost."
      },
      {
        "date": "2021-09-14T07:02:00.000Z",
        "voteCount": 1,
        "content": "Cloud Shell provisions 5 GB of free persistent disk storage mounted as your $HOME"
      },
      {
        "date": "2019-10-25T14:21:00.000Z",
        "voteCount": 21,
        "content": "Well, I just double checked and if they were referring to the PATH variable then /usr/local/bin is also a correct answer........................................"
      },
      {
        "date": "2023-11-14T09:18:00.000Z",
        "voteCount": 2,
        "content": "/usr/local/bin is the place where the files will persist across sessions.\nHence, D."
      },
      {
        "date": "2023-08-22T12:48:00.000Z",
        "voteCount": 1,
        "content": "A is correct!"
      },
      {
        "date": "2023-02-04T10:22:00.000Z",
        "voteCount": 2,
        "content": "I tested this. Although ~/bin is not in the default $PATH, choice D is definitely not persisting across sessions."
      },
      {
        "date": "2023-06-13T03:11:00.000Z",
        "voteCount": 4,
        "content": "1. When logging in to cloud shell for the first time, the ~/bin directory does not exist\n2. mkdir ~/bin\n3. After re-login to the cloud shell, $PATH will automatically add ~/bin\nSo A is the correct answer"
      },
      {
        "date": "2022-12-29T20:12:00.000Z",
        "voteCount": 2,
        "content": "At this moment default directory cant be set as Cloud storage bucket, so no C.\nA will be correct as zonal PD with preinstalled tools 5gb  available that does not timeout!"
      },
      {
        "date": "2022-12-21T02:15:00.000Z",
        "voteCount": 3,
        "content": "The recommended location for storing a custom utility file that you want to use in Cloud Shell and that should be in the default execution path and persist across sessions is option A: ~/bin.\n\nThe ~/bin directory is a personal directory that is in the default execution path for all users in Cloud Shell. Any executable files that you place in this directory will be available to you whenever you log in to Cloud Shell, and they will persist across sessions."
      },
      {
        "date": "2022-12-21T02:15:00.000Z",
        "voteCount": 3,
        "content": "Option B: Cloud Storage is not a suitable location for storing a custom utility file that you want to use in Cloud Shell, as it is not in the default execution path and would require additional steps to make it accessible.\n\nOption C: The /google/scripts directory is not a suitable location for storing a custom utility file, as it is not in the default execution path and is intended for use by Google Cloud system processes.\n\nOption D: The /usr/local/bin directory is a system directory that is in the default execution path for all users, but it is not a suitable location for storing a custom utility file, as any files that you place in this directory may be deleted or overwritten during system updates."
      },
      {
        "date": "2022-11-07T08:00:00.000Z",
        "voteCount": 1,
        "content": "For sure correct answer is A"
      },
      {
        "date": "2022-11-06T08:10:00.000Z",
        "voteCount": 1,
        "content": "ok for A"
      },
      {
        "date": "2022-10-13T07:15:00.000Z",
        "voteCount": 1,
        "content": "A. ~/bin"
      },
      {
        "date": "2022-02-14T03:48:00.000Z",
        "voteCount": 4,
        "content": "Cloud Shell provisions 5 GB of persistent disk storage mounted as your $HOME directory on the Cloud Shell instance. All files you store in your home directory, including scripts and user configuration files like .bashrc and .vimrc, persist between sessions.\n\nReference- https://cloud.google.com/shell/?utm_source=google&amp;utm_medium=cpc&amp;utm_campaign=japac-IN-all-en-dr-bkwsrmkt-all-all-trial-e-dr-1009882&amp;utm_content=text-ad-none-none-DEV_c-CRE_442449534611-ADGP_Hybrid%20%7C%20BKWS%20-%20EXA%20%7C%20Txt%20~%20Management%20Tools%20~%20Cloud%20Shell_cloud%20shell-general%20-%20Products-KWID_43700054972141701-kwd-837034669893&amp;userloc_9302140-network_g&amp;utm_term=KW_gcp%20cloud%20shell&amp;gclsrc=ds&amp;gclsrc=ds"
      },
      {
        "date": "2022-01-01T03:23:00.000Z",
        "voteCount": 2,
        "content": "I think D is correct.ummm"
      },
      {
        "date": "2021-12-25T04:55:00.000Z",
        "voteCount": 1,
        "content": "A is the correct answer"
      },
      {
        "date": "2021-11-04T08:11:00.000Z",
        "voteCount": 1,
        "content": "A is for sure. ~ stands for user's home"
      },
      {
        "date": "2021-07-08T05:06:00.000Z",
        "voteCount": 1,
        "content": "Answer is A"
      },
      {
        "date": "2021-06-28T11:58:00.000Z",
        "voteCount": 1,
        "content": "hey guys check Q3 for new Qs, 49 New Qs"
      },
      {
        "date": "2021-06-27T00:21:00.000Z",
        "voteCount": 1,
        "content": "A is correct. Cloud Shell provides 5 GB persistent disk and data in Home directory will persist"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 43,
    "url": "https://www.examtopics.com/discussions/google/view/11804-exam-professional-cloud-architect-topic-1-question-43/",
    "body": "You want to create a private connection between your instances on Compute Engine and your on-premises data center. You require a connection of at least 20<br>Gbps. You want to follow Google-recommended practices. How should you set up the connection?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a VPC and connect it to your on-premises data center using Dedicated Interconnect.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a VPC and connect it to your on-premises data center using a single Cloud VPN.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Cloud Content Delivery Network (Cloud CDN) and connect it to your on-premises data center using Dedicated Interconnect.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Cloud Content Delivery Network (Cloud CDN) and connect it to your on-premises datacenter using a single Cloud VPN."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-01-12T05:13:00.000Z",
        "voteCount": 45,
        "content": "Cloud VPN supports unto 3 Gbps where as Interconnect can support unto 100 gbps... I'll go with A"
      },
      {
        "date": "2020-08-06T01:20:00.000Z",
        "voteCount": 7,
        "content": "A is ok"
      },
      {
        "date": "2021-01-13T09:33:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/network-connectivity/docs/vpn/concepts/overview#network-bandwidth"
      },
      {
        "date": "2021-03-04T15:36:00.000Z",
        "voteCount": 2,
        "content": "A, 20Gbps dedicated interconnect is the way."
      },
      {
        "date": "2022-10-16T15:03:00.000Z",
        "voteCount": 2,
        "content": "A is required for consistent speed and VPN not supports that speed"
      },
      {
        "date": "2022-12-21T02:17:00.000Z",
        "voteCount": 11,
        "content": "Answer is A: Dedicated Interconnect is a service that allows you to create a dedicated, high-bandwidth network connection between your on-premises data center and Google Cloud. It is the recommended solution for creating a private connection between your on-premises data center and Google Cloud when you require a connection of at least 20 Gbps.\n\nOption B: Using a single Cloud VPN to connect your VPC to your on-premises data center is not suitable for a connection of at least 20 Gbps, as Cloud VPN has a maximum capacity of 30 Gbps.\n\nOption C: The Cloud Content Delivery Network (Cloud CDN) is a globally distributed network of caching servers that speeds up the delivery of static and dynamic web content. It is not suitable for creating a private connection between your instances on Compute Engine and your on-premises data center.\n\nOption D: Connecting your Cloud CDN to your on-premises data center using a single Cloud VPN is not suitable for a connection of at least 20 Gbps, as Cloud VPN has a maximum capacity of 30 Gbps."
      },
      {
        "date": "2023-02-28T07:03:00.000Z",
        "voteCount": 4,
        "content": "Very nice answer, I think you meant 3 Gbps for Cloud VPN."
      },
      {
        "date": "2023-08-22T12:49:00.000Z",
        "voteCount": 1,
        "content": "high bandwidth means A"
      },
      {
        "date": "2022-12-16T06:09:00.000Z",
        "voteCount": 1,
        "content": "the question mention 20gbs for the least, it should be Dedicated Interconnect. The answer is A"
      },
      {
        "date": "2022-11-06T08:12:00.000Z",
        "voteCount": 1,
        "content": "ok for A"
      },
      {
        "date": "2022-09-04T06:21:00.000Z",
        "voteCount": 1,
        "content": "Any connection between On-Prem and GCP and requires high speed I'd choose dedicated interconnect"
      },
      {
        "date": "2022-05-14T01:21:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2021-12-25T04:57:00.000Z",
        "voteCount": 2,
        "content": "A is the correct answer"
      },
      {
        "date": "2021-12-05T04:15:00.000Z",
        "voteCount": 2,
        "content": "Go for A"
      },
      {
        "date": "2021-11-28T16:47:00.000Z",
        "voteCount": 2,
        "content": "vote A"
      },
      {
        "date": "2021-11-28T16:45:00.000Z",
        "voteCount": 3,
        "content": "vote A"
      },
      {
        "date": "2021-10-09T02:39:00.000Z",
        "voteCount": 1,
        "content": "A is good option (easily elminate C &amp; D)  and B with connection speed.\n10Gbps per link for Dedicated Interconnect and Direct Peering\n1.5-3Gbps per tunnel for Cloud VPN\n50Mbps to 10Gbps per connection - Partner Interconnect\nnoSLA - Carrier Peering"
      },
      {
        "date": "2021-08-29T07:08:00.000Z",
        "voteCount": 3,
        "content": "Let's go with option elimination\nA. Create a VPC and connect it to your on-premises data centre using Dedicated Interconnect.\n&gt;&gt; Is the only remaining best option after elimination. As per the document, its partner interconnects with VPN. Interconnect is between GCP and on-prem. URL1\n\nB. Create a VPC and connect it to your on-premises data centre using a single Cloud VPN.\n&gt;&gt; max 3 gigabits per second (Gbps) eliminate the option.\n\nC. Create a Cloud Content Delivery Network (Cloud CDN) and connect it to your on-premises data centre using Dedicated Interconnect.\n&gt;&gt; CDN is for egress traffic or static content hosting hence eliminate the option URL2\n\nD. Create a Cloud Content Delivery Network (Cloud CDN) and connect it to your on-premises datacenter using a single Cloud VPN.\n&gt;&gt; CDN is for egress traffic or static content hosting hence eliminate the option URL2\n\nHence A\n\nURL1 https://cloud.google.com/network-connectivity/docs/interconnect/concepts/overview\nURL2 https://cloud.google.com/network-connectivity/docs/cdn-interconnect"
      },
      {
        "date": "2021-07-08T05:07:00.000Z",
        "voteCount": 1,
        "content": "Answer is A"
      },
      {
        "date": "2021-06-27T00:26:00.000Z",
        "voteCount": 1,
        "content": "A is correct. Dedicated Interconnect supports upto 80 GBPS"
      },
      {
        "date": "2021-05-19T00:55:00.000Z",
        "voteCount": 2,
        "content": "A. Create a VPC and connect it to your on-premises data center using Dedicated Interconnect."
      },
      {
        "date": "2021-05-12T01:24:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 44,
    "url": "https://www.examtopics.com/discussions/google/view/7190-exam-professional-cloud-architect-topic-1-question-44/",
    "body": "You are analyzing and defining business processes to support your startup's trial usage of GCP, and you don't yet know what consumer demand for your product will be. Your manager requires you to minimize GCP service costs and adhere to Google best practices. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUtilize free tier and sustained use discounts. Provision a staff position for service cost management.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUtilize free tier and sustained use discounts. Provide training to the team about service cost management.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUtilize free tier and committed use discounts. Provision a staff position for service cost management.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUtilize free tier and committed use discounts. Provide training to the team about service cost management."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-12-27T09:33:00.000Z",
        "voteCount": 43,
        "content": "Sustained are automatic discounts for running specific GCE a significant portion of the billing month: https://cloud.google.com/compute/docs/sustained-use-discounts\n\nCommitted is for workloads with predictable resource needs between 1 year or 3 year, discount is up to 57% for most resources:  https://cloud.google.com/compute/docs/instances/signing-up-committed-use-discounts"
      },
      {
        "date": "2024-01-23T06:28:00.000Z",
        "voteCount": 2,
        "content": "Adding to it. \n Provide training to the team about service cost management: Because we are still using free tier, and no need a separate position even if that need arises. Its startup Company any way, and they are trained to work beyond hours :-)"
      },
      {
        "date": "2022-01-26T07:57:00.000Z",
        "voteCount": 5,
        "content": "Best answer!"
      },
      {
        "date": "2019-10-25T00:49:00.000Z",
        "voteCount": 40,
        "content": "I would choose \"B\""
      },
      {
        "date": "2020-08-06T01:22:00.000Z",
        "voteCount": 11,
        "content": "B is ok"
      },
      {
        "date": "2021-03-04T15:36:00.000Z",
        "voteCount": 3,
        "content": "B reason minimize GCP service costs"
      },
      {
        "date": "2022-12-21T02:29:00.000Z",
        "voteCount": 7,
        "content": "The recommended approach for minimizing GCP service costs and adhering to Google best practices are: \n- Free tier: Google Cloud offers a free tier of services that allows you to try out many of its products for free, up to certain usage limits. Utilizing the free tier can help you minimize your GCP service costs while you are in the trial usage phase.\n\n- Committed use discounts: Committed use discounts are a type of discount that you can apply to certain GCP products by committing to a certain level of usage over a one- or three-year period. Committed use discounts can help you save on your GCP service costs, but they require you to commit to a certain level of usage, which may not be suitable if you are unsure of your future demand.\n\n- Providing training to the team about service cost management: It is important that your team is aware of the different options available for minimizing GCP service costs and understands how to manage and monitor their usage of GCP services. Providing training on service cost management can help your team make informed decisions about how to use GCP services in the most cost-effective way."
      },
      {
        "date": "2022-12-21T02:29:00.000Z",
        "voteCount": 1,
        "content": "The recommended approach for minimizing GCP service costs and adhering to Google best practices while your startup is in the trial usage phase and you don't yet know what consumer demand for your product will be is option D: Utilize free tier and committed use discounts. Provide training to the team about service cost management."
      },
      {
        "date": "2022-12-21T02:30:00.000Z",
        "voteCount": 1,
        "content": "Sustained use discounts are based on your usage of GCP services over a certain period, and are not available for all GCP products. \n\nProvisioning a staff position for service cost management may not be necessary if you provide training to the team about service cost management."
      },
      {
        "date": "2022-11-24T21:53:00.000Z",
        "voteCount": 1,
        "content": "Sustained use discounts - Compute Engine - Google Cloudhttps://cloud.google.com \u203a ... \u203a Documentation\nCompute Engine offers sustained use discounts on resources that are used for more than 25% of a billing month   - There trial could be more than 7 or 8 days , at this point commitment of use can not be provided due to trails stage of gcp use"
      },
      {
        "date": "2022-11-13T05:22:00.000Z",
        "voteCount": 1,
        "content": "The answer is B, because Sustained use discount can give up to 30% and requires no commitment."
      },
      {
        "date": "2022-11-06T08:15:00.000Z",
        "voteCount": 1,
        "content": "ok for B"
      },
      {
        "date": "2022-10-16T23:01:00.000Z",
        "voteCount": 1,
        "content": "committed use discounts are for long-run discounts which in the case of startup they're trying GCP. So options C and D are out\nB is the correct answer"
      },
      {
        "date": "2022-10-16T15:04:00.000Z",
        "voteCount": 1,
        "content": "I will choose B, D is only long term commitment"
      },
      {
        "date": "2022-10-13T07:26:00.000Z",
        "voteCount": 2,
        "content": "B. Utilize free tier and sustained use discounts. Provide training to the team about service cost management."
      },
      {
        "date": "2022-09-28T11:30:00.000Z",
        "voteCount": 1,
        "content": "Sustained use discount makes sense over committed as not enough info to know what to comit to.  Provide staff training on how to keep things cheap gonna further keep cost down."
      },
      {
        "date": "2022-04-01T12:55:00.000Z",
        "voteCount": 1,
        "content": "B is the correct option"
      },
      {
        "date": "2022-02-23T02:28:00.000Z",
        "voteCount": 2,
        "content": "free tier (monthly discounts) does not make sense combined with committed use discounts - anual base, dont't you think so?"
      },
      {
        "date": "2021-12-25T05:04:00.000Z",
        "voteCount": 1,
        "content": "B is the correct answer"
      },
      {
        "date": "2021-09-08T15:31:00.000Z",
        "voteCount": 1,
        "content": "Answer is B"
      },
      {
        "date": "2021-07-30T02:28:00.000Z",
        "voteCount": 5,
        "content": "Answer B\n Sustained use discounts are applied on incremental use after you reach certain usage thresholds. This means that you pay only for the number of minutes that you use an instance, and Compute Engine automatically gives you the best price. There's no reason to run an instance for longer than you need it.\n - https://cloud.google.com/compute/docs/sustained-use-discounts\nCommitted use discounts are ideal for workloads with predictable resource needs. When you purchase a committed use contract, you purchase compute resource (vCPUs, memory, GPUs, and local SSDs) at a discounted price in return for committing to paying for those resources for 1 year or 3 years. The discount is up to 57% for most resources like machine types or GPUs. The discount is up to 70% for memory-optimized machine types. For committed use prices for different machine types, see VM instances pricing.\n - https://cloud.google.com/compute/docs/instances/signing-up-committed-use-discounts"
      },
      {
        "date": "2021-07-08T05:26:00.000Z",
        "voteCount": 1,
        "content": "Answer is B"
      },
      {
        "date": "2021-06-27T00:28:00.000Z",
        "voteCount": 1,
        "content": "B is correct as demand is not known"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 45,
    "url": "https://www.examtopics.com/discussions/google/view/8197-exam-professional-cloud-architect-topic-1-question-45/",
    "body": "You are building a continuous deployment pipeline for a project stored in a Git source repository and want to ensure that code changes can be verified before deploying to production. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Spinnaker to deploy builds to production using the red/black deployment strategy so that changes can easily be rolled back.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Spinnaker to deploy builds to production and run tests on production deployments.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Jenkins to build the staging branches and the master branch. Build and deploy changes to production for 10% of users before doing a complete rollout.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Jenkins to monitor tags in the repository. Deploy staging tags to a staging environment for testing. After testing, tag the repository for production and deploy that to the production environment.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 23,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-04-11T04:00:00.000Z",
        "voteCount": 59,
        "content": "I believe the best answer is D, because the tagging is a best practice that is recommended on Jenkins/Spinnaker to deploy the right code and prevent accidentally (or intentionally) push of wrong code to production environments. See https://stackify.com/continuous-delivery-git-jenkins/"
      },
      {
        "date": "2020-06-03T06:28:00.000Z",
        "voteCount": 10,
        "content": "Agreed with D as the right answer. The url provided explains it better"
      },
      {
        "date": "2022-10-16T15:06:00.000Z",
        "voteCount": 1,
        "content": "yes, D is correct"
      },
      {
        "date": "2022-10-16T23:04:00.000Z",
        "voteCount": 5,
        "content": "How can I join Google"
      },
      {
        "date": "2020-08-22T23:25:00.000Z",
        "voteCount": 53,
        "content": "I got this question in real exam. This question states \"before deploying to production\" environment. So i picked D . I passed the exam."
      },
      {
        "date": "2021-01-25T21:07:00.000Z",
        "voteCount": 5,
        "content": "that resolved the puzzle :)"
      },
      {
        "date": "2021-10-24T20:22:00.000Z",
        "voteCount": 2,
        "content": "congrats for passing the exam. practising all 255 questions is sufficient for passing the exam? how much percentage of questions you got from here roughly?"
      },
      {
        "date": "2022-10-16T23:05:00.000Z",
        "voteCount": 9,
        "content": "He won't answer, he already passed the exam"
      },
      {
        "date": "2021-08-27T12:51:00.000Z",
        "voteCount": 2,
        "content": "Agree, its the only answer that meets the requirement of \"before deploying to production\""
      },
      {
        "date": "2021-02-13T03:07:00.000Z",
        "voteCount": 2,
        "content": "not only 1 Q passed! C is beeter"
      },
      {
        "date": "2024-02-21T00:56:00.000Z",
        "voteCount": 1,
        "content": "ABC is about to deploy to production without prior testing. Hence D is the only reasonable choice."
      },
      {
        "date": "2024-01-23T06:34:00.000Z",
        "voteCount": 1,
        "content": "A B and C are very risky options and irrelevant as involving production. Whereas question is to test before rolling out to production.\nHence D"
      },
      {
        "date": "2023-09-13T06:58:00.000Z",
        "voteCount": 1,
        "content": "No discussion here."
      },
      {
        "date": "2022-11-06T08:18:00.000Z",
        "voteCount": 1,
        "content": "ok for D"
      },
      {
        "date": "2022-09-28T11:32:00.000Z",
        "voteCount": 1,
        "content": "Given the question D is the only answer.  Everything else pushes to production immediately."
      },
      {
        "date": "2022-03-27T23:39:00.000Z",
        "voteCount": 1,
        "content": "I don't think it was a good idea when new edtion be created and directly deploy to the production ENV without any testing stage even using Canary deployment."
      },
      {
        "date": "2022-03-27T23:40:00.000Z",
        "voteCount": 1,
        "content": "D is better."
      },
      {
        "date": "2022-03-21T06:36:00.000Z",
        "voteCount": 2,
        "content": "The question states: \"... code changes can be verified BEFORE deploying to production\", it eliminates option C.\nThe approach of tagging is the correct practise that DevOps use"
      },
      {
        "date": "2022-01-21T14:44:00.000Z",
        "voteCount": 4,
        "content": "Correct answer is D. Question talks about 'before deploying to production'. C talks about after deploying to production."
      },
      {
        "date": "2022-01-13T00:47:00.000Z",
        "voteCount": 1,
        "content": "C is the closest answer\nIf question is asking 'what's Jenkin best practise' then D is the answer"
      },
      {
        "date": "2022-03-21T06:37:00.000Z",
        "voteCount": 2,
        "content": "C involves deploying into production. the question specifies: \"BEFORE deploying to production\""
      },
      {
        "date": "2023-09-13T06:57:00.000Z",
        "voteCount": 1,
        "content": "Incorrect. D is the correct answer. 1. It should test BEFORE deploy to production. 2. It's a DevOps practice"
      },
      {
        "date": "2022-01-07T07:36:00.000Z",
        "voteCount": 1,
        "content": "I choose D as we want to ensure that code changes can be verified BEFORE deploying to production. Option C suggests that we build and deploy changes to production for 10% of users."
      },
      {
        "date": "2022-01-01T03:58:00.000Z",
        "voteCount": 3,
        "content": "Vote D.\nC is canary deploy.\nBut the sentence in question has no word to mean \"tested by a small number of users\""
      },
      {
        "date": "2022-01-01T04:03:00.000Z",
        "voteCount": 1,
        "content": "The revelal solution on this site is wrong, isn't it? I'm getting anxious."
      },
      {
        "date": "2021-12-25T05:07:00.000Z",
        "voteCount": 1,
        "content": "D is the correct answer"
      },
      {
        "date": "2021-12-17T04:09:00.000Z",
        "voteCount": 2,
        "content": "clearly"
      },
      {
        "date": "2021-12-15T20:56:00.000Z",
        "voteCount": 3,
        "content": "Question asks to verify before Production. So tagging and deploying appropriately is the right approach."
      },
      {
        "date": "2021-12-05T04:25:00.000Z",
        "voteCount": 1,
        "content": "Go for D.\n\u201cBEFORE DEPLOYING TO PRODUCTION\u201d"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 46,
    "url": "https://www.examtopics.com/discussions/google/view/6953-exam-professional-cloud-architect-topic-1-question-46/",
    "body": "You have an outage in your Compute Engine managed instance group: all instances keep restarting after 5 seconds. You have a health check configured, but autoscaling is disabled. Your colleague, who is a Linux expert, offered to look into the issue. You need to make sure that he can access the VMs. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGrant your colleague the IAM role of project Viewer",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPerform a rolling restart on the instance group",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDisable the health check for the instance group. Add his SSH key to the project-wide SSH Keys\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDisable autoscaling for the instance group. Add his SSH key to the project-wide SSH Keys"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 21,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-01-20T06:50:00.000Z",
        "voteCount": 88,
        "content": "C, is the correct answer. As per the requirement linux expert would need access to VM to troubleshoot the issue. With health check enabled, old VM will be terminated as soon as health-check fails for the VM and new VM will be auto-created. So, this situation will prevent linux expert to troubleshoot the issue. Had it been the case that stack-drover logging is enabled and the expert just want to view the logs from the Cloud-logs than role to project-viewer could help. But it is specifically mentioned that expert will login into VM to troubleshoot the issue and not looking at the cloud Logs. So, Option-C is the correct answer."
      },
      {
        "date": "2022-02-25T14:19:00.000Z",
        "voteCount": 2,
        "content": "great answer"
      },
      {
        "date": "2022-05-16T00:33:00.000Z",
        "voteCount": 2,
        "content": "This looks best justification between A and C.. So, C should be correct answer."
      },
      {
        "date": "2022-08-20T19:07:00.000Z",
        "voteCount": 2,
        "content": "Good explanation"
      },
      {
        "date": "2024-01-23T06:47:00.000Z",
        "voteCount": 1,
        "content": "The key element in C is \"Disable the Health check.\", so that server wont restart automatically.\nBut before that the actual troubleshooting step is to check Cloud console -&gt; Instance template -&gt; Metadata-&gt; and see if any startup script is there, if yes review it and possibly remove it.  [Consider the case, a script is causing restarting the VM, (possibly in Metadata).   ]"
      },
      {
        "date": "2019-10-23T04:57:00.000Z",
        "voteCount": 39,
        "content": "C should be correct answer."
      },
      {
        "date": "2024-03-17T03:36:00.000Z",
        "voteCount": 1,
        "content": "C is correct!"
      },
      {
        "date": "2023-08-22T13:12:00.000Z",
        "voteCount": 1,
        "content": "I like C"
      },
      {
        "date": "2023-03-15T07:20:00.000Z",
        "voteCount": 3,
        "content": "To allow your colleague, who is a Linux expert, to access the VMs and troubleshoot the issue, you should disable the health check for the instance group. This will prevent the instance group from automatically removing and replacing unhealthy instances.\n\nYou should also add your colleague's SSH key to the project-wide SSH Keys to allow him to SSH into the instances and perform troubleshooting. This can be done through the Google Cloud Console or the gcloud command-line tool."
      },
      {
        "date": "2023-03-06T08:41:00.000Z",
        "voteCount": 4,
        "content": "C. Disable the health check for the instance group. Add his SSH key to the project-wide SSH Keys.\n\nGranting the IAM role of project Viewer (Option A) would allow your colleague to view the project resources but not necessarily give them access to the specific VM instances. Performing a rolling restart on the instance group (Option B) would not resolve the issue, as the instances keep restarting after 5 seconds. Disabling autoscaling for the instance group (Option D) is not relevant since autoscaling is already disabled.\n\nDisabling the health check for the instance group will prevent the managed instance group from automatically recreating the instances. Adding your colleague's SSH key to the project-wide SSH Keys will allow them to access the VMs and troubleshoot the issue. Once the issue is resolved, you can re-enable the health check for the instance group."
      },
      {
        "date": "2023-02-06T19:21:00.000Z",
        "voteCount": 1,
        "content": "C is the answer"
      },
      {
        "date": "2023-01-03T03:31:00.000Z",
        "voteCount": 1,
        "content": "C as the machines will keep restarting if hc is not disabled , and then our expert can look around for RCA"
      },
      {
        "date": "2022-12-21T02:34:00.000Z",
        "voteCount": 2,
        "content": "To allow your colleague access the instances in MIG you should do option D: Disable autoscaling for the instance group. Add his SSH key to the project-wide SSH Keys.\n\nDisabling autoscaling for the instance group will prevent new instances from being created or terminated while your colleague is trying to troubleshoot the issue. This will give him a stable environment to work in and will ensure that he can access the instances that are currently in the instance group.\n\nAdding his SSH key to the project-wide SSH Keys will allow him to connect to the instances using Secure Shell (SSH) without requiring a password. This is a convenient way to give him access to the instances and can help him troubleshoot the issue more efficiently."
      },
      {
        "date": "2023-01-12T00:30:00.000Z",
        "voteCount": 1,
        "content": "autoscaling is already disabled. Answer D is not make sense here. i voted it wrongly instead of reply."
      },
      {
        "date": "2022-12-21T02:34:00.000Z",
        "voteCount": 2,
        "content": "Option A: Granting your colleague the IAM role of project Viewer will not allow him to access the instances in the managed instance group. The project Viewer role does not include any permissions to access Compute Engine resources.\n\nOption B: Performing a rolling restart on the instance group will not solve the issue and may even make it worse if the instances are not able to start up properly.\n\nOption C: Disabling the health check for the instance group will not solve the issue and may even make it worse if the instances are not able to start up properly. Adding his SSH key to the project-wide SSH Keys will allow him to access the instances, but it is not a sufficient solution on its own."
      },
      {
        "date": "2022-12-19T18:53:00.000Z",
        "voteCount": 1,
        "content": "C is the correct answer"
      },
      {
        "date": "2022-11-06T08:21:00.000Z",
        "voteCount": 1,
        "content": "ok for C"
      },
      {
        "date": "2022-10-13T07:51:00.000Z",
        "voteCount": 2,
        "content": "C. Disable the health check for the instance group. Add his SSH key to the project-wide SSH Keys. This will allow the engineer to logon to the VM's and check the logs. Disabling health checks will prevent rebooting of the VM's."
      },
      {
        "date": "2022-09-22T04:09:00.000Z",
        "voteCount": 1,
        "content": "C should be the correct answer"
      },
      {
        "date": "2022-08-30T12:21:00.000Z",
        "voteCount": 1,
        "content": "Disable the health check for the instance group. Add his SSH key to the project-wide SSH Keys"
      },
      {
        "date": "2022-08-08T04:31:00.000Z",
        "voteCount": 1,
        "content": "vote C\nwhat can project viewer do without access vm by linux expert?"
      },
      {
        "date": "2022-07-10T08:00:00.000Z",
        "voteCount": 1,
        "content": "I believe that answer C is correct."
      },
      {
        "date": "2022-05-31T20:45:00.000Z",
        "voteCount": 1,
        "content": "About create SSH Keys to VM https://cloud.google.com/compute/docs/connect/create-ssh-keys"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 47,
    "url": "https://www.examtopics.com/discussions/google/view/54735-exam-professional-cloud-architect-topic-1-question-47/",
    "body": "Your company is migrating its on-premises data center into the cloud. As part of the migration, you want to integrate Google Kubernetes Engine (GKE) for workload orchestration. Parts of your architecture must also be PCI DSS-compliant. Which of the following is most accurate?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tApp Engine is the only compute platform on GCP that is certified for PCI DSS hosting.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGKE cannot be used under PCI DSS because it is considered shared hosting.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGKE and GCP provide the tools you need to build a PCI DSS-compliant environment.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAll Google Cloud services are usable because Google Cloud Platform is certified PCI-compliant."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-06-06T09:36:00.000Z",
        "voteCount": 45,
        "content": "Link : https://cloud.google.com/security/compliance/pci-dss \nClearly mention GKE as PCI DSS-Compliant but not all GCP service are PCI DSS-Compliant so answer is definitely C."
      },
      {
        "date": "2022-08-05T02:37:00.000Z",
        "voteCount": 7,
        "content": "In 2022, GCP is now fully PCI-DSS compliant, so technically D is perfectly true.\nBut you still have to check that your application is PCI-DSS compliant.\n\nso C is still the best answer."
      },
      {
        "date": "2021-10-25T09:12:00.000Z",
        "voteCount": 1,
        "content": "C \u2013 Kubernetes Engine provides tools you need to build to PCI-DSS compliant environment."
      },
      {
        "date": "2021-12-05T04:35:00.000Z",
        "voteCount": 2,
        "content": "But, The paragraph 3 says that all products of google are certified by PCI."
      },
      {
        "date": "2021-06-27T00:51:00.000Z",
        "voteCount": 5,
        "content": "C: GKE &amp; Compute Engine is PCI DSS compliant while Cloud Function, App Engine are not PC compliant"
      },
      {
        "date": "2023-07-27T01:33:00.000Z",
        "voteCount": 1,
        "content": "We still have to configure our env to comply with PCI/DSS. https://cloud.google.com/architecture/pci-dss-compliance-in-gcp#kubernetes_engine"
      },
      {
        "date": "2022-12-21T02:37:00.000Z",
        "voteCount": 3,
        "content": "The most accurate statement is option C: GKE and GCP provide the tools you need to build a PCI DSS-compliant environment.\n\nGoogle Kubernetes Engine (GKE) is a fully managed service that allows you to deploy and manage containerized applications on Google Cloud. It is not specifically certified for PCI DSS hosting, but it can be used as part of a PCI DSS-compliant environment if the necessary controls and safeguards are in place.\n\nGoogle Cloud Platform (GCP) provides a range of tools and services that can be used to build a PCI DSS-compliant environment, including Cloud Identity and Access Management (IAM) for controlling access to resources, Cloud Key Management Service (KMS) for managing encryption keys, and Cloud Security Command Center for monitoring and detecting security threats."
      },
      {
        "date": "2022-12-21T02:38:00.000Z",
        "voteCount": 2,
        "content": "Option A: App Engine is a fully managed platform for building and deploying web and mobile applications, but it is not the only compute platform on GCP that is certified for PCI DSS hosting. Other compute platforms such as Compute Engine and Google Kubernetes Engine can also be used as part of a PCI DSS-compliant environment.\n\nOption B: GKE is not considered shared hosting and can be used as part of a PCI DSS-compliant environment if the necessary controls and safeguards are in place.\n\nOption D: While Google Cloud Platform is certified PCI-compliant, not all of its services are automatically usable in a PCI DSS-compliant environment. It is up to the user to ensure that they are using the appropriate controls and safeguards to meet the requirements of the PCI DSS."
      },
      {
        "date": "2022-08-30T12:43:00.000Z",
        "voteCount": 1,
        "content": "C is the right answer"
      },
      {
        "date": "2022-02-11T13:26:00.000Z",
        "voteCount": 3,
        "content": "I got similar question on my exam."
      },
      {
        "date": "2021-12-25T05:14:00.000Z",
        "voteCount": 1,
        "content": "C is the correct answer"
      },
      {
        "date": "2021-12-05T04:36:00.000Z",
        "voteCount": 1,
        "content": "Go for C."
      },
      {
        "date": "2021-11-24T21:51:00.000Z",
        "voteCount": 1,
        "content": "C- All of them: GKE, GCE, and GAE ate PCI-DSS-Compliant but A &amp; B says it's only GAE and GCE respectively so cancel them out.\nD says all of GCP is PCI DSS-Compliant but it's not true.\nSo, C seems to be the right answer."
      },
      {
        "date": "2021-10-06T12:31:00.000Z",
        "voteCount": 1,
        "content": "C is the right answer"
      },
      {
        "date": "2021-07-08T05:46:00.000Z",
        "voteCount": 3,
        "content": "Answer is C"
      },
      {
        "date": "2021-07-06T02:25:00.000Z",
        "voteCount": 1,
        "content": "C. GKE and GCP provide the tools you need to build a PCI DSS-compliant environment."
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 48,
    "url": "https://www.examtopics.com/discussions/google/view/7267-exam-professional-cloud-architect-topic-1-question-48/",
    "body": "Your company has multiple on-premises systems that serve as sources for reporting. The data has not been maintained well and has become degraded over time.<br>You want to use Google-recommended practices to detect anomalies in your company data. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpload your files into Cloud Storage. Use Cloud Datalab to explore and clean your data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpload your files into Cloud Storage. Use Cloud Dataprep to explore and clean your data.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConnect Cloud Datalab to your on-premises systems. Use Cloud Datalab to explore and clean your data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConnect Cloud Dataprep to your on-premises systems. Use Cloud Dataprep to explore and clean your data."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-04-13T05:02:00.000Z",
        "voteCount": 57,
        "content": "Answer is B:\n\nKeynotes from question:\n1- On-premise data sources\n2- Unfit data; not well maintained and degraded\n3- Google-recommended best practice to \"detect anomalies\" &lt;&lt;-Very important.\n\nExplanation:\nA &amp; C - incorrect; Datalab does not provide anomaly detection OOTB. It is used more for data science scenarios like interactive data analysis and build ML models. \nB - CORRECT; DataPrep OOTB provides for fast exploration and anomaly detection and lists cloud storage as an ingestion medium. Refer to ELT pipeline architecture here = https://cloud.google.com/dataprep\nD - incorrect; At this time DataPrep cannot connect to SaaS or on-premise source. Not to be confused for DataFlow which can!"
      },
      {
        "date": "2019-10-26T06:31:00.000Z",
        "voteCount": 12,
        "content": "Both B and D work, because the question says \"Google's Best Practices\" uploading the files first would keep the original copies Google encrypted and stored."
      },
      {
        "date": "2020-05-14T20:19:00.000Z",
        "voteCount": 1,
        "content": "Both of them works...."
      },
      {
        "date": "2020-08-01T05:25:00.000Z",
        "voteCount": 9,
        "content": "You can't connect DataPrep to your on-prem systems. You simply upload a file, but that is not connecting it to your systems. Because of that, I'd discard D and stay with B."
      },
      {
        "date": "2020-08-06T01:46:00.000Z",
        "voteCount": 9,
        "content": "B is ok"
      },
      {
        "date": "2021-03-04T15:55:00.000Z",
        "voteCount": 7,
        "content": "B, dataprep = visually explore, clean, and prepare data for analysis"
      },
      {
        "date": "2022-10-16T15:11:00.000Z",
        "voteCount": 1,
        "content": "B is better choice"
      },
      {
        "date": "2024-08-13T03:33:00.000Z",
        "voteCount": 1,
        "content": "Datalab was shutdown. Its replacement is vertex AI. Read question accordingly"
      },
      {
        "date": "2023-11-11T06:19:00.000Z",
        "voteCount": 3,
        "content": "Cloud Datalab is a powerful interactive tool created to explore, analyze, transform, and visualize data and build machine learning models on Google Cloud Platform.\nDataprep by Trifacta is an intelligent data service for visually exploring, cleaning, and preparing structured and unstructured data for analysis, reporting, and machine learning.\nDataprep do not have an integration for on-prem: https://console.cloud.google.com/marketplace/product/endpoints/cloud-dataprep-editions-v2?project=fast-art-401415\n\nSo, clearly, the only option left is B."
      },
      {
        "date": "2023-08-22T13:16:00.000Z",
        "voteCount": 1,
        "content": "B is correct."
      },
      {
        "date": "2023-02-27T11:46:00.000Z",
        "voteCount": 1,
        "content": "Today, data ingestion to DataPrep can be Application, file upload, database.\nso B is also now valid"
      },
      {
        "date": "2022-12-21T02:41:00.000Z",
        "voteCount": 2,
        "content": "The recommended approach for detecting anomalies in your company data using Google-recommended practices is option B: Upload your files into Cloud Storage. Use Cloud Dataprep to explore and clean your data.\n\nCloud Storage is a highly scalable, durable, and secure object storage service that can be used to store and retrieve data from anywhere on the web. You can use Cloud Storage to store your company data files and make them available for analysis.\n\nCloud Dataprep is a fully managed data preparation service that allows you to quickly and easily explore, clean, and transform your data for analysis. It can help you detect anomalies in your data by providing features such as data profiling, data cleansing, and data transformation."
      },
      {
        "date": "2022-12-21T02:41:00.000Z",
        "voteCount": 1,
        "content": "Option A: Using Cloud Datalab to explore and clean your data is not a recommended approach, as Cloud Datalab is a collaborative data exploration and visualization platform that is not specifically designed for data preparation tasks such as cleansing and transformation.\n\nOption C: Connecting Cloud Datalab to your on-premises systems is not a recommended approach, as Cloud Datalab is a collaborative data exploration and visualization platform and is not designed for data preparation tasks such as cleansing and transformation.\n\nOption D: Connecting Cloud Dataprep to your on-premises systems is not necessary, as you can use Cloud Dataprep to explore and clean data stored in Cloud Storage."
      },
      {
        "date": "2022-12-10T22:28:00.000Z",
        "voteCount": 1,
        "content": "ok for B &amp; D, but B is suitable to gcp"
      },
      {
        "date": "2022-12-01T23:59:00.000Z",
        "voteCount": 1,
        "content": "Datalab is deprecated : https://cloud.google.com/datalab/docs\nNew Cloud Dataprep options will give connectivity to relational databases, business applications and extend our integrations across Google Cloud with Google Sheets: https://www.trifacta.com/blog/cloud-dataprep-trifacta/"
      },
      {
        "date": "2022-11-06T08:24:00.000Z",
        "voteCount": 1,
        "content": "ok for B"
      },
      {
        "date": "2022-07-24T10:39:00.000Z",
        "voteCount": 3,
        "content": "Could anyone provide a link where it explicitly says that Datprep does not connect to on-premises data sources. \n\nIn the ingestion layer on the diagram at https://cloud.google.com/dataprep it shows databases as a source. \nI can't see anywhere that there is a limitation connecting to on-premises. Would be great if someone could share that."
      },
      {
        "date": "2022-06-29T12:50:00.000Z",
        "voteCount": 1,
        "content": "It's gotta be B."
      },
      {
        "date": "2022-06-18T01:53:00.000Z",
        "voteCount": 1,
        "content": "Keyword : Anamolies Data prep is the only product ... So options A and C is eliminated ... Cost effective is storing the data in GCS Cloud storage ... So option is B"
      },
      {
        "date": "2022-04-22T22:30:00.000Z",
        "voteCount": 1,
        "content": "Dataprep to detect anomalies in Data is the right choice."
      },
      {
        "date": "2022-01-10T19:59:00.000Z",
        "voteCount": 1,
        "content": "B...It supports only CloudStorage and Bigquery...\"So you can start transforming datasets, you hereby instruct Google to allow Trifacta, who provides the service Dataprep in collaboration with Google, to view and modify project data in Cloud Storage and BigQuery, run Dataflow jobs, and use all project service accounts.\""
      },
      {
        "date": "2021-12-05T04:39:00.000Z",
        "voteCount": 1,
        "content": "Go for B."
      },
      {
        "date": "2021-12-05T04:43:00.000Z",
        "voteCount": 1,
        "content": "The question says \u201cbest practice\u201d. In GCP , a best practice for many use cases is load to cloud storage and then processing data."
      },
      {
        "date": "2021-11-28T16:52:00.000Z",
        "voteCount": 1,
        "content": "vote B"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 49,
    "url": "https://www.examtopics.com/discussions/google/view/6846-exam-professional-cloud-architect-topic-1-question-49/",
    "body": "Google Cloud Platform resources are managed hierarchically using organization, folders, and projects. When Cloud Identity and Access Management (IAM) policies exist at these different levels, what is the effective policy at a particular node of the hierarchy?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe effective policy is determined only by the policy set at the node",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe effective policy is the policy set at the node and restricted by the policies of its ancestors",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe effective policy is the union of the policy set at the node and policies inherited from its ancestors\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe effective policy is the intersection of the policy set at the node and policies inherited from its ancestors"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2019-12-17T15:45:00.000Z",
        "voteCount": 31,
        "content": "The effective policy for a resource is the union of the policy set at that resource and the policy inherited from its parent.https://cloud.google.com/iam/docs/resource-hierarchy-access-control"
      },
      {
        "date": "2022-01-31T02:53:00.000Z",
        "voteCount": 13,
        "content": "You can set IAM policies at the level of the node, in addition to policies inherited from its parent. Hence, it is a union."
      },
      {
        "date": "2023-08-07T09:31:00.000Z",
        "voteCount": 3,
        "content": "From google doc: Google Cloud resources are organized hierarchically, where the organization node is the root node in the hierarchy, the projects are the children of the organization, and the other resources are descendants of projects. You can set allow policies at different levels of the resource hierarchy. Resources inherit the allow policies of the parent resource. The effective allow policy for a resource is the union of the allow policy set at that resource and the allow policy inherited from its parent."
      },
      {
        "date": "2022-12-21T02:47:00.000Z",
        "voteCount": 1,
        "content": "The effective policy at a particular node in the resource hierarchy in GCP is determined by the intersection of the policy set at the node and policies inherited from its ancestors, as described in option D\n\nCloud IAM policies in GCP are hierarchical, meaning that policies set at higher levels of the resource hierarchy can be inherited by lower levels. When a user or service account attempts to access a resource, the effective policy at that resource is determined by evaluating the policies set at the resource itself and all of its ancestors in the hierarchy. If any of the policies deny access, the user or service account will be denied access.\n\nFor example, consider the following resource hierarchy:\nOrganization =&gt; Folder =&gt; Project =&gt; Compute Engine instance\nIf an IAM policy is set at the organization level that allows read access to all Compute Engine instances, and a policy is set at the project level that denies read access to a specific Compute Engine instance, the effective policy for that instance will be the intersection of the two policies, which will be to deny read access to the instance."
      },
      {
        "date": "2022-12-21T02:47:00.000Z",
        "voteCount": 1,
        "content": "Option A: The effective policy is not determined only by the policy set at the node, as policies set at higher levels in the hierarchy can also have an impact on the effective policy.\n\nOption B: The effective policy is not restricted by the policies of its ancestors, as the policies of its ancestors can also be included in the effective policy if they allow access.\n\nOption C: The effective policy is not the union of the policy set at the node and policies inherited from its ancestors, as the intersection of the policies is used to determine the effective policy."
      },
      {
        "date": "2022-11-27T04:55:00.000Z",
        "voteCount": 2,
        "content": "C. Is a skewed wording question. Cannot be comprehended right away."
      },
      {
        "date": "2022-11-06T08:26:00.000Z",
        "voteCount": 1,
        "content": "ok for C"
      },
      {
        "date": "2022-10-22T14:15:00.000Z",
        "voteCount": 1,
        "content": "C is correct answer"
      },
      {
        "date": "2022-10-16T23:21:00.000Z",
        "voteCount": 5,
        "content": "English as a second language will struggle here. Good luck to us"
      },
      {
        "date": "2022-09-30T10:15:00.000Z",
        "voteCount": 3,
        "content": "A: Would mean polcies set at the project or higher meant nothing, this is obviously wrong\nB: would mean you could not grant a permissions to a single VM, it would need to be at project or above (you restrict by not giving the permission)\nC : The permission is the sum of all the permissions you are given through the hierarchy, this is correct, you cannot restrict once it is given at a higher level.\nD: Would mean you would need the permission set at ancestor and the node, this would mean to get access to a single VM you would need to be given access to all VMs at the project level."
      },
      {
        "date": "2022-09-22T04:22:00.000Z",
        "voteCount": 2,
        "content": "C is correct answer as it inheritance is the basic model of IAM"
      },
      {
        "date": "2022-05-14T01:32:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2021-12-28T01:25:00.000Z",
        "voteCount": 3,
        "content": "C\nGoogle Cloud resources are organized hierarchically, where the organization node is the root node in the hierarchy, the projects are the children of the organization, and the other resources are descendants of projects. You can set Identity and Access Management (IAM) policies at different levels of the resource hierarchy. Resources inherit the policies of the parent resource. The effective policy for a resource is the union of the policy set at that resource and the policy inherited from its parent."
      },
      {
        "date": "2021-12-25T05:20:00.000Z",
        "voteCount": 2,
        "content": "C is the correct answer"
      },
      {
        "date": "2021-12-05T04:44:00.000Z",
        "voteCount": 1,
        "content": "Go for C."
      },
      {
        "date": "2021-07-08T05:49:00.000Z",
        "voteCount": 3,
        "content": "Answer is C"
      },
      {
        "date": "2021-05-19T01:00:00.000Z",
        "voteCount": 2,
        "content": "C. The effective policy is the union of the policy set at the node and policies inherited from its ancestors"
      },
      {
        "date": "2021-05-17T07:14:00.000Z",
        "voteCount": 1,
        "content": "Def answer C. Key word is 'union'."
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 50,
    "url": "https://www.examtopics.com/discussions/google/view/7042-exam-professional-cloud-architect-topic-1-question-50/",
    "body": "You are migrating your on-premises solution to Google Cloud in several phases. You will use Cloud VPN to maintain a connection between your on-premises systems and Google Cloud until the migration is completed. You want to make sure all your on-premise systems remain reachable during this period. How should you organize your networking in Google Cloud?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the same IP range on Google Cloud as you use on-premises",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the same IP range on Google Cloud as you use on-premises for your primary IP range and use a secondary range that does not overlap with the range you use on-premises",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an IP range on Google Cloud that does not overlap with the range you use on-premises\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an IP range on Google Cloud that does not overlap with the range you use on-premises for your primary IP range and use a secondary range with the same IP range as you use on-premises"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 17,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-01-26T02:42:00.000Z",
        "voteCount": 130,
        "content": "Ans is C, \nhttps://cloud.google.com/vpc/docs/using-vpc\n\n\"Primary and secondary ranges can't conflict with on-premises IP ranges if you have connected your VPC network to another network with Cloud VPN, Dedicated Interconnect, or Partner Interconnect.\""
      },
      {
        "date": "2020-02-14T17:42:00.000Z",
        "voteCount": 2,
        "content": "Agreed!"
      },
      {
        "date": "2020-05-28T01:58:00.000Z",
        "voteCount": 2,
        "content": "Perfect.. Exact find in link."
      },
      {
        "date": "2022-05-31T22:51:00.000Z",
        "voteCount": 2,
        "content": "agree, any ip range, shall use filewall rule to communicate, instead of setting same IP range, which is a mess to control."
      },
      {
        "date": "2020-06-10T14:25:00.000Z",
        "voteCount": 13,
        "content": "from the above link - it clearly states - \"Primary and secondary ranges for subnets cannot overlap with any allocated range, any primary or secondary range of another subnet in the same network, or any IP ranges of subnets in peered networks.\" once we create a VPN, they all are part of the same network . Hence option C is correct"
      },
      {
        "date": "2019-10-23T00:05:00.000Z",
        "voteCount": 21,
        "content": "I think C is correct."
      },
      {
        "date": "2019-11-14T08:09:00.000Z",
        "voteCount": 10,
        "content": "Agree with C. Secondary IP range still can't overlap"
      },
      {
        "date": "2020-01-12T05:31:00.000Z",
        "voteCount": 3,
        "content": "\".... and Google Cloud until the migration is completed.\" Taking this as the key, the intention is to remove the connection b/w on-prem and GCP once the migration is done. and then the secondary IPs will act as primary. So I will choose D"
      },
      {
        "date": "2020-08-06T02:15:00.000Z",
        "voteCount": 10,
        "content": "C is ok"
      },
      {
        "date": "2021-10-25T09:44:00.000Z",
        "voteCount": 2,
        "content": "B, The key points here:\n- migrating in several phases\n- use Cloud VPN until the migration is completed\n- all your on-premise systems remain reachable during this period"
      },
      {
        "date": "2021-09-14T07:29:00.000Z",
        "voteCount": 1,
        "content": "how to manage the routing table in VPC if is present a subnet with the same network of vpn remote net? the correct answer is C"
      },
      {
        "date": "2020-11-01T05:40:00.000Z",
        "voteCount": 2,
        "content": "Yes C it is"
      },
      {
        "date": "2021-03-04T15:59:00.000Z",
        "voteCount": 4,
        "content": "C, no brainer. You have on-prem &lt;--&gt; VPN &lt;---&gt; GCP only way this data flow to work in non-over lapping subnets. You can stretch subnets at layer 7 but you wont be able to route it via VPN."
      },
      {
        "date": "2024-07-10T14:42:00.000Z",
        "voteCount": 1,
        "content": "question is tricky. as network architect knowing gcp i have exp that you can use non-overlapping secondary ranges for vpn as well. \n\nin many migrations it is not possible to make new addressing hence you need to make them overlapping. this is why 2nd ranges are so useful. \n\nB is better choice. more realistic and possible in gcp. \n\nfrom overall perspective i  agree to have non-overlapping but do not forget this is migration and you need to have full connectivity all the time. it is also not mentioning about what ips should be used"
      },
      {
        "date": "2024-08-01T08:07:00.000Z",
        "voteCount": 1,
        "content": "When migrating to the cloud, best practices for IP schema generally involve avoiding duplicate IP addresses and keeping cloud and on-premise IP ranges separate"
      },
      {
        "date": "2023-08-22T13:20:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2023-03-15T18:02:00.000Z",
        "voteCount": 3,
        "content": "Using an IP range on Google Cloud that does not overlap with the range used on-premises (option C) is a good choice to avoid IP address conflicts. However, it is important to use the same IP range as the on-premises applications for the primary IP range to ensure that the on-premises systems remain accessible. Therefore, using the same IP range on Google Cloud as on-premises for the primary IP range and using a secondary range that does not overlap with the range used on-premises can avoid IP address duplication and ensure that the on-premises systems remain accessible. Hence, option B is the better choice."
      },
      {
        "date": "2022-12-21T02:50:00.000Z",
        "voteCount": 2,
        "content": "The recommended approach for organizing your networking in Google Cloud to ensure that all your on-premises systems remain reachable during the migration is option C: Use an IP range on Google Cloud that does not overlap with the range you use on-premises.\n\nWhen using Cloud VPN to establish a connection between your on-premises systems and Google Cloud, it is important to ensure that the IP ranges used in your on-premises systems and Google Cloud do not overlap. If the IP ranges overlap, it can cause conflicts and make it difficult to route traffic between your on-premises systems and Google Cloud.\n\nTo avoid IP range conflicts, you should use an IP range on Google Cloud that is different from the range you use on-premises. This will ensure that all your on-premises systems remain reachable during the migration."
      },
      {
        "date": "2022-12-21T02:50:00.000Z",
        "voteCount": 1,
        "content": "Option A: Using the same IP range on Google Cloud as you use on-premises is not a recommended approach, as it can cause IP range conflicts and make it difficult to route traffic between your on-premises systems and Google Cloud.\n\nOption B: Using the same IP range on Google Cloud as you use on-premises for your primary IP range and a secondary range that does not overlap with the range you use on-premises is not a recommended approach, as it can still cause IP range conflicts and make it difficult to route traffic between your on-premises systems and Google Cloud.\n\nOption D: Using an IP range on Google Cloud that does not overlap with the range you use on-premises for your primary"
      },
      {
        "date": "2022-11-06T08:53:00.000Z",
        "voteCount": 1,
        "content": "ok for C"
      },
      {
        "date": "2022-10-16T23:24:00.000Z",
        "voteCount": 1,
        "content": "no overlapping"
      },
      {
        "date": "2022-10-16T15:16:00.000Z",
        "voteCount": 1,
        "content": "C. Use an IP range on Google Cloud that does not overlap with the range you use on-premises"
      },
      {
        "date": "2022-08-31T05:10:00.000Z",
        "voteCount": 1,
        "content": "C, IP should never overlap if avoidable. double nat is nasty"
      },
      {
        "date": "2022-06-27T03:28:00.000Z",
        "voteCount": 2,
        "content": "The correct answer is C"
      },
      {
        "date": "2022-06-08T19:44:00.000Z",
        "voteCount": 1,
        "content": "C\n\nWhy would you ever create an IP overlap?"
      },
      {
        "date": "2022-04-16T17:05:00.000Z",
        "voteCount": 1,
        "content": "Answer is C"
      },
      {
        "date": "2022-03-21T06:47:00.000Z",
        "voteCount": 1,
        "content": "From here: https://cloud.google.com/vpc/docs/create-modify-vpc-networks\n\"Primary and secondary ranges can't conflict with on-premises IP ranges if you have connected your VPC network to another network with  Cloud VPN, Dedicated Interconnect, or Partner Interconnect.\""
      },
      {
        "date": "2022-02-11T13:27:00.000Z",
        "voteCount": 3,
        "content": "I got similar question on my exam."
      },
      {
        "date": "2022-01-21T06:47:00.000Z",
        "voteCount": 2,
        "content": "ANS - C\nPrimary and secondary ranges for subnets cannot overlap with any allocated range, any primary or secondary range of another subnet in the same network, or any IPv4 ranges of subnets in peered networks."
      },
      {
        "date": "2022-01-03T22:52:00.000Z",
        "voteCount": 2,
        "content": "Is D corecct?! \nReally?\n\nI agree with C is correct."
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 51,
    "url": "https://www.examtopics.com/discussions/google/view/7231-exam-professional-cloud-architect-topic-1-question-51/",
    "body": "You have found an error in your App Engine application caused by missing Cloud Datastore indexes. You have created a YAML file with the required indexes and want to deploy these new indexes to Cloud Datastore. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPoint gcloud datastore create-indexes to your configuration file\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpload the configuration file to App Engine's default Cloud Storage bucket, and have App Engine detect the new indexes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the GCP Console, use Datastore Admin to delete the current indexes and upload the new configuration file",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an HTTP request to the built-in python module to send the index configuration file to your application"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 18,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2019-10-26T01:02:00.000Z",
        "voteCount": 30,
        "content": "Correct A, you have to recreate the indexes"
      },
      {
        "date": "2021-03-04T16:01:00.000Z",
        "voteCount": 4,
        "content": "A, if index is missing then create it."
      },
      {
        "date": "2020-08-06T02:27:00.000Z",
        "voteCount": 11,
        "content": "A is ok"
      },
      {
        "date": "2020-11-01T05:42:00.000Z",
        "voteCount": 4,
        "content": "Yes, use this command in cloud sell to create indexes. \ngcloud datastore create indexes"
      },
      {
        "date": "2019-10-26T06:57:00.000Z",
        "voteCount": 17,
        "content": "A is incorrect because the command is actually gcloud datastore indexes create. (https://cloud.google.com/sdk/gcloud/reference/datastore/indexes/create)."
      },
      {
        "date": "2021-02-20T10:36:00.000Z",
        "voteCount": 15,
        "content": "It might have changed recently - I was able to find documentation mentioning \"datastore create-indexes\":\n\nhttps://cloud.google.com/appengine/docs/standard/python/datastore/indexes"
      },
      {
        "date": "2024-05-27T23:43:00.000Z",
        "voteCount": 1,
        "content": "To deploy new Cloud Datastore indexes using the YAML file you created, you should use the `gcloud` command-line tool. Specifically, the correct option is:\n\n**A. Point gcloud datastore create-indexes to your configuration file**\n\nHere\u2019s how you can do it:\n\n1. **Ensure you have the `gcloud` CLI installed**: You need the Google Cloud SDK installed and set up on your local machine. If you haven't done this yet, follow the [installation guide](https://cloud.google.com/sdk/docs/install).\n\n2. **Navigate to the directory containing your `index.yaml` file**: This file contains the definitions of the indexes you want to deploy.\n\n3. **Run the following command**:\n   ```bash\n   gcloud datastore indexes create index.yaml\n   ```\n   This command will deploy the indexes defined in the `index.yaml` file to Cloud Datastore."
      },
      {
        "date": "2023-11-09T04:54:00.000Z",
        "voteCount": 3,
        "content": "A is correct: https://cloud.google.com/sdk/gcloud/reference/datastore/indexes/create"
      },
      {
        "date": "2023-03-20T12:41:00.000Z",
        "voteCount": 2,
        "content": "A)\nhttps://cloud.google.com/datastore/docs/tools/indexconfig#Datastore_Updating_indexes"
      },
      {
        "date": "2023-03-07T14:22:00.000Z",
        "voteCount": 11,
        "content": "A. Point gcloud datastore create-indexes to your configuration file.\n\nTo deploy new indexes to Cloud Datastore, you can use the gcloud datastore create-indexes command and point it to the YAML configuration file containing the required indexes. This command will create the new indexes in Cloud Datastore for your application.\n\nOption B is not correct because App Engine does not automatically detect and create indexes from uploaded configuration files in Cloud Storage.\n\nOption C is also not correct because deleting current indexes in Datastore Admin is not necessary to upload new indexes.\n\nOption D is not correct because there is no built-in Python module that can send the index configuration file to your application"
      },
      {
        "date": "2022-12-21T03:43:00.000Z",
        "voteCount": 6,
        "content": "To deploy new indexes to Cloud Datastore, you should use the gcloud datastore create-indexes command and point it to your configuration file. The correct option is therefore A: Point gcloud datastore create-indexes to your configuration file.\n\nHere's the general format of the gcloud datastore create-indexes command:\n\nCopy code\ngcloud datastore create-indexes [FILE]\nWhere [FILE] is the path to your configuration file. The configuration file should be in YAML format and contain a list of indexes that you want to create.\n\nFor example:\n\nCopy code\ngcloud datastore create-indexes index.yaml\nThis command will create the indexes specified in the index.yaml file in Cloud Datastore."
      },
      {
        "date": "2022-11-06T09:03:00.000Z",
        "voteCount": 1,
        "content": "ok for A"
      },
      {
        "date": "2022-10-22T14:17:00.000Z",
        "voteCount": 1,
        "content": "A is correct answer"
      },
      {
        "date": "2022-10-16T08:44:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2022-10-13T23:19:00.000Z",
        "voteCount": 2,
        "content": "gcloud app deploy index.yaml"
      },
      {
        "date": "2022-10-10T11:06:00.000Z",
        "voteCount": 1,
        "content": "Answer is A.\nOption C: no create index from configuration file in the firestore console."
      },
      {
        "date": "2022-09-25T10:58:00.000Z",
        "voteCount": 1,
        "content": "Is A still the valid answer, many other website resources saying C is the right answer"
      },
      {
        "date": "2022-09-22T04:28:00.000Z",
        "voteCount": 1,
        "content": "A looks like more logical answer"
      },
      {
        "date": "2022-04-08T05:09:00.000Z",
        "voteCount": 7,
        "content": "A is incorrect.\nThere is no correct answer, in fact.  In 2022, the gcloud parameter \"create-index\" doesn't exist anymore. However, that was in 2017*. Today, the right CLI command** should be gcloud datastore \"indexes create\".\n\nEvidences: \n*  https://stackoverflow.com/questions/43041126/how-to-create-datastore-composite-indexes-with-just-cloud-functions)\n** https://cloud.google.com/sdk/gcloud/reference/datastore/indexes/create"
      },
      {
        "date": "2021-12-25T05:26:00.000Z",
        "voteCount": 1,
        "content": "A is the correct answer"
      },
      {
        "date": "2021-12-06T23:11:00.000Z",
        "voteCount": 3,
        "content": "Why not C? Shouldnt the command be datastore indexes create and not create-index as incorrectly stated in option A? Please advice"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 52,
    "url": "https://www.examtopics.com/discussions/google/view/7273-exam-professional-cloud-architect-topic-1-question-52/",
    "body": "You have an application that will run on Compute Engine. You need to design an architecture that takes into account a disaster recovery plan that requires your application to fail over to another region in case of a regional outage. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the application on two Compute Engine instances in the same project but in a different region. Use the first instance to serve traffic, and use the HTTP load balancing service to fail over to the standby instance in case of a disaster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the application on a Compute Engine instance. Use the instance to serve traffic, and use the HTTP load balancing service to fail over to an instance on your premises in case of a disaster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the application on two Compute Engine instance groups, each in the same project but in a different region. Use the first instance group to serve traffic, and use the HTTP load balancing service to fail over to the standby instance group in case of a disaster.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the application on two Compute Engine instance groups, each in a separate project and a different region. Use the first instance group to serve traffic,  and use the HTTP load balancing service to fail over to the standby instance group in case of a disaster."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 19,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2019-10-26T07:40:00.000Z",
        "voteCount": 35,
        "content": "Groups are better for management that non-groups so A and B are eliminated. Keeping the the instances in the same project will help maintain consistency, so C is better than D."
      },
      {
        "date": "2023-06-01T11:02:00.000Z",
        "voteCount": 1,
        "content": "make sense."
      },
      {
        "date": "2021-03-04T16:09:00.000Z",
        "voteCount": 13,
        "content": "C, because external LB needs **IG** period. It can either be managed or un-managed. You can not do External HTTP LB on instances. Also External HHTP LB is a Regional resource."
      },
      {
        "date": "2021-09-15T02:57:00.000Z",
        "voteCount": 12,
        "content": "Yes, but why not choose a cost-effective solution like A, preferring a not required performance optimization solution like C? The question it's just asking for a simple fail over"
      },
      {
        "date": "2022-03-28T14:04:00.000Z",
        "voteCount": 6,
        "content": "In real word situation, I'd choose A for a customer. The question doesn't mention performance to be enhanced or scalability."
      },
      {
        "date": "2023-07-27T14:15:00.000Z",
        "voteCount": 4,
        "content": "LB does not work without MIG"
      },
      {
        "date": "2023-12-26T20:47:00.000Z",
        "voteCount": 1,
        "content": "Use two groups, each group contain one instance would be more flixible than answer A."
      },
      {
        "date": "2023-10-14T07:44:00.000Z",
        "voteCount": 4,
        "content": "Now you can use regional load balancer to route traffic to instance groups in different GCP projects. Welcome to cloud :-)\nhttps://cloud.google.com/blog/products/networking/cloud-load-balancing-gets-cross-project-service-referencing"
      },
      {
        "date": "2023-08-22T13:24:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2023-03-15T09:35:00.000Z",
        "voteCount": 5,
        "content": "To set up a load balancer with a Compute Engine backend, your VMs need to be in an instance group. The managed instance group provides VMs running the backend servers of an external HTTP load balancer\n\nTherefore C"
      },
      {
        "date": "2023-06-01T11:04:00.000Z",
        "voteCount": 2,
        "content": "good catch ."
      },
      {
        "date": "2023-03-15T09:31:00.000Z",
        "voteCount": 1,
        "content": "Why can't this be A since there is no mention of scaling?"
      },
      {
        "date": "2023-07-27T14:15:00.000Z",
        "voteCount": 1,
        "content": "LB does not work without MIG"
      },
      {
        "date": "2023-12-06T06:50:00.000Z",
        "voteCount": 2,
        "content": "You can use either MIG or unmanaged..\nhttps://cloud.google.com/compute/docs/instance-groups/creating-groups-of-unmanaged-instances"
      },
      {
        "date": "2023-01-11T15:15:00.000Z",
        "voteCount": 8,
        "content": "Google recommend using MIG for Zonal outage and multiple MIG for regional outage\nhttps://cloud.google.com/architecture/disaster-recovery#compute-engine\nsentence says:\nCompute Engine instances are zonal resources, so in the event of a zone outage instances are unavailable by default. Compute Engine does offer managed instance groups (MIGs) which can automatically scale up additional VMs from pre-configured instance templates, both within a single zone and across multiple zones within a region. MIGs are ideal for applications that require resilience to zone loss and are stateless, but require configuration and resource planning. Multiple regional MIGs can be used to achieve region outage resilience for stateless applications."
      },
      {
        "date": "2023-01-03T11:18:00.000Z",
        "voteCount": 1,
        "content": "MIG , in 1 project"
      },
      {
        "date": "2022-12-26T05:00:00.000Z",
        "voteCount": 1,
        "content": "the only solution that they want is to address regional outage. scalability or performance is not a concern at this point. MIG is better but that not what is solving the regional outage."
      },
      {
        "date": "2023-01-11T15:12:00.000Z",
        "voteCount": 1,
        "content": "According to this link, MIG is not only for performance and scalability. It is also for reliability\nhttps://cloud.google.com/architecture/disaster-recovery#compute-engine\nsentence says: \nCompute Engine instances are zonal resources, so in the event of a zone outage instances are unavailable by default. Compute Engine does offer managed instance groups (MIGs) which can automatically scale up additional VMs from pre-configured instance templates, both within a single zone and across multiple zones within a region. MIGs are ideal for applications that require resilience to zone loss and are stateless, but require configuration and resource planning. Multiple regional MIGs can be used to achieve region outage resilience for stateless applications."
      },
      {
        "date": "2022-12-21T03:53:00.000Z",
        "voteCount": 2,
        "content": "The correct answer is D: Deploy the application on two Compute Engine instance groups, each in a separate project and a different region. Use the first instance group to serve traffic, and use the HTTP load balancing service to fail over to the standby instance group in case of a disaster.\n\nTo implement a disaster recovery plan that requires your application to fail over to another region in case of a regional outage, you should deploy the application on two Compute Engine instance groups, each in a separate project and a different region. This will ensure that the application is running in at least two regions, so that if one region experiences an outage, the application can still be accessed from the other region.\n\nYou can use the HTTP load balancing service to distribute traffic between the two instance groups and to fail over to the standby instance group in case of a disaster. This will ensure that your application is always available, even in the event of a regional outage."
      },
      {
        "date": "2022-12-21T03:53:00.000Z",
        "voteCount": 2,
        "content": "Option A: Deploying the application on two Compute Engine instances in the same project but in a different region will not provide enough redundancy, as the instances are still in the same project and could be affected by the same regional outage.\n\nOption B: Deploying the application on a Compute Engine instance and using the HTTP load balancing service to fail over to an instance on your premises in case of a disaster is not a valid option, as it does not provide the required disaster recovery capability.\n\nOption C: Deploying the application on two Compute Engine instance groups, each in the same project but in a different region, is not a valid option, as it does not provide the required disaster recovery capability."
      },
      {
        "date": "2022-12-13T07:47:00.000Z",
        "voteCount": 1,
        "content": "IMHO A would also work (test env). In production relying on instances from other regions (introducing latency) just for possible expected zonal outages, would not be my best practice."
      },
      {
        "date": "2022-11-20T05:40:00.000Z",
        "voteCount": 2,
        "content": "Correct answer is C. No need to host MIG in separate projects."
      },
      {
        "date": "2022-11-06T09:11:00.000Z",
        "voteCount": 1,
        "content": "ok for C"
      },
      {
        "date": "2022-10-16T08:44:00.000Z",
        "voteCount": 1,
        "content": "C is right choice"
      },
      {
        "date": "2022-10-13T23:46:00.000Z",
        "voteCount": 1,
        "content": "C - using instance groups when planning for DR is better than having single vm's (https://cloud.google.com/architecture/disaster-recovery). Having the resources in the same project is probably good for resource management."
      },
      {
        "date": "2022-10-03T02:52:00.000Z",
        "voteCount": 1,
        "content": "Answer should have been A as autoscaling requirements are not mentioned."
      },
      {
        "date": "2022-10-01T06:42:00.000Z",
        "voteCount": 2,
        "content": "I think you need an group for the load balancer so A no good?"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 53,
    "url": "https://www.examtopics.com/discussions/google/view/6314-exam-professional-cloud-architect-topic-1-question-53/",
    "body": "You are deploying an application on App Engine that needs to integrate with an on-premises database. For security purposes, your on-premises database must not be accessible through the public internet. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy your application on App Engine standard environment and use App Engine firewall rules to limit access to the open on-premises database.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy your application on App Engine standard environment and use Cloud VPN to limit access to the on-premises database.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy your application on App Engine flexible environment and use App Engine firewall rules to limit access to the on-premises database.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy your application on App Engine flexible environment and use Cloud VPN to limit access to the on-premises database.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 45,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 44,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 7,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2019-12-27T04:15:00.000Z",
        "voteCount": 52,
        "content": "Agree with D   - \"When to choose the flexible environment\"    \"Accesses the resources or services of your Google Cloud project that reside in the Compute Engine network.\"\nhttps://cloud.google.com/appengine/docs/the-appengine-environments"
      },
      {
        "date": "2020-01-12T05:53:00.000Z",
        "voteCount": 7,
        "content": "Why not B ? https://cloud.google.com/appengine/docs/flexible/python/using-third-party-databases"
      },
      {
        "date": "2021-06-11T17:17:00.000Z",
        "voteCount": 27,
        "content": "because app engine standard cant connect to on-prem db"
      },
      {
        "date": "2023-02-01T06:04:00.000Z",
        "voteCount": 6,
        "content": "Where does it say appengine cannot connect to on-prem db? With CloudVPN, it shoudl connect as per this https://cloud.google.com/appengine/docs/flexible/storage-options#on_premises\n\nAlso going with D will require app to be containerized. That is not listed in the requirement."
      },
      {
        "date": "2023-08-14T05:36:00.000Z",
        "voteCount": 4,
        "content": "This is the link for Standard Env\nhttps://cloud.google.com/appengine/docs/standard/storage-options\n\nBoth standard and Flexible can connect to a VPC with Serverless VPC connector. Once it connects to a VPC, connecting to onprem is same for any service."
      },
      {
        "date": "2023-10-08T22:12:00.000Z",
        "voteCount": 4,
        "content": "I just had the same confusion. Serverless VPC Connector is something relatively newer than this question on the exam, so probably it's safer to assume that a VPC connection is not supported (at least directly) by App Engine Standard. \n\nBesides, this would add extra overhead, and would also increase the costs for the solution. \n\nMost of these questions haven't been updated or repurposed according to newer products and services. For this particular question, using a Serverless VPC Connector would add unnecessary complexity and the solution would become more expensive. \n\nI swore to god it was B lol, but after a few hours of reading the documentation, I changed my mind and switched to option D. You might want to do the same."
      },
      {
        "date": "2022-06-01T04:15:00.000Z",
        "voteCount": 1,
        "content": "Isn't the question said \"not public internet access\"?"
      },
      {
        "date": "2024-05-18T01:48:00.000Z",
        "voteCount": 1,
        "content": "Yes, that phrase in the question bothers me too. However, when I check this:\n\nhttps://cloud.google.com/appengine/docs/flexible/storage-options#:~:text=On%20premises,-If%20you%20have&amp;text=Because%20App%20Engine%20and%20Compute,database%20server's%20internal%20IP%20address.\n\nit says \"If you have existing on-premises databases that you want to make accessible to your App Engine app, you can either configure your internal network and firewall to give the database a public IP address or connect using a VPN.\"\n\nSo I think the question should have skipped the words \"not public internet access\" if they want us to choose VPN."
      },
      {
        "date": "2021-12-05T06:08:00.000Z",
        "voteCount": 5,
        "content": "In a forum mentions that GCE and CAP flex are designed for connect to VPC . With GAP standard is needed a proxy .\nhttps://stackoverflow.com/questions/47537204/how-to-connect-app-engine-and-on-premise-server-through-vpn"
      },
      {
        "date": "2019-10-26T01:06:00.000Z",
        "voteCount": 18,
        "content": "Right is D:\nhttps://stackoverflow.com/questions/37137914/is-it-possible-to-use-google-app-engine-with-google-cloud-vpn"
      },
      {
        "date": "2022-05-11T09:13:00.000Z",
        "voteCount": 5,
        "content": "Question is can we restrict acess with VP N ?"
      },
      {
        "date": "2022-07-21T13:28:00.000Z",
        "voteCount": 1,
        "content": "The stackoverflow reference if older that the answer (6 years) I think that has changed."
      },
      {
        "date": "2024-09-23T07:15:00.000Z",
        "voteCount": 1,
        "content": "B and C are not appropriate and wherein in App Engine flex resources reside in VPC NEtwo\nhttps://cloud.google.com/appengine/docs/the-appengine-environments#app-engine-environments\n\nD"
      },
      {
        "date": "2024-09-03T06:34:00.000Z",
        "voteCount": 2,
        "content": "App Engine flexible environment provides more flexibility and supports VPC (Virtual Private Cloud) connectivity, which allows you to set up a Cloud VPN connection. The VPN can be used to securely connect your App Engine application to the on-premises database without exposing it to the public internet."
      },
      {
        "date": "2024-08-06T20:33:00.000Z",
        "voteCount": 1,
        "content": "flexible is more expensive. standard will suffice"
      },
      {
        "date": "2024-07-26T20:49:00.000Z",
        "voteCount": 1,
        "content": "D\net up a VPN connection between your on-premises network and Google Cloud.\nThis establishes a secure tunnel for communication.\nYour App Engine"
      },
      {
        "date": "2024-07-06T17:53:00.000Z",
        "voteCount": 1,
        "content": "right answer is B"
      },
      {
        "date": "2024-06-29T07:34:00.000Z",
        "voteCount": 1,
        "content": "\"your on-premises database must not be accessible through the public internet\" \n=&gt; definitely C"
      },
      {
        "date": "2024-06-29T07:36:00.000Z",
        "voteCount": 1,
        "content": "sorry was a typo, It's D"
      },
      {
        "date": "2024-07-25T14:07:00.000Z",
        "voteCount": 1,
        "content": "B and D both use public internet, ie, VPN. So B is the easier option as per \nhttps://cloud.google.com/appengine/docs/standard/connecting-vpc"
      },
      {
        "date": "2024-05-28T12:12:00.000Z",
        "voteCount": 4,
        "content": "The documentation mentions that App Engine Standard can connect to on-prem database using VPN. Link of the documentation: https://cloud.google.com/appengine/docs/standard/storage-options"
      },
      {
        "date": "2024-05-11T02:35:00.000Z",
        "voteCount": 3,
        "content": "now you can use option B"
      },
      {
        "date": "2024-05-05T07:06:00.000Z",
        "voteCount": 3,
        "content": "you can use a Serverless VPC Connector to connect App Engine Standard Environment to an on-premise resource via Cloud VPN"
      },
      {
        "date": "2024-04-07T17:20:00.000Z",
        "voteCount": 4,
        "content": "https://cloud.google.com/appengine/docs/flexible/flexible-for-standard-users\nStandard environment tend to be stateless web applications . Flexi is better for intergrating with on premise database"
      },
      {
        "date": "2024-01-22T13:28:00.000Z",
        "voteCount": 3,
        "content": "It is B"
      },
      {
        "date": "2024-01-20T13:45:00.000Z",
        "voteCount": 3,
        "content": "B. As per documentation\nhttps://cloud.google.com/appengine/docs/flexible/storage-options. Clearly App Engine standard can connect to on prem database."
      },
      {
        "date": "2024-04-29T08:12:00.000Z",
        "voteCount": 1,
        "content": "correct, the link is https://cloud.google.com/appengine/docs/standard/connecting-vpc"
      },
      {
        "date": "2024-03-21T05:42:00.000Z",
        "voteCount": 2,
        "content": "I believe that above link treats about Flexible env. - even URL contains \"flexible\" part."
      },
      {
        "date": "2024-03-21T05:47:00.000Z",
        "voteCount": 1,
        "content": "Ok, for Standard is exactly the same statement."
      },
      {
        "date": "2024-01-14T08:42:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2024-01-17T02:39:00.000Z",
        "voteCount": 2,
        "content": "Explain?"
      },
      {
        "date": "2023-11-21T12:06:00.000Z",
        "voteCount": 3,
        "content": "I vote B, flexible is just needed if you need no standard softwere etc...\nhttps://cloud.google.com/appengine/docs/flexible/storage-options#on_premises"
      },
      {
        "date": "2023-10-08T18:41:00.000Z",
        "voteCount": 2,
        "content": "I had originally chosen option B because both App Engine Standard and App Engine Flex can connect via Cloud VPN starting a few years ago. For App Engine Standard to connect with a VPC (which would be required to use Cloud VPN), we need to create a Serverless VPC Connector (as well as for other Cloud Run and Cloud Functions). This is something relatively new and probably came out after this question was designed for the exam: https://cloud.google.com/vpc/docs/serverless-vpc-access#supported_services. \n\nBased on the fact that the Serverless VPC Connector would add extra complexity to the network topology and incur in additional costs, I'm going with D."
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 54,
    "url": "https://www.examtopics.com/discussions/google/view/7180-exam-professional-cloud-architect-topic-1-question-54/",
    "body": "You are working in a highly secured environment where public Internet access from the Compute Engine VMs is not allowed. You do not yet have a VPN connection to access an on-premises file server. You need to install specific software on a Compute Engine instance. How should you install the software?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpload the required installation files to Cloud Storage. Configure the VM on a subnet with a Private Google Access subnet. Assign only an internal IP address to the VM. Download the installation files to the VM using gsutil.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpload the required installation files to Cloud Storage and use firewall rules to block all traffic except the IP address range for Cloud Storage. Download the files to the VM using gsutil.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpload the required installation files to Cloud Source Repositories. Configure the VM on a subnet with a Private Google Access subnet. Assign only an internal IP address to the VM. Download the installation files to the VM using gcloud.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpload the required installation files to Cloud Source Repositories and use firewall rules to block all traffic except the IP address range for Cloud Source Repositories. Download the files to the VM using gsutil."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 20,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2019-10-24T18:57:00.000Z",
        "voteCount": 55,
        "content": "Internet access is not allowed so it should be A. CMIIW"
      },
      {
        "date": "2020-08-06T02:39:00.000Z",
        "voteCount": 10,
        "content": "A is ok"
      },
      {
        "date": "2020-11-01T06:51:00.000Z",
        "voteCount": 3,
        "content": "A is the answer"
      },
      {
        "date": "2021-03-04T16:20:00.000Z",
        "voteCount": 3,
        "content": "A is the best answer."
      },
      {
        "date": "2020-02-12T10:09:00.000Z",
        "voteCount": 19,
        "content": "Should be A\nhttps://cloud.google.com/vpc/docs/configure-private-services-access\nNote: Even though the IP addresses for Google APIs and services are public, the traffic path from instances that are using Private Google Access to the Google APIs remains within Google's network."
      },
      {
        "date": "2024-09-04T23:23:00.000Z",
        "voteCount": 2,
        "content": "Private Google Access ensures the VM can reach Cloud Storage using its internal IP, while still restricting public internet access."
      },
      {
        "date": "2024-09-02T05:23:00.000Z",
        "voteCount": 1,
        "content": "The Answer is D"
      },
      {
        "date": "2023-10-05T09:03:00.000Z",
        "voteCount": 1,
        "content": "A. Upload the required installation files to Cloud Storage. Configure the VM on a subnet with a Private Google Access subnet. Assign only an internal IP address to the VM. Download the installation files to the VM using gsutil."
      },
      {
        "date": "2022-12-25T21:31:00.000Z",
        "voteCount": 5,
        "content": "Those who are opting for B, Can please explain without Internet access and without Private Google Access enabled how will they communicate with Cloud Storage ? :)"
      },
      {
        "date": "2022-12-21T04:02:00.000Z",
        "voteCount": 6,
        "content": "The correct answer is A: Upload the required installation files to Cloud Storage. Configure the VM on a subnet with a Private Google Access subnet. Assign only an internal IP address to the VM. Download the installation files to the VM using gsutil.\n\nTo install specific software on a Compute Engine instance in a highly secured environment where public Internet access is not allowed, you can follow these steps:\n\nUpload the required installation files to Cloud Storage.\nConfigure the VM on a subnet with a Private Google Access subnet. This will allow the VM to access Google APIs and services, such as Cloud Storage, without requiring a public IP address or internet access.\nAssign only an internal IP address to the VM. This will ensure that the VM is not accessible from the public internet.\nDownload the installation files to the VM using gsutil, which is a command-line tool that allows you to access Cloud Storage from the VM."
      },
      {
        "date": "2022-12-21T04:02:00.000Z",
        "voteCount": 6,
        "content": "Option B: Uploading the required installation files to Cloud Storage and using firewall rules to block all traffic except the IP address range for Cloud Storage is not a valid option, as it does not allow the VM to access the installation files without public internet access.\n\nOption C: Uploading the required installation files to Cloud Source Repositories and using gcloud to download the files to the VM is not a valid option, as Cloud Source Repositories does not support storing large binary files such as installation files.\n\nOption D: Uploading the required installation files to Cloud Source Repositories and using firewall rules to block all traffic except the IP address range for Cloud Source Repositories is not a valid option, as it does not allow the VM to access the installation files without public internet access."
      },
      {
        "date": "2022-11-27T06:37:00.000Z",
        "voteCount": 1,
        "content": "Eliminate B&amp;D as it connect via public networks despite it being a Google Cloud service."
      },
      {
        "date": "2022-11-06T09:19:00.000Z",
        "voteCount": 1,
        "content": "ok for A"
      },
      {
        "date": "2022-11-04T23:10:00.000Z",
        "voteCount": 2,
        "content": "With private Google access subnet, the vm can reach external network. With this setting, it violates \u201cpublic Internet access from the Compute Engine VMs is not allowed\u201d. Can someone explain why it\u2019s not B instead?"
      },
      {
        "date": "2022-11-29T21:22:00.000Z",
        "voteCount": 3,
        "content": "Private Google access means - refer to https://www.youtube.com/watch?v=yd5FtV8aJkk"
      },
      {
        "date": "2022-10-16T08:43:00.000Z",
        "voteCount": 1,
        "content": "A is good"
      },
      {
        "date": "2022-10-14T01:58:00.000Z",
        "voteCount": 1,
        "content": "A. Upload the required installation files to Cloud Storage. Configure the VM on a subnet with a Private Google Access subnet. Assign only an internal IP address to the VM. Download the installation files to the VM using gsutil."
      },
      {
        "date": "2022-09-06T12:09:00.000Z",
        "voteCount": 4,
        "content": "Configuring Private Google Access is the best way to access Google Services for VM that does not have access to the internet. In order to access Google Private APIs egress should be opened to the following IP Address restricted.googleapis.com (199.36.153.4/30). VM will leverage internal networking to access Cloud Storage \n\nhttps://cloud.google.com/vpc/docs/configure-private-google-access"
      },
      {
        "date": "2022-08-22T21:10:00.000Z",
        "voteCount": 1,
        "content": "C because Cloud repositories is a private Git within Google cloud. Hence it is ideal for simple pull, push, clone type \"git\" operations. As this is within Google cloud and is a private git, you do not need public internet access"
      },
      {
        "date": "2022-10-01T06:58:00.000Z",
        "voteCount": 1,
        "content": "I think it's not this because Clouse Source Repositories is for source code.  Sounds like we are looking for an executable?"
      },
      {
        "date": "2022-05-11T09:59:00.000Z",
        "voteCount": 3,
        "content": "C&amp;D we are all eliminating becoz of source storage repo\nBetween A&amp; B B looks more tempting to select because it mentions fire wallrule But the problem with B is the statement is wrong the access will happen from VM to storage and the statement mentions traffic from storage to Vm.\nHence A"
      },
      {
        "date": "2022-01-03T00:42:00.000Z",
        "voteCount": 3,
        "content": "You have to set Private Google Access for communicating between VM and Storage"
      },
      {
        "date": "2021-12-27T11:34:00.000Z",
        "voteCount": 3,
        "content": "Unfortunately the question it's poorly designed.\nB is correct: https://cloud.google.com/vpc/docs/configure-private-google-access"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 55,
    "url": "https://www.examtopics.com/discussions/google/view/7043-exam-professional-cloud-architect-topic-1-question-55/",
    "body": "Your company is moving 75 TB of data into Google Cloud. You want to use Cloud Storage and follow Google-recommended practices. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMove your data onto a Transfer Appliance. Use a Transfer Appliance Rehydrator to decrypt the data into Cloud Storage.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMove your data onto a Transfer Appliance. Use Cloud Dataprep to decrypt the data into Cloud Storage.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall gsutil on each server that contains data. Use resumable transfers to upload the data into Cloud Storage.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall gsutil on each server containing data. Use streaming transfers to upload the data into Cloud Storage."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 21,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2019-12-17T08:17:00.000Z",
        "voteCount": 34,
        "content": "It should be 'A'\nTransfer Appliance lets you quickly and securely transfer large amounts of data to Google Cloud Platform via a high capacity storage server that you lease from Google and ship to our datacenter. Transfer Appliance is recommended for data that exceeds 20 TB or would take more than a week to upload."
      },
      {
        "date": "2019-12-30T19:23:00.000Z",
        "voteCount": 1,
        "content": "where did u get that 20 TB number - can help to share link?"
      },
      {
        "date": "2020-12-10T21:49:00.000Z",
        "voteCount": 4,
        "content": "Here is the link:\nhttps://cloud.google.com/transfer-appliance/docs/2.2/overview"
      },
      {
        "date": "2021-12-12T21:14:00.000Z",
        "voteCount": 2,
        "content": "But that link mentions a few hundred terabytes to 1 petabyte not 20TB or did I read that incorrectly?"
      },
      {
        "date": "2023-10-27T19:03:00.000Z",
        "voteCount": 2,
        "content": "The request transfer appliance UI seems to suggest that it is not cost effective under 20TB of data."
      },
      {
        "date": "2020-07-07T22:27:00.000Z",
        "voteCount": 11,
        "content": "To this date Transfer Appliance supported locations are only\nUnited States \t\t\nCanada \t\t\nEuropean Union \t\t\nNorway \nSwitzerland.\nWhat if data reside in a location other than this?\nC is the most convenience for this scenario."
      },
      {
        "date": "2024-06-18T08:49:00.000Z",
        "voteCount": 1,
        "content": "stupid answer"
      },
      {
        "date": "2024-01-26T06:37:00.000Z",
        "voteCount": 1,
        "content": "There is NO mention of region - you dont assume anything NOT mentioned in the question therefore - Ans =A"
      },
      {
        "date": "2022-08-19T21:33:00.000Z",
        "voteCount": 5,
        "content": "Why assume a scenario no provided in the question. We need to choose the best case scenario based on available information instead of making assumptions. So A should be good."
      },
      {
        "date": "2019-10-23T00:07:00.000Z",
        "voteCount": 30,
        "content": "Why not A?"
      },
      {
        "date": "2020-08-14T03:16:00.000Z",
        "voteCount": 9,
        "content": "A is ok"
      },
      {
        "date": "2020-11-01T06:52:00.000Z",
        "voteCount": 2,
        "content": "It is A"
      },
      {
        "date": "2021-03-04T16:20:00.000Z",
        "voteCount": 14,
        "content": "A, anything over 10TB goes via appliance."
      },
      {
        "date": "2022-09-29T07:23:00.000Z",
        "voteCount": 1,
        "content": "I have moved 120 TB using gsutil- cost effectively!"
      },
      {
        "date": "2022-10-16T23:46:00.000Z",
        "voteCount": 1,
        "content": "takes longer though"
      },
      {
        "date": "2021-05-03T07:50:00.000Z",
        "voteCount": 4,
        "content": "Answer is A."
      },
      {
        "date": "2024-01-14T10:23:00.000Z",
        "voteCount": 1,
        "content": "A - Correct"
      },
      {
        "date": "2023-10-05T09:08:00.000Z",
        "voteCount": 1,
        "content": "Sending 75 TB of data on reliable 1.5 Gbps line will take about 6 days to complete data transfer online at the same time it will consume entire bandwidth. Hence use of Transfer appliace is required."
      },
      {
        "date": "2023-06-20T03:31:00.000Z",
        "voteCount": 2,
        "content": "Opion A is partially correct as you would not use a Transfer Appliance Rehydrator to decrypt the data. The Transfer Appliance itself is used to encrypt and decrypt the data. Option C is correct."
      },
      {
        "date": "2023-01-28T09:20:00.000Z",
        "voteCount": 3,
        "content": "gsutil is recommanded for data size less than a TB.  That rules out C and D\nB says decrypt data using Dataprep not sure this is possible. \nhttps://cloud.google.com/solutions/migration-to-google-cloud-transferring-your-large-datasets#transfer-options"
      },
      {
        "date": "2023-01-11T14:13:00.000Z",
        "voteCount": 3,
        "content": "It will be more precise with info about the bandwidth\nGoogle says: \nThe two main criteria to consider with Transfer Appliance are cost and speed. With reasonable network connectivity (for example, 1 Gbps), transferring 100 TB of data online takes over 10 days to complete. If this rate is acceptable, an online transfer is likely a good solution for your needs. If you only have a 100 Mbps connection (or worse from a remote location), the same transfer takes over 100 days. At this point, it's worth considering an offline-transfer option such as Transfer Appliance.\nSo even for such 100 TB google choose between transfer appliance or online transfer. not going with gsutil at all. it is clear gsutil is suitable for small to medium size (less than 1 TB)\n\nso with no more details, google recommendation is A"
      },
      {
        "date": "2022-12-21T04:04:00.000Z",
        "voteCount": 3,
        "content": "The correct answer is A: Move your data onto a Transfer Appliance. Use a Transfer Appliance Rehydrator to decrypt the data into Cloud Storage.\n\nTo move large amounts of data into Google Cloud, it is recommended to use Transfer Appliance. Transfer Appliance is a physical storage device that you can use to transfer large amounts of data to Google Cloud quickly and securely. Once you have moved your data onto a Transfer Appliance, you can use a Transfer Appliance Rehydrator to decrypt the data and load it into Cloud Storage."
      },
      {
        "date": "2022-12-21T04:05:00.000Z",
        "voteCount": 5,
        "content": "Option B: Using Cloud Dataprep to decrypt the data into Cloud Storage is not a valid option, as Cloud Dataprep is a data preparation tool that does not support data transfer or decryption.\n\nOption C: Using resumable transfers to upload the data into Cloud Storage is not a recommended option for moving large amounts of data, as resumable transfers are designed for smaller data sets and may not be efficient for transferring large amounts of data.\n\nOption D: Using streaming transfers to upload the data into Cloud Storage is not a recommended option for moving large amounts of data, as streaming transfers are designed for transferring real-time data streams and may not be efficient for transferring large amounts of data.\n\nTherefore, the correct answer is A: Move your data onto a Transfer Appliance. Use a Transfer Appliance Rehydrator to decrypt the data into Cloud Storage."
      },
      {
        "date": "2022-11-06T09:21:00.000Z",
        "voteCount": 1,
        "content": "ok for A"
      },
      {
        "date": "2022-10-16T08:45:00.000Z",
        "voteCount": 1,
        "content": "Option A Use a Transfer Appliance Rehydrator"
      },
      {
        "date": "2022-10-14T02:06:00.000Z",
        "voteCount": 1,
        "content": "A. Move your data onto a Transfer Appliance. Use a Transfer Appliance Rehydrator to decrypt the data into Cloud Storage."
      },
      {
        "date": "2022-07-31T10:05:00.000Z",
        "voteCount": 3,
        "content": "A is correct ...\nhttps://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets#transfer-options \n1. gsutil is for lessthan &lt;1 TB data with enough bandwidth, so C and D can be eliminated\n2. option b can be eliminated since dataprep for decription is not correct\n3. so only left over is a and its offline transfer, since the question did not give any time line when the transfer to be completed"
      },
      {
        "date": "2022-05-19T21:39:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/transfer-appliance/docs/4.0/overview#suitability"
      },
      {
        "date": "2022-05-11T10:09:00.000Z",
        "voteCount": 1,
        "content": "I would eliminate B &amp; C as the question clearly methos google recomendations.About 10 TB or canal to we need to me transfer appliance. Let's not worry about what regions Ohk for now.\nBetween A &amp; B B is over kill as transfer applion allows decryption. Hence A"
      },
      {
        "date": "2022-04-15T09:47:00.000Z",
        "voteCount": 3,
        "content": "Is Transfer Appliance suitable for me?\n\"Your data size is greater than or equal to 10TB.\"\nhttps://cloud.google.com/transfer-appliance/docs/4.0/overview#suitability"
      },
      {
        "date": "2022-02-11T20:20:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/blog/ja/topics/developers-practitioners/how-transfer-your-data-google-cloud"
      },
      {
        "date": "2022-01-20T07:28:00.000Z",
        "voteCount": 2,
        "content": "A, is the correct option for transferring data of few TB to PB.\n gsutil is viable option if the data size is about 1TB or less than that. \n\nReference: https://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 56,
    "url": "https://www.examtopics.com/discussions/google/view/7184-exam-professional-cloud-architect-topic-1-question-56/",
    "body": "You have an application deployed on Google Kubernetes Engine using a Deployment named echo-deployment. The deployment is exposed using a Service called echo-service. You need to perform an update to the application with minimal downtime to the application. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse kubectl set image deployment/echo-deployment &lt;new-image&gt;\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the rolling update functionality of the Instance Group behind the Kubernetes cluster",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the deployment yaml file with the new container image. Use kubectl delete deployment/echo-deployment and kubectl create \u05d2\u20ac\"f &lt;yaml-file&gt;",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the service yaml file which the new container image. Use kubectl delete service/echo-service and kubectl create \u05d2\u20ac\"f &lt;yaml-file&gt;"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 36,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2019-10-24T22:28:00.000Z",
        "voteCount": 47,
        "content": "A  is correct. \n\nB is funny"
      },
      {
        "date": "2022-05-16T02:30:00.000Z",
        "voteCount": 6,
        "content": "B looks most sensible (not funny). rolling update is a deployment strategy, which will deploy on pods 1 by 1,. i.e. by the time first pod is getting newer version of application, other pods are running with older version... In this way, there will be no downtime of application.. which is real ask from this question.\nI recommend B"
      },
      {
        "date": "2024-04-29T03:06:00.000Z",
        "voteCount": 4,
        "content": "Use the rolling update functionality of the &gt;[Instance Group behind the Kubernetes cluster]&lt;. It's not a rolling update of a Deployment. Read carefully."
      },
      {
        "date": "2020-08-06T02:53:00.000Z",
        "voteCount": 14,
        "content": "A is ok"
      },
      {
        "date": "2020-11-01T06:54:00.000Z",
        "voteCount": 3,
        "content": "Yes A is correct"
      },
      {
        "date": "2021-03-04T16:22:00.000Z",
        "voteCount": 1,
        "content": "Only logical answer is A."
      },
      {
        "date": "2019-10-26T01:11:00.000Z",
        "voteCount": 13,
        "content": "Correct is A"
      },
      {
        "date": "2024-07-08T08:13:00.000Z",
        "voteCount": 2,
        "content": "It's A. As per K8s documentation:\nTo update the image of the application to version 2, use the set image subcommand, followed by the deployment name and the new image version:\n\nkubectl set image deployments/kubernetes-bootcamp kubernetes-bootcamp=docker.io/jocatalin/kubernetes-bootcamp:v2\n\nThe command notified the Deployment to use a different image for your app and initiated a rolling update.\"\n\""
      },
      {
        "date": "2024-01-14T10:27:00.000Z",
        "voteCount": 1,
        "content": "Option C -  is the best option to perform an update to an application deployed on Google Kubernetes Engine with minimal downtime because it provides control over the update process, ensures high availability, and minimizes disruption. Rolling update functionality can also be used but requires more effort to implement. \nOption A and Option D may result in downtime if the new image is incompatible with the existing application."
      },
      {
        "date": "2023-11-11T05:42:00.000Z",
        "voteCount": 4,
        "content": "A\nhttps://cloud.google.com/kubernetes-engine/docs/how-to/updating-apps#updating_an_application"
      },
      {
        "date": "2023-10-05T09:14:00.000Z",
        "voteCount": 1,
        "content": "C. Update the deployment yaml file with the new container image. Use kubectl delete deployment/echo-deployment and kubectl create \u05d2\u20ac\"f &lt;yaml-file&gt;\n\nI agreed with omermahgoub with his explanation."
      },
      {
        "date": "2023-12-30T11:20:00.000Z",
        "voteCount": 1,
        "content": "delete.. minimal downtime.. A is correct ^_^U"
      },
      {
        "date": "2023-01-31T00:42:00.000Z",
        "voteCount": 4,
        "content": "To perform an update to the application with minimal downtime on Google Kubernetes Engine (GKE), you can use a rolling update strategy, which involves updating the application incrementally, one pod at a time, while ensuring that the updated pods are functioning properly before updating the next set. Here's the general process:\nkubectl set image deployment/echo-deployment echo=&lt;new_image_tag&gt;"
      },
      {
        "date": "2023-01-03T12:19:00.000Z",
        "voteCount": 9,
        "content": "https://cloud.google.com/kubernetes-engine/docs/how-to/updating-apps \nsays rolling updates and mentions same command . \nSo 100 % A"
      },
      {
        "date": "2022-12-21T04:16:00.000Z",
        "voteCount": 3,
        "content": "The correct answer is C: Update the deployment yaml file with the new container image. Use kubectl delete deployment/echo-deployment and kubectl create \u2013f &lt;yaml-file&gt;.\n\nTo perform an update to an application deployed on Google Kubernetes Engine with minimal downtime, you can follow these steps:\n\nUpdate the deployment yaml file with the new container image.\nUse the kubectl delete deployment/echo-deployment command to delete the existing deployment.\nUse the kubectl create \u2013f &lt;yaml-file&gt; command to create a new deployment using the updated yaml file.\nThis process, known as a rolling update, allows you to update your application with minimal downtime by replacing the old version of the application with the new version one pod at a time, while ensuring that there is always at least one pod available to serve traffic."
      },
      {
        "date": "2022-12-21T04:22:00.000Z",
        "voteCount": 1,
        "content": "using kubectl set image deployment/deployment &lt;new-image&gt; will not allow you to perform an update to the application with minimal downtime, even if the deployment is exposed using a Service.\n\nThis command will update the image of the containers in the deployment, but it will not perform a rolling update. A rolling update allows you to update your application with minimal downtime by replacing the old version of the application with the new version one pod at a time, while ensuring that there is always at least one pod available to serve traffic. Without a rolling update, all of the pods in the deployment will be replaced at the same time, which may result in downtime for the application."
      },
      {
        "date": "2022-12-21T04:16:00.000Z",
        "voteCount": 3,
        "content": "Option A: Using kubectl set image deployment/echo-deployment &lt;new-image&gt; will update the image of the containers in the deployment, but it will not perform a rolling update and may result in downtime for the application.\n\nOption B: Using the rolling update functionality of the Instance Group behind the Kubernetes cluster is not a valid option, as the rolling update functionality is used to update the instances in the instance group, not the containers in a deployment.\n\nOption D: Updating the service yaml file with the new container image and using kubectl delete service/echo-service and kubectl create \u2013f &lt;yaml-file&gt; is not a valid option, as the service is not responsible for running the application containers and updating the service will not update the application."
      },
      {
        "date": "2023-01-25T06:31:00.000Z",
        "voteCount": 1,
        "content": "A is correct, update template spec image in deployment yml will trigger rollout deploy"
      },
      {
        "date": "2023-10-16T08:04:00.000Z",
        "voteCount": 3,
        "content": "Answer is A, you are wrong as hell, if you delete deployment its obvious app will face downtime"
      },
      {
        "date": "2022-12-02T13:43:00.000Z",
        "voteCount": 3,
        "content": "I think A is correct:\n\nB. I don't understand the objective of this option.\nC and D. These are eliminated because they involve suffering a downtime when the resources are eliminated, so they are not fulfilling one of the requirements."
      },
      {
        "date": "2022-11-30T03:15:00.000Z",
        "voteCount": 5,
        "content": "Example from official Kubernetes docu (for NGINX):\nkubectl set image deployment/nginx-deployment nginx=nginx:1.16.1\n\nhttps://kubernetes.io/docs/concepts/workloads/controllers/deployment/"
      },
      {
        "date": "2022-11-06T09:33:00.000Z",
        "voteCount": 2,
        "content": "ok for A"
      },
      {
        "date": "2022-10-16T08:46:00.000Z",
        "voteCount": 1,
        "content": "A is right -- kubectl set image deployment/echo-deployment"
      },
      {
        "date": "2022-08-18T09:27:00.000Z",
        "voteCount": 8,
        "content": "Source: https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#updating-a-deployment\n\nDeployment ensures that only a certain number of Pods are down while they are being updated. By default, it ensures that at least 75% of the desired number of Pods are up (25% max unavailable).\n\nDeployment also ensures that only a certain number of Pods are created above the desired number of Pods. By default, it ensures that at most 125% of the desired number of Pods are up (25% max surge)."
      },
      {
        "date": "2022-08-05T03:33:00.000Z",
        "voteCount": 2,
        "content": "Answer is A\n\nIt can't be C, if you delete and recreate the deployment you will have a downtime between the deletion and the recreation."
      },
      {
        "date": "2022-07-19T08:40:00.000Z",
        "voteCount": 2,
        "content": "It's definitely A.  See here under updating a deployment on the right hand side. \nhttps://kubernetes.io/docs/concepts/workloads/controllers/deployment/#updating-a-deployment"
      },
      {
        "date": "2022-07-13T11:00:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/kubernetes-engine/docs/how-to/updating-apps#updating_an_application"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 57,
    "url": "https://www.examtopics.com/discussions/google/view/68708-exam-professional-cloud-architect-topic-1-question-57/",
    "body": "Your company is using BigQuery as its enterprise data warehouse. Data is distributed over several Google Cloud projects. All queries on BigQuery need to be billed on a single project. You want to make sure that no query costs are incurred on the projects that contain the data. Users should be able to query the datasets, but not edit them.<br>How should you configure users' access roles?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd all users to a group. Grant the group the role of BigQuery user on the billing project and BigQuery dataViewer on the projects that contain the data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd all users to a group. Grant the group the roles of BigQuery dataViewer on the billing project and BigQuery user on the projects that contain the data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd all users to a group. Grant the group the roles of BigQuery jobUser on the billing project and BigQuery dataViewer on the projects that contain the data.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd all users to a group. Grant the group the roles of BigQuery dataViewer on the billing project and BigQuery jobUser on the projects that contain the data."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 57,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-08-18T09:40:00.000Z",
        "voteCount": 24,
        "content": "Both A &amp; C are correct but using the principle of least privileges C is the most appropriate.\n\nBigQuery User: (roles/bigquery.user)\nWhen applied to a dataset, this role provides the ability to read the dataset's metadata and list tables in the dataset.\nWhen applied to a project, this role also provides the ability to run jobs, including queries, within the project. A principal with this role can enumerate their own jobs, cancel their own jobs, and enumerate datasets within a project. &lt;b&gt;Additionally, allows the creation of new datasets within the project; the creator is granted the BigQuery Data Owner role(roles/bigquery.dataOwner) on these new datasets.&lt;/b&gt; \nLowest-level resources where you can grant this role: Dataset\n\nBigQuery Job User: (roles/bigquery.jobUser)\nProvides permissions to run jobs, including queries, within the project.\nLowest-level resources where you can grant this role: Project\n\nSource: https://cloud.google.com/bigquery/docs/access-control"
      },
      {
        "date": "2022-02-05T07:11:00.000Z",
        "voteCount": 20,
        "content": "C is the correct Answer ,\nA is wrong because bq User Permission will allow you to edit the dataset, which is something that we don't want in this scenario.\nB and D is wrong because \"You want to make sure that no query costs are incurred on the projects that contain the data\" so you don't want users to fire quires on the Project that contains the dataset , hence the \"dataViewer\" permission\n\nhttps://cloud.google.com/bigquery/docs/access-control"
      },
      {
        "date": "2023-03-21T02:10:00.000Z",
        "voteCount": 1,
        "content": "It seems that User Permission doesn't allow to edit data, isn't it?"
      },
      {
        "date": "2024-02-19T01:25:00.000Z",
        "voteCount": 1,
        "content": "The link to refer here: https://cloud.google.com/bigquery/docs/access-control"
      },
      {
        "date": "2023-03-28T19:01:00.000Z",
        "voteCount": 3,
        "content": "The \"roles/bigquery.jobUser\" role provides the permission to run jobs, including querying, exporting and copying data, and creating views and materialized views. This role does not provide permissions to create, update, or delete BigQuery resources, such as datasets, tables, and models. Users with this role can only interact with BigQuery through jobs.\n\nThe \"roles/bigquery.User\" role, on the other hand, provides the permission to create, update, and delete BigQuery resources, as well as run jobs. This role includes all the permissions of the \"roles/bigquery.jobUser\" role, and in addition allows users to manage BigQuery resources, such as creating datasets, tables, and models, and modifying their schema and access controls."
      },
      {
        "date": "2023-03-20T13:34:00.000Z",
        "voteCount": 1,
        "content": "A is wrong because https://cloud.google.com/bigquery/docs/access-control#bigquery.user\nC is correct because https://cloud.google.com/bigquery/docs/access-control#bigquery.jobUser"
      },
      {
        "date": "2022-12-24T11:50:00.000Z",
        "voteCount": 7,
        "content": "Important statements from the prompt\n1. All queries need to be billed to a single project - one project that queries data stored on other projects. Let's call this our billing project. \n       a. jobUser is the best role to satisfy this need, because it provides permission to run jobs \n            and queries within a project.\n\n2. Other projects is where the data resides. These projects don't need much access besides the ability to be viewed (not edited).\n       a. The dataViewer role provide permission to read all datasets in the project."
      },
      {
        "date": "2022-12-21T04:27:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is A: Add all users to a group. Grant the group the role of BigQuery user on the billing project and BigQuery dataViewer on the projects that contain the data.\n\nTo make sure that no query costs are incurred on the projects that contain the data and allow users to query the datasets but not edit them, you should follow these steps:\n\nAdd all users to a group.\nGrant the group the role of BigQuery user on the billing project. This will allow the group to run queries on BigQuery and incur costs on the billing project.\nGrant the group the role of BigQuery dataViewer on the projects that contain the data. This will allow the group to view the datasets and run queries on them, but not edit them."
      },
      {
        "date": "2024-04-10T03:22:00.000Z",
        "voteCount": 1,
        "content": "BigQuery User \n(roles/bigquery.user)\nWhen applied to a dataset, this role provides the ability to read the dataset's metadata and list tables in the dataset.\n\nWhen applied to a project, this role also provides the ability to run jobs, including queries, within the project. A principal with this role can enumerate their own jobs, cancel their own jobs, and enumerate datasets within a project. Additionally, allows the creation of new datasets within the project; the creator is granted the BigQuery Data Owner role (roles/bigquery.dataOwner) on these new datasets.\n\nBigquery.user has potential to create a dataset inside the project and creates becomes owner of the dataset. This is not the requirement stated in the question scenario.\nAnswer is C"
      },
      {
        "date": "2022-12-21T04:27:00.000Z",
        "voteCount": 1,
        "content": "Option B: Granting the group the roles of BigQuery dataViewer on the billing project and BigQuery user on the projects that contain the data will not allow the group to incur costs on the billing project and will not meet the requirements of the scenario.\n\nOption C: Granting the group the roles of BigQuery jobUser on the billing project and BigQuery dataViewer on the projects that contain the data will not allow the group to incur costs on the billing project and will not meet the requirements of the scenario.\n\nOption D: Granting the group the roles of BigQuery dataViewer on the billing project and BigQuery jobUser on the projects that contain the data will not allow the group to incur costs on the billing project and will not meet the requirements of the scenario."
      },
      {
        "date": "2022-12-21T04:31:00.000Z",
        "voteCount": 3,
        "content": "The BigQuery Job User role (roles/bigquery.jobUser) and the BigQuery User role (roles/bigquery.user) have similar permissions, but they differ in the scope of their permissions.\n\nThe BigQuery Job User role grants users the ability to create and modify query jobs, but it does not grant them the ability to run queries or incur costs on the project. This role is intended for users who need to create and manage query jobs, but who should not be able to run queries or incur costs.\n\nThe BigQuery User role grants users the ability to run queries and incur costs on the project, in addition to the ability to create and modify query jobs. This role is intended for users who need to run queries and incur costs on the project, as well as create and manage query jobs."
      },
      {
        "date": "2022-12-21T04:31:00.000Z",
        "voteCount": 2,
        "content": "Here is a summary of the differences between the BigQuery Job User role and the BigQuery User role:\n\nBigQuery Job User role (roles/bigquery.jobUser):\n\nCan create and modify query jobs\nCannot run queries or incur costs on the project\nBigQuery User role (roles/bigquery.user):\n\nCan create and modify query jobs\nCan run queries and incur costs on the project\nIf you want to grant users the ability to create and modify query jobs, but not run queries or incur costs on the project, you should use the BigQuery Job User role. If you want to grant users the ability to run queries and incur costs on the project, in addition to the ability to create and modify query jobs, you should use the BigQuery User role."
      },
      {
        "date": "2022-10-16T08:48:00.000Z",
        "voteCount": 1,
        "content": "C is right \nAdd all users to a group. Grant the group the roles of BigQuery jobUser on the billing project and BigQuery dataViewer on the projects that contain the data."
      },
      {
        "date": "2022-10-14T02:15:00.000Z",
        "voteCount": 1,
        "content": "C. Add all users to a group. Grant the group the roles of BigQuery jobUser on the billing project and BigQuery dataViewer on the projects that contain the data."
      },
      {
        "date": "2022-10-07T14:08:00.000Z",
        "voteCount": 1,
        "content": "D is the answer:\nCloud BigQuery Roles\nCloud BigQuery IAM Roles\nBigQuery Admin - bigquery.*\nBigQuery Data Owner - bigquery.datasets.*, bigquery.models.*, bigquery.routines.*,\nbigquery.tables.* (Does NOT have access to Jobs!)\nBigQuery Data Editor - bigquery.tables.(create/delete/export/get/getData/getIamPolicy/\nlist/update/updateData/updateTag), bigquery.models.*, bigquery.routines.*,\nbigquery.datasets.(create/get/getIamPolicy/updateTag)\nBigQuery Data Viewer - get/list bigquery.(datasets/models/routines/tables)\nBigQuery Job User - bigquery.jobs.create\nBigQuery User - BigQuery Data Viewer + get/list (jobs, capacityCommitments, reservations\netc)\nTo see data, you need either BigQuery User or BigQuery Data Viewer roles\nYou CANNOT see data with BigQuery Job User roles\nBigQuery Data Owner or Data Viewer roles do NOT have access to jobs!"
      },
      {
        "date": "2022-02-05T07:10:00.000Z",
        "voteCount": 1,
        "content": "C is the correct Answer , \nA is wrong because bq User Permission will allow you to edit the dataset, which is something that we don't want in this scenario.\nB and D is wrong because \"You want to make sure that no query costs are incurred on the projects that contain the data\" so you don't want users to fire quires on the Project that contains the dataset , hence the \"dataViewer\" permission\n\nhttps://cloud.google.com/bigquery/docs/access-control"
      },
      {
        "date": "2022-01-01T15:29:00.000Z",
        "voteCount": 4,
        "content": "C. Add all users to a group. Grant the group the roles of BigQuery jobUser on the billing project and BigQuery dataViewer on the projects that contain the data."
      },
      {
        "date": "2021-12-28T02:14:00.000Z",
        "voteCount": 2,
        "content": "C looks to be the correct answer"
      },
      {
        "date": "2021-12-28T00:09:00.000Z",
        "voteCount": 1,
        "content": "JobUser is the correct terminology for bq. Only read access to data sources is required."
      },
      {
        "date": "2021-12-28T00:08:00.000Z",
        "voteCount": 2,
        "content": "bq is using jobs - so \"user\" isn't specific enough, jobuser is."
      },
      {
        "date": "2022-01-03T06:53:00.000Z",
        "voteCount": 1,
        "content": "Hence C"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 58,
    "url": "https://www.examtopics.com/discussions/google/view/6889-exam-professional-cloud-architect-topic-1-question-58/",
    "body": "You have developed an application using Cloud ML Engine that recognizes famous paintings from uploaded images. You want to test the application and allow specific people to upload images for the next 24 hours. Not all users have a Google Account. How should you have users upload images?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tHave users upload the images to Cloud Storage. Protect the bucket with a password that expires after 24 hours.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tHave users upload the images to Cloud Storage using a signed URL that expires after 24 hours.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an App Engine web application where users can upload images. Configure App Engine to disable the application after 24 hours. Authenticate users via Cloud Identity.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an App Engine web application where users can upload images for the next 24 hours. Authenticate users via Cloud Identity."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2019-10-21T07:21:00.000Z",
        "voteCount": 44,
        "content": "Correct answer is B"
      },
      {
        "date": "2020-08-06T03:02:00.000Z",
        "voteCount": 8,
        "content": "B is ok"
      },
      {
        "date": "2020-11-01T06:58:00.000Z",
        "voteCount": 3,
        "content": "Signed URL ... B is correct"
      },
      {
        "date": "2021-03-04T16:30:00.000Z",
        "voteCount": 3,
        "content": "B signed URL"
      },
      {
        "date": "2019-12-31T02:53:00.000Z",
        "voteCount": 27,
        "content": "Ans B\n\"When should you use a signed URL? In some scenarios, you might not want to require your users to have a Google account in order to access Cloud Storage\"  \"Signed URLs contain authentication information in their query string, allowing users without credentials to perform specific actions on a resource\"\nhttps://cloud.google.com/storage/docs/access-control/signed-urls"
      },
      {
        "date": "2024-01-06T19:41:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is B"
      },
      {
        "date": "2023-07-21T08:07:00.000Z",
        "voteCount": 1,
        "content": "B is the answer"
      },
      {
        "date": "2023-03-25T16:48:00.000Z",
        "voteCount": 3,
        "content": "The correct answer is B.\n\nA is not a good choice because it is not possible to set an expiration time for a password protected Cloud Storage bucket. This means that if a user had the password, they would be able to upload images to the bucket even after the 24 hour period has expired.\n\nB is the correct answer because a signed URL can be generated to allow specific users to upload images to Cloud Storage without requiring them to have a Google Account. The URL can be set to expire after 24 hours, which ensures that users can only upload images during the allowed time period.\n\nC is not the best choice because it involves creating an App Engine web application, which is more complex than using Cloud Storage with a signed URL. Additionally, App Engine instances cannot be turned off programmatically, so it would not be possible to disable the application after 24 hours.\n\nD option is similar to option C, but it involves creating an App Engine web application. This would add unnecessary complexity to the solution, and it would not provide any additional benefits compared to using Cloud Storage with a signed URL."
      },
      {
        "date": "2022-12-21T04:39:00.000Z",
        "voteCount": 3,
        "content": "The correct answer is B: Have users upload the images to Cloud Storage using a signed URL that expires after 24 hours.\n\nTo allow specific users to upload images to Cloud Storage for testing your Cloud ML Engine application, and to not require all users to have a Google Account, you should use signed URLs. A signed URL is a URL that allows access to a specific resource in Cloud Storage, and that is only valid for a specified period of time.\n\nTo create a signed URL that expires after 24 hours, you can use the gsutil signurl command. For example:\n\nCopy code\ngsutil signurl -d 24h service-account.json gs://bucket-name/object-name\nThis will generate a signed URL that allows users to upload an object to the specified bucket with the specified name, and that will only be valid for 24 hours."
      },
      {
        "date": "2022-12-21T04:39:00.000Z",
        "voteCount": 1,
        "content": "Option A: Protecting the bucket with a password that expires after 24 hours would not be a secure or scalable solution, as it would require you to distribute the password to all users and to update the password every 24 hours.\n\nOption C: Creating an App Engine web application where users can upload images, and configuring App Engine to disable the application after 24 hours, would not allow users to upload images after the application is disabled.\n\nOption D: Creating an App Engine web application where users can upload images for the next 24 hours and authenticating users via Cloud Identity would not allow users to upload images if they do not have a Google Account."
      },
      {
        "date": "2022-11-07T00:19:00.000Z",
        "voteCount": 1,
        "content": "B is ok"
      },
      {
        "date": "2022-10-16T08:50:00.000Z",
        "voteCount": 1,
        "content": "B is right, Signed URL's will help in this scnerio."
      },
      {
        "date": "2022-10-14T02:22:00.000Z",
        "voteCount": 1,
        "content": "B. Have users upload the images to Cloud Storage using a signed URL that expires after 24 hours."
      },
      {
        "date": "2022-07-01T10:10:00.000Z",
        "voteCount": 2,
        "content": "On 06/30/2022 Exam."
      },
      {
        "date": "2022-03-29T10:55:00.000Z",
        "voteCount": 1,
        "content": "signed url"
      },
      {
        "date": "2021-12-25T06:15:00.000Z",
        "voteCount": 2,
        "content": "B is the correct answer"
      },
      {
        "date": "2021-12-06T15:39:00.000Z",
        "voteCount": 1,
        "content": "Go for B."
      },
      {
        "date": "2021-10-26T00:41:00.000Z",
        "voteCount": 1,
        "content": "B \u2013 Have users upload the images to Cloud Storage via signed URL which expires after 24 hours.\nSigned URL is a preferable way to allow something with limited timeframe, doesn't require the account"
      },
      {
        "date": "2021-10-23T22:36:00.000Z",
        "voteCount": 1,
        "content": "B is right. Signed URL are best for users for short term access."
      },
      {
        "date": "2021-07-08T10:51:00.000Z",
        "voteCount": 3,
        "content": "Answer is B"
      },
      {
        "date": "2021-06-07T02:34:00.000Z",
        "voteCount": 4,
        "content": "Answer B\nA signed URL is a URL that provides limited permission and time to make a request. Signed URLs contain authentication information in their query string, allowing users without credentials to perform specific actions on a resource. When you generate a signed URL, you specify a user or service account which must have sufficient permission to make the request that the signed URL will make. After you generate a signed URL, anyone who possesses it can use the signed URL to perform specified actions, such as reading an object, within a specified period of time.\n\nWhen should you use a signed URL?\nIn some scenarios, you might not want to require your users to have a Google account in order to access Cloud Storage, but you still want to control access using your application-specific logic."
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 59,
    "url": "https://www.examtopics.com/discussions/google/view/7285-exam-professional-cloud-architect-topic-1-question-59/",
    "body": "Your web application must comply with the requirements of the European Union's General Data Protection Regulation (GDPR). You are responsible for the technical architecture of your web application. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure that your web application only uses native features and services of Google Cloud Platform, because Google already has various certifications and provides \u05d2\u20acpass-on\u05d2\u20ac compliance when you use native features.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable the relevant GDPR compliance setting within the GCPConsole for each of the services in use within your application.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure that Cloud Security Scanner is part of your test planning strategy in order to pick up any compliance gaps.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDefine a design for the security of data in your web application that meets GDPR requirements.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-01-12T06:09:00.000Z",
        "voteCount": 17,
        "content": "Agree D"
      },
      {
        "date": "2020-09-15T14:05:00.000Z",
        "voteCount": 15,
        "content": "D - https://cloud.google.com/security/gdpr \nThe GDPR lays out specific requirements for businesses and organizations who are established in Europe or who serve users in Europe. It:\n\nRegulates how businesses can collect, use, and store personal data\nBuilds upon current documentation and reporting requirements to increase accountability\nAuthorizes fines on businesses who fail to meet its requirements"
      },
      {
        "date": "2022-12-21T05:55:00.000Z",
        "voteCount": 4,
        "content": "The correct answer is option D: Define a design for the security of data in your web application that meets GDPR requirements.\n\nThe General Data Protection Regulation (GDPR) is a comprehensive data protection law that applies to any company that processes the personal data of individuals in the European Union (EU). As the technical architect of your web application, it is your responsibility to ensure that the application is compliant with GDPR requirements."
      },
      {
        "date": "2022-12-21T05:55:00.000Z",
        "voteCount": 8,
        "content": "Option A: While it is true that Google has various certifications and provides pass-on compliance when you use native features, simply using native features and services of Google Cloud Platform is not sufficient to ensure compliance with GDPR. You still need to implement appropriate controls and safeguards to protect personal data and meet GDPR requirements.\n\nOption B: Enabling the relevant GDPR compliance setting within the GCP console for each of the services in use within your application may help ensure compliance with GDPR, but it is not sufficient on its own. You still need to implement appropriate controls and safeguards to protect personal data and meet GDPR requirements.\n\nOption C: Using Cloud Security Scanner as part of your test planning strategy can help identify potential security vulnerabilities and compliance gaps in your web application, but it is not sufficient on its own to ensure compliance with GDPR. You still need to implement appropriate controls and safeguards to protect personal data and meet GDPR requirements."
      },
      {
        "date": "2022-11-07T00:21:00.000Z",
        "voteCount": 1,
        "content": "D is ok"
      },
      {
        "date": "2022-10-16T08:52:00.000Z",
        "voteCount": 1,
        "content": "Define a design for the security of data in your web application that meets GDPR requirements.  D is right"
      },
      {
        "date": "2022-02-11T13:29:00.000Z",
        "voteCount": 6,
        "content": "I got similar question on my exam."
      },
      {
        "date": "2021-12-25T06:19:00.000Z",
        "voteCount": 1,
        "content": "D is the correct answer"
      },
      {
        "date": "2021-12-06T15:46:00.000Z",
        "voteCount": 1,
        "content": "Go for D"
      },
      {
        "date": "2021-11-24T06:31:00.000Z",
        "voteCount": 1,
        "content": "vote D"
      },
      {
        "date": "2021-10-26T00:42:00.000Z",
        "voteCount": 1,
        "content": "D \u2013 Define a design for the security of data in your web app that meets GDPR requirements."
      },
      {
        "date": "2021-09-03T02:53:00.000Z",
        "voteCount": 1,
        "content": "A is wrong D is correct. The q refers is \u201cMicrosoft sql\u201d not \u201cMySQL\u201d. App replication in MSsql is achieved with Availability Groups within MSsql\nhttps://docs.microsoft.com/en-us/sql/database-engine/availability-groups/windows/overview-of-always-on-availability-groups-sql-server?view=sql-server-ver15"
      },
      {
        "date": "2021-07-08T10:52:00.000Z",
        "voteCount": 3,
        "content": "Answer is D"
      },
      {
        "date": "2021-05-19T01:13:00.000Z",
        "voteCount": 2,
        "content": "D. Define a design for the security of data in your web application that meets GDPR requirements."
      },
      {
        "date": "2021-05-13T00:07:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2021-03-31T23:28:00.000Z",
        "voteCount": 1,
        "content": "Answer is D"
      },
      {
        "date": "2021-03-26T09:55:00.000Z",
        "voteCount": 1,
        "content": "D is ok"
      },
      {
        "date": "2021-02-14T03:24:00.000Z",
        "voteCount": 3,
        "content": "you should design your app such that they meet GDPR req. As a customer google cloud, GDPR should be part of protection strategy .So the ans is D."
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 60,
    "url": "https://www.examtopics.com/discussions/google/view/6443-exam-professional-cloud-architect-topic-1-question-60/",
    "body": "You need to set up Microsoft SQL Server on GCP. Management requires that there's no downtime in case of a data center outage in any of the zones within a<br>GCP region. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a Cloud SQL instance with high availability enabled.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a Cloud Spanner instance with a regional instance configuration.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up SQL Server on Compute Engine, using Always On Availability Groups using Windows Failover Clustering. Place nodes in different subnets.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up SQL Server Always On Availability Groups using Windows Failover Clustering. Place nodes in different zones."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 70,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 64,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-01-28T09:32:00.000Z",
        "voteCount": 61,
        "content": "A seems correct.\n\"...  high availability (HA) configuration for Cloud SQL instances... A Cloud SQL instance configured for HA is also called a regional instance and is located in a primary and secondary zone within the configured region.\nIn the event of an instance or zone failure, this configuration reduces downtime, and your data continues to be available to client applications.\""
      },
      {
        "date": "2022-04-09T03:49:00.000Z",
        "voteCount": 5,
        "content": "You need to set up a Microsoft SQL server. Why are we talking about Cloud SQL"
      },
      {
        "date": "2022-08-13T10:38:00.000Z",
        "voteCount": 10,
        "content": "and what is Cloud SQL -&gt; a managed service for MySQL, Posrgers, and MS SQL server"
      },
      {
        "date": "2024-08-19T14:13:00.000Z",
        "voteCount": 1,
        "content": "Actually, although Cloud SQL offers high availability configurations, it currently doesn't support Microsoft SQL Server as one of its managed database engines. It primarily focuses on MySQL, PostgreSQL, and SQL Server (but not the full Microsoft SQL Server). And the question clearly states \"you need to set up Microsoft SQL.\"  Very tricky question. The answer is D"
      },
      {
        "date": "2021-07-20T11:16:00.000Z",
        "voteCount": 12,
        "content": "but it says: you need to setup SQL Server"
      },
      {
        "date": "2020-01-28T09:34:00.000Z",
        "voteCount": 5,
        "content": "It applies for MySQL and HA is not available for MS SQL"
      },
      {
        "date": "2020-01-30T01:35:00.000Z",
        "voteCount": 3,
        "content": "Yes it is available, its in beta, but when creating a \"SQL Server 2017 Standard\" in Cloud SQL menu you can chose single one or HA (regional)."
      },
      {
        "date": "2024-06-05T12:43:00.000Z",
        "voteCount": 1,
        "content": "The problem is that these questions are ancient (talking about StackDriver all the time for example, it was rebranded in 2020!!!). So unfortunately we need to think of \"What did the professor think 2-4 years ago\" when this question was created. Otherwise I'd say A is the best all day!"
      },
      {
        "date": "2020-07-06T00:54:00.000Z",
        "voteCount": 10,
        "content": "It is available, please see;\nhttps://cloud.google.com/sql/docs/sqlserver/high-availability?_ga=2.30855355.-503483612.1582800507\nAlso a video from Google;\nhttps://youtu.be/vMUpNoukwnM"
      },
      {
        "date": "2022-05-16T02:53:00.000Z",
        "voteCount": 4,
        "content": "D is correct.\nQuestion is - \"no downtime while installing MS SQL\" , not on choosing or replacing with GCP product. I agree A is good solution for this requirements.. however question is not on choosing database.. its for HA.. so I will choose D."
      },
      {
        "date": "2020-03-14T16:53:00.000Z",
        "voteCount": 30,
        "content": "Answer is A. Cloud SQL supports SQL Server and selecting high availability provides automatic failover within a region."
      },
      {
        "date": "2024-09-07T06:57:00.000Z",
        "voteCount": 3,
        "content": "The correct answer is D. Set up SQL Server Always On Availability Groups using Windows Failover Clustering. Place nodes in different zones.\n\nHere\u2019s why:\n\nSQL Server Always On Availability Groups is a high-availability and disaster recovery solution for SQL Server that works across multiple zones, ensuring minimal downtime in case of a data center outage within a region.\nBy placing the nodes in different zones, you ensure that the database remains accessible even if one zone goes down, meeting the requirement of no downtime in case of a zone failure."
      },
      {
        "date": "2024-09-07T06:57:00.000Z",
        "voteCount": 2,
        "content": "The other options are less suitable for this scenario:\n\nA. Cloud SQL with high availability enabled would not work because Cloud SQL supports MySQL, PostgreSQL, and SQL Server but has limitations for high-availability setups with SQL Server compared to Always On Availability Groups.\nB. Cloud Spanner is a Google-native distributed database solution and not directly related to SQL Server.\nC mentions using different subnets, but you specifically need to place the nodes in different zones to ensure availability across multiple data centers within a region.\nThus, D is the most accurate solution for ensuring no downtime in the event of a zone failure while using Microsoft SQL Server on GCP."
      },
      {
        "date": "2024-08-01T08:35:00.000Z",
        "voteCount": 2,
        "content": "Cloud SQL for SQL Server is a fully managed relational database service from Google Cloud that allows users to set up, maintain, and manage Microsoft SQL Server databases in the cloud. Cloud SQL also supports MySQL and PostgreSQL"
      },
      {
        "date": "2024-08-01T08:35:00.000Z",
        "voteCount": 1,
        "content": "Answer A"
      },
      {
        "date": "2024-06-29T07:50:00.000Z",
        "voteCount": 1,
        "content": "go for A, Why not use the available option?"
      },
      {
        "date": "2024-06-09T04:29:00.000Z",
        "voteCount": 2,
        "content": "It is A,\nWho say D, better not to work in cloud, but yes in on premises windows servers LOL"
      },
      {
        "date": "2024-05-16T09:42:00.000Z",
        "voteCount": 1,
        "content": "A because of requirement: \"no downtime in case of a data center outage in any of the zones within a GCP region\", Spanner is for multi-region presence."
      },
      {
        "date": "2024-04-19T20:47:00.000Z",
        "voteCount": 2,
        "content": "Definitely A. Google always favors managed services over services you manage.  It is a lot of work to set up and maintain your own server."
      },
      {
        "date": "2024-02-12T05:39:00.000Z",
        "voteCount": 2,
        "content": "This configuration ensures that if one zone goes down, the other can continue serving requests, thus maintaining operations without downtime during a data center outage. Windows Failover Clustering and SQL Server Always On Availability Groups are designed to support such high availability configurations, making this the best option to achieve the management's requirement."
      },
      {
        "date": "2024-06-09T04:28:00.000Z",
        "voteCount": 1,
        "content": "No, you are not correct. GCP services vs something you manage..."
      },
      {
        "date": "2024-01-21T04:07:00.000Z",
        "voteCount": 1,
        "content": "A for me"
      },
      {
        "date": "2024-01-20T14:38:00.000Z",
        "voteCount": 2,
        "content": "A: Main requirement is high availability therefore Cloud sql is the correct choice.\nhttps://cloud.google.com/blog/products/databases/managing-high-availability-with-cloud-sql-features"
      },
      {
        "date": "2024-01-14T12:10:00.000Z",
        "voteCount": 1,
        "content": "Answer is A \nhttps://www.youtube.com/watch?v=vMUpNoukwnM"
      },
      {
        "date": "2024-01-12T21:21:00.000Z",
        "voteCount": 2,
        "content": "Answer is D\nI checked with one of the engineer in G, he said that Cloud sql doesn't support MS sql directly. So A is wrong"
      },
      {
        "date": "2023-12-14T05:22:00.000Z",
        "voteCount": 1,
        "content": "The correct Answer is D, you have to select HA at config"
      },
      {
        "date": "2023-11-24T14:15:00.000Z",
        "voteCount": 2,
        "content": "While A would provide you HA through a regional persistent disk usage (see 1), D exactly corresponds to the requirements of this question (see 1). So it's D.\n\n1. https://cloud.google.com/sql/docs/features#sql_server_feature_support_for\n\n2. https://cloud.google.com/compute/docs/instances/sql-server/configure-availability"
      },
      {
        "date": "2023-11-16T19:45:00.000Z",
        "voteCount": 1,
        "content": "C or D. A can be configured only single region"
      },
      {
        "date": "2023-11-13T03:11:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/sql/docs/sqlserver/high-availability\nHigh availability feature is available in cloud SQL. \nWe dont have to create compute instance, install SQL server and place the db and log file in group of windows compute engine machines with failover clustering. Always chose readymade services from GCP."
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 61,
    "url": "https://www.examtopics.com/discussions/google/view/6330-exam-professional-cloud-architect-topic-1-question-61/",
    "body": "The development team has provided you with a Kubernetes Deployment file. You have no infrastructure yet and need to deploy the application. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse gcloud to create a Kubernetes cluster. Use Deployment Manager to create the deployment.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse gcloud to create a Kubernetes cluster. Use kubectl to create the deployment.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse kubectl to create a Kubernetes cluster. Use Deployment Manager to create the deployment.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse kubectl to create a Kubernetes cluster. Use kubectl to create the deployment."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2019-10-20T08:46:00.000Z",
        "voteCount": 54,
        "content": "It has to be B. gcloud for creating cluster and kubectl for creating deployment"
      },
      {
        "date": "2019-10-09T06:13:00.000Z",
        "voteCount": 26,
        "content": "May I ask why C is correct?\nI thought B was correct."
      },
      {
        "date": "2020-07-01T01:31:00.000Z",
        "voteCount": 4,
        "content": "yeap, gcloud command to create K8s cluster https://cloud.google.com/kubernetes-engine/docs/how-to/creating-a-cluster"
      },
      {
        "date": "2020-08-06T03:18:00.000Z",
        "voteCount": 9,
        "content": "B is ok"
      },
      {
        "date": "2020-11-01T07:03:00.000Z",
        "voteCount": 4,
        "content": "B is correct, when you create a nodes in GKE you use gcloud rather than kubectl..."
      },
      {
        "date": "2021-03-04T16:37:00.000Z",
        "voteCount": 2,
        "content": "B, gcloud to manage GKE and to manage pods use kubctl."
      },
      {
        "date": "2023-12-19T03:13:00.000Z",
        "voteCount": 1,
        "content": "Answer is B %100"
      },
      {
        "date": "2023-01-31T00:58:00.000Z",
        "voteCount": 2,
        "content": "Create a Google Kubernetes Engine (GKE) cluster: You can use the Google Cloud Console or the gcloud command-line tool to create a GKE cluster, which will provide the underlying infrastructure for running your application.\n\nDeploy the application to the cluster: You can use the kubectl command-line tool to apply the Kubernetes Deployment file provided by the development team to the cluster.kubectl apply -f deployment.yaml"
      },
      {
        "date": "2022-11-07T00:58:00.000Z",
        "voteCount": 1,
        "content": "B is ok"
      },
      {
        "date": "2022-10-22T18:14:00.000Z",
        "voteCount": 1,
        "content": "is the correct answer https://cloud.google.com/kubernetes-engine/docs/how-to/creating-a-cluster"
      },
      {
        "date": "2022-10-22T18:14:00.000Z",
        "voteCount": 1,
        "content": "B is the correct answer cluster https://cloud.google.com/kubernetes-engine/docs/how-to/creating-a-cluster"
      },
      {
        "date": "2022-10-16T11:05:00.000Z",
        "voteCount": 1,
        "content": "B is right"
      },
      {
        "date": "2022-03-12T05:30:00.000Z",
        "voteCount": 2,
        "content": "Kubctle comes live only when cluster has been created in the cloud console using cloud command"
      },
      {
        "date": "2022-01-31T04:29:00.000Z",
        "voteCount": 8,
        "content": "Deployment Manager is used to automate the process of provisioning infrastructure. Therefore, gcloud and Deployment Manager do the same thing. Meanwhile, kubectl is used to run commands against an already created cluster."
      },
      {
        "date": "2021-12-06T15:55:00.000Z",
        "voteCount": 2,
        "content": "Go for B.\ngcloud for create clusters.\nkubectl is used when the cluster already has been created. For example to create deployments.\nKubectl has configured a config file where is specified the default cluster."
      },
      {
        "date": "2021-11-27T09:33:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2021-11-03T02:51:00.000Z",
        "voteCount": 1,
        "content": "Hi all may someone please share the link for the bank of questions because I cannot seem to locate them.\n\nthank you"
      },
      {
        "date": "2021-10-26T08:46:00.000Z",
        "voteCount": 1,
        "content": "B \u2013 use gcloud to create cluster, use kubectl to create a deployment.\nhttps://cloud.google.com/kubernetes-engine/docs/how-to/creating-a-zonal-cluster\nIn fact, kubectl run creates a deployment.\nhttps://cloud.google.com/kubernetes-engine/docs/tutorials/hello-app"
      },
      {
        "date": "2021-09-20T08:31:00.000Z",
        "voteCount": 3,
        "content": "Question for all , do we know if only new questions are part of the bank for new exam?  Have any of the old questions appeared on new exam?"
      },
      {
        "date": "2021-09-19T14:45:00.000Z",
        "voteCount": 1,
        "content": "B is corrent"
      },
      {
        "date": "2021-09-19T10:14:00.000Z",
        "voteCount": 3,
        "content": "Why not A ?"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 62,
    "url": "https://www.examtopics.com/discussions/google/view/8373-exam-professional-cloud-architect-topic-1-question-62/",
    "body": "You need to evaluate your team readiness for a new GCP project. You must perform the evaluation and create a skills gap plan which incorporates the business goal of cost optimization. Your team has deployed two GCP projects successfully to date. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAllocate budget for team training. Set a deadline for the new GCP project.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAllocate budget for team training. Create a roadmap for your team to achieve Google Cloud certification based on job role.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAllocate budget to hire skilled external consultants. Set a deadline for the new GCP project.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAllocate budget to hire skilled external consultants. Create a roadmap for your team to achieve Google Cloud certification based on job role."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 41,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 8,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2019-11-16T17:33:00.000Z",
        "voteCount": 46,
        "content": "B is correct."
      },
      {
        "date": "2020-11-01T07:04:00.000Z",
        "voteCount": 1,
        "content": "Yes it is"
      },
      {
        "date": "2019-12-18T03:46:00.000Z",
        "voteCount": 4,
        "content": "I would agree with you because the question says create a skills gap plan"
      },
      {
        "date": "2021-03-04T16:39:00.000Z",
        "voteCount": 10,
        "content": "B, looks like cooked up question. Not gonna show up on actual test. Even if it does show up, its not market."
      },
      {
        "date": "2021-09-24T05:37:00.000Z",
        "voteCount": 1,
        "content": "exactly"
      },
      {
        "date": "2020-02-18T17:33:00.000Z",
        "voteCount": 20,
        "content": "I think it's B. \"You must perform the evaluation and create a skills gap plan incorporates the business goal of cost optimization.\" The goal is to cost optimize - they might have deployed 2 projects but are they cost optimized? I think the only way to evaluate the skills gap in cost optimization is to make them get certified and use the results to determine cost optimization skills gap. Quickly pushing another project deadline would not help with cost optimization."
      },
      {
        "date": "2020-02-20T13:07:00.000Z",
        "voteCount": 3,
        "content": "Agreed. How is setting up a GCP project deadline helping towards skill gap and cost optimization."
      },
      {
        "date": "2024-09-05T01:54:00.000Z",
        "voteCount": 1,
        "content": "Explanation:\n\nTraining and certification based on specific job roles (e.g., Cloud Architect, Data Engineer) will ensure your team has the necessary skills for the new GCP project and will help align their capabilities with cost optimization strategies.\nSince your team has already successfully deployed two GCP projects, upskilling them with targeted training is a more cost-effective and sustainable solution than relying on external consultants.\nThis option balances both team readiness and the business goal of cost optimization, while building long-term internal expertise."
      },
      {
        "date": "2024-08-06T23:44:00.000Z",
        "voteCount": 1,
        "content": "b is obvious..u need to address the skill gap"
      },
      {
        "date": "2024-07-02T03:14:00.000Z",
        "voteCount": 1,
        "content": "B is correct."
      },
      {
        "date": "2024-05-29T12:35:00.000Z",
        "voteCount": 1,
        "content": "B makes the most sense. If readiness is the goal, it makes sense to invest in formation"
      },
      {
        "date": "2024-04-22T14:28:00.000Z",
        "voteCount": 1,
        "content": "C and D excluded because the teams have successfully deployed 2 GCP project to date, seems best option is to train them"
      },
      {
        "date": "2024-01-20T14:42:00.000Z",
        "voteCount": 1,
        "content": "B: this is logical answer. Setting project deadline or getting external consultants is not a good planning option."
      },
      {
        "date": "2023-10-09T11:14:00.000Z",
        "voteCount": 1,
        "content": "Answer A is more realistic. Certification is important, but not to be in a question in the exam."
      },
      {
        "date": "2023-10-05T19:19:00.000Z",
        "voteCount": 2,
        "content": "You need to evaluate your team readiness for a new GCP project. You must perform the evaluation and create a skills gap plan which incorporates the business goal of cost optimization.\n\nThe business Goal (Long Term Goal) is cost optimization, hence investment will be needed. Option A says setting up deadlines, it will not help in cost optimization. But giving training and obtaining certification will do."
      },
      {
        "date": "2023-09-24T13:55:00.000Z",
        "voteCount": 2,
        "content": "In the short run, A seems like the best option because this would enable a quicker transition to the cloud which is the business Goal of cost optimization. In terms of cost, certification will cost the organisation more.\n\nIn the long run, B is the right option I would recommend as an architect. This would reduce the skill gap, increase proficiency and ensure repeatability. The organisation will incur more but will reap the reward... Except they have a bad culture and the staff resign right after getting the certification. Lol."
      },
      {
        "date": "2023-09-08T00:30:00.000Z",
        "voteCount": 2,
        "content": "B, better to train your team than hire, and setting a deadline would defeat the purpose of evaluating the team's readiness."
      },
      {
        "date": "2023-06-20T06:39:00.000Z",
        "voteCount": 2,
        "content": "B is a better answer"
      },
      {
        "date": "2023-05-16T18:19:00.000Z",
        "voteCount": 1,
        "content": "GCP partnership need 3 project"
      },
      {
        "date": "2023-05-15T07:09:00.000Z",
        "voteCount": 2,
        "content": "B is the right answer"
      },
      {
        "date": "2023-04-02T03:15:00.000Z",
        "voteCount": 1,
        "content": "It shows more like a trick than a really question.\nFurthermore are this question relevant for certification?"
      },
      {
        "date": "2023-03-13T09:02:00.000Z",
        "voteCount": 3,
        "content": "You are in a google exam. Always choose certification for your teams."
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 63,
    "url": "https://www.examtopics.com/discussions/google/view/68714-exam-professional-cloud-architect-topic-1-question-63/",
    "body": "You are designing an application for use only during business hours. For the minimum viable product release, you'd like to use a managed product that automatically `scales to zero` so you don't incur costs when there is no activity.<br>Which primary compute resource should you choose?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Functions\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCompute Engine",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGoogle Kubernetes Engine",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAppEngine flexible environment"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 34,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-09-13T14:50:00.000Z",
        "voteCount": 24,
        "content": "A. Cloud Functions - managed service scales down to 0\nB. Compute Engine - not a managed service\nC. Google Kubernetes Engine - not a managed service and wont scale down to 0\nD. AppEngine flexible environment - managed service but wont scale down to 0"
      },
      {
        "date": "2022-10-16T11:03:00.000Z",
        "voteCount": 3,
        "content": "Agree with A"
      },
      {
        "date": "2024-02-26T02:48:00.000Z",
        "voteCount": 3,
        "content": "GKE is a managed service."
      },
      {
        "date": "2022-01-01T15:31:00.000Z",
        "voteCount": 10,
        "content": "A. Cloud Functions"
      },
      {
        "date": "2022-02-14T04:50:00.000Z",
        "voteCount": 5,
        "content": "Cloud function is more for event driven computing. We surely need k8s or app engine. Flex always have 1 instance running. So GKE should be the option"
      },
      {
        "date": "2022-05-07T09:27:00.000Z",
        "voteCount": 1,
        "content": "from the doc :https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-autoscaler\n\nNote: If you specify a minimum of zero nodes, an idle node pool can scale down completely. However, at least one node must always be available in the cluster to run system Pods."
      },
      {
        "date": "2022-08-24T11:42:00.000Z",
        "voteCount": 1,
        "content": "But no cost for System/Control nodes"
      },
      {
        "date": "2024-07-25T14:45:00.000Z",
        "voteCount": 3,
        "content": "The only correct answer here is cloud run which isn't listed as an option.\n(A) can scale to zero, but is mean't for event driven workloads,not full sized applications, especially in the routing department\n(B) Needs manual stoppage, managed groups also keep one instance alive always\n(C) You're billed for the control plane even if the node pools are empty\n(D) Flex can't scale down to zero currently"
      },
      {
        "date": "2023-10-29T21:15:00.000Z",
        "voteCount": 1,
        "content": "This question is confused. It's seems that the answer is cloud function because is the only platform that can scale 0 natively. If you want to scale to zero k8s, you have to create a solution based in scheduler, function and pub sub. Compute Engine is not a managed service, and app engine flex doesn't scale to 0. I would go to A but it's not clear."
      },
      {
        "date": "2023-05-13T11:32:00.000Z",
        "voteCount": 1,
        "content": "agree A"
      },
      {
        "date": "2023-04-02T03:23:00.000Z",
        "voteCount": 3,
        "content": "Selected answer: D\nI choosed D, it appears that we do not. have a right answer here. \nA. Cloud Functions -  its more for event driven computing, not for full application\nB. Compute Engine - not a managed service\nC. Google Kubernetes Engine - not a managed service and wont scale down to 0\nD. AppEngine flexible environment - Only Standard App Engine can scale to 0."
      },
      {
        "date": "2023-04-27T07:19:00.000Z",
        "voteCount": 3,
        "content": "yes, but only Standard environment, not flexible environment"
      },
      {
        "date": "2023-03-26T08:27:00.000Z",
        "voteCount": 1,
        "content": "For an application that is only used during business hours and needs to scale to zero during periods of inactivity to minimize costs, a good choice would be a Function-as-a-Service (FaaS) product like AWS Lambda or Google Cloud Functions."
      },
      {
        "date": "2023-03-20T14:37:00.000Z",
        "voteCount": 1,
        "content": "A a cloud function is not an application\n\nB compute engine via MIG you can use an autoscaler with a schedule.\nhttps://cloud.google.com/compute/docs/autoscaler/scaling-schedules\nYou then can go from 0 to more instance when required\n\nC K8s is two complex for this. \nYou can have an autoscaler for the cluster in order to get the node number to 0, but it require the node to have no pods running. So you have to configure your deployments and all your workload to scale to 0 too.\nOther interference will be pod affinity, anti-affinity, disruption budget or unmanaged pod preventing pod eviction from node. But if the pod is not evicted, the node cannot be deleted.\n\"autoscaler respects scheduling and eviction rules set on Pods. These restrictions can prevent a node from being deleted by the autoscaler. \" =&gt; https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-autoscaler#scheduling-and-disruption\n\nD flexible cannot scale to 0"
      },
      {
        "date": "2023-10-08T20:59:00.000Z",
        "voteCount": 1,
        "content": "App Engine is not an application on its own either, nor B or C options. B and C are not managed products; \"managed\" means that all of the infrastructure work (such as setting up an autoscheduler or autoscaler) will be managed by Google and not by the user. Using an IaaS solution for a managed use case is already contradictory. \n\nApp Engine Flex will always have at least 1 instance running. \n\nWhile Cloud Functions is not an application itself, it's the only resource that can scale to 0. Plus, the requirement states that they will have an application running during business hours, but they never mentioned that the app will only be running in GCP or in Cloud Functions. They could be running their app anywhere else, and only call Cloud Functions when needed."
      },
      {
        "date": "2023-12-13T19:24:00.000Z",
        "voteCount": 1,
        "content": "That is not the concept of managed: https://cloud.google.com/blog/topics/developers-practitioners/serverless-vs-fully-managed-whats-difference"
      },
      {
        "date": "2023-03-07T08:49:00.000Z",
        "voteCount": 2,
        "content": "A. Cloud Functions"
      },
      {
        "date": "2023-01-08T01:10:00.000Z",
        "voteCount": 3,
        "content": "Why not D? App Engines also can scale down to zero when there is no activity. https://cloud.google.com/appengine/docs/the-appengine-environments#:~:text=Intended%20to%20run%20for%20free,when%20there%20is%20no%20traffic. Intended to run for free or at very low cost, where you pay only for what you need and when you need it. For example, your application can scale to 0 instances when there is no traffic."
      },
      {
        "date": "2023-01-26T16:55:00.000Z",
        "voteCount": 4,
        "content": "There are 2 mode for App engines, standard and flexible.\n\n\nThe standard environment can scale from zero instances up to thousands very quickly. In contrast, the flexible environment must have at least one instance running for each active version and can take longer to scale out in response to traffic."
      },
      {
        "date": "2022-12-24T10:51:00.000Z",
        "voteCount": 2,
        "content": "the only to scale to 0 is A"
      },
      {
        "date": "2022-12-21T06:58:00.000Z",
        "voteCount": 2,
        "content": "The correct answer is A. Cloud Functions.\n\nCloud Functions is a serverless compute service that lets you run code without provisioning or managing infrastructure. One of the key benefits of using Cloud Functions is that it automatically scales to meet the demands of your workload and automatically scales down to zero when there is no activity. This means that you only pay for the compute resources that you consume, which can help to reduce costs when your application is not in use. Additionally, Cloud Functions is easy to use and allows you to deploy your code with minimal effort, making it a good choice for a minimum viable product release."
      },
      {
        "date": "2022-08-09T04:20:00.000Z",
        "voteCount": 2,
        "content": "vote A\nthis is easy one. key word: managed product, scales to zero\nscale to zero: app engine standard, cloud function"
      },
      {
        "date": "2022-07-12T01:33:00.000Z",
        "voteCount": 1,
        "content": "Answer is B"
      },
      {
        "date": "2022-06-18T02:19:00.000Z",
        "voteCount": 2,
        "content": "Cloud Functions can scale to zero when not in use"
      },
      {
        "date": "2022-01-19T14:53:00.000Z",
        "voteCount": 4,
        "content": "Answer is A, As Option D App engine flexible can have minimum 1 instance active."
      },
      {
        "date": "2022-01-19T14:52:00.000Z",
        "voteCount": 1,
        "content": "Answer is A,  As Option D App engine flexible can have minimum 1 instance active."
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 64,
    "url": "https://www.examtopics.com/discussions/google/view/7290-exam-professional-cloud-architect-topic-1-question-64/",
    "body": "You are creating an App Engine application that uses Cloud Datastore as its persistence layer. You need to retrieve several root entities for which you have the identifiers. You want to minimize the overhead in operations performed by Cloud Datastore. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate the Key object for each Entity and run a batch get operation\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate the Key object for each Entity and run multiple get operations, one operation for each entity",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the identifiers to create a query filter and run a batch query operation",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the identifiers to create a query filter and run multiple query operations, one operation for each entity"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-06-15T04:07:00.000Z",
        "voteCount": 47,
        "content": "Correct Answer: A\nCreate the Key object for each Entity and run a batch get operation\nhttps://cloud.google.com/datastore/docs/best-practices \nUse batch operations for your reads, writes, and deletes instead of single operations. Batch operations are more efficient because they perform multiple operations with the same overhead as a single operation.\nFirestore in Datastore mode supports batch versions of the operations which allow it to operate on multiple objects in a single Datastore mode call.\nSuch batch calls are faster than making separate calls for each individual entity because they incur the overhead for only one service call. If multiple entity groups are involved, the work for all the groups is performed in parallel on the server side."
      },
      {
        "date": "2022-10-16T11:02:00.000Z",
        "voteCount": 2,
        "content": "works fine .. A is right"
      },
      {
        "date": "2020-01-12T06:24:00.000Z",
        "voteCount": 7,
        "content": "Agree A"
      },
      {
        "date": "2024-05-27T22:21:00.000Z",
        "voteCount": 4,
        "content": "Keep in mind that datastore is discontinued, Firestore being the recommended alternative."
      },
      {
        "date": "2024-01-14T14:58:00.000Z",
        "voteCount": 3,
        "content": "According to \n\"A. Create the Key object for each Entity and run a batch get operation\"\nwhich is wrong as the key is already created for each entity whenever it's persisted.\n\nI believe the correct answer is C, -- to use a bulk query (a.k.a. \"batch\" in their terms).\nYou need a query with a criteria anyway to find a resultset, and not just a fetch by \"get\" operation to load by surrogate keys."
      },
      {
        "date": "2023-01-31T01:31:00.000Z",
        "voteCount": 3,
        "content": "By using the \"lookup by key\" API of Cloud Datastore, you can minimize the overhead in operations performed by Cloud Datastore and optimize the performance of your App Engine application.\nfrom google.cloud import datastore\n\nclient = datastore.Client()\nkeys = [client.key('EntityKind', id) for id in entity_ids]\nentities = client.get_multi(keys)"
      },
      {
        "date": "2022-12-21T07:18:00.000Z",
        "voteCount": 6,
        "content": "A. Create the Key object for each Entity and run a batch get operation\n\nTo minimize the overhead in operations performed by Cloud Datastore, you should use the batch get operation to retrieve multiple entities in a single API call. To do this, you should create a Key object for each entity that you want to retrieve, then pass the Key objects to the batch get operation. This will allow you to retrieve multiple entities in a single API call, reducing the number of operations performed by Cloud Datastore and improving the efficiency of your application."
      },
      {
        "date": "2022-12-21T07:18:00.000Z",
        "voteCount": 5,
        "content": "Option B, running multiple get operations, one operation for each entity, would not be an efficient way to retrieve the entities because it would require multiple API calls to Cloud Datastore, which would increase the overhead and decrease the efficiency of the application.\n\nOption C, using the identifiers to create a query filter and running a batch query operation, would not be an efficient way to retrieve the entities because it would require performing a query operation, which is generally more expensive than a get operation.\n\nOption D, using the identifiers to create a query filter and running multiple query operations, one operation for each entity, would not be an efficient way to retrieve the entities because it would require performing multiple query operations, which are generally more expensive than get operations."
      },
      {
        "date": "2022-11-07T01:05:00.000Z",
        "voteCount": 1,
        "content": "A is ok"
      },
      {
        "date": "2022-10-22T18:35:00.000Z",
        "voteCount": 2,
        "content": "A is correct https://cloud.google.com/datastore/docs/best-practices#api_calls"
      },
      {
        "date": "2022-08-18T10:25:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/datastore/docs/concepts/entities#datastore-datastore-batch-lookup-python"
      },
      {
        "date": "2021-12-06T16:09:00.000Z",
        "voteCount": 2,
        "content": "go for A."
      },
      {
        "date": "2021-11-27T09:56:00.000Z",
        "voteCount": 2,
        "content": "A is the right answer"
      },
      {
        "date": "2021-11-24T06:37:00.000Z",
        "voteCount": 2,
        "content": "vote A"
      },
      {
        "date": "2021-10-26T09:02:00.000Z",
        "voteCount": 1,
        "content": "A \u2013 create a key object for each entity, and run a batch get operations.\nSee Batch Operations section here: https://cloud.google.com/datastore/docs/concepts/entities\nvar keys = new Key[] { _keyFactory.CreateKey(1), _keyFactory.CreateKey(2) };\nvar tasks = _db.Lookup(keys[0], keys[1]);\n\n1 and 2 are identifiers of the Key. Check Key / Identifier definition on the same link (top of that page)\nSuch batch calls are faster than making separate calls for each individual entity because they incur the overhead for only one service call."
      },
      {
        "date": "2021-05-19T04:43:00.000Z",
        "voteCount": 1,
        "content": "A. Create the Key object for each Entity and run a batch get operation"
      },
      {
        "date": "2021-05-13T04:39:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2021-03-31T23:36:00.000Z",
        "voteCount": 1,
        "content": "Answer is A"
      },
      {
        "date": "2021-03-31T23:35:00.000Z",
        "voteCount": 1,
        "content": "Answer is A"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 65,
    "url": "https://www.examtopics.com/discussions/google/view/6308-exam-professional-cloud-architect-topic-1-question-65/",
    "body": "You need to upload files from your on-premises environment to Cloud Storage. You want the files to be encrypted on Cloud Storage using customer-supplied encryption keys. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSupply the encryption key in a .boto configuration file. Use gsutil to upload the files.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSupply the encryption key using gcloud config. Use gsutil to upload the files to that bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse gsutil to upload the files, and use the flag --encryption-key to supply the encryption key.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse gsutil to create a bucket, and use the flag --encryption-key to supply the encryption key. Use gsutil to upload the files to that bucket."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 33,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 29,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2019-10-08T22:51:00.000Z",
        "voteCount": 46,
        "content": "In GCP document, key could be configured in .boto.\nI didn't find information show gsutil suppots flag \"--encryption-key\".\n\nhttps://cloud.google.com/storage/docs/encryption/customer-supplied-keys"
      },
      {
        "date": "2024-06-12T00:53:00.000Z",
        "voteCount": 10,
        "content": "The documentation is here:\nhttps://cloud.google.com/storage/docs/encryption/using-customer-supplied-keys#upload-encrypt\n\nOption C is correct. You can upload a file using customer-supplied encryption with the command:\ngcloud storage cp SOURCE_DATA gs://BUCKET_NAME/OBJECT_NAME --encryption-key=YOUR_ENCRYPTION_KEY"
      },
      {
        "date": "2020-08-06T04:05:00.000Z",
        "voteCount": 16,
        "content": "A is ok"
      },
      {
        "date": "2021-03-04T16:45:00.000Z",
        "voteCount": 4,
        "content": "A is correct"
      },
      {
        "date": "2020-11-01T07:10:00.000Z",
        "voteCount": 3,
        "content": ".boto file with encryption key, but it will works for individual users, every user should update their own .boto with same key. Also while retrieving you should use the same key to decryption."
      },
      {
        "date": "2019-10-26T10:08:00.000Z",
        "voteCount": 18,
        "content": "I agree, A.(https://cloud.google.com/storage/docs/gsutil/addlhelp/UsingEncryptionKeys#generating-customer-supplied-encryption-keys)"
      },
      {
        "date": "2024-10-01T22:54:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2024-10-01T06:55:00.000Z",
        "voteCount": 2,
        "content": "Option C is correct."
      },
      {
        "date": "2024-09-30T08:55:00.000Z",
        "voteCount": 2,
        "content": "Straight for the docs: https://cloud.google.com/storage/docs/encryption/using-customer-supplied-keys#upload-encrypt"
      },
      {
        "date": "2024-09-27T06:36:00.000Z",
        "voteCount": 3,
        "content": "Option C is correct\nC. Use gsutil to upload the files, and use the flag --encryption-key to supply the encryption key.\n\ngsutil -o \"GSUtil:encryption_key=YOUR_BASE64_ENCRYPTION_KEY\" cp your_file.txt gs://your-bucket/"
      },
      {
        "date": "2024-09-09T01:16:00.000Z",
        "voteCount": 3,
        "content": "When using customer-supplied encryption keys (CSEK) in Google Cloud Storage, you can provide the encryption key directly in your gsutil command during the upload operation. The --encryption-key flag allows you to specify the encryption key for encrypting the files as they are uploaded."
      },
      {
        "date": "2024-09-07T07:15:00.000Z",
        "voteCount": 2,
        "content": "The correct answer is C. Use gsutil to upload the files, and use the flag --encryption-key to supply the encryption key.\n\nHere\u2019s why:\n\nTo encrypt files with a customer-supplied encryption key (CSEK), you can use the gsutil command along with the --encryption-key flag to specify the encryption key when uploading files to Cloud Storage.\nThis allows each file to be encrypted using your specified encryption key, providing an additional layer of security beyond Google-managed encryption.\nThe other options are incorrect:\n\nA and B reference .boto configuration files and gcloud config, but those methods are not used to specify customer-supplied encryption keys for file uploads.\nD incorrectly suggests using --encryption-key when creating a bucket, but encryption keys are supplied during file uploads, not during bucket creation.\nThus, C is the correct option to upload files with customer-supplied encryption keys using gsutil."
      },
      {
        "date": "2024-08-08T20:45:00.000Z",
        "voteCount": 1,
        "content": "Choose A"
      },
      {
        "date": "2024-07-16T00:28:00.000Z",
        "voteCount": 1,
        "content": "A is OK"
      },
      {
        "date": "2024-07-02T08:22:00.000Z",
        "voteCount": 3,
        "content": "The .boto file was indeed used for configuration in older versions of Google Cloud Storage, particularly with the gsutil tool. However, this method is now considered legacy and is not recommended for modern Google Cloud configurations.\n\nOption C is correct."
      },
      {
        "date": "2024-06-07T11:23:00.000Z",
        "voteCount": 3,
        "content": "A is correct. Link here: https://www.cloudskillsboost.google/focuses/19181?parent=catalog"
      },
      {
        "date": "2024-05-30T21:35:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/storage/docs/encryption/using-customer-supplied-keys#command-line"
      },
      {
        "date": "2024-04-23T17:40:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/storage/docs/encryption/using-customer-supplied-keys#upload-encrypt"
      },
      {
        "date": "2024-02-11T06:21:00.000Z",
        "voteCount": 2,
        "content": "C - https://cloud.google.com/storage/docs/encryption/using-customer-supplied-keys#console"
      },
      {
        "date": "2024-01-14T15:21:00.000Z",
        "voteCount": 1,
        "content": "D - Correct\ngsutil mb -k &lt; Key &gt; gs://xx\ngsutil -m cp -r xx gs://xx"
      },
      {
        "date": "2023-12-06T19:17:00.000Z",
        "voteCount": 1,
        "content": "A is ok"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 66,
    "url": "https://www.examtopics.com/discussions/google/view/9222-exam-professional-cloud-architect-topic-1-question-66/",
    "body": "Your customer wants to capture multiple GBs of aggregate real-time key performance indicators (KPIs) from their game servers running on Google Cloud Platform and monitor the KPIs with low latency. How should they capture the KPIs?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore time-series data from the game servers in Google Bigtable, and view it using Google Data Studio.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOutput custom metrics to Stackdriver from the game servers, and create a Dashboard in Stackdriver Monitoring Console to view them.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSchedule BigQuery load jobs to ingest analytics files uploaded to Cloud Storage every ten minutes, and visualize the results in Google Data Studio.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInsert the KPIs into Cloud Datastore entities, and run ad hoc analysis and visualizations of them in Cloud Datalab."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 29,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2019-12-20T14:47:00.000Z",
        "voteCount": 33,
        "content": "Ans is B. Data studio cannot be used with BigTable\nhttps://datastudio.google.com/datahttps://datastudio.google.com/data"
      },
      {
        "date": "2021-09-17T22:17:00.000Z",
        "voteCount": 4,
        "content": "A is correct"
      },
      {
        "date": "2022-11-07T09:39:00.000Z",
        "voteCount": 7,
        "content": "As of today you can"
      },
      {
        "date": "2022-12-15T10:43:00.000Z",
        "voteCount": 2,
        "content": "Source?\n\nhttps://lookerstudio.google.com/data?search=big"
      },
      {
        "date": "2023-03-27T06:49:00.000Z",
        "voteCount": 2,
        "content": "That\u2019s BigQuery, not BigTable, no?"
      },
      {
        "date": "2023-10-24T20:06:00.000Z",
        "voteCount": 2,
        "content": "Looker is not the same as Data Studio. I know it came to replace it, but these questions are kind of old, so unless it clearly says \"looker\", I wouldn't take both as the same."
      },
      {
        "date": "2019-12-07T09:29:00.000Z",
        "voteCount": 12,
        "content": "correct is B"
      },
      {
        "date": "2024-01-14T15:31:00.000Z",
        "voteCount": 1,
        "content": "B - Correct\nA - It is not a real-time solution"
      },
      {
        "date": "2024-02-19T04:39:00.000Z",
        "voteCount": 1,
        "content": "Why is A not a real time solution? https://cloud.google.com/bigtable/docs/integrations#opentsdb \n\nit says: OpenTSDB is a time-series database that can use Bigtable for storage. Monitoring time-series data with OpenTSDB on Bigtable and GKE shows how to use OpenTSDB to collect, record, and monitor time-series data on Google Cloud. The OpenTSDB documentation provides additional information to help you get started."
      },
      {
        "date": "2023-10-05T19:48:00.000Z",
        "voteCount": 1,
        "content": "BigTable doesn't integrate with Data Studio\n\nhttps://cloud.google.com/bigtable/docs/integrations"
      },
      {
        "date": "2023-05-13T11:37:00.000Z",
        "voteCount": 1,
        "content": "B is correct, you should create custom KPI in Stack Driver"
      },
      {
        "date": "2022-12-21T22:51:00.000Z",
        "voteCount": 9,
        "content": "B. Output custom metrics to Stackdriver from the game servers, and create a Dashboard in Stackdriver Monitoring Console to view them.\n\nTo capture multiple GBs of aggregate real-time KPIs from game servers running on Google Cloud Platform and monitor them with low latency, the customer should output custom metrics to Stackdriver from the game servers. Stackdriver allows you to collect and store custom metrics, as well as view and analyze them in real-time using the Stackdriver Monitoring Console. The customer can create a Dashboard in the Monitoring Console to view the KPIs and monitor them with low latency."
      },
      {
        "date": "2022-12-21T22:51:00.000Z",
        "voteCount": 9,
        "content": "Option A, storing time-series data in Bigtable and viewing it using Data Studio, would not be suitable for capturing and monitoring real-time KPIs with low latency. Bigtable is a scalable NoSQL database that is optimized for large-scale batch processing, and Data Studio is a visualization tool that is not designed for real-time data analysis.\n\nOption C, scheduling BigQuery load jobs to ingest analytics files uploaded to Cloud Storage every ten minutes and visualizing the results in Data Studio, would not be suitable for capturing and monitoring real-time KPIs with low latency. BigQuery is a data warehouse that is optimized for batch processing, and it is not designed for real-time data analysis.\n\nOption D, inserting the KPIs into Cloud Datastore entities and running ad hoc analysis and visualizations of them in Cloud Datalab, would not be suitable for capturing and monitoring real-time KPIs with low latency. Cloud Datastore is a NoSQL document database, and Cloud Datalab is a data analysis and visualization tool that is not designed for real-time data analysis."
      },
      {
        "date": "2023-03-20T14:50:00.000Z",
        "voteCount": 4,
        "content": "big table is not for batch. It is used in IOT...\nhttps://cloud.google.com/bigtable"
      },
      {
        "date": "2022-11-15T17:04:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2022-11-09T09:19:00.000Z",
        "voteCount": 1,
        "content": "B is ok"
      },
      {
        "date": "2022-10-22T18:57:00.000Z",
        "voteCount": 4,
        "content": "B is correct as Data studio does not support bigtable as a source"
      },
      {
        "date": "2022-10-17T00:20:00.000Z",
        "voteCount": 2,
        "content": "KPI, SLO,SLI all those work with observability which stackdriver"
      },
      {
        "date": "2022-10-15T07:03:00.000Z",
        "voteCount": 1,
        "content": "The reference provided seems irrelevant to this question."
      },
      {
        "date": "2022-06-07T08:44:00.000Z",
        "voteCount": 1,
        "content": "A is the correct answer, the key word here, real time and low latency."
      },
      {
        "date": "2022-05-08T21:41:00.000Z",
        "voteCount": 6,
        "content": "BigTable has no connection to data studio. \nhttps://datastudio.google.com/data?search=Big"
      },
      {
        "date": "2022-01-22T06:13:00.000Z",
        "voteCount": 2,
        "content": "B, it is"
      },
      {
        "date": "2022-01-04T00:54:00.000Z",
        "voteCount": 2,
        "content": "I don't think there is a correct answer, but B looks correct in this.\nIf use Bigquery, then A is correct .\nC is not for realtime.\nD Datastore is for small usecase.\nKeywords 'real time' ,'analytics' \nhttps://events.withgoogle.com/solution-design-pattern-gaming/analytics-pattern/"
      },
      {
        "date": "2021-12-28T03:03:00.000Z",
        "voteCount": 1,
        "content": "B is okay"
      },
      {
        "date": "2021-12-13T13:05:00.000Z",
        "voteCount": 1,
        "content": "I will go with B because you can meet the requirement with Cloud Monitoring, formerly Stackdriver monitoring"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 67,
    "url": "https://www.examtopics.com/discussions/google/view/6890-exam-professional-cloud-architect-topic-1-question-67/",
    "body": "You have a Python web application with many dependencies that requires 0.1 CPU cores and 128 MB of memory to operate in production. You want to monitor and maximize machine utilization. You also want to reliably deploy new versions of the application. Which set of steps should you take?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPerform the following: 1. Create a managed instance group with f1-micro type machines. 2. Use a startup script to clone the repository, check out the production branch, install the dependencies, and start the Python app. 3. Restart the instances to automatically deploy new production releases.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPerform the following: 1. Create a managed instance group with n1-standard-1 type machines. 2. Build a Compute Engine image from the production branch that contains all of the dependencies and automatically starts the Python app. 3. Rebuild the Compute Engine image, and update the instance template to deploy new production releases.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPerform the following: 1. Create a Google Kubernetes Engine (GKE) cluster with n1-standard-1 type machines. 2. Build a Docker image from the production branch with all of the dependencies, and tag it with the version number. 3. Create a Kubernetes Deployment with the imagePullPolicy set to 'IfNotPresent' in the staging namespace, and then promote it to the production namespace after testing.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPerform the following: 1. Create a GKE cluster with n1-standard-4 type machines. 2. Build a Docker image from the master branch with all of the dependencies, and tag it with 'latest'. 3. Create a Kubernetes Deployment in the default namespace with the imagePullPolicy set to 'Always'. Restart the pods to automatically deploy new production releases."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 54,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 37,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 9,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2019-10-26T01:57:00.000Z",
        "voteCount": 40,
        "content": "C is correct, need \"ifnotpresent\"when uploads to container registry"
      },
      {
        "date": "2023-04-20T09:00:00.000Z",
        "voteCount": 4,
        "content": "ifnotpresent won't pull new version."
      },
      {
        "date": "2023-08-24T05:42:00.000Z",
        "voteCount": 1,
        "content": "yes i agree"
      },
      {
        "date": "2019-11-23T07:26:00.000Z",
        "voteCount": 23,
        "content": "C is the best choice. You can create a k8s cluster with just one node and use a different namespaces for staging and production. In staging, you will test the changes"
      },
      {
        "date": "2022-10-16T10:55:00.000Z",
        "voteCount": 1,
        "content": "Agreed"
      },
      {
        "date": "2024-08-24T18:12:00.000Z",
        "voteCount": 1,
        "content": "should be option C because if you are working in real world, GKE is the best solution for such a case. Furthermore, its reliable, scalable, flexible, at least the best option among the other three."
      },
      {
        "date": "2024-08-22T22:49:00.000Z",
        "voteCount": 1,
        "content": "Ngl it's A. Don't use GKE, it won't schedule the deployment as most of the resources already occupied by kube-system"
      },
      {
        "date": "2024-08-22T22:50:00.000Z",
        "voteCount": 1,
        "content": "Also you can deploy COS Containerd in a VM"
      },
      {
        "date": "2024-07-26T10:01:00.000Z",
        "voteCount": 1,
        "content": "imagePullPolicy: Always ensures that the latest version of the image is always pulled, which guarantees that the most recent code is deployed.\nRestarting pods ensures that the new version is deployed without requiring manual intervention."
      },
      {
        "date": "2024-03-01T09:58:00.000Z",
        "voteCount": 1,
        "content": "B because Kubernetes will be overkill and A is not reliable"
      },
      {
        "date": "2024-02-06T03:27:00.000Z",
        "voteCount": 1,
        "content": "A is wrong as after the restart the script will be rerun and fetch the code directly from the repo (even if production). The load of the massive number of dependencies will take a lot of timee, and the application version will be fuzzy."
      },
      {
        "date": "2024-01-26T00:57:00.000Z",
        "voteCount": 2,
        "content": "C is correct, B (instance template cannote be updated once created."
      },
      {
        "date": "2024-01-15T07:30:00.000Z",
        "voteCount": 1,
        "content": "C - Correct"
      },
      {
        "date": "2024-01-09T18:20:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is C. Because it is the only option that RELIABLY tests the app in staging before it is applied to production. Remember that one of the requirements in the question is to reliably deploy the app."
      },
      {
        "date": "2023-12-18T01:27:00.000Z",
        "voteCount": 3,
        "content": "You don't need GKE for 0.1 CPU, only A meet hte needs"
      },
      {
        "date": "2023-12-07T17:26:00.000Z",
        "voteCount": 4,
        "content": "For 0.1 CPU I will never use GKE, considering the associated cost with control plane and not even one option in the question mentioning micro instances for the node pool"
      },
      {
        "date": "2023-11-24T03:18:00.000Z",
        "voteCount": 4,
        "content": "When we read the question - \"0.1 CPU cores and 128 MB of memory\" to operate in production. You want to monitor and \"maximize machine utilization\"... Answer A should be a fit based on the question details. Would GKE for tiny application be overkill?"
      },
      {
        "date": "2023-10-12T06:51:00.000Z",
        "voteCount": 1,
        "content": "python app on compute engine is a disastrous architecture. C is the correct architecture which tests the app before putting to prod"
      },
      {
        "date": "2023-10-05T19:53:00.000Z",
        "voteCount": 7,
        "content": "You should use GKE, because your can scale up and down based on your demand. Also you can specifiy the resource size like 0.1 CPU and 128 MB of memory per Pod. \n\nSecondly, Kubernetes Deployment with the imagePullPolicy set to \u201cIfNotPresent\u201d in the staging namespace, and then promote it to production namespace after testing. is best practice."
      },
      {
        "date": "2023-11-09T14:23:00.000Z",
        "voteCount": 2,
        "content": "Ref: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/\n\nCorrect. Because we can spec the resources on our pods is why C is chosen over A (f1-micro). This is what allows us to \"maximize machine utilization\"!"
      },
      {
        "date": "2023-08-19T09:08:00.000Z",
        "voteCount": 8,
        "content": "f1 micro managed instance group fits all the requirements, with capability to run upto 2 instances per per for given requirement\nGKE cluster would be an overkill, with control plane nodes/cost not even being considered. n1-standard-1 instance would require 10 application instances/pod to be running (assuming 0 ds) and would still be leaving 75% memory unused"
      },
      {
        "date": "2023-06-06T11:53:00.000Z",
        "voteCount": 2,
        "content": "C without doubts"
      },
      {
        "date": "2023-06-27T10:45:00.000Z",
        "voteCount": 1,
        "content": "How about price for a 0.1 CPU app?"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 68,
    "url": "https://www.examtopics.com/discussions/google/view/6528-exam-professional-cloud-architect-topic-1-question-68/",
    "body": "Your company wants to start using Google Cloud resources but wants to retain their on-premises Active Directory domain controller for identity management.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Admin Directory API to authenticate against the Active Directory domain controller.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Google Cloud Directory Sync to synchronize Active Directory usernames with cloud identities and configure SAML SSO.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Identity-Aware Proxy configured to use the on-premises Active Directory domain controller as an identity provider.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Compute Engine to create an Active Directory (AD) domain controller that is a replica of the on-premises AD domain controller using Google Cloud Directory Sync."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 7,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2019-10-13T04:46:00.000Z",
        "voteCount": 44,
        "content": "According to the reference, my understanding is B is correct.\nAnd in the document(https://cloud.google.com/iap/docs/concepts-overview), it says:\nIf you need to create Google Accounts for your existing users, you can use Google Cloud Directory Sync to synchronize with your Active Directory or LDAP server.\n\nIs it possible to explain why correct answer is C?"
      },
      {
        "date": "2021-09-03T03:07:00.000Z",
        "voteCount": 5,
        "content": "It\u2019s simple. Domain controllers are not meant authenticate saas or web applications. This includes iam. Domain controllers speak ntlm and Kerberos. \nThis why we use federation. Because web apps do not speak Kerberos or ntlm. They speak languages such oauth. Hence the need for ad federation proxy\nB is correct"
      },
      {
        "date": "2021-10-13T02:07:00.000Z",
        "voteCount": 2,
        "content": "thanks for the explanation, may I ask if we go with SAML, why need sync the useraccount? seems we just need set up the federation between cloud and on-premise"
      },
      {
        "date": "2022-10-03T09:23:00.000Z",
        "voteCount": 3,
        "content": "\"...As a prerequisite for access to GCP resources, employees must have a Google identity set up...\""
      },
      {
        "date": "2020-08-06T04:22:00.000Z",
        "voteCount": 9,
        "content": "B is ok"
      },
      {
        "date": "2020-11-01T07:14:00.000Z",
        "voteCount": 5,
        "content": "B should be correct"
      },
      {
        "date": "2021-03-04T16:53:00.000Z",
        "voteCount": 5,
        "content": "B, use GCDS."
      },
      {
        "date": "2019-10-20T09:41:00.000Z",
        "voteCount": 25,
        "content": "B is the nearest answer I feel !"
      },
      {
        "date": "2024-06-03T03:35:00.000Z",
        "voteCount": 1,
        "content": "To integrate Google Cloud with your on-premises Active Directory (AD) domain controller for identity management while retaining your on-premises AD, the best approach is:\n\nB. Use Google Cloud Directory Sync to synchronize Active Directory usernames with cloud identities and configure SAML SSO."
      },
      {
        "date": "2024-05-08T22:46:00.000Z",
        "voteCount": 1,
        "content": "The most suitable option for integrating Google Cloud resources with an on-premises Active Directory domain controller for identity management is option D. This involves creating a replica of the on-premises Active Directory domain controller using Compute Engine and Google Cloud Directory Sync for synchronization."
      },
      {
        "date": "2023-05-13T11:42:00.000Z",
        "voteCount": 2,
        "content": "B is correct https://cloud.google.com/architecture/identity/federating-gcp-with-active-directory-introduction"
      },
      {
        "date": "2023-01-31T02:06:00.000Z",
        "voteCount": 2,
        "content": "Connect your on-premises Active Directory to Google Cloud: You can use Google Cloud Directory Sync (GCDS) to synchronize your on-premises Active Directory with Google Cloud. This allows you to use your existing Active Directory users and groups in Google Cloud.\n\nSet up single sign-on (SSO): You can use Google Cloud Identity-Aware Proxy (IAP) to set up SSO for your Google Cloud resources. IAP integrates with your on-premises Active Directory and allows users to log in to Google Cloud using their existing Active Directory credentials."
      },
      {
        "date": "2022-12-21T23:03:00.000Z",
        "voteCount": 7,
        "content": "B. Use Google Cloud Directory Sync to synchronize Active Directory usernames with cloud identities and configure SAML SSO.\n\nTo retain their on-premises Active Directory domain controller for identity management while using Google Cloud resources, the company can use Google Cloud Directory Sync to synchronize Active Directory usernames with cloud identities and configure SAML single sign-on (SSO). This will allow users to use their existing Active Directory credentials to access Google Cloud resources, while still maintaining their on-premises Active Directory domain controller as the primary source of identity management."
      },
      {
        "date": "2022-12-21T23:03:00.000Z",
        "voteCount": 3,
        "content": "Option A, using the Admin Directory API to authenticate against the Active Directory domain controller, would not be a suitable solution because it would require implementing custom authentication logic in the application, which would be time-consuming and error-prone.\n\nOption C, using Cloud Identity-Aware Proxy configured to use the on-premises Active Directory domain controller as an identity provider, would be a suitable solution, but it would not allow you to synchronize Active Directory usernames with cloud identities.\n\nOption D, using Compute Engine to create an Active Directory (AD) domain controller that is a replica of the on-premises AD domain controller using Google Cloud Directory Sync, would not be a suitable solution because it would require setting up and maintaining an additional AD domain controller in Google Cloud, which would be unnecessary if the company wants to retain their on-premises AD domain controller as the primary source of identity management."
      },
      {
        "date": "2022-11-27T17:37:00.000Z",
        "voteCount": 1,
        "content": "GCDS and Cloud Identity is provided exactly for this use case"
      },
      {
        "date": "2022-11-09T09:28:00.000Z",
        "voteCount": 2,
        "content": "B is ok"
      },
      {
        "date": "2022-10-22T19:28:00.000Z",
        "voteCount": 2,
        "content": "B is correct https://cloud.google.com/architecture/identity/federating-gcp-with-active-directory-introduction"
      },
      {
        "date": "2022-07-04T10:18:00.000Z",
        "voteCount": 1,
        "content": "Answer is B"
      },
      {
        "date": "2022-03-12T06:18:00.000Z",
        "voteCount": 1,
        "content": "https://support.google.com/a/answer/106368?hl=en"
      },
      {
        "date": "2021-12-07T00:08:00.000Z",
        "voteCount": 4,
        "content": "Go for B.\nCloud Directory Sync\nhttps://cloud.google.com/blog/products/identity-security/using-your-existing-identity-management-system-with-google-cloud-platform"
      },
      {
        "date": "2021-11-27T10:51:00.000Z",
        "voteCount": 1,
        "content": "B is the correct answer"
      },
      {
        "date": "2021-11-21T11:00:00.000Z",
        "voteCount": 1,
        "content": "B as AD groups are directly mapped to Cloud Directory Sync"
      },
      {
        "date": "2021-10-27T11:45:00.000Z",
        "voteCount": 2,
        "content": "B \u2013 use Google Cloud Directory Sync to sync Active Directory user names with cloud identities and configure SAML SSO.\nCheck the flowchart here illustrating integration of your existing identity management system into GCP: https://cloud.google.com/blog/products/identity-security/using-your-existing-identity-management-system-with-google-cloud-platform\nC \u2013 does not work, since Cloud IAP serves different purpose. It s a building block toward BeyondCorp, an enterprise security model that enables every employee to work from untrusted networks without the use of a VPN."
      },
      {
        "date": "2021-07-08T11:15:00.000Z",
        "voteCount": 4,
        "content": "Answer is B"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 69,
    "url": "https://www.examtopics.com/discussions/google/view/6892-exam-professional-cloud-architect-topic-1-question-69/",
    "body": "You are running a cluster on Kubernetes Engine (GKE) to serve a web application. Users are reporting that a specific part of the application is not responding anymore. You notice that all pods of your deployment keep restarting after 2 seconds. The application writes logs to standard output. You want to inspect the logs to find the cause of the issue. Which approach can you take?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReview the Stackdriver logs for each Compute Engine instance that is serving as a node in the cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReview the Stackdriver logs for the specific GKE  container that is serving the unresponsive part of the application.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConnect to the cluster using gcloud credentials and connect to a container in one of the pods to read the logs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReview the Serial Port logs for each Compute Engine instance that is serving as a node in the cluster."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2019-10-21T08:05:00.000Z",
        "voteCount": 34,
        "content": "think answer is B. C cannot be, you don't need to connect to the container to view logs, you connect to stackdriver for this"
      },
      {
        "date": "2019-10-22T08:29:00.000Z",
        "voteCount": 1,
        "content": "Is Stackdriver enabled by default?\nStackdriver Logging is independent and first needs to enable with GKE I guess?"
      },
      {
        "date": "2020-05-15T13:35:00.000Z",
        "voteCount": 1,
        "content": "Yes for GKE"
      },
      {
        "date": "2020-08-06T04:27:00.000Z",
        "voteCount": 7,
        "content": "B is ok"
      },
      {
        "date": "2019-10-24T06:42:00.000Z",
        "voteCount": 8,
        "content": "Please forget this comment ^\nAnswer B should be correct."
      },
      {
        "date": "2020-11-01T07:16:00.000Z",
        "voteCount": 3,
        "content": "Yes it is B"
      },
      {
        "date": "2019-10-24T06:41:00.000Z",
        "voteCount": 10,
        "content": "Stackdriver Logging seems to be enabled by default for GKE.\n\nLooking here:\nhttps://cloud.google.com/monitoring/kubernetes-engine/legacy-stackdriver/logging\nFor container and system logs, GKE deploys a per-node logging agent that reads container logs, adds helpful metadata, and then stores them. The logging agent checks for container logs in the following sources:\n\nStandard output and standard error logs from containerized processes\n\nI would also go with B"
      },
      {
        "date": "2022-10-16T10:52:00.000Z",
        "voteCount": 1,
        "content": "agreed with B"
      },
      {
        "date": "2021-03-04T16:56:00.000Z",
        "voteCount": 6,
        "content": "B, google wants you to use stackdriver."
      },
      {
        "date": "2019-11-14T08:51:00.000Z",
        "voteCount": 9,
        "content": "B is correct. Serial console doesnt give you StdOut"
      },
      {
        "date": "2023-10-05T20:04:00.000Z",
        "voteCount": 1,
        "content": "B. Review the Stackdriver logs for the specific GKE container that is serving the unresponsive part of the application.\n\nGKE be default integrats with Google Operation Suit (Stackdriver) and you can filter the logs for more specific part of application i.e container to view logs. Also it is most efficient way of investigation."
      },
      {
        "date": "2023-08-15T07:24:00.000Z",
        "voteCount": 1,
        "content": "B is the simples option and more effective"
      },
      {
        "date": "2023-03-08T16:35:00.000Z",
        "voteCount": 1,
        "content": "B. Review the Stackdriver logs for the specific GKE container that is serving the unresponsive part of the application.\n\nSince the application writes logs to standard output, the logs should be available in the Stackdriver logs for the container running the unresponsive part of the application. Kubernetes Engine automatically exports these logs to Stackdriver, so you can use the Stackdriver Logging console to view the logs. Option A is not the best choice because reviewing the logs for each Compute Engine instance would be time-consuming and may not provide the necessary information. Option C may work, but it involves extra steps and may not be necessary if the logs are available in Stackdriver. Option D is not relevant in this case because Serial Port logs are not likely to provide useful information for troubleshooting an unresponsive web application."
      },
      {
        "date": "2022-12-21T23:09:00.000Z",
        "voteCount": 2,
        "content": "C. Connect to the cluster using gcloud credentials and connect to a container in one of the pods to read the logs.\n\nTo inspect the logs of a Kubernetes Engine (GKE) cluster to find the cause of an issue, you can connect to the cluster using gcloud credentials and connect to a container in one of the pods to read the logs. This will allow you to access the logs of the application as it is running in the cluster, which should help you identify the cause of the issue."
      },
      {
        "date": "2022-12-21T23:09:00.000Z",
        "voteCount": 3,
        "content": "Option A, reviewing the Stackdriver logs for each Compute Engine instance that is serving as a node in the cluster, would not be suitable because the application writes logs to standard output, not to Stackdriver.\n\nOption B, reviewing the Stackdriver logs for the specific GKE container that is serving the unresponsive part of the application, would not be suitable because the application writes logs to standard output, not to Stackdriver.\n\nOption D, reviewing the Serial Port logs for each Compute Engine instance that is serving as a node in the cluster, would not be suitable because the application writes logs to standard output, not to the Serial Port."
      },
      {
        "date": "2022-11-28T19:24:00.000Z",
        "voteCount": 1,
        "content": "This should be easy, the answer is B. \nJust eliminate the wrong answers (A) is not correct because the question is about GKE and not CE.\nC and D are totally lost"
      },
      {
        "date": "2022-11-28T03:55:00.000Z",
        "voteCount": 1,
        "content": "A, C, D all sounds unfeasible (credentials, and compute engine)"
      },
      {
        "date": "2022-11-09T09:36:00.000Z",
        "voteCount": 1,
        "content": "B is ok"
      },
      {
        "date": "2022-10-22T19:40:00.000Z",
        "voteCount": 1,
        "content": "is correct answer"
      },
      {
        "date": "2022-05-31T08:53:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2022-01-04T01:01:00.000Z",
        "voteCount": 2,
        "content": "B is correct ans.I agree.\nhttps://cloud.google.com/blog/ja/products/management-tools/finding-your-gke-logs"
      },
      {
        "date": "2021-12-07T00:13:00.000Z",
        "voteCount": 1,
        "content": "Go for B"
      },
      {
        "date": "2021-11-27T10:53:00.000Z",
        "voteCount": 1,
        "content": "B is the correct answer"
      },
      {
        "date": "2021-10-27T11:48:00.000Z",
        "voteCount": 3,
        "content": "B \u2013 Review Stackdriver logs for specific GKE container that is serving the unresponsive part of the app.\nThis is a most directly matching answer for this Q, since it reviews GKE container logs, by that advertising this Stackdriver feature.\n\u201cFor container and system logs, GKE deploys a per-node logging agent that reads container logs, adds helpful metadata, and then stores them. The logging agent checks for container logs in the following sources:\n\u2022\tStandard output and standard error logs from containerized processes\n\u2022\tkubelet and container runtime logs\n\u2022\tLogs for system components, such as VM startup scripts\u201d\nOriginally we thought, that D is a right answer, since were confused with 2 seconds restart. But, that\u2019s restart for Pod, not for Node (GCE)\nD \u2013 Review Serial Port logs for each Compute Engince instance, that is serving as the in the cluster.\nSerial Port output is standard feature of Compute Engine (which retains 1 MB most recent logs for analysis). But, it is irrelevant for Pod\u2019s restart, caused by malfunction of some container."
      },
      {
        "date": "2021-07-08T11:16:00.000Z",
        "voteCount": 3,
        "content": "Answer is B"
      },
      {
        "date": "2021-07-07T07:15:00.000Z",
        "voteCount": 2,
        "content": "B is right answer. There is a catch here - Legacy logging of GKE with Stackdriver has deprecated. If this is used, you need to migrate to Cloud Operations for GKE, a new enhanced offering by Google with same functionality. \nFuture questions will have the answer choices with new tool \"Cloud Operations for GKE\" instead of Stackdriver.\nhttps://cloud.google.com/monitoring/kubernetes-engine/legacy-stackdriver/logging"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 70,
    "url": "https://www.examtopics.com/discussions/google/view/11815-exam-professional-cloud-architect-topic-1-question-70/",
    "body": "You are using a single Cloud SQL instance to serve your application from a specific zone. You want to introduce high availability. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a read replica instance in a different region",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a failover replica instance in a different region",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a read replica instance in the same region, but in a different zone",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a failover replica instance in the same region, but in a different zone\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 31,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 18,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-01-12T06:36:00.000Z",
        "voteCount": 36,
        "content": "Agree D"
      },
      {
        "date": "2020-08-06T04:31:00.000Z",
        "voteCount": 7,
        "content": "D is ok"
      },
      {
        "date": "2020-11-01T07:17:00.000Z",
        "voteCount": 4,
        "content": "Yes D is right"
      },
      {
        "date": "2022-06-05T04:36:00.000Z",
        "voteCount": 25,
        "content": "this Question is very Old and should be deleted from the exam , there is no Failover replica now , to do an HA we just confer it for the SQL instance that we have ."
      },
      {
        "date": "2022-06-02T02:43:00.000Z",
        "voteCount": 7,
        "content": "https://cloud.google.com/sql/docs/mysql/replication#:~:text=Read%20replicas%20neither%20provide%20high%20availability%20nor%20offer%20it.&amp;text=A%20primary%20instance%20cannot%20failover,any%20way%20during%20an%20outage.&amp;text=Maintenance%20windows%20cannot%20be%20set,windows%20with%20the%20primary%20instance.\n\n-\tRead replicas neither provide high availability nor offer it.\n\nAgree D"
      },
      {
        "date": "2022-09-13T09:42:00.000Z",
        "voteCount": 2,
        "content": "That link is helpful! I navigated to the \"quick reference for Cloud SQL read replicas\" and read the \"failover\" and \"high availability\" topics. They state: \n1. Failover - \"A primary instance cannot failover to a read replica, and read replicas are unable to failover in any way during an outage.\"\n2. High Availability - \"Read replicas neither provide high availability nor offer it.\""
      },
      {
        "date": "2024-01-31T11:09:00.000Z",
        "voteCount": 1,
        "content": "Sorry for this, but in the same link you can read: \nHigh availability\tRead replicas allow you to enable high availability on the replicas.\n\n(What I understand is that Read Replicas give you high availability on reads, of course, not in writes)."
      },
      {
        "date": "2020-05-15T04:14:00.000Z",
        "voteCount": 16,
        "content": "Cloud SQL is regional. For high availability, we need to think fo a failover strategy. So Option D meets the requirement.\ncreate failover replica in the same region but in different Zone"
      },
      {
        "date": "2024-04-23T17:53:00.000Z",
        "voteCount": 2,
        "content": "The HA configuration provides data redundancy. A Cloud SQL instance configured for HA is also called a regional instance and has a primary and secondary zone within the configured region. Within a regional instance, the configuration is made up of a primary instance and a standby instance. Through synchronous replication to each zone's persistent disk, all writes made to the primary instance are replicated to disks in both zones before a transaction is reported as committed. In the event of an instance or zone failure, the standby instance becomes the new primary instance. Users are then rerouted to the new primary instance. This process is called a failover.\nhttps://cloud.google.com/sql/docs/mysql/high-availability#HA-configuration"
      },
      {
        "date": "2024-02-06T03:49:00.000Z",
        "voteCount": 2,
        "content": "\"Note: Read replicas do not provide failover capability. To provide failover capability for an instance, see Configuring an instance for high availability.\"\nhttps://cloud.google.com/sql/docs/mysql/replication/"
      },
      {
        "date": "2023-12-15T04:46:00.000Z",
        "voteCount": 1,
        "content": "Correct answer would be D as a failover replica acts as a redundant copy incase of zone failure. However, option C causes confusion because a read replica can provide availability for reads, in case of zone failure for primary, but they cant provide support for writes. They would only work for reads."
      },
      {
        "date": "2023-12-14T06:14:00.000Z",
        "voteCount": 1,
        "content": "D, its regional product and failover is required for HA"
      },
      {
        "date": "2023-11-10T09:51:00.000Z",
        "voteCount": 1,
        "content": "C\n1. Failover replica is a legacy way and is not available in GCP now - B and D are not the options: https://cloud.google.com/sql/docs/mysql/high-availability#legacy_mysql_high_availability_option\n2. Cloud SQL is regional resource. However, cross-region read replicas are allowed now in Cloud SQL (https://cloud.google.com/blog/products/databases/introducing-cross-region-replica-for-cloud-sql) - A and C are options. \nChosen C, as there is no requirement or mention of cross-regional / global db."
      },
      {
        "date": "2024-01-08T14:49:00.000Z",
        "voteCount": 1,
        "content": "Read replica is not a valid choice for HA configurations. It does not provide automatic failover that is required for HA. It may be called something different or this answer has changed, but D is still the best option."
      },
      {
        "date": "2023-09-24T14:22:00.000Z",
        "voteCount": 1,
        "content": "The key is High Availability, not Resilience or Disaster Recovery. Therefore my answer is C"
      },
      {
        "date": "2023-09-09T05:11:00.000Z",
        "voteCount": 2,
        "content": "D.\nIn HA config, the second replica is caled stand by. The process of replacing the primary damaged node is called failover.\nhttps://cloud.google.com/sql/docs/postgres/high-availability"
      },
      {
        "date": "2023-09-04T05:22:00.000Z",
        "voteCount": 1,
        "content": "C for sure"
      },
      {
        "date": "2023-06-06T11:58:00.000Z",
        "voteCount": 4,
        "content": "C.\nFailover is so old and deprecated"
      },
      {
        "date": "2023-05-13T11:46:00.000Z",
        "voteCount": 1,
        "content": "this Question is very Old and should be deleted from the exam , there is no Failover replica now , to do an HA we just confer it for the SQL instance that we have  .. agreed tested as well"
      },
      {
        "date": "2023-04-19T02:01:00.000Z",
        "voteCount": 4,
        "content": "Option C is not the best choice because it suggests creating a read replica instance, which is designed to handle read traffic and provide better performance in read-heavy workloads, but it is not intended for high availability.\n\nOn the other hand, Option D suggests creating a failover replica instance in the same region but in a different zone. Failover replicas are designed specifically for high availability, as they maintain an up-to-date copy of the primary instance's data. If the primary instance becomes unresponsive or fails, Cloud SQL automatically switches to the failover replica with minimal downtime.\n\nIn summary, to introduce high availability for your Cloud SQL instance, you should create a failover replica instance in the same region but in a different zone (Option D) rather than creating a read replica instance (Option C), which doesn't provide high availability in case of primary instance failures."
      },
      {
        "date": "2023-06-13T03:13:00.000Z",
        "voteCount": 1,
        "content": "thanks!"
      },
      {
        "date": "2023-03-15T20:45:00.000Z",
        "voteCount": 1,
        "content": "D\nC is also not ideal for high availability because creating a read replica in the same region but in a different zone does not provide automatic failover. A read replica is used for scaling reads and can improve performance, but it is not a failover mechanism."
      },
      {
        "date": "2023-03-02T09:59:00.000Z",
        "voteCount": 1,
        "content": "Agree D.\nhttps://cloud.google.com/sql/docs/mysql/configure-ha\nThe legacy configuration for high availability used a failover replica instance. The new configuration does not use a failover replica. Instead, it uses Google's regional persistent disks, which synchronously replicate data at the block level between two zones in a region."
      },
      {
        "date": "2023-02-28T12:15:00.000Z",
        "voteCount": 2,
        "content": "The high availability configuration for Cloud SQL has recently changed. Failover replicas will no longer be included in the new Google recommended configuration and will be considered legacy. Google is moving towards persistent regional disks. This question as well as the solutions should be updated.\n\n\"The legacy configuration for high availability used a failover replica instance. The new configuration does not use a failover replica. Instead, it uses Google's regional persistent disks, which synchronously replicate data at the block level between two zones in a region.  If you have an existing MySQL instance that uses the legacy high availability configuration, you can update your configuration to use the current version.\"\nSource: https://cloud.google.com/sql/docs/mysql/configure-ha"
      },
      {
        "date": "2023-02-22T13:40:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/sql/docs/mysql/replication\nonly read replicas exists"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 71,
    "url": "https://www.examtopics.com/discussions/google/view/11301-exam-professional-cloud-architect-topic-1-question-71/",
    "body": "Your company is running a stateless application on a Compute Engine instance. The application is used heavily during regular business hours and lightly outside of business hours. Users are reporting that the application is slow during peak hours. You need to optimize the application's performance. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a snapshot of the existing disk. Create an instance template from the snapshot. Create an autoscaled managed instance group from the instance template.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a snapshot of the existing disk. Create a custom image from the snapshot. Create an autoscaled managed instance group from the custom image.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a custom image from the existing disk. Create an instance template from the custom image. Create an autoscaled managed instance group from the instance template.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an instance template from the existing disk. Create a custom image from the instance template. Create an autoscaled managed instance group from the custom image."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 18,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-12-21T22:49:00.000Z",
        "voteCount": 30,
        "content": "The easiest way would be to create template from --source-instance, and then create MIG, but it is not listed here, also you cannot create a MIG from image directly, you need a template, so answer is C (image -&gt; template -&gt; mig)."
      },
      {
        "date": "2022-08-24T21:00:00.000Z",
        "voteCount": 8,
        "content": "C is correct. \nTo sdsdfasd4's point - Not recommended to create template from --source-instance as If the existing instance contains a static external IP address, that address is copied into the instance template and might limit the use of the template. \nTemplates are best created from images or other templates. Creating the template from a running instance may require work to clean it up before it can be used for a MIG"
      },
      {
        "date": "2020-01-12T06:38:00.000Z",
        "voteCount": 12,
        "content": "C is the right answer"
      },
      {
        "date": "2023-08-24T05:51:00.000Z",
        "voteCount": 1,
        "content": "C is definitely the right answer"
      },
      {
        "date": "2024-06-27T07:53:00.000Z",
        "voteCount": 1,
        "content": "I prefer B, is better because I don't need to stop the instance to create the disk image."
      },
      {
        "date": "2023-12-01T02:26:00.000Z",
        "voteCount": 3,
        "content": "C. The key here is stateless, so we don't need snapshot the actual instance, we can start from zero."
      },
      {
        "date": "2023-12-01T01:29:00.000Z",
        "voteCount": 3,
        "content": "C\nWe can create an instance from an image or a custom image or a snapshot but an instance template can be created using either image or custom image only. \n\nAlso refer: https://cloud.google.com/compute/docs/instance-templates/create-instance-templates"
      },
      {
        "date": "2023-03-04T06:31:00.000Z",
        "voteCount": 6,
        "content": "Option C is the correct choice because creating a custom image from the existing disk ensures that the application environment is consistent and does not change between instances, which can reduce variability in performance. Creating an instance template from the custom image allows you to easily create new instances that are based on the same image, which can save time and effort. Finally, creating an autoscaled managed instance group allows you to automatically scale the number of instances based on demand, which can ensure that there are enough instances to handle peak traffic while minimizing costs during periods of low traffic"
      },
      {
        "date": "2022-11-07T01:26:00.000Z",
        "voteCount": 2,
        "content": "C is ok"
      },
      {
        "date": "2022-10-16T10:48:00.000Z",
        "voteCount": 1,
        "content": "C is right.\nCreate a custom image from the existing disk. Create an instance template from the custom image. Create an autoscaled managed instance group from the instance template."
      },
      {
        "date": "2022-09-13T15:41:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2022-07-05T05:19:00.000Z",
        "voteCount": 9,
        "content": "06/30/2022 Exam"
      },
      {
        "date": "2022-02-09T17:12:00.000Z",
        "voteCount": 2,
        "content": "I think Snapshot option are not correct in this scenario as to take snapshot you need to stop the VM so C looks best option"
      },
      {
        "date": "2024-02-01T01:35:00.000Z",
        "voteCount": 1,
        "content": "it's possible to create a snapshot of running VM by reducing I/O disks"
      },
      {
        "date": "2021-12-29T08:54:00.000Z",
        "voteCount": 1,
        "content": "C is the answer"
      },
      {
        "date": "2021-12-26T04:06:00.000Z",
        "voteCount": 1,
        "content": "C is the answer"
      },
      {
        "date": "2021-12-07T00:29:00.000Z",
        "voteCount": 3,
        "content": "Go for C.\nInstance template can not be created from snapshot. Only from an image."
      },
      {
        "date": "2021-11-27T23:07:00.000Z",
        "voteCount": 3,
        "content": "C is the right answer.\nhttps://cloud.google.com/compute/docs/instance-templates/create-instance-templates#using_custom_or_public_images_in_your_instance_templates"
      },
      {
        "date": "2021-11-24T16:52:00.000Z",
        "voteCount": 1,
        "content": "vote C"
      },
      {
        "date": "2021-10-28T06:36:00.000Z",
        "voteCount": 4,
        "content": "C \u2013 create a custom image from the existing disk. Create an instance template from the custom image. Create an autoscaled MIG from instance template.\nA could work if a snapshot was transformed to a custom image. Instance Template can be created only from image."
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 72,
    "url": "https://www.examtopics.com/discussions/google/view/11816-exam-professional-cloud-architect-topic-1-question-72/",
    "body": "Your web application has several VM instances running within a VPC. You want to restrict communications between instances to only the paths and ports you authorize, but you don't want to rely on static IP addresses or subnets because the app can autoscale. How should you restrict communications?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse separate VPCs to restrict traffic",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse firewall rules based on network tags attached to the compute instances\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud DNS and only allow connections from authorized hostnames",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse service accounts and configure the web application to authorize particular service accounts to have access"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-01-12T06:39:00.000Z",
        "voteCount": 24,
        "content": "Agree B"
      },
      {
        "date": "2020-11-01T07:20:00.000Z",
        "voteCount": 2,
        "content": "Yes B it is"
      },
      {
        "date": "2021-03-04T17:02:00.000Z",
        "voteCount": 2,
        "content": "B is correct"
      },
      {
        "date": "2022-12-21T23:29:00.000Z",
        "voteCount": 11,
        "content": "B. Use firewall rules based on network tags attached to the compute instances\n\nTo restrict communications between VM instances within a VPC without relying on static IP addresses or subnets, you can use firewall rules based on network tags attached to the compute instances. This will allow you to specify which instances are allowed to communicate with each other and on which paths and ports. You can then attach the relevant network tags to the compute instances when they are created, allowing you to control communication between the instances without relying on static IP addresses or subnets."
      },
      {
        "date": "2022-12-21T23:29:00.000Z",
        "voteCount": 4,
        "content": "Option A, using separate VPCs to restrict traffic, would not be a suitable solution because it would not allow the instances to communicate with each other, which is likely necessary for the functioning of the web application.\n\nOption C, using Cloud DNS and only allowing connections from authorized hostnames, would not be a suitable solution because it would not allow you to control communication between the instances based on their IP addresses or other characteristics.\n\nOption D, using service accounts and configuring the web application to authorize particular service accounts to have access, would not be a suitable solution because it would not allow you to control communication between the instances based on their IP addresses or other characteristics."
      },
      {
        "date": "2022-11-27T18:19:00.000Z",
        "voteCount": 4,
        "content": "Access to specific ports and protocol can be controlled only by firewall rule - Hence B is correct. D is not correct as service account is to authenticate and Authorized a specific machine to resource or service not ports and protocols"
      },
      {
        "date": "2022-11-07T01:27:00.000Z",
        "voteCount": 1,
        "content": "B is ok"
      },
      {
        "date": "2022-10-16T10:45:00.000Z",
        "voteCount": 1,
        "content": "B is the best option."
      },
      {
        "date": "2022-09-13T15:44:00.000Z",
        "voteCount": 2,
        "content": "Use firewall rules based on network tags attached to the compute instances"
      },
      {
        "date": "2022-09-09T21:58:00.000Z",
        "voteCount": 5,
        "content": "The secret is \"paths and ports\".\nWhich tell us Firewall as our only option."
      },
      {
        "date": "2023-04-20T13:00:00.000Z",
        "voteCount": 3,
        "content": "And how does firewall restrict \"paths\" pretty please?"
      },
      {
        "date": "2022-07-05T03:18:00.000Z",
        "voteCount": 1,
        "content": "B Firewall rules to restrict traffic"
      },
      {
        "date": "2021-12-07T00:30:00.000Z",
        "voteCount": 2,
        "content": "Go for B."
      },
      {
        "date": "2021-11-27T23:09:00.000Z",
        "voteCount": 2,
        "content": "B is the right answer"
      },
      {
        "date": "2021-10-28T06:38:00.000Z",
        "voteCount": 3,
        "content": "B \u2013 use firewall rules based on network tags attached to the compute instances \nThis answer avoids using IP, which are replaced by tags."
      },
      {
        "date": "2021-07-08T11:49:00.000Z",
        "voteCount": 4,
        "content": "Answer is B"
      },
      {
        "date": "2021-06-09T04:37:00.000Z",
        "voteCount": 1,
        "content": "B is ok"
      },
      {
        "date": "2021-05-19T04:54:00.000Z",
        "voteCount": 2,
        "content": "B. Use firewall rules based on network tags attached to the compute instances"
      },
      {
        "date": "2021-03-31T23:52:00.000Z",
        "voteCount": 1,
        "content": "Answer is B"
      },
      {
        "date": "2021-03-27T01:30:00.000Z",
        "voteCount": 1,
        "content": "B is ok"
      },
      {
        "date": "2021-02-26T21:08:00.000Z",
        "voteCount": 1,
        "content": "Agree B"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 73,
    "url": "https://www.examtopics.com/discussions/google/view/6529-exam-professional-cloud-architect-topic-1-question-73/",
    "body": "You are using Cloud SQL as the database backend for a large CRM deployment. You want to scale as usage increases and ensure that you don't run out of storage, maintain 75% CPU usage cores, and keep replication lag below 60 seconds. What are the correct steps to meet your requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Enable automatic storage increase for the instance. 2. Create a Stackdriver alert when CPU usage exceeds 75%, and change the instance type to reduce CPU usage. 3. Create a Stackdriver alert for replication lag, and shard the database to reduce replication time.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Enable automatic storage increase for the instance. 2. Change the instance type to a 32-core machine type to keep CPU usage below 75%. 3. Create a Stackdriver alert for replication lag, and deploy memcache to reduce load on the master.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a Stackdriver alert when storage exceeds 75%, and increase the available storage on the instance to create more space. 2. Deploy memcached to reduce CPU load. 3. Change the instance type to a 32-core machine type to reduce replication lag.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a Stackdriver alert when storage exceeds 75%, and increase the available storage on the instance to create more space. 2. Deploy memcached to reduce CPU load. 3. Create a Stackdriver alert for replication lag, and change the instance type to a 32-core machine type to reduce replication lag."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 6,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-01-12T06:41:00.000Z",
        "voteCount": 25,
        "content": "Agree with A"
      },
      {
        "date": "2023-08-24T05:59:00.000Z",
        "voteCount": 4,
        "content": "Sharding database will reduce latency"
      },
      {
        "date": "2022-10-16T10:43:00.000Z",
        "voteCount": 2,
        "content": "1. Enable automatic storage increase for the instance. 2. Create a Stackdriver alert when CPU usage exceeds 75%, and change the instance type to reduce CPU usage. 3. Create a Stackdriver alert for replication lag, and shard the database to reduce replication time."
      },
      {
        "date": "2022-06-07T15:51:00.000Z",
        "voteCount": 16,
        "content": "Has anyone who has taken the exam recently seen any lingering questions with the Stackdriver nomenclature or is it all cloud logging, cloud monitoring, etc.?"
      },
      {
        "date": "2023-12-29T14:01:00.000Z",
        "voteCount": 2,
        "content": "While I understand the doubts of selecting a 32 core machine from the start, answer A might be wrong...\nAccording to this article: \nhttps://cloud.google.com/sql/docs/mysql/instance-settings#impact\n''For MySQL instances, changing either the machine type or the zone of the instance results in the instance going offline for several minutes.''\nAnd I understand that instance type == machine type.\nIf we switch to a PostgreSQL or SQL Server instance, similar warnings appear:\nPostgreSQL: ''Changing the number of CPUs or the memory size results in the instance going offline for less than 60 seconds. The total time for the changes to take effect can take several minutes.''\nSQL Server: ''Changing the number of CPUs or the memory size results in the instance going offline for less than 60 seconds.''"
      },
      {
        "date": "2023-12-29T14:02:00.000Z",
        "voteCount": 1,
        "content": "But I also understand that the question didn't mention anything about needing the DB to be online always, so maybe these offline times are acceptable..."
      },
      {
        "date": "2023-10-05T20:19:00.000Z",
        "voteCount": 2,
        "content": "You are using Cloud SQL as the database backend for a large CRM deployment. You want to scale as usage increases and ensure that you don't run out of storage, maintain 75% CPU usage cores, and keep replication lag below 60 seconds. What are the correct steps to meet your requirements?\n\n\nC &amp; D is out of question as it is talking of 75% of storage, where in question it says 75% of CPU.\nOption A says monitoring before before taking action and sharding will also help in reducing latency.\nOption B specifies specific machine type, which is not correct and also memcache which is used to recude the round trip to fetch data, it will help in reducing latency.\n\nI would prefer to go with Option A, as it is correct sequence to solve the problem."
      },
      {
        "date": "2023-06-06T12:09:00.000Z",
        "voteCount": 1,
        "content": "For me is A."
      },
      {
        "date": "2023-02-12T00:03:00.000Z",
        "voteCount": 4,
        "content": "The correct answer is D.\n\n1.  Create a Stackdriver alert when storage exceeds 75%, and increase the available storage on the instance to create more space.\n2.  Deploy memcached to reduce CPU load.\n3.  Create a Stackdriver alert for replication lag, and change the instance type to a 32-core machine type to reduce replication lag.\n\nThis approach ensures that you are able to address the three requirements specified in the question:\n\n-   Monitoring storage usage and increasing storage when it exceeds 75% to avoid running out of storage.\n-   Reducing CPU load by deploying memcached, which can be used to cache frequently-used data, offloading some of the load from the database.\n-   Monitoring replication lag and increasing the number of cores to reduce lag."
      },
      {
        "date": "2022-12-27T13:56:00.000Z",
        "voteCount": 5,
        "content": "It may be A for the simple fact that all the other answers throw in a tiny detail about 32 cores. This seems like a red herring (unnecessary details that are meant to distract), so for that reason, A is the answer."
      },
      {
        "date": "2022-11-07T09:33:00.000Z",
        "voteCount": 1,
        "content": "A is ok"
      },
      {
        "date": "2022-10-16T10:43:00.000Z",
        "voteCount": 1,
        "content": "A is right"
      },
      {
        "date": "2022-08-22T22:35:00.000Z",
        "voteCount": 3,
        "content": "A is incorrect. because of the wording \"Shard the database\". How can you shard the database in Cloud SQL without causing major disruptions? Sharding is not a core feature of RDBMS. \n\nB should be correct. inspite of the mention of a fixed 32 core"
      },
      {
        "date": "2022-10-16T20:35:00.000Z",
        "voteCount": 4,
        "content": "Ii\nKk\nKk\nYou can shard cloudsql. Review this article - https://cloud.google.com/community/tutorials/horizontally-scale-mysql-database-backend-with-google-cloud-sql-and-proxysql#:~:text=Common%20approaches%20for%20horizontally%20scaling,with%20Cloud%20SQL%20and%20ProxySQL."
      },
      {
        "date": "2022-12-06T19:07:00.000Z",
        "voteCount": 1,
        "content": "The article only mentions sharding as a concept, and not a solution for cloudsql."
      },
      {
        "date": "2023-02-27T13:52:00.000Z",
        "voteCount": 4,
        "content": "https://cloud.google.com/community/tutorials/horizontally-scale-mysql-database-backend-with-google-cloud-sql-and-proxysql#:~:text=SQL%20and%20ProxySQL.-,Sharding,logic%20or%20a%20query%20router.\n\nYou can shard MySQL. \n\nAnswer should be A."
      },
      {
        "date": "2022-03-11T01:07:00.000Z",
        "voteCount": 2,
        "content": "vote for A"
      },
      {
        "date": "2021-12-07T00:34:00.000Z",
        "voteCount": 2,
        "content": "Go for A"
      },
      {
        "date": "2021-11-17T23:50:00.000Z",
        "voteCount": 2,
        "content": "A is the correct answer"
      },
      {
        "date": "2021-08-31T01:27:00.000Z",
        "voteCount": 4,
        "content": "We can directly eliminate C and D we are doing some work that is already automated.\n\nStill, I cannot make a point why not B is better than A?\nI believe adding memcash will give an additional boost\n\nCan someone help me point out why A is better than B?"
      },
      {
        "date": "2021-10-10T14:27:00.000Z",
        "voteCount": 3,
        "content": "Just to back up what amxexam said, here is the link on automatically increasing storage based on trend analysis:  \n\nhttps://cloud.google.com/sql/docs/mysql/instance-settings#storage-capacity-2ndgen"
      },
      {
        "date": "2021-12-28T01:11:00.000Z",
        "voteCount": 1,
        "content": "That is correct - but doc only mentions auto storage increase for this specific product (cloud SQL)."
      },
      {
        "date": "2022-10-18T10:39:00.000Z",
        "voteCount": 2,
        "content": "Should read MySQl"
      },
      {
        "date": "2021-10-13T01:44:00.000Z",
        "voteCount": 5,
        "content": "I suppose B is not a better option, since it indicates 'add 32core cpu', with no info of the current usage that seems like a over-kill."
      },
      {
        "date": "2022-08-05T02:57:00.000Z",
        "voteCount": 4,
        "content": "I would say only because of the below line|\n\"You want to scale as usage increases\" Line 1\nCreating a 32 core machine upfront where we do not know what was the source machine cores would not be ideal .\nin that situation i would go with A"
      },
      {
        "date": "2021-07-08T11:52:00.000Z",
        "voteCount": 4,
        "content": "Answer is A"
      },
      {
        "date": "2021-06-09T05:37:00.000Z",
        "voteCount": 1,
        "content": "A it is"
      },
      {
        "date": "2021-05-19T05:18:00.000Z",
        "voteCount": 1,
        "content": "A. 1. Enable automatic storage increase for the instance. 2. Create a Stackdriver alert when CPU usage exceeds 75%, and change the instance type to reduce CPU usage. 3. Create a Stackdriver alert for replication lag, and shard the database to reduce replication time."
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 74,
    "url": "https://www.examtopics.com/discussions/google/view/11817-exam-professional-cloud-architect-topic-1-question-74/",
    "body": "You are tasked with building an online analytical processing (OLAP) marketing analytics and reporting tool. This requires a relational database that can operate on hundreds of terabytes of data. What is the Google-recommended tool for such applications?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Spanner, because it is globally distributed",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud SQL, because it is a fully managed relational database",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Firestore, because it offers real-time synchronization across devices",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBigQuery, because it is designed for large-scale processing of tabular data\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 16,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-01-12T06:42:00.000Z",
        "voteCount": 20,
        "content": "Agree D"
      },
      {
        "date": "2020-08-06T04:42:00.000Z",
        "voteCount": 5,
        "content": "D is ok"
      },
      {
        "date": "2021-01-27T16:54:00.000Z",
        "voteCount": 4,
        "content": "What about the relational part? BigQuery uses SQL but it's not relational... I'm not sure its D"
      },
      {
        "date": "2021-06-10T01:34:00.000Z",
        "voteCount": 5,
        "content": "BigQuery is relational!"
      },
      {
        "date": "2021-07-08T00:08:00.000Z",
        "voteCount": 13,
        "content": "Pls do not confuse - Cloud SQL and BigQuery are RDBMS. Cloud Datastore, Bigtable are NoSQL.\nRight answer is D - BQ"
      },
      {
        "date": "2020-11-01T07:22:00.000Z",
        "voteCount": 2,
        "content": "Yes it is D"
      },
      {
        "date": "2021-03-04T17:06:00.000Z",
        "voteCount": 3,
        "content": "D, OLAP=BQ"
      },
      {
        "date": "2023-05-08T02:11:00.000Z",
        "voteCount": 1,
        "content": "Well Said"
      },
      {
        "date": "2021-10-11T16:48:00.000Z",
        "voteCount": 2,
        "content": "The question asks \"This requires a relational database that can operate on hundreds of terabytes of data\", but bq doesn't meet this condition?"
      },
      {
        "date": "2022-06-01T08:42:00.000Z",
        "voteCount": 2,
        "content": "BigQuery supports relational and query of join tables."
      },
      {
        "date": "2020-06-10T02:42:00.000Z",
        "voteCount": 15,
        "content": "D, for sure.\nBigQuery for OLAP\nGoogle Cloud&nbsp;Spanner for OLTP."
      },
      {
        "date": "2024-07-11T14:56:00.000Z",
        "voteCount": 1,
        "content": "sql is oltp \nolap is data cube. lots of data which we try to process somehow. biq query is for that. \n\nfirestore is for mobile and web apps. not so fast nosql db."
      },
      {
        "date": "2024-02-21T08:11:00.000Z",
        "voteCount": 1,
        "content": "Agreed"
      },
      {
        "date": "2023-10-05T20:23:00.000Z",
        "voteCount": 1,
        "content": "4 reasons to choose BQ (Supports Petabytes of data)\n - OLAP Data \n - Relational DB (SQL)\n - 100s of TB data\n - Analystics and Reporting"
      },
      {
        "date": "2023-03-11T14:21:00.000Z",
        "voteCount": 1,
        "content": "D is obvious"
      },
      {
        "date": "2023-03-07T09:50:00.000Z",
        "voteCount": 1,
        "content": "D is the right one"
      },
      {
        "date": "2022-12-15T20:55:00.000Z",
        "voteCount": 1,
        "content": "Cloud SQL/Spanner is OLTP DB but not OLAP. BQ is a well-known OLAP for analytics and also supports RBMS feature too... so I would got with D"
      },
      {
        "date": "2022-11-21T02:15:00.000Z",
        "voteCount": 1,
        "content": "D is correct. BigQuery is relational. Cloud SQL is not OLAP; moreover it can not store/process hundreds of TB of data. Max size is 64 TB only."
      },
      {
        "date": "2022-11-07T09:46:00.000Z",
        "voteCount": 1,
        "content": "D is ok"
      },
      {
        "date": "2022-10-30T02:40:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/products/databases."
      },
      {
        "date": "2022-10-17T01:34:00.000Z",
        "voteCount": 1,
        "content": "The words you need to focus \"You are tasked with building an online analytical processing (OLAP) marketing analytics and reporting tool\" which is BigQuery"
      },
      {
        "date": "2022-10-16T10:39:00.000Z",
        "voteCount": 1,
        "content": "Big Query for large analytics , D is right"
      },
      {
        "date": "2022-09-18T20:21:00.000Z",
        "voteCount": 2,
        "content": "The keyword in this context is OLAP. CloudSQL is Relational SQL for OLTP. Capacity wise, BQ supports for PB+ while CloudSQL only have max capacity of up to ~10TB. Again the questions specifically mention \"hundreds of TB of data\". So D is the answer."
      },
      {
        "date": "2022-09-14T07:24:00.000Z",
        "voteCount": 1,
        "content": "Why is it not CloudSQL? It supports TB data storage and the question is about a relational database, not a data warehouse such as BigQuery."
      },
      {
        "date": "2022-09-18T20:20:00.000Z",
        "voteCount": 1,
        "content": "The keyword in this context is OLAP. CloudSQL is Relational SQL for OLTP. Capacity wise, BQ supports for PB+ while CloudSQL only have max capacity of up to ~10TB. Again the questions specifically mention \"hundreds of TB of data\". So D is the answer."
      },
      {
        "date": "2022-09-09T22:03:00.000Z",
        "voteCount": 1,
        "content": "The answer is Big Query, D\nSecret: Analytical, Hundreds of TBTs. Relational. \nAll of this are strictly meet by Big Query, if it had not said Analytical but rather, other keywords like High Availability then Cloud Spanner."
      },
      {
        "date": "2022-07-21T06:22:00.000Z",
        "voteCount": 1,
        "content": "Guys, this is easy:\nOLTP - Cloud Spanner &amp; Cloud SQL\nOLAP - Big Query\nNoSQL - Filestore and Big Table\n\nSo answer is D."
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 75,
    "url": "https://www.examtopics.com/discussions/google/view/6452-exam-professional-cloud-architect-topic-1-question-75/",
    "body": "You have deployed an application to Google Kubernetes Engine (GKE), and are using the Cloud SQL proxy container to make the Cloud SQL database available to the services running on Kubernetes. You are notified that the application is reporting database connection issues. Your company policies require a post- mortem. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse gcloud sql instances restart.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tValidate that the Service Account used by the Cloud SQL proxy container still has the Cloud Build Editor role.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the GCP Console, navigate to Stackdriver Logging. Consult logs for (GKE) and Cloud SQL.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the GCP Console, navigate to Cloud SQL. Restore the latest backup. Use kubectl to restart all pods."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2019-10-21T08:20:00.000Z",
        "voteCount": 64,
        "content": "post mortem always includes log analysis, answer is C"
      },
      {
        "date": "2020-01-12T06:44:00.000Z",
        "voteCount": 3,
        "content": "AGREE C"
      },
      {
        "date": "2022-10-16T10:38:00.000Z",
        "voteCount": 1,
        "content": "C is right for Root Cause Analysis."
      },
      {
        "date": "2023-05-08T02:16:00.000Z",
        "voteCount": 1,
        "content": "Thanks for the info"
      },
      {
        "date": "2021-07-08T11:55:00.000Z",
        "voteCount": 5,
        "content": "Answer is C"
      },
      {
        "date": "2023-12-01T02:30:00.000Z",
        "voteCount": 2,
        "content": "C -&gt; post-mortem = log analysis"
      },
      {
        "date": "2023-10-05T20:27:00.000Z",
        "voteCount": 1,
        "content": "You can jump on to the conlusion hence answer is not B. Consulting logs is always a good way to start investigation. and A and D is not a choice."
      },
      {
        "date": "2022-11-15T01:11:00.000Z",
        "voteCount": 2,
        "content": "Stackdriver is deprecated, now you must navigate to Cloud Logging."
      },
      {
        "date": "2022-11-07T09:49:00.000Z",
        "voteCount": 1,
        "content": "C is ok"
      },
      {
        "date": "2022-10-22T20:04:00.000Z",
        "voteCount": 1,
        "content": "C is the right answer"
      },
      {
        "date": "2022-09-05T01:51:00.000Z",
        "voteCount": 1,
        "content": "Logical answer is C. But is Stackdriver Logging enabled by default? Appreciate if someone could answer this?"
      },
      {
        "date": "2021-12-07T00:37:00.000Z",
        "voteCount": 1,
        "content": "Go for C"
      },
      {
        "date": "2021-12-05T06:14:00.000Z",
        "voteCount": 1,
        "content": "post mortem = logs"
      },
      {
        "date": "2021-11-27T23:30:00.000Z",
        "voteCount": 1,
        "content": "C is the correct answer"
      },
      {
        "date": "2021-11-24T16:54:00.000Z",
        "voteCount": 1,
        "content": "vote C"
      },
      {
        "date": "2021-10-28T07:09:00.000Z",
        "voteCount": 4,
        "content": "C \u2013 in GCP Console navigate to Stackdriver Logging. Consult logs for Kubernetes Engine and Cloud SQL.\nA/D \u2013 is an immediate attempt to fix an issue. No analysis.\nB \u2013 is irrelevant at all. Cloud SQL proxy should not build anything in production."
      },
      {
        "date": "2021-07-08T05:49:00.000Z",
        "voteCount": 4,
        "content": "Answer is C. I request all here - not to blindly follow the answers published at coursera or udemy as most of them are copy-pasted answer and are not real. Examtopis provides the more accurate answers and also support with comments"
      },
      {
        "date": "2021-07-08T05:49:00.000Z",
        "voteCount": 2,
        "content": "Answer is B. I request all here - not to blindly follow the answers published at coursera or udemy as most of them are copy-pasted answer and are not real. Examtopis provides the more accurate answers and also support with comments"
      },
      {
        "date": "2021-10-13T12:14:00.000Z",
        "voteCount": 4,
        "content": "Why Service Account needs Cloud Build Editor role for accessing Cloud SQL?\nThe role is misleading/wrong, so B is wrong."
      },
      {
        "date": "2021-05-19T05:18:00.000Z",
        "voteCount": 2,
        "content": "C. In the GCP Console, navigate to Stackdriver Logging. Consult logs for Kubernetes Engine and Cloud SQL."
      },
      {
        "date": "2021-05-15T07:17:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 76,
    "url": "https://www.examtopics.com/discussions/google/view/11818-exam-professional-cloud-architect-topic-1-question-76/",
    "body": "Your company pushes batches of sensitive transaction data from its application server VMs to Cloud Pub/Sub for processing and storage. What is the Google- recommended way for your application to authenticate to the required Google Cloud services?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure that VM service accounts are granted the appropriate Cloud Pub/Sub IAM roles.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure that VM service accounts do not have access to Cloud Pub/Sub, and use VM access scopes to grant the appropriate Cloud Pub/Sub IAM roles.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGenerate an OAuth2 access token for accessing Cloud Pub/Sub, encrypt it, and store it in Cloud Storage for access from each VM.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a gateway to Cloud Pub/Sub using a Cloud Function, and grant the Cloud Function service account the appropriate Cloud Pub/Sub IAM roles."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-01-12T06:45:00.000Z",
        "voteCount": 25,
        "content": "Agree A"
      },
      {
        "date": "2020-11-01T07:55:00.000Z",
        "voteCount": 2,
        "content": "Yes A it is"
      },
      {
        "date": "2021-03-04T17:09:00.000Z",
        "voteCount": 2,
        "content": "A is correct"
      },
      {
        "date": "2021-09-04T21:48:00.000Z",
        "voteCount": 12,
        "content": "It's because of questions like these that I do not feel guilty about using question banks :D In what world would you accept value requirements like this from your user? Wouldn't you ask \"Do you want to just authenticate? or the data to be encrypted on its way to pub/sub?\"\nI'll ignore the first part of the question and assume all data is sensitive, and focus on \"What is the Google- recommended way for your application to authenticate to the required Google Cloud services?\" -- The answer then is A. \n\nUse encryption and defense-in-depth for the first part."
      },
      {
        "date": "2022-10-07T10:08:00.000Z",
        "voteCount": 1,
        "content": "Service accounts use keys"
      },
      {
        "date": "2024-01-03T13:17:00.000Z",
        "voteCount": 3,
        "content": "&gt; It's because of questions like these that I do not feel guilty about using question banks :D\n\nSame. To me, it wasn't clear whether the servers were in google or not due to the question about accessing google cloud. It was asked as if the VMs were outside of google"
      },
      {
        "date": "2023-06-06T23:25:00.000Z",
        "voteCount": 2,
        "content": "A is correct for me. It's batch, so no cloud function"
      },
      {
        "date": "2022-12-21T23:45:00.000Z",
        "voteCount": 7,
        "content": "A. Ensure that VM service accounts are granted the appropriate Cloud Pub/Sub IAM roles.\n\nThe Google-recommended way for your application to authenticate to Cloud Pub/Sub and other Google Cloud services when running on Compute Engine VMs is to use VM service accounts. VM service accounts are automatically created when you create a Compute Engine VM, and they are associated with the VM instance. To authenticate to Cloud Pub/Sub and other Google Cloud services, you should ensure that the VM service accounts are granted the appropriate IAM roles."
      },
      {
        "date": "2022-12-21T23:46:00.000Z",
        "voteCount": 3,
        "content": "Option B, ensuring that VM service accounts do not have access to Cloud Pub/Sub and using VM access scopes to grant the appropriate Cloud Pub/Sub IAM roles, would not be a suitable solution because VM service accounts are required for authentication to Google Cloud services.\n\nOption C, generating an OAuth2 access token for accessing Cloud Pub/Sub, encrypting it, and storing it in Cloud Storage for access from each VM, would not be a suitable solution because it would require manual management of access tokens, which can be error-prone and insecure.\n\nOption D, creating a gateway to Cloud Pub/Sub using a Cloud Function and granting the Cloud Function service account the appropriate Cloud Pub/Sub IAM roles, would not be a suitable solution because it would not allow the application to directly authenticate to Cloud Pub/Sub."
      },
      {
        "date": "2023-05-08T02:22:00.000Z",
        "voteCount": 1,
        "content": "Great way of explanation..By removing/elimination approach"
      },
      {
        "date": "2022-11-07T09:53:00.000Z",
        "voteCount": 1,
        "content": "A is ok"
      },
      {
        "date": "2022-10-22T20:06:00.000Z",
        "voteCount": 1,
        "content": "A is the correct answer"
      },
      {
        "date": "2022-08-04T04:29:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/iam/docs/understanding-service-accounts"
      },
      {
        "date": "2022-01-31T05:34:00.000Z",
        "voteCount": 2,
        "content": "The combination of Roles assigned to Service accounts granted to VMs is the way to go. :)"
      },
      {
        "date": "2022-01-03T08:27:00.000Z",
        "voteCount": 3,
        "content": "Service accounts are recommended for almost all cases in Pub/Sub (see https://cloud.google.com/pubsub/docs/authentication#service-accounts)"
      },
      {
        "date": "2021-12-07T00:43:00.000Z",
        "voteCount": 2,
        "content": "Go for A."
      },
      {
        "date": "2021-11-27T23:35:00.000Z",
        "voteCount": 1,
        "content": "A is the correct answer"
      },
      {
        "date": "2021-10-28T10:20:00.000Z",
        "voteCount": 3,
        "content": "A \u2013 ensure that VM service accounts are granted the appropriate Cloud Pub/Sub IAM roles.\nCheck Migrating Data to GCP section of this page:\nhttps://cloud.google.com/iam/docs/understanding-service-accounts \nYou will create a service account key and use it from an external process to call Cloud Platform APIs."
      },
      {
        "date": "2021-10-23T02:50:00.000Z",
        "voteCount": 1,
        "content": "A is very correct"
      },
      {
        "date": "2021-07-08T11:56:00.000Z",
        "voteCount": 2,
        "content": "Answer is A"
      },
      {
        "date": "2021-05-19T05:14:00.000Z",
        "voteCount": 3,
        "content": "A. Ensure that VM service accounts are granted the appropriate Cloud Pub/Sub IAM roles."
      },
      {
        "date": "2022-10-16T10:36:00.000Z",
        "voteCount": 1,
        "content": "Agreed with A"
      },
      {
        "date": "2021-05-15T07:23:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2021-04-01T10:48:00.000Z",
        "voteCount": 1,
        "content": "Ans. A"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 77,
    "url": "https://www.examtopics.com/discussions/google/view/11819-exam-professional-cloud-architect-topic-1-question-77/",
    "body": "You want to establish a Compute Engine application in a single VPC across two regions. The application must communicate over VPN to an on-premises network.<br>How should you deploy the VPN?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse VPC Network Peering between the VPC and the on-premises network.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExpose the VPC to the on-premises network using IAM and VPC Sharing.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a global Cloud VPN Gateway with VPN tunnels from each region to the on-premises peer gateway.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy Cloud VPN Gateway in each region. Ensure that each region has at least one VPN tunnel to the on-premises peer gateway.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 16,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-04-12T06:24:00.000Z",
        "voteCount": 44,
        "content": "It can't be -A - VPC Network Peering only allows private RFC 1918 connectivity across two Virtual Private Cloud (VPC) networks. In this example is one VPC with on-premise network\nhttps://cloud.google.com/vpc/docs/vpc-peering\n\nIt is not definitely - B - Can't be\n\nIt is not C - Because Cloud VPN gateways and tunnels are regional objects, not global\n\nSo, it the answer is D - \nhttps://cloud.google.com/vpn/docs/how-to/creating-static-vpns"
      },
      {
        "date": "2021-09-08T05:27:00.000Z",
        "voteCount": 1,
        "content": "Why not A?\nhttps://cloud.google.com/vpc/docs/vpc-peering#benefits_of_exchanging_custom_routes\nThe second use case is exactly what is in the question.\n\nDon't get the argument about RFC 1918.\nWill go with A"
      },
      {
        "date": "2021-12-16T08:42:00.000Z",
        "voteCount": 4,
        "content": "https://cloud.google.com/vpc/docs/vpc-peering allows internal IP address connectivity across two VPC so A is not the answer as the on premise network need to use public IP. cmiiw"
      },
      {
        "date": "2023-10-05T22:06:00.000Z",
        "voteCount": 2,
        "content": "The question clearly asks us to use VPN."
      },
      {
        "date": "2022-10-16T10:33:00.000Z",
        "voteCount": 1,
        "content": "Agreed with D."
      },
      {
        "date": "2020-11-18T11:46:00.000Z",
        "voteCount": 20,
        "content": "Just Passed my exam and I answered (D) for this question"
      },
      {
        "date": "2021-12-05T23:24:00.000Z",
        "voteCount": 3,
        "content": "sound promising dude"
      },
      {
        "date": "2023-05-08T02:28:00.000Z",
        "voteCount": 3,
        "content": "IS the Exam Idea questions enough dude, for passing this exam?"
      },
      {
        "date": "2024-06-27T01:20:00.000Z",
        "voteCount": 1,
        "content": "Option C: Create a global VPN gateway and establish VPN tunnels from each region to the on-premises peer gateway. This suggests that a single global VPN gateway manages the tunnels from both regions.\nOption D: Deploy a VPN gateway in each region and ensure that each region has at least one VPN tunnel to the on-premises peer gateway. This indicates that each region has its own VPN gateway.\n\n&gt;Option D ensures that there is a VPN gateway in each region, providing greater redundancy. If a gateway in one region fails, the gateway in the other region remains operational."
      },
      {
        "date": "2024-03-27T00:31:00.000Z",
        "voteCount": 1,
        "content": "Global Cloud VPN Gateway: This feature allows for the creation of a single VPN gateway that can serve multiple regions within the same VPC network. By creating a global VPN gateway, you can efficiently manage VPN connections from all regions of your VPC to your on-premises network.\n\nSimplicity and Efficiency: Using a global gateway simplifies the configuration and management of VPN connections as opposed to maintaining separate regional VPN gateways. It centralizes the VPN endpoint on the Google Cloud side, reducing the complexity of the network setup.\n\nReliable and Secure Communication: The global Cloud VPN Gateway allows for secure, encrypted tunnels between Google Cloud and the on-premises network, ensuring that the application\u2019s inter-regional and on-premises communications are secure."
      },
      {
        "date": "2024-01-03T06:53:00.000Z",
        "voteCount": 1,
        "content": "C is wrong. A global vpn is a single region resource.\nhttps://cloud.google.com/network-connectivity/docs/vpn/how-to/creating-ha-vpn?hl=it\n\ngcloud compute vpn-gateways create GW_NAME \\\n    --network=NETWORK \\\n    --region=REGION \\\n    --stack-type=IP_STACK\n\nso D is the answer"
      },
      {
        "date": "2023-12-30T09:24:00.000Z",
        "voteCount": 2,
        "content": "It\u00b4s option C! So, while the VPN Gateway itself is a regional resource, its scope can be effectively global as it can serve resources across different regions within the same Virtual Private Cloud (VPC). This is why it\u2019s sometimes referred to as a \u2018global\u2019 service in the context of its functionality, even though strictly speaking, it\u2019s a regional resource."
      },
      {
        "date": "2023-10-05T22:06:00.000Z",
        "voteCount": 2,
        "content": "Each Cloud VPN gateway is a regional resource that uses one or more regional external IP addresses. A Cloud VPN gateway can connect to a peer VPN gateway."
      },
      {
        "date": "2023-05-13T12:00:00.000Z",
        "voteCount": 3,
        "content": "It can't be -A - VPC Network Peering only allows private RFC 1918 connectivity across two Virtual Private Cloud (VPC) networks. In this example is one VPC with on-premise network https://cloud.google.com/vpc/docs/vpc-peering  It is not definitely - B - Can't be  It is not C - Because Cloud VPN gateways and tunnels are regional objects, not global  So, it the answer is D -  https://cloud.google.com/vpn/docs/how-to/creating-static-vpn"
      },
      {
        "date": "2023-01-14T19:37:00.000Z",
        "voteCount": 1,
        "content": "D looks fine."
      },
      {
        "date": "2022-12-19T09:28:00.000Z",
        "voteCount": 2,
        "content": "As HA isn't required, why do we need two VPN gateways?"
      },
      {
        "date": "2022-11-07T09:58:00.000Z",
        "voteCount": 1,
        "content": "D is ok"
      },
      {
        "date": "2022-10-22T20:11:00.000Z",
        "voteCount": 1,
        "content": "D is the correct answer,  in order to do A you will need VPN., or interconnect"
      },
      {
        "date": "2022-10-17T01:48:00.000Z",
        "voteCount": 1,
        "content": "there is two VPN:\n1. classic VPN\n2. HA VPN"
      },
      {
        "date": "2022-08-04T04:36:00.000Z",
        "voteCount": 4,
        "content": "Cloud VPN Gateway is a regional service, not global."
      },
      {
        "date": "2022-06-01T08:58:00.000Z",
        "voteCount": 2,
        "content": "Why not C? services across regions can communicate to each other, VPN only connects to the closet region, and all the VPC shall be connected if firewall's set."
      },
      {
        "date": "2021-12-07T01:16:00.000Z",
        "voteCount": 2,
        "content": "Go for D.\nCloud VPN Gateway is regional. NOt Global\ngcloud compute vpn-gateways create GW_NAME \\\n    --network=NETWORK \\\n    --region=REGION"
      },
      {
        "date": "2021-11-28T00:28:00.000Z",
        "voteCount": 2,
        "content": "D is the correct answer"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 78,
    "url": "https://www.examtopics.com/discussions/google/view/6455-exam-professional-cloud-architect-topic-1-question-78/",
    "body": "Your applications will be writing their logs to BigQuery for analysis. Each application should have its own table. Any logs older than 45 days should be removed.<br>You want to optimize storage and follow Google-recommended practices. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the expiration time for your tables at 45 days",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMake the tables time-partitioned, and configure the partition expiration at 45 days\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRely on BigQuery's default behavior to prune application logs older than 45 days",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a script that uses the BigQuery command line tool (bq) to remove records older than 45 days"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 19,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2019-10-11T04:47:00.000Z",
        "voteCount": 39,
        "content": "Could you please help clarify? I think B is correct.\nIt looks like table will be deleted with option A.\nhttps://cloud.google.com/bigquery/docs/managing-tables#updating_a_tables_expiration_time\nWhen you delete a table, any data in the table is also deleted. To automatically delete tables after a specified period of time, set the&nbsp;default table expiration&nbsp;for the dataset or set the expiration time when you&nbsp;create the table."
      },
      {
        "date": "2020-08-06T17:56:00.000Z",
        "voteCount": 8,
        "content": "B is ok"
      },
      {
        "date": "2021-03-04T17:53:00.000Z",
        "voteCount": 4,
        "content": "B partition table"
      },
      {
        "date": "2020-11-01T08:00:00.000Z",
        "voteCount": 11,
        "content": "it is B, if you use option A, on 46th day there is no table/content in table for application :)"
      },
      {
        "date": "2022-10-16T10:31:00.000Z",
        "voteCount": 2,
        "content": "Agreed and going with B"
      },
      {
        "date": "2019-12-15T22:24:00.000Z",
        "voteCount": 10,
        "content": "Agreed with B."
      },
      {
        "date": "2024-02-22T00:13:00.000Z",
        "voteCount": 1,
        "content": "I think B is correct."
      },
      {
        "date": "2023-10-05T22:20:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/bigquery/docs/managing-partitioned-tables#partition-expiration\n\nB is the correct answer."
      },
      {
        "date": "2023-07-06T11:52:00.000Z",
        "voteCount": 4,
        "content": "https://cloud.google.com/bigquery/docs/managing-partitioned-tables#partition-expiration"
      },
      {
        "date": "2023-06-23T12:03:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2023-05-08T02:32:00.000Z",
        "voteCount": 1,
        "content": "B seems correct as this will partitioning will create a filter criteria on the basis of which specified actions on logs will be taken"
      },
      {
        "date": "2022-12-28T21:59:00.000Z",
        "voteCount": 6,
        "content": "B is the correct answer,\n\nIf your tables are partitioned by date, the dataset's default table expiration applies to the individual partitions. You can also control partition expiration using the time_partitioning_expiration flag in the bq command-line tool or the expirationMs configuration setting in the API. When a partition expires, data in the partition is deleted but the partitioned table is not dropped even if the table is empty.\n\nhttps://cloud.google.com/bigquery/docs/best-practices-storage"
      },
      {
        "date": "2022-11-07T09:59:00.000Z",
        "voteCount": 1,
        "content": "B is ok"
      },
      {
        "date": "2022-10-29T08:21:00.000Z",
        "voteCount": 2,
        "content": "B - You can control partition expiration using the time_partitioning_expiration flag in the bq command-line\nhttps://cloud.google.com/bigquery/docs/best-practices-storage"
      },
      {
        "date": "2022-09-17T10:44:00.000Z",
        "voteCount": 1,
        "content": "B is okay"
      },
      {
        "date": "2022-08-04T04:39:00.000Z",
        "voteCount": 1,
        "content": "Using Table-Partitions."
      },
      {
        "date": "2022-08-04T04:38:00.000Z",
        "voteCount": 1,
        "content": "Using Table-Partitions."
      },
      {
        "date": "2022-02-11T13:33:00.000Z",
        "voteCount": 1,
        "content": "I got similar question on my exam."
      },
      {
        "date": "2021-12-07T01:24:00.000Z",
        "voteCount": 2,
        "content": "Go for B.\nhttps://cloud.google.com/bigquery/docs/creating-partitioned-tables#sql\nCREATE TABLE\n  mydataset.newtable (transaction_id INT64, transaction_date DATE)\nPARTITION BY\n  transaction_date\nOPTIONS(\n  partition_expiration_days=3,\n  require_partition_filter=true\n)"
      },
      {
        "date": "2021-11-28T00:31:00.000Z",
        "voteCount": 2,
        "content": "B is the correct answer"
      },
      {
        "date": "2021-10-28T10:30:00.000Z",
        "voteCount": 2,
        "content": "B \u2013 Make the tables time-partitioned and configure the partition expiration at 45 days.\nA \u2013 if you use table expiration time, then it will remove the whole table after 45 days.\nD \u2013 requires extra work and is not automatic."
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 79,
    "url": "https://www.examtopics.com/discussions/google/view/7323-exam-professional-cloud-architect-topic-1-question-79/",
    "body": "You want your Google Kubernetes Engine cluster to automatically add or remove nodes based on CPU load.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a HorizontalPodAutoscaler with a target CPU usage. Enable the Cluster Autoscaler from the GCP Console.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a HorizontalPodAutoscaler with a target CPU usage. Enable autoscaling on the managed instance group for the cluster using the gcloud command.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a deployment and set the maxUnavailable and maxSurge properties. Enable the Cluster Autoscaler using the gcloud command.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a deployment and set the maxUnavailable and maxSurge properties. Enable autoscaling on the cluster managed instance group from the GCP Console."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 13,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-07-22T02:39:00.000Z",
        "voteCount": 61,
        "content": "Answer: A\nSupport: \nHow does Horizontal Pod Autoscaler work with Cluster Autoscaler?\n\nHorizontal Pod Autoscaler changes the deployment's or replicaset's number of replicas based on the current CPU load. If the load increases, HPA will create new replicas, for which there may or may not be enough space in the cluster. If there are not enough resources, CA will try to bring up some nodes, so that the HPA-created pods have a place to run. If the load decreases, HPA will stop some of the replicas. As a result, some nodes may become underutilized or completely empty, and then CA will terminate such unneeded nodes."
      },
      {
        "date": "2021-12-06T03:43:00.000Z",
        "voteCount": 3,
        "content": "Good Explaination"
      },
      {
        "date": "2023-05-13T12:03:00.000Z",
        "voteCount": 1,
        "content": "Nice and detailed explanation. I agree with A."
      },
      {
        "date": "2022-10-16T10:28:00.000Z",
        "voteCount": 1,
        "content": "Nice and detailed explanation. I agree with A."
      },
      {
        "date": "2023-08-24T06:08:00.000Z",
        "voteCount": 1,
        "content": "very well explained"
      },
      {
        "date": "2020-01-26T08:38:00.000Z",
        "voteCount": 25,
        "content": "i'm for A, but the question in ambiguous, because requires the autoscale of nodes (not pod) when the cpu overload, but in answer use k8s pod autoscaler based on cpu load ( cpu load for pod, not nodes ). strange"
      },
      {
        "date": "2020-05-12T23:35:00.000Z",
        "voteCount": 6,
        "content": "Confuse with the question like you mentioned.  Autoscale is via nodes not pod.. and can only be configure using gcloud command."
      },
      {
        "date": "2020-11-15T02:13:00.000Z",
        "voteCount": 20,
        "content": "Agreed, the question is not about pods, but answers are also talking about pods (not only)\nA is correct because B is wrong according to \nhttps://cloud.google.com/kubernetes-engine/docs/concepts/cluster-autoscaler\n\n\"Caution: Do not enable Compute Engine autoscaling for managed instance groups for your cluster nodes. GKE's cluster autoscaler is separate from Compute Engine autoscaling\""
      },
      {
        "date": "2023-05-13T12:03:00.000Z",
        "voteCount": 1,
        "content": "Nice and detailed explanation. I agree with A."
      },
      {
        "date": "2023-05-08T02:34:00.000Z",
        "voteCount": 1,
        "content": "A seems correct. y to create managed instance groups unnecessarily?"
      },
      {
        "date": "2023-02-27T17:21:00.000Z",
        "voteCount": 1,
        "content": "The answer is A.\nMore nodes mean it's horizontal scaling (increase VMs means vertical scaling of infrastructure). Cluster AutoScalar is used for increasing number of nodes."
      },
      {
        "date": "2022-12-28T23:03:00.000Z",
        "voteCount": 2,
        "content": "A is the Correct answer, Horizontal Pod Autoscaler and Cluster Autoscaler can be used together to provision new pods and new nodes as per the CPU utilization.\n\nhttps://www.youtube.com/watch?v=VNAWA6NkoBs"
      },
      {
        "date": "2022-11-05T03:01:00.000Z",
        "voteCount": 1,
        "content": "ok for A"
      },
      {
        "date": "2022-10-07T17:36:00.000Z",
        "voteCount": 1,
        "content": "MIG not for GKE as option B and C, D are not relevant to question"
      },
      {
        "date": "2022-09-15T12:54:00.000Z",
        "voteCount": 1,
        "content": "Configure a HorizontalPodAutoscaler with a target CPU usage. Enable the Cluster Autoscaler from the GCP Console."
      },
      {
        "date": "2022-09-08T22:49:00.000Z",
        "voteCount": 1,
        "content": "A...\n\nThe HPA and CA complement each other for truly efficient scaling. If the load increases, HPA will create new replicas. If there isn\u2019t enough space for these replicas, CA will provision some nodes, so that the HPA-created pods have a place to run.\n\nThe Horizontal Pod Autoscaler changes the shape of your Kubernetes workload by automatically increasing or decreasing the number of Pods in response to the workload's CPU or memory consumption, or in response to custom metrics reported from within Kubernetes or external metrics from sources outside of your cluster."
      },
      {
        "date": "2022-08-24T21:25:00.000Z",
        "voteCount": 2,
        "content": "A is wrong.\nPod scaling only spins up additional pods. Not nodes.\nCluster Autoscaler does adding of nodes automatically.\nI am surprised that so many people think that A is the correct answer.\n\nCorrect answer per me is C"
      },
      {
        "date": "2022-08-04T04:41:00.000Z",
        "voteCount": 2,
        "content": "Horizontal Pod Autoscaler changes the deployment's or replicaset's number of replicas based on the current CPU load. If the load increases, HPA will create new replicas, for which there may or may not be enough space in the cluster. If there are not enough resources, CA will try to bring up some nodes, so that the HPA-created pods have a place to run. If the load decreases, HPA will stop some of the replicas. As a result, some nodes may become underutilized or completely empty, and then CA will terminate such unneeded nodes."
      },
      {
        "date": "2022-02-11T13:34:00.000Z",
        "voteCount": 1,
        "content": "I got one question on my exam which showed autoscaling configuration and was asked to select correct configuration."
      },
      {
        "date": "2022-01-04T02:04:00.000Z",
        "voteCount": 1,
        "content": "I agree A is correct.\nI found quicklab.\nUnderstanding and Combining GKE Autoscaling Strategies."
      },
      {
        "date": "2021-12-27T17:49:00.000Z",
        "voteCount": 5,
        "content": "B and D: You must never change the GKE managed instance group.\nC and D: maxUnavailable and maxSurge are used for rolling update\nA. It is the correct."
      },
      {
        "date": "2021-12-07T01:27:00.000Z",
        "voteCount": 1,
        "content": "Go for A"
      },
      {
        "date": "2021-10-28T10:57:00.000Z",
        "voteCount": 6,
        "content": "Create Horizontal Autoscaler (min,max for pods):\nkubectl autoscale deployment my-app --max 6 --min 4 --cpu-percent 50\nAutoscaling cluster:\ngcloud container clusters create example-cluster \\\n--zone us-central1-a \\\n--node-locations us-central1-a,us-central1-b,us-central1-f \\\n--num-nodes 2 --enable-autoscaling --min-nodes 1 --max-nodes 4\nCheck scaling an application and Horizontal Pod Autoscaler: \nhttps://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/\nManual Cluster Resizing: https://cloud.google.com/kubernetes-engine/docs/how-to/resizing-a-cluster\nhttps://cloud.google.com/kubernetes-engine/docs/how-to/scaling-apps"
      },
      {
        "date": "2021-10-28T10:58:00.000Z",
        "voteCount": 1,
        "content": "Correct answer A."
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 80,
    "url": "https://www.examtopics.com/discussions/google/view/6399-exam-professional-cloud-architect-topic-1-question-80/",
    "body": "You need to develop procedures to verify resilience of disaster recovery for remote recovery using GCP. Your production environment is hosted on-premises. You need to establish a secure, redundant connection between your on-premises network and the GCP network.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tVerify that Dedicated Interconnect can replicate files to GCP. Verify that direct peering can establish a secure connection between your networks if Dedicated Interconnect fails.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tVerify that Dedicated Interconnect can replicate files to GCP. Verify that Cloud VPN can establish a secure connection between your networks if Dedicated Interconnect fails.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tVerify that the Transfer Appliance can replicate files to GCP. Verify that direct peering can establish a secure connection between your networks if the Transfer Appliance fails.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tVerify that the Transfer Appliance can replicate files to GCP. Verify that Cloud VPN can establish a secure connection between your networks if the Transfer Appliance fails."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 10,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2019-10-10T03:42:00.000Z",
        "voteCount": 44,
        "content": "I think B is correct answer."
      },
      {
        "date": "2020-08-06T18:09:00.000Z",
        "voteCount": 8,
        "content": "B is ok"
      },
      {
        "date": "2020-11-01T08:03:00.000Z",
        "voteCount": 6,
        "content": "Its quite a fun to use Transfer Appliance for DR, I think answer is B"
      },
      {
        "date": "2023-05-08T02:39:00.000Z",
        "voteCount": 1,
        "content": "Actually, how ca this be given as a option even?"
      },
      {
        "date": "2021-03-04T17:55:00.000Z",
        "voteCount": 1,
        "content": "only B works"
      },
      {
        "date": "2019-10-17T04:27:00.000Z",
        "voteCount": 27,
        "content": "Agree B is correct. Transfer appliance is a physical appliance for transferring huge bulk of data. does not fit into disaster recovery testing. out of A and B, B seems to be more nearest answer. One would not have direct peering and Dedicated interconnect in a solution"
      },
      {
        "date": "2024-05-16T12:31:00.000Z",
        "voteCount": 3,
        "content": "I go to B, because direct peering anyway requires VPN connection if you want to get access to VPC."
      },
      {
        "date": "2023-12-16T19:30:00.000Z",
        "voteCount": 2,
        "content": "Why you guys are choosing VPN? the reason to use Dedicated Interconnect is to have the max bandwidth available, does VPN give you that option in the first place? why not thinking about separate direct peering connection which might give a better performance than VPN?"
      },
      {
        "date": "2023-12-16T19:31:00.000Z",
        "voteCount": 1,
        "content": "BTW with direct peering you are going through the service provider network which makes more sense to get different connectivity option"
      },
      {
        "date": "2024-04-10T11:40:00.000Z",
        "voteCount": 2,
        "content": "Direct peering allows only on premises to connect to Google services in GCP . If needed to connect with Google workspace where all projects hosted they dedicated or partner interconnect is required. \nhttps://cloud.google.com/network-connectivity/docs/direct-peering\n\nB is the best answer"
      },
      {
        "date": "2023-10-05T22:35:00.000Z",
        "voteCount": 1,
        "content": "You need to develop procedures to verify resilience of disaster recovery for remote recovery using GCP. Your production environment is hosted on-premises. You need to establish a secure, redundant connection between your on-premises network and the GCP network.\nWhat should you do?\n\nA. Verify that Dedicated Interconnect can replicate files to GCP. Verify that direct peering can establish a secure connection between your networks if Dedicated Interconnect fails.\n\nB. Verify that Dedicated Interconnect can replicate files to GCP. Verify that Cloud VPN can establish a secure connection between your networks if Dedicated Interconnect fails.\n\nWhy Not A, as question asks \"to establish a secure, redundant connection between your on-premises network and the GCP network.\"\n\nIs VPN considered more reliable than Direct Peering?? Both VPN and Direct Peering will provide redundant connection.\nI am not concerned about cost Direct Interconnect is already there."
      },
      {
        "date": "2023-07-01T10:02:00.000Z",
        "voteCount": 1,
        "content": "B is right,"
      },
      {
        "date": "2022-12-22T01:09:00.000Z",
        "voteCount": 7,
        "content": "The correct answer is B. Verify that Dedicated Interconnect can replicate files to GCP. Verify that Cloud VPN can establish a secure connection between your networks if Dedicated Interconnect fails.\n\nDedicated Interconnect is a connection that provides a private, dedicated connection between your on-premises network and GCP over a Google-owned network. It is a secure and reliable option for connecting your on-premises network to GCP. You can use it to replicate files to GCP as a part of your disaster recovery plan.\n\nIf Dedicated Interconnect fails for any reason, it is a good idea to have a backup solution in place to establish a secure connection between your networks. Cloud VPN is a secure and reliable solution for establishing a connection between your on-premises network and GCP. It uses a virtual private network (VPN) tunnel to securely connect the networks, and it is a good backup option if Dedicated Interconnect fails."
      },
      {
        "date": "2022-12-22T01:09:00.000Z",
        "voteCount": 1,
        "content": "The Transfer Appliance is a physical storage device that you can use to transfer large amounts of data from your on-premises storage to GCP. It is not a connection option and cannot be used to establish a secure connection between your on-premises network and GCP. Therefore, the options C and D are not correct."
      },
      {
        "date": "2023-11-30T07:19:00.000Z",
        "voteCount": 2,
        "content": "Why not A? Is Cloud VPN better than Direct Peering in this scenario?"
      },
      {
        "date": "2022-11-09T09:38:00.000Z",
        "voteCount": 1,
        "content": "B is ok"
      },
      {
        "date": "2022-10-17T02:02:00.000Z",
        "voteCount": 1,
        "content": "For DR with Google Cloud and on-prem use Dedicated Interconnect with HA VPN"
      },
      {
        "date": "2022-10-16T10:27:00.000Z",
        "voteCount": 1,
        "content": "B is right without any second thought. Question is straight forward."
      },
      {
        "date": "2022-10-07T17:40:00.000Z",
        "voteCount": 1,
        "content": "Transfer appliance you need to carry to GCP center like water bottle :)"
      },
      {
        "date": "2022-09-15T12:53:00.000Z",
        "voteCount": 1,
        "content": "Verify that Dedicated Interconnect can replicate files to GCP. Verify that direct peering can establish a secure connection between your networks if Dedicated Interconnect fails."
      },
      {
        "date": "2022-09-10T19:32:00.000Z",
        "voteCount": 3,
        "content": "It is definitely B\n1. Interconnect is the first option so that is right.\n2. Eliminates A, since Direct Peering is not supported in GCP, the option is Google Cloud VPN connection to onpremises site."
      },
      {
        "date": "2023-03-08T07:49:00.000Z",
        "voteCount": 2,
        "content": "GCP supports direct peering in 100 locations"
      },
      {
        "date": "2022-08-04T04:44:00.000Z",
        "voteCount": 1,
        "content": "Transfer appliance is a physical appliance for transferring huge bulk of data. does not fit into disaster recovery testing"
      },
      {
        "date": "2022-07-26T04:12:00.000Z",
        "voteCount": 1,
        "content": "only B have redundacy"
      },
      {
        "date": "2021-12-07T01:40:00.000Z",
        "voteCount": 4,
        "content": "Go for B.\nOnly when u need connect con G.Suite applications you must use Peering."
      },
      {
        "date": "2021-11-28T02:07:00.000Z",
        "voteCount": 1,
        "content": "B is the correct answer"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 81,
    "url": "https://www.examtopics.com/discussions/google/view/6896-exam-professional-cloud-architect-topic-1-question-81/",
    "body": "Your company operates nationally and plans to use GCP for multiple batch workloads, including some that are not time-critical. You also need to use GCP services that are HIPAA-certified and manage service costs.<br>How should you design to meet Google best practices?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tProvision preemptible VMs to reduce cost. Discontinue use of all GCP services and APIs that are not HIPAA-compliant.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tProvision preemptible VMs to reduce cost. Disable and then discontinue use of all GCP services and APIs that are not HIPAA-compliant.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tProvision standard VMs in the same region to reduce cost. Discontinue use of all GCP services and APIs that are not HIPAA-compliant.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tProvision standard VMs to the same region to reduce cost. Disable and then discontinue use of all GCP services and APIs that are not HIPAA-compliant."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 16,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 5,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2019-10-27T09:42:00.000Z",
        "voteCount": 40,
        "content": "Disabling and then discontinuing allows you to see the effects of not using the APIs, so you can gauge (check) alternatives. So that leaves B and D as viable answers. The question says only some are not time-critical which implies others are... this means preemptible VMs are good because they will secure a spot for scaling when needed. So I'm also going to choose B."
      },
      {
        "date": "2020-08-02T02:12:00.000Z",
        "voteCount": 11,
        "content": "If others are time-critical, preemtible does not fit. Answer is D."
      },
      {
        "date": "2021-04-02T06:46:00.000Z",
        "voteCount": 6,
        "content": "No mention of others in the question. In an exam it's important to not being in individual assumptions and focus on the information in question. Key word here is \"not time-critical\""
      },
      {
        "date": "2022-10-16T10:25:00.000Z",
        "voteCount": 2,
        "content": "agree otherwise answer goes to non-preemtible VM's"
      },
      {
        "date": "2023-05-08T02:50:00.000Z",
        "voteCount": 2,
        "content": "Ver well said...\"In an exam it's important to not being in individual assumptions and focus on the information in question. Key word here is \"not time-critical\"\""
      },
      {
        "date": "2020-09-13T01:18:00.000Z",
        "voteCount": 3,
        "content": "And the others are not spoken about. By taking the question just by the context that it sets, preemptible  is what I choose. So it's B according to me."
      },
      {
        "date": "2023-05-12T03:06:00.000Z",
        "voteCount": 1,
        "content": "Correctly explained"
      },
      {
        "date": "2022-10-16T10:24:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2024-02-11T10:18:00.000Z",
        "voteCount": 1,
        "content": "Why you decided to emphasize on key word \"not time-critical\" but not\" \"operates nationally\"?"
      },
      {
        "date": "2020-07-14T06:57:00.000Z",
        "voteCount": 19,
        "content": "They say that some (not all) of the Batch workloads are not time critical which implies that there are time critical Batch workloads for which Preemptible VMs are not appropriate, so going with D as the answer"
      },
      {
        "date": "2020-08-04T07:56:00.000Z",
        "voteCount": 10,
        "content": "I dont think it means use premtible vms for everything. It says to use preemtible vms to reduce cost"
      },
      {
        "date": "2024-07-04T06:28:00.000Z",
        "voteCount": 1,
        "content": "D is the correct answer and for the following regions:\n1. Deploying a read replica and then manually failing over by stopping the current cloud SQL instance will only impact the authentication layer, which is what we're looking to test.\n2. Stopping all VM's won't have an impact on the authentication layer testing becasuse Cloud SQL is a PaaS service, you cannot just turn off the VM's"
      },
      {
        "date": "2024-06-17T08:37:00.000Z",
        "voteCount": 1,
        "content": "What is the difference between A and B other than adding the obvious that you'd disable them if you discontinue using them. This is the most obnoxiously confusing question I've ever read and someone else pointed out that *some* are not time critical which implies others are time critical."
      },
      {
        "date": "2024-06-02T11:44:00.000Z",
        "voteCount": 1,
        "content": "If the question would have said: \"The batch are not time-critical\", then Option B with preentible VMs.\n\nBUT, it clearly points that only SOME are not time-critical\". Option D is the only one that satisfies all conditions"
      },
      {
        "date": "2023-11-19T23:29:00.000Z",
        "voteCount": 1,
        "content": "D\nAs per the requirement, some of the batches are not time critical - which means some are critical. Choosing preemptible VMs may mean time critical batches may be affected. Cost effective solutions should not come at cost of requirements. \nDisabling and then discontinuing allows you to see the effects of not using the APIs, so you can gauge (check) alternatives."
      },
      {
        "date": "2023-10-13T05:52:00.000Z",
        "voteCount": 2,
        "content": "When it is \"some\" batches are non-critical - when we want to focus on price reduction and if it is batch apps, then we can definitely choose \"Pre-emptible VMs\". Also non-compliant APIs needs to be disabled for sure (APIs can be enabled if there is a need).\n\nPutting altogether, B is the right answer"
      },
      {
        "date": "2023-08-05T16:47:00.000Z",
        "voteCount": 3,
        "content": "I think the answer should be D. some of the batches are not time critical which means some are. Choosing premtible VMs may mean time critical batches may be affected in some cases. Even though the solution needs to be cost effective, it should not come at cost of requirements. Hence D"
      },
      {
        "date": "2024-04-10T12:07:00.000Z",
        "voteCount": 1,
        "content": "Premptible VMs is better for batch workloads which are critical  and not critical since it takes bit longer to complete the load when some VMs are preempted . It greatly reduces cost with using preempted vms for batch workloads. \n\nB is the better answer"
      },
      {
        "date": "2023-05-08T02:49:00.000Z",
        "voteCount": 1,
        "content": "B looks good to me"
      },
      {
        "date": "2023-02-15T16:38:00.000Z",
        "voteCount": 1,
        "content": "To design a GCP solution that meets Google's best practices for operating nationally with multiple batch workloads, including some that are not time-critical, and using HIPAA-compliant services while managing service costs, you should provision standard VMs in the same region to reduce cost, and use GCP services that are HIPAA-compliant as needed. Therefore, the correct option is C.\n\nPreemptible VMs can provide cost savings, but they are not recommended for workloads that are not time-critical, as they may be interrupted at any time. Provisioning standard VMs in the same region will provide better performance and stability, and can still be cost-effective by using features such as sustained-use discounts and committed use discounts."
      },
      {
        "date": "2022-11-09T09:56:00.000Z",
        "voteCount": 1,
        "content": "B is ok"
      },
      {
        "date": "2022-10-14T07:09:00.000Z",
        "voteCount": 4,
        "content": "Assumption here is that cost is more important that the time critical batches, therefore use preemptible instances. Disable and discontinue is a better option as it gives you opportunity to see the impact before blasting any APIs or services that are not certified."
      },
      {
        "date": "2022-10-03T06:12:00.000Z",
        "voteCount": 2,
        "content": "ans is B - HAHAHA"
      },
      {
        "date": "2022-08-04T04:51:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/security/compliance/hipaa#unique_features"
      },
      {
        "date": "2022-07-23T05:03:00.000Z",
        "voteCount": 1,
        "content": "vote B\nit's obviously, keywords \"multiple batch workloads\" and \"not time-critical\", preemptible vm first choice."
      },
      {
        "date": "2022-06-22T10:31:00.000Z",
        "voteCount": 1,
        "content": "I think answer is D cos for using preemtible instances, you need to guarantee batch jobs are able to continue without losing data or causing an issue when vm restarts. There is no that guarantee in the question."
      },
      {
        "date": "2022-04-18T06:13:00.000Z",
        "voteCount": 2,
        "content": "It took me a while to finally decided the answer should be B, for the following reasons :\n- in the question it said \"multiple batch workloads\" it doesn't matter if it's critical or not it's still patching , then we need to pick Preemptable VM \n\n- from GCP documentation ( https://cloud.google.com/security/compliance/hipaa#unique_features ) , they explicitly   talked about Preemabtible VM covered by the HIPAA , and this question want t make sure that we know this info."
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 82,
    "url": "https://www.examtopics.com/discussions/google/view/6709-exam-professional-cloud-architect-topic-1-question-82/",
    "body": "Your customer wants to do resilience testing of their authentication layer. This consists of a regional managed instance group serving a public REST API that reads from and writes to a Cloud SQL instance.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEngage with a security company to run web scrapers that look your for users' authentication data om malicious websites and notify you if any is found.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy intrusion detection software to your virtual machines to detect and log unauthorized access.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSchedule a disaster simulation exercise during which you can shut off all VMs in a zone to see how your application behaves.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a read replica for your Cloud SQL instance in a different zone than the master, and then manually trigger a failover while monitoring KPIs for our REST API."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 48,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 27,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2019-12-03T04:16:00.000Z",
        "voteCount": 60,
        "content": "As per google documentation(https://cloud.google.com/solutions/scalable-and-resilient-apps) answer is C.\n\nC: A well-designed application should scale seamlessly as demand increases and decreases, and be resilient enough to withstand the loss of one or more compute resources.\nResilience: designed to withstand the unexpected\nA highly-available, or resilient, application is one that continues to function despite expected or unexpected failures of components in the system. If a single instance fails or an entire zone experiences a problem, a resilient application remains fault tolerant\u2014continuing to function and repairing itself automatically if necessary. Because stateful information isn\u2019t stored on any single instance, the loss of an instance\u2014or even an entire zone\u2014should not impact the application\u2019s performance."
      },
      {
        "date": "2020-05-31T13:35:00.000Z",
        "voteCount": 6,
        "content": "Shutting off all VMs in a zone is not good approach for testing of authentication"
      },
      {
        "date": "2021-12-17T17:02:00.000Z",
        "voteCount": 17,
        "content": "You're not testing *authentication*, you're testing *the resilience of the authentication layer*. \"A resilient app is one that continues to function despite failures of system components\" (https://cloud.google.com/architecture/scalable-and-resilient-apps#resilience_designing_to_withstand_failures) - such as shutting down all VMs in a zone."
      },
      {
        "date": "2022-06-01T20:47:00.000Z",
        "voteCount": 4,
        "content": "Agree, Chaos testing is to shutdown random instances."
      },
      {
        "date": "2023-08-24T06:17:00.000Z",
        "voteCount": 2,
        "content": "yes chaos testing is industry standard"
      },
      {
        "date": "2019-11-16T18:51:00.000Z",
        "voteCount": 16,
        "content": "Since the question is asking to do a resilience testing, I prefer C."
      },
      {
        "date": "2020-09-13T01:21:00.000Z",
        "voteCount": 4,
        "content": "Resilience testing of the \"Authentication Layer\", not the \"Application\". So the answer is B."
      },
      {
        "date": "2024-09-29T07:57:00.000Z",
        "voteCount": 1,
        "content": "Option C, which involves scheduling a disaster simulation exercise to shut off all VMs in a zone, is indeed a strong choice for resilience testing. This approach helps you understand how your application behaves under failure conditions and ensures that your system can handle unexpected disruptions.\n\nHowever, Option D is also highly relevant. Configuring a read replica for your Cloud SQL instance in a different zone and manually triggering a failover while monitoring KPIs for your REST API directly tests the resilience of your database layer. This can provide valuable insights into the failover process and the impact on your application\u2019s performance and availability.\n\nBoth options have their merits, but if the primary goal is to test the resilience of the authentication layer specifically, Option D might be more targeted and effective."
      },
      {
        "date": "2024-05-16T12:39:00.000Z",
        "voteCount": 2,
        "content": "C is correct. It is not D because you are not designing system, your goal is testing existed system"
      },
      {
        "date": "2024-04-24T14:08:00.000Z",
        "voteCount": 3,
        "content": "Chaos testing"
      },
      {
        "date": "2024-04-04T17:30:00.000Z",
        "voteCount": 2,
        "content": "I choose Answer C\nhttps://cloud.google.com/sql/docs/mysql/replication  \nThis URL states \"Read replicas are read-only; you cannot write to them. The read replica processes queries, read requests, and analytics traffic, thus reducing the load on the primary instance.\"  \n\"Note: Read replicas do not provide failover capability. To provide failover capability for an instance, see Configuring an instance for high availability.\"\n\"As a best practice, put read replicas in a different zone than the primary instance when you use HA on your primary instance. This practice ensures that read replicas continue to operate when the zone that contains the primary instance has an outage. See the Overview of high availability for more information.\""
      },
      {
        "date": "2024-03-27T02:48:00.000Z",
        "voteCount": 1,
        "content": "Testing Database Resilience: By setting up a read replica in a different zone and triggering a manual failover, you simulate a failure of the primary database. This allows you to assess how well your authentication layer and the overall application cope with the loss of the primary database.\n\nMonitoring Performance and Availability: During the failover, monitoring key performance indicators (KPIs) for your REST API will give you insights into how the application's performance and availability are impacted. This helps in identifying potential bottlenecks and areas for improvement in your resilience strategy.\n\nEnsuring Data Continuity: A read replica ensures data continuity and minimizes downtime, which is critical for an authentication system. The replica will take over as the primary database during the failover, ensuring that the authentication service remains functional."
      },
      {
        "date": "2024-03-19T04:02:00.000Z",
        "voteCount": 1,
        "content": "D is okay"
      },
      {
        "date": "2024-02-15T16:01:00.000Z",
        "voteCount": 2,
        "content": "Authentication layer resiliency can be covered as part of overall application resiliency testing. Option C is asking to use read replica which is not useful in case of testing resiliency in case of failure"
      },
      {
        "date": "2024-02-11T09:55:00.000Z",
        "voteCount": 3,
        "content": "Read replicas do not provide failover capability. \nhttps://cloud.google.com/sql/docs/mysql/replication#read-replicas"
      },
      {
        "date": "2024-01-21T11:50:00.000Z",
        "voteCount": 2,
        "content": "It is c"
      },
      {
        "date": "2023-12-25T22:58:00.000Z",
        "voteCount": 6,
        "content": "Read replica do not provide failover capability \nhttps://cloud.google.com/sql/docs/mysql/replication#:~:text=Note%3A%20Read%20replicas%20do%20not,HA%20on%20your%20primary%20instance."
      },
      {
        "date": "2023-12-14T06:51:00.000Z",
        "voteCount": 2,
        "content": "C is the good choice"
      },
      {
        "date": "2023-11-03T09:11:00.000Z",
        "voteCount": 6,
        "content": "I choose C.\nI don't say D because the REST API read and WRITE in the database, if you create a READ replica in Cloud SQL, the REST API will not have the possibility of write in the database. The answer D doesn't mention anything about promote the read replica to master."
      },
      {
        "date": "2023-12-15T22:50:00.000Z",
        "voteCount": 1,
        "content": "Exactly. Because in GCP a read replica cant be auto upgraded to become a master in case of failover. So basically the database will allow only READ operations and not WRITE operations. Basically leaving it non-functional"
      },
      {
        "date": "2023-10-16T14:24:00.000Z",
        "voteCount": 2,
        "content": "C is good"
      },
      {
        "date": "2023-10-04T23:43:00.000Z",
        "voteCount": 3,
        "content": "D can be correct if it's a Failover replica but it's a read replica and not have anything to do with resiliency. It's C"
      },
      {
        "date": "2023-09-27T06:29:00.000Z",
        "voteCount": 3,
        "content": "Option A is focused on external threat intelligence and is more suited to security testing rather than resilience testing.\n\nOption B is related to security monitoring, and while important, does not directly address resilience testing requirements.\n\nOption C simulates a zone failure scenario. This could provide insights into how the application behaves in a failure scenario, making it a valid resilience testing method. However, it does not specifically address the interaction with Cloud SQL.\n\nOption D directly addresses resilience testing involving a Cloud SQL instance by creating a read replica in a different zone and simulating a failover. This will allow you to assess the impact on your REST API and verify whether the authentication layer remains functional and available, even when the primary Cloud SQL instance is inaccessible."
      },
      {
        "date": "2024-01-27T02:47:00.000Z",
        "voteCount": 1,
        "content": "I think answer should be C, because Read Replicas are there for improving scalability and not availability. So whenever, a node/zone goes down, a read replica wont auto transform to a master. For that you need a failover replica by enabling HA configuration in VMs."
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 83,
    "url": "https://www.examtopics.com/discussions/google/view/8378-exam-professional-cloud-architect-topic-1-question-83/",
    "body": "Your BigQuery project has several users. For audit purposes, you need to see how many queries each user ran in the last month. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConnect Google Data Studio to BigQuery. Create a dimension for the users and a metric for the amount of queries per user.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the BigQuery interface, execute a query on the JOBS table to get the required information.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse 'bq show' to list all jobs. Per job, use 'bq ls' to list job information and get the required information.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Audit Logging to view Cloud Audit Logs, and create a filter on the query operation to get the required information.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 26,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 21,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-04-12T09:28:00.000Z",
        "voteCount": 48,
        "content": "D- reasons:\n1.-Cloud Audit Logs maintains audit logs for admin activity, data access and system events. BIGQUERY is automatically send to cloud audit log functionality.\n2.- In the filter you can filter relevant BigQuery Audit messages, you can express filters as part of the export\n\nhttps://cloud.google.com/logging/docs/audit\nhttps://cloud.google.com/bigquery/docs/reference/auditlogs#ids\nhttps://cloud.google.com/bigquery/docs/reference/auditlogs#auditdata_examples"
      },
      {
        "date": "2020-07-25T13:34:00.000Z",
        "voteCount": 9,
        "content": "D is the right as you can get the monthly view of the query usage across all the users and projects for auditing purpose. C does need appropriate permission to see the detail level data. Monthly view is tough to get directly from the bq ls or bq show commands."
      },
      {
        "date": "2023-08-24T06:19:00.000Z",
        "voteCount": 1,
        "content": "Answer is D"
      },
      {
        "date": "2020-05-04T16:30:00.000Z",
        "voteCount": 27,
        "content": "Answer is D:\nhttps://cloud.google.com/bigquery/docs/reference/auditlogs#example_query_cost_breakdown_by_identity"
      },
      {
        "date": "2021-12-23T08:10:00.000Z",
        "voteCount": 2,
        "content": "Nailed it"
      },
      {
        "date": "2022-11-07T05:32:00.000Z",
        "voteCount": 1,
        "content": "No mention about exporting to bq"
      },
      {
        "date": "2024-07-26T13:04:00.000Z",
        "voteCount": 4,
        "content": "Using the INFORMATION_SCHEMA.JOBS_BY_USER table within BigQuery is the most efficient and straightforward method to get the required audit information about the number of queries each user ran in the last month. Therefore, option B is the best choice.. D.While Cloud Audit Logs can provide detailed logs of activities, querying them directly for this purpose is less efficient than using the JOBS table in BigQuery. Additionally, setting up and querying audit logs involves more steps and may require exporting logs to BigQuery for complex queries."
      },
      {
        "date": "2024-07-26T13:01:00.000Z",
        "voteCount": 3,
        "content": "Why B is the Best Answer:\n\nDirect Access to Job Metadata: BigQuery maintains metadata about jobs (including query jobs) in the INFORMATION_SCHEMA views, specifically in the INFORMATION_SCHEMA.JOBS table.\nDetailed Information: This table contains information about all jobs, including who ran them, when they were run, and the type of job. This makes it easy to filter and count queries by user.\nQuerying JOBS Table: You can write a SQL query to count the number of queries executed by each user over the specified period."
      },
      {
        "date": "2024-06-04T23:25:00.000Z",
        "voteCount": 3,
        "content": "Querying the INFORMATION_SCHEMA.JOBS_BY_USER view in BigQuery is the most efficient and straightforward way to obtain the number of queries each user ran in the last month. This method leverages built-in BigQuery capabilities designed specifically for auditing and monitoring query jobs.\nCloud Audit Logs provide detailed logging information but are more complex to query for specific metrics like the number of queries run by each user. BigQuery\u2019s INFORMATION_SCHEMA.JOBS_BY_USER is designed for this purpose and is easier to use for querying job data."
      },
      {
        "date": "2024-06-02T11:52:00.000Z",
        "voteCount": 1,
        "content": "Audit logs, Option D"
      },
      {
        "date": "2024-02-10T02:20:00.000Z",
        "voteCount": 1,
        "content": "reason:\nhttps://cloud.google.com/logging/docs/audit#data-access\n\nData Access audit logs\u2014except for BigQuery Data Access audit logs\u2014are disabled by default because audit logs can be quite large. If you want Data Access audit logs to be written for Google Cloud services other than BigQuery, you must explicitly enable them"
      },
      {
        "date": "2024-01-16T06:14:00.000Z",
        "voteCount": 2,
        "content": "I finally decide to go with Option D over B  because we or the auditor might not have access to the metadata. In fact, in our project, not all of us had access to query this view.\n\n\"To get the permission that you need to query the INFORMATION_SCHEMA.JOBS view, ask your administrator to grant you the BigQuery Resource Viewer\"\nhttps://cloud.google.com/bigquery/docs/information-schema-jobs#required_role.\n\n(And not because of the wordings \"Table\" instead of \"view\" - don't think an architect exam will try to assess your memory of whether it is a table or a view or your understanding of the difference between a table and a view)."
      },
      {
        "date": "2024-01-14T04:27:00.000Z",
        "voteCount": 1,
        "content": "C - bq show: To view job details (https://cloud.google.com/bigquery/docs/managing-jobs#view_job_details_2)\nbq ls: To list jobs (https://cloud.google.com/bigquery/docs/managing-jobs#list_jobs)\n\nSo D is the correct one."
      },
      {
        "date": "2023-12-12T05:47:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/bigquery/docs/managing-jobs#list_jobs_in_a_project"
      },
      {
        "date": "2023-09-18T22:34:00.000Z",
        "voteCount": 1,
        "content": "I think B is the correct answer"
      },
      {
        "date": "2023-05-15T09:47:00.000Z",
        "voteCount": 1,
        "content": "Cloud Logging"
      },
      {
        "date": "2023-05-07T23:07:00.000Z",
        "voteCount": 5,
        "content": "B is correct. here's the link - https://cloud.google.com/bigquery/docs/information-schema-jobs"
      },
      {
        "date": "2023-04-21T10:02:00.000Z",
        "voteCount": 2,
        "content": "JOBS system table does exist and it contains exactly the info we need: one record for each job executed by users (query is one of the type of the jobs)"
      },
      {
        "date": "2024-07-22T12:04:00.000Z",
        "voteCount": 1,
        "content": "Yes, but this is assuming you have the required role of BigQuery Resource Viewer which is needed and does not clarify in the question! So does that make D the right answer?  And with D, you need the logs viewer role.  The question is a bad one as it doesn't clarify any roles in this scenario."
      },
      {
        "date": "2023-04-19T19:09:00.000Z",
        "voteCount": 3,
        "content": "D. Use Cloud Audit Logging to view Cloud Audit Logs, and create a filter on the query operation to get the required information.\n\nCloud Audit Logging records activities and API calls in Google Cloud services, including BigQuery. You can use Cloud Audit Logging to view logs and filter them based on specific operations, such as queries in BigQuery. By filtering on the query operation, you can gather the required information about how many queries each user ran in the last month, which is essential for audit purposes."
      },
      {
        "date": "2023-01-08T07:38:00.000Z",
        "voteCount": 4,
        "content": "A is not possible.\nB is possible if VIEW is used instead of TABLE in the description. I use this view to get this information regularly.\nC. I have no cloud how this can be right answer.\nD. Only possible as per text descriptions."
      },
      {
        "date": "2023-04-21T10:04:00.000Z",
        "voteCount": 2,
        "content": "JOBS being read-only for the user doesn't make it a view."
      },
      {
        "date": "2023-01-06T13:49:00.000Z",
        "voteCount": 1,
        "content": "d is correct"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 84,
    "url": "https://www.examtopics.com/discussions/google/view/6873-exam-professional-cloud-architect-topic-1-question-84/",
    "body": "You want to automate the creation of a managed instance group. The VMs have many OS package dependencies. You want to minimize the startup time for new<br>VMs in the instance group.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Terraform to create the managed instance group and a startup script to install the OS package dependencies.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a custom VM image with all OS package dependencies. Use Deployment Manager to create the managed instance group with the VM image.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Puppet to create the managed instance group and install the OS package dependencies.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Deployment Manager to create the managed instance group and Ansible to install the OS package dependencies."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2019-10-21T00:51:00.000Z",
        "voteCount": 42,
        "content": "Why is it not answer B?"
      },
      {
        "date": "2020-11-01T08:16:00.000Z",
        "voteCount": 6,
        "content": "B is the answer,"
      },
      {
        "date": "2020-01-20T09:29:00.000Z",
        "voteCount": 10,
        "content": "It is."
      },
      {
        "date": "2020-08-06T19:48:00.000Z",
        "voteCount": 11,
        "content": "B is ok"
      },
      {
        "date": "2021-03-04T18:04:00.000Z",
        "voteCount": 4,
        "content": "It is B"
      },
      {
        "date": "2019-11-14T09:10:00.000Z",
        "voteCount": 21,
        "content": "B- minimal start time means a pre-baked golden image"
      },
      {
        "date": "2022-12-22T01:34:00.000Z",
        "voteCount": 11,
        "content": "The correct answer is B. Create a custom VM image with all OS package dependencies. Use Deployment Manager to create the managed instance group with the VM image.\n\nManaged instance groups are a way to manage a group of Compute Engine instances as a single entity. If you want to automate the creation of a managed instance group, you can use tools such as Terraform, Deployment Manager, or Puppet to automate the process.\n\nTo minimize the startup time for new VMs in the instance group, you should create a custom VM image with all of the OS package dependencies pre-installed. This will allow you to create new VMs from the custom image, which will significantly reduce the startup time compared to installing the dependencies on each VM individually. You can then use Deployment Manager to create the managed instance group with the custom VM image."
      },
      {
        "date": "2022-12-22T01:35:00.000Z",
        "voteCount": 4,
        "content": "Option A, using Terraform to create the managed instance group and a startup script to install the OS package dependencies, would not minimize the startup time for new VMs in the instance group. \n\nOption C, using Puppet to create the managed instance group and install the OS package dependencies, would not minimize the startup time for new VMs in the instance group. \n\nOption D, using Deployment Manager to create the managed instance group and Ansible to install the OS package dependencies, would not minimize the startup time for new VMs in the instance group."
      },
      {
        "date": "2022-11-10T00:28:00.000Z",
        "voteCount": 1,
        "content": "B is ok"
      },
      {
        "date": "2022-10-16T09:59:00.000Z",
        "voteCount": 1,
        "content": "B. Create a custom VM image with all OS package dependencies. Use Deployment Manager to create the managed instance group with the VM image."
      },
      {
        "date": "2022-10-14T07:38:00.000Z",
        "voteCount": 1,
        "content": "B will reduce the startup time"
      },
      {
        "date": "2022-08-04T04:59:00.000Z",
        "voteCount": 4,
        "content": "B- minimal start time means a pre-baked golden image"
      },
      {
        "date": "2022-07-18T07:04:00.000Z",
        "voteCount": 1,
        "content": "As someone who works on Terraform. It may not be Googles best practice, even though it's built in just need to be initialized. But it is the easiest way to build and restructure infrastructure with a simple line of code change and a quick shell command to apply terraform. I mean B would work. But it doesn't include the start-up script for the OS dependencies to be loaded. ?&gt;?&gt;? Any feedback?"
      },
      {
        "date": "2022-07-20T09:11:00.000Z",
        "voteCount": 2,
        "content": "Start up scripts aren't need here as you're making a custom OS image with all OS package dependencies.  Question is not asking for the easiest way, it's asking how to minimize VM startup times. Not having to run the startup scripts because it's baked into the image is how I understand and interpret this, therefore B."
      },
      {
        "date": "2022-07-05T13:14:00.000Z",
        "voteCount": 5,
        "content": "06/30/2022 Exam"
      },
      {
        "date": "2022-01-18T12:07:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2021-12-07T02:04:00.000Z",
        "voteCount": 1,
        "content": "Go for B"
      },
      {
        "date": "2021-12-03T02:56:00.000Z",
        "voteCount": 2,
        "content": "yes B is right"
      },
      {
        "date": "2021-11-28T02:37:00.000Z",
        "voteCount": 2,
        "content": "B is the right answer"
      },
      {
        "date": "2021-11-05T08:51:00.000Z",
        "voteCount": 1,
        "content": "go with B. D: it involves so many other third software to configure/manage which makes build more complicated."
      },
      {
        "date": "2021-10-29T09:20:00.000Z",
        "voteCount": 3,
        "content": "B \u2013 create a custom VM instance image with all OS dependencies. Use Deployment Manager to create a MIG with the VM image.\nRead more about Public and Custom VM Images: https://cloud.google.com/compute/docs/images\nCustom images are available in your project only, they don\u2019t add cost to your VM instances, incur image storage cost (0.085$ GB/month)\nD \u2013 could be also an alternative (if to consider requirement to install dependencies in start up script). But, last sentence stresses on \u201cminimize VM\u2019s start up time\u201d. So, B is fastest solution. Also, what is a point to use Ansible if you can complete same task via startup script of Deployment Manager. Ansible won\u2019t make this faster, but just will add 3rd party dependency."
      },
      {
        "date": "2021-05-19T05:56:00.000Z",
        "voteCount": 2,
        "content": "B. Create a custom VM image with all OS package dependencies. Use Deployment Manager to create the managed instance group with the VM image."
      },
      {
        "date": "2021-05-16T01:50:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 85,
    "url": "https://www.examtopics.com/discussions/google/view/6457-exam-professional-cloud-architect-topic-1-question-85/",
    "body": "Your company captures all web traffic data in Google Analytics 360 and stores it in BigQuery. Each country has its own dataset. Each dataset has multiple tables.<br>You want analysts from each country to be able to see and query only the data for their respective countries.<br>How should you configure the access rights?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a group per country. Add analysts to their respective country-groups. Create a single group 'all_analysts', and add all country-groups as members. Grant the 'all_analysts' group the IAM role of BigQuery jobUser. Share the appropriate dataset with view access with each respective analyst country-group.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a group per country. Add analysts to their respective country-groups. Create a single group 'all_analysts', and add all country-groups as members. Grant the 'all_analysts' group the IAM role of BigQuery jobUser. Share the appropriate tables with view access with each respective analyst country-group.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a group per country. Add analysts to their respective country-groups. Create a single group 'all_analysts', and add all country-groups as members. Grant the 'all_analysts' group the IAM role of BigQuery dataViewer. Share the appropriate dataset with view access with each respective analyst country- group.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a group per country. Add analysts to their respective country-groups. Create a single group 'all_analysts', and add all country-groups as members. Grant the 'all_analysts' group the IAM role of BigQuery dataViewer. Share the appropriate table with view access with each respective analyst country-group."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 48,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 15,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2019-11-30T19:21:00.000Z",
        "voteCount": 61,
        "content": "It should be A. The question requires that user from each country can only view a specific data set, so BQ dataViewer cannot be assigned at project level. Only A could limit the user to query and view the data that they are supposed to be allowed to."
      },
      {
        "date": "2023-04-18T07:21:00.000Z",
        "voteCount": 11,
        "content": "Should be C.\n\nhttps://cloud.google.com/bigquery/docs/access-control#bigquery.dataViewer\n\nData viewer role can be applied to a Table and a View. \n\nJobUser can be applied only at a Project level not at a Dataset level\n\nhttps://cloud.google.com/bigquery/docs/access-control#bigquery.jobUser"
      },
      {
        "date": "2023-08-31T14:02:00.000Z",
        "voteCount": 3,
        "content": "incorrect, should be A, BigQuery Job User \n(roles/bigquery.jobUser)\n\nProvides permissions to run jobs, including queries, within the project."
      },
      {
        "date": "2023-09-29T10:41:00.000Z",
        "voteCount": 1,
        "content": "A is wrong"
      },
      {
        "date": "2019-10-19T20:19:00.000Z",
        "voteCount": 32,
        "content": "Should be C\nhttps://cloud.google.com/bigquery/docs/access-control#bigquery.dataViewer\n\nWhen applied to a dataset, dataViewer provides permissions to:\n\nRead the dataset's metadata and to list tables in the dataset.\nRead data and metadata from the dataset's tables.\nWhen applied at the project or organization level, this role can also enumerate all datasets in the project. Additional roles, however, are necessary to allow the running of jobs."
      },
      {
        "date": "2020-05-15T10:22:00.000Z",
        "voteCount": 31,
        "content": "Option C grant read permission to all datasets globally, which violated the request \"You want analysts from each country\nto be able to see and query only the data for their respective countries\"\n\nSo the correct answer is A."
      },
      {
        "date": "2021-10-16T06:07:00.000Z",
        "voteCount": 9,
        "content": "https://cloud.google.com/bigquery/docs/access-control#bigquery.dataViewer\n\"When applied to a dataset..\" you can apply dataViewer role to a specific dataset."
      },
      {
        "date": "2024-03-18T06:00:00.000Z",
        "voteCount": 1,
        "content": "It is C.\nQuestion says analyst should be  able to see and query only the data for their respective countries. BigQueryDta viewer permission will allow only to read and query the table/view data"
      },
      {
        "date": "2024-07-26T13:17:00.000Z",
        "voteCount": 2,
        "content": "You cant query with dataviewer. user with the roles/bigquery.dataViewer role has read-only access to datasets and tables but does not inherently have the permissions to run queries (which are considered jobs in BigQuery). The dataViewer role allows users to view dataset metadata and table contents but does not include the ability to create or execute jobs.The dataViewer role alone does not allow users to run queries. Analysts need the ability to run queries, which requires the jobUser role."
      },
      {
        "date": "2024-02-03T23:20:00.000Z",
        "voteCount": 1,
        "content": "Go with a."
      },
      {
        "date": "2024-01-20T13:33:00.000Z",
        "voteCount": 1,
        "content": "C is right, even if DataViwer is granted on Project level but Dataset is shared with view access to only the country group."
      },
      {
        "date": "2024-01-11T09:34:00.000Z",
        "voteCount": 6,
        "content": "A is the correct answer. Tested the two scenarios, with `jobUser` permissions it does not allow the user to see a dataset. Whereas with `dataViewer` it has permissions for all the datasets. Note the difference is in the initial permission across the project and not per dataset."
      },
      {
        "date": "2024-01-03T13:37:00.000Z",
        "voteCount": 1,
        "content": "It's A because in order to query, on needs the jobUser role. dataViewer doesn't grant the ability to actually query the datasets one has been given access to.\n\nhttps://cloud.google.com/bigquery/docs/running-queries#required_permissions"
      },
      {
        "date": "2023-12-29T11:06:00.000Z",
        "voteCount": 1,
        "content": "I'm siding with C on this one.\njobUser role has the bigquery.jobs.create permission, which allow it to load data into BQ, which analyst shouldn't do.\nData Viewer has no permissions to add or edit data (It can create a snapshot of the data, extract it or replicate it at most)"
      },
      {
        "date": "2023-12-26T21:08:00.000Z",
        "voteCount": 2,
        "content": "BigQuery Data Viewer \n(roles/bigquery.dataViewer)\nWhen applied to a table or view, this role provides permissions to:\nRead data and metadata from the table or view.\nThis role cannot be applied to individual models or routines.\nWhen applied to a dataset, this role provides permissions to:\nRead the dataset's metadata and list tables in the dataset.\nRead data and metadata from the dataset's tables.\nWhen applied at the project or organization level, this role can also enumerate all datasets in the project. Additional roles, however, are necessary to allow the running of jobs.\nLowest-level resources where you can grant this role:\nTable and view\n\nBigQuery Job User \n(roles/bigquery.jobUser)\nProvides permissions to run jobs, including queries, within the project.\nLowest-level resources where you can grant this role:\nProject\n\nAnalyst must query data --&gt; BigQuery Data Viewer"
      },
      {
        "date": "2023-10-14T00:17:00.000Z",
        "voteCount": 1,
        "content": "A: JobUser  to execute queries in general. Data viewer for viewing the country dataset."
      },
      {
        "date": "2023-10-06T18:45:00.000Z",
        "voteCount": 1,
        "content": "Lowest-level resources where you can grant this role: \ndataViewer: Table, View\njobUser: Project\n\nYou don't want to grant access to the entire project, only the dataset which is divided per country. Definitely C. \nhttps://cloud.google.com/bigquery/docs/access-control#bigquery.dataViewer"
      },
      {
        "date": "2024-01-15T15:04:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is A.\n\nNote this: \"Share the appropriate dataset with *view access* with each respective analyst country-group\".\n\n\"view access\" is the key."
      },
      {
        "date": "2023-10-05T23:40:00.000Z",
        "voteCount": 1,
        "content": "A. Create a group per country. Add analysts to their respective country-groups. Create a single group 'all_analysts', and add all country-groups as members. Grant the 'all_analysts' group the IAM role of BigQuery jobUser. Share the appropriate dataset with view access with each respective analyst country-group.\n\nAs all analysts need to execute query, they need JobUser role.\nThey should be restricted to view all datasets (not tables) of respective country."
      },
      {
        "date": "2023-09-29T10:40:00.000Z",
        "voteCount": 1,
        "content": "It is C for Sure, A give Project level permissions, which defied requirement to have access to the Data set level."
      },
      {
        "date": "2023-08-31T14:02:00.000Z",
        "voteCount": 1,
        "content": "JobUser required to run queries"
      },
      {
        "date": "2023-08-31T13:56:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/bigquery/docs/access-control#bigquery.dataViewer\n\nData viewer role can be applied to a Table and a View.\n\nJobUser can be applied only at a Project level not at a Dataset level\n\nhttps://cloud.google.com/bigquery/docs/access-control#bigquery.jobUser"
      },
      {
        "date": "2023-08-31T14:02:00.000Z",
        "voteCount": 2,
        "content": "Incorrect - should be A"
      },
      {
        "date": "2023-08-31T11:44:00.000Z",
        "voteCount": 1,
        "content": "ChatGPT says C"
      },
      {
        "date": "2023-08-01T13:14:00.000Z",
        "voteCount": 3,
        "content": "Job user = Able to create query\nData Viewer = Able to view the data"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 86,
    "url": "https://www.examtopics.com/discussions/google/view/7468-exam-professional-cloud-architect-topic-1-question-86/",
    "body": "You have been engaged by your client to lead the migration of their application infrastructure to GCP. One of their current problems is that the on-premises high performance SAN is requiring frequent and expensive upgrades to keep up with the variety of workloads that are identified as follows: 20 TB of log archives retained for legal reasons; 500 GB of VM boot/data volumes and templates; 500 GB of image thumbnails; 200 GB of customer session state data that allows customers to restart sessions even if off-line for several days.<br>Which of the following best reflects your recommendations for a cost-effective storage allocation?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLocal SSD for customer session state data. Lifecycle-managed Cloud Storage for log archives, thumbnails, and VM boot/data volumes.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMemcache backed by Cloud Datastore for the customer session state data. Lifecycle-managed Cloud Storage for log archives, thumbnails, and VM boot/data volumes.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMemcache backed by Cloud SQL for customer session state data. Assorted local SSD-backed instances for VM boot/data volumes. Cloud Storage for log archives and thumbnails.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMemcache backed by Persistent Disk SSD storage for customer session state data. Assorted local SSD-backed instances for VM boot/data volumes. Cloud Storage for log archives and thumbnails."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 26,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 13,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-12-05T05:13:00.000Z",
        "voteCount": 106,
        "content": "B is correct.\nWHY NOT OTHERS.\nA: is wrong Local SSD in non-persistent therefore cannot be used for session state (as questions also need to save data for users who are offline for several days).\nC: Again Local SSD cannot be used for boot volume (because its Non-persistent again) and always used for temporary data storage.\nD: Same reason as C.\nWHY B?\nLeft with B that's why, but the question is how to store Boot/Data volume on Cloud Storage?\n- Storing other type of data is easy but most comments were about boot volume.\n- Boot volume can be stored to Cloud Storage by creating an Custom Image.\nhttps://cloud.google.com/compute/docs/images/create-delete-deprecate-private-images#selecting_image_storage_location\n---- Upvote if agree for the clarification of others ----"
      },
      {
        "date": "2021-06-10T21:49:00.000Z",
        "voteCount": 8,
        "content": "Cloud Storage can be used to store image but it can't be used for boot."
      },
      {
        "date": "2022-03-21T12:19:00.000Z",
        "voteCount": 1,
        "content": "\"If you need to move your Compute Engine boot disk data outside of your Compute Engine project, you can export a boot disk image to Cloud Storage as a tar.gz file\"\nDoc ref: https://cloud.google.com/compute/docs/images/export-image"
      },
      {
        "date": "2022-08-08T00:13:00.000Z",
        "voteCount": 2,
        "content": "Customer is migrating their apps , not only data.\nSo B is wrong.\nApp wont work with data volumes in compresses format on cloud storage ( obvious)"
      },
      {
        "date": "2022-08-07T07:18:00.000Z",
        "voteCount": 1,
        "content": "Cloud Volumes Service has the ability to send volumes of the CVS service type to Google Cloud Object Storage for long-term backup and archive. This data-management feature complements volume snapshots, which provide access for development or test use cases that require short-term recovery\n\nhttps://cloud.google.com/architecture/partners/netapp-cloud-volumes/back-up"
      },
      {
        "date": "2021-09-05T08:38:00.000Z",
        "voteCount": 2,
        "content": "it's B. the question is all about storing data. B is right answer"
      },
      {
        "date": "2022-08-08T00:11:00.000Z",
        "voteCount": 1,
        "content": "How can u use cloud storage for VM boot/data volumnes ?\nB is wrong"
      },
      {
        "date": "2022-08-08T00:18:00.000Z",
        "voteCount": 3,
        "content": "All the options are debatable and have some flaw. \nBut closes answer is B\nalthough it has a flaw mentioned above but is still better than other options"
      },
      {
        "date": "2022-01-19T07:53:00.000Z",
        "voteCount": 5,
        "content": "Answer is D - boot volumes (not boot images) cannot come from Cloud Storage - so B is not the answer."
      },
      {
        "date": "2022-08-18T03:53:00.000Z",
        "voteCount": 2,
        "content": "Guys I think its a english error.\nThe last line need to be read carefully\nDecouple the line after , and Vm boot/data volumes.\nI think they mean to use vm persistent disks for boot and data volumes.\nB is the answer"
      },
      {
        "date": "2020-03-09T08:46:00.000Z",
        "voteCount": 16,
        "content": "IMHO Answer is B:\n\nMemcache backed by Cloud Datastore\nhttps://cloud.google.com/appengine/docs/standard/python/memcache\n\nCompute Engine image can be stored in Cloud Storage\nhttps://cloud.google.com/solutions/image-management-best-practices\nAfter the complete sequence of bytes from the disk are written to the file, the file is archived using the tar format and then compressed using the GZIP format. You can then upload the resulting *.tar.gz file to Cloud Storage and register it as an image in Compute Engine."
      },
      {
        "date": "2020-04-26T03:29:00.000Z",
        "voteCount": 10,
        "content": "The problem with B is that they are using SAN for data volumes of working VMs, not just to store templates/images. All answers here are quite bad. But I would go with D, as they are talking about several days of saving users' stale session data, which is something that can be accomplished with SSD."
      },
      {
        "date": "2020-11-08T02:04:00.000Z",
        "voteCount": 1,
        "content": "@ayzen yes.  IS cloud datastore optimized to handle such a data (200GB)"
      },
      {
        "date": "2024-07-27T23:30:00.000Z",
        "voteCount": 1,
        "content": "None of these provide an effective method of storing boot/data volumes\nThe correct approach would be to create persistent disk Drives for boot/data volumes directly and go with B or D for the remainder of requirements."
      },
      {
        "date": "2024-01-01T20:48:00.000Z",
        "voteCount": 1,
        "content": "B is correct. the question lays emphasis on  a cost-effective storage allocation, and persistent disks are costly than B (that rules out D)"
      },
      {
        "date": "2023-12-29T09:44:00.000Z",
        "voteCount": 1,
        "content": "This is a troublesome question...\nIMHO, it's D. I was between C &amp; D, but seeing how they stored the customer session data as files, storing it in SQL would require refactoring, and maybe higher latencies.\nI'm against B because of the latency added by booting &amp; loading data off Cloud Storage, since it adds network latency to the equation.\nBUT this is assuming the method of using cloud storage is via gcsfuse, that is, using the bucket as a HD.\nNow if the way of using it is via image of the disk that is loaded when the instance starts, that would be ok. And that's what I expected of option D, that it would load an image of the boot/data volume in it's non-persistent disks. No 'persistent' data would be stored in it, so anything lost when it's shutdown can be ignored."
      },
      {
        "date": "2023-10-05T23:46:00.000Z",
        "voteCount": 1,
        "content": "D is correct answer because\n\n - Memcache backed by Persistent Disk SSD storage for customer session state data. - Persistent disk will ensure that Session data is preserved evern if not used for long time.\n\n - Assorted local SSD-backed instances for VM boot/data volumes. Provides faster boot up for VMs. (There is no requirement for persistent storage)\n\n - Cloud Storage for log archives and thumbnails. - For low cost and scalable solution."
      },
      {
        "date": "2023-09-28T19:58:00.000Z",
        "voteCount": 2,
        "content": "d wrong because of local ssd cannot use for boot image. its temporary and will be cleared if vm was suspended &gt;&gt; so chose b"
      },
      {
        "date": "2023-11-09T18:39:00.000Z",
        "voteCount": 2,
        "content": "A lot of folks keep saying D is wrong because of \"local SSD\". It never mentioned \"local\" in option D. It said \"Persistent Disk SSD\".\n\nhttps://cloud.google.com/compute/docs/disks\n\nI would definitely choose D."
      },
      {
        "date": "2023-11-09T18:43:00.000Z",
        "voteCount": 1,
        "content": "My bad, I was focusing on the memcache. I would choose C. The \"assorted local ssd\" for VM boot/data volumes can't work, as Local SSDs are ephemeral, meaning it's lost if the VM instance is stopped or deleted."
      },
      {
        "date": "2023-11-09T18:45:00.000Z",
        "voteCount": 1,
        "content": "I think I'm tired. I meant to write Option B as the correct answer.\nMemcache backed by Cloud Datastore, etc."
      },
      {
        "date": "2023-09-08T08:45:00.000Z",
        "voteCount": 1,
        "content": "Local SSD means Ephemeral ( Temp Disk )  so do not confused with PERSISTENT disk ( Permanent Disk ). The data that you store on a local SSD persists only until the instance is stopped or deleted."
      },
      {
        "date": "2023-04-21T10:36:00.000Z",
        "voteCount": 1,
        "content": "Local SSD cannot be used for neither boot nor data!!! This rules out B&amp;C. Oh, and A too."
      },
      {
        "date": "2023-04-11T06:08:00.000Z",
        "voteCount": 1,
        "content": "Cloud Datastore is the right solution to store the session data"
      },
      {
        "date": "2023-03-21T18:05:00.000Z",
        "voteCount": 1,
        "content": "B Seems correct."
      },
      {
        "date": "2023-03-02T04:08:00.000Z",
        "voteCount": 1,
        "content": "D. makes more sense"
      },
      {
        "date": "2023-03-01T13:56:00.000Z",
        "voteCount": 1,
        "content": "d makes more sense"
      },
      {
        "date": "2023-02-27T20:16:00.000Z",
        "voteCount": 1,
        "content": "ChatGPT says option D."
      },
      {
        "date": "2023-03-04T20:41:00.000Z",
        "voteCount": 3,
        "content": "chatgpt has knowledge till Sept 2021. Don't rely on it bro"
      },
      {
        "date": "2023-01-15T05:32:00.000Z",
        "voteCount": 1,
        "content": "My answer is D"
      },
      {
        "date": "2022-12-25T06:29:00.000Z",
        "voteCount": 2,
        "content": "I will go with Option D as it is best practice to keep similar data together and  seprate OS, Volatile and permanent"
      },
      {
        "date": "2022-12-24T13:59:00.000Z",
        "voteCount": 2,
        "content": "i'll go D because i don't think Cloud storage can be used for booting a VM."
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 87,
    "url": "https://www.examtopics.com/discussions/google/view/7328-exam-professional-cloud-architect-topic-1-question-87/",
    "body": "Your web application uses Google Kubernetes Engine to manage several workloads. One workload requires a consistent set of hostnames even after pod scaling and relaunches.<br>Which feature of Kubernetes should you use to accomplish this?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStatefulSets\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRole-based access control",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tContainer environment variables",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPersistent Volumes"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2019-10-27T11:17:00.000Z",
        "voteCount": 56,
        "content": "StatefulSets is a feature of Kubernetes, which the question asks about. Yes, Persistent volumes are required by StatefulSets (https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/). See the Google documentations for mentioning of hostnames (https://cloud.google.com/kubernetes-engine/docs/concepts/statefulset)... Answer A"
      },
      {
        "date": "2020-08-07T01:39:00.000Z",
        "voteCount": 6,
        "content": "A is ok"
      },
      {
        "date": "2022-01-04T02:42:00.000Z",
        "voteCount": 1,
        "content": "thank you!"
      },
      {
        "date": "2020-11-01T08:21:00.000Z",
        "voteCount": 2,
        "content": "A is correct, statefulset"
      },
      {
        "date": "2021-03-04T18:09:00.000Z",
        "voteCount": 2,
        "content": "It is A"
      },
      {
        "date": "2022-12-22T02:17:00.000Z",
        "voteCount": 17,
        "content": "A. StatefulSets\n\nTo ensure that a workload in Kubernetes has a consistent set of hostnames even after pod scaling and relaunches, you should use StatefulSets. StatefulSets are a type of controller in Kubernetes that is used to manage stateful applications. They provide a number of features that are specifically designed to support stateful applications, including:\n\nStable, unique network identifiers for each pod in the set\nPersistent storage that is automatically attached to pods\nOrdered, graceful deployment and scaling of pods\nOrdered, graceful deletion and termination of pods\nBy using StatefulSets, you can ensure that your workload has a consistent set of hostnames even if pods are scaled or relaunched, which can be important for applications that rely on stable network identifiers."
      },
      {
        "date": "2023-02-08T04:32:00.000Z",
        "voteCount": 2,
        "content": "You are the best.. thanks for all the hard work to explain"
      },
      {
        "date": "2023-03-30T05:06:00.000Z",
        "voteCount": 1,
        "content": "obviously from chatGPT, but still good to share."
      },
      {
        "date": "2023-01-15T05:34:00.000Z",
        "voteCount": 1,
        "content": "A is the answer"
      },
      {
        "date": "2022-11-10T00:51:00.000Z",
        "voteCount": 1,
        "content": "A is ok"
      },
      {
        "date": "2022-11-07T00:55:00.000Z",
        "voteCount": 2,
        "content": "A StatefulSet is the Kubernetes controller used to run the stateful application as containers (Pods) in the Kubernetes cluster. StatefulSets assign a sticky identity\u2014an ordinal number starting from zero\u2014to each Pod instead of assigning random IDs for each replica Pod. A new Pod is created by cloning the previous Pod\u2019s data."
      },
      {
        "date": "2022-10-16T09:48:00.000Z",
        "voteCount": 1,
        "content": "this is straight forward question if you know kubernetes concepts. A is right"
      },
      {
        "date": "2022-10-17T03:40:00.000Z",
        "voteCount": 2,
        "content": "I do not know Kubernetes"
      },
      {
        "date": "2022-08-04T05:12:00.000Z",
        "voteCount": 3,
        "content": "https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/"
      },
      {
        "date": "2022-07-05T13:16:00.000Z",
        "voteCount": 3,
        "content": "06/30/2022 Exam"
      },
      {
        "date": "2021-12-07T03:15:00.000Z",
        "voteCount": 2,
        "content": "Go for A.\nhttps://kubernetes.io/docs/concepts/workloads/controllers/statefulset/"
      },
      {
        "date": "2021-11-28T06:23:00.000Z",
        "voteCount": 2,
        "content": "A is the correct answer"
      },
      {
        "date": "2021-10-29T10:13:00.000Z",
        "voteCount": 6,
        "content": "A \u2013 StatefulSets\nStatefulSets are suitable for deploying Kafka, MySQL, Redis, ZooKeeper, and other applications needing unique, persistent identities and stable hostnames. Read more about StatefulSets. https://cloud.google.com/kubernetes-engine/docs/concepts/statefulset\n\nC \u2013 Container Env Variable, are good if you need to init containers with some static content. E.g. Pod passes to containers its HOSTNAME (where containers are running), namespace and user defined vars. So, env vars is a way for Pod to init containers at start up. But, stable hostnames must be preserved at Pod level via StatefulSets.\nDefining Env Vars for Container: https://kubernetes.io/docs/tasks/inject-data-application/define-environment-variable-container/"
      },
      {
        "date": "2021-10-25T01:51:00.000Z",
        "voteCount": 2,
        "content": "StatefulSets are designed to deploy stateful applications and clustered applications that save data to persistent storage, such as Compute Engine persistent disks. StatefulSets are suitable for deploying Kafka, MySQL, Redis, ZooKeeper, and other applications needing unique, persistent identities and \"stable hostnames\". Answer is A"
      },
      {
        "date": "2021-05-19T05:42:00.000Z",
        "voteCount": 3,
        "content": "A. StatefulSets"
      },
      {
        "date": "2021-05-16T02:49:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2021-04-01T00:39:00.000Z",
        "voteCount": 1,
        "content": "Answer is A"
      },
      {
        "date": "2020-12-08T20:06:00.000Z",
        "voteCount": 1,
        "content": "A is the Ans \nhttps://kubernetes.io/docs/concepts/workloads/controllers/statefulset/"
      },
      {
        "date": "2020-12-02T02:14:00.000Z",
        "voteCount": 1,
        "content": "StatefulSets for sequencing.. \nA is correct"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 88,
    "url": "https://www.examtopics.com/discussions/google/view/9188-exam-professional-cloud-architect-topic-1-question-88/",
    "body": "You are using Cloud CDN to deliver static HTTP(S) website content hosted on a Compute Engine instance group. You want to improve the cache hit ratio.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCustomize the cache keys to omit the protocol from the key.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tShorten the expiration time of the cached objects.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMake sure the HTTP(S) header \u05d2\u20acCache-Region\u05d2\u20ac points to the closest region of your users.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReplicate the static content in a Cloud Storage bucket. Point CloudCDN toward a load balancer on that bucket."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 10,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 6,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2019-11-26T17:00:00.000Z",
        "voteCount": 23,
        "content": "Option A is Correct.\nhttps://cloud.google.com/cdn/docs/caching#cache-keys"
      },
      {
        "date": "2020-08-07T01:39:00.000Z",
        "voteCount": 7,
        "content": "A is ok"
      },
      {
        "date": "2020-11-01T08:23:00.000Z",
        "voteCount": 2,
        "content": "Yes, A is correct"
      },
      {
        "date": "2021-03-04T18:13:00.000Z",
        "voteCount": 3,
        "content": "A, both http and https will use same key."
      },
      {
        "date": "2023-03-10T07:20:00.000Z",
        "voteCount": 6,
        "content": "https://cloud.google.com/cdn/docs/best-practices#cache-hit-ratio"
      },
      {
        "date": "2020-06-02T00:08:00.000Z",
        "voteCount": 9,
        "content": "A, for sure.\nBy default, Cloud CDN uses the complete request URL to build the cache key. For performance and scalability, it\u2019s important to optimize cache hit ratio. To help optimize your cache hit ratio, you can use custom cache keys ....."
      },
      {
        "date": "2024-02-24T02:40:00.000Z",
        "voteCount": 2,
        "content": "A is right"
      },
      {
        "date": "2024-02-16T07:30:00.000Z",
        "voteCount": 2,
        "content": "option D"
      },
      {
        "date": "2024-02-02T04:28:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/cdn/docs/best-practices#cache-hit-ratio"
      },
      {
        "date": "2024-02-01T06:07:00.000Z",
        "voteCount": 2,
        "content": "i'm with D Option"
      },
      {
        "date": "2023-10-05T23:53:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/cdn/docs/best-practices#using_custom_cache_keys_to_improve_cache_hit_ratio\n\nA is correct option, use custom keys."
      },
      {
        "date": "2023-09-24T04:23:00.000Z",
        "voteCount": 1,
        "content": "A and D could be OK but there is no information in the question that both http and https protocols are used (it is in fact not probable to use both protocols). Anwer A would be only valid when both http and https are in use."
      },
      {
        "date": "2023-12-05T18:05:00.000Z",
        "voteCount": 2,
        "content": "Incorrect, respectfully. \"HTTP(S)\" (the parenthesis) implies that HTTP requests can be made using *either* HTTP or HTTPS, especially if TLS is not mandatory.  If a site returns a 200 OK for http://www.example.com/picture/of/a/cat.jpg and httpS://www.example.come/picture/of/a/cat.jpg, then you should exclude the protocol in the cache key to increase the cache-hit ratio. If you don't, the CDN will treat both URLs as different objects."
      },
      {
        "date": "2023-08-24T06:58:00.000Z",
        "voteCount": 1,
        "content": "A sounds correct"
      },
      {
        "date": "2023-08-19T22:27:00.000Z",
        "voteCount": 1,
        "content": "Replicating static content in a Cloud Storage bucket and pointing CloudCDN toward a load balancer on that bucket may improve the distribution of content but is not directly related to improving the cache hit ratio based on the customizing of cache keys."
      },
      {
        "date": "2023-05-08T01:09:00.000Z",
        "voteCount": 2,
        "content": "D\nThis option is the best because Cloud Storage has built-in caching and can serve content faster than Compute Engine instances. It also allows for better scalability and availability. By pointing Cloud CDN towards a load balancer on the Cloud Storage bucket, the cache hit ratio can be improved as the content will be served directly from the cache without needing to access the Compute Engine instances. \n\nOption A (Customize the cache keys to omit the protocol from the key) may not be effective in improving the cache hit ratio as it only removes the protocol from the cache key and does not address the underlying issue of slow content delivery."
      },
      {
        "date": "2023-02-27T20:26:00.000Z",
        "voteCount": 1,
        "content": "Customizing the cache keys by omitting the protocol from the key (option A) can be a valid approach to improve the cache hit ratio for CDN delivered Compute Engine, but it may not be the most effective solution for all cases.\n\nCustomizing the cache keys can improve the cache hit ratio by reducing the number of cache misses caused by variations in the request URL, headers, and parameters. However, customizing the cache keys requires careful consideration of the caching policies, traffic patterns, and content types"
      },
      {
        "date": "2022-12-22T02:24:00.000Z",
        "voteCount": 8,
        "content": "A. Customize the cache keys to omit the protocol from the key.\n\nTo improve the cache hit ratio with Cloud CDN, you should customize the cache keys to omit the protocol (e.g. HTTP or HTTPS) from the key. This will allow Cloud CDN to cache the same content under both HTTP and HTTPS, which can help to improve the hit ratio by allowing Cloud CDN to serve content from cache more frequently.\n\nTo customize the cache keys, you can use the --key-include-protocol flag when enabling Cloud CDN for your Compute Engine instance group or load balancer. Setting this flag to false will cause Cloud CDN to omit the protocol from the cache key.\n\nOther options, such as shortening the expiration time of cached objects or replicating content in Cloud Storage, may also help to improve the cache hit ratio, but customizing the cache keys to omit the protocol is likely to have the greatest impact."
      },
      {
        "date": "2022-10-16T09:43:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/cdn/docs/best-practices#cache-hit-ratio. A is right"
      },
      {
        "date": "2022-10-16T09:46:00.000Z",
        "voteCount": 2,
        "content": "Each cache entry in a Cloud CDN cache is identified by a cache key. When a request comes into the cache, the cache converts the URI of the request into a cache key, and then compares it with keys of cached entries. If it finds a match, the cache returns the object associated with that key."
      },
      {
        "date": "2022-09-01T08:25:00.000Z",
        "voteCount": 2,
        "content": "Use case:\n\"A logo needs to be cached whether displayed through HTTP or HTTPS. When you customize the cache keys for the backend service that holds the logo, clear the Protocol checkbox so that requests through HTTP and HTTPS count as matches for the logo's cache entry.\"\n\nhttps://cloud.google.com/cdn/docs/best-practices#using_custom_cache_keys_to_improve_cache_hit_ratio"
      },
      {
        "date": "2022-08-04T05:15:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/cdn/docs/best-practices#using_custom_cache_keys_to_improve_cache_hit_ratio"
      },
      {
        "date": "2021-12-28T06:38:00.000Z",
        "voteCount": 2,
        "content": "I agree that 'A' action will increase the cache hit ratio, but it doesn't make sense for me to remove HTTPS from parts of my app. All access must be over HTTPS and HTTP must be blocked or redirected to HTTPS."
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 89,
    "url": "https://www.examtopics.com/discussions/google/view/6712-exam-professional-cloud-architect-topic-1-question-89/",
    "body": "Your architecture calls for the centralized collection of all admin activity and VM system logs within your project.<br>How should you collect these logs from both VMs and services?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAll admin and VM system logs are automatically collected by Stackdriver.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStackdriver automatically collects admin activity logs for most services. The Stackdriver Logging agent must be installed on each instance to collect system logs.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLaunch a custom syslogd compute instance and configure your GCP project and VMs to forward all logs to it.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall the Stackdriver Logging agent on a single compute instance and let it collect all audit and access logs for your environment."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 12,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2019-10-17T05:43:00.000Z",
        "voteCount": 43,
        "content": "Does not agree with D. B is the nearest answer I feel !"
      },
      {
        "date": "2020-11-01T08:25:00.000Z",
        "voteCount": 2,
        "content": "B is correct, D is SPOF ..."
      },
      {
        "date": "2021-03-04T18:14:00.000Z",
        "voteCount": 2,
        "content": "It is B, all rest are BS"
      },
      {
        "date": "2019-10-17T15:59:00.000Z",
        "voteCount": 9,
        "content": "Agree."
      },
      {
        "date": "2020-08-07T01:42:00.000Z",
        "voteCount": 12,
        "content": "B is ok"
      },
      {
        "date": "2019-11-26T17:20:00.000Z",
        "voteCount": 20,
        "content": "Admin and event logs are configured by default. VM System logs require a logging agent to be configured. So A is not valid. Answer is B"
      },
      {
        "date": "2024-06-02T12:12:00.000Z",
        "voteCount": 1,
        "content": "Admin and event logs are configured by default. VM System logs require a logging agent to be configured. Answer is B"
      },
      {
        "date": "2024-05-16T13:10:00.000Z",
        "voteCount": 1,
        "content": "Stackdriver Logging agent doesn't need for audit logs"
      },
      {
        "date": "2024-01-02T06:53:00.000Z",
        "voteCount": 2,
        "content": "B is correct\nNow it is recommended to use OpsAgent as a replacement. Although you can create a VM instance with OpsAgent automatically enabled, which makes it look like 'the logging is automatically enabled', under the hood you need to install the agent on the instance.\nhttps://cloud.google.com/stackdriver/docs/solutions/agents/ops-agent/install-agent-vm-creation"
      },
      {
        "date": "2023-12-13T12:57:00.000Z",
        "voteCount": 2,
        "content": "A: Stackdriver already collects admin logs and GCE logs."
      },
      {
        "date": "2023-12-10T03:23:00.000Z",
        "voteCount": 2,
        "content": "Answer is A\nyou don't need to install the Stackdriver Logging agent or any other agents in order to collect admin or system logs from compute engine instances (VM):\nhttps://cloud.google.com/compute/docs/logging/activity-logs\nhttps://cloud.google.com/compute/docs/logging/audit-logging"
      },
      {
        "date": "2023-11-12T13:42:00.000Z",
        "voteCount": 1,
        "content": "Answer is B, but it would be more accurate if the answer mentioned install the Ops Agent on the VMs. https://cloud.google.com/logging/docs/agent/ops-agent"
      },
      {
        "date": "2023-03-22T14:50:00.000Z",
        "voteCount": 1,
        "content": "answer is B as per this tutorial step: https://cloud.google.com/logging/docs/logging-gce-quickstart#install-agent"
      },
      {
        "date": "2022-12-22T02:32:00.000Z",
        "voteCount": 6,
        "content": "Stackdriver does not require the Stackdriver Logging agent to be installed in order to collect system logs.\nStackdriver is a cloud monitoring and logging platform that is integrated with Google Cloud Platform (GCP) and is designed to collect, monitor, and troubleshoot logs from your GCP resources. By default, Stackdriver automatically collects admin activity logs for most GCP services, as well as VM system logs. This means that you don't need to install the Stackdriver Logging agent or any other agents in order to collect these logs - they are automatically collected and centralized by Stackdriver.\n\nHowever, if you want to collect logs from other sources that are not automatically collected by Stackdriver (e.g. logs from applications running on your VMs, logs from on-premises systems, etc.), you can use the Stackdriver Logging agent to forward these logs to Stackdriver. The agent is a lightweight daemon that runs on your VMs or other hosts, and it can be used to collect logs from various sources and forward them to Stackdriver for centralized storage and analysis."
      },
      {
        "date": "2022-12-22T02:33:00.000Z",
        "voteCount": 6,
        "content": "Answer is A\n\nIn Google Cloud Platform (GCP), you can use Stackdriver to collect and centralize all admin activity and VM system logs within your project. Stackdriver is a powerful cloud monitoring and logging platform that is integrated with GCP, and it provides a number of features that are specifically designed to help you collect, monitor, and troubleshoot logs from your GCP resources.\n\nOne of the key features of Stackdriver is that it automatically collects admin activity logs for most GCP services, as well as VM system logs. This means that you don't need to install any agents or configure any additional components to collect these logs - they are automatically collected and centralized by Stackdriver.\n\nTo view and analyze your logs in Stackdriver, you can use the Stackdriver Logs Viewer, which provides a powerful interface for searching, filtering, and aggregating your logs. You can also use the Stackdriver Logs API to programmatically access your logs, or use the Stackdriver Logging agent to forward your logs to other log management or analysis tools."
      },
      {
        "date": "2022-11-10T00:57:00.000Z",
        "voteCount": 1,
        "content": "B is ok"
      },
      {
        "date": "2022-08-04T05:18:00.000Z",
        "voteCount": 5,
        "content": "https://cloud.google.com/logging/docs/agent/logging/installation#before_you_begin"
      },
      {
        "date": "2022-10-16T09:39:00.000Z",
        "voteCount": 1,
        "content": "Thank you for sharing the link, B is right"
      },
      {
        "date": "2022-05-21T21:16:00.000Z",
        "voteCount": 2,
        "content": "it's B"
      },
      {
        "date": "2021-12-07T03:20:00.000Z",
        "voteCount": 2,
        "content": "Go for B."
      },
      {
        "date": "2021-11-28T10:24:00.000Z",
        "voteCount": 2,
        "content": "B is the correct answer."
      },
      {
        "date": "2021-10-29T10:21:00.000Z",
        "voteCount": 3,
        "content": "B \u2013 stackdriver automatically collects admin activity logs for most services. Stackdriver Logging Agenct must be installed on each instance to collect system logs.\nRead more about Logging Agent. https://cloud.google.com/logging/docs/agent/\nLogging agent streams logs from 3rd party apps and systems SW (syslog on Linux) to Logging. It is best practice to run the Logging agent on all your VM instances. It runs on Linux and Windows.\nCloud Audit Logs says that Admin Activity audit logs are always enabled."
      },
      {
        "date": "2021-10-09T01:45:00.000Z",
        "voteCount": 1,
        "content": "Agree with B"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 90,
    "url": "https://www.examtopics.com/discussions/google/view/6462-exam-professional-cloud-architect-topic-1-question-90/",
    "body": "You have an App Engine application that needs to be updated. You want to test the update with production traffic before replacing the current application version.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the update using the Instance Group Updater to create a partial rollout, which allows for canary testing.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the update as a new version in the App Engine application, and split traffic between the new and current versions.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the update in a new VPC, and use Google's global HTTP load balancing to split traffic between the update and current applications.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the update as a new App Engine application, and use Google's global HTTP load balancing to split traffic between the new and current applications."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2019-10-11T06:02:00.000Z",
        "voteCount": 60,
        "content": "I think B is correct. Because GAE supports service version control and A/B test.\nIs my understanding correct?"
      },
      {
        "date": "2020-11-01T08:26:00.000Z",
        "voteCount": 5,
        "content": "Yes, B is correct"
      },
      {
        "date": "2021-03-04T18:15:00.000Z",
        "voteCount": 4,
        "content": "Only B works."
      },
      {
        "date": "2020-02-13T15:05:00.000Z",
        "voteCount": 15,
        "content": "Only one  App Engine application can be created per Project.\nSo it's B."
      },
      {
        "date": "2022-12-22T02:35:00.000Z",
        "voteCount": 11,
        "content": "B. Deploy the update as a new version in the App Engine application, and split traffic between the new and current versions.\n\nTo test an update to an App Engine application with production traffic before replacing the current version, you can deploy the update as a new version in the App Engine application and split traffic between the new and current versions. This is known as a \"blue-green\" deployment, and it allows you to test the new version with a portion of production traffic while the current version is still serving the remainder of traffic.\n\nTo split traffic between the new and current versions, you can use the App Engine traffic splitting feature. This feature allows you to specify the percentage of traffic that should be sent to each version, and it can be used to gradually ramp up traffic to the new version over time. This allows you to test the new version with a small portion of traffic initially, and gradually increase the traffic as you become more confident in the update."
      },
      {
        "date": "2022-12-22T02:35:00.000Z",
        "voteCount": 3,
        "content": "Other options, such as deploying the update in a new VPC or as a new App Engine application, are not recommended for testing updates with production traffic, as they can be more complex and may require additional steps to set up."
      },
      {
        "date": "2022-12-06T11:51:00.000Z",
        "voteCount": 3,
        "content": "Answer B :  You can use traffic splitting to specify a percentage distribution of traffic across two or more of the versions within a service. Splitting traffic allows you to conduct A/B testing between your versions and provides control over the pace when rolling out features.\nTraffic splitting is applied to URLs that do not explicitly target a version. For example, the following URLs split traffic because they target all the available versions within the specified service:\nhttps://cloud.google.com/appengine/docs/standard/splitting-traffic"
      },
      {
        "date": "2022-11-10T01:17:00.000Z",
        "voteCount": 2,
        "content": "B is ok"
      },
      {
        "date": "2022-10-16T09:28:00.000Z",
        "voteCount": 2,
        "content": "B is right , Option D is just to confuse you.\nDeploy the update as a new version in the App Engine application, and split traffic between the new and current versions."
      },
      {
        "date": "2022-08-04T05:20:00.000Z",
        "voteCount": 2,
        "content": "Versioning is supported in App Engine."
      },
      {
        "date": "2022-02-08T02:15:00.000Z",
        "voteCount": 2,
        "content": "Versioning is supported in App Engine."
      },
      {
        "date": "2021-12-07T03:27:00.000Z",
        "voteCount": 1,
        "content": "Go for D,\nThe option B don\u00b4t say with wich service will split the traffic.\nThe option D gives more datail and makes sense."
      },
      {
        "date": "2022-11-21T07:06:00.000Z",
        "voteCount": 4,
        "content": "No mate, only one app engine per project can be deployed, you can have multiple version on the same app tho. D is to confuse you. B is the only feasible answer in here."
      },
      {
        "date": "2021-11-28T10:35:00.000Z",
        "voteCount": 1,
        "content": "B is the correct answer"
      },
      {
        "date": "2021-11-09T03:52:00.000Z",
        "voteCount": 1,
        "content": "A is not because \"Instance Group Updater \" is only for Computer Engine MIG"
      },
      {
        "date": "2021-10-29T10:23:00.000Z",
        "voteCount": 6,
        "content": "B \u2013 Deploy the update as a new version in AppEngine app, and split traffic between the new and current versions.\nTraffic Splitting is feature of AppEngine for A/B testing. \nhttps://cloud.google.com/appengine/docs/standard/python/splitting-traffic"
      },
      {
        "date": "2021-10-23T23:04:00.000Z",
        "voteCount": 1,
        "content": "B is correct. App Engine supports versioning."
      },
      {
        "date": "2021-10-09T01:43:00.000Z",
        "voteCount": 1,
        "content": "B is correct... Canary Testing -&gt; Traffic Splitting"
      },
      {
        "date": "2021-05-19T06:16:00.000Z",
        "voteCount": 3,
        "content": "B. Deploy the update as a new version in the App Engine application, and split traffic between the new and current versions"
      },
      {
        "date": "2021-05-16T05:15:00.000Z",
        "voteCount": 1,
        "content": "B is the answer"
      },
      {
        "date": "2021-04-07T23:17:00.000Z",
        "voteCount": 3,
        "content": "Answer - B \nConfigure how much traffic the version that you just deployed should receive.\n\nBy default, the initial version that you deploy to your App Engine application is automatically configured to receive 100% of traffic. However, all subsequent versions that you deploy to that same App Engine application must be manually configured, otherwise they receive no traffic.\n\nFor details about how to configure traffic for your versions, see Migrating and Splitting Traffic.\nhttps://cloud.google.com/appengine/docs/admin-api/migrating-splitting-traffic"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 91,
    "url": "https://www.examtopics.com/discussions/google/view/6817-exam-professional-cloud-architect-topic-1-question-91/",
    "body": "All Compute Engine instances in your VPC should be able to connect to an Active Directory server on specific ports. Any other traffic emerging from your instances is not allowed. You want to enforce this using VPC firewall rules.<br>How should you configure the firewall rules?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an egress rule with priority 1000 to deny all traffic for all instances. Create another egress rule with priority 100 to allow the Active Directory traffic for all instances.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an egress rule with priority 100 to deny all traffic for all instances. Create another egress rule with priority 1000 to allow the Active Directory traffic for all instances.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an egress rule with priority 1000 to allow the Active Directory traffic. Rely on the implied deny egress rule with priority 100 to block all traffic for all instances.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an egress rule with priority 100 to allow the Active Directory traffic. Rely on the implied deny egress rule with priority 1000 to block all traffic for all instances."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2019-10-19T20:52:00.000Z",
        "voteCount": 91,
        "content": "Should be A, there is no implied deny egress but only implied allow egress\n\nhttps://cloud.google.com/vpc/docs/firewalls#default_firewall_rules\n\nEvery VPC network has two implied firewall rules. These rules exist, but are not shown in the Cloud Console:\n\nThe implied allow egress rule: An egress rule whose action is allow, destination is 0.0.0.0/0, and priority is the lowest possible (65535) lets any instance send traffic to any destination, except for traffic blocked by GCP. Outbound access may be restricted by a higher priority firewall rule. Internet access is allowed if no other firewall rules deny outbound traffic and if the instance has an external IP address or uses a NAT instance. Refer to Internet access requirements for more details.\n\nThe implied deny ingress rule: An ingress rule whose action is deny, source is 0.0.0.0/0, and priority is the lowest possible (65535) protects all instances by blocking incoming traffic to them. Incoming access may be allowed by a higher priority rule. Note that the default network includes some additional rules that override this one, allowing certain types of incoming traffic."
      },
      {
        "date": "2020-11-15T09:53:00.000Z",
        "voteCount": 13,
        "content": "from a book:\n\"Firewall rules control network traffic by blocking or allowing traffic into (ingress) or out of (egress) a network. Two implied firewall rules are defined with VPCs: one blocks all incoming traffic, and the other allows all outgoing traffic. You can change this behavior\nVirtual Private Clouds 115\n116 Chapter 6 \u25a0 Designing Networks\nby defining firewall rules with higher priority. Firewall rules have a priority specified by an integer from 0 to 65535, with 0 being the highest priority and 65535 being the lowest.\"\n\nso this confirms A"
      },
      {
        "date": "2024-01-16T22:06:00.000Z",
        "voteCount": 1,
        "content": "Good summary. To the point!"
      },
      {
        "date": "2020-11-01T08:27:00.000Z",
        "voteCount": 2,
        "content": "B is correct..."
      },
      {
        "date": "2021-03-04T18:17:00.000Z",
        "voteCount": 2,
        "content": "It is A, rest all do not make sense. If you think of any other option then go back and read about firewalls. Seriously you are not ready for this exam."
      },
      {
        "date": "2022-10-17T03:54:00.000Z",
        "voteCount": 1,
        "content": "thank you"
      },
      {
        "date": "2019-10-20T11:28:00.000Z",
        "voteCount": 10,
        "content": "Agree Correct is A. There is no implied deny egress only deny ingress rule"
      },
      {
        "date": "2019-12-31T05:55:00.000Z",
        "voteCount": 3,
        "content": "Agree with A . only Implied allow egress rule (or) Implied deny ingress rule. \nThere is No \"Implied deny egress rule\" which rules out C &amp; D"
      },
      {
        "date": "2023-07-30T03:04:00.000Z",
        "voteCount": 2,
        "content": "B. Create an egress rule with priority 100 to deny all traffic for all instances. Create another egress rule with priority 1000 to allow the Active Directory traffic for all instances.\n\nThis option creates a deny all rule with a lower priority and an allow rule with a higher priority. \n\nThis option will work as intended, as the Active Directory traffic will be allowed and all other outbound traffic will be blocked."
      },
      {
        "date": "2023-03-13T20:01:00.000Z",
        "voteCount": 2,
        "content": "The answer to this question is A.\n\nExplanation:\nTo enforce the requirement that all Compute Engine instances in your VPC should be able to connect to an Active Directory server on specific ports while blocking any other traffic emerging from instances, the following two egress rules should be created:\n\nCreate an egress rule with priority 1000 to deny all traffic for all instances.\nCreate another egress rule with priority 100 to allow the Active Directory traffic for all instances.\nIn this configuration, the rule that allows the AD traffic has a lower priority number than the rule that denies all other traffic. Therefore, this rule should be evaluated first."
      },
      {
        "date": "2023-03-04T21:07:00.000Z",
        "voteCount": 1,
        "content": "It should be A.\nIt cannot be D as The Implied allow egress rule, with its action of \u201callow\u201d, allows all traffic out to the 0.0. 0.0/0 destination, which basically means everywhere. The priority of the implied allow egress rule is the lowest possible, 65535. The implied deny ingress rule, with an action of \u201cdeny\u201d, blocks all incoming connections."
      },
      {
        "date": "2023-02-15T17:42:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is B.\n\nTo enforce that all Compute Engine instances in a VPC can connect to an Active Directory server on specific ports while blocking any other traffic, you should create an egress rule with a high priority (lower numerical value) to deny all traffic from all instances, and another egress rule with a lower priority (higher numerical value) to allow traffic to the Active Directory server on the specific ports.\n\nOption B creates the necessary egress rules in the correct order: a deny-all rule with a high priority (100), followed by an allow rule for the Active Directory traffic with a lower priority (1000). This way, traffic to the Active Directory server is allowed, but all other traffic is denied."
      },
      {
        "date": "2022-11-10T01:21:00.000Z",
        "voteCount": 1,
        "content": "A is ok"
      },
      {
        "date": "2022-10-16T09:27:00.000Z",
        "voteCount": 2,
        "content": "It is pretty straight forward question, It this case priority low should be allow and high priority rules deny all requests. A is right"
      },
      {
        "date": "2022-08-04T05:23:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/vpc/docs/firewalls#priority_order_for_firewall_rules"
      },
      {
        "date": "2022-07-06T09:29:00.000Z",
        "voteCount": 6,
        "content": "06/30/2022 Exam question."
      },
      {
        "date": "2022-07-17T09:06:00.000Z",
        "voteCount": 1,
        "content": "Oh, really? I got this question on my exam 2 years ago, I did not expect to repeat this kind of questions in the current exam."
      },
      {
        "date": "2022-02-28T12:02:00.000Z",
        "voteCount": 1,
        "content": "OT: why is there no way to mark questions for review/repeat later on?"
      },
      {
        "date": "2021-12-07T13:50:00.000Z",
        "voteCount": 1,
        "content": "Go for A.\nWhile the priority is higher, the egress rule is more restricted.\nWhile the priority is higher, the ingress rule is more free."
      },
      {
        "date": "2021-11-28T10:49:00.000Z",
        "voteCount": 1,
        "content": "A is correct answer"
      },
      {
        "date": "2021-11-27T12:37:00.000Z",
        "voteCount": 1,
        "content": "to understand rules priority:\nhttps://cloud.google.com/vpc/docs/firewalls#priority_order_for_firewall_rules"
      },
      {
        "date": "2021-11-26T19:16:00.000Z",
        "voteCount": 1,
        "content": "Vote A"
      },
      {
        "date": "2021-10-29T23:06:00.000Z",
        "voteCount": 2,
        "content": "A \u2013 create an egress rule with priority 1000 to deny all traffic for all instances. Create another egress rule with priority 100 to allow the Active Directory traffic for all instances.\nDefault Firewall rules (aka implied rules) are following:\n1)\tEgress traffic is allowed to all IP/ports.\n2)\tIngress traffic is disabled completely.\nBoth these rules have lowest priority (65535) and cannot be removed.\nhttps://cloud.google.com/vpc/docs/firewalls#default_firewall_rules"
      },
      {
        "date": "2021-05-19T06:15:00.000Z",
        "voteCount": 2,
        "content": "A. Create an egress rule with priority 1000 to deny all traffic for all instances. Create another egress rule with priority 100 to allow the Active Directory traffic for all instances."
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 92,
    "url": "https://www.examtopics.com/discussions/google/view/68718-exam-professional-cloud-architect-topic-1-question-92/",
    "body": "Your customer runs a web service used by e-commerce sites to offer product recommendations to users. The company has begun experimenting with a machine learning model on Google Cloud Platform to improve the quality of results.<br>What should the customer do to improve their model's results over time?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExport Cloud Machine Learning Engine performance metrics from Stackdriver to BigQuery, to be used to analyze the efficiency of the model.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBuild a roadmap to move the machine learning model training from Cloud GPUs to Cloud TPUs, which offer better results.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMonitor Compute Engine announcements for availability of newer CPU architectures, and deploy the model to them as soon as they are available for additional performance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSave a history of recommendations and results of the recommendations in BigQuery, to be used as training data.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 34,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-02-08T02:22:00.000Z",
        "voteCount": 19,
        "content": "Model performance is generally based on the volume of its training data input. The more the data, the better the model."
      },
      {
        "date": "2022-10-16T09:24:00.000Z",
        "voteCount": 1,
        "content": "I agree with you, D is right"
      },
      {
        "date": "2023-05-08T06:02:00.000Z",
        "voteCount": 1,
        "content": "Yes, correctly said..This is actually a question for Data Engineer role"
      },
      {
        "date": "2022-07-31T13:14:00.000Z",
        "voteCount": 6,
        "content": "A,B,C is defining about the performance of ML but not the result....only the training data will give good ML result/predictions"
      },
      {
        "date": "2023-03-04T21:13:00.000Z",
        "voteCount": 1,
        "content": "Best answer is D. Other 3 makes no sense"
      },
      {
        "date": "2023-02-18T03:31:00.000Z",
        "voteCount": 1,
        "content": "Need to improve the model results and not performance .. hence D"
      },
      {
        "date": "2022-12-11T01:25:00.000Z",
        "voteCount": 1,
        "content": "Answer is D"
      },
      {
        "date": "2022-08-04T05:25:00.000Z",
        "voteCount": 3,
        "content": "Model performance is generally based on the volume of its training data input. The more the data, the better the model."
      },
      {
        "date": "2022-03-30T14:42:00.000Z",
        "voteCount": 2,
        "content": "The following insights and recommendations can be exported (to bigquery):\nIAM recommender\nVM machine type recommender\nManaged instance group machine type recommender\t\nIdle PD recommender\nIdle VM recommender\nCloud SQL overprovisioned instance recommender\t\nCloud SQL idle instance recommender\nUnattended project recommender\nCloud Run Service Identity recommender\nhttps://cloud.google.com/recommender/docs/bq-export/export-recommendations-to-bq\n\nNone of this is correlated with Machine Learning, how can be D? looks more A the answer"
      },
      {
        "date": "2022-04-20T08:23:00.000Z",
        "voteCount": 2,
        "content": "what we will do with metrics , it won't improve our Machine learning model , D is the closest answer , also it didn't say export it said Save , which could be manually moving the data to BQ"
      },
      {
        "date": "2022-01-29T10:54:00.000Z",
        "voteCount": 3,
        "content": "i vote D"
      },
      {
        "date": "2022-01-01T16:13:00.000Z",
        "voteCount": 2,
        "content": "D. Save a history of recommendations and results of the recommendations in BigQuery, to be used as training data."
      },
      {
        "date": "2021-12-30T06:22:00.000Z",
        "voteCount": 4,
        "content": "\"training data\" is the key in option \"D\" and that's the answer"
      },
      {
        "date": "2021-12-28T02:11:00.000Z",
        "voteCount": 1,
        "content": "D seems to be the correct answer"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 93,
    "url": "https://www.examtopics.com/discussions/google/view/7123-exam-professional-cloud-architect-topic-1-question-93/",
    "body": "A development team at your company has created a dockerized HTTPS web application. You need to deploy the application on Google Kubernetes Engine (GKE) and make sure that the application scales automatically.<br>How should you deploy to GKE?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Horizontal Pod Autoscaler and enable cluster autoscaling. Use an Ingress resource to load-balance the HTTPS traffic.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Horizontal Pod Autoscaler and enable cluster autoscaling on the Kubernetes cluster. Use a Service resource of type LoadBalancer to load-balance the HTTPS traffic.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable autoscaling on the Compute Engine instance group. Use an Ingress resource to load-balance the HTTPS traffic.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable autoscaling on the Compute Engine instance group. Use a Service resource of type LoadBalancer to load-balance the HTTPS traffic."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 59,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 24,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2019-10-24T05:11:00.000Z",
        "voteCount": 28,
        "content": "Why not using Ingress? (A)"
      },
      {
        "date": "2020-11-30T09:53:00.000Z",
        "voteCount": 2,
        "content": "I think A is OK:"
      },
      {
        "date": "2021-03-04T18:19:00.000Z",
        "voteCount": 5,
        "content": "It is A, K8s best way to LB is Ingress."
      },
      {
        "date": "2020-02-25T11:06:00.000Z",
        "voteCount": 41,
        "content": "\"Ingress is a Kubernetes resource that encapsulates a collection of rules and configuration for routing external HTTP(S) traffic to internal services.\n\nOn GKE, Ingress is implemented using Cloud Load Balancing. When you create an Ingress in your cluster, GKE creates an HTTP(S) load balancer and configures it to route traffic to your application.\"\n\nAre you exposing multiple services through single IP address? Hence, do you need routing your traffic?\n\nCorrect answer is B."
      },
      {
        "date": "2020-02-25T11:16:00.000Z",
        "voteCount": 45,
        "content": "My bad, as stated by other, Service doesn't support L7 load balancing. Hence, need to setup ingress resource. Correct answer is A."
      },
      {
        "date": "2020-08-07T02:09:00.000Z",
        "voteCount": 9,
        "content": "B is ok.\nhttps://cloud.google.com/kubernetes-engine/docs/tutorials/hello-app"
      },
      {
        "date": "2020-10-14T23:39:00.000Z",
        "voteCount": 8,
        "content": "service resource does a NLB using IP address, however, Ingress does HTTP(S) Load balancer. A should be an answer."
      },
      {
        "date": "2019-10-26T02:44:00.000Z",
        "voteCount": 13,
        "content": "Name is service resource, it's B:\n\nhttps://cloud.google.com/kubernetes-engine/docs/concepts/service?hl=es-419"
      },
      {
        "date": "2024-07-03T05:51:00.000Z",
        "voteCount": 2,
        "content": "A and B both create under the hood a Service of type LoadBalancer with external IP address. However, when it comes to http(s) traffic an ingress is the way to go because of ssl termination and for the routing options."
      },
      {
        "date": "2024-05-24T06:44:00.000Z",
        "voteCount": 3,
        "content": "C &amp; D is clearly incorrect.\n\nB is incorrect because of this:\n\"service of type LoadBalancer to load-balance the HTTPS traffic.\"  \nGKE Service Load Balancer is L4 Network or Internal Load Balancer, does not support HTTPS traffic.\n\nThus only A is correct."
      },
      {
        "date": "2024-05-16T13:25:00.000Z",
        "voteCount": 3,
        "content": "The clue is HTTPS traffic. You need L7 stack. It can be achieved only through ingress controller."
      },
      {
        "date": "2024-05-11T20:17:00.000Z",
        "voteCount": 1,
        "content": "B\n\nA. Ingress resource: While Ingress can be used for external load balancing, it often requires additional configuration for HTTPS termination (offloading SSL from your application containers). Additionally, LoadBalancer services typically offer a simpler setup for basic external load balancing without HTTPS termination concerns.\nC &amp; D. Compute Engine Instance Group Autoscaling: GKE manages its own nodes separate from Compute Engine instances. Autoscaling on a Compute Engine instance group wouldn't manage the Kubernetes pods or nodes effectively in this scenario."
      },
      {
        "date": "2024-02-02T04:09:00.000Z",
        "voteCount": 3,
        "content": "service loadBalancer: https://cloud.google.com/kubernetes-engine/docs/concepts/service-load-balancer\nThis page provides a general overview of how Google Kubernetes Engine (GKE) creates and manages Google Cloud load balancers when you apply a Kubernetes LoadBalancer Services manifest. It describes the different types of load balancers and how settings like the externalTrafficPolicy and GKE subsetting for L4 internal load balancers determine how the load balancers are configured. -&gt; l4 tcp/udp not https\nIngress: https://cloud.google.com/kubernetes-engine/docs/concepts/ingress This page provides a general overview of what Ingress for external Application Load Balancers is and how it works. Google Kubernetes Engine (GKE) provides a built-in and managed Ingress controller called GKE Ingress. This controller implements Ingress resources as Google Cloud load balancers for HTTP(S) workloads in GKE. -S http(s)"
      },
      {
        "date": "2024-01-09T09:15:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2024-01-03T13:55:00.000Z",
        "voteCount": 2,
        "content": "I'm assuming B is the suggested answer because a the question doesn't state that the application should be available externally. Services allow exposing resources internally and to load balancers.\n\nHowever, it should be A, as the assumption would be a an external web application.\n\nhttps://cloud.google.com/kubernetes-engine/docs/concepts/service"
      },
      {
        "date": "2023-12-07T20:31:00.000Z",
        "voteCount": 2,
        "content": "Most if the labs in Google boost skills discuss how to expose the deployment using a load balancer."
      },
      {
        "date": "2023-11-24T23:28:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/kubernetes-engine/docs/concepts/ingress\n\"This page provides a general overview of what Ingress for external Application Load Balancers is and how it works. Google Kubernetes Engine (GKE) provides a built-in and managed Ingress controller called GKE Ingress. This controller implements Ingress resources as Google Cloud load balancers for HTTP(S) workloads in GKE.\""
      },
      {
        "date": "2023-11-09T04:14:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/kubernetes-engine/docs/concepts/ingress\nAs there is no mention about the type of the traffic, Internal or external - Going with A - Ingress."
      },
      {
        "date": "2023-10-13T08:53:00.000Z",
        "voteCount": 1,
        "content": "Option-C and D are straightforwardly wrong\n\nBetween A and B : B is the correct answer, because it makes use of loadbalancing the ingress in K8S native style. That is the reason why cluster scaling is also done. \n\nThis is how it should \nExternal Load Balancing Ingress --&gt; K8S Service of type LoadBalancer --&gt; pods that can autoscale\n\nDirectly allowing external loadbalcing ingress to autoscaled Pod, doesn't makes sense to use GKE"
      },
      {
        "date": "2023-09-29T23:15:00.000Z",
        "voteCount": 2,
        "content": "Ingress is Https while Service is TCP/UDP. \nhttps://cloud.google.com/load-balancing/docs/choosing-load-balancer\nhttps://cloud.google.com/kubernetes-engine/docs/concepts/service-networking"
      },
      {
        "date": "2023-08-24T11:25:00.000Z",
        "voteCount": 2,
        "content": "B is correct"
      },
      {
        "date": "2023-08-17T22:48:00.000Z",
        "voteCount": 1,
        "content": "A Is the best choice"
      },
      {
        "date": "2023-08-14T00:13:00.000Z",
        "voteCount": 1,
        "content": "Bowth options A and B can satisfy the requirements, they are both based on a load balancer. Option A is more adapted and more flexible as later on, you can set up routing rules to expose more then just one service using the same loadbalancer which can help reduce cost, you don't really need that flexibity for this case, but since it's gonna cost the same thing for now (const of a loadbalancer). Its better to go with the ingress option."
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 94,
    "url": "https://www.examtopics.com/discussions/google/view/10289-exam-professional-cloud-architect-topic-1-question-94/",
    "body": "You need to design a solution for global load balancing based on the URL path being requested. You need to ensure operations reliability and end-to-end in- transit encryption based on Google best practices.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a cross-region load balancer with URL Maps.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an HTTPS load balancer with URL Maps.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate appropriate instance groups and instances. Configure SSL proxy load balancing.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a global forwarding rule. Configure SSL proxy load balancing."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 19,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-05-19T06:29:00.000Z",
        "voteCount": 14,
        "content": "B. Create an HTTPS load balancer with URL maps."
      },
      {
        "date": "2019-12-27T03:19:00.000Z",
        "voteCount": 9,
        "content": "URL paths supported only in HTTP(S) Load balancing \nhttps://cloud.google.com/load-balancing/docs/ssl/#FAQ"
      },
      {
        "date": "2022-12-30T21:02:00.000Z",
        "voteCount": 6,
        "content": "B is the correct Answer,\n\nGoogle Cloud HTTP(S) load balancers and Traffic Director use a Google Cloud configuration resource called a URL map to route HTTP(S) requests to backend services or backend buckets.\n\nFor example, with an external HTTP(S) load balancer, you can use a single URL map to route requests to different destinations based on the rules configured in the URL map:\n\nRequests for https://example.com/video go to one backend service.\nRequests for https://example.com/audio go to a different backend service.\nRequests for https://example.com/images go to a Cloud Storage backend bucket.\nRequests for any other host and path combination go to a default backend service.\nURL maps are used with the following Google Cloud products:\n\nExternal HTTP(S) Load Balancing (global, regional, and classic modes)\nInternal HTTP(S) Load Balancing\nTraffic Director\n\nhttps://cloud.google.com/load-balancing/docs/url-map-concepts"
      },
      {
        "date": "2022-12-22T02:54:00.000Z",
        "voteCount": 1,
        "content": "B. Create an HTTPS load balancer with URL Maps.\n\nAn HTTPS load balancer is a type of load balancer that can distribute incoming HTTPS traffic to one or more back-end services, such as Compute Engine instances or Google Kubernetes Engine clusters. It can also provide SSL/TLS termination, enabling you to use your own SSL/TLS certificates and keys.\n\nYou can use URL Maps to configure the HTTPS load balancer to route traffic based on the URL path being requested. This allows you to set up different URL paths to be served by different back-end services, providing a high level of flexibility in your load balancing configuration."
      },
      {
        "date": "2022-12-22T02:54:00.000Z",
        "voteCount": 3,
        "content": "Option A, creating a cross-region load balancer with URL Maps, is also a valid solution, but it is not specifically designed for end-to-end in-transit encryption.\n\nOption C, creating appropriate instance groups and instances and configuring SSL proxy load balancing, is not a complete solution for global load balancing. SSL proxy load balancing is a feature that enables you to terminate SSL/TLS connections at the load balancer and establish a new SSL/TLS connection between the load balancer and the back-end service. It is not a global load balancing solution in and of itself.\n\nOption D, creating a global forwarding rule and configuring SSL proxy load balancing, is not a complete solution for global load balancing based on the URL path being requested. A global forwarding rule is a type of load balancing configuration that directs traffic to a specific back-end service based on the IP address and port of the incoming request. It does not allow for routing based on the URL path.\n\n\n\n\nRegenerate"
      },
      {
        "date": "2022-12-06T12:13:00.000Z",
        "voteCount": 2,
        "content": "Answer B:  URL maps used with global external HTTP(S) load balancers and regional external HTTP(S) load balancer support several advanced traffic management features such as header-based traffic steering, weight-based traffic splitting, and request mirroring.\n\nhttps://cloud.google.com/load-balancing/docs/https#url-maps"
      },
      {
        "date": "2022-11-10T01:59:00.000Z",
        "voteCount": 1,
        "content": "B is ok"
      },
      {
        "date": "2022-09-07T22:24:00.000Z",
        "voteCount": 5,
        "content": "UrlMaps are used to route requests to a backend service based on rules that you define for the host and path of an incoming URL."
      },
      {
        "date": "2022-09-07T22:23:00.000Z",
        "voteCount": 2,
        "content": "https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/compute_url_map ANS B"
      },
      {
        "date": "2022-08-04T05:31:00.000Z",
        "voteCount": 4,
        "content": "https://cloud.google.com/load-balancing/docs/https/url-map"
      },
      {
        "date": "2022-10-16T09:18:00.000Z",
        "voteCount": 1,
        "content": "thank you for pointing the link"
      },
      {
        "date": "2021-12-08T04:09:00.000Z",
        "voteCount": 2,
        "content": "Go for B"
      },
      {
        "date": "2021-11-29T19:04:00.000Z",
        "voteCount": 2,
        "content": "B is correct answer"
      },
      {
        "date": "2021-11-26T19:39:00.000Z",
        "voteCount": 1,
        "content": "Vote B"
      },
      {
        "date": "2021-05-16T05:55:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2021-04-15T04:03:00.000Z",
        "voteCount": 1,
        "content": "there are interl https load balancers they are regional https://cloud.google.com/load-balancing/docs/l7-internal"
      },
      {
        "date": "2021-04-01T00:55:00.000Z",
        "voteCount": 2,
        "content": "Answer is B"
      },
      {
        "date": "2021-01-30T13:59:00.000Z",
        "voteCount": 5,
        "content": "confused with A vs B. A has the word \"cross region\" but finally find out HTTP/S Load Balancing is naturally global.\n- B"
      },
      {
        "date": "2020-12-06T15:56:00.000Z",
        "voteCount": 2,
        "content": "B easy"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 95,
    "url": "https://www.examtopics.com/discussions/google/view/7962-exam-professional-cloud-architect-topic-1-question-95/",
    "body": "You have an application that makes HTTP requests to Cloud Storage. Occasionally the requests fail with HTTP status codes of 5xx and 429.<br>How should you handle these types of errors?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse gRPC instead of HTTP for better performance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement retry logic using a truncated exponential backoff strategy.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMake sure the Cloud Storage bucket is multi-regional for geo-redundancy.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMonitor https://status.cloud.google.com/feed.atom and only make requests if Cloud Storage is not reporting an incident."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2019-11-11T06:49:00.000Z",
        "voteCount": 39,
        "content": "Answer is B\nYou should use exponential backoff to retry your requests when receiving errors with 5xx or 429 response codes from Cloud Storage.\nhttps://cloud.google.com/storage/docs/request-rate"
      },
      {
        "date": "2021-03-04T18:58:00.000Z",
        "voteCount": 1,
        "content": "It is B"
      },
      {
        "date": "2022-10-16T09:16:00.000Z",
        "voteCount": 1,
        "content": "I agree with you, B should be right"
      },
      {
        "date": "2022-09-07T22:32:00.000Z",
        "voteCount": 13,
        "content": "HTTP 408, 429, and 5xx response codes.\n\nExponential backoff algorithm\nFor requests that meet both the response and idempotency criteria, you should generally use truncated exponential backoff.\n\nTruncated exponential backoff is a standard error handling strategy for network applications in which a client periodically retries a failed request with increasing delays between requests.\n\nAn exponential backoff algorithm retries requests exponentially, increasing the waiting time between retries up to a maximum backoff time. See the following workflow example to learn how exponential backoff works:\n\nYou make a request to Cloud Storage.\n\nIf the request fails, wait 1 + random_number_milliseconds seconds and retry the request.\n\nIf the request fails, wait 2 + random_number_milliseconds seconds and retry the request.\n\nIf the request fails, wait 4 + random_number_milliseconds seconds and retry the request.\n\nAnd so on, up to a maximum_backoff time.\n\nContinue waiting and retrying up to a maximum amount of time (deadline), but do not increase the maximum_backoff wait period between retries"
      },
      {
        "date": "2022-12-22T02:58:00.000Z",
        "voteCount": 3,
        "content": ". Implement retry logic using a truncated exponential backoff strategy.\n\nHTTP status codes of 5xx and 429 typically indicate that there is a temporary issue with the service or that the rate of requests is too high. To handle these types of errors, it is generally recommended to implement retry logic in your application using a truncated exponential backoff strategy.\n\nTruncated exponential backoff involves retrying the request after an initial delay, and then increasing the delay exponentially for each subsequent retry up to a maximum delay. This approach helps to reduce the number of failed requests and can improve the reliability of your application."
      },
      {
        "date": "2022-12-22T02:58:00.000Z",
        "voteCount": 2,
        "content": "Option A, using gRPC instead of HTTP for better performance, is not directly related to handling HTTP status codes of 5xx and 429. gRPC is a high-performance RPC framework that can be used in place of HTTP, but it is not a solution for handling errors.\n\nOption C, making sure the Cloud Storage bucket is multi-regional for geo-redundancy, may help improve the reliability of the service, but it is not a solution for handling errors.\n\nOption D, monitoring https://status.cloud.google.com/feed.atom and only making requests if Cloud Storage is not reporting an incident, is not a practical solution for handling errors. This approach would require constantly monitoring the status page and could result in significant delays in processing requests. Instead, it is generally recommended to implement retry logic in your application to handle errors."
      },
      {
        "date": "2022-11-10T02:02:00.000Z",
        "voteCount": 1,
        "content": "B is ok"
      },
      {
        "date": "2022-09-07T22:31:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/storage/docs/retry-strategy"
      },
      {
        "date": "2022-08-04T09:44:00.000Z",
        "voteCount": 2,
        "content": "2xx \u2013 successful requests;\n4xx, 5xx \u2013 failed requests;\n3xx \u2013 requests that require redirect.\nhttps://cloud.google.com/storage/docs/json_api/v1/status-codes"
      },
      {
        "date": "2021-12-08T10:51:00.000Z",
        "voteCount": 2,
        "content": "Go for B"
      },
      {
        "date": "2021-11-28T11:32:00.000Z",
        "voteCount": 2,
        "content": "B is the correct answer"
      },
      {
        "date": "2021-11-26T19:41:00.000Z",
        "voteCount": 2,
        "content": "Vote B"
      },
      {
        "date": "2021-11-24T17:29:00.000Z",
        "voteCount": 1,
        "content": "vote B"
      },
      {
        "date": "2021-10-29T23:22:00.000Z",
        "voteCount": 2,
        "content": "B \u2013 Implement retry logic using a truncated exponential backoff strategy.\nPer HTTP status and error codes for JSON the status codes are:\n2xx \u2013 successful requests;\n4xx, 5xx \u2013 failed requests;\n3xx \u2013 requests that require redirect.\nhttps://cloud.google.com/storage/docs/json_api/v1/status-codes\n429 \u2013 Too many requests: your app tries to use more that its limit, additional requests will fail. Decrease your client\u2019s requests and/or use truncated exponential backoff (used for all requests with 5xx and 429 errors).\nhttps://cloud.google.com/storage/docs/retry-strategy"
      },
      {
        "date": "2021-05-19T06:26:00.000Z",
        "voteCount": 2,
        "content": "B. Use Deployment Manager to automate service provisioning. Use Stackdriver to monitor and debug your tests."
      },
      {
        "date": "2021-05-19T06:28:00.000Z",
        "voteCount": 1,
        "content": "This B. Implement retry logic using a truncated exponential backoff strategy."
      },
      {
        "date": "2021-05-16T05:58:00.000Z",
        "voteCount": 1,
        "content": "Answer is B.\nLink provided by  bigob4ek has details"
      },
      {
        "date": "2021-04-01T00:56:00.000Z",
        "voteCount": 1,
        "content": "Answer is B"
      },
      {
        "date": "2021-02-15T09:44:00.000Z",
        "voteCount": 2,
        "content": "As per google, if you run into any issue as increase latency or erroe rate ,pause your ramp up this give cloudstorage more time to scale your bucket . Best is backoff when 5xx ,429,408 response code"
      },
      {
        "date": "2021-01-30T14:03:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/storage/docs/exponential-backoff\n- B"
      },
      {
        "date": "2020-10-07T23:15:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 96,
    "url": "https://www.examtopics.com/discussions/google/view/7125-exam-professional-cloud-architect-topic-1-question-96/",
    "body": "You need to develop procedures to test a disaster plan for a mission-critical application. You want to use Google-recommended practices and native capabilities within GCP.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Deployment Manager to automate service provisioning. Use Activity Logs to monitor and debug your tests.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Deployment Manager to automate service  provisioning. Use Stackdriver to monitor and debug your tests.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse gcloud scripts to automate service provisioning. Use Activity Logs to monitor and debug your tests.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse gcloud scripts to automate service provisioning. Use Stackdriver to monitor and debug your tests."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 7,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2019-10-24T05:18:00.000Z",
        "voteCount": 53,
        "content": "I think answer B is correct:\nhttps://cloud.google.com/solutions/dr-scenarios-planning-guide"
      },
      {
        "date": "2020-08-07T02:15:00.000Z",
        "voteCount": 11,
        "content": "B is ok"
      },
      {
        "date": "2020-11-01T08:34:00.000Z",
        "voteCount": 2,
        "content": "B is correct"
      },
      {
        "date": "2021-01-17T11:32:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/solutions/dr-scenarios-planning-guide#test_your_plan_regularly"
      },
      {
        "date": "2021-03-04T18:59:00.000Z",
        "voteCount": 11,
        "content": "It is B, Google Best practice ---&gt; never use scripts. They do not trust anyone else's code it seems."
      },
      {
        "date": "2019-12-18T13:31:00.000Z",
        "voteCount": 23,
        "content": "Boom, everyone studied and did their labs, stackdriver is google's recommended tool for monitoring and debbuging. I agree with u all that B is the correct answer"
      },
      {
        "date": "2023-09-09T06:33:00.000Z",
        "voteCount": 1,
        "content": "D its correct cause are 3 multiregions-availables and one bucket only can deploy in one multi region https://cloud.google.com/storage/docs/locations?hl=es-419#location-mr"
      },
      {
        "date": "2023-08-17T07:26:00.000Z",
        "voteCount": 1,
        "content": "I think B is the correct answer"
      },
      {
        "date": "2023-06-12T19:12:00.000Z",
        "voteCount": 2,
        "content": "Why not A?"
      },
      {
        "date": "2022-12-13T07:39:00.000Z",
        "voteCount": 1,
        "content": "Deploy managment + Stackdriver trained ig GCSB"
      },
      {
        "date": "2022-11-10T02:03:00.000Z",
        "voteCount": 1,
        "content": "B is ok"
      },
      {
        "date": "2022-10-16T09:14:00.000Z",
        "voteCount": 1,
        "content": "B is right"
      },
      {
        "date": "2022-08-15T06:17:00.000Z",
        "voteCount": 1,
        "content": "in practice, D could work as well.."
      },
      {
        "date": "2023-02-06T06:02:00.000Z",
        "voteCount": 1,
        "content": "yeah, but only native solutions should be taken into consideration (as stated in requirements), so scripts are basically ruled out"
      },
      {
        "date": "2022-04-07T03:56:00.000Z",
        "voteCount": 1,
        "content": "Answer B is correct"
      },
      {
        "date": "2022-03-03T09:49:00.000Z",
        "voteCount": 1,
        "content": "Deployment Manager + Cloud Monitoring and Logging solution."
      },
      {
        "date": "2021-12-08T10:53:00.000Z",
        "voteCount": 1,
        "content": "Go for B"
      },
      {
        "date": "2021-11-29T19:09:00.000Z",
        "voteCount": 1,
        "content": "B is the correct answer"
      },
      {
        "date": "2021-11-26T20:02:00.000Z",
        "voteCount": 1,
        "content": "Vote B"
      },
      {
        "date": "2021-11-18T08:28:00.000Z",
        "voteCount": 1,
        "content": "Google recommended Practice"
      },
      {
        "date": "2021-05-19T06:26:00.000Z",
        "voteCount": 4,
        "content": "B. Use Deployment Manager to automate service provisioning. Use Stackdriver to monitor and debug your tests."
      },
      {
        "date": "2021-05-16T05:59:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 97,
    "url": "https://www.examtopics.com/discussions/google/view/6466-exam-professional-cloud-architect-topic-1-question-97/",
    "body": "Your company creates rendering software which users can download from the company website. Your company has customers all over the world. You want to minimize latency for all your customers. You want to follow Google-recommended practices.<br>How should you store the files?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSave the files in a Multi-Regional Cloud Storage bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSave the files in a Regional Cloud Storage bucket, one bucket per zone of the region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSave the files in multiple Regional Cloud Storage buckets, one bucket per zone per region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSave the files in multiple Multi-Regional Cloud Storage buckets, one bucket per multi-region.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 56,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 22,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 10,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2019-11-14T09:46:00.000Z",
        "voteCount": 64,
        "content": "Its D, create multi region buckets in Americas, Europe and Asia"
      },
      {
        "date": "2022-05-17T00:15:00.000Z",
        "voteCount": 15,
        "content": "What is point of Multi-Regional bucket, if this need to saved multiple times. I believe option (D) is for creating confusion only. It should be (A).."
      },
      {
        "date": "2024-06-02T12:52:00.000Z",
        "voteCount": 2,
        "content": "Let's try option A: you select a single multi-region bucket (e.g. Americas). Are you improving the latency of your clients in Asia? You do not.\n\nThus, Option A is not complete."
      },
      {
        "date": "2022-05-17T00:18:00.000Z",
        "voteCount": 10,
        "content": "Read the question again.. I think (d) is correct.. eg. 1 bucket in US-multi-region, 2nd in AS-multi-region, 3rd in EU-multi-region"
      },
      {
        "date": "2023-03-22T16:32:00.000Z",
        "voteCount": 6,
        "content": "Yes, D seems correct. There are 3 multi-regions: ASIA, EU and US. In order to be global, there must be multi-region buckets in this 3 locations.\n\nReference: https://cloud.google.com/storage/docs/locations#location-mr"
      },
      {
        "date": "2022-11-16T17:50:00.000Z",
        "voteCount": 13,
        "content": "Check the current create bucket UI. You cannot select Asia multi-region and US multi-region at the same go. So to support global customer, you need to create multiple Multi-region buckets."
      },
      {
        "date": "2021-12-13T18:43:00.000Z",
        "voteCount": 6,
        "content": "This can't be D. It should be A."
      },
      {
        "date": "2023-07-14T02:00:00.000Z",
        "voteCount": 1,
        "content": "wrong its A"
      },
      {
        "date": "2020-01-01T21:18:00.000Z",
        "voteCount": 14,
        "content": "why \" multiple Multi-Regional\"?  - A should be the right ans &amp; addressing the global users - \"More importantly, is that multiregional heavily leverages Edge caching and CDNs to provide the content to the end user\"\nhttps://medium.com/google-cloud/google-cloud-storage-what-bucket-class-for-the-best-performance-5c847ac8f9f2"
      },
      {
        "date": "2021-08-20T07:20:00.000Z",
        "voteCount": 13,
        "content": "because a multi-regional includes all the locations of ONE region, not the others."
      },
      {
        "date": "2019-10-18T03:00:00.000Z",
        "voteCount": 32,
        "content": "I would go with A  (https://cloud.google.com/storage/docs/locations)"
      },
      {
        "date": "2024-06-02T12:53:00.000Z",
        "voteCount": 2,
        "content": "Its D, create multi region buckets in Americas, Europe and Asia"
      },
      {
        "date": "2024-05-28T06:43:00.000Z",
        "voteCount": 1,
        "content": "I think it is D. \nKeyword: Your company has customers all over the world.\nLists Multi Regional cloud\nMulti-Region Name\tMulti-Region Description\nASIA\tData centers in Asia, excluding Hong Kong and Indonesia\nEU\tData centers within member states of the European Union*\nUS\tData centers in the United States\n\n\nAns A. suggests \"A\" multi-regional Cloud. that means one of the above multi-regional cloud\nAns D: suggests  \"multiple\" Multi-Regional so 2 or (preferably) all of the multi-regional cloud with one bucket per multi-region (less task)"
      },
      {
        "date": "2024-04-07T01:35:00.000Z",
        "voteCount": 1,
        "content": "Agree with A"
      },
      {
        "date": "2024-03-11T17:00:00.000Z",
        "voteCount": 1,
        "content": "Answer is A.\nAs for D:  This would lead to data duplication and increased storage costs, as well as potential data consistency issues across different multi-regional buckets."
      },
      {
        "date": "2024-02-18T03:13:00.000Z",
        "voteCount": 2,
        "content": "Behind Multi-Regional Cloud Storage bucket, CDN is used hence good option to use for software download service."
      },
      {
        "date": "2024-02-17T04:07:00.000Z",
        "voteCount": 2,
        "content": "Buckets are not with zones, so B,C,D should be wrong. I go with A"
      },
      {
        "date": "2024-02-05T03:25:00.000Z",
        "voteCount": 1,
        "content": "I vote for A. Creating multiple multiregional Bucket is not seems practical and efficient. Besides that you can use CDN for minimizing latency also."
      },
      {
        "date": "2024-01-16T23:32:00.000Z",
        "voteCount": 1,
        "content": "One doubt - if we are going with option D, how can we handle this in the application logic - pointing to different buckets depending on region? Pls suggest."
      },
      {
        "date": "2023-12-28T22:09:00.000Z",
        "voteCount": 1,
        "content": "I'm going with C. Totally agree with 'theBestStudent' comment.\nMulti-regional offers more availability for files, but worse latency, which was the requirement.\nThis link says it all:\nhttps://cloud.google.com/storage/docs/locations#considerations\nIf you look at the table you can see the performance of the regional bucket (200Gbps) is much higher than the Multi-regional (50Gbps).\nAlso, Multi-regional buckets are 'only' available in US, Europe and East Asia. You would be letting behind places like South America, Africa, Canada, India, Indonesia, Middle East and Australia."
      },
      {
        "date": "2023-12-16T18:49:00.000Z",
        "voteCount": 4,
        "content": "It is C. Certainly is C, the other ones make no sense.\n \nMulti regional is mostly for HA, performance is lower than regional. Regional gives better latency, so what you need to do is to have multiple regional buckets, in different regions of course https://cloud.google.com/storage/docs/locations"
      },
      {
        "date": "2023-10-16T17:45:00.000Z",
        "voteCount": 1,
        "content": "A is correct. Regional and Dual-Regional buckets are optimized for latency. 200Gps vs. 50Gps of multi-region bucket. \nhttps://cloud.google.com/storage/docs/locations"
      },
      {
        "date": "2023-10-14T02:07:00.000Z",
        "voteCount": 2,
        "content": "A is the right answer- Multi-region buckets should be used for high availability / content delivery.\n\nD is the wrong answer - There is nothing called \"multiple multi region\". This coinage itself is wrong"
      },
      {
        "date": "2023-10-14T00:52:00.000Z",
        "voteCount": 1,
        "content": "D is correct. https://cloud.google.com/storage/docs/locations#location-mr"
      },
      {
        "date": "2023-09-29T22:54:00.000Z",
        "voteCount": 2,
        "content": "A:\nFirst: https://cloud.google.com/storage/docs/locations#location_recommendations\ntells us to use Multi-regioun for Content serving;\nSecond: if you define multiple buckets, then you have to handle the synch of files between them. This is much more difficult than just maybe configuring Cloud CDN for this bucket."
      },
      {
        "date": "2023-08-11T08:06:00.000Z",
        "voteCount": 4,
        "content": "I am not sure if documentation of GCS has changed recently. But I was reading about performance of each location type in GCS here - https://cloud.google.com/storage/docs/locations?_ga=2.177249189.-163973188.1685035440#considerations\n\nAnd found that multi-regional has 50 Gbps bandwidth whereas regional has 200 Gbps. So with that context, it sounds more relevant to pick regional or dual-regional location type and option A &amp; D out of the race now.\nNow it is clear that if we stick to 1 regional bucket, then lower latency will only be catered to that region. And hence we need multiple regional buckets. Conclusively, Option C seems more precise option as of today (11th Aug 2023) to me."
      },
      {
        "date": "2024-07-28T03:44:00.000Z",
        "voteCount": 1,
        "content": "Best option for latency yes, \nBut now you need to deal with managing data consistency across all those buckets not to mention the ridiculous cost of data replication in every gcp region, D seems to the best middlegroud to me"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 98,
    "url": "https://www.examtopics.com/discussions/google/view/7376-exam-professional-cloud-architect-topic-1-question-98/",
    "body": "Your company acquired a healthcare startup and must retain its customers' medical information for up to 4 more years, depending on when it was created. Your corporate policy is to securely retain this data, and then delete it as soon as regulations allow.<br>Which approach should you take?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the data in Google Drive and manually delete records as they expire.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAnonymize the data using the Cloud Data Loss Prevention API and store it indefinitely.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the data in Cloud Storage and use lifecycle management to delete files when they expire.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the data in Cloud Storage and run a nightly batch script that deletes all expired data."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-01-12T07:41:00.000Z",
        "voteCount": 23,
        "content": "Agree C"
      },
      {
        "date": "2023-02-26T13:05:00.000Z",
        "voteCount": 1,
        "content": "It's C"
      },
      {
        "date": "2022-11-10T02:11:00.000Z",
        "voteCount": 1,
        "content": "C is ok"
      },
      {
        "date": "2022-10-16T09:05:00.000Z",
        "voteCount": 1,
        "content": "I agree with C"
      },
      {
        "date": "2022-08-13T19:42:00.000Z",
        "voteCount": 4,
        "content": "I got this question in exam."
      },
      {
        "date": "2022-08-04T19:20:00.000Z",
        "voteCount": 1,
        "content": "go for C"
      },
      {
        "date": "2022-06-19T11:39:00.000Z",
        "voteCount": 1,
        "content": "Options C undoubtedly"
      },
      {
        "date": "2022-04-07T04:03:00.000Z",
        "voteCount": 1,
        "content": "Go for C"
      },
      {
        "date": "2022-02-11T13:37:00.000Z",
        "voteCount": 3,
        "content": "I got similar question on my exam which involved life cycle management and bucket lock."
      },
      {
        "date": "2021-12-27T04:15:00.000Z",
        "voteCount": 1,
        "content": "Go for C"
      },
      {
        "date": "2021-12-08T11:14:00.000Z",
        "voteCount": 1,
        "content": "Go for C"
      },
      {
        "date": "2021-12-01T01:11:00.000Z",
        "voteCount": 1,
        "content": "C is the correct answer"
      },
      {
        "date": "2021-11-30T06:07:00.000Z",
        "voteCount": 1,
        "content": "D sounds like i would do it, but C sound like a lab or exam"
      },
      {
        "date": "2021-11-26T20:05:00.000Z",
        "voteCount": 1,
        "content": "Vote C"
      },
      {
        "date": "2021-05-20T05:45:00.000Z",
        "voteCount": 4,
        "content": "C. Store the data in Cloud Storage and use lifecycle management to delete files when they expire."
      },
      {
        "date": "2021-05-16T06:15:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2021-04-23T07:03:00.000Z",
        "voteCount": 1,
        "content": "Agree C"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 99,
    "url": "https://www.examtopics.com/discussions/google/view/7377-exam-professional-cloud-architect-topic-1-question-99/",
    "body": "You are deploying a PHP App Engine Standard service with Cloud SQL as the backend. You want to minimize the number of queries to the database.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the memcache service level to dedicated. Create a key from the hash of the query, and return database values from memcache before issuing a query to Cloud SQL.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the memcache service level to dedicated. Create a cron task that runs every minute to populate the cache with keys containing query results.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the memcache service level to shared. Create a cron task that runs every minute to save all expected queries to a key called \u05d2\u20accached_queries\u05d2\u20ac.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the memcache service level to shared. Create a key called \u05d2\u20accached_queries\u05d2\u20ac, and return database values from the key before using a query to Cloud SQL."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-12-13T11:56:00.000Z",
        "voteCount": 28,
        "content": "A dedicated memset is always better than shared until cost-effectiveness specify in the exam as objective. So Option C and D are ruled out. \n\nFrom A and B, Option B is sending and updating query every minutes which is over killing. So reasonable option left with A which balance performance and cost. \n\nMy answer will be A"
      },
      {
        "date": "2023-11-15T13:40:00.000Z",
        "voteCount": 1,
        "content": "Good job bro"
      },
      {
        "date": "2019-10-28T08:18:00.000Z",
        "voteCount": 23,
        "content": "https://cloud.google.com/appengine/docs/standard/php/memcache/using"
      },
      {
        "date": "2020-08-07T02:26:00.000Z",
        "voteCount": 11,
        "content": "A is ok"
      },
      {
        "date": "2021-06-09T07:13:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/memorystore/docs/redis/redis-overview"
      },
      {
        "date": "2021-03-04T19:15:00.000Z",
        "voteCount": 6,
        "content": "A is correct"
      },
      {
        "date": "2023-05-08T22:33:00.000Z",
        "voteCount": 1,
        "content": "Best is A"
      },
      {
        "date": "2022-11-10T02:14:00.000Z",
        "voteCount": 1,
        "content": "A is ok"
      },
      {
        "date": "2022-10-16T09:02:00.000Z",
        "voteCount": 1,
        "content": "A is fine.. dedicated mem cache"
      },
      {
        "date": "2022-08-13T19:44:00.000Z",
        "voteCount": 5,
        "content": "I got this question in exam."
      },
      {
        "date": "2022-08-04T19:23:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/appengine/docs/standard/php/memcache/using"
      },
      {
        "date": "2022-04-07T04:05:00.000Z",
        "voteCount": 1,
        "content": "Obviously, the answer is A"
      },
      {
        "date": "2021-12-28T09:04:00.000Z",
        "voteCount": 3,
        "content": "Dedicated and shared will resolve the problem, the key is: store all queries in only one key \"cached_queries\" is not good, we have limits: https://cloud.google.com/appengine/docs/standard/python/memcache\nCreate a key of each query is better."
      },
      {
        "date": "2021-12-01T01:14:00.000Z",
        "voteCount": 2,
        "content": "A is the correct answer"
      },
      {
        "date": "2021-11-26T20:06:00.000Z",
        "voteCount": 1,
        "content": "Vote A"
      },
      {
        "date": "2021-11-25T05:45:00.000Z",
        "voteCount": 1,
        "content": "vote A"
      },
      {
        "date": "2021-05-20T05:45:00.000Z",
        "voteCount": 3,
        "content": "A. Set the memcache service level to dedicated. Create a key from the hash of the query, and return database values from memcache before issuing a query to Cloud SQL."
      },
      {
        "date": "2021-05-16T06:16:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2021-04-06T22:18:00.000Z",
        "voteCount": 1,
        "content": "Answer is A"
      },
      {
        "date": "2021-02-23T10:07:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2021-02-20T07:09:00.000Z",
        "voteCount": 1,
        "content": "My answer is A"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 100,
    "url": "https://www.examtopics.com/discussions/google/view/7233-exam-professional-cloud-architect-topic-1-question-100/",
    "body": "You need to ensure reliability for your application and operations by supporting reliable task scheduling for compute on GCP. Leveraging Google best practices, what should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUsing the Cron service provided by App Engine, publish messages directly to a message-processing utility service running on Compute Engine instances.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUsing the Cron service provided by App Engine, publish messages to a Cloud Pub/Sub topic. Subscribe to that topic using a message-processing utility service running on Compute Engine instances.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUsing the Cron service provided by Google Kubernetes Engine (GKE), publish messages directly to a message-processing utility service running on Compute Engine instances.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUsing the Cron service provided by GKE, publish messages to a Cloud Pub/Sub topic. Subscribe to that topic using a message-processing utility service running on Compute Engine instances."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 24,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2019-11-14T09:47:00.000Z",
        "voteCount": 32,
        "content": "Answer is B"
      },
      {
        "date": "2020-02-25T12:32:00.000Z",
        "voteCount": 29,
        "content": "B is correct. More appropriately: https://cloud.google.com/solutions/reliable-task-scheduling-compute-engine"
      },
      {
        "date": "2021-01-17T12:02:00.000Z",
        "voteCount": 4,
        "content": "https://cloud.google.com/solutions/reliable-task-scheduling-compute-engine#schedule-compute-engine"
      },
      {
        "date": "2024-02-02T03:53:00.000Z",
        "voteCount": 1,
        "content": "You can create Cron job using HTTP endpoint, Pub/Sub and App engine."
      },
      {
        "date": "2023-11-18T04:22:00.000Z",
        "voteCount": 14,
        "content": "Answer is B, but this question is outdated, Today the best practices for cron is Cloud Scheduler: fully managed enterprise-grade cron job scheduler\nhttps://cloud.google.com/scheduler/?gad_source=1&amp;gclsrc=ds&amp;gclsrc=ds"
      },
      {
        "date": "2024-06-02T12:57:00.000Z",
        "voteCount": 1,
        "content": "Thanks... I was a little confused by this options"
      },
      {
        "date": "2023-10-11T01:25:00.000Z",
        "voteCount": 8,
        "content": "This seems to be an old question, despite B could be the more correct answer, it is not exactly a good one. 'Using the Cron service provided by App Engine', the cron service is provided by Cloud Scheduler, not App Engine. App Engine HTTP endpoint can be a target for the cron task."
      },
      {
        "date": "2023-05-04T02:23:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/blog/products/gcp/reliable-task-scheduling-on-google-compute-engine"
      },
      {
        "date": "2023-03-12T12:44:00.000Z",
        "voteCount": 6,
        "content": "Something feels missing/broken about this question\n\nEven before comments in discussion that correctly mentioned Cloud Scheduler, which is not mentioned in the question"
      },
      {
        "date": "2023-12-16T00:58:00.000Z",
        "voteCount": 1,
        "content": "This is because cloud scheduler is a newly released service which is a replacement to cloud app engine cron service."
      },
      {
        "date": "2023-01-17T17:55:00.000Z",
        "voteCount": 2,
        "content": "\"By using Cloud Scheduler for scheduling and Pub/Sub for distributed messaging, you can build an application to reliably schedule tasks across a fleet of Compute Engine instances.\"  https://cloud.google.com/architecture/reliable-task-scheduling-compute-engine\n\nAnswer is B.  (Note: It was down to B or D but containerization was not mentioned)"
      },
      {
        "date": "2022-12-31T15:25:00.000Z",
        "voteCount": 8,
        "content": "Answer is B.\nCloud Scheduler provides a fully managed, enterprise-grade service that lets you schedule events. After you have scheduled a job, Cloud Scheduler will call the configured event handlers, which can be App Engine services, HTTP endpoints, or Pub/Sub subscriptions.\n\nTo run tasks on your Compute Engine instance in response to Cloud Scheduler events, you need to relay the events to those instances. One way to do this is by calling an HTTP endpoint that runs on your Compute Engine instances. Another option is to pass messages from Cloud Scheduler to your Compute Engine instances using Pub/Sub."
      },
      {
        "date": "2022-11-29T11:14:00.000Z",
        "voteCount": 2,
        "content": "A and C are out\u2026 messages are to be sent to pub sub and processed using a client. D is overkill for this purpose"
      },
      {
        "date": "2022-11-10T02:18:00.000Z",
        "voteCount": 1,
        "content": "B is ok"
      },
      {
        "date": "2022-10-15T14:10:00.000Z",
        "voteCount": 1,
        "content": "B is right"
      },
      {
        "date": "2022-09-16T22:33:00.000Z",
        "voteCount": 1,
        "content": "Ans is B https://cloud.google.com/architecture/reliable-task-scheduling-compute-engine"
      },
      {
        "date": "2022-09-08T01:39:00.000Z",
        "voteCount": 1,
        "content": "Ans is B https://cloud.google.com/architecture/reliable-task-scheduling-compute-engine\n\nrefer the examples with diagram"
      },
      {
        "date": "2022-09-15T08:16:00.000Z",
        "voteCount": 3,
        "content": "the link points to use Cloud Scheduler, and not Cron service provided by App Engine."
      },
      {
        "date": "2022-10-17T04:29:00.000Z",
        "voteCount": 2,
        "content": "This is the new way to run schedule"
      },
      {
        "date": "2022-09-01T02:16:00.000Z",
        "voteCount": 5,
        "content": "I got this question in exam 01/09/2022"
      },
      {
        "date": "2022-08-27T17:00:00.000Z",
        "voteCount": 1,
        "content": "This solution can be implemented using both A and D\n1) With App Engine - https://cloud.google.com/appengine/docs/flexible/nodejs/scheduling-jobs-with-cron-yaml\n2) With GKE - https://cloud.google.com/kubernetes-engine/docs/how-to/cronjobs\n\nThey ask for best practices and it's well known that GKE (aka containers) is the best practice for building modern infra solution. \n\nYet another confusing PCA question on the card. Honestly, think the quality of the questions can be mightily improved."
      },
      {
        "date": "2023-04-22T00:13:00.000Z",
        "voteCount": 1,
        "content": "A is a bad solution as \"send message directly to the utility\" is not really reliable, you'd want pub/sub in between."
      },
      {
        "date": "2022-08-27T17:01:00.000Z",
        "voteCount": 2,
        "content": "Sorry, can be implemented using both B and D"
      },
      {
        "date": "2023-07-27T02:28:00.000Z",
        "voteCount": 1,
        "content": "You right, but D is overkill. \nSo B if the best practices for this task."
      },
      {
        "date": "2022-10-21T05:25:00.000Z",
        "voteCount": 1,
        "content": "GKE is too expensive if all you are after is cron scheduling."
      },
      {
        "date": "2022-08-25T21:32:00.000Z",
        "voteCount": 2,
        "content": "B says Appengine.\nBut Cloud Scheduler is itself a managed service.\nTo schedule jobs via AppEngine, the cron.yaml has to be used.\nIt can be done similarly via GKE as well.\nThis question is confusing"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 101,
    "url": "https://www.examtopics.com/discussions/google/view/6306-exam-professional-cloud-architect-topic-1-question-101/",
    "body": "Your company is building a new architecture to support its data-centric business focus. You are responsible for setting up the network. Your company's mobile and web-facing applications will be deployed on-premises, and all data analysis will be conducted in GCP. The plan is to process and load 7 years of archived .csv files totaling 900 TB of data and then continue loading 10 TB of data daily. You currently have an existing 100-MB internet connection.<br>What actions will meet your company's needs?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCompress and upload both archived  files and files uploaded daily using the gsutil \u05d2\u20ac\"m option.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLease a Transfer Appliance, upload archived files to it, and send it to Google to transfer archived data to Cloud Storage. Establish a connection with Google using a Dedicated Interconnect or Direct Peering connection and use it to upload files daily.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLease a Transfer Appliance, upload archived files to it, and send it to Google to transfer archived data to Cloud Storage. Establish one Cloud VPN Tunnel to VPC networks over the public internet, and compress and upload files daily using the gsutil \u05d2\u20ac\"m option.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLease a Transfer Appliance, upload archived files to it, and send it to Google to transfer archived data to Cloud Storage. Establish a Cloud VPN Tunnel to VPC networks over the public internet, and compress and upload files daily."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 16,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2019-10-08T22:42:00.000Z",
        "voteCount": 51,
        "content": "With option A, daily data would take 27 hours.\nMy answer is B.\nHow do you think?"
      },
      {
        "date": "2024-02-02T03:46:00.000Z",
        "voteCount": 1,
        "content": "Also with option A you need months to download archive files(900TB)"
      },
      {
        "date": "2020-11-01T09:31:00.000Z",
        "voteCount": 5,
        "content": "B is correct"
      },
      {
        "date": "2020-12-18T06:53:00.000Z",
        "voteCount": 5,
        "content": "ok but dedicated connection is available from 10 GBPS right where as in question it says internet connection is 100 MB, to me D is correct."
      },
      {
        "date": "2022-06-02T21:14:00.000Z",
        "voteCount": 5,
        "content": "Dedicated Interconnect will be a new connection and will not run over the existing internet connection. With dedicated interconnect the existing ISP becomes irrelevant. If you were trying to use VPN the existing internet connection would be relevant. Answer is B."
      },
      {
        "date": "2021-03-04T19:18:00.000Z",
        "voteCount": 7,
        "content": "it is B"
      },
      {
        "date": "2021-05-20T11:20:00.000Z",
        "voteCount": 2,
        "content": "Direct peering is meant only to connect to G Suite Services. Its reference may invalidate the whole answer."
      },
      {
        "date": "2022-06-24T10:14:00.000Z",
        "voteCount": 1,
        "content": "True. B is the most apt answer with just this extra bit \"direct peering\" raising some confusion."
      },
      {
        "date": "2019-10-19T21:31:00.000Z",
        "voteCount": 21,
        "content": "Agree B. 100Mbps connections for 10TB data transfer is takes too long\n\nhttps://cloud.google.com/solutions/transferring-big-data-sets-to-gcp#close"
      },
      {
        "date": "2019-12-03T22:49:00.000Z",
        "voteCount": 3,
        "content": "not 100Mbps. 100MB"
      },
      {
        "date": "2020-06-01T13:07:00.000Z",
        "voteCount": 5,
        "content": "even with 100MB internet it's slow. It's 800 Mbps and transfer for 10 TB will take 2 days"
      },
      {
        "date": "2021-02-22T09:40:00.000Z",
        "voteCount": 5,
        "content": "There is no such thing as a \"100MB\" internet connection :) . That must be a speed (per second), and I would guess that the \"B\" is just a typo (it is highly atypical to measure bandwidth in Bps)."
      },
      {
        "date": "2024-07-11T09:23:00.000Z",
        "voteCount": 1,
        "content": "There is no option than B."
      },
      {
        "date": "2024-05-25T11:32:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2024-01-25T13:12:00.000Z",
        "voteCount": 1,
        "content": "B would be correct as pubsub b service might redeliver messages. When you receive messages in order and the Pub/Sub service redelivers a message with an ordering key, Pub/Sub maintains order by also redelivering the subsequent messages with the same ordering key. The Pub/Sub service redelivers these messages in the order that it originally received them."
      },
      {
        "date": "2024-01-06T21:56:00.000Z",
        "voteCount": 1,
        "content": "B has an option of direct peering too which is not a recommended practise"
      },
      {
        "date": "2023-12-05T07:53:00.000Z",
        "voteCount": 1,
        "content": "B makes sense"
      },
      {
        "date": "2023-11-18T04:28:00.000Z",
        "voteCount": 1,
        "content": "B. https://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets#time"
      },
      {
        "date": "2023-10-16T01:38:00.000Z",
        "voteCount": 1,
        "content": "I don't think the company needs \"dedicated intereconnect\" - because it is clearly said that the company wants to do \"data-centric\" business. Dedicated interconnect is more for having private-access to google cloud. \n\nC seems like a correct option to me"
      },
      {
        "date": "2023-11-18T04:27:00.000Z",
        "voteCount": 1,
        "content": "It's B, transferring over 100mbps 10TB daily is not possible, because could take up to 12 days.. You need a better connection."
      },
      {
        "date": "2023-02-27T08:22:00.000Z",
        "voteCount": 1,
        "content": "Answer B =&gt; Dedicated interconnect will provide a private network with 10gbs. The internet limited to 100 mb is not possible to use cloud VPN ( it will use public internet so be limited for the daily)"
      },
      {
        "date": "2022-12-30T23:42:00.000Z",
        "voteCount": 1,
        "content": "B is correct."
      },
      {
        "date": "2022-11-29T11:16:00.000Z",
        "voteCount": 1,
        "content": "B. Since it is a new network just sign up for a dedicated line\u2026"
      },
      {
        "date": "2022-11-10T02:33:00.000Z",
        "voteCount": 1,
        "content": "B is ok"
      },
      {
        "date": "2022-10-26T21:59:00.000Z",
        "voteCount": 1,
        "content": "its option B. i think"
      },
      {
        "date": "2022-10-17T19:31:00.000Z",
        "voteCount": 2,
        "content": "you can not use gsutil to load 10TB daily &gt;&gt;&gt;and then continue loading 10 TB of data daily&lt;&lt;&lt; it will take longer than 24hrs to upload using gsutil"
      },
      {
        "date": "2022-10-15T14:11:00.000Z",
        "voteCount": 2,
        "content": "B is the best, VPN doesn't scale very well for huge data"
      },
      {
        "date": "2022-09-16T22:45:00.000Z",
        "voteCount": 1,
        "content": "B is ok"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 102,
    "url": "https://www.examtopics.com/discussions/google/view/6747-exam-professional-cloud-architect-topic-1-question-102/",
    "body": "You are developing a globally scaled frontend for a legacy streaming backend data API. This API expects events in strict chronological order with no repeat data for proper processing.<br>Which products should you deploy to ensure guaranteed-once FIFO (first-in, first-out) delivery of data?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Pub/Sub alone",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Pub/Sub to Cloud Dataflow\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Pub/Sub to Stackdriver",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Pub/Sub to Cloud SQL"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 44,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 33,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-01-02T07:49:00.000Z",
        "voteCount": 68,
        "content": "I believe the answer is B.  \"Pub/Sub doesn't provide guarantees about the order of message delivery. Strict message ordering can be achieved with buffering, often using Dataflow.\"  https://cloud.google.com/solutions/data-lifecycle-cloud-platform"
      },
      {
        "date": "2021-04-24T15:09:00.000Z",
        "voteCount": 9,
        "content": "Now Pub/Sub guarantees message order. Until the exam does not change I would pick B."
      },
      {
        "date": "2021-09-29T22:34:00.000Z",
        "voteCount": 13,
        "content": "Answer is B. The question is talking about guaranteed-once FIFO delivery of data. Although Pub/sub provides data in order (FIFO) but it does 'at-least' once delivery of data. So, we need Dataflow for deduplication of data."
      },
      {
        "date": "2022-10-10T17:17:00.000Z",
        "voteCount": 9,
        "content": "I believe Pub/Sub now also supports exactly once delivery (in preview):\nhttps://cloud.google.com/pubsub/docs/exactly-once-delivery"
      },
      {
        "date": "2022-10-15T12:51:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/pubsub/docs/exactly-once-delivery\nreference"
      },
      {
        "date": "2022-10-15T12:50:00.000Z",
        "voteCount": 8,
        "content": "Pub/Sub supports exactly-once delivery, within a cloud region.\nThe question states \u00a8global\u00a8, so needs Dataflow"
      },
      {
        "date": "2023-01-29T08:24:00.000Z",
        "voteCount": 3,
        "content": "I believe that only the frontend is scaled globally. The backend API is the one that requires ordered delivery of the messages and guaranteed-once delivery of data. Currently, Pub/Sub supports ordered delivery within the same region (https://cloud.google.com/pubsub/docs/ordering#receiving_messages_in_order) and exactly-once delivery within the same region (https://cloud.google.com/pubsub/docs/exactly-once-delivery#exactly-once_delivery_guarantees).\nThe right answer could be A, Pub/Sub alone."
      },
      {
        "date": "2022-03-13T08:52:00.000Z",
        "voteCount": 1,
        "content": "the correct is B https://cloud.google.com/pubsub/docs/stream-messages-dataflow"
      },
      {
        "date": "2020-05-03T18:44:00.000Z",
        "voteCount": 28,
        "content": "B is the answer. CloudSQL is only for storage, to get the messages in order you need timestamp processed in dataflow to arrange them before putting it in any storage volume. The system described is not querying a db it is expecting a stream of messages only dataflow can correct the order. ACID has no value here because the db is not being queried. You'll not find any documentation on pub/sub order being corrected with a db. See notes below on pub/sub and dataflow using timestamps and windows to ensure order\n\nhttps://cloud.google.com/pubsub/docs/pubsub-dataflow"
      },
      {
        "date": "2024-07-26T14:05:00.000Z",
        "voteCount": 3,
        "content": "Google Cloud Pub/Sub now supports message ordering, which ensures that messages with the same ordering key are delivered in the exact order they were published. This feature addresses the requirement for strict chronological order without the need for additional services.\n\nKey Features of Cloud Pub/Sub with Message Ordering:\nMessage Ordering: By using ordering keys, Pub/Sub can guarantee that messages are delivered in the order they are published.\nExactly-once Delivery: Pub/Sub supports at-least-once delivery and can be configured to handle duplicate messages.\nScalability and Reliability: Pub/Sub is a fully managed service that scales automatically and ensures high availability."
      },
      {
        "date": "2024-05-20T06:34:00.000Z",
        "voteCount": 5,
        "content": "A. Cloud Pub/Sub alone\n\nCloud Pub/Sub Ordered Delivery: Cloud Pub/Sub natively supports ordered delivery when using the same ordering key. This guarantees that messages with the same key are delivered to subscribers in the order they were published, preventing out-of-order events.\nExactly-once Delivery: Cloud Pub/Sub also offers exactly-once delivery within a region, ensuring that each message is delivered to a subscriber only once."
      },
      {
        "date": "2024-05-16T18:01:00.000Z",
        "voteCount": 1,
        "content": "Pub/Sub provide ordered delivery but doesn't ensure deduplication. Cloud SQL is regional resource and it cant perform custom logic. I go with B"
      },
      {
        "date": "2024-03-03T14:20:00.000Z",
        "voteCount": 2,
        "content": "A is correct. While Cloud Dataflow can be used for data processing, it doesn't guarantee FIFO order on its own. Additionally, introducing another processing layer adds complexity and might not be necessary for this specific requirement."
      },
      {
        "date": "2024-02-13T18:50:00.000Z",
        "voteCount": 1,
        "content": "Dont see a need for SQL here as the answer suggests. Option B is more appropriate"
      },
      {
        "date": "2024-01-25T13:14:00.000Z",
        "voteCount": 1,
        "content": "B would be correct as pubsub b service might redeliver messages. When you receive messages in order and the Pub/Sub service redelivers a message with an ordering key, Pub/Sub maintains order by also redelivering the subsequent messages with the same ordering key. The Pub/Sub service redelivers these messages in the order that it originally received them."
      },
      {
        "date": "2024-01-21T06:26:00.000Z",
        "voteCount": 3,
        "content": "b, even if pubsub now have exactly-once-delivery this is within a region. question is for a global app.\nhttps://cloud.google.com/pubsub/docs/exactly-once-delivery#exactly-once_delivery_2"
      },
      {
        "date": "2023-10-22T20:03:00.000Z",
        "voteCount": 1,
        "content": "Answer is B.\nKey words: globally scaled  and ensure delivery only once: Pubsub + Dataflow.\n\nIf it were only one region, it would be fine to say just pubsub, but it is globally scaled."
      },
      {
        "date": "2023-09-08T06:28:00.000Z",
        "voteCount": 2,
        "content": "B, you need dataflow to deduplicate. Pub/Sub does \"at-least\" once delivery."
      },
      {
        "date": "2023-08-13T19:19:00.000Z",
        "voteCount": 1,
        "content": "It should be A"
      },
      {
        "date": "2023-06-12T12:57:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/pubsub/docs/exactly-once-delivery"
      },
      {
        "date": "2023-06-07T05:20:00.000Z",
        "voteCount": 1,
        "content": "Answer: B"
      },
      {
        "date": "2023-05-15T10:45:00.000Z",
        "voteCount": 1,
        "content": "Pub/Sub and Dataflow"
      },
      {
        "date": "2023-04-22T02:40:00.000Z",
        "voteCount": 1,
        "content": "Pub/Sub is an at-least-once service. So you need to deduplicate messages with DataFlow as a data pipeline."
      },
      {
        "date": "2023-04-12T01:57:00.000Z",
        "voteCount": 3,
        "content": "B\nCloud Pub/Sub to Cloud Dataflow: Cloud Dataflow is a managed data processing service that can be used to process and transform streaming data. By using Cloud Dataflow with Cloud Pub/Sub, you can implement custom processing logic to ensure data is delivered in strict chronological order with no repeats. This combination allows you to achieve guaranteed-once FIFO delivery."
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 103,
    "url": "https://www.examtopics.com/discussions/google/view/56841-exam-professional-cloud-architect-topic-1-question-103/",
    "body": "Your company is planning to perform a lift and shift migration of their Linux RHEL 6.5+ virtual machines. The virtual machines are running in an on-premises<br>VMware environment. You want to migrate them to Compute Engine following Google-recommended practices. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Define a migration plan based on the list of the applications and their dependencies. 2. Migrate all virtual machines into Compute Engine individually with Migrate for Compute Engine.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Perform an assessment of virtual machines running in the current VMware environment. 2. Create images of all disks. Import disks on Compute Engine. 3. Create standard virtual machines where the boot disks are the ones you have imported.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Perform an assessment of virtual machines running in the current VMware environment. 2. Define a migration plan, prepare a Migrate for Compute Engine migration RunBook, and execute the migration.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Perform an assessment of virtual machines running in the current VMware environment. 2. Install a third-party agent on all selected virtual machines. 3. Migrate all virtual machines into Compute Engine."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 23,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-07-01T14:53:00.000Z",
        "voteCount": 34,
        "content": "Ans ) C ,\nMigrate for Compute Engine organizes groups of VMs into Waves. After understanding the dependencies of your applications, create runbooks that contain groups of VMs and begin your migration!\nhttps://cloud.google.com/migrate/compute-engine/docs/4.5/how-to/migrate-on-premises-to-gcp/overview"
      },
      {
        "date": "2022-01-19T09:17:00.000Z",
        "voteCount": 15,
        "content": "I got this question in my exam."
      },
      {
        "date": "2023-05-08T22:56:00.000Z",
        "voteCount": 1,
        "content": "Did u passed...? If yes, then Congratulations and let me know the correct answer"
      },
      {
        "date": "2024-10-01T07:03:00.000Z",
        "voteCount": 1,
        "content": "Assess, Plan, Migrate. Textbook perfect"
      },
      {
        "date": "2023-05-10T07:42:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/migrate/compute-engine/docs/4.11/how-to/migrate-on-premises-to-gcp/overview"
      },
      {
        "date": "2023-02-15T18:49:00.000Z",
        "voteCount": 2,
        "content": "The reason why Option B is preferable over Option C is that it involves creating images of all disks and importing them into Compute Engine, which can significantly reduce the amount of time required for the migration. Additionally, creating standard virtual machines from the imported disks is a straightforward process, and it ensures that the migrated virtual machines are identical to the on-premises virtual machines, which can simplify the migration process and minimize the risk of compatibility issues."
      },
      {
        "date": "2023-01-01T16:56:00.000Z",
        "voteCount": 1,
        "content": "C is the correct answer,\n\nRunbooks are created from the Migrate for Compute Engine Manager. The system queries VMware or AWS for VMs and generates a CSV for you to edit.\n\nBy editing the CSV, you define:\n\nThe VMs in a wave.\nThe order in which those VMs are migrated.\nThe type and disk space of VMs that are launched on Google Cloud.\nOther characteristics that are defined in the Runbook reference.\n\nhttps://cloud.google.com/migrate/compute-engine/docs/4.8/how-to/organizing-migrations/creating-and-modifying-runbooks#generating_runbook_templates"
      },
      {
        "date": "2022-11-12T03:13:00.000Z",
        "voteCount": 1,
        "content": "C is ok"
      },
      {
        "date": "2022-10-15T14:14:00.000Z",
        "voteCount": 1,
        "content": "C is most suitable for this use"
      },
      {
        "date": "2022-08-13T19:46:00.000Z",
        "voteCount": 5,
        "content": "I got this question in exam."
      },
      {
        "date": "2022-07-02T12:58:00.000Z",
        "voteCount": 2,
        "content": "C is right, It defines all logical steps to migrate on-premise to google cloud."
      },
      {
        "date": "2022-04-16T14:11:00.000Z",
        "voteCount": 2,
        "content": "Does Ans. C) still valid as of latest GCE 5.0? \nin the doc \"Migrating VM groups\" with version GCE 5.0, I do not see \"runbook\" anymore which is explained up to version GCE 4.8.\nhttps://cloud.google.com/migrate/compute-engine/docs/5.0/how-to/migrating-vm-groups"
      },
      {
        "date": "2022-04-07T04:20:00.000Z",
        "voteCount": 1,
        "content": "Go for C"
      },
      {
        "date": "2022-02-11T13:38:00.000Z",
        "voteCount": 2,
        "content": "I got this question on my exam. Answered C."
      },
      {
        "date": "2023-05-08T22:57:00.000Z",
        "voteCount": 1,
        "content": "Thanks"
      },
      {
        "date": "2021-12-09T01:40:00.000Z",
        "voteCount": 1,
        "content": "Go for C."
      },
      {
        "date": "2021-12-05T03:05:00.000Z",
        "voteCount": 1,
        "content": "why not A?\nseems pretty obvious if you look at the google doc: https://cloud.google.com/migrate/compute-engine/docs/5.0/concepts/lifecycle"
      },
      {
        "date": "2021-12-24T09:12:00.000Z",
        "voteCount": 3,
        "content": "When you are doing cloud migrations, you do migrations in \"waves\" which are groupings of one or more applications/workloads. Moving machines individually would break things, such as dependencies. This is standard industry practice."
      },
      {
        "date": "2021-12-01T04:50:00.000Z",
        "voteCount": 1,
        "content": "C is the correct answer."
      },
      {
        "date": "2021-11-25T05:48:00.000Z",
        "voteCount": 2,
        "content": "vote C"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 104,
    "url": "https://www.examtopics.com/discussions/google/view/56360-exam-professional-cloud-architect-topic-1-question-104/",
    "body": "You need to deploy an application to Google Cloud. The application receives traffic via TCP and reads and writes data to the filesystem. The application does not support horizontal scaling. The application process requires full control over the data on the file system because concurrent access causes corruption. The business is willing to accept a downtime when an incident occurs, but the application must be available 24/7 to support their business operations. You need to design the architecture of this application on Google Cloud. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a managed instance group with instances in multiple zones, use Cloud Filestore, and use an HTTP load balancer in front of the instances.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a managed instance group with instances in multiple zones, use Cloud Filestore, and use a network load balancer in front of the instances.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an unmanaged instance group with an active and standby instance in different zones, use a regional persistent disk, and use an HTTP load balancer in front of the instances.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an unmanaged instance group with an active and standby instance in different zones, use a regional persistent disk, and use a network load balancer in front of the instances.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 44,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 10,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-07-06T10:44:00.000Z",
        "voteCount": 62,
        "content": "Correct Ans : D \nSince the Traffic is TCP, Ans A &amp; C gets eliminated as HTTPS load balance is not supported.\nB - File storage system is Cloud Firestore which do not give full control, hence eliminated.\nD - Unmanaged instance group with network load balance with regional persistent disk for storage gives full control which is required for the migration."
      },
      {
        "date": "2021-07-27T17:08:00.000Z",
        "voteCount": 10,
        "content": "almost all good, except for File Storage, is not Cloud Firestore, it is a new service for sharing filesystems across VMs (like a NAS in a traditional infrastructure)."
      },
      {
        "date": "2022-06-05T07:47:00.000Z",
        "voteCount": 7,
        "content": "what about the fact that is the unmanaged instance group is not regional , so you can't create it in more than 1 zone ?"
      },
      {
        "date": "2023-08-02T17:39:00.000Z",
        "voteCount": 1,
        "content": "Can we group to running instances in different zones to an unmanaged instance group?"
      },
      {
        "date": "2021-07-01T14:53:00.000Z",
        "voteCount": 28,
        "content": "Ans ) D , unmanaged instance group as application does not support horizontal scaling and network load balancer as no mention of http traffic ."
      },
      {
        "date": "2024-03-14T14:18:00.000Z",
        "voteCount": 1,
        "content": "In Unmanaged Instance Group instances cannot be in different zones. I think that correct is D but maybe a mistake in the question."
      },
      {
        "date": "2024-01-22T02:55:00.000Z",
        "voteCount": 1,
        "content": "Why not a B? \nBecause the application doesn't support for horizonal scale.\nI chose D."
      },
      {
        "date": "2023-09-09T06:45:00.000Z",
        "voteCount": 2,
        "content": "D is corrrect TCP load balancer plus UNMANAGED"
      },
      {
        "date": "2023-05-15T10:47:00.000Z",
        "voteCount": 1,
        "content": "must be unmanaged"
      },
      {
        "date": "2023-03-19T18:57:00.000Z",
        "voteCount": 15,
        "content": "Since the application does not support horizontal scaling, a managed instance group is not required. Instead, an unmanaged instance group can be used to ensure that the application runs on multiple instances in different zones for high availability.\nThe network load balancer is designed to handle TCP and UDP traffic\nThe HTTP(S) load balancer is designed specifically for HTTP and HTTPS traffic."
      },
      {
        "date": "2023-05-08T23:00:00.000Z",
        "voteCount": 2,
        "content": "Thanks for the apt explanation"
      },
      {
        "date": "2022-12-22T04:04:00.000Z",
        "voteCount": 20,
        "content": "An unmanaged instance group allows you to create and manage a group of Compute Engine instances manually, rather than using an autoscaling solution like a managed instance group. This is appropriate for an application that does not support horizontal scaling, as you can manually create and manage the number of instances needed to meet the traffic demands.\n\nTo ensure high availability and minimize downtime, you should deploy the instances in different zones and use a regional persistent disk to store the application's data. This will ensure that the application is still available even if one of the instances or a zone experiences an outage.\n\nA network load balancer should be used in front of the instances to distribute traffic to the instances. A network load balancer is a highly available and scalable load balancing solution that operates at the network layer and can handle high volumes of traffic. It can also balance traffic across multiple zones to ensure that the application is always available to users."
      },
      {
        "date": "2022-12-22T04:04:00.000Z",
        "voteCount": 10,
        "content": "Therefore, the correct answer is option D: Use an unmanaged instance group with an active and standby instance in different zones, use a regional persistent disk, and use a network load balancer in front of the instances."
      },
      {
        "date": "2022-12-21T02:48:00.000Z",
        "voteCount": 1,
        "content": "Regional Persistent Disk, as App requires full control of filesystem data without concurrent access (block storage vs. file storage (NAS).\nhttps://cloud.google.com/compute/docs/instance-groups\nhttps://cloud.google.com/load-balancing/docs/choosing-load-balancer"
      },
      {
        "date": "2022-12-20T06:14:00.000Z",
        "voteCount": 5,
        "content": "Option A &amp; B eliminated because we cannot use managed instance group since the app does not support Horizontal scaling \nOption C &gt; HTTP load balancer is Layer 7 &amp; application is receiving traffic via TCP\nOption D &gt; is best answer because we are using Network load balancer Layer 4 which meets the condition \"application receives traffic via TCP\""
      },
      {
        "date": "2022-12-12T19:22:00.000Z",
        "voteCount": 2,
        "content": "Checking the comparative analysis of storage options, we can see that Filestore is not suitable for the workload, hence A and B are out. C is out because it restricts to HTTP traffic. Answer is D"
      },
      {
        "date": "2022-12-12T19:22:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/architecture/storage-advisor#comparative_analysis"
      },
      {
        "date": "2022-11-12T03:17:00.000Z",
        "voteCount": 1,
        "content": "D is ok"
      },
      {
        "date": "2022-11-01T05:45:00.000Z",
        "voteCount": 3,
        "content": "D is not possibile you cannot create a regional unmanaged instance group. https://cloud.google.com/compute/docs/instance-groups/creating-groups-of-unmanaged-instances"
      },
      {
        "date": "2022-12-11T10:29:00.000Z",
        "voteCount": 3,
        "content": "Downtime is acceptable, Disk is regional, in case of issue the unmanaged instance group can be moved to other zone, disk has data. A&amp;B are not at all an option"
      },
      {
        "date": "2022-10-17T02:42:00.000Z",
        "voteCount": 1,
        "content": "D. Use an unmanaged instance group with an active and standby instance in different zones, use a regional persistent disk, and use a network load balancer in front of the instances.\nD is the only option as the application does not support horizontal scaling (no MIG), it needs full control (no filestore) and has TCP traffic (no HTTP LB)."
      },
      {
        "date": "2022-10-10T23:55:00.000Z",
        "voteCount": 1,
        "content": "D is more appropriate solution ---In the ques. it is mentioned done not support horizontal scaling --&gt; hence Unmanaged Instance , and traffic is TCP ---  hence N/W \nLoad balancer"
      },
      {
        "date": "2022-10-07T23:31:00.000Z",
        "voteCount": 1,
        "content": "does not support horizontal scaling so MIG not needed. TCP traffic so NW LB is ok"
      },
      {
        "date": "2022-09-16T22:52:00.000Z",
        "voteCount": 1,
        "content": "concurrent access causes corruption - it can not  be a managed group"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 105,
    "url": "https://www.examtopics.com/discussions/google/view/56368-exam-professional-cloud-architect-topic-1-question-105/",
    "body": "Your company has an application running on multiple Compute Engine instances. You need to ensure that the application can communicate with an on-premises service that requires high throughput via internal IPs, while minimizing latency. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse OpenVPN to configure a VPN tunnel between the on-premises environment and Google Cloud.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a direct peering connection between the on-premises environment and Google Cloud.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud VPN to configure a VPN tunnel between the on-premises environment and Google Cloud.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a Cloud Dedicated Interconnect connection between the on-premises environment and Google Cloud.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 55,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-07-01T14:53:00.000Z",
        "voteCount": 67,
        "content": "Ans ) D , Reason : high throughput via internal IPs"
      },
      {
        "date": "2022-08-15T18:20:00.000Z",
        "voteCount": 2,
        "content": "This is tricky questions , it can be achieved by C and D ... Multiple Computes and Costs .. they are trying to test knowledge on VPN and Tunnels ...."
      },
      {
        "date": "2021-06-30T07:39:00.000Z",
        "voteCount": 23,
        "content": "IMHO the correct answer is D.\nReason:  \"requires high throughput via internal IPs, while minimizing latency\" - both are aspects you cannot guarantee with using VPN traversing the internet."
      },
      {
        "date": "2024-08-24T08:12:00.000Z",
        "voteCount": 1,
        "content": "D seems to be correct"
      },
      {
        "date": "2024-01-15T17:57:00.000Z",
        "voteCount": 1,
        "content": "Should be D as high throughput"
      },
      {
        "date": "2023-12-03T17:54:00.000Z",
        "voteCount": 1,
        "content": "Justo be D"
      },
      {
        "date": "2023-12-03T17:55:00.000Z",
        "voteCount": 1,
        "content": "Should be D"
      },
      {
        "date": "2023-12-01T04:11:00.000Z",
        "voteCount": 1,
        "content": "It should be D, since need to communicate via Internal IP"
      },
      {
        "date": "2023-11-13T08:03:00.000Z",
        "voteCount": 1,
        "content": "Communication through internal IPs - VPN. So, C"
      },
      {
        "date": "2024-06-08T08:34:00.000Z",
        "voteCount": 1,
        "content": "yes, you are very smart\nbut question says high throughput and minimizing latency &gt; Interconnect"
      },
      {
        "date": "2023-12-27T20:48:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/network-connectivity/docs/interconnect/concepts/overview#:~:text=Also%2C%20Cloud%20Interconnect%20connections%20provide,directly%20accessible%20from%20both%20networks."
      },
      {
        "date": "2023-09-09T06:46:00.000Z",
        "voteCount": 2,
        "content": "D cause you need high throughput"
      },
      {
        "date": "2023-08-06T16:44:00.000Z",
        "voteCount": 1,
        "content": "D - high throughput"
      },
      {
        "date": "2023-07-27T03:31:00.000Z",
        "voteCount": 2,
        "content": "How is C marked a correct one?"
      },
      {
        "date": "2023-04-29T08:51:00.000Z",
        "voteCount": 2,
        "content": "Initially thought 'D' but the question says 'high throughput via internal IPs' so go with VPN answer 'C'"
      },
      {
        "date": "2023-04-02T10:08:00.000Z",
        "voteCount": 1,
        "content": "It can\u00b4t be VPN, Only interconnect can minimizing latency. \n\nD is the right answer."
      },
      {
        "date": "2023-02-12T16:05:00.000Z",
        "voteCount": 5,
        "content": "Internal IP +  high throughput"
      },
      {
        "date": "2023-01-11T17:18:00.000Z",
        "voteCount": 3,
        "content": "Ans ) D , Reason : high throughput via internal IPs"
      },
      {
        "date": "2022-12-22T04:08:00.000Z",
        "voteCount": 3,
        "content": "D. Configure a Cloud Dedicated Interconnect connection between the on-premises environment and Google Cloud.\n\nA Cloud Dedicated Interconnect is a high-bandwidth, low-latency network connection that allows you to connect your on-premises environment to Google Cloud Platform (GCP) using a dedicated network connection. It provides a direct physical connection between your on-premises network and GCP, which can help to reduce latency and increase the throughput of your application."
      },
      {
        "date": "2022-12-22T04:08:00.000Z",
        "voteCount": 2,
        "content": "Option A, using OpenVPN to configure a VPN tunnel between the on-premises environment and Google Cloud, is not a recommended approach. OpenVPN is a free and open-source software application that implements virtual private network (VPN) techniques to create secure point-to-point connections. While OpenVPN can be used to establish a VPN tunnel between an on-premises environment and GCP, it may not provide the level of performance and reliability required for high-throughput applications.\n\nOption B, configuring a direct peering connection between the on-premises environment and Google Cloud, is not a recommended approach. Direct Peering is a high-bandwidth, low-latency network connection that allows you to connect your on-premises network directly to Google's network. While Direct Peering can be used to connect your on-premises environment to GCP, it is typically used for high-bandwidth workloads such as video streaming and may not be suitable for applications that require low latency.\n\nOption C, using Cloud VPN to configure a VPN tunnel between the on-premises"
      },
      {
        "date": "2022-12-22T04:09:00.000Z",
        "voteCount": 1,
        "content": "Cloud VPN, would also involve routing traffic over the public internet and would not provide the low latency and high throughput that you need."
      },
      {
        "date": "2023-04-22T01:54:00.000Z",
        "voteCount": 2,
        "content": "Yeah, direct peering is high bandwidth and low latency, but may not be suitable for applications that require low latency.\n\nGood job, ChatGPT..."
      },
      {
        "date": "2022-11-28T17:59:00.000Z",
        "voteCount": 3,
        "content": "Highthoughput connection VPN - No Interconnect - Y \nvia Internal IP addresses. VPN - yes Interconnect - Y \n Interconnect  yes for both so D"
      },
      {
        "date": "2022-11-12T03:21:00.000Z",
        "voteCount": 1,
        "content": "D is ok"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 106,
    "url": "https://www.examtopics.com/discussions/google/view/56373-exam-professional-cloud-architect-topic-1-question-106/",
    "body": "You are managing an application deployed on Cloud Run for Anthos, and you need to define a strategy for deploying new versions of the application. You want to evaluate the new code with a subset of production traffic to decide whether to proceed with the rollout. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy a new revision to Cloud Run with the new version. Configure traffic percentage between revisions.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy a new service to Cloud Run with the new version. Add a Cloud Load Balancing instance in front of both services.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the Google Cloud Console page for Cloud Run, set up continuous deployment using Cloud Build for the development branch. As part of the Cloud Build trigger, configure the substitution variable TRAFFIC_PERCENTAGE with the percentage of traffic you want directed to a new version.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the Google Cloud Console, configure Traffic Director with a new Service that points to the new version of the application on Cloud Run. Configure Traffic Director to send a small percentage of traffic to the new version of the application."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 26,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 9,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-07-07T10:56:00.000Z",
        "voteCount": 54,
        "content": "\uf0fc\tCorrect Answer: A\no\tEach deployment to a service creates a revision. A revision consists of a specific container image, along with environment settings such as environment variables, memory limits, or concurrency value.\no\tOnce the new revision is deployed to a Service you can manage the traffic using MANAGE TRAFFIC option inside the revision tab\n\uf02d\thttps://cloud.google.com/run/docs/resource-model"
      },
      {
        "date": "2022-12-22T04:11:00.000Z",
        "voteCount": 19,
        "content": "The correct answer is A. Deploy a new revision to Cloud Run with the new version. Configure traffic percentage between revisions.\n\nCloud Run for Anthos allows you to deploy new revisions of your application with a specific percentage of traffic, which allows you to perform a gradual rollout of the new version. To do this, you can follow these steps:\n\nDeploy a new revision of your application to Cloud Run with the new version.\n\nIn the Cloud Run for Anthos console, navigate to the service that you want to roll out the new version for.\n\nIn the \"Revisions\" tab, you should see the new revision listed alongside the current revision.\n\nUse the traffic percentage slider to specify the percentage of traffic that you want to send to the new revision. You can set the percentage to a small value initially, such as 5%, and gradually increase it over time as you evaluate the new version.\n\nOnce you have set the traffic percentage, Cloud Run for Anthos will start directing a portion of the traffic to the new revision, allowing you to evaluate the new version with a subset of production traffic."
      },
      {
        "date": "2023-05-08T23:14:00.000Z",
        "voteCount": 1,
        "content": "Extremely convinced by your explanations..Have u given this exam?"
      },
      {
        "date": "2024-06-08T08:29:00.000Z",
        "voteCount": 2,
        "content": "Yes, chatgpt did the exam, LOL"
      },
      {
        "date": "2023-06-16T07:08:00.000Z",
        "voteCount": 1,
        "content": "Those Answers are surely generated by ChatgGPT"
      },
      {
        "date": "2022-12-22T04:12:00.000Z",
        "voteCount": 11,
        "content": "Option B, deploying a new service and adding a Cloud Load Balancer instance in front of both services, is not recommended because it would require you to create and manage a separate service for the new version, which would be more complex and less efficient than deploying a new revision.\n\nOption C, using continuous deployment with Cloud Build and substitution variables, is not relevant to this scenario because it involves deploying new versions automatically based on changes to a development branch, rather than manually deploying new revisions with a specific percentage of traffic. \n\n\nOption D, using Traffic Director, is also not relevant because Traffic Director is used for managing traffic between different services or clusters, rather than between revisions of the same service."
      },
      {
        "date": "2024-02-02T00:50:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/run/docs/rollouts-rollbacks-traffic-migration#gradual"
      },
      {
        "date": "2023-11-27T11:23:00.000Z",
        "voteCount": 1,
        "content": "A:\nhttps://cloud.google.com/run/docs/rollouts-rollbacks-traffic-migration#gradual"
      },
      {
        "date": "2023-11-18T05:12:00.000Z",
        "voteCount": 1,
        "content": "La Opci\u00f3n A - El uso de dashboards predefinidos proporciona una visi\u00f3n inmediata y eficiente del estado del sistema, y la capacidad de agregar m\u00e9tricas personalizadas y crear pol\u00edticas de alertas permite una respuesta r\u00e1pida y efectiva a los incidentes. Generar un Dashboard por incidente introduce complejidad innecesaria."
      },
      {
        "date": "2023-11-18T05:05:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/anthos/run/docs/rollouts-rollbacks-traffic-migration"
      },
      {
        "date": "2023-10-11T06:12:00.000Z",
        "voteCount": 1,
        "content": "You all forget that in the exam they expect you to choose the most complete answer, and C is the most efficient and complete one, where you show knowledge of Cloud Build also, and in the real life that is how you should implement this solution."
      },
      {
        "date": "2024-01-17T01:24:00.000Z",
        "voteCount": 2,
        "content": "C is saying \"from development branch\"!"
      },
      {
        "date": "2023-09-05T16:30:00.000Z",
        "voteCount": 2,
        "content": "A is correct - https://cloud.google.com/anthos/run/docs/rollouts-rollbacks-traffic-migration#gradual"
      },
      {
        "date": "2023-07-07T01:43:00.000Z",
        "voteCount": 3,
        "content": "C is ok as we want to DEFINE A STRATEGY"
      },
      {
        "date": "2023-05-15T11:46:00.000Z",
        "voteCount": 1,
        "content": "Answer is A"
      },
      {
        "date": "2023-02-12T16:09:00.000Z",
        "voteCount": 4,
        "content": "https://cloud.google.com/anthos/run/docs/rollouts-rollbacks-traffic-migration"
      },
      {
        "date": "2022-12-17T23:30:00.000Z",
        "voteCount": 1,
        "content": "Selected A"
      },
      {
        "date": "2022-11-12T06:40:00.000Z",
        "voteCount": 1,
        "content": "A is ok"
      },
      {
        "date": "2022-10-22T11:36:00.000Z",
        "voteCount": 2,
        "content": "A is correct\ncurrently there is possibility to use tags to test in production without receiving real traffic: https://cloud.google.com/anthos/run/docs/rollouts-rollbacks-traffic-migration#tags"
      },
      {
        "date": "2022-10-21T08:56:00.000Z",
        "voteCount": 2,
        "content": "whats wrong with C is configuring deployment from 'development branch'\nthis is supper ugly"
      },
      {
        "date": "2022-10-17T20:24:00.000Z",
        "voteCount": 1,
        "content": "you can do a lab on this &gt;&gt;&gt;Deploy Your Website on Cloud Run&lt;&lt;&lt; Manage traffic is there"
      },
      {
        "date": "2022-10-17T03:18:00.000Z",
        "voteCount": 1,
        "content": "A. Deploy a new revision to Cloud Run with the new version. Configure traffic percentage between revisions."
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 107,
    "url": "https://www.examtopics.com/discussions/google/view/56365-exam-professional-cloud-architect-topic-1-question-107/",
    "body": "You are monitoring Google Kubernetes Engine (GKE) clusters in a Cloud Monitoring workspace. As a Site Reliability Engineer (SRE), you need to triage incidents quickly. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNavigate the predefined dashboards in the Cloud Monitoring workspace, and then add metrics and create alert policies.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNavigate the predefined dashboards in the Cloud Monitoring workspace, create custom metrics, and install alerting software on a Compute Engine instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite a shell script that gathers metrics from GKE nodes, publish these metrics to a Pub/Sub topic, export the data to BigQuery, and make a Data Studio dashboard.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a custom dashboard in the Cloud Monitoring workspace for each incident, and then add metrics and create alert policies."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 57,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 43,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-07-01T14:52:00.000Z",
        "voteCount": 57,
        "content": "Ans ) A ."
      },
      {
        "date": "2021-07-07T07:28:00.000Z",
        "voteCount": 31,
        "content": "It's A for me... Create a dashboard for each incident?? I think D isn't a good choice..."
      },
      {
        "date": "2024-01-05T13:51:00.000Z",
        "voteCount": 3,
        "content": "Yeah, creating a new dashboard for each incident doesn't seem like the quickest option."
      },
      {
        "date": "2024-05-29T11:14:00.000Z",
        "voteCount": 1,
        "content": "quickly would mean custom, and you can call it what you want.  D"
      },
      {
        "date": "2024-05-16T18:20:00.000Z",
        "voteCount": 1,
        "content": "If you need extended functionality you should create your dashboards, metrics and alerts. Dont mess existing ones."
      },
      {
        "date": "2024-04-28T05:22:00.000Z",
        "voteCount": 1,
        "content": "Key is custom dashboard"
      },
      {
        "date": "2024-04-18T07:47:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2024-03-04T03:40:00.000Z",
        "voteCount": 1,
        "content": "A is correct. Option D is highly inefficient and time-consuming. Creating individual dashboards for every incident is impractical and slows down the triage process."
      },
      {
        "date": "2024-01-22T03:01:00.000Z",
        "voteCount": 1,
        "content": "I will go with A.\nuhhm, opinions are divided. In such cases, Q is often not good."
      },
      {
        "date": "2024-01-22T02:32:00.000Z",
        "voteCount": 3,
        "content": "Explanation: Cloud Monitoring provides predefined dashboards for monitoring GKE clusters, which facilitate an immediate and comprehensive view of cluster performance and health. As an SRE, utilizing these dashboards helps triage incidents quickly. You can also add additional metrics that are pertinent to the incident and create alert policies that will notify you when specific conditions indicative of an incident are met. This strategy allows for the proactive monitoring of incidents and rapid response when necessary."
      },
      {
        "date": "2024-01-17T01:45:00.000Z",
        "voteCount": 3,
        "content": "Ans: D. Although creating dashboard per incident sounds confusing and inefficient, it is still better than the impossible option A as we can't edit or add metrics to a predefined dashboard. Inefficient option vs Impossible Option - Inefficient one is ok!"
      },
      {
        "date": "2024-04-18T07:01:00.000Z",
        "voteCount": 2,
        "content": "Optipn A is possible. You can't add widgets to predefined dashboard but you can create alert policies based on metrics. Although the structure of the question is actually misleading"
      },
      {
        "date": "2023-12-13T10:16:00.000Z",
        "voteCount": 1,
        "content": "A is correct. For D, creating dashboards for each incident isn't practical"
      },
      {
        "date": "2023-12-10T11:35:00.000Z",
        "voteCount": 1,
        "content": "Currently you can not modify pre-defined dashboards.\nHas no sense to create a dashboard for each incident.\nConclusion: no answer is good enough"
      },
      {
        "date": "2023-11-13T15:32:00.000Z",
        "voteCount": 2,
        "content": "We do have dashboards for each incident"
      },
      {
        "date": "2023-10-01T12:35:00.000Z",
        "voteCount": 3,
        "content": "\"You can't delete or modify the automatically-created dashboards; however, when support for copying the dashboard exists, you can modify the copy. In general, you can also copy charts on a predefined dashboard to a dashboard that you create. Dashboards that you create are custom dashboards. Custom dashboards let you display information that is of interest to you, organized in a way that's useful to you. \"\nhttps://cloud.google.com/monitoring/charts/predefined-dashboards"
      },
      {
        "date": "2023-11-16T05:59:00.000Z",
        "voteCount": 1,
        "content": "Create a dashboard for each inc ????? serious"
      },
      {
        "date": "2023-09-16T12:39:00.000Z",
        "voteCount": 3,
        "content": "\"To view the chart associated with an alerting policy and information about incidents in the same context as your metric data, add alert charts and incident widgets to your CUSTOM dashboard.\" https://cloud.google.com/monitoring/dashboards/alerts-and-incidents"
      },
      {
        "date": "2023-09-09T11:41:00.000Z",
        "voteCount": 1,
        "content": "Ans A any sense a dashboar per incident"
      },
      {
        "date": "2023-08-25T06:42:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 108,
    "url": "https://www.examtopics.com/discussions/google/view/56369-exam-professional-cloud-architect-topic-1-question-108/",
    "body": "You are implementing a single Cloud SQL MySQL second-generation database that contains business-critical transaction data. You want to ensure that the minimum amount of data is lost in case of catastrophic failure. Which two features should you implement? (Choose two.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSharding",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRead replicas",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBinary logging\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAutomated backups\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSemisynchronous replication"
    ],
    "answer": "CD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "CD",
        "count": 21,
        "isMostVoted": true
      },
      {
        "answer": "BD",
        "count": 7,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-07-01T14:52:00.000Z",
        "voteCount": 37,
        "content": "Ans) C and D\nCloud SQL. If you use Cloud SQL, the fully managed Google Cloud MySQL database, you should enable automated backups and binary logging for your Cloud SQL instances. This allows you to perform a point-in-time recovery, which restores your database from a backup and recovers it to a fresh Cloud SQL instance"
      },
      {
        "date": "2022-10-31T04:28:00.000Z",
        "voteCount": 5,
        "content": "And: a read-replica won't help against \"catastrophic failures\" like accidental deletions"
      },
      {
        "date": "2023-02-07T16:53:00.000Z",
        "voteCount": 4,
        "content": "catastrophic  failure means disaster like a zonal datacenter level failure or regional failure"
      },
      {
        "date": "2021-07-10T23:55:00.000Z",
        "voteCount": 11,
        "content": "C. Binary logging\nD. Automated backups"
      },
      {
        "date": "2023-12-27T21:16:00.000Z",
        "voteCount": 3,
        "content": "CD\n\nBinary Logging: Binary logging in MySQL records changes to the database. It can be used for backup and replication, and it's essential for point-in-time recovery. With binary logging, you can roll your database forward to any point in time, minimizing data loss.\n\nAutomated Backups: Automated backups periodically take a snapshot of your database. In the event of a catastrophic failure, you can restore your database to the state it was in at the time of the last backup. This can also help minimize data loss.\n\nWhile read replicas and semisynchronous replication can enhance availability and performance, they do not directly minimize data loss.\nAlso, you cannot create a read replica without enabling Automated backups and Enable binary logging\n\nSharding can improve performance but it's not directly aimed at data loss prevention.\n\nhttps://cloud.google.com/sql/docs/mysql/backup-recovery/backups"
      },
      {
        "date": "2023-11-18T05:19:00.000Z",
        "voteCount": 5,
        "content": "Prerequisites for creating a read replica\n\nBefore you can create a read replica of a primary Cloud SQL instance, the instance must meet the following requirements:\n\n    Automated backups must be enabled.\n    Binary logging must be enabled which requires point-in-time recovery to be enabled. Learn more about the impact of these logs.\n    At least one backup must have been created after binary logging was enabled.\nhttps://cloud.google.com/sql/docs/mysql/replication#requirements"
      },
      {
        "date": "2023-09-13T04:29:00.000Z",
        "voteCount": 1,
        "content": "CD\nBefore being able to create a read replica, you have to make sure \"binary logging and automated backup\" are enabled. So picking only D or C without the other one makes no sense.\nhttps://cloud.google.com/sql/docs/mysql/replication/create-replica"
      },
      {
        "date": "2023-09-05T16:44:00.000Z",
        "voteCount": 4,
        "content": "The correct answers are C. &amp; D.\n\n\nBinary logging is a feature of MySQL that records all changes made to the database. This log can be used to restore the database to a previous state in case of a failure.\n\nAutomated backups are regularly scheduled backups of the database. They are the most reliable way to ensure that data is not lost in case of a catastrophic failure."
      },
      {
        "date": "2023-08-25T06:44:00.000Z",
        "voteCount": 1,
        "content": "C and D. Read replicas wont work in this case."
      },
      {
        "date": "2023-06-27T10:47:00.000Z",
        "voteCount": 1,
        "content": "C. Binary logging\nD. Automated backups"
      },
      {
        "date": "2023-03-31T02:21:00.000Z",
        "voteCount": 3,
        "content": "Binary logging records changes to the data, which can help you recover data and minimize data loss during an unexpected failure. Automated backups create regular backups of your database, allowing you to restore the database to a specific point in time in case of a catastrophic failure."
      },
      {
        "date": "2023-03-02T05:44:00.000Z",
        "voteCount": 1,
        "content": "backup data automatically"
      },
      {
        "date": "2023-02-27T08:34:00.000Z",
        "voteCount": 1,
        "content": "CD =&gt; the answer B is for performance issue. The question focus on data loss prevention."
      },
      {
        "date": "2023-04-22T02:22:00.000Z",
        "voteCount": 1,
        "content": "So you are going with a SINGLE instance of MySQL for a critical business application."
      },
      {
        "date": "2023-01-29T07:35:00.000Z",
        "voteCount": 3,
        "content": "B and D:\nNo need to explain D, but B... here is why\nWhen you set up a read replica, automaticaly binary logging is activated. Then, in case of desaster, you can promote manually a read replica and it will have all data before the desaster occurs."
      },
      {
        "date": "2023-02-18T14:01:00.000Z",
        "voteCount": 2,
        "content": "sure, binary logging starts Automatically upon configuring read-replica?? \n- Don't think so,\nhttps://cloud.google.com/sql/docs/mysql/replication/create-replica"
      },
      {
        "date": "2023-01-28T20:53:00.000Z",
        "voteCount": 1,
        "content": "B and D are correct answers as per below reference,\n1. Before you can create a read replica of a primary Cloud SQL instance, the instance must meet the following requirements:\n\nAutomated backups must be enabled.\n\n2. Binary logging must be enabled which requires point-in-time recovery to be enabled. Learn more about the impact of these logs.\n\n3. At least one backup must have been created after binary logging was enabled.\n\n\n\nIt means creating read replica already covers binary logging.\n\n\n\nPlease read the following references for more information\n\nhttps://cloud.google.com/solutions/cloud-sql-mysql-disaster-recovery-complete-failover-fallback\n\nhttps://medium.com/google-cloud/cloud-sql-recovering-from-regional-failure-in-10-minutes-or-less-mysql-fc055540a8f0\n\nReplication in Cloud SQL | Cloud SQL for MySQL | Google Cloud"
      },
      {
        "date": "2023-05-08T05:11:00.000Z",
        "voteCount": 3,
        "content": "Yes, you are correct that creating a read replica requires binary logging to be enabled on the primary instance. However, the purpose of a read replica is to scale read traffic and offload it from the primary instance, not to prevent data loss in case of catastrophic failure. While enabling binary logging is a requirement for creating a read replica, it is not the primary purpose of a read replica. IMO the two features that should be implemented to ensure minimum data loss in case of catastrophic failure are Binary logging and Automated backups."
      },
      {
        "date": "2023-01-01T21:23:00.000Z",
        "voteCount": 2,
        "content": "C and D are correct answers,\n\nBackups help you restore lost data to your Cloud SQL instance. Additionally, if an instance is having a problem, you can restore it to a previous state by using the backup to overwrite it. Enable automated backups for any instance that contains necessary data. Backups protect your data from loss or damage.\n\nEnabling automated backups, along with binary logging, is also required for some operations, such as clone and replica creation.\n\nhttps://cloud.google.com/sql/docs/mysql/backup-recovery/backups#what_backups_provide"
      },
      {
        "date": "2022-12-22T04:18:00.000Z",
        "voteCount": 4,
        "content": "C. Binary logging\nD. Automated backups\n\nBinary logging is a feature of MySQL that records all changes made to the database in a binary log file. By enabling binary logging on your Cloud SQL instance, you can use the log file to recover your database in case of catastrophic failure.\n\nAutomated backups are a feature of Cloud SQL that allows you to automatically create and retain backups of your database. By enabling automated backups, you can restore your database in case of catastrophic failure or other data loss events.\n\nOption A, sharding, is not a recommended approach. Sharding is a technique for distributing data across multiple servers to improve performance and scalability. While sharding can help to improve the performance of a database, it is not specifically designed to protect against data loss in case of catastrophic failure."
      },
      {
        "date": "2022-12-22T04:18:00.000Z",
        "voteCount": 2,
        "content": "Option B, read replicas, is not a recommended approach. Read replicas are copies of a database that can be used to offload read traffic from the primary database. While read replicas can improve the performance of a database, they are not specifically designed to protect against data loss in case of catastrophic failure.\n\nOption E, semisynchronous replication, is not a recommended approach. Semisynchronous replication is a method of replicating data between a primary database and one or more secondary databases. While semisynchronous replication can help to ensure that data is replicated quickly and accurately, it is not specifically designed to protect against data loss in case of catastrophic failure."
      },
      {
        "date": "2022-11-12T06:48:00.000Z",
        "voteCount": 1,
        "content": "cd is ok"
      },
      {
        "date": "2022-11-01T18:50:00.000Z",
        "voteCount": 1,
        "content": "I see Read Replicas as more of a performance thing, than DR thing"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 109,
    "url": "https://www.examtopics.com/discussions/google/view/56381-exam-professional-cloud-architect-topic-1-question-109/",
    "body": "You are working at a sports association whose members range in age from 8 to 30. The association collects a large amount of health data, such as sustained injuries. You are storing this data in BigQuery. Current legislation requires you to delete such information upon request of the subject. You want to design a solution that can accommodate such a request. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a unique identifier for each individual. Upon a deletion request, delete all rows from BigQuery with this identifier.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWhen ingesting new data in BigQuery, run the data through the Data Loss Prevention (DLP) API to identify any personal information. As part of the DLP scan, save the result to Data Catalog. Upon a deletion request, query Data Catalog to find the column with personal information.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a BigQuery view over the table that contains all data. Upon a deletion request, exclude the rows that affect the subject's data from this view. Use this view instead of the source table for all analysis tasks.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a unique identifier for each individual. Upon a deletion request, overwrite the column with the unique identifier with a salted SHA256 of its value."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 56,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 33,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-07-09T03:45:00.000Z",
        "voteCount": 83,
        "content": "According to me, the question states \"The association collects a large amount of health data, such as sustained injuries.\" and the nuance on the word such =&gt; \" Current legislation requires you to delete \"SUCH\" information upon request of the subject. \" So from that point of view the question is not to delete the entire user records but specific data related to personal health data. With DLP you can use InfoTypes and InfoType detectors to specifically scan for those entries and how to act upon them (link https://cloud.google.com/dlp/docs/concepts-infotypes)\nI would say B."
      },
      {
        "date": "2022-05-17T01:25:00.000Z",
        "voteCount": 11,
        "content": "(A) - Primary task is \"legislation requires you to delete\" .. and B is not deleting.\nonly A is deleting"
      },
      {
        "date": "2023-03-09T17:01:00.000Z",
        "voteCount": 1,
        "content": "Deletion is implied in \"Upon a deletion request, query Data Catalog to find the column with personal information.\""
      },
      {
        "date": "2022-03-13T09:07:00.000Z",
        "voteCount": 7,
        "content": "I want to delete all the informations about the user, not only those individuate by DLP. ALL THE INFORMATIONS of the users...B is not correct! the correct is A"
      },
      {
        "date": "2022-08-17T20:51:00.000Z",
        "voteCount": 12,
        "content": "There is no need of DLP.\nAll the data is sensitive but only upon user request it needs deletion.\nSo A should be the correct answer."
      },
      {
        "date": "2021-11-26T07:54:00.000Z",
        "voteCount": 12,
        "content": "as PhilipKoku mentioned below:\nA) is the correct answer. B) is only masking the data and then when a request is received, it identified the record but it doesn\u2019t delete it. D) Is masking the ID."
      },
      {
        "date": "2021-12-12T00:51:00.000Z",
        "voteCount": 6,
        "content": "B is not masking the data but identifying where it is to take action on at later date if required"
      },
      {
        "date": "2021-06-30T08:12:00.000Z",
        "voteCount": 35,
        "content": "IMHO a) is the correct answer because it is easier to operate. The question is not how to mask data and so on but just to delete data on request, so I don't think that we have to use for just the deletion of specific data DLP."
      },
      {
        "date": "2024-06-18T08:34:00.000Z",
        "voteCount": 1,
        "content": "It had better be A, if not then you're not a good organization"
      },
      {
        "date": "2024-05-16T19:05:00.000Z",
        "voteCount": 1,
        "content": "Data Loss Prevention must have!"
      },
      {
        "date": "2024-05-06T04:56:00.000Z",
        "voteCount": 2,
        "content": "I vote for B. \nI had some doubts whether A was correct, but:\n  - I'm not convinced by the argument \"only A talks about deleting\" (it would be too easy if it was about choosing an answer containing the word \"delete\" ;)\n  - the question says \"design a solution that can accommodate such a request\" - I'm not very fluent in english, but \"accommodate\" imho means more \"facilitate\" than \"accomplish\" here\n  - I think that the task is about deleting health data not everything related with unique identifier\n- Data Catalog allows you to manage data, knowing in which datasets and in which tables what data is stored. Answer \"A\" somehow imposes the data model - each table with data related to a given individual must contain the ID of this individual (in a real data model this does not have to be the case)."
      },
      {
        "date": "2024-03-19T07:41:00.000Z",
        "voteCount": 2,
        "content": "Should be A)"
      },
      {
        "date": "2024-03-04T03:42:00.000Z",
        "voteCount": 3,
        "content": "A is correct. As for option B: While DLP is valuable for identifying sensitive data, it might not be sufficient for this specific case. DLP cannot necessarily determine an individual's right to deletion based solely on data classification. Additionally, relying on Data Catalog to store the results adds unnecessary complexity and potential inconsistencies."
      },
      {
        "date": "2024-02-06T12:01:00.000Z",
        "voteCount": 2,
        "content": "B. The A removes all data, not SUCH only."
      },
      {
        "date": "2024-01-30T06:06:00.000Z",
        "voteCount": 2,
        "content": "Ans. B assumes you will delete the Personal Information found in the Catalog...  Some people are reading GDPR into this question (we are not told what country and what legislation). The question states you must delete all information (not just personal informarion) on request.  Ans B is a red herring !\nAnswer must =  A"
      },
      {
        "date": "2023-12-27T02:55:00.000Z",
        "voteCount": 5,
        "content": "(A) - Primary task is \"legislation requires you to delete\" .. and B is not deleting.\nonly A is deleting"
      },
      {
        "date": "2023-12-04T10:09:00.000Z",
        "voteCount": 1,
        "content": "Well, A would delete all rows with the identifier, I guess including the ones that are not confidential, also what does it mean unique identifier? each user is unique already. Ridiculous.  B would identify the columns that contain personal data, but B is prone to errors as changes in legislation of what is consider injury would be excluded and all data would need to be re- ingested.  Unfortunately B is closer and less damaging than A."
      },
      {
        "date": "2023-11-12T21:48:00.000Z",
        "voteCount": 2,
        "content": "Either A or B is the answer. \nA - will delete all the info about the subject, which is not the intension. Only the sensitive data to be deleted. Hence, B."
      },
      {
        "date": "2023-10-27T03:36:00.000Z",
        "voteCount": 1,
        "content": "B is correct.. Check in chatGPT also."
      },
      {
        "date": "2023-12-10T03:58:00.000Z",
        "voteCount": 1,
        "content": "chatgpt select A\nThe most appropriate solution for accommodating the deletion request of personal health data stored in BigQuery, as per current legislation, would be:\n\nA. Use a unique identifier for each individual. Upon a deletion request, delete all rows from BigQuery with this identifier.\n\nHere's why this approach is suitable:\n\nUnique Identifier: Assigning a unique identifier to each individual is a standard practice in managing and querying datasets. It helps in precisely identifying and isolating records associated with a specific individual.\n\nDirect Deletion of Rows: Upon receiving a deletion request, you can directly delete all rows associated with the individual's unique identifier. This approach ensures that the data is completely removed from your dataset, complying with the legislative requirement to delete personal information upon request."
      },
      {
        "date": "2023-10-11T06:46:00.000Z",
        "voteCount": 1,
        "content": "From one side A is an easy way, low effort, to implement this solution, but if we think like an architect and like an exam question, B is more complete and a better solution, since it can mask all the sensitive information, not only for the users that request it, but for all, which is a best practice."
      },
      {
        "date": "2023-10-06T03:31:00.000Z",
        "voteCount": 2,
        "content": "We do not need to delete entire recrod of sports person but some health information collected by association. B would be correct answer."
      },
      {
        "date": "2023-09-16T13:28:00.000Z",
        "voteCount": 1,
        "content": "The problem I see with A is that it doesn't offer you a way to find the original subject's information once they request for their information to be deleted (no mapping from the unique identifier back to their person).  Only B offers the solution design for this ability.  The deletion step may not be included in B, but the ability to delete is always present.  You're designing the ability to accommodate the request, which is to look up the individual who is asking for their information to be deleted."
      },
      {
        "date": "2023-09-09T11:45:00.000Z",
        "voteCount": 1,
        "content": "DLP its correct product to deal with personal data"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 110,
    "url": "https://www.examtopics.com/discussions/google/view/56840-exam-professional-cloud-architect-topic-1-question-110/",
    "body": "Your company has announced that they will be outsourcing operations functions. You want to allow developers to easily stage new versions of a cloud-based application in the production environment and allow the outsourced operations team to autonomously promote staged versions to production. You want to minimize the operational overhead of the solution. Which Google Cloud product should you migrate to?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tApp Engine\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGKE On-Prem",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCompute Engine",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGoogle Kubernetes Engine"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 46,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 6,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-07-01T14:50:00.000Z",
        "voteCount": 36,
        "content": "A. App Engine"
      },
      {
        "date": "2021-07-24T04:30:00.000Z",
        "voteCount": 24,
        "content": "Answer should be A as only with App Engine we have a default service account which allows the user to deploy the changes per project. for GKE we may have to configure additional permission for both DEV and Operations team to deploy the changes.\n\nhttps://cloud.google.com/appengine/docs/standard/php/service-account"
      },
      {
        "date": "2024-06-18T09:05:00.000Z",
        "voteCount": 1,
        "content": "A. App Engine.\n\nExplanation:\nWhy A is correct:\n\nApp Engine: Google App Engine is a fully managed platform-as-a-service (PaaS) that allows developers to build and deploy applications quickly and easily without worrying about managing the underlying infrastructure. It supports continuous integration and continuous deployment (CI/CD) processes, enabling developers to stage new versions of applications easily.\n\nStaging and Promotion: App Engine has built-in support for traffic splitting and versioning, which allows you to stage new versions of your application and gradually promote them to production. This can be done with minimal operational overhead, making it ideal for scenarios where operational functions are outsourced.\n\nMinimal Operational Overhead: Since App Engine is fully managed, it reduces the operational burden significantly, making it easier for the outsourced operations team to handle promotions and manage the application."
      },
      {
        "date": "2024-06-03T11:24:00.000Z",
        "voteCount": 1,
        "content": "A. App Engine"
      },
      {
        "date": "2024-04-18T23:05:00.000Z",
        "voteCount": 3,
        "content": "Is this answers are really correct or misleading to us..?"
      },
      {
        "date": "2024-03-04T03:48:00.000Z",
        "voteCount": 2,
        "content": "While both GKE and App Engine offer functionalities for deploying cloud-based applications, App Engine is more managed service compared to GKE, resulting in lower operational overhead."
      },
      {
        "date": "2024-02-21T08:37:00.000Z",
        "voteCount": 3,
        "content": "I did my exam today and saw this question. But I am sure A was the answer due to the operational overhead phrase"
      },
      {
        "date": "2023-12-27T21:28:00.000Z",
        "voteCount": 2,
        "content": "A\n\n\nwhy not D?\nIt requires more operational overhead compared to App Engine, as it involves managing the Kubernetes infrastructure."
      },
      {
        "date": "2023-11-12T21:35:00.000Z",
        "voteCount": 2,
        "content": "In the question, we see \"cloud-based application\" - I assume, it means cloud native -&gt; dockers/containers -&gt; K8s -&gt; GKE.\nHence, D is my option."
      },
      {
        "date": "2023-10-06T03:35:00.000Z",
        "voteCount": 1,
        "content": "I agreed with \"omermahgoub\" the answer should be D.\nAs you will bundle the application and its dependencies into container image and deploy. All environments will have same image deployed from Dev, TEST, Staging to PROD. There will be less operational overheard for operations team."
      },
      {
        "date": "2023-09-14T11:09:00.000Z",
        "voteCount": 2,
        "content": "A\n\nApp Engine reduces ops overhead"
      },
      {
        "date": "2023-09-10T23:50:00.000Z",
        "voteCount": 2,
        "content": "A. You deploy your new version to App Engine without setting it as the default version. The ops team then just has to make it the default version when they want to promote it. Simplest answer."
      },
      {
        "date": "2023-08-25T06:55:00.000Z",
        "voteCount": 1,
        "content": "App Engine because of less ovehead."
      },
      {
        "date": "2023-06-03T05:25:00.000Z",
        "voteCount": 3,
        "content": "Answer is A.\nBy process of elimination you arrive at App Engine or GKE. Now the requirement is to \"to minimize the operational overhead of the solution\". \nOn the IaaS to PaaS spectrum, this can only be App Engine!\n\nIaaS = Compute Engine.\nHybrid = GKE (engineering heavy).\nPaaS = App Engine."
      },
      {
        "date": "2023-05-30T02:36:00.000Z",
        "voteCount": 3,
        "content": "\"You want to minimize the operational overhead of the solution\" ..This sentence is they key to go with Option A. GKE carries overhead as it's not purely PaaS."
      },
      {
        "date": "2023-05-08T23:37:00.000Z",
        "voteCount": 1,
        "content": "It should be D. As GKE is considered to be the master product/service for creating a deployment and managing and keeping all the environments in SYNC"
      },
      {
        "date": "2023-03-15T17:33:00.000Z",
        "voteCount": 1,
        "content": "A is right as the requirement is to deploy new changes and manage the application with no operational overhead."
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 111,
    "url": "https://www.examtopics.com/discussions/google/view/56686-exam-professional-cloud-architect-topic-1-question-111/",
    "body": "Your company is running its application workloads on Compute Engine. The applications have been deployed in production, acceptance, and development environments. The production environment is business-critical and is used 24/7, while the acceptance and development environments are only critical during office hours. Your CFO has asked you to optimize these environments to achieve cost savings during idle times. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a shell script that uses the gcloud command to change the machine type of the development and acceptance instances to a smaller machine type outside of office hours. Schedule the shell script on one of the production instances to automate the task.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Scheduler to trigger a Cloud Function that will stop the development and acceptance environments after office hours and start them just before office hours.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the development and acceptance applications on a managed instance group and enable autoscaling.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse regular Compute Engine instances for the production environment, and use preemptible VMs for the acceptance and development environments."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 27,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 10,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-07-07T06:17:00.000Z",
        "voteCount": 36,
        "content": "B is the answer.\n\nhttps://cloud.google.com/blog/products/it-ops/best-practices-for-optimizing-your-cloud-costs \nSchedule VMs to auto start and stop: The benefit of a platform like Compute Engine is that you only pay for the compute resources that you use. Production systems tend to run 24/7; however, VMs in development, test or personal environments tend to only be used during business hours, and turning them off can save you a lot of money! \n\nhttps://cloud.google.com/blog/products/storage-data-transfer/save-money-by-stopping-and-starting-compute-engine-instances-on-schedule\n\nCloud Scheduler, GCP\u2019s fully managed cron job scheduler, provides a straightforward solution for automatically stopping and starting VMs. By employing Cloud Scheduler with Cloud Pub/Sub to trigger Cloud Functions on schedule, you can stop and start groups of VMs identified with labels of your choice (created in Compute Engine). Here you can see an example schedule that stops all VMs labeled \"dev\" at 5pm and restarts them at 9am, while leaving VMs labeled \"prod\" untouched"
      },
      {
        "date": "2022-08-01T12:08:00.000Z",
        "voteCount": 16,
        "content": "Excellent ......even the good CFO is telling leave the office after 5.oo and come next day to work :)"
      },
      {
        "date": "2022-07-15T14:26:00.000Z",
        "voteCount": 2,
        "content": "Great answer and documentation. Def B"
      },
      {
        "date": "2022-07-30T13:04:00.000Z",
        "voteCount": 17,
        "content": "Question says that dev/test are \"not critical\", it doesn't mean that they are not needed at all ..."
      },
      {
        "date": "2021-07-01T14:49:00.000Z",
        "voteCount": 25,
        "content": "Ans ) B , assuming VM doesn't need to be up after office hours ."
      },
      {
        "date": "2024-09-25T11:18:00.000Z",
        "voteCount": 1,
        "content": "Stop the dev and acceptance envs is super weird. Any critical problems or overtimes will be an issue with this approach. Simple auto scaling environment is a good solution IMHO"
      },
      {
        "date": "2024-04-19T07:55:00.000Z",
        "voteCount": 1,
        "content": "B is right answer"
      },
      {
        "date": "2024-04-07T23:24:00.000Z",
        "voteCount": 1,
        "content": "Agree with B"
      },
      {
        "date": "2024-02-03T09:38:00.000Z",
        "voteCount": 3,
        "content": "Answer D\n\nA: too complex and maybe small or zero saving if you can't find a valid smaller machine type\nB: Not valid. Question says that PRE environments are not critical after office hours. But it doesn't say no service at all\nC: Some risk is introduced if you have different architecture on PRE than PRO envs\nD: It's the only valid and realiable option. Simple and effective. It's my choice. In a real scenario I will first start with this and then review if the savings are enough before more complicated choices"
      },
      {
        "date": "2024-04-19T07:54:00.000Z",
        "voteCount": 1,
        "content": "Ad. \"B: Not valid. Question says that PRE environments are not critical after office hours. But it doesn't say no service at all\"\n\nBut the Question says that PRE environments are critical during office hours, so you can't use preemptible VMs -  \"Compute Engine might stop (preempt) these instances if it needs to reclaim the compute capacity for allocation to other VMs\""
      },
      {
        "date": "2024-01-15T18:06:00.000Z",
        "voteCount": 3,
        "content": "MIG's with autoscaling will scale to Zero if not needed"
      },
      {
        "date": "2024-02-03T09:29:00.000Z",
        "voteCount": 1,
        "content": "some risks are added if you have different architecture on PRO and PRE envs"
      },
      {
        "date": "2024-01-02T18:58:00.000Z",
        "voteCount": 1,
        "content": "B for sure"
      },
      {
        "date": "2023-12-20T22:25:00.000Z",
        "voteCount": 3,
        "content": "Also managed instance group reduces instances in case of low/no-traffic incurring lesser charges. Ideally, its a cleaner approach considering the ask is to optimize during \"idle time\". Incase people are working in different time zones, late shifts it doesnt make sense to trigger shutdown at a predefined times."
      },
      {
        "date": "2023-11-18T05:35:00.000Z",
        "voteCount": 1,
        "content": "B is the answer. But today, you don't need complicated CRON + CF. Auto shutdown by cron expression it's a feature built in: https://cloud.google.com/compute/docs/instances/schedule-instance-start-stop"
      },
      {
        "date": "2023-10-23T12:17:00.000Z",
        "voteCount": 5,
        "content": "really wondering why not C...Not critical is not equivalent with not running at all...."
      },
      {
        "date": "2023-12-20T22:24:00.000Z",
        "voteCount": 2,
        "content": "Also managed instance group reduces instances in case of low/no-traffic incurring lesser charges. Ideally, its a cleaner approach considering the ask is to optimize during \"idle time\". Incase people are working in different time zones, late shifts it doesnt make sense to trigger shutdown at a predefined times."
      },
      {
        "date": "2023-07-27T04:06:00.000Z",
        "voteCount": 5,
        "content": "\"are only critical during office hours\" does not mean it could be completely stopped. So may the option C correct?"
      },
      {
        "date": "2023-03-25T01:08:00.000Z",
        "voteCount": 3,
        "content": "In my opinion B is over-engineered:\nWhy not just add an \"instance schedule\" for start/stop the Compute Engines?\nWhy creating a scheduler and writing a Cloud Function..."
      },
      {
        "date": "2024-06-08T08:00:00.000Z",
        "voteCount": 1,
        "content": "Just exactly what I have thought. It is enough with instance schedule\". But GCP wants you to spend money and use cloud functions LOL"
      },
      {
        "date": "2023-02-23T12:56:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/compute/docs/instances/viewing-and-applying-idle-vm-recommendations\nB seems close to this Google provided service option, the extra step should be using idle VM recommendations to find and stop idle VM instances to reduce waste of resources"
      },
      {
        "date": "2023-02-19T08:58:00.000Z",
        "voteCount": 3,
        "content": "Since the price of preemptibles is 1/4 the price of a standard machine D costs far less than B since office hours are 1/3 of whole day. It costs less to keep them running 24h as preemptibles."
      },
      {
        "date": "2023-06-13T09:07:00.000Z",
        "voteCount": 1,
        "content": "Yes, but preemptibles use GCP excess resources so you will achieve the opposite of the desired effect, during office hours, they will underperform in the best case (worst case will stop altogether) and, during non-office hours, preemptibles will work well..."
      },
      {
        "date": "2022-12-31T11:43:00.000Z",
        "voteCount": 3,
        "content": "The Answer is B.\n\nJust had my exam today with a pass, this question was in the exam. Dated 31/12/22\nThanks to this site it was by far my most valuable"
      },
      {
        "date": "2022-12-22T04:32:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is B. Use Cloud Scheduler to trigger a Cloud Function that will stop the development and acceptance environments after office hours and start them just before office hours.\n\nOne way to optimize the cost of your Compute Engine environments is to stop non-critical instances when they are not in use. In this case, you could use Cloud Scheduler to trigger a Cloud Function that will stop the acceptance and development environments after office hours and start them just before office hours. This will allow you to take advantage of the cost savings of not running these environments during idle times, while still ensuring that they are available during office hours when they are critical."
      },
      {
        "date": "2022-12-22T04:33:00.000Z",
        "voteCount": 1,
        "content": "Option A, creating a shell script to change the machine type of the development and acceptance instances, is not relevant because it does not address the issue of cost optimization during idle times. \n\n\nOption C, using a managed instance group with autoscaling, is not recommended because it would not allow you to take advantage of the cost savings of stopping the instances during idle times. \n\nOption D, using preemptible VMs for the acceptance and development environments, is also not recommended because preemptible VMs may be terminated by Google at any time, which would not be suitable for workloads that are critical during office hours."
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 112,
    "url": "https://www.examtopics.com/discussions/google/view/56692-exam-professional-cloud-architect-topic-1-question-112/",
    "body": "You are moving an application that uses MySQL from on-premises to Google Cloud. The application will run on Compute Engine and will use Cloud SQL. You want to cut over to the Compute Engine deployment of the application with minimal downtime and no data loss to your customers. You want to migrate the application with minimal modification. You also need to determine the cutover strategy. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Set up Cloud VPN to provide private network connectivity between the Compute Engine application and the on-premises MySQL server. 2. Stop the on-premises application. 3. Create a mysqldump of the on-premises MySQL server. 4. Upload the dump to a Cloud Storage bucket. 5. Import the dump into Cloud SQL. 6. Modify the source code of the application to write queries to both databases and read from its local database. 7. Start the Compute Engine application. 8. Stop the on-premises application.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Set up Cloud SQL proxy and MySQL proxy. 2. Create a mysqldump of the on-premises MySQL server. 3. Upload the dump to a Cloud Storage bucket. 4. Import the dump into Cloud SQL. 5. Stop the on-premises application. 6. Start the Compute Engine application.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Set up Cloud VPN to provide private network connectivity between the Compute Engine application and the on-premises MySQL server. 2. Stop the on-premises application. 3. Start the Compute Engine application, configured to read and write to the on-premises MySQL server. 4. Create the replication configuration in Cloud SQL. 5. Configure the source database server to accept connections from the Cloud SQL replica. 6. Finalize the Cloud SQL replica configuration. 7. When replication has been completed, stop the Compute Engine application. 8. Promote the Cloud SQL replica to a standalone instance. 9. Restart the Compute Engine application, configured to read and write to the Cloud SQL standalone instance.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Stop the on-premises application. 2. Create a mysqldump of the on-premises MySQL server. 3. Upload the dump to a Cloud Storage bucket. 4. Import the dump into Cloud SQL. 5. Start the application on Compute Engine."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 34,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-07-10T23:41:00.000Z",
        "voteCount": 33,
        "content": "C. 1. Set up Cloud VPN to provide private network connectivity between the Compute Engine application and the on-premises MySQL server. 2. Stop the on-premises application. 3. Start the Compute Engine application, configured to read and write to the on-premises MySQL server. 4. Create the replication configuration in Cloud SQL. 5. Configure the source database server to accept connections from the Cloud SQL replica. 6. Finalize the Cloud SQL replica configuration. 7. When replication has been completed, stop the Compute Engine application. 8. Promote the Cloud SQL replica to a standalone instance. 9. Restart the Compute Engine application, configured to read and write to the Cloud SQL standalone instance."
      },
      {
        "date": "2024-01-16T20:07:00.000Z",
        "voteCount": 1,
        "content": "Agree with C.\n\nThe only confusing is step \"5. Configure the source database server to accept connections from the Cloud SQL replica.\"\n\nIs that not replication should go in the opposite direction from the on-premise (a.k.a. \"source\") database to Cloud SQL replica (presuming the latter is configured with a public IP address)?"
      },
      {
        "date": "2021-07-01T14:48:00.000Z",
        "voteCount": 20,
        "content": "Ans C, from this guy muhasinem \n\nExternal replica promotion migration\nIn the migration strategy of external replica promotion, you create an external database replica and synchronize the existing data to that replica. This can happen with minimal downtime to the existing database.\nWhen you have a replica database, the two databases have different roles that are referred to in this document as primary and replica.\nAfter the data is synchronized, you promote the replica to be the primary in order to move the management layer with minimal impact to database uptime.\nIn Cloud SQL, an easy way to accomplish the external replica promotion is to use the automated migration workflow. This process automates many of the steps that are needed for this type of migration."
      },
      {
        "date": "2024-06-07T06:45:00.000Z",
        "voteCount": 1,
        "content": "Minimal downtime. Downtime in A is time to take mysql dump + fix potential failures, not good. Downtime in C is just the time from restart the service."
      },
      {
        "date": "2024-04-19T08:29:00.000Z",
        "voteCount": 1,
        "content": "I wonder how Examtopics determines the so-called \"Correct Answer\" ..... C is correct"
      },
      {
        "date": "2023-08-25T07:05:00.000Z",
        "voteCount": 1,
        "content": "Answer is C"
      },
      {
        "date": "2023-05-25T04:25:00.000Z",
        "voteCount": 2,
        "content": "B is the correct answ"
      },
      {
        "date": "2023-04-23T01:12:00.000Z",
        "voteCount": 2,
        "content": "Option A, writing to two databases form the app :(\nOption C all the way, it also aligns to GCP Data Migration Service."
      },
      {
        "date": "2023-04-09T08:08:00.000Z",
        "voteCount": 4,
        "content": "mysql dump always causes long downtime."
      },
      {
        "date": "2023-03-30T12:40:00.000Z",
        "voteCount": 2,
        "content": "Correct Answer A\nC is unnecceory expensive and loss of data at after step 3"
      },
      {
        "date": "2023-03-09T17:34:00.000Z",
        "voteCount": 2,
        "content": "C seems to be the best answer but it is still a bit confusing.\n\nSo basically there's a bi-directional sync between the 2 databases? Cloud instance is the primary and is writing into the on-prem and on-prem is being replicated into the Cloud."
      },
      {
        "date": "2023-01-01T11:06:00.000Z",
        "voteCount": 2,
        "content": "Option C is not the correct answer because it involves modifying the application to read and write to both the on-premises MySQL server and Cloud SQL, which would involve significant modification to the application and could introduce potential complications or errors. It is generally better to minimize modification to the application when performing a migration. Option D, on the other hand, involves simply importing a mysqldump of the on-premises MySQL server into Cloud SQL and starting the application on Compute Engine, which is a simpler and more straightforward approach that involves minimal modification to the application."
      },
      {
        "date": "2022-12-27T21:41:00.000Z",
        "voteCount": 6,
        "content": "Examtopic providing A as correct answer is causing confusion,"
      },
      {
        "date": "2022-11-29T02:17:00.000Z",
        "voteCount": 2,
        "content": "C for minimal downtime"
      },
      {
        "date": "2022-11-12T07:07:00.000Z",
        "voteCount": 1,
        "content": "C is ok"
      },
      {
        "date": "2022-10-17T20:58:00.000Z",
        "voteCount": 7,
        "content": "Answer is C \nwe have a new service https://cloud.google.com/database-migration"
      },
      {
        "date": "2022-10-17T04:06:00.000Z",
        "voteCount": 2,
        "content": "C because it has minimal modification to the application or database. Also it's easier to fail back to the original solution if the cloud implementation has issues (assuming that there will be a \"post-go-live\" monitoring period)."
      },
      {
        "date": "2022-10-17T04:06:00.000Z",
        "voteCount": 1,
        "content": "C because it has minimal modification to the application or database. Also it's easier to fail back to the original solution if the cloud implementation has issues (assuming that there will be a \"post-go-live\" monitoring period)."
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 113,
    "url": "https://www.examtopics.com/discussions/google/view/56576-exam-professional-cloud-architect-topic-1-question-113/",
    "body": "Your organization has decided to restrict the use of external IP addresses on instances to only approved instances. You want to enforce this requirement across all of your Virtual Private Clouds (VPCs). What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRemove the default route on all VPCs. Move all approved instances into a new subnet that has a default route to an internet gateway.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new VPC in custom mode. Create a new subnet for the approved instances, and set a default route to the internet gateway on this new subnet.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement a Cloud NAT solution to remove the need for external IP addresses entirely.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet an Organization Policy with a constraint on constraints/compute.vmExternalIpAccess. List the approved instances in the allowedValues list.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 12,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-07-11T00:35:00.000Z",
        "voteCount": 24,
        "content": "D. Set an Organization Policy with a constraint on constraints/compute.vmExternalIpAccess. List the approved instances in the allowedValues list."
      },
      {
        "date": "2021-08-16T07:53:00.000Z",
        "voteCount": 19,
        "content": "Ans - D,  https://cloud.google.com/compute/docs/ip-addresses/reserve-static-external-ip-address#disableexternalip\n\nyou might want to restrict external IP address so that only specific VM instances can use them. This option can help to prevent data exfiltration or maintain network isolation. Using an Organization Policy, you can restrict external IP addresses to specific VM instances with constraints to control use of external IP addresses for your VM instances within an organization or a project."
      },
      {
        "date": "2024-05-28T01:28:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/compute/docs/ip-addresses/configure-static-external-ip-address#disableexternalip"
      },
      {
        "date": "2024-02-26T16:23:00.000Z",
        "voteCount": 1,
        "content": "\"You cannot apply the constraint retroactively. All VMs that have external IP addresses before you enable the policy retain their external IP addresses.\" \nhttps://cloud.google.com/compute/docs/ip-addresses/reserve-static-external-ip-address#disableexternalip\n\nIt shouldn't be option D then"
      },
      {
        "date": "2023-01-03T08:47:00.000Z",
        "voteCount": 3,
        "content": "D is correct one. \nhttps://cloud.google.com/compute/docs/ip-addresses/reserve-static-external-ip-address#disableexternalip"
      },
      {
        "date": "2022-12-14T08:21:00.000Z",
        "voteCount": 2,
        "content": "Show on my Exam,unfortunatekt im failed..:("
      },
      {
        "date": "2022-11-12T07:12:00.000Z",
        "voteCount": 1,
        "content": "D is ok"
      },
      {
        "date": "2022-10-15T15:18:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2022-09-11T06:06:00.000Z",
        "voteCount": 2,
        "content": "option D"
      },
      {
        "date": "2022-08-13T20:08:00.000Z",
        "voteCount": 2,
        "content": "I got this question in exam."
      },
      {
        "date": "2023-05-09T00:57:00.000Z",
        "voteCount": 1,
        "content": "Answer pleaase"
      },
      {
        "date": "2022-07-02T14:38:00.000Z",
        "voteCount": 1,
        "content": "D is right. constraints/compute.vmExternalIpAccess"
      },
      {
        "date": "2022-05-10T20:56:00.000Z",
        "voteCount": 2,
        "content": "vote for D\nhttps://cloud.google.com/compute/docs/ip-addresses/reserve-static-external-ip-address#disableexternalip"
      },
      {
        "date": "2022-03-05T10:42:00.000Z",
        "voteCount": 1,
        "content": "D it is"
      },
      {
        "date": "2022-02-11T13:40:00.000Z",
        "voteCount": 3,
        "content": "I got similar question on my exam. Answered D."
      },
      {
        "date": "2022-01-19T09:19:00.000Z",
        "voteCount": 2,
        "content": "Got this question in my exam, answered D"
      },
      {
        "date": "2021-12-09T02:56:00.000Z",
        "voteCount": 2,
        "content": "Go for D.\nhttps://cloud.google.com/compute/docs/ip-addresses/reserve-static-external-ip-address#disableexternalip"
      },
      {
        "date": "2021-12-02T19:00:00.000Z",
        "voteCount": 1,
        "content": "D is the correct answer\nhttps://cloud.google.com/compute/docs/ip-addresses/reserve-static-external-ip-address#disableexternalip"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 114,
    "url": "https://www.examtopics.com/discussions/google/view/56375-exam-professional-cloud-architect-topic-1-question-114/",
    "body": "Your company uses the Firewall Insights feature in the Google Network Intelligence Center. You have several firewall rules applied to Compute Engine instances.<br>You need to evaluate the efficiency of the applied firewall ruleset. When you bring up the Firewall Insights page in the Google Cloud Console, you notice that there are no log rows to display. What should you do to troubleshoot the issue?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Virtual Private Cloud (VPC) flow logging.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Firewall Rules Logging for the firewall rules you want to monitor.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tVerify that your user account is assigned the compute.networkAdmin Identity and Access Management (IAM) role.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall the Google Cloud SDK, and verify that there are no Firewall logs in the command line output."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 12,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-07-04T16:54:00.000Z",
        "voteCount": 35,
        "content": "Answer is B\nwhen you create a firewall rule there is an option for firewall rule logging on/off. It is set to off by default.\nTo get firewall insights or view the logs for a specific firewall rule you need to enable logging while creating the rule or you can enable it by editing that rule.\nhttps://cloud.google.com/network-intelligence-center/docs/firewall-insights/how-to/using-firewall-insights#enabling-fw-rules-logging"
      },
      {
        "date": "2021-07-11T00:34:00.000Z",
        "voteCount": 15,
        "content": "B. Enable Firewall Rules Logging for the firewall rules you want to monitor."
      },
      {
        "date": "2024-07-18T08:54:00.000Z",
        "voteCount": 1,
        "content": "First D, then B"
      },
      {
        "date": "2024-04-19T08:42:00.000Z",
        "voteCount": 2,
        "content": "Corrent answer is B"
      },
      {
        "date": "2023-02-02T19:41:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/vpc/docs/firewall-rules-logging"
      },
      {
        "date": "2022-12-31T11:44:00.000Z",
        "voteCount": 5,
        "content": "The Answer is B\n\nJust had my exam today with a pass, this question was in the exam. Dated 31/12/22\nThanks to this site it was by far my most valuable"
      },
      {
        "date": "2022-12-20T10:00:00.000Z",
        "voteCount": 1,
        "content": "You have to enable logging for a firewall rule in order to see the rows. \n\n\"When you enable logging for a firewall rule, Google Cloud creates an entry called a connection record each time the rule allows or denies traffic.\"\n\nhttps://cloud.google.com/vpc/docs/firewall-rules-logging"
      },
      {
        "date": "2022-11-12T07:14:00.000Z",
        "voteCount": 1,
        "content": "B is ok"
      },
      {
        "date": "2022-10-17T04:18:00.000Z",
        "voteCount": 1,
        "content": "B. Enable Firewall Rules Logging for the firewall rules you want to monitor."
      },
      {
        "date": "2022-10-15T15:19:00.000Z",
        "voteCount": 1,
        "content": "Enable firewall rules logging , B is right"
      },
      {
        "date": "2022-08-04T20:41:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/network-intelligence-center/docs/firewall-insights/how-to/using-firewall-insights#enabling-fw-rules-logging"
      },
      {
        "date": "2022-07-02T14:40:00.000Z",
        "voteCount": 1,
        "content": "B is most appropriate answer, I will choose B."
      },
      {
        "date": "2022-07-02T14:42:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/vpc/docs/firewall-rules-logging"
      },
      {
        "date": "2022-05-07T09:03:00.000Z",
        "voteCount": 2,
        "content": "Answered B. Got this question!"
      },
      {
        "date": "2022-02-15T14:15:00.000Z",
        "voteCount": 1,
        "content": "02/15/21 exam"
      },
      {
        "date": "2021-12-09T03:35:00.000Z",
        "voteCount": 1,
        "content": "Go for B"
      },
      {
        "date": "2021-12-02T19:18:00.000Z",
        "voteCount": 1,
        "content": "B is the correct answer\nhttps://cloud.google.com/network-intelligence-center/docs/firewall-insights/how-to/using-firewall-insights"
      },
      {
        "date": "2021-12-02T08:05:00.000Z",
        "voteCount": 1,
        "content": "B is the answer here"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 115,
    "url": "https://www.examtopics.com/discussions/google/view/56376-exam-professional-cloud-architect-topic-1-question-115/",
    "body": "Your company has sensitive data in Cloud Storage buckets. Data analysts have Identity Access Management (IAM) permissions to read the buckets. You want to prevent data analysts from retrieving the data in the buckets from outside the office network. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a VPC Service Controls perimeter that includes the projects with the buckets. 2. Create an access level with the CIDR of the office network.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a firewall rule for all instances in the Virtual Private Cloud (VPC) network for source range. 2. Use the Classless Inter-domain Routing (CIDR) of the office network.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a Cloud Function to remove IAM permissions from the buckets, and another Cloud Function to add IAM permissions to the buckets. 2. Schedule the Cloud Functions with Cloud Scheduler to add permissions at the start of business and remove permissions at the end of business.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a Cloud VPN to the office network. 2. Configure Private Google Access for on-premises hosts."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 33,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-07-01T00:00:00.000Z",
        "voteCount": 71,
        "content": "Should be A.\nFor all Google Cloud services secured with VPC Service Controls, you can ensure that:\nResources within a perimeter are accessed only from clients within authorized VPC networks using Private Google Access with either Google Cloud or on-premises.\nhttps://cloud.google.com/vpc-service-controls/docs/overview"
      },
      {
        "date": "2021-07-27T17:58:00.000Z",
        "voteCount": 14,
        "content": "Correct, this is about data exfiltration.\nSee: https://youtu.be/EXwJFL24QzY"
      },
      {
        "date": "2022-06-27T17:25:00.000Z",
        "voteCount": 1,
        "content": "Thanks for including the youtube video it was very helpful"
      },
      {
        "date": "2022-09-19T16:55:00.000Z",
        "voteCount": 2,
        "content": "nice one, thank you man"
      },
      {
        "date": "2023-11-18T14:07:00.000Z",
        "voteCount": 1,
        "content": "Enforce a security perimeter with VPC Service Controls to isolate resources of multi-tenant Google Cloud services\u2014reducing the risk of data exfiltration or data breach."
      },
      {
        "date": "2021-06-30T09:19:00.000Z",
        "voteCount": 17,
        "content": "IMHO c is wrong - the question is not to restrict access only for business hours but to restrict access to office network.\n\nIn my opinion the only realistic approach seems to be a)\n\nhttps://cloud.google.com/vpc-service-controls/docs/supported-products#table_storage"
      },
      {
        "date": "2024-05-20T06:52:00.000Z",
        "voteCount": 1,
        "content": "It's obviously A.\nC mentions office hours which has nothing to do with the question!"
      },
      {
        "date": "2024-04-19T08:50:00.000Z",
        "voteCount": 2,
        "content": "A is correct answer. Examtopics should change so-called \"Correct Answer\" from C to A to stop confusin users."
      },
      {
        "date": "2024-04-11T05:54:00.000Z",
        "voteCount": 1,
        "content": "I'm preparing for a test and see that questions from 115 onwards are considered valid. Can anyone who's taken the test offer any insights or advice? Thank you!"
      },
      {
        "date": "2024-01-06T06:59:00.000Z",
        "voteCount": 1,
        "content": "A is the correct answer. The question is specific to accessing the data outside of the office network. If the question talked about outside of work business hours then, we can consider C"
      },
      {
        "date": "2023-10-11T11:15:00.000Z",
        "voteCount": 2,
        "content": "How can be possible that Examtopics say that the correct answer is C?! It doesn't make any sense! A is the correct one."
      },
      {
        "date": "2023-08-28T05:31:00.000Z",
        "voteCount": 1,
        "content": "A is correct answer"
      },
      {
        "date": "2023-02-03T15:59:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/vpc-service-controls/docs/overview"
      },
      {
        "date": "2023-01-30T04:54:00.000Z",
        "voteCount": 1,
        "content": "A is correct because, For all Google Cloud services secured with VPC Service\nControls, you can ensure that resources within a perimeter are accessed only\nfrom clients within authorized VPC networks using Private Google Access with\neither Google Cloud or on-premises."
      },
      {
        "date": "2023-01-04T20:05:00.000Z",
        "voteCount": 4,
        "content": "A is the correct answer,\nhttps://cloud.google.com/vpc-service-controls/docs/overview#isolate\n* A VM within a Virtual Private Cloud (VPC) network that is part of a service perimeter can read from or write to a Cloud Storage bucket in the same perimeter. However, VPC Service Controls doesn't allow VMs within VPC networks that are outside the perimeter to access Cloud Storage buckets that are inside the perimeter.\n\n* A copy operation between two Cloud Storage buckets succeeds if both buckets are in the same service perimeter, but if one of the buckets is outside the perimeter, the copy operation fails.\n\n* VPC Service Controls doesn't allow a VM within a VPC network that is inside a service perimeter to access Cloud Storage buckets that are outside the perimeter."
      },
      {
        "date": "2022-12-25T08:38:00.000Z",
        "voteCount": 1,
        "content": "answer C will not prevent connection from outside of office network"
      },
      {
        "date": "2022-12-18T08:27:00.000Z",
        "voteCount": 2,
        "content": "For all Google Cloud services secured with VPC Service Controls, you can ensure that:\nResources within a perimeter are accessed only from clients within authorized VPC networks using Private Google Access with either Google Cloud or on-premises.\nhttps://cloud.google.com/vpc-service-controls/docs/overview"
      },
      {
        "date": "2022-11-12T07:17:00.000Z",
        "voteCount": 1,
        "content": "A is ok"
      },
      {
        "date": "2022-10-17T04:27:00.000Z",
        "voteCount": 1,
        "content": "A. Best option\nB. Not all instances need this restriction\nC. You are not restricting remote access. The users can still access remotely using their credentials during the business day. The ask is to restrict data retrieval from outside the office network (what if they are working from home...?)\nD. VPN - too much overhead"
      },
      {
        "date": "2022-10-17T04:27:00.000Z",
        "voteCount": 2,
        "content": "A. Best option\nB. Not all instances need this restriction\nC. You are not restricting remote access. The users can still access remotely using their credentials during the business day. The ask is to restrict data retrieval from outside the office network (what if they are working from home...?)\nD.  VPN - too much overhead"
      },
      {
        "date": "2022-10-15T08:36:00.000Z",
        "voteCount": 1,
        "content": "A for obvious reasons"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 116,
    "url": "https://www.examtopics.com/discussions/google/view/56399-exam-professional-cloud-architect-topic-1-question-116/",
    "body": "You have developed a non-critical update to your application that is running in a managed instance group, and have created a new instance template with the update that you want to release. To prevent any possible impact to the application, you don't want to update any running instances. You want any new instances that are created by the managed instance group to contain the new update. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStart a new rolling restart operation.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStart a new rolling replace operation.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStart a new rolling update. Select the Proactive update mode.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStart a new rolling update. Select the Opportunistic update mode.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 55,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-06-30T09:32:00.000Z",
        "voteCount": 57,
        "content": "IMHO the correct answer is d) opportunistic mode, not c) proactive mode.\n\nThe requirement is not to update any running instances. \n\nsee: https://cloud.google.com/compute/docs/instance-groups/rolling-out-updates-to-managed-instance-groups\nFor automated rolling updates, you must set the mode to proactive.\n\nAlternatively, if an automated update is potentially too disruptive, you can choose to perform an opportunistic update. The MIG applies an opportunistic update only when you manually initiate the update on selected instances or when new instances are created. New instances can be created when you or another service, such as an autoscaler, resizes the MIG."
      },
      {
        "date": "2021-07-11T00:28:00.000Z",
        "voteCount": 12,
        "content": "D. Start a new rolling update. Select the Opportunistic update mode."
      },
      {
        "date": "2024-09-22T01:31:00.000Z",
        "voteCount": 1,
        "content": "In Google Cloud, the main difference between proactive and opportunistic updates in a managed instance group (MIG) is when they are applied:\nProactive updates: Automatically apply updates to existing VMs.\nOpportunistic updates: Only apply updates when you manually select a VM to update or when new instances are created. \n\nfor all those who thinks its C please google and check its a straight forward question and you guyz are confusing people"
      },
      {
        "date": "2024-03-10T04:26:00.000Z",
        "voteCount": 4,
        "content": "Option C. To release a non-critical update to your application without updating any running instances and ensuring that new instances created by the managed instance group contain the new update, you should choose option C: Start a new rolling update and select the Proactive update mode.\n\nIn the Proactive update mode, the managed instance group creates new instances with the updated template while keeping the existing instances running until they are eventually replaced. This allows you to roll out the update gradually without affecting the currently running instances."
      },
      {
        "date": "2024-01-15T17:49:00.000Z",
        "voteCount": 3,
        "content": "D - Correct\nManaged instance groups support two types of update:\n\nAutomatic, or proactive, updates\nSelective, or opportunistic, updates\nIf you want to apply updates automatically, set the type to proactive.\n\nAlternatively, if an automated update is potentially too disruptive, you can choose to perform an opportunistic update. The MIG applies an opportunistic update only when you manually initiate the update on selected instances or when new instances are created."
      },
      {
        "date": "2024-01-06T07:20:00.000Z",
        "voteCount": 1,
        "content": "D is correct, per Google's documentation (The MIG applies an opportunistic update only when you manually initiate the update on selected instances or when new instances are created. New instances can be created when you or another service, such as an autoscaler, resizes the MIG.)"
      },
      {
        "date": "2023-12-27T22:26:00.000Z",
        "voteCount": 1,
        "content": "D\n\nBecause the question says: \"you don't want to update any running instances. You want any new instances that are created by the managed instance group to contain the new update.\"\nFor the above case, we chose opportunistic\n\n\nProactive vs Opportunistic:\nProactive: If you want to apply updates automatically, set the type to proactive.\nOpportunistic: Alternatively, if an automated update is potentially too disruptive, you can choose to perform an opportunistic update. The MIG applies an opportunistic update only when you manually initiate the update on selected instances or when new instances are created.\n\nhttps://cloud.google.com/compute/docs/instance-groups/rolling-out-updates-to-managed-instance-groups#type"
      },
      {
        "date": "2023-12-17T06:35:00.000Z",
        "voteCount": 2,
        "content": "Selected answer: D\n\n\"To prevent any possible impact to the application, you don't want to update any running instances\"\nthis is automatic achievable only by using opportunistic applying it during autoscale actions as documentation says: if you want to selectively apply a new configuration only to new or to specific instances in a MIG, see Selectively apply VM configuration updates in a MIG"
      },
      {
        "date": "2023-11-29T10:03:00.000Z",
        "voteCount": 2,
        "content": "In my opinion C is correct for Proactive update mode. Considering the following doc; \"....Automated updates support up to two instance template versions in your MIG. This means that you can specify two different instance template versions for your group, which is useful for performing canary updates.\n\nTo start a basic rolling update where the update is applied to all instances in the group, follow the instructions below.....\"\n\nSee Also:\n\"Update type\nManaged instance groups support two types of update:\n\nAutomatic, or proactive, updates\nSelective, or opportunistic, updates\nIf you want to apply updates automatically, set the type to proactive.\"\nhttps://cloud.google.com/compute/docs/instance-groups/rolling-out-updates-to-managed-instance-groups#starting_a_basic_rolling_update"
      },
      {
        "date": "2023-11-13T08:50:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/compute/docs/instance-groups/updating-migs#choosing_between_automated_and_selective_updates\n\nAutomatic (proactive): Use this method if you want the MIG to automatically apply new configurations to all or to a subset of existing VMs in the group. The level of disruption to running VMs depends on the update policy that you configure. You can use this method to canary update new instance templates. To use this method, set the MIG's update type to \"proactive\".\n\nSelective (opportunistic): Use this method if you want to apply the update manually or if you want to update all existing VMs in the group at once. You target any or all VMs to be updated to the latest configuration. To use this method, set the MIG's update type to \"opportunistic\".\n\nHence, C"
      },
      {
        "date": "2023-10-12T02:00:00.000Z",
        "voteCount": 2,
        "content": "It is option C, with proactive update, you are not updating the running instances, you start new ones with the new configuration template. And stop the old ones, so there is not disruption to the service."
      },
      {
        "date": "2023-10-06T03:53:00.000Z",
        "voteCount": 2,
        "content": "To apply an updated configuration to existing VMs, you can set up an automatic update\u2013also known as a proactive update type. The MIG automatically rolls out configuration updates to all or to a subset of the group's VMs.\n\nAlternatively, if an automated update is potentially too disruptive, you can choose to perform an opportunistic update. The MIG applies an opportunistic update only when you manually initiate the update on selected instances or when new instances are created. New instances can be created when you or another service, such as an autoscaler, resizes the MIG.\n\nhttps://cloud.google.com/compute/docs/instance-groups/rolling-out-updates-to-managed-instance-groups"
      },
      {
        "date": "2023-09-10T01:05:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/compute/docs/instance-groups/updating-migs#selective_updates\nno more explications neded"
      },
      {
        "date": "2023-08-28T11:34:00.000Z",
        "voteCount": 1,
        "content": "The key here is \"you don't want to update any running instances\". Only opportunistic support this."
      },
      {
        "date": "2023-08-28T05:33:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2023-08-14T10:26:00.000Z",
        "voteCount": 1,
        "content": "Are the people creating these tests retarded? C is correct? how? Correct answer is D"
      },
      {
        "date": "2023-08-09T21:43:00.000Z",
        "voteCount": 1,
        "content": "If you want to apply updates automatically, set the type to proactive."
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 117,
    "url": "https://www.examtopics.com/discussions/google/view/56403-exam-professional-cloud-architect-topic-1-question-117/",
    "body": "Your company is designing its application landscape on Compute Engine. Whenever a zonal outage occurs, the application should be restored in another zone as quickly as possible with the latest application data. You need to design the solution to meet this requirement. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a snapshot schedule for the disk containing the application data. Whenever a zonal outage occurs, use the latest snapshot to restore the disk in the same zone.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the Compute Engine instances with an instance template for the application, and use a regional persistent disk for the application data. Whenever a zonal outage occurs, use the instance template to spin up the application in another zone in the same region. Use the regional persistent disk for the application data.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a snapshot schedule for the disk containing the application data. Whenever a zonal outage occurs, use the latest snapshot to restore the disk in another zone within the same region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the Compute Engine instances with an instance template for the application, and use a regional persistent disk for the application data. Whenever a zonal outage occurs, use the instance template to spin up the application in another region. Use the regional persistent disk for the application data."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 33,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-07-01T00:15:00.000Z",
        "voteCount": 50,
        "content": "Answer is B, it only request zonal resiliency.\nRegional persistent disk is a storage option that provides synchronous replication of data between two zones in a region. Regional persistent disks can be a good building block to use when you implement HA services in Compute Engine.\n\nhttps://cloud.google.com/compute/docs/disks/high-availability-regional-persistent-disk"
      },
      {
        "date": "2022-05-26T19:35:00.000Z",
        "voteCount": 4,
        "content": "B. Agree, clearly it\u2019s B. Focus on keyword \u201czone\u201d"
      },
      {
        "date": "2021-07-03T04:05:00.000Z",
        "voteCount": 14,
        "content": "Answer is B"
      },
      {
        "date": "2024-04-25T11:22:00.000Z",
        "voteCount": 1,
        "content": "why D ?"
      },
      {
        "date": "2024-03-03T11:24:00.000Z",
        "voteCount": 1,
        "content": "It is B. As for D: Spinning up the application in another region might be too geographically distant, leading to higher latency and potential issues."
      },
      {
        "date": "2024-01-22T08:49:00.000Z",
        "voteCount": 1,
        "content": "A regional persistent disk is designed to provide synchronous replication of data between two zones in the same region, ensuring that data remains available even if one zone is affected by an outage. By using an instance template along with a regional disk, you can quickly create new instances in an available zone during a zonal outage and attach the regional persistent disk to continue operations with the latest application data."
      },
      {
        "date": "2024-01-17T03:30:00.000Z",
        "voteCount": 1,
        "content": "Can someone explain why not C - snapshot?"
      },
      {
        "date": "2024-01-31T07:40:00.000Z",
        "voteCount": 1,
        "content": "B - uses instance template which is ready for deploy. Option C requires manual configuration and this may take more time . But we need as quickly as possible ."
      },
      {
        "date": "2023-11-13T23:35:00.000Z",
        "voteCount": 1,
        "content": "Option D suggests using the same approach as Option B but restoring the application in another region instead of the same region. This approach provides high availability and disaster recovery across regions, making it suitable for applications that require high RTOs and minimal data loss."
      },
      {
        "date": "2024-01-03T06:17:00.000Z",
        "voteCount": 3,
        "content": "can you use the regional persistent disk in a different region?"
      },
      {
        "date": "2024-01-16T21:41:00.000Z",
        "voteCount": 1,
        "content": "apparently, nope."
      },
      {
        "date": "2023-10-12T03:37:00.000Z",
        "voteCount": 2,
        "content": "Of course it is answer B. I would like to understand who chooses the right answers in examtopics! Choosing here option D is completely wrong. This takes people less instructed to be mistaken."
      },
      {
        "date": "2023-10-06T03:56:00.000Z",
        "voteCount": 1,
        "content": "B. Configure the Compute Engine instances with an instance template for the application, and use a regional persistent disk for the application data. Whenever a zonal outage occurs, use the instance template to spin up the application in another zone in the same region. Use the regional persistent disk for the application data. Most Voted\n\nWhy to change the region as mentioned in Option D, when the ask is different zone only."
      },
      {
        "date": "2023-09-10T01:07:00.000Z",
        "voteCount": 1,
        "content": "Answer is B, it only request zonal resiliency"
      },
      {
        "date": "2023-08-28T05:35:00.000Z",
        "voteCount": 1,
        "content": "It says restore in a zonal. Hence answer is B."
      },
      {
        "date": "2023-03-25T01:20:00.000Z",
        "voteCount": 2,
        "content": "B. In my opinion, I cannot use a regional disk in a different region!!! So, it can only be another zone in the same region. Therefore D must be wrong!"
      },
      {
        "date": "2023-01-31T22:09:00.000Z",
        "voteCount": 1,
        "content": "Answer B is Correct - since it talks about spin up application in different zone but same region. \n\nWhereas,D is incoorect , since its talking about spin up application in different region which is not our requirement."
      },
      {
        "date": "2023-01-29T11:31:00.000Z",
        "voteCount": 2,
        "content": "If it is a regional persistent disk created in region A for example. If I start the compute engine instance in the region B, how am I going to use a regional disk from region A (another region)? I do not think it is possible. So answer D should be excluded. \nI do believe that the correct answer is B."
      },
      {
        "date": "2022-12-31T11:46:00.000Z",
        "voteCount": 5,
        "content": "The Answer is B\n\nJust had my exam today with a pass, this question was in the exam. Dated 31/12/22\nThanks to this site it was by far my most valuable"
      },
      {
        "date": "2022-12-14T08:07:00.000Z",
        "voteCount": 1,
        "content": "14/12/22 Exam,but IM failed :("
      },
      {
        "date": "2022-12-26T08:03:00.000Z",
        "voteCount": 2,
        "content": "Tomorrow 12/27/22 is my exam :)"
      },
      {
        "date": "2023-05-09T02:37:00.000Z",
        "voteCount": 1,
        "content": "How was it?"
      },
      {
        "date": "2022-11-12T07:26:00.000Z",
        "voteCount": 1,
        "content": "B is ok"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 118,
    "url": "https://www.examtopics.com/discussions/google/view/56680-exam-professional-cloud-architect-topic-1-question-118/",
    "body": "Your company has just acquired another company, and you have been asked to integrate their existing Google Cloud environment into your company's data center. Upon investigation, you discover that some of the RFC 1918 IP ranges being used in the new company's Virtual Private Cloud (VPC) overlap with your data center IP space. What should you do to enable connectivity and make sure that there are no routing conflicts when connectivity is established?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Cloud VPN connection from the new VPC to the data center, create a Cloud Router, and apply new IP addresses so there is no overlapping IP space.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Cloud VPN connection from the new VPC to the data center, and create a Cloud NAT instance to perform NAT on the overlapping IP space.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Cloud VPN connection from the new VPC to the data center, create a Cloud Router, and apply a custom route advertisement to block the overlapping IP space.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Cloud VPN connection from the new VPC to the data center, and apply a firewall rule that blocks the overlapping IP space."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 33,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 19,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 12,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-07-30T09:14:00.000Z",
        "voteCount": 42,
        "content": "Correct Answer: A\n- IP Should not overlap so applying new IP address is the solution"
      },
      {
        "date": "2022-03-14T00:28:00.000Z",
        "voteCount": 7,
        "content": "A is not correct. \"What should you do to enable connectivity and make sure that there are no routing conflicts when connectivity is established?\" if you apply VPN con BGP, the actual IP address will be propagated to on prem environment with overlapping RFC1918 as result. B is correct with custom route"
      },
      {
        "date": "2021-07-01T00:41:00.000Z",
        "voteCount": 36,
        "content": "Answer is C.\nhttps://cloud.google.com/network-connectivity/docs/router/how-to/advertising-custom-ip"
      },
      {
        "date": "2021-10-13T01:39:00.000Z",
        "voteCount": 5,
        "content": "The Q states to establish connectivity. This would merely prevent that. Ans is A"
      },
      {
        "date": "2024-01-16T21:48:00.000Z",
        "voteCount": 2,
        "content": "I would also agree with C.\n\nStill, this part is confusing: \"C. Create a Cloud VPN connection from the new VPC to the data center, create a Cloud Router, and apply a custom route advertisement to *block* the overlapping IP space.\"\n\nTo *block*? Not to block. just to alias with advertised IP addresses."
      },
      {
        "date": "2021-07-12T20:37:00.000Z",
        "voteCount": 8,
        "content": "ANS is B\nhttps://cloud.google.com/architecture/best-practices-vpc-design"
      },
      {
        "date": "2021-07-28T05:04:00.000Z",
        "voteCount": 13,
        "content": "B is NOT correct. Cloud NAT is specifically used for translating the IP address of the outbound packets destined to the Internet. But this question is about using VPN communication between two private IP address spaces (RFC1918). Cloud NAT cannot achieve the purpose here, you can't use Cloud NAT to translate from one private IP to another private ip. I would vote for C."
      },
      {
        "date": "2024-04-18T08:32:00.000Z",
        "voteCount": 2,
        "content": "You can use private or hybrid NAT\nhttps://cloud.google.com/nat/docs/overview#private-nat"
      },
      {
        "date": "2021-10-18T06:53:00.000Z",
        "voteCount": 1,
        "content": "Thanks for the clarification, just one question, without a solution like NAT or reip, the service on the devices with overlapping IP subnet will be unavailable for on-premise devices, not sure if the question also about this"
      },
      {
        "date": "2023-10-02T08:46:00.000Z",
        "voteCount": 2,
        "content": "It will be a NAT Router instance, which will route the traffic. I have practically applied the configuration."
      },
      {
        "date": "2022-01-06T02:42:00.000Z",
        "voteCount": 16,
        "content": "You can't use Cloud NAT according to this documentation: https://cloud.google.com/nat/docs/troubleshooting#overlapping-ip-addresses\n\n\"Can I use Cloud NAT to connect a VPC network to another network to work around overlapping IP addresses? No, Cloud NAT cannot apply to any custom route whose next hop is not the default internet gateway. For example, Cloud NAT cannot apply to traffic sent to a next hop Cloud VPN tunnel, even if the destination is a publicly routable IP address.\""
      },
      {
        "date": "2024-10-06T12:56:00.000Z",
        "voteCount": 1,
        "content": "Correct Option:\nB. Create a Cloud VPN connection from the new VPC to the data center, and create a Cloud NAT instance to perform NAT on the overlapping IP space.\n\nThis option effectively allows you to connect the two environments while addressing the overlapping IP space issue through NAT, ensuring that the VMs can communicate without conflicts."
      },
      {
        "date": "2024-10-01T10:18:00.000Z",
        "voteCount": 2,
        "content": "There is a Private NAT you can use and is specifically designed to resolve overlapping private IP issues: https://medium.com/niveus-solutions/private-cloud-nat-and-why-we-need-it-on-gcp-f6ad0c96facb#:~:text=Private%20Cloud%20NAT%20with%20NCC,helps%20connect%20onprem%20to%20gcp."
      },
      {
        "date": "2024-07-31T00:21:00.000Z",
        "voteCount": 1,
        "content": "Given that you are not going out to the internet and you need to use a Cloud Router for your VPC, you need to ensure that there is no overlap in the IP ranges between your data center and the newly acquired company's VPC. The best approach to manage this without renumbering the entire network is to use Network Address Translation (NAT) to handle the overlapping IP addresses."
      },
      {
        "date": "2024-07-04T05:29:00.000Z",
        "voteCount": 1,
        "content": "would go for B"
      },
      {
        "date": "2024-06-18T09:57:00.000Z",
        "voteCount": 1,
        "content": "The answer is B. Cloud VPN and Cloud NAT help you get around this problem easily without all the work of creating a new subnet and reassigning IPs to everything.\n\nCloud NAT: Network Address Translation (NAT) allows you to translate IP addresses in your VPC to a different IP range, avoiding conflicts with overlapping IP ranges in your data center. This ensures that traffic can flow between the environments without routing conflicts.\nCloud VPN: Establishing a Cloud VPN connection provides secure connectivity between the new VPC and your data center. By combining this with Cloud NAT, you can effectively manage and resolve the IP address overlap."
      },
      {
        "date": "2024-06-18T09:59:00.000Z",
        "voteCount": 1,
        "content": "Cloud NAT does not directly resolve IP address conflicts due to overlapping ranges. Cloud NAT is typically used for instances without external IP addresses to access the internet while preserving their internal IPs for internal communications."
      },
      {
        "date": "2024-06-05T03:28:00.000Z",
        "voteCount": 3,
        "content": "Using Cloud NAT to translate overlapping IP addresses is the most effective solution to ensure seamless connectivity between the new company's VPC and your company's data center without routing conflicts. This approach avoids the complexity of reconfiguring IP addresses and ensures that both networks can communicate effectively. https://cloud.google.com/nat/docs/overview#private-nat"
      },
      {
        "date": "2024-06-08T07:36:00.000Z",
        "voteCount": 1,
        "content": "It is not NAT, we are not going out to internet. We need cloud router"
      },
      {
        "date": "2024-06-01T00:38:00.000Z",
        "voteCount": 1,
        "content": "Ans is B"
      },
      {
        "date": "2024-05-11T07:38:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/nat/docs/overview#private-nat\n\nAssume that the resources in your VPC network need to communicate with the resources in a VPC network or an on-premises or other cloud provider network that is owned by a different business entity. However, the VPC network of that business entity contains subnets whose IP addresses overlap with the IP addresses of your VPC network. In this scenario, you create a Private NAT gateway that routes traffic between the subnets in your VPC network to the non-overlapping subnets of that business entity."
      },
      {
        "date": "2024-03-18T14:03:00.000Z",
        "voteCount": 7,
        "content": "I was absolutely sure that B was obviously wrong until I found that \nhttps://cloud.google.com/nat/docs/overview#private-nat\nSo it seems like the answer is B..."
      },
      {
        "date": "2024-05-06T13:13:00.000Z",
        "voteCount": 1,
        "content": "B. THIS should be the accepted answer, the link you provide is 100% certain. It's a Private Hybrid NAT:\n\" ...private-to-private translations... traffic between VPC networks and on-premises networks...\"\n\"...IP addresses overlap with the IP addresses of your VPC network. In this scenario, you create a Private NAT gateway...\"\nB, 100%"
      },
      {
        "date": "2024-03-10T05:00:00.000Z",
        "voteCount": 1,
        "content": "The challenge with Option A is that changing IP addresses can be complex and might impact existing applications, configurations, and dependencies within the new company's VPC. It might introduce additional complexity and potential risks during the integration process.\n\nOption C, on the other hand, allows you to maintain the existing IP addressing in the new company's VPC while selectively blocking the overlapping IP space during the routing process. This can be a more flexible and less disruptive approach, especially in scenarios where readdressing is not practical.\n\nIn summary, both options might have their use cases, but Option C provides a solution that doesn't require changing IP addresses and can help avoid potential disruptions caused by such changes."
      },
      {
        "date": "2024-02-02T01:06:00.000Z",
        "voteCount": 1,
        "content": "with C option we would not able to connect to VM with those overlapping IP.\nwe need to add a middle VPC between them. it will be more complicated.\nwe have not choice here except reassigning IP adresses so i choose option A"
      },
      {
        "date": "2023-12-13T14:31:00.000Z",
        "voteCount": 7,
        "content": "I think now the answer should change since Private NAT is publicly available: https://cloud.google.com/nat/docs/private-nat"
      },
      {
        "date": "2023-12-04T11:17:00.000Z",
        "voteCount": 1,
        "content": "Apply new IP addresses? You do not apply new IP, you replace them. Either poorly written or deceiving. To enable connectivity and avoid routing conflicts, C is perfect. Long term of course we need to replace IP, but not to enable connectivity. C."
      },
      {
        "date": "2023-10-29T07:26:00.000Z",
        "voteCount": 1,
        "content": "All answers are incorrect. Overall, it is a NAT question, but cloud NAT can't nat private IP space. No idea how route can solve the overlapping issue. \nThere is a third party NAT option: https://www.linkedin.com/pulse/resolving-overlapping-ip-issue-when-connecting-tofrom-bayu-wibowo"
      },
      {
        "date": "2023-07-19T00:37:00.000Z",
        "voteCount": 5,
        "content": "Could not be B, as Cloud NAT only apply on route targeting default gateway.\nCould not be C : if you block route advertisement, then you will have no route to your datacenter, and you will be unable to connect your datacenter\nCould not be D : blocking using firewall the overlapping IP space will not provide connectivity to these ressource\n\nSo answer could only be A : user should update its IP space so it does not overlap"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 119,
    "url": "https://www.examtopics.com/discussions/google/view/56684-exam-professional-cloud-architect-topic-1-question-119/",
    "body": "You need to migrate Hadoop jobs for your company's Data Science team without modifying the underlying infrastructure. You want to minimize costs and infrastructure management effort. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Dataproc cluster using standard worker instances.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Dataproc cluster using preemptible worker instances.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tManually deploy a Hadoop cluster on Compute Engine using standard instances.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tManually deploy a Hadoop cluster on Compute Engine using preemptible instances."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 45,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 34,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-07-01T00:47:00.000Z",
        "voteCount": 67,
        "content": "Should be B,  you want to minimize costs.\nhttps://cloud.google.com/dataproc/docs/concepts/compute/secondary-vms#preemptible_and_non-preemptible_secondary_workers"
      },
      {
        "date": "2021-10-13T17:57:00.000Z",
        "voteCount": 4,
        "content": "Agree, the migration guide also recommends to think about preemptible worker nodes: https://cloud.google.com/architecture/hadoop/hadoop-gcp-migration-jobs#using_preemptible_worker_nodes"
      },
      {
        "date": "2022-11-24T08:21:00.000Z",
        "voteCount": 2,
        "content": "I think it's A.\nThe question does not mention anything about minimize the costs, all the questions in GCP exams that require minimize the costs as requirement literally mention that in the question.\nAlso in order to minimize the costs you need to build jobs that are fault tolerant, as workers instances are preemptible. This also requires some kind of Dev investment of work. So if not mentioned in the question fault tolerant and minimize costs then is not required/needed.\n\nDoc states below:\nOnly use preemptible nodes for jobs that are fault-tolerant or that are low enough priority that occasional job failure won't disrupt your business."
      },
      {
        "date": "2023-04-02T10:38:00.000Z",
        "voteCount": 5,
        "content": "OMG, you again?\n\n zetalexg says:\nIt's dissapointing that you waste your time writting on this topic instead of paying attention at the questions."
      },
      {
        "date": "2021-07-01T08:00:00.000Z",
        "voteCount": 5,
        "content": "Hi TotoroChina, \nI had the same thought when I first read the question - the problem I see is, in real business I think you would try to mix preemtible instances and on-demand instances... Here you have to choose between only preemtible instances and on-demand instances... Preemptible instances have some downsides - so we would need more details and ideally a mixed approach. That's why both answers might be correcy, a) and b)...\nDo you see that different?\nThanks!\nCheers,\nD."
      },
      {
        "date": "2021-07-07T20:10:00.000Z",
        "voteCount": 5,
        "content": "but you need to reduce management overhead so B\nif you create a cluster manually and create and maintain GCE is not the way to go"
      },
      {
        "date": "2022-11-10T12:34:00.000Z",
        "voteCount": 2,
        "content": "B requires to create new instances at least every 24h."
      },
      {
        "date": "2022-10-30T10:02:00.000Z",
        "voteCount": 7,
        "content": "\"without modifying the underlying infrastructure\" is the watch word. Most likely did not utilize preemptible on-premises"
      },
      {
        "date": "2023-01-19T03:22:00.000Z",
        "voteCount": 3,
        "content": "A cost-savings consideration: Using preemptible VMs does not always save costs since preemptions can cause longer job execution with resulting higher job costs. This is mentioned in above link So I think Ans should be A"
      },
      {
        "date": "2021-07-28T07:43:00.000Z",
        "voteCount": 35,
        "content": "It's A, the primary workers can only be standard, where secondary workers can be preemtible.------In addition to using standard Compute Engine VMs as Dataproc workers (called \"primary\" workers), Dataproc clusters can use \"secondary\" workers.\nThere are two types of secondary workers: preemptible and non-preemptible. All secondary workers in your cluster must be of the same type, either preemptible or non-preemptible. The default is preemptible."
      },
      {
        "date": "2021-09-06T02:57:00.000Z",
        "voteCount": 2,
        "content": "agreed"
      },
      {
        "date": "2024-09-30T04:36:00.000Z",
        "voteCount": 1,
        "content": "Probably A\n- Primary workers must be standard\n- Preemptible doesn't always save cost\n- Infrastructure on prem doesn't have spot machines\n- You cannot choose spot/preemptible when creating the cluster, only when provisioning secondary nodes, which are actually preemptible by default.\n- They do not store data, only do data processing"
      },
      {
        "date": "2024-09-28T00:54:00.000Z",
        "voteCount": 1,
        "content": "B: being a multiple-choice question, we need to focus on explicit keywords here. \nManagement effort =&gt; Managed service -&gt; Dataproc.\nCost-optimization =&gt; Preemptible. \n\nFor ones who say \"but that also requires fault toleration\": well, there is no explicit keyword in the question says \"we have critical jobs\" or \"out data scientists team has not takes into account toleration\". So we must not assume that's needed."
      },
      {
        "date": "2024-07-13T13:42:00.000Z",
        "voteCount": 1,
        "content": "I will go with A , reason preemptible instances are unpredictable and there is no mention of work criticallity. So my answer is A against B"
      },
      {
        "date": "2024-04-24T04:33:00.000Z",
        "voteCount": 1,
        "content": "A - only secondary workers can be preemptible and \"Using preemptible VMs does not always save costs since preemptions can cause longer job execution with resulting higher job costs\" according to: https://cloud.google.com/dataproc/docs/concepts/compute/secondary-vms#preemptible_and_non-preemptible_secondary_workers"
      },
      {
        "date": "2024-04-18T08:46:00.000Z",
        "voteCount": 1,
        "content": "Agree with B"
      },
      {
        "date": "2024-03-27T11:49:00.000Z",
        "voteCount": 1,
        "content": "Answer is B. \nThe  secondary worker type instance for default  Dataproc cluster is preemptible VMs.\nhttps://cloud.google.com/dataproc/docs/concepts/compute/secondary-vms"
      },
      {
        "date": "2024-03-10T05:09:00.000Z",
        "voteCount": 1,
        "content": "Dataproc: Dataproc is a fully managed Apache Spark and Hadoop service on Google Cloud Platform. It allows you to run clusters without the need to manually deploy and manage Hadoop clusters on Compute Engine.\n\nPreemptible Worker Instances: Preemptible instances are short-lived, cost-effective virtual machine instances that are suitable for fault-tolerant and batch processing workloads. Since Hadoop jobs can often tolerate interruptions, using preemptible instances can significantly reduce costs.\n\nOption B leverages the benefits of Dataproc for managing Hadoop clusters without the need for manual deployment and takes advantage of preemptible instances to minimize costs. This aligns well with the goal of minimizing both costs and infrastructure management efforts."
      },
      {
        "date": "2024-02-27T01:27:00.000Z",
        "voteCount": 1,
        "content": "Using preemptible VMs does not always save costs since preemptions can cause longer job execution with resulting higher job costs"
      },
      {
        "date": "2024-02-18T05:19:00.000Z",
        "voteCount": 1,
        "content": "Using standard Compute Engine VMs as Dataproc workers (called \"primary\" workers), Preemptible can be only used for secondary workers hence A is valid answer"
      },
      {
        "date": "2024-02-02T00:39:00.000Z",
        "voteCount": 2,
        "content": "minimize costs -&gt; preemtipble"
      },
      {
        "date": "2024-02-01T23:08:00.000Z",
        "voteCount": 2,
        "content": "You want to minimize costs and infrastructure management effort &gt; B"
      },
      {
        "date": "2024-02-01T23:08:00.000Z",
        "voteCount": 1,
        "content": "\"You want to minimize costs and infrastructure management effort\" -&gt; B"
      },
      {
        "date": "2024-01-18T12:34:00.000Z",
        "voteCount": 1,
        "content": "It is A"
      },
      {
        "date": "2024-01-10T00:58:00.000Z",
        "voteCount": 1,
        "content": "Answer should be B, because minizing the costs is wanted."
      },
      {
        "date": "2024-01-06T07:42:00.000Z",
        "voteCount": 1,
        "content": "A is the correct response. Per documentation \"You can gain low-cost processing power for your jobs by adding preemptible worker nodes to your cluster. These nodes use preemptible virtual machines.\" The focus of the question is to reduce cost, hence preempttible VM works best"
      },
      {
        "date": "2024-01-06T07:46:00.000Z",
        "voteCount": 1,
        "content": "B not A"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 120,
    "url": "https://www.examtopics.com/discussions/google/view/56416-exam-professional-cloud-architect-topic-1-question-120/",
    "body": "Your company has a project in Google Cloud with three Virtual Private Clouds (VPCs). There is a Compute Engine instance on each VPC. Network subnets do not overlap and must remain separated. The network configuration is shown below.<br><img src=\"/assets/media/exam-media/04339/0013300001.jpg\" class=\"in-exam-image\"><br>Instance #1 is an exception and must communicate directly with both Instance #2 and Instance #3 via internal IPs. How should you accomplish this?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a cloud router to advertise subnet #2 and subnet #3 to subnet #1.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd two additional NICs to Instance #1 with the following configuration: \u05d2\u20ac\u00a2 NIC1 \u05d2\u2014\u2039 VPC: VPC #2 \u05d2\u2014\u2039 SUBNETWORK: subnet #2 \u05d2\u20ac\u00a2 NIC2 \u05d2\u2014\u2039 VPC: VPC #3 \u05d2\u2014\u2039 SUBNETWORK: subnet #3 Update firewall rules to enable traffic between instances.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate two VPN tunnels via CloudVPN: \u05d2\u20ac\u00a2 1 between VPC #1 and VPC #2. \u05d2\u20ac\u00a2 1 between VPC #2 and VPC #3. Update firewall rules to enable traffic between the instances.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPeer all three VPCs: \u05d2\u20ac\u00a2 Peer VPC #1 with VPC #2. \u05d2\u20ac\u00a2 Peer VPC #2 with VPC #3. Update firewall rules to enable traffic between the instances."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 23,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 5,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 5,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-06-30T09:57:00.000Z",
        "voteCount": 25,
        "content": "According to my understanding the requirement is that only VM1 shall be able to communicate with VM2 and VM3, but not VM2 with VM3.\nWe can exclude d) as d) would enable VM2 to communicate with VM3 as well - my assumption is, that if the quizzer wanted that d) is the correct answer, he would make just 2 peerings - 1x between VM1 and VM2 and 1x between VM1 and VM3 repectively the VPCs.\nWe can exclude c) as well - there is no connection between VPC1 and VPC3.\nIMHO a) will not work.\nSo the only correct answer seems to be b) - what I don't understand is why we have to update the firewall rules as IMHO the default firewall rules enable such communication (maybe some restrictive rules are implemented - not enough details in the question to clarify that part). Please correct me if I am wrong."
      },
      {
        "date": "2021-07-06T03:17:00.000Z",
        "voteCount": 2,
        "content": "Correct, maybe fw on the VM"
      },
      {
        "date": "2021-07-22T01:45:00.000Z",
        "voteCount": 7,
        "content": "I think it is because the instances are in separate VPCs.\n\n\"Google Cloud Virtual Private Cloud (VPC) networks are by default isolated private networking domains. Networks have a global scope and contain regional subnets. VM instances within a VPC network can communicate among themselves using internal IP addresses as long as firewall rules permit. However, NO INTERNAL IP ADDRESS COMMUNICATION IS ALLOWED BETWEEN networks, unless you set up mechanisms such as VPC Network Peering or Cloud VPN.\"\n\nThe instructions for setting up multiple interfaces tells you to check your firewall rules as as the firewall rules of the VPC apply to the network interface that it is attached to.\n\nhttps://cloud.google.com/vpc/docs/multiple-interfaces-concepts#firewall_rules_and_multiple_network_interfaces"
      },
      {
        "date": "2022-08-20T05:00:00.000Z",
        "voteCount": 4,
        "content": "The answer is \"B\".  The following link has this - \"Use multiple network interfaces when an individual instance needs access to more than one VPC network, but you don't want to connect both networks directly.\" https://cloud.google.com/vpc/docs/multiple-interfaces-concepts"
      },
      {
        "date": "2023-10-09T23:17:00.000Z",
        "voteCount": 1,
        "content": "you can not add additional network interface to existing VM's"
      },
      {
        "date": "2022-08-20T05:00:00.000Z",
        "voteCount": 13,
        "content": "The answer is \"B\".  The following link has this - \"Use multiple network interfaces when an individual instance needs access to more than one VPC network, but you don't want to connect both networks directly.\" https://cloud.google.com/vpc/docs/multiple-interfaces-concepts"
      },
      {
        "date": "2022-11-20T03:26:00.000Z",
        "voteCount": 1,
        "content": "B will not work.\nVM instances within a VPC network can communicate among themselves using internal IP addresses as long as firewall rules permit. However, no internal IP address communication is allowed between networks, unless you set up mechanisms such as VPC Network Peering or Cloud VPN."
      },
      {
        "date": "2022-12-27T05:12:00.000Z",
        "voteCount": 6,
        "content": "this link says VM can have multiple NICs and attached to different VPCs.\nhttps://cloud.google.com/vpc/docs/create-use-multiple-interfaces\nso B is the answer"
      },
      {
        "date": "2021-07-07T21:50:00.000Z",
        "voteCount": 11,
        "content": "Answer is B"
      },
      {
        "date": "2022-05-09T11:14:00.000Z",
        "voteCount": 6,
        "content": "Instances are exist. You can not add or remove additional NICs to a VM"
      },
      {
        "date": "2024-07-26T15:28:00.000Z",
        "voteCount": 2,
        "content": "Direct Connectivity:\n\nAdding multiple NICs to Instance #1 allows it to be part of multiple VPCs directly. This configuration enables direct communication with Instance #2 and Instance #3 via internal IPs without requiring additional routing configurations.\nSimplicity:\n\nThis approach is straightforward and avoids the complexity of setting up VPC peering or VPN tunnels. It ensures that only Instance #1 has access to both VPC #2 and VPC #3, maintaining the separation of the other VPCs."
      },
      {
        "date": "2024-07-13T13:44:00.000Z",
        "voteCount": 1,
        "content": "VPC peering will allow access to instance 2 &amp; 3 from 1 with internal IP, with necessary firewall rules added."
      },
      {
        "date": "2024-04-18T08:48:00.000Z",
        "voteCount": 1,
        "content": "B for sure"
      },
      {
        "date": "2024-03-10T05:17:00.000Z",
        "voteCount": 1,
        "content": "Option B allows you to add additional NICs to Instance #1, each connected to a different VPC, facilitating direct communication between Instance #1 and the other instances while maintaining separate subnets."
      },
      {
        "date": "2024-01-02T04:26:00.000Z",
        "voteCount": 2,
        "content": "B is wrong. NIC can only be configured while creating the instance. Here the instance is already created. \nC is correct answer. \n\nRefer limitation in this link:\nhttps://cloud.google.com/vpc/docs/create-use-multiple-interfaces"
      },
      {
        "date": "2023-10-06T04:14:00.000Z",
        "voteCount": 1,
        "content": "Router, VPN and VPC Peering for all 3 network is not required.\n\nOnly option B solves the given scenario."
      },
      {
        "date": "2023-08-15T02:55:00.000Z",
        "voteCount": 3,
        "content": "All answers are incorrect: subnets do not overlap and must remain separated. =&gt; can't choose A or C or D. \nWhich leaves us with A: you can't attach nics to a compute engine instance after creation : see: https://cloud.google.com/vpc/docs/create-use-multiple-interfaces"
      },
      {
        "date": "2023-04-15T06:41:00.000Z",
        "voteCount": 1,
        "content": "Is D the correct, peering with adeguate forewall rule for only communication of Instance 1 with Instance 2 and 3"
      },
      {
        "date": "2023-03-25T01:27:00.000Z",
        "voteCount": 1,
        "content": "I vote for B:\nVPC peering does not support \"cascading\". Peer VPC 1 with VPC 2, and VPC 2 with VPC 3 does not allow traffic from VPC 1 to VPC 3."
      },
      {
        "date": "2023-02-17T04:11:00.000Z",
        "voteCount": 1,
        "content": "B: NIC usecase  when an individual instance needs access to more than one VPC network, but you don't want to connect both networks directly\nhttps://cloud.google.com/vpc/docs/multiple-interfaces-concepts"
      },
      {
        "date": "2023-01-04T23:12:00.000Z",
        "voteCount": 1,
        "content": "B is the correct answer,\n\nConnect the VPC1 instance to VPC2 instance with NIC1 and Connect VPC1 instance to VPC3 instance with NIC2. And update firewall rules to enable traffic between them.\n\nhttps://cloud.google.com/vpc/docs/multiple-interfaces-concepts#firewall_rules_and_multiple_network_interfaces"
      },
      {
        "date": "2022-12-25T09:50:00.000Z",
        "voteCount": 1,
        "content": "best practice is to add NIC to first instance"
      },
      {
        "date": "2022-12-06T12:41:00.000Z",
        "voteCount": 1,
        "content": "Only solution is peering. N1 peering to  n3 and n3 to n1 makes all network peered. So answer should be D"
      },
      {
        "date": "2022-11-20T03:21:00.000Z",
        "voteCount": 1,
        "content": "B would be incorrect --&gt; As without VPC peering or VPN it will not come into Play.\nD --&gt; This is good as once VPN is established from 1 --&gt; 2 and from 2 --&gt; 3 ... data can flow from 1 to 3 via 2 ..."
      },
      {
        "date": "2022-11-20T03:22:00.000Z",
        "voteCount": 1,
        "content": "I mean C should be correct .."
      },
      {
        "date": "2022-11-14T01:42:00.000Z",
        "voteCount": 2,
        "content": "B is ok. C&amp;D are wrong because they connect 1 to 2 and 2 to 3 , not 1 to3. 2 and 3 must be unreachable"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 121,
    "url": "https://www.examtopics.com/discussions/google/view/57270-exam-professional-cloud-architect-topic-1-question-121/",
    "body": "You need to deploy an application on Google Cloud that must run on a Debian Linux environment. The application requires extensive configuration in order to operate correctly. You want to ensure that you can install Debian distribution updates with minimal manual intervention whenever they become available. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Compute Engine instance template using the most recent Debian image. Create an instance from this template, and install and configure the application as part of the startup script. Repeat this process whenever a new Google-managed Debian image becomes available.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Debian-based Compute Engine instance, install and configure the application, and use OS patch management to install available updates.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an instance with the latest available Debian image. Connect to the instance via SSH, and install and configure the application on the instance. Repeat this process whenever a new Google-managed Debian image becomes available.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Docker container with Debian as the base image. Install and configure the application as part of the Docker image creation process. Host the container on Google Kubernetes Engine and restart the container whenever a new update is available."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 23,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-07-11T06:15:00.000Z",
        "voteCount": 24,
        "content": "B. Create a Debian-based Compute Engine instance, install and configure the application, and use OS patch management to install available updates."
      },
      {
        "date": "2021-07-07T21:57:00.000Z",
        "voteCount": 9,
        "content": "Answer is B"
      },
      {
        "date": "2024-06-08T04:34:00.000Z",
        "voteCount": 1,
        "content": "It is B, using Os path. \n\nIt is interesting the OS policy also, to run commands inside the VMs."
      },
      {
        "date": "2024-01-31T01:55:00.000Z",
        "voteCount": 2,
        "content": "The question requires MINIMAL manual intervention for patching Debian Linux.  Therefore it makes most sense to create a Docker container using the Debian Marketplace Distro  (as long as the FROM field in your Dockerfile points to `$distro:latest` from Cloud Marketplace)\nRef: \nhttps://cloud.google.com/blog/products/containers-kubernetes/exploring-container-security-let-google-do-the-patching-with-new-managed-base-images\nand\nhttps://cloud.google.com/blog/products/containers-kubernetes/exploring-container-security-how-containers-enable-passive-patching-and-a-better-model-for-supply-chain-security"
      },
      {
        "date": "2024-01-11T09:04:00.000Z",
        "voteCount": 1,
        "content": "You want to ensure that you can install Debian distribution updates with minimal manual intervention whenever they become available... | if it is in the cloud then option A | if it is on the internet then option b... It is a trick question!"
      },
      {
        "date": "2024-01-11T09:05:00.000Z",
        "voteCount": 1,
        "content": "I vote for A"
      },
      {
        "date": "2023-07-07T10:10:00.000Z",
        "voteCount": 3,
        "content": "Option A, \"Create a Compute Engine instance template using the most recent Debian image. Create an instance from this template, and install and configure the application as part of the startup script. Repeat this process whenever a new Google-managed Debian image becomes available,\" is the correct choice.\n\nThis approach allows you to automate the process of creating a new instance with the latest Debian image and configuring the application. By using an instance template and a startup script, you can ensure that the application is correctly configured each time a new instance is created. When a new Debian image becomes available, you can simply create a new instance template and repeat the process.\n\nOption B, using OS patch management, would allow you to install updates, but it wouldn't necessarily ensure that the application is correctly configured after an update."
      },
      {
        "date": "2023-07-02T19:35:00.000Z",
        "voteCount": 3,
        "content": "Can someone explain why the answer is not D ? Isn't the best practice to use containers ?"
      },
      {
        "date": "2024-07-13T13:51:00.000Z",
        "voteCount": 1,
        "content": "because restarting pod does not change the base image version of the application image. Hence D is a wrong answer"
      },
      {
        "date": "2023-08-19T05:58:00.000Z",
        "voteCount": 3,
        "content": "Because it mentions \"restart container whenever update is available\". Restarting doesn't just update the OS. You need to build docker image with new version"
      },
      {
        "date": "2023-10-12T05:09:00.000Z",
        "voteCount": 1,
        "content": "When you restart the container, will it not download the new image and reconfigure the application? I believe so, there is an option to configure the pods to download always the last image."
      },
      {
        "date": "2023-04-15T06:47:00.000Z",
        "voteCount": 2,
        "content": "A is correct, with template and startup script you can create multiple instance with minimal manual intervention; when the new debian release will be available, you need update only the template with new image of debian distribution."
      },
      {
        "date": "2023-01-04T23:16:00.000Z",
        "voteCount": 4,
        "content": "B is the correct answer,\n\nUse OS patch management to apply operating system patches across a set of Compute Engine VM instances (VMs). Long running VMs require periodic system updates to protect against defects and vulnerabilities.\n\nThe OS patch management service has two main components:\n\nPatch compliance reporting, which provides insights on the patch status of your VM instances across Windows and Linux distributions. Along with the insights, you can also view recommendations for your VM instances.\nPatch deployment, which automates the operating system and software patch update process. A patch deployment schedules patch jobs. A patch job runs across VM instances and applies patches.\n\nhttps://cloud.google.com/compute/docs/os-patch-management"
      },
      {
        "date": "2022-11-14T01:46:00.000Z",
        "voteCount": 2,
        "content": "B is ok"
      },
      {
        "date": "2022-10-17T06:25:00.000Z",
        "voteCount": 2,
        "content": "B is the simplest option and with minimal intervention. Other answers may be technically possible but the question does not ask for anything else (e.g. containers, templates, etc.)"
      },
      {
        "date": "2022-09-18T11:00:00.000Z",
        "voteCount": 1,
        "content": "I will go with B , there is no need to application configuration each time"
      },
      {
        "date": "2022-09-17T05:52:00.000Z",
        "voteCount": 2,
        "content": "B is the answer.\n\nhttps://cloud.google.com/compute/docs/os-patch-management\nUse OS patch management to apply operating system patches across a set of Compute Engine VM instances (VMs)."
      },
      {
        "date": "2022-09-10T14:37:00.000Z",
        "voteCount": 2,
        "content": "Key words : \"with minimal manual intervention whenever they become available\"\n\nWith OS Patch Management the OS will have all the latest available updates at that point automatically which wouldn't be the case with A."
      },
      {
        "date": "2022-08-11T06:21:00.000Z",
        "voteCount": 2,
        "content": "vote B\nabout A using latest template for update patch, next time you have to make template again.\nans B, make once setting for good basically with no intervention."
      },
      {
        "date": "2022-08-10T03:05:00.000Z",
        "voteCount": 1,
        "content": "A is correct answer.\nI am not able to understand B option and ppl are simply adding patch management URL as reference. How this will relate with answer ? \n\nA is correct answer."
      },
      {
        "date": "2022-10-16T04:03:00.000Z",
        "voteCount": 1,
        "content": "A only answers to 'minimal management overhead', but not to 'install patches as soon as possible'."
      },
      {
        "date": "2022-07-16T12:58:00.000Z",
        "voteCount": 2,
        "content": "The ask in this question is \"you want to ensure that you can install Debian distribution updates (which is OS updates specifically) with minimal manual intervention whenever they become available.\"  That is accomplished by an OS patch management. https://cloud.google.com/compute/docs/os-patch-management"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 122,
    "url": "https://www.examtopics.com/discussions/google/view/56425-exam-professional-cloud-architect-topic-1-question-122/",
    "body": "You have an application that runs in Google Kubernetes Engine (GKE). Over the last 2 weeks, customers have reported that a specific part of the application returns errors very frequently. You currently have no logging or monitoring solution enabled on your GKE cluster. You want to diagnose the problem, but you have not been able to replicate the issue. You want to cause minimal disruption to the application. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Update your GKE cluster to use Cloud Operations for GKE. 2. Use the GKE Monitoring dashboard to investigate logs from affected Pods.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a new GKE cluster with Cloud Operations for GKE enabled. 2. Migrate the affected Pods to the new cluster, and redirect traffic for those Pods to the new cluster. 3. Use the GKE Monitoring dashboard to investigate logs from affected Pods.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Update your GKE cluster to use Cloud Operations for GKE, and deploy Prometheus. 2. Set an alert to trigger whenever the application returns an error.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a new GKE cluster with Cloud Operations for GKE enabled, and deploy Prometheus. 2. Migrate the affected Pods to the new cluster, and redirect traffic for those Pods to the new cluster. 3. Set an alert to trigger whenever the application returns an error."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 38,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 20,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-07-01T01:24:00.000Z",
        "voteCount": 47,
        "content": "According to the reference, answer should be A.\nhttps://cloud.google.com/blog/products/management-tools/using-logging-your-apps-running-kubernetes-engine"
      },
      {
        "date": "2021-12-29T04:19:00.000Z",
        "voteCount": 5,
        "content": "But updating cluster requires downtime, isn't it?"
      },
      {
        "date": "2022-04-18T02:24:00.000Z",
        "voteCount": 5,
        "content": "No it actually does not require to shut down the cluster: https://cloud.google.com/stackdriver/docs/solutions/gke/installing#console_1"
      },
      {
        "date": "2022-10-16T04:07:00.000Z",
        "voteCount": 2,
        "content": "The problem in A) answer is that it is not alert-based. All recent trainings recommend use of alerts for troubleshooting, not dashboards."
      },
      {
        "date": "2024-01-16T22:21:00.000Z",
        "voteCount": 1,
        "content": "What about \"2. Use the GKE Monitoring dashboard to investigate logs from affected Pods\" then?\n\nI'd really like to learn how anyone can use \"Monitoring dashboard to investigate logs\".\nIt's just absurd."
      },
      {
        "date": "2024-03-14T04:25:00.000Z",
        "voteCount": 1,
        "content": "Monitoring console \u2013 In the Kubernetes Engine section of the Monitoring console, select the appropriate cluster, nodes, pod or containers to view the associated logs."
      },
      {
        "date": "2024-01-16T22:28:00.000Z",
        "voteCount": 1,
        "content": "Also, an \"alert\" is the keyword here.\nNo need to make anyone pressing a button every 5 minutes  or so (like in  In the TV show \"Lost,\" where not pushing the button in the Swan Station results in a catastrophic electromagnetic event. o-;"
      },
      {
        "date": "2021-07-28T14:03:00.000Z",
        "voteCount": 19,
        "content": "correct, from GCP best practices for GKE we should rely on native logging capabilities. No need for additional solutions like Prometheus. Also it is about reviewing logs, monitoring the service, not receiving alerts each time its happens, that will not provide any insight on the issue."
      },
      {
        "date": "2021-08-30T01:07:00.000Z",
        "voteCount": 6,
        "content": "Also, as long you know there is a problem, i think you should investigate immediately the issue, not wait for new errors"
      },
      {
        "date": "2021-06-30T10:19:00.000Z",
        "voteCount": 18,
        "content": "IMHO a) is the correct answer, not c)\nThe point is, that we have a scenario in that often errors in GKE happen - within 2 week a lot of people complained about a lot of errors. For the past we have no data at all as we have not monitored anything. That means we will collect data from now on to find out what the problem is. The additional value of an alert is not clear - and it for me not clear why we need additionally to install Prometheus considering that until now we had no GKE monitoring at all. Please correct me if I am wrong."
      },
      {
        "date": "2023-12-27T16:40:00.000Z",
        "voteCount": 1,
        "content": "Right, if we enable Cloud Operations we should be able to see the logs from this point onwards. Data of past errors would not be visible. It's not rational to expect developers to check every hour for appearances of the error in the logs, and that's where an alert comes in handy. It'll notify you when the conditions that led to the error appear again so that developers can analyze the logs and understand the problem.\nI agree that installing Prometheus is not needed today, but it seems that it was the only option at the time to set up alerts and, in my opinion, the alerts are vital to diagnose the problem."
      },
      {
        "date": "2024-07-13T13:54:00.000Z",
        "voteCount": 1,
        "content": "B &amp; D are asking to create new GKE cluster so we can ignore them\nFrom A and C , we can achieve the requirement with the help of A itself. Monitoring &amp; Logging should help investigate the issue."
      },
      {
        "date": "2024-04-24T05:47:00.000Z",
        "voteCount": 1,
        "content": "C\n1. \"You currently have no logging or monitoring solution enabled on your GKE cluster\" and \"you have not been able to replicate the issue\"- nothing interesting in GKE monitoring dashboard\n2. No alerting in answer A"
      },
      {
        "date": "2024-03-08T06:48:00.000Z",
        "voteCount": 1,
        "content": "Options A and C are less disruptive. Option C adds Prometheus on top which looks like overkills for this simple/initial level of troubleshooting. I would go with option A"
      },
      {
        "date": "2023-12-27T22:59:00.000Z",
        "voteCount": 1,
        "content": "A\n\nA provides native solution to GCLoud\nWhy not C?\nfrom GCP best practices for GKE we should rely on native logging capabilities. No need for additional solutions like Prometheus. Also it is about reviewing logs, monitoring the service, not receiving alerts each time its happens, that will not provide any insight on the issue. \nPrometheus could potentially help identify when the issue occurs, it doesn't directly help with diagnosing the root cause of the problem.\n\nB &amp; D rejected because migration will cause distruption. \n\nhttps://cloud.google.com/blog/products/management-tools/using-logging-your-apps-running-kubernetes-engine"
      },
      {
        "date": "2023-12-27T16:45:00.000Z",
        "voteCount": 2,
        "content": "If we enable Cloud Operations we should be able to see the logs from this point onwards. Data of past errors would not be visible. It's not rational to expect developers to check every hour for appearances of the error in the logs, and that's where an alert comes in handy. It'll notify you when the conditions that led to the error appear again so that developers can analyze the logs and understand the problem.\nI agree that installing Prometheus is not needed today, but it seems that it was the only option at the time they created this question to set up alerts and, in my opinion, the alerts are vital to diagnose the problem."
      },
      {
        "date": "2023-11-27T10:31:00.000Z",
        "voteCount": 1,
        "content": "Marked in Green is the real exam ans, or the community most voted one? I'm confused now hehe"
      },
      {
        "date": "2023-12-27T22:56:00.000Z",
        "voteCount": 1,
        "content": "Look for answers in discussion, read related documents as well. 95% times, most voted answer in discussions is corrrect, but reading google's doc is also necessary"
      },
      {
        "date": "2023-12-27T16:35:00.000Z",
        "voteCount": 1,
        "content": "As far as I know, it's the answer the uploader of the question set as correct. But it's not necesarily the correct one. The community voted one is more probably right."
      },
      {
        "date": "2023-11-14T03:09:00.000Z",
        "voteCount": 1,
        "content": "As per https://cloud.google.com/stackdriver/docs/managed-prometheus - Correct option, I feel is C."
      },
      {
        "date": "2023-10-23T08:51:00.000Z",
        "voteCount": 1,
        "content": "Answer is A"
      },
      {
        "date": "2023-10-12T05:18:00.000Z",
        "voteCount": 1,
        "content": "I think answer A is enough, but if you want a more complete solution C could be a good option: https://cloud.google.com/stackdriver/docs/managed-prometheus"
      },
      {
        "date": "2023-09-08T01:11:00.000Z",
        "voteCount": 2,
        "content": "Could anyone kindly explain why B is incorrect? Thank you."
      },
      {
        "date": "2023-04-12T18:54:00.000Z",
        "voteCount": 3,
        "content": "A. 1. Update your GKE cluster to use Cloud Operations for GKE. 2. Use the GKE Monitoring dashboard to investigate logs from affected Pods.\n\nBy updating your GKE cluster to use Cloud Operations for GKE (formerly known as Stackdriver), you enable monitoring and logging without disrupting the application. The GKE Monitoring dashboard allows you to investigate logs from affected Pods, which helps you diagnose the problem that customers have reported. This approach minimizes disruption to the application while providing the necessary information to identify and resolve the issue"
      },
      {
        "date": "2023-04-11T08:29:00.000Z",
        "voteCount": 1,
        "content": "As described here\nhttps://cloud.google.com/stackdriver/docs/solutions/gke\nis it possible to install prometheus as part of cloud operation suite."
      },
      {
        "date": "2022-12-25T10:00:00.000Z",
        "voteCount": 1,
        "content": "you want to minimize change so A is the best answer also you don't know what is the issue by reviewing the logs you can find something"
      },
      {
        "date": "2022-12-24T23:36:00.000Z",
        "voteCount": 3,
        "content": "The best option is D. 1. Create a new GKE cluster with Cloud Operations for GKE enabled, and deploy Prometheus. 2. Migrate the affected Pods to the new cluster, and redirect traffic for those Pods to the new cluster. 3. Set an alert to trigger whenever the application returns an error.\n\nHere's why:\n\nOption A does not involve creating a new GKE cluster, which means you will not be able to isolate the affected Pods from the rest of the application. This can make it difficult to diagnose the issue without disrupting the entire application."
      },
      {
        "date": "2022-12-24T23:37:00.000Z",
        "voteCount": 1,
        "content": "Option B involves deploying a new version of the application, which may or may not fix the issue. Additionally, this option does not address the root cause of the issue or provide a way to monitor the application for future errors.\n\nOption C involves deploying a new version of the application and setting an alert to trigger whenever the application returns an error. However, this option does not involve creating a new GKE cluster or migrating the affected Pods to a separate cluster, which means that the issue could continue to affect the entire application."
      },
      {
        "date": "2023-01-29T11:48:00.000Z",
        "voteCount": 2,
        "content": "You cannot automatically migrate pods from one cluster to another. You would have to manually deploy the workloads on the new cluster. And you have the problem of configuring the services in the new cluster. You will need to use new IP addresses for the services, modify the DNS to direct the client applications to the services from the new cluster. Very, very complicated. I would exclude the answer that propose creation of new GKE clusters."
      },
      {
        "date": "2023-05-09T03:03:00.000Z",
        "voteCount": 1,
        "content": "Thanks again for such a descriptive and well convinced explanation"
      },
      {
        "date": "2022-12-13T10:28:00.000Z",
        "voteCount": 2,
        "content": "Question states problem cannot be replicated. So alerting is required to review the right logs at right time. Hence A is not adequate solution and C is the right one"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 123,
    "url": "https://www.examtopics.com/discussions/google/view/56384-exam-professional-cloud-architect-topic-1-question-123/",
    "body": "You need to deploy a stateful workload on Google Cloud. The workload can scale horizontally, but each instance needs to read and write to the same POSIX filesystem. At high load, the stateful workload needs to support up to 100 MB/s of writes. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a persistent disk for each instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a regional persistent disk for each instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Cloud Filestore instance and mount it in each instance.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Cloud Storage bucket and mount it in each instance using gcsfuse."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 28,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-07-01T01:33:00.000Z",
        "voteCount": 36,
        "content": "Answer should be C,\nhttps://cloud.google.com/storage/docs/gcs-fuse#notes"
      },
      {
        "date": "2021-07-07T00:21:00.000Z",
        "voteCount": 5,
        "content": "Agreed - C"
      },
      {
        "date": "2021-12-16T13:22:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/filestore"
      },
      {
        "date": "2022-06-03T00:05:00.000Z",
        "voteCount": 4,
        "content": "\"\u201cCloud Storage FUSE is an open source&nbsp;[FUSE](http://fuse.sourceforge.net/)&nbsp;adapter that allows you to mount Cloud Storage buckets as file systems on Linux or macOS systems. It also provides a way for applications to upload and download Cloud Storage objects using standard file system semantics. Cloud Storage FUSE can be run anywhere with connectivity to Cloud Storage, including Google Compute Engine VMs or on-premises systems[**1**](https://cloud.google.com/storage/docs/gcs-fuse#f1-note).\" \n\nD says \"gcsfuse\", should be D"
      },
      {
        "date": "2022-10-25T19:30:00.000Z",
        "voteCount": 11,
        "content": "FUSE is not posix"
      },
      {
        "date": "2023-08-03T19:06:00.000Z",
        "voteCount": 8,
        "content": "directly from the documentation of gcs fuse &gt; While Cloud Storage FUSE has a file system interface, it is not like an NFS or CIFS file system on the backend. Additionally, Cloud Storage FUSE is not POSIX compliant. For a POSIX file system product in Google Cloud, see Filestore."
      },
      {
        "date": "2023-11-18T14:21:00.000Z",
        "voteCount": 3,
        "content": "Google Cloud Storage Fuse is not POSIX compliant so C"
      },
      {
        "date": "2023-11-18T08:08:00.000Z",
        "voteCount": 2,
        "content": "Not, you need a file system not a blob storage..."
      },
      {
        "date": "2023-12-28T11:47:00.000Z",
        "voteCount": 5,
        "content": "''While Cloud Storage FUSE has a file system interface, it is not like an NFS or CIFS file system on the backend. Additionally, Cloud Storage FUSE is not POSIX compliant. For a POSIX file system product in Google Cloud, see Filestore.''"
      },
      {
        "date": "2021-06-30T10:29:00.000Z",
        "voteCount": 15,
        "content": "IMHO d) is wrong, the correct answer is c).\nThe requirement is explicitly POSIX filesystem - using gcsfuse Cloud Storage still remains an object storage - IMHO gcsfuse brings a lot of downsizes compared with Filestore and in the question there are no indications that a non-POSIX filesystem shall be used."
      },
      {
        "date": "2023-06-29T20:30:00.000Z",
        "voteCount": 2,
        "content": "Additional google explicitly states that Cloud Storage fuse is not POSIX compliant\nhttps://cloud.google.com/storage/docs/gcs-fuse#differences-and-limitations"
      },
      {
        "date": "2023-06-29T20:31:00.000Z",
        "voteCount": 1,
        "content": "So the correct answer is C"
      },
      {
        "date": "2024-07-13T13:57:00.000Z",
        "voteCount": 1,
        "content": "Answer is C\nAs filestore can be attached to multiple pods/vm in R/W mode\n\nA &amp; B are wrong for the same reason, PD can be attached to single pod for write operation."
      },
      {
        "date": "2024-04-24T06:17:00.000Z",
        "voteCount": 1,
        "content": "C\n1. \"each Instance needs to read and wrlte to the same POSIX filesystem\"\n2. Cloud Storage is not POSIX compliant filesystem but Object Storage and gcsfuse only \"simulates\" file system\n3. See: https://cloud.google.com/storage/docs/gcs-fuse#differences-and-limitations\n \"Cloud Storage FUSE is not POSIX compliant. For a POSIX file system product in Google Cloud, see Filestore."
      },
      {
        "date": "2024-03-18T16:54:00.000Z",
        "voteCount": 2,
        "content": "From https://cloud.google.com/storage/docs/gcs-fuse#differences-and-limitations\nCloud Storage FUSE is not POSIX compliant. For a POSIX file system product in Google Cloud, see Filestore."
      },
      {
        "date": "2024-02-02T00:34:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/storage/docs/gcs-fuse#notes"
      },
      {
        "date": "2024-01-06T08:22:00.000Z",
        "voteCount": 1,
        "content": "C is correct, Per documentation ( Cloud Storage FUSE is not POSIX compliant. For a POSIX file system product in Google Cloud, see Filestore)"
      },
      {
        "date": "2023-12-28T05:55:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/storage/docs/gcs-fuse, not POSIX complieant."
      },
      {
        "date": "2023-12-27T23:05:00.000Z",
        "voteCount": 1,
        "content": "C\n\n\nExplanation:\nA &amp; B: persistent disk won't be shared. Question says \"each instance needs to read and write to the same POSIX filesystem.\" Although, now u can share persistent disk(https://cloud.google.com/compute/docs/disks/sharing-disks-between-vms#:~:text=Note%3A%20You%20can%20share%20Persistent%20Disk%20volumes%20only%20with%20VMs%20that%20are%20in%20the%20same%20zone%20as%20the%20disk.) but question doesnt mention that all VMs are in same zone as disk. \n\nC: Filestore ideal for NFS and POSIX\n\nD: shared access can be achieved using GCSFuse, still, it's not POSIX complaint (https://cloud.google.com/storage/docs/gcs-fuse#differences-and-limitations)"
      },
      {
        "date": "2023-11-29T15:44:00.000Z",
        "voteCount": 2,
        "content": "Yes, the correct answer is C:\nLIMITATIONS: \"While Cloud Storage FUSE has a file system interface, it is not like an NFS or CIFS file system on the backend. Additionally, Cloud Storage FUSE is not POSIX compliant. For a POSIX file system product in Google Cloud, see Filestore.\"\nhttps://cloud.google.com/storage/docs/gcs-fuse#differences-and-limitations"
      },
      {
        "date": "2023-11-18T08:09:00.000Z",
        "voteCount": 2,
        "content": "You need a file system not a blob storage..."
      },
      {
        "date": "2023-11-14T03:20:00.000Z",
        "voteCount": 1,
        "content": "As per the documentation, https://cloud.google.com/storage/docs/gcs-fuse#differences-and-limitations. The option is C."
      },
      {
        "date": "2023-08-09T21:51:00.000Z",
        "voteCount": 1,
        "content": "firestore is POSIX"
      },
      {
        "date": "2023-06-17T10:33:00.000Z",
        "voteCount": 1,
        "content": "Could anyone please tell why A or B is wrong? Thanks"
      },
      {
        "date": "2023-04-25T14:42:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/storage/docs/gcs-fuse#differences-and-limitations \nAnswer C because POSIX and FUSE are not compatible. Google recommends using Filestore to address POSIX file operations"
      },
      {
        "date": "2023-04-03T10:48:00.000Z",
        "voteCount": 2,
        "content": "Answer is C\nhttps://cloud.google.com/storage/docs/gcs-fuse\nClearly mentioned to see filestore for POSIX"
      },
      {
        "date": "2022-11-30T21:38:00.000Z",
        "voteCount": 4,
        "content": "GCFUSE does not work with POSIX File System , refer - https://cloud.google.com/storage/docs/gcs-fuse\n\nSo Answer should be C"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 124,
    "url": "https://www.examtopics.com/discussions/google/view/57009-exam-professional-cloud-architect-topic-1-question-124/",
    "body": "Your company has an application deployed on Anthos clusters (formerly Anthos GKE) that is running multiple microservices. The cluster has both Anthos Service<br>Mesh and Anthos Config Management configured. End users inform you that the application is responding very slowly. You want to identify the microservice that is causing the delay. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Service Mesh visualization in the Cloud Console to inspect the telemetry between the microservices.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Anthos Config Management to create a ClusterSelector selecting the relevant cluster. On the Google Cloud Console page for Google Kubernetes Engine, view the Workloads and filter on the cluster. Inspect the configurations of the filtered workloads.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Anthos Config Management to create a namespaceSelector selecting the relevant cluster namespace. On the Google Cloud Console page for Google Kubernetes Engine, visit the workloads and filter on the namespace. Inspect the configurations of the filtered workloads.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReinstall istio using the default istio profile in order to collect request latency. Evaluate the telemetry between the microservices in the Cloud Console."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-08-04T22:53:00.000Z",
        "voteCount": 21,
        "content": "The Anthos Service Mesh pages in the Google Cloud Console provide both summary and in-depth metrics, charts, and graphs that enable you to observe service behavior. You can monitor the overall health of your services, or drill down on a specific service to set a service level objective (SLO) or troubleshoot an issue.\n\nhttps://cloud.google.com/service-mesh/docs/observability/explore-dashboard"
      },
      {
        "date": "2021-07-08T02:05:00.000Z",
        "voteCount": 13,
        "content": "Answer is A"
      },
      {
        "date": "2024-07-13T13:58:00.000Z",
        "voteCount": 1,
        "content": "Right answer is A\nService Mesh helps in monitoring at single place."
      },
      {
        "date": "2023-11-18T08:12:00.000Z",
        "voteCount": 3,
        "content": "Anthos Service Mesh Visualization\nhttps://cloud.google.com/service-mesh/docs/observability-overview"
      },
      {
        "date": "2022-11-14T01:57:00.000Z",
        "voteCount": 1,
        "content": "A is ok"
      },
      {
        "date": "2022-08-13T20:12:00.000Z",
        "voteCount": 3,
        "content": "I got this question in exam."
      },
      {
        "date": "2022-11-07T07:01:00.000Z",
        "voteCount": 2,
        "content": "what is the answer"
      },
      {
        "date": "2022-07-03T08:11:00.000Z",
        "voteCount": 1,
        "content": "A is right."
      },
      {
        "date": "2022-01-19T09:20:00.000Z",
        "voteCount": 5,
        "content": "Got this question in my exam, answered A"
      },
      {
        "date": "2023-03-09T19:25:00.000Z",
        "voteCount": 1,
        "content": "How many question from this entire question bank were common?"
      },
      {
        "date": "2023-05-09T03:30:00.000Z",
        "voteCount": 1,
        "content": "Thanks dear"
      },
      {
        "date": "2021-12-09T08:55:00.000Z",
        "voteCount": 1,
        "content": "Go for A"
      },
      {
        "date": "2021-12-04T03:38:00.000Z",
        "voteCount": 1,
        "content": "A is the correct answer"
      },
      {
        "date": "2021-11-25T06:34:00.000Z",
        "voteCount": 4,
        "content": "vote A"
      },
      {
        "date": "2021-08-04T22:57:00.000Z",
        "voteCount": 6,
        "content": "Ans-A\nhttps://cloud.google.com/service-mesh/docs/observability/explore-dashboard"
      },
      {
        "date": "2021-07-11T10:07:00.000Z",
        "voteCount": 9,
        "content": "Ans : A\nAnthos Service Mesh\u2019s robust tracing, monitoring, and logging features give you deep insights into how your services are performing, how that performance affects other processes, and any issues that might exist."
      },
      {
        "date": "2021-07-11T07:06:00.000Z",
        "voteCount": 5,
        "content": "A. Use the Service Mesh visualization in the Cloud Console to inspect the telemetry between the microservices."
      },
      {
        "date": "2021-07-03T04:54:00.000Z",
        "voteCount": 3,
        "content": "Answer: A, Service Mesh\nhttps://cloud.google.com/anthos/service-mesh"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 125,
    "url": "https://www.examtopics.com/discussions/google/view/57007-exam-professional-cloud-architect-topic-1-question-125/",
    "body": "You are working at a financial institution that stores mortgage loan approval documents on Cloud Storage. Any change to these approval documents must be uploaded as a separate approval file, so you want to ensure that these documents cannot be deleted or overwritten for the next 5 years. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a retention policy on the bucket for the duration of 5 years. Create a lock on the retention policy.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate the bucket with uniform bucket-level access, and grant a service account the role of Object Writer. Use the service account to upload new files.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a customer-managed key for the encryption of the bucket. Rotate the key after 5 years.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate the bucket with fine-grained access control, and grant a service account the role of Object Writer. Use the service account to upload new files."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-07-11T10:18:00.000Z",
        "voteCount": 31,
        "content": "Answer A\no\tIf a bucket has a retention policy, objects in the bucket can only be deleted or replaced once their age is greater than the retention period.\no\tOnce you lock a retention policy, you cannot remove it or reduce the retention period it has."
      },
      {
        "date": "2022-02-15T14:18:00.000Z",
        "voteCount": 7,
        "content": "2/15/21 exam"
      },
      {
        "date": "2024-07-13T14:01:00.000Z",
        "voteCount": 1,
        "content": "A is correct because retention policy on bucket with lock , make sure no one can delete the object until it expires."
      },
      {
        "date": "2023-12-27T23:10:00.000Z",
        "voteCount": 1,
        "content": "A\nhttps://cloud.google.com/storage/docs/bucket-lock#policy-locks"
      },
      {
        "date": "2023-01-05T17:48:00.000Z",
        "voteCount": 4,
        "content": "A is the correct answer,\n\nYou can include a retention policy when creating a new bucket, or you can add a retention policy to an existing bucket. Placing a retention policy on a bucket ensures that all current and future objects in the bucket cannot be deleted or replaced until they reach the age you define in the retention policy. Attempts to delete or replace objects whose age is less than the retention period fail with a 403 - retentionPolicyNotMet error.\n\nhttps://cloud.google.com/storage/docs/bucket-lock#retention-policy"
      },
      {
        "date": "2022-12-28T11:24:00.000Z",
        "voteCount": 2,
        "content": "It appeared in 12/12/22 Exam"
      },
      {
        "date": "2023-03-09T19:28:00.000Z",
        "voteCount": 1,
        "content": "How many question came from this question bank?"
      },
      {
        "date": "2022-11-14T02:01:00.000Z",
        "voteCount": 1,
        "content": "A is ok"
      },
      {
        "date": "2022-08-13T20:13:00.000Z",
        "voteCount": 2,
        "content": "I got this question in exam."
      },
      {
        "date": "2022-07-08T10:51:00.000Z",
        "voteCount": 1,
        "content": "A is the only applicable answer"
      },
      {
        "date": "2022-07-03T08:13:00.000Z",
        "voteCount": 1,
        "content": "A is right"
      },
      {
        "date": "2022-06-20T02:39:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2022-03-05T14:32:00.000Z",
        "voteCount": 1,
        "content": "A is correct. Create retention policy"
      },
      {
        "date": "2022-01-19T09:21:00.000Z",
        "voteCount": 3,
        "content": "Got this question in my exam, answered A"
      },
      {
        "date": "2023-05-14T02:17:00.000Z",
        "voteCount": 1,
        "content": "Thanks dear!"
      },
      {
        "date": "2021-12-09T09:03:00.000Z",
        "voteCount": 1,
        "content": "Go for A"
      },
      {
        "date": "2021-12-04T04:04:00.000Z",
        "voteCount": 1,
        "content": "A is the correct answer"
      },
      {
        "date": "2021-11-25T16:27:00.000Z",
        "voteCount": 3,
        "content": "vote A"
      },
      {
        "date": "2021-10-06T01:52:00.000Z",
        "voteCount": 1,
        "content": "I agree with A"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 126,
    "url": "https://www.examtopics.com/discussions/google/view/56702-exam-professional-cloud-architect-topic-1-question-126/",
    "body": "Your team will start developing a new application using microservices architecture on Kubernetes Engine. As part of the development lifecycle, any code change that has been pushed to the remote develop branch on your GitHub repository should be built and tested automatically. When the build and test are successful, the relevant microservice will be deployed automatically in the development environment. You want to ensure that all code deployed in the development environment follows this process. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tHave each developer install a pre-commit hook on their workstation that tests the code and builds the container when committing on the development branch. After a successful commit, have the developer deploy the newly built container image on the development cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall a post-commit hook on the remote git repository that tests the code and builds the container when code is pushed to the development branch. After a successful commit, have the developer deploy the newly built container image on the development cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Cloud Build trigger based on the development branch that tests the code, builds the container, and stores it in Container Registry. Create a deployment pipeline that watches for new images and deploys the new image on the development cluster. Ensure only the deployment tool has access to deploy new versions.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Cloud Build trigger based on the development branch to build a new container image and store it in Container Registry. Rely on Vulnerability Scanning to ensure the code tests succeed. As the final step of the Cloud Build process, deploy the new container image on the development cluster. Ensure only Cloud Build has access to deploy new versions."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 32,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-07-01T01:45:00.000Z",
        "voteCount": 46,
        "content": "Answer should be C, obviously."
      },
      {
        "date": "2021-07-16T21:24:00.000Z",
        "voteCount": 41,
        "content": "Questions say \"relevant microservice will be deployed automatically in the development environment.\" Therefore A and B are out. D says \"Rely on Vulnerability Scanning to ensure the code tests succeed.\" Vulnerability Scanning is not test so D is out. The correct Answer is therefore C."
      },
      {
        "date": "2024-06-07T07:56:00.000Z",
        "voteCount": 2,
        "content": "Automatic, no developer running stuff, plus relying in pre-commit hooks alone is not a CD strategy. The C over D is because the pipeline in cloudbuild should run the tests, not the Vulnerability Scanner. While vulnerability scanner is useful, it's not required in this context, and there're no tests run in answer D."
      },
      {
        "date": "2024-04-24T08:36:00.000Z",
        "voteCount": 1,
        "content": "Automatic deployment required in the question and manual deployment by developer in answer marked as correct !\nAnce again....Who is responsible for marking answers as correct on examtopics platform ? Enyone from examtopics read responses from community and correct wrong answers ???\nA few days after purchasing full access to this platform, I am disgusted :("
      },
      {
        "date": "2024-06-08T04:20:00.000Z",
        "voteCount": 1,
        "content": "Do not be angry. That the answers are incorrect is on purpose. First so that the pdf is not shared and they are left without business. And secondly so that Google does not change the questions because then there are no correct answers filtered."
      },
      {
        "date": "2024-02-14T13:27:00.000Z",
        "voteCount": 1,
        "content": "C but.... \"build container and add to the registry\" ????  Not container but image."
      },
      {
        "date": "2024-01-11T04:59:00.000Z",
        "voteCount": 1,
        "content": "Nah, no way to be A."
      },
      {
        "date": "2024-01-03T13:31:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is C."
      },
      {
        "date": "2023-10-06T08:04:00.000Z",
        "voteCount": 1,
        "content": "Agreed with omermahgoub\nAnswer should be C."
      },
      {
        "date": "2023-03-31T19:04:00.000Z",
        "voteCount": 1,
        "content": "By elimination method, the answer is C."
      },
      {
        "date": "2023-03-27T01:50:00.000Z",
        "voteCount": 1,
        "content": "The answer is C."
      },
      {
        "date": "2023-01-29T02:27:00.000Z",
        "voteCount": 1,
        "content": "Clearly C"
      },
      {
        "date": "2022-12-25T02:08:00.000Z",
        "voteCount": 8,
        "content": "The correct answer is C: Create a Cloud Build trigger based on the development branch that tests the code, builds the container, and stores it in Container Registry. Create a deployment pipeline that watches for new images and deploys the new image on the development cluster. Ensure only the deployment tool has access to deploy new versions.\n\nTo automate the build and deployment process for your microservices in the development environment, you can use Cloud Build to set up a trigger that listens for code pushes to the development branch on your GitHub repository. When a code change is pushed to the branch, Cloud Build can test the code, build the container image, and store it in Container Registry. You can then create a deployment pipeline that watches for new images in Container Registry and deploys them automatically on the development cluster. To ensure that only code that has been properly tested and built is deployed in the development environment, you should ensure that only the deployment tool has access to deploy new versions."
      },
      {
        "date": "2022-12-25T02:08:00.000Z",
        "voteCount": 3,
        "content": "Option A is incorrect because installing a pre-commit hook on each developer's workstation does not ensure that the build and test process is followed consistently for all code changes. It also does not provide a centralized way to track the deployments in the development environment.\n\nOption B is incorrect for the same reason. A post-commit hook on the remote repository does not provide a centralized way to manage the build and deployment process for all code changes in the development environment.\n\nOption D is incorrect because relying on Vulnerability Scanning alone is not sufficient to ensure that the code changes are properly tested and built before being deployed in the development environment. A more comprehensive build and test process, such as the one described in option C, is recommended to ensure the quality and reliability of the code being deployed."
      },
      {
        "date": "2022-12-21T22:09:00.000Z",
        "voteCount": 1,
        "content": "\"any code change that has been pushed to the remote develop branch on your GitHub repository should be built\"...this excludes A and B since both happen locally before a push.\nAnswer 'D' only performs security scanning (no test) and is not automatically deployed which is what was requested."
      },
      {
        "date": "2022-11-14T02:04:00.000Z",
        "voteCount": 1,
        "content": "C is ok"
      },
      {
        "date": "2022-10-17T07:10:00.000Z",
        "voteCount": 1,
        "content": "C. Create a Cloud Build trigger based on the development branch that tests the code, builds the container, and stores it in Container Registry. Create a deployment pipeline that watches for new images and deploys the new image on the development cluster. Ensure only the deployment tool has access to deploy new versions."
      },
      {
        "date": "2022-09-15T13:15:00.000Z",
        "voteCount": 1,
        "content": "C doesn't include test success. D should be the option."
      },
      {
        "date": "2022-10-17T22:02:00.000Z",
        "voteCount": 1,
        "content": "It does include tests succuss, the question is using CI/CD"
      },
      {
        "date": "2022-09-17T20:56:00.000Z",
        "voteCount": 1,
        "content": "Vulnerability Scanning is not relevant though for the question."
      },
      {
        "date": "2022-08-02T02:14:00.000Z",
        "voteCount": 2,
        "content": "C- the only answer"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 127,
    "url": "https://www.examtopics.com/discussions/google/view/56603-exam-professional-cloud-architect-topic-1-question-127/",
    "body": "Your operations team has asked you to help diagnose a performance issue in a production application that runs on Compute Engine. The application is dropping requests that reach it when under heavy load. The process list for affected instances shows a single application process that is consuming all available CPU, and autoscaling has reached the upper limit of instances. There is no abnormal load on any other related systems, including the database. You want to allow production traffic to be served again as quickly as possible. Which action should you recommend?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the autoscaling metric to agent.googleapis.com/memory/percent_used.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRestart the affected instances on a staggered schedule.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSSH to each instance and restart the application process.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the maximum number of instances in the autoscaling group.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 42,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-07-01T02:07:00.000Z",
        "voteCount": 44,
        "content": "Answer should be D.\nI doubt it is intended to provide wrong answer."
      },
      {
        "date": "2021-08-30T01:15:00.000Z",
        "voteCount": 15,
        "content": "why almost all answers are wrong?"
      },
      {
        "date": "2022-10-17T22:05:00.000Z",
        "voteCount": 10,
        "content": "to prevent us from memorizing the answers and hopefully, the site can not be shut down"
      },
      {
        "date": "2023-12-19T20:01:00.000Z",
        "voteCount": 2,
        "content": "Yes I always consider this to be the reason behind so many obviously wrong answers too... but who knows"
      },
      {
        "date": "2021-07-28T14:28:00.000Z",
        "voteCount": 9,
        "content": "Agree. \n\nCannot be A), since changing the metric used for autoscaling will not solve the issue, the CPU is already over utilized, hence the unique \"workaround\" meanwhile the application causing the issue is fixed (connection leaks, infinite loops, etc.) is to allow introducing new nodes/workers/VMs."
      },
      {
        "date": "2021-07-08T02:13:00.000Z",
        "voteCount": 12,
        "content": "Answer is D"
      },
      {
        "date": "2024-01-03T13:45:00.000Z",
        "voteCount": 3,
        "content": "The question is not asking for a permanent solution to the problem, it is asking what to do to have the production traffic to be served again as quickly as possible.  Therefore, the best answer is D."
      },
      {
        "date": "2023-11-18T08:21:00.000Z",
        "voteCount": 2,
        "content": "Answer D is correct.\nPrioritize the availability on production environment."
      },
      {
        "date": "2023-08-24T13:53:00.000Z",
        "voteCount": 5,
        "content": "it says \"autoscaling has reached the upper limit of instances\" and there are no abnormal errors... so the upper limit for autoscaling has to be increased."
      },
      {
        "date": "2023-07-17T04:18:00.000Z",
        "voteCount": 1,
        "content": "Increase the maximum number of instances in the autoscaling group"
      },
      {
        "date": "2023-06-10T07:22:00.000Z",
        "voteCount": 1,
        "content": "Answer D is correct.\nIn order to prioritize the availability on production environment (as per question), first we need to increase the number of max instances in the instance group, then, for sure we can investigate and restart application process.\nBe careful, often the answer is in the question"
      },
      {
        "date": "2023-04-02T11:40:00.000Z",
        "voteCount": 7,
        "content": "I choose for A, but D is the best choice. \n\nThe trick is: \"process that is consuming all available CPU\" and \"autoscaling has reached the upper limit of instances\"\nIf the process is consuming all available CPU, we need to reconfigure our metrics for best tresholds (Option A)\nAND\nif the autoscaling reached the upper limit of instances, so we need to increase this limit (Option D), \n\n\nBUT, after  reached the upper limit of instances, it doesn't matter the tresholds, the process will consume all resources that have available. So, option D is the best option."
      },
      {
        "date": "2023-05-09T05:24:00.000Z",
        "voteCount": 2,
        "content": "Loved the way you made us travel through the roots of the question"
      },
      {
        "date": "2023-03-09T04:49:00.000Z",
        "voteCount": 6,
        "content": "The application is dropping requests because the available CPU is exhausted. Autoscaling has reached the upper limit of instances, so it cannot increase the number of instances to meet the demand. The best way to allow production traffic to be served again is to increase the maximum number of instances in the autoscaling group.\n\nThis will allow autoscaling to increase the number of instances to meet the demand without exhausting the available CPU. Restarting the affected instances or SSHing to each instance and restarting the application process will not solve the problem because the root cause is that there are not enough instances to meet the demand."
      },
      {
        "date": "2022-12-25T02:10:00.000Z",
        "voteCount": 7,
        "content": "The correct answer is D: Increase the maximum number of instances in the autoscaling group.\n\nIf the application is dropping requests under heavy load and the process list for affected instances shows a single application process consuming all available CPU, increasing the maximum number of instances in the autoscaling group may help to alleviate the performance issue. By adding more instances to the group, you can distribute the load across multiple instances, which should help to reduce the strain on any single instance. This will allow production traffic to be served again more quickly."
      },
      {
        "date": "2022-12-25T02:10:00.000Z",
        "voteCount": 4,
        "content": "Option A is incorrect because changing the autoscaling metric to agent.googleapis.com/memory/percent_used will not address the root cause of the performance issue. The issue is related to CPU utilization, not memory usage.\n\nOption B is incorrect because restarting the affected instances on a staggered schedule will not address the root cause of the performance issue. It may provide temporary relief, but the issue is likely to recur once the instances are under heavy load again.\n\nOption C is incorrect because restarting the application process on each instance will not address the root cause of the performance issue. It may provide temporary relief, but the issue is likely to recur once the instances are under heavy load again. Increasing the maximum number of instances in the autoscaling group is a more effective solution in this case."
      },
      {
        "date": "2022-12-14T18:21:00.000Z",
        "voteCount": 1,
        "content": "Given the is not no abnormal load, autoscaling will is not required, restarting should kill the single application process consuming excess CPU"
      },
      {
        "date": "2022-12-14T02:39:00.000Z",
        "voteCount": 1,
        "content": "D is the correct answer"
      },
      {
        "date": "2022-12-06T13:56:00.000Z",
        "voteCount": 2,
        "content": "The keyword is \"You want to allow production traffic to be served again as quickly as possible\" so D should be the only answer so as to resume production traffic and then you can do a root cause analysis and take further action depending upon the findings."
      },
      {
        "date": "2022-11-14T02:10:00.000Z",
        "voteCount": 1,
        "content": "D is ok"
      },
      {
        "date": "2022-10-17T07:21:00.000Z",
        "voteCount": 3,
        "content": "It all depends on how you want to troubleshoot the issue. Do you want to check the application before or after increasing the max number of instances in the scaling group. I guess in real life people will ask for an increase in the max number of instances and if the application process continues to consume all the CPU then they will probably stop/restart the app. \nD is the only sensible option.\nA is not an option\nB you could restart but you dont know if that will fix the issue\nC SSH assumes unix vm's (?)....!"
      },
      {
        "date": "2022-10-18T20:16:00.000Z",
        "voteCount": 2,
        "content": "autoscaling has reached the upper limit of instances. There is no abnormal load on any other related systems, including the database. This so junk question, only D seems viable option."
      },
      {
        "date": "2022-09-06T21:32:00.000Z",
        "voteCount": 3,
        "content": "I feel increasing the autoscale limit seems to be the logical answer"
      },
      {
        "date": "2022-08-13T09:41:00.000Z",
        "voteCount": 1,
        "content": "D seems to be least wrong"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 128,
    "url": "https://www.examtopics.com/discussions/google/view/56612-exam-professional-cloud-architect-topic-1-question-128/",
    "body": "You are implementing the infrastructure for a web service on Google Cloud. The web service needs to receive and store the data from 500,000 requests per second. The data will be queried later in real time, based on exact matches of a known set of attributes. There will be periods where the web service will not receive any requests. The business wants to keep costs low. Which web service platform and database should you use for the application?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Run and BigQuery",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Run and Cloud Bigtable\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA Compute Engine autoscaling managed instance group and BigQuery",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA Compute Engine autoscaling managed instance group and Cloud Bigtable"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 62,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 36,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-07-01T01:45:00.000Z",
        "voteCount": 78,
        "content": "Any correct answer must involve Cloud Bigtable over BigQuery since Bigtable is optimized for heavy write loads. That leaves B and D. I would suggest B b/c it is lower cost (\"The business wants to keep costs low\")"
      },
      {
        "date": "2022-05-26T19:46:00.000Z",
        "voteCount": 4,
        "content": "B. Agree. Additionally data need to store now so use Bigtable as question is not for analysing or data Analytics  etc"
      },
      {
        "date": "2022-03-14T02:15:00.000Z",
        "voteCount": 2,
        "content": "the correct is B"
      },
      {
        "date": "2021-12-15T02:30:00.000Z",
        "voteCount": 21,
        "content": "Not only: occasionally there will be no requests. so Cloud Run will scale to zero"
      },
      {
        "date": "2023-05-30T00:36:00.000Z",
        "voteCount": 3,
        "content": "Plus, we are talking about a predefined set of queries. For any predefined list of (simple) queries, we use Bigtable, and for any (complex) queries that we do not know ahead of time, we use BigQuery."
      },
      {
        "date": "2024-01-02T12:18:00.000Z",
        "voteCount": 1,
        "content": "But cloud run can't support 50,000 request per second. Even cloud run 2nd gen supports 1000 requests per second. B is eliminated."
      },
      {
        "date": "2024-01-24T20:33:00.000Z",
        "voteCount": 2,
        "content": "That's incorrect. https://cloud.google.com/run/quotas"
      },
      {
        "date": "2021-07-06T21:56:00.000Z",
        "voteCount": 16,
        "content": "B is correct answer."
      },
      {
        "date": "2024-08-22T08:54:00.000Z",
        "voteCount": 1,
        "content": "Cloud Run and Cloud Bigtable is the best choice because it meets all the requirements:\n\nCloud Run can scale automatically to handle 500,000 requests per second and scales to zero during periods of no requests, reducing costs. Cloud Bigtable is designed for real-time queries with exact match attributes. \n\nAutoscaling Managed Instance Group can not scale to zero during periods of no requests*"
      },
      {
        "date": "2024-07-13T14:15:00.000Z",
        "voteCount": 1,
        "content": "It's hard for Cloud Run to scale to accept 500k rps so choosing Option D"
      },
      {
        "date": "2024-03-08T07:57:00.000Z",
        "voteCount": 1,
        "content": "Not sure why this is voted between B and D. It should be A. \nMIG wont support, that rules out C and D.\nbetween BQ and BT, please see that \"data will be queried later in real time, based on exact matches of a known set of attributes\". This is supported by BQ alone. So I would go with A."
      },
      {
        "date": "2024-02-20T04:20:00.000Z",
        "voteCount": 2,
        "content": "B is correct"
      },
      {
        "date": "2024-01-24T20:33:00.000Z",
        "voteCount": 4,
        "content": "https://cloud.google.com/run/quotas\n\nThere is no direct limit for:\n    The size of container images you can deploy.\n    The number of concurrent requests served by a Cloud Run service."
      },
      {
        "date": "2024-01-15T20:24:00.000Z",
        "voteCount": 2,
        "content": "Cloud Run can handle this amount if there were like 500 instances which would cost a pretty ridiculous amount per minute, so unfortunately there isnt enough information in this question around how long the gaps are without data to make a proper decision.\n\nAutoscaling Managed Instance Groups can scale to zero and 500k per second would be relatively easily handled by a few instances."
      },
      {
        "date": "2024-01-11T05:16:00.000Z",
        "voteCount": 4,
        "content": "50,000 rps\nAt first I thought Cloud Run could not handle this request rate and then chose D. After a little bit of research on the docs I changed my mind to B. \n\nOn each instance concurrency, it clearly says\n\n&gt; By default each Cloud Run instance can receive up to 80 requests at the same time; you can increase this to a maximum of 1000\n\nhttps://cloud.google.com/run/docs/about-concurrency\n\nThe maximum number of auto-scaling instances by default is 100, which can be configured depending on the regional quota. With the default max instances it can already handle 100 * 1000 = 100,000 requests concurrently, which should be able to achieve the 50,000 rps requirement.\n\nhttps://cloud.google.com/run/docs/about-instance-autoscaling"
      },
      {
        "date": "2024-07-13T14:14:00.000Z",
        "voteCount": 1,
        "content": "question says it's 500k and not 50k rps"
      },
      {
        "date": "2024-01-02T12:20:00.000Z",
        "voteCount": 1,
        "content": "Cloud Run can't support 50,000 requests per second.\n\nCorrect answer should be D."
      },
      {
        "date": "2024-01-02T12:15:00.000Z",
        "voteCount": 1,
        "content": "Cloud Run can't handle 50,000 requests per second\nA &amp; B is eliminated"
      },
      {
        "date": "2023-12-30T10:50:00.000Z",
        "voteCount": 1,
        "content": "Not receive any request = Cloud Run"
      },
      {
        "date": "2023-12-23T02:50:00.000Z",
        "voteCount": 2,
        "content": "At first I through its B, but then I thought about the number of requests that will be over 1 minute, if we calculated it = 30 million request per minute, and based on cloud run pricing this will cost only for the number of requests: 24 USD. so cloud run will cost the company 24 USD / min. which might be a very costly option.   \nBut in the cloud run pricing there is 2 modes: \n- CPU allocated when receiving requests: and there is a cost for CPU and requests\n- CPU always allocated: and there is only a cost for the CPU and zero price for the number of requests. \n\nI think we need someone experiencing the billing of a cloud run under a heavy load like this :)"
      },
      {
        "date": "2023-12-13T06:02:00.000Z",
        "voteCount": 1,
        "content": "Apart from the reason that cloud run can scale to zero, another benefit in this scenario is the fact Cloud run will provide out of the box revision maintenance for the web service."
      },
      {
        "date": "2023-11-18T08:30:00.000Z",
        "voteCount": 1,
        "content": "Compute:\nCloud Run vs. Compute Engine autoscaling managed instance group\nCloud Run wins because can scale down up to 0 instances -&gt; in Spike workflows will be cheaper.\nStorage:\nBigQuery vs. Big Table.\n500,000 requests per second it\u2019s not suitable in BQ:\nhttps://cloud.google.com/bigquery/quotas\n\u201cA user can make up to 100 API requests per second to an API method\u201d\nSo answer most be B."
      },
      {
        "date": "2023-11-11T07:58:00.000Z",
        "voteCount": 1,
        "content": "MIGs cannot scale the VMs to 0 as per https://cloud.google.com/compute/docs/autoscaler/scaling-cloud-monitoring-metrics#configure_utilization_target\nSo B is the answer."
      },
      {
        "date": "2023-10-11T18:52:00.000Z",
        "voteCount": 1,
        "content": "Cloud Functions can also scale to 0. But I guess because it manageable scaling can be done faster on functions level"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 129,
    "url": "https://www.examtopics.com/discussions/google/view/57424-exam-professional-cloud-architect-topic-1-question-129/",
    "body": "You are developing an application using different microservices that should remain internal to the cluster. You want to be able to configure each microservice with a specific number of replicas. You also want to be able to address a specific microservice from any other microservice in a uniform way, regardless of the number of replicas the microservice scales to. You need to implement this solution on Google Kubernetes Engine. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy each microservice as a Deployment. Expose the Deployment in the cluster using a Service, and use the Service DNS name to address it from other microservices within the cluster.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy each microservice as a Deployment. Expose the Deployment in the cluster using an Ingress, and use the Ingress IP address to address the Deployment from other microservices within the cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy each microservice as a Pod. Expose the Pod in the cluster using a Service, and use the Service DNS name to address the microservice from other microservices within the cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy each microservice as a Pod. Expose the Pod in the cluster using an Ingress, and use the Ingress IP address name to address the Pod from other microservices within the cluster."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 19,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-07-08T02:15:00.000Z",
        "voteCount": 29,
        "content": "Answer is A"
      },
      {
        "date": "2021-08-05T00:15:00.000Z",
        "voteCount": 18,
        "content": "Answer is A 100%\nB is incorrect. Ingress comes with a HTTP(S) LB with external IP hence is not needed for communications within the cluster internally."
      },
      {
        "date": "2024-01-03T20:49:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is A"
      },
      {
        "date": "2023-08-29T05:52:00.000Z",
        "voteCount": 1,
        "content": "A is correct answer"
      },
      {
        "date": "2023-07-27T07:06:00.000Z",
        "voteCount": 8,
        "content": "each microservice with a specific number of replicas = Deployment\ninternal to the cluster = Service"
      },
      {
        "date": "2023-01-09T04:42:00.000Z",
        "voteCount": 2,
        "content": "answer is A"
      },
      {
        "date": "2022-12-29T10:05:00.000Z",
        "voteCount": 7,
        "content": "Benefit of using Service\nLeveraging service allows for you to set up your environment with static IP addresses. So when your pods die and restart the IP address associated with the deceased pod remains\nfor the new pod that replaces it (ephemeral). I think using \"Service\" is helpful if you are setting up your pods to be able to communicate with specific pods in the cluster.\n\nBenefit of using DNS for Service\nUsing DNS for your Service (static IP) you can look up Services and/or Pods by name instead of IP. Addressability by name instead of IP is easier for me."
      },
      {
        "date": "2022-12-29T10:04:00.000Z",
        "voteCount": 3,
        "content": "l\nl\nBenefit of using Service\nLeveraging service allows for you to set up your environment with static IP addresses. So when your pods die and restart the IP address associated with the deceased pod remains \nfor the new pod that replaces it (ephemeral). I think using \"Service\" is helpful if you are setting up your pods to be able to communicate with specific pods in the cluster. \n\nBenefit of using DNS for Service\nUsing DNS for your Service (static IP) you can look up Services and/or Pods by name instead of IP. Addressability by name instead of IP is easier for me."
      },
      {
        "date": "2022-12-10T15:57:00.000Z",
        "voteCount": 1,
        "content": "A is ok"
      },
      {
        "date": "2022-11-15T23:45:00.000Z",
        "voteCount": 1,
        "content": "A is ok"
      },
      {
        "date": "2022-10-17T07:28:00.000Z",
        "voteCount": 1,
        "content": "A. Deploy each microservice as a Deployment. Expose the Deployment in the cluster using a Service, and use the Service DNS name to address it from other microservices within the cluster."
      },
      {
        "date": "2022-08-13T20:14:00.000Z",
        "voteCount": 4,
        "content": "I got this question in exam."
      },
      {
        "date": "2023-05-09T05:09:00.000Z",
        "voteCount": 2,
        "content": "I have received this sentence from u in every comment posted by u"
      },
      {
        "date": "2022-08-04T22:13:00.000Z",
        "voteCount": 3,
        "content": "ngress comes with a HTTP(S) LB with external IP hence is not needed for communications within the cluster internally.Microservice as Deployment - used to create replicas as per this request\nDNS name - used as an alias service name for External name which is user for internal requests"
      },
      {
        "date": "2022-07-22T06:19:00.000Z",
        "voteCount": 1,
        "content": "vote A"
      },
      {
        "date": "2022-07-03T08:28:00.000Z",
        "voteCount": 1,
        "content": "A is right, There is no need of Ingress here because all service need to communicate internally..."
      },
      {
        "date": "2022-05-12T01:08:00.000Z",
        "voteCount": 8,
        "content": "Vote A\n1. Based on the description \"You want to be able to configure each microservice with a specific number of replicas.\", It's a hint to use either Deployment or StatefulSet based on the service type is stateless or stateful, since the option only has Deployment, thus Option C and D is out.\n2. Based on the description \"You also want to be able to address a specific microservice from any other microservice in a uniform way, regardless of the number of replicas the microservice scales to.\" the later part is the key point, which means the traffic direct to each service is based on some certain rules, in K8S this means URL, which is Ingress with external HTTP LB."
      },
      {
        "date": "2022-03-05T15:10:00.000Z",
        "voteCount": 1,
        "content": "Of course it is A"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 130,
    "url": "https://www.examtopics.com/discussions/google/view/57301-exam-professional-cloud-architect-topic-1-question-130/",
    "body": "Your company has a networking team and a development team. The development team runs applications on Compute Engine instances that contain sensitive data. The development team requires administrative permissions for Compute Engine. Your company requires all network resources to be managed by the networking team. The development team does not want the networking team to have access to the sensitive data on the instances. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a project with a standalone VPC and assign the Network Admin role to the networking team. 2. Create a second project with a standalone VPC and assign the Compute Admin role to the development team. 3. Use Cloud VPN to join the two VPCs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a project with a standalone Virtual Private Cloud (VPC), assign the Network Admin role to the networking team, and assign the Compute Admin role to the development team.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a project with a Shared VPC and assign the Network Admin role to the networking team. 2. Create a second project without a VPC, configure it as a Shared VPC service project, and assign the Compute Admin role to the development team.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a project with a standalone VPC and assign the Network Admin role to the networking team. 2. Create a second project with a standalone VPC and assign the Compute Admin role to the development team. 3. Use VPC Peering to join the two VPCs."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 106,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 81,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-12-08T09:11:00.000Z",
        "voteCount": 55,
        "content": "For the same project , same VPC, Network Admin role to the networking team, and Compute Admin role to the development team. What is the need for another project?"
      },
      {
        "date": "2023-06-25T10:54:00.000Z",
        "voteCount": 13,
        "content": "For full separation of the teams you will need to use a shared VPC in this case. If you compare the two roles you will see that Compute Admin includes the permissions of the Network Admin so with option B you don't separate the teams as Compute Admin includes compute.network.* permissions (and others). https://cloud.google.com/iam/docs/understanding-roles"
      },
      {
        "date": "2023-09-25T03:27:00.000Z",
        "voteCount": 2,
        "content": "Complete separation was not required. However, the networking team shouldn't have access to the compute engine. For this, no need a full separation. Any better idea?"
      },
      {
        "date": "2024-01-09T11:41:00.000Z",
        "voteCount": 4,
        "content": "They're getting the Compute Admin permissions either way. The key words in the statement are actually \"Create a second project without a VPC, configure it as a Shared VPC service project.\" Since the VPC being used doesn't exist in their project, they're unable to manage network changes."
      },
      {
        "date": "2024-07-26T16:42:00.000Z",
        "voteCount": 1,
        "content": "love this explanation"
      },
      {
        "date": "2021-07-11T07:32:00.000Z",
        "voteCount": 30,
        "content": "C. 1. Create a project with a Shared VPC and assign the Network Admin role to the networking team. 2. Create a second project without a VPC, configure it as a Shared VPC service project, and assign the Compute Admin role to the development team."
      },
      {
        "date": "2022-04-18T08:50:00.000Z",
        "voteCount": 18,
        "content": "I do not understand why do we need to have a shared VPC. With B the dev team will not be able to make any network change and the Network team will not be able to do any change on the CE. I would say B is the correct answer although C does not seem wrong"
      },
      {
        "date": "2023-04-22T07:58:00.000Z",
        "voteCount": 6,
        "content": "Because Compute Admin has compute.* permissions, which includes Network Admin's."
      },
      {
        "date": "2022-10-22T12:48:00.000Z",
        "voteCount": 6,
        "content": "because dev team will have several projects - for dev, qa and prod per app they are developing so C is most scalable solution"
      },
      {
        "date": "2023-03-10T08:41:00.000Z",
        "voteCount": 1,
        "content": "How does 1 additional VPC solve what you're expressing -- that the dev team needs a dev and qa environment."
      },
      {
        "date": "2024-09-28T01:40:00.000Z",
        "voteCount": 1,
        "content": "This is tricky. Both B &amp; C could seem okay, but C is the right answer.\n\nThe compute.networkAdmin role gives broad permissions on the project, which also affects compute instances. For instance, it gives \"compute.instances.get\" and \"compute.instances.use\" permissions. Even though this roles does not grant permissions to start/stop/create/delete instances, it still gives broad permissions on compute instances.\n\nThis gets much clearer if we do the same analysis on the other role: compute.Admin.\nThis role gives permissions on \"compute.*\", which also includes \"compute.networks.*\". That is exactly what we don't want to happen. If we spawn the VPC and the compute VMs in the same project, then compute admins will be able to mess around with the VPC. \n\nThat is why we need to separate networks and compute within 2 projects, unless creating custom roles, etc. Shared VPC are aimed at that. Therefore, C is the right answer."
      },
      {
        "date": "2024-07-28T10:07:00.000Z",
        "voteCount": 2,
        "content": "If it was compute instance admin instead of compute admin, then B would do it,\nBut since They specifically mention Compute Admin, C is the only option.\nBut that being said, there's nothing stopping the apps team from creating a vpc in the new project since they have all the required permissions for it, kind of an oversight"
      },
      {
        "date": "2024-03-08T08:30:00.000Z",
        "voteCount": 2,
        "content": "While IAM roles can technically achieve some separation in option B, Shared VPC (option C) offers a more secure, well-defined, and recommended approach for this scenario. It reduces the risk of accidental access and promotes a cleaner separation of duties between the networking and development teams."
      },
      {
        "date": "2024-02-18T07:20:00.000Z",
        "voteCount": 1,
        "content": "Shared ~VPC provide a feature to have segregation of such roles."
      },
      {
        "date": "2024-02-13T10:07:00.000Z",
        "voteCount": 1,
        "content": "The project I am part of follows exactly the same architecture as Option C."
      },
      {
        "date": "2024-02-07T00:56:00.000Z",
        "voteCount": 1,
        "content": "C is ok."
      },
      {
        "date": "2024-02-04T03:27:00.000Z",
        "voteCount": 1,
        "content": "No one is a valid answer (sorry for that) because all answers assign COMPUTE ADMIN to the developers.\nIf Your company requires all network resources to be managed by the networking team you must NOT assigne COMPUTE ADMIN to the developers because you give them the power to managed the network. No matter the project they are assigned, they could do things forbidden for the requeriment."
      },
      {
        "date": "2023-12-15T10:18:00.000Z",
        "voteCount": 2,
        "content": "Not B because Compute Admin has networking roles."
      },
      {
        "date": "2023-10-30T02:36:00.000Z",
        "voteCount": 5,
        "content": "https://cloud.google.com/vpc/docs/shared-vpc\n\nWhen you use Shared VPC, you designate a project as a host project and attach one or more other service projects to it. The VPC networks in the host project are called Shared VPC networks. Eligible resources from service projects can use subnets in the Shared VPC network.\n\nShared VPC lets organization administrators delegate administrative responsibilities, such as creating and managing instances, to Service Project Admins while maintaining centralized control over network resources like subnets, routes, and firewalls."
      },
      {
        "date": "2023-10-24T16:33:00.000Z",
        "voteCount": 4,
        "content": "Two reasons for me to select C:\n1. Compute admin has network admin roles included\n2. If Dev team adds more projects for testing/staging later, they can be centrally handled by the host project and Google also recommends separation of duties"
      },
      {
        "date": "2023-10-06T08:17:00.000Z",
        "voteCount": 3,
        "content": "Option B suggests creating a single project with a standalone VPC, and assigning both the Network Admin and Compute Admin roles to the respective teams. However, this solution does not enforce the required separation of duties between the networking and development teams.\n\nOption C suggests using a Shared VPC. A Shared VPC allows for separation of duties between teams while sharing network resources. The networking team can manage the Shared VPC, and the development team can create Compute Engine instances in the Shared VPC without the networking team having access to the sensitive data on the instances. The development team can be assigned the Compute Admin role for the Shared VPC service project, and the networking team can be assigned the Network Admin role for the Shared VPC host project."
      },
      {
        "date": "2023-09-25T04:30:00.000Z",
        "voteCount": 1,
        "content": "Can this be handled with IAM roles? Yes. Why go through extra effort, in the event you want to add a feature or couple other services, would you still continue tweaking?"
      },
      {
        "date": "2023-08-24T03:54:00.000Z",
        "voteCount": 2,
        "content": "to prevent dev team from having network admin (same project will mean dev team with compute admin will also have network permission).  That's my take on this one."
      },
      {
        "date": "2023-07-24T13:35:00.000Z",
        "voteCount": 1,
        "content": "It talks about managing all network resources in a company. Google always recommends having a shared VPC to maintain network resources in an organization. The separation of roles adds to the favour of having a shared vpc."
      },
      {
        "date": "2023-07-16T15:40:00.000Z",
        "voteCount": 2,
        "content": "C is not correct as it doesn't make sense to have 2 vpcs for managing roles"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 131,
    "url": "https://www.examtopics.com/discussions/google/view/56615-exam-professional-cloud-architect-topic-1-question-131/",
    "body": "Your company wants you to build a highly reliable web application with a few public APIs as the backend. You don't expect a lot of user traffic, but traffic could spike occasionally. You want to leverage Cloud Load Balancing, and the solution must be cost-effective for users. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore static content such as HTML and images in Cloud CDN. Host the APIs on App Engine and store the user data in Cloud SQL.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore static content such as HTML and images in a Cloud Storage bucket. Host the APIs on a zonal Google Kubernetes Engine cluster with worker nodes in multiple zones, and save the user data in Cloud Spanner.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore static content such as HTML and images in Cloud CDN. Use Cloud Run to host the APIs and save the user data in Cloud SQL.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore static content such as HTML and images in a Cloud Storage bucket. Use Cloud Functions to host the APIs and save the user data in Firestore.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 41,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 11,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 9,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-07-01T02:32:00.000Z",
        "voteCount": 54,
        "content": "Answer should be D,\nhttps://cloud.google.com/load-balancing/docs/https/setting-up-https-serverless#gcloud:-cloud-functions\nhttps://cloud.google.com/blog/products/networking/better-load-balancing-for-app-engine-cloud-run-and-functions"
      },
      {
        "date": "2021-07-07T07:55:00.000Z",
        "voteCount": 1,
        "content": "D not use CDN, is D correct answer?"
      },
      {
        "date": "2021-07-18T11:00:00.000Z",
        "voteCount": 6,
        "content": "CDN is not needed here. You don't need to service users globally thus latency and locality isn't critical"
      },
      {
        "date": "2021-10-31T02:24:00.000Z",
        "voteCount": 5,
        "content": "IMHO CDN is not storage solution to store static html or image"
      },
      {
        "date": "2022-02-10T10:24:00.000Z",
        "voteCount": 1,
        "content": "You should look at this\nhttps://cloud.google.com/storage/docs/caching"
      },
      {
        "date": "2021-07-27T08:23:00.000Z",
        "voteCount": 10,
        "content": "Spanner is expensive"
      },
      {
        "date": "2021-10-21T04:58:00.000Z",
        "voteCount": 5,
        "content": "but is it Cloud Functions used for hosting APIs?"
      },
      {
        "date": "2022-11-12T16:41:00.000Z",
        "voteCount": 4,
        "content": "Can be hosted. It's cost effective since you get charged on per call basis. If no traffic then no cost will be charged."
      },
      {
        "date": "2023-12-21T02:24:00.000Z",
        "voteCount": 1,
        "content": "If CF you want to use for hosting APIs why not option C. to use CloudRun? It too autoscales to 0 instances for no traffic.."
      },
      {
        "date": "2024-05-24T22:39:00.000Z",
        "voteCount": 1,
        "content": "CE using cloud CDN to host static contents which is incorrect."
      },
      {
        "date": "2021-10-24T01:35:00.000Z",
        "voteCount": 7,
        "content": "IMHO, i agree with you. Furthermore:\nCloud Storage buckets are a good choice for static web content. Cloud storage buckets behave like a CDN Network:\nhttps://cloud.google.com/storage/docs/caching\nSo it is lower cost than CDN."
      },
      {
        "date": "2021-06-30T21:55:00.000Z",
        "voteCount": 17,
        "content": "IMHO it is d), not b).\nReason is that you don't need Cloud Spanner just to store user data - FireStore is the better solution. Additionally, I see no indications concerning the requirement to use GKE... Please correct me when I am wrong."
      },
      {
        "date": "2021-12-11T09:04:00.000Z",
        "voteCount": 3,
        "content": "agree with u"
      },
      {
        "date": "2024-01-01T10:38:00.000Z",
        "voteCount": 1,
        "content": "That's correct"
      },
      {
        "date": "2024-01-01T10:38:00.000Z",
        "voteCount": 1,
        "content": "That's correct"
      },
      {
        "date": "2024-07-13T01:50:00.000Z",
        "voteCount": 1,
        "content": "As per ChatGPT , answer is C"
      },
      {
        "date": "2024-07-12T01:40:00.000Z",
        "voteCount": 1,
        "content": "xcvxcv"
      },
      {
        "date": "2024-03-29T18:51:00.000Z",
        "voteCount": 1,
        "content": "Answer is D. \nSolution should be cost effective and highly reliable. Cloud storage and firestore is suitable ."
      },
      {
        "date": "2024-03-02T09:46:00.000Z",
        "voteCount": 1,
        "content": "absolutely D because kubernetes is not cost effective. cloud functions are a better shot"
      },
      {
        "date": "2024-01-11T05:37:00.000Z",
        "voteCount": 3,
        "content": "CDN is for caching. For static website hosting a storage bucket is a good choice, thus A and C are eliminated.\n\nB, Cloud Spanner, my nightmare. I accidentally created an empty cloud spanner and it burned like 30-40 USD per day! I got a huge amount of billing from GCP that month! No way to be cost-effective.\n\nD, Cloud Functions are good for simple API services and have no cost if not in use. SQL or NoSQL for user data is not a strong factor here, either should be fine."
      },
      {
        "date": "2024-01-03T22:08:00.000Z",
        "voteCount": 1,
        "content": "The correct answer with the lowest cost is D"
      },
      {
        "date": "2024-01-01T03:19:00.000Z",
        "voteCount": 2,
        "content": "in B) it says zonal GKE with worker nodes in multiple zones\nhow zonal and in multiple zones?! \nAnd that 100% eliminate B"
      },
      {
        "date": "2024-04-24T10:49:00.000Z",
        "voteCount": 1,
        "content": "\"zonal\" is for conrol plane, workers can be in multiple zones in multi-zone zonal cluster - the wording is just strange, see: https://cloud.google.com/kubernetes-engine/docs/how-to/creating-a-zonal-cluster\n\nNevertheless B is still incorrect"
      },
      {
        "date": "2023-12-04T12:06:00.000Z",
        "voteCount": 1,
        "content": "Actually it is B. Highly reliable, using Load Balancer, Spikes and several API. D is less reliable, as functions will take long to cold start and will have a time out and not using LB. The only draw back is cost."
      },
      {
        "date": "2023-11-11T09:10:00.000Z",
        "voteCount": 1,
        "content": "User data in a relational database is not a good option, A B C are ruled out. \nLeft with D - User data in Firestore."
      },
      {
        "date": "2023-10-28T17:57:00.000Z",
        "voteCount": 1,
        "content": "D is the correct option since CDN is not for storage of html content, so you have only the option B and D. And the option B required that you have a containerezed application in other way, you can't use k8s. So the option is D."
      },
      {
        "date": "2023-10-26T00:52:00.000Z",
        "voteCount": 3,
        "content": "D is the correct answer\n\n1. Cloud CDN is not meant for storing contents. It can only cache but not act as the source-of-truth and moreover the question tells that the user traffic is less. Cloud CDN performs better when the user traffic is high\n\n2. The spikes are very rare and the app gets low traffic most of the time. That said, putting a cluster is an expensive option. choosing a serverless supported app-platform (Cloud Run) and serverless supported DB (Firestore) makes perfect choice here to handle cost during low traffic and also handle any incoming sudden spikes"
      },
      {
        "date": "2023-10-12T07:01:00.000Z",
        "voteCount": 2,
        "content": "Cloud Spanner is expensive, so it cannot be that option."
      },
      {
        "date": "2023-08-01T02:15:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer Is D"
      },
      {
        "date": "2023-07-17T05:32:00.000Z",
        "voteCount": 2,
        "content": "Store static content such as HTML and images in a Cloud Storage bucket. Use Cloud Functions to host the APIs and save the user data in Firestore"
      },
      {
        "date": "2023-06-10T03:18:00.000Z",
        "voteCount": 1,
        "content": "The user of Spanner is mad (way to expensive for the requirement).  CDN is not a storage solution.  I suspect the charging model of firestore is also better for this application, not to mention document databases typically preferred for storing user data."
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 132,
    "url": "https://www.examtopics.com/discussions/google/view/57043-exam-professional-cloud-architect-topic-1-question-132/",
    "body": "Your company sends all Google Cloud logs to Cloud Logging. Your security team wants to monitor the logs. You want to ensure that the security team can react quickly if an anomaly such as an unwanted firewall change or server breach is detected. You want to follow Google-recommended practices. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSchedule a cron job with Cloud Scheduler. The scheduled job queries the logs every minute for the relevant events.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExport logs to BigQuery, and trigger a query in BigQuery to process the log data for the relevant events.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExport logs to a Pub/Sub topic, and trigger Cloud Function with the relevant log events.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExport logs to a Cloud Storage bucket, and trigger Cloud Run with the relevant log events."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 15,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-07-07T21:24:00.000Z",
        "voteCount": 47,
        "content": "I think C using BigQuery can get expensive if you have somehow check the logs for anomalies\n\nhttps://cloud.google.com/blog/products/management-tools/automate-your-response-to-a-cloud-logging-event\n\ncheck there is a diagram"
      },
      {
        "date": "2021-07-21T04:43:00.000Z",
        "voteCount": 5,
        "content": "love you :)"
      },
      {
        "date": "2021-12-16T17:47:00.000Z",
        "voteCount": 2,
        "content": "cloud function is also key point"
      },
      {
        "date": "2021-09-10T18:24:00.000Z",
        "voteCount": 4,
        "content": "It may get expensive but GCP recommended way , they not asking for self alternative for cheap solution."
      },
      {
        "date": "2022-09-19T18:01:00.000Z",
        "voteCount": 3,
        "content": "C is absolutely make sense, Thank you for sharing the link."
      },
      {
        "date": "2021-07-04T10:33:00.000Z",
        "voteCount": 9,
        "content": "c) is correct as quickly action is required for unwanted event/access should be actioned."
      },
      {
        "date": "2023-11-14T03:45:00.000Z",
        "voteCount": 2,
        "content": "Option is C\nThe clean and neat way to architect the solution is C."
      },
      {
        "date": "2023-06-13T09:34:00.000Z",
        "voteCount": 1,
        "content": "One of your key employees received a job offer from another cloud company. S/he left the Organization without giving notice. His Google Account was kept active for 3 weeks. How can you find out if the employee accessed any sensitive data after s/he left?"
      },
      {
        "date": "2024-01-10T10:41:00.000Z",
        "voteCount": 2,
        "content": "use user activity log"
      },
      {
        "date": "2022-12-14T03:48:00.000Z",
        "voteCount": 1,
        "content": "C Is the correct answer"
      },
      {
        "date": "2022-11-16T00:46:00.000Z",
        "voteCount": 1,
        "content": "C is ok"
      },
      {
        "date": "2022-10-19T12:08:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2022-10-18T01:10:00.000Z",
        "voteCount": 2,
        "content": "C - check https://cloud.google.com/blog/products/management-tools/automate-your-response-to-a-cloud-logging-event"
      },
      {
        "date": "2022-08-04T22:32:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/blog/products/management-tools/automate-your-response-to-a-cloud-logging-event"
      },
      {
        "date": "2022-07-03T08:44:00.000Z",
        "voteCount": 1,
        "content": "Pub/Sub &amp; Cloud Function serves the purpose , I am choosing C as right !"
      },
      {
        "date": "2022-03-05T18:53:00.000Z",
        "voteCount": 1,
        "content": "C is the correct one"
      },
      {
        "date": "2022-02-15T14:20:00.000Z",
        "voteCount": 2,
        "content": "2/15/21 exam"
      },
      {
        "date": "2022-02-23T14:31:00.000Z",
        "voteCount": 2,
        "content": "21 or 22 ?"
      },
      {
        "date": "2022-02-11T13:47:00.000Z",
        "voteCount": 3,
        "content": "I got similar question on my exam. Answered C."
      },
      {
        "date": "2022-01-17T05:05:00.000Z",
        "voteCount": 2,
        "content": "B is correct because exported logs can be analyzed in Bigquery to identity anomalies by executing scheduled queries on the exported data."
      },
      {
        "date": "2022-01-17T05:05:00.000Z",
        "voteCount": 2,
        "content": "B is correct because exported logs can be analyzed in Bigquery to identity anomalies by executing scheduled queries on the exported data."
      },
      {
        "date": "2021-12-29T05:04:00.000Z",
        "voteCount": 4,
        "content": "The logs already on Cloud Logging, we can just create a metric and an alert for it. No need any development."
      },
      {
        "date": "2021-12-14T23:55:00.000Z",
        "voteCount": 1,
        "content": "Go for C"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 133,
    "url": "https://www.examtopics.com/discussions/google/view/56751-exam-professional-cloud-architect-topic-1-question-133/",
    "body": "You have deployed several instances on Compute Engine. As a security requirement, instances cannot have a public IP address. There is no VPN connection between Google Cloud and your office, and you need to connect via SSH into a specific machine without violating the security requirements. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Cloud NAT on the subnet where the instance is hosted. Create an SSH connection to the Cloud NAT IP address to reach the instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd all instances to an unmanaged instance group. Configure TCP Proxy Load Balancing with the instance group as a backend. Connect to the instance using the TCP Proxy IP.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Identity-Aware Proxy (IAP) for the instance and ensure that you have the role of IAP-secured Tunnel User. Use the gcloud command line tool to ssh into the instance.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a bastion host in the network to SSH into the bastion host from your office location. From the bastion host, SSH into the desired instance."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 32,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 12,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-07-01T04:43:00.000Z",
        "voteCount": 59,
        "content": "Answer is C.\nhttps://cloud.google.com/iap/docs/using-tcp-forwarding#tunneling_with_ssh"
      },
      {
        "date": "2022-08-19T18:30:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/iap/docs/using-tcp-forwarding#tunneling_with_ssh\n\n\"IAP TCP forwarding allows you to establish an encrypted tunnel over which you can forward SSH, RDP, and other traffic to VM instances\"\nBut Options C says ,,,,, SSH from IAP .. which is not true."
      },
      {
        "date": "2021-10-13T02:07:00.000Z",
        "voteCount": 11,
        "content": "100% Agree. I use IAP all the time which allows me to reduce exposure to VM from public internet. Ans is C"
      },
      {
        "date": "2021-10-24T01:46:00.000Z",
        "voteCount": 12,
        "content": "Agee too. Bastion host violates security requirements due to it has public IP :)"
      },
      {
        "date": "2021-07-10T15:42:00.000Z",
        "voteCount": 16,
        "content": "And D seems correct, bastion host is specifically used for this purpose, using option C user can connect through cloud only.\nBy using a bastion host, you can connect to an VM that does not have an external IP address. This approach allows you to connect to a development environment or manage the database instance for your external application, for example, without configuring additional firewall rules.\nhttps://cloud.google.com/solutions/connecting-securely"
      },
      {
        "date": "2021-10-09T10:05:00.000Z",
        "voteCount": 6,
        "content": "Except the policy is no machines can have public IP's, how do you connect to the bastion?"
      },
      {
        "date": "2022-06-03T03:35:00.000Z",
        "voteCount": 2,
        "content": "It's never mentioned that there's no public IP in all GCP services, it just said instances no public IP, which is very normal. that's why bastion inward, and NAT outward."
      },
      {
        "date": "2022-04-14T09:41:00.000Z",
        "voteCount": 2,
        "content": "C. no network connection between office and cloud. Can't use bastion. What C fails to say or specify is if you are either using cloud shell gcloud or you downloaded the sdk on local. Dumb question without clarification. Assuming silly test writers conflate gcloud always being used in cloud shell. So you are in cloud shell, you have internal access since the shell resides inside the VPC network with all perms."
      },
      {
        "date": "2022-07-22T00:44:00.000Z",
        "voteCount": 1,
        "content": "\" There is no VPN connection between Google Cloud and your office\". If there would be no network connection betweek office and the cloud you could not use any of google services"
      },
      {
        "date": "2022-08-19T18:32:00.000Z",
        "voteCount": 1,
        "content": "But you can always SSH to bastion host from internet .. as ports are open usually\n\nhttps://cloud.google.com/iap/docs/using-tcp-forwarding#tunneling_with_ssh\n\n\"IAP TCP forwarding allows you to establish an encrypted tunnel over which you can forward SSH, RDP, and other traffic to VM instances\". it is Traffic forwarding ... \nBut Options C says ,,,,, SSH from IAP .. which is not true."
      },
      {
        "date": "2022-11-12T16:48:00.000Z",
        "voteCount": 1,
        "content": "If you're looking for word precision then:\n\nOption C says \"Use the gcloud command line tool to ssh into the instance. Most Voted\"\n\nSo I think C is still correct."
      },
      {
        "date": "2023-08-24T14:04:00.000Z",
        "voteCount": 1,
        "content": "Question states: \"As a security requirement, instances cannot have a public IP address\"\nIf you install a Public IP on the GCE Bastion host you violate the security requirement.\nIf you install a Private IP on the GCE bastion host you need a private route (e.g. VPN) or NAT to it.\nThe question scenario seems specific to point to the IAP SSH tunnel feature."
      },
      {
        "date": "2024-10-01T10:56:00.000Z",
        "voteCount": 1,
        "content": "C is the answer. D also works as a solution but it's not using GCP native features when available. Since it's a test about GCP, always go with the answer that uses GCP services."
      },
      {
        "date": "2024-04-25T03:19:00.000Z",
        "voteCount": 1,
        "content": "According to https://cloud.google.com/solutions/connecting-securely : \"Using SSH with IAP's TCP forwarding feature wraps an SSH connection inside HTTPS. IAP's TCP forwarding feature then sends it to the remote VM.\"\n\nSo is it ssh or http connection ? Very tricky question....."
      },
      {
        "date": "2024-04-09T07:27:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/iap/docs/tutorial-gce\n\nAnswer is D.  For C to be corrected it should mention the IAP-secured Web App User role. No the one listed on the question which is wrong"
      },
      {
        "date": "2024-04-09T07:23:00.000Z",
        "voteCount": 1,
        "content": "Answer is D.  Wrong user mentioned on C\nStep 6: Test IAP\nTo test that IAP is working correctly, follow the steps below:\n\nIn your web browser, navigate to your domain.\nIf you see \"Unauthorized request\", try again in a few minutes.\nWhen you see a Google sign-in screen, sign in using the Google Account you gave access to in the previous step.\nYou should see a message like \"Hi, user@example.com. I am my-managed-instance-group-29z6.\"\nTry refreshing the page. Your browser should show the names of the 3 machines in your managed instance group. This is the load balancer distributing traffic across the VMs in the group."
      },
      {
        "date": "2024-06-08T03:52:00.000Z",
        "voteCount": 1,
        "content": "you are wrong, study first IAP. It is C."
      },
      {
        "date": "2024-03-04T05:46:00.000Z",
        "voteCount": 1,
        "content": "C is correct: IAP offers a secure and controlled way to access internal instances without assigning them public IP addresses. It uses IAM permissions to restrict access only to authorized users and provides a temporary connection tunnel for SSH access using the gcloud command-line tool."
      },
      {
        "date": "2024-01-24T06:04:00.000Z",
        "voteCount": 1,
        "content": "C is the best answer"
      },
      {
        "date": "2024-01-11T05:46:00.000Z",
        "voteCount": 1,
        "content": "For D,  Create a bastion host in the network to SSH into the bastion host from your office location. From the bastion host, SSH into the desired instance.\n\nHow could you SSH into the bastion host? All VMs do not have public IP"
      },
      {
        "date": "2023-12-15T10:56:00.000Z",
        "voteCount": 1,
        "content": "C is ok"
      },
      {
        "date": "2023-11-14T03:57:00.000Z",
        "voteCount": 1,
        "content": "As per the documentation, https://cloud.google.com/iap/docs/tcp-forwarding-overview/ The option is C"
      },
      {
        "date": "2023-09-12T14:50:00.000Z",
        "voteCount": 1,
        "content": "C is the correct answer in this case.\n\nQuestion quote \"There is no VPN connection between Google Cloud and your office\"\nAnswer D \"...from your office location...\"\n\nThe only way to achieve this with Bastion Host is giving it a Public IP. At least in this case."
      },
      {
        "date": "2023-06-10T04:46:00.000Z",
        "voteCount": 2,
        "content": "I think it's Bastion host.  In my org (large bluechip) all connections are via bastion host to provide a single point of audit and control."
      },
      {
        "date": "2023-06-25T06:15:00.000Z",
        "voteCount": 3,
        "content": "Instances cannot have public IP bastian host will still need IP"
      },
      {
        "date": "2023-05-30T03:11:00.000Z",
        "voteCount": 2,
        "content": "Bastion host service is specifically designed for this purpose. No need to do over-engineering too much here."
      },
      {
        "date": "2023-05-03T16:58:00.000Z",
        "voteCount": 1,
        "content": "As per ChatGPT:\nSince instances cannot have a public IP address, the best option is to use a bastion host to access the instance securely. Therefore, option D is the correct choice.\n\nHere's what you would do:\n\nCreate a new instance that will serve as a bastion host. Assign it a static IP address.\nConfigure the firewall rules for the bastion host to allow incoming SSH traffic from your office location.\nConnect to the bastion host via SSH from your office location.\nOnce connected to the bastion host, use SSH to connect to the desired instance on the same network.\nThis way, you can securely access the instance without violating the security requirements."
      },
      {
        "date": "2023-03-31T07:54:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/iap/docs/using-tcp-forwarding"
      },
      {
        "date": "2023-03-07T01:58:00.000Z",
        "voteCount": 5,
        "content": "As per chatGPT answer is C.\nIdentity-Aware Proxy (IAP) is a Google Cloud service that provides secure access to VM instances without exposing them to the internet. It allows you to establish a secure SSH connection to a VM instance via the Google Cloud Console or the gcloud command-line tool, using OAuth 2.0-based authentication and authorization. With IAP, you can set up secure, encrypted tunnels to your VM instances, without the need for a VPN or an external bastion host.\n\nBy configuring IAP for the instance and ensuring that you have the IAP-secured Tunnel User role, you can securely access the instance using the gcloud command-line tool to SSH into the instance, without violating the security requirements."
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 134,
    "url": "https://www.examtopics.com/discussions/google/view/56975-exam-professional-cloud-architect-topic-1-question-134/",
    "body": "Your company is using Google Cloud. You have two folders under the Organization: Finance and Shopping. The members of the development team are in a<br>Google Group. The development team group has been assigned the Project Owner role on the Organization. You want to prevent the development team from creating resources in projects in the Finance folder. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAssign the development team group the Project Viewer role on the Finance folder, and assign the development team group the Project Owner role on the Shopping folder.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAssign the development team group only the Project Viewer role on the Finance folder.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAssign the development team group the Project Owner role on the Shopping folder, and remove the development team group Project Owner role from the Organization.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAssign the development team group only the Project Owner role on the Shopping folder."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 20,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-07-02T19:14:00.000Z",
        "voteCount": 32,
        "content": "It is C"
      },
      {
        "date": "2021-07-10T05:39:00.000Z",
        "voteCount": 14,
        "content": "Answer C is correct.\nAnswer A and B are both overridden by the less-restrictive permission on Organization level.\nAnswer D permission was already there on Organization level, and does not remove the project owner permission on the other folder"
      },
      {
        "date": "2023-11-14T04:03:00.000Z",
        "voteCount": 1,
        "content": "Out of the four options given + principle of least privileges - C is the best option."
      },
      {
        "date": "2023-09-19T06:18:00.000Z",
        "voteCount": 1,
        "content": "C is correct. It is required to remove the Project Owner role from the Organization."
      },
      {
        "date": "2023-08-01T03:37:00.000Z",
        "voteCount": 1,
        "content": "Option C suggests assigning the development team group the Project Owner role on the Shopping folder and removing the development team group's Project Owner role from the Organization. However, this approach would not achieve the desired outcome of preventing the development team from creating resources in projects within the Finance folder.\n\nCorrect Answer is A"
      },
      {
        "date": "2023-02-14T04:34:00.000Z",
        "voteCount": 1,
        "content": "C seems right, you have to remove the role from organization level"
      },
      {
        "date": "2022-12-14T03:54:00.000Z",
        "voteCount": 1,
        "content": "C is the correct answer"
      },
      {
        "date": "2022-11-16T00:54:00.000Z",
        "voteCount": 1,
        "content": "C is ok"
      },
      {
        "date": "2022-10-19T12:18:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2022-09-11T13:42:00.000Z",
        "voteCount": 10,
        "content": "It says \"The development team has received Project Owner Role in the Organization level\"\nThis means the only answer viable to not be able to access Finance folder resources is to remove that initial role that they were assigned and assign the new one, hence only C mentions removing the role. \nRemember IAM roles are hereditary downwards, therefor even if we assign Project Viewer to that Group, they still keep their Project Owner in the Project, unless removed."
      },
      {
        "date": "2022-09-19T18:07:00.000Z",
        "voteCount": 2,
        "content": "C is perfect for this scenario."
      },
      {
        "date": "2023-11-12T12:28:00.000Z",
        "voteCount": 1,
        "content": "excellent explanation"
      },
      {
        "date": "2022-08-12T04:58:00.000Z",
        "voteCount": 2,
        "content": "selected answer C\nbasic concept"
      },
      {
        "date": "2022-07-10T06:09:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2022-06-20T05:48:00.000Z",
        "voteCount": 1,
        "content": "C is right"
      },
      {
        "date": "2022-05-26T12:56:00.000Z",
        "voteCount": 2,
        "content": "Answer is C, adding Project Viewer on folder doesn't overwrite the Project Owner at org level, so you have to delete it."
      },
      {
        "date": "2022-05-19T08:19:00.000Z",
        "voteCount": 1,
        "content": "A is better than B. And Correct."
      },
      {
        "date": "2022-05-19T08:17:00.000Z",
        "voteCount": 1,
        "content": "It is obviously B"
      },
      {
        "date": "2022-02-15T14:21:00.000Z",
        "voteCount": 1,
        "content": "2/15/21 exam"
      },
      {
        "date": "2022-03-09T17:39:00.000Z",
        "voteCount": 4,
        "content": "give some context on your 2/15/21 post in every discussion"
      },
      {
        "date": "2022-07-31T16:19:00.000Z",
        "voteCount": 2,
        "content": "I guess he got this question on that day exam."
      },
      {
        "date": "2022-04-15T08:06:00.000Z",
        "voteCount": 1,
        "content": "Seriously :D"
      },
      {
        "date": "2022-07-03T09:01:00.000Z",
        "voteCount": 1,
        "content": "what do u mean by the date, It doesn't help anyone"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 135,
    "url": "https://www.examtopics.com/discussions/google/view/56640-exam-professional-cloud-architect-topic-1-question-135/",
    "body": "You are developing your microservices application on Google Kubernetes Engine. During testing, you want to validate the behavior of your application in case a specific microservice should suddenly crash. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a taint to one of the nodes of the Kubernetes cluster. For the specific microservice, configure a pod anti-affinity label that has the name of the tainted node as a value.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Istio's fault injection on the particular microservice whose faulty behavior you want to simulate.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDestroy one of the nodes of the Kubernetes cluster to observe the behavior.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Istio's traffic management features to steer the traffic away from a crashing microservice."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 28,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-07-01T04:50:00.000Z",
        "voteCount": 45,
        "content": "Answer is B.\napplication crash, not node."
      },
      {
        "date": "2021-07-02T12:02:00.000Z",
        "voteCount": 5,
        "content": "I see it the same way - it is b)"
      },
      {
        "date": "2021-06-30T22:40:00.000Z",
        "voteCount": 21,
        "content": "I think that c) is not the correct answer.\nI am not a GKE or Kubernetes expert, so maybe I am wrong.\nMy understanding is, that in Kubernetes a microservice can run on pods on different nodes and one node can contain pods running differend microservices - so to kill one node will not kill a microservice but several pods running on that node. Please correct me if I am wrong."
      },
      {
        "date": "2024-06-07T08:18:00.000Z",
        "voteCount": 2,
        "content": "Between B and C.\n\nC is the wrong answer as the microservice may very well be running in one of the other nodes that were not destroyed."
      },
      {
        "date": "2024-01-24T06:23:00.000Z",
        "voteCount": 3,
        "content": "Fault injection is a technique used in chaos engineering to deliberately introduce errors into a system to test its resilience and observe its behavior under failure conditions. Istio is a service mesh that can manage the traffic between microservices. It includes fault injection capabilities that enable you to simulate failures such as delays or crashed services without actually stopping the service or damaging the environment. This allows you to validate how the rest of your application reacts to the failure of a specific microservice."
      },
      {
        "date": "2024-01-15T08:58:00.000Z",
        "voteCount": 1,
        "content": "Testing Objective: The choice between options B and C depends on the testing objectives. If the goal is to understand the behavior of the system when a specific microservice fails (this is how the question is formulated), then a targeted approach (Option B) is more appropriate. If the goal is to understand the broader resiliency of the system to node failures, then Option C would be more relevant.\nMicroservice Focus: Given the question's focus on a specific microservice, Istio's fault injection (Option B) provides a more direct and controlled way to simulate the failure of that microservice and observe the system's response, aligning better with the scenario described."
      },
      {
        "date": "2024-01-03T10:43:00.000Z",
        "voteCount": 1,
        "content": "Selected Answer: B\nKill one randomly selected k8s cluster node doesn't guarantee you kill pods of the microservice. The node could or colud not have microservices of that app running on it."
      },
      {
        "date": "2023-11-24T12:47:00.000Z",
        "voteCount": 1,
        "content": "IMHO the right answer is C. This link clearly explains why (remember that we are in a testing environment, so what's the problem in generating a cras of an entire node of the cluster?):\nhttps://www.linkedin.com/pulse/google-cloud-architect-case-study-5-biswa-prakash-nayak/"
      },
      {
        "date": "2024-04-25T07:11:00.000Z",
        "voteCount": 1,
        "content": "The problem is that according to the question: \"During testing, you want to validate the behavior of your application in case a specific micsoservice should suddenly crash\"....Even on testing environment specific microservice can run on more then one node and not on the one you destroyed.....moreover you should not affect other parts o the application (other microservices running on the destroyed node)"
      },
      {
        "date": "2023-10-29T15:39:00.000Z",
        "voteCount": 1,
        "content": "B is the correct option, if you deleted a node you will deleted more than the microservices that you wanted to eliminated."
      },
      {
        "date": "2023-09-27T01:24:00.000Z",
        "voteCount": 2,
        "content": "https://istiobyexample.dev/fault-injection/\nIn a Kubernetes environment, you can approach chaos testing at different layers - for instance, by deleting pods at random, or shutting off entire nodes.\n\nBut failures also happen at the application layer. Infinite loops, broken client libraries - application code can fail in an infinite number of ways! This is where Istio fault injection comes in. You can use Istio VirtualServices to do chaos testing at the application layer, by injecting timeouts or HTTP errors into your services, without actually updating your app code. Let\u2019s see how.\nSo both C and B work."
      },
      {
        "date": "2023-09-27T01:26:00.000Z",
        "voteCount": 2,
        "content": "Probably C is the best to simulate the entire microservice crashing."
      },
      {
        "date": "2023-09-19T06:25:00.000Z",
        "voteCount": 2,
        "content": "B seems correct. Istio's fault injection is right way to introduce fault."
      },
      {
        "date": "2023-06-10T05:01:00.000Z",
        "voteCount": 1,
        "content": "I think perhaps C is correct due to the very specific test scenario.  Chaos testing would be great for generally resilience test but it wants to test the behaviour of the app when the microservice crashed.  Shutting down the microservice seems the simplest way to test this scenario."
      },
      {
        "date": "2023-11-24T12:50:00.000Z",
        "voteCount": 1,
        "content": "I agree, mostly because we are in a testing environment, so a node crash is not a great disaster even if there are other microservices running in thst node's pods."
      },
      {
        "date": "2023-03-02T18:48:00.000Z",
        "voteCount": 1,
        "content": "Isn\u2019t Istio used in the context of Anthos vs. GKE?"
      },
      {
        "date": "2023-01-05T02:43:00.000Z",
        "voteCount": 2,
        "content": "Istio fault injection  :https://istiobyexample.dev/fault-injection/"
      },
      {
        "date": "2022-12-14T03:56:00.000Z",
        "voteCount": 1,
        "content": "B is the correct answer"
      },
      {
        "date": "2022-12-03T01:41:00.000Z",
        "voteCount": 2,
        "content": "Istio is replicated and replaced by Anthos service mesh, please update the answer."
      },
      {
        "date": "2022-11-16T03:10:00.000Z",
        "voteCount": 1,
        "content": "B is ok"
      },
      {
        "date": "2022-10-19T11:24:00.000Z",
        "voteCount": 1,
        "content": "destroying a node is not really testing microservices failure"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 136,
    "url": "https://www.examtopics.com/discussions/google/view/56754-exam-professional-cloud-architect-topic-1-question-136/",
    "body": "Your company is developing a new application that will allow globally distributed users to upload pictures and share them with other selected users. The application will support millions of concurrent users. You want to allow developers to focus on just building code without having to create and maintain the underlying infrastructure. Which service should you use to deploy the application?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tApp Engine\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Endpoints",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCompute Engine",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGoogle Kubernetes Engine"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 10,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-07-02T19:16:00.000Z",
        "voteCount": 27,
        "content": "A, App Engine, you just want you people dedicated to the App"
      },
      {
        "date": "2021-09-23T12:05:00.000Z",
        "voteCount": 13,
        "content": "AppEngine is regional.  Millions of distributed global users = GkE."
      },
      {
        "date": "2021-12-01T06:18:00.000Z",
        "voteCount": 7,
        "content": "But \"focus on just building code without having to create and maintain the underlying infrastructure\" =&gt; A right"
      },
      {
        "date": "2023-11-28T03:33:00.000Z",
        "voteCount": 1,
        "content": "D\nGKE - Auto mode is free from managing the underlying infra."
      },
      {
        "date": "2023-09-19T06:31:00.000Z",
        "voteCount": 5,
        "content": "Compute Engine &amp; Google Kubernetes Engine are not pure PaaS, so they will require some management and maintenance. So C &amp; D are eliminated. Cloud Endpoints is also not best fit for the given requirement, so B is also eliminated.\nApp Engine is fully managed platform."
      },
      {
        "date": "2022-11-23T05:17:00.000Z",
        "voteCount": 1,
        "content": "App Engine would work"
      },
      {
        "date": "2022-11-16T03:11:00.000Z",
        "voteCount": 1,
        "content": "A is ok"
      },
      {
        "date": "2022-07-03T09:06:00.000Z",
        "voteCount": 3,
        "content": "App Engine is right choice, Developer can focus on developing code rather than worry about infrastructure. A is right"
      },
      {
        "date": "2022-06-07T17:38:00.000Z",
        "voteCount": 1,
        "content": "The users are global but doesn't mean that app can't be regional. A is the correct answer it seems."
      },
      {
        "date": "2022-04-14T09:46:00.000Z",
        "voteCount": 2,
        "content": "Another dumb question. Does the company have an operations team? K8s is specifically meant for appcode devs to be abstracted away from infra and your devops/ops handles the infra making it so all a dev has to do is say \"here's my built container image that was built in a fully automated fashion that was completely abstracted from me after I merged my pull request\". Then again, test writes are def going for app engine here."
      },
      {
        "date": "2022-02-15T14:21:00.000Z",
        "voteCount": 2,
        "content": "2/15/21 exam"
      },
      {
        "date": "2022-02-11T09:13:00.000Z",
        "voteCount": 1,
        "content": "I am seeing some confusion in answers for questions relating to requests or concurrent sessions being served by a solution. I think we ought not to look at API rate limits being synonymous to data handling limits, as these are programming limits to that API. An analogy would be comparing the control plane request limits to the data limits."
      },
      {
        "date": "2022-02-08T19:41:00.000Z",
        "voteCount": 1,
        "content": "go for A"
      },
      {
        "date": "2021-12-08T20:40:00.000Z",
        "voteCount": 1,
        "content": "Why not Cloud Endpoint?"
      },
      {
        "date": "2021-12-16T19:48:00.000Z",
        "voteCount": 1,
        "content": "Not to manage infra so it's A"
      },
      {
        "date": "2021-12-04T20:13:00.000Z",
        "voteCount": 2,
        "content": "A is the correct answer"
      },
      {
        "date": "2021-11-21T06:22:00.000Z",
        "voteCount": 4,
        "content": "In GKE, you have to create underlying infrastructure. It is not PAAS. Only app engine provide capability for developers to focus on code. The GKE, you need to configure many other items Apart from Code. So A seems to be more accurate. Regarding global nature, the app engine application servers users globally. I agree that there may be little latency for regions other than NA, however since the focus of the question is \"Code\", I would select A."
      },
      {
        "date": "2021-11-13T15:50:00.000Z",
        "voteCount": 1,
        "content": "App Engine support limited numbers of languages, they does not mention which interface, so for flexibity i will go for D"
      },
      {
        "date": "2021-10-21T00:23:00.000Z",
        "voteCount": 2,
        "content": "I would say it's App Engine for developers to focus on code."
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 137,
    "url": "https://www.examtopics.com/discussions/google/view/56656-exam-professional-cloud-architect-topic-1-question-137/",
    "body": "Your company provides a recommendation engine for retail customers. You are providing retail customers with an API where they can submit a user ID and the<br>API returns a list of recommendations for that user. You are responsible for the API lifecycle and want to ensure stability for your customers in case the API makes backward-incompatible changes. You want to follow Google-recommended practices. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a distribution list of all customers to inform them of an upcoming backward-incompatible change at least one month before replacing the old API with the new API.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an automated process to generate API documentation, and update the public API documentation as part of the CI/CD process when deploying an update to the API.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a versioning strategy for the APIs that increases the version number on every backward-incompatible change.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a versioning strategy for the APIs that adds the suffix \u05d2\u20acDEPRECATED\u05d2\u20ac to the current API version number on every backward-incompatible change. Use the current version number for the new API."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 33,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-07-02T19:16:00.000Z",
        "voteCount": 35,
        "content": "It is C"
      },
      {
        "date": "2021-08-04T06:25:00.000Z",
        "voteCount": 9,
        "content": "https://cloud.google.com/apis/design/versioning"
      },
      {
        "date": "2023-01-06T12:37:00.000Z",
        "voteCount": 5,
        "content": "more specifically: https://cloud.google.com/apis/design/versioning#release-based_versioning"
      },
      {
        "date": "2021-07-13T06:26:00.000Z",
        "voteCount": 23,
        "content": "Answer C\nAll Google API interfaces must provide a major version number, which is encoded at the end of the protobuf package, and included as the first part of the URI path for REST APIs. If an API introduces a breaking change, such as removing or renaming a field, it must increment its API version number to ensure that existing user code does not suddenly break."
      },
      {
        "date": "2024-06-07T08:30:00.000Z",
        "voteCount": 1,
        "content": "Breaking changes = version bump in the API url"
      },
      {
        "date": "2024-02-06T11:23:00.000Z",
        "voteCount": 1,
        "content": "Answer is A\nQuestion asks: You are responsible for the API lifecycle and want to ensure stability for your customers in case the API makes backward-incompatible changes.\n\nOption C is a must for the API lifecycle but doing only that will not help your customers. In my opinion they have to be notified of backward-incompatible changes with time enough to stay ready"
      },
      {
        "date": "2024-01-11T06:02:00.000Z",
        "voteCount": 2,
        "content": "This is not a GCP-specific or even cloud-specific question, but a development best practice. No need to look at other options."
      },
      {
        "date": "2023-12-05T22:56:00.000Z",
        "voteCount": 4,
        "content": "I had a test today. The overall question was almost the same, but question mentioned  Apigee."
      },
      {
        "date": "2023-08-29T09:52:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2023-06-10T05:05:00.000Z",
        "voteCount": 1,
        "content": "Maybe A on top of C but are we really going to mandate customers only have 1 month to be ready?  I don't think so, use the version strategy and let them migrate when ready!"
      },
      {
        "date": "2023-05-30T03:31:00.000Z",
        "voteCount": 1,
        "content": "Anyways option A is also a recommended best practice. Its version incompatible change afterall."
      },
      {
        "date": "2023-04-18T18:53:00.000Z",
        "voteCount": 1,
        "content": "Clearly C"
      },
      {
        "date": "2023-03-18T10:15:00.000Z",
        "voteCount": 1,
        "content": "c version numbers cannot be changed as given in d"
      },
      {
        "date": "2023-03-09T06:30:00.000Z",
        "voteCount": 3,
        "content": "The correct answer is: C. Use a versioning strategy for the APIs that increases the version number on every backward-incompatible change.\n\nA versioning strategy for the APIs that increases the version number on every backward-incompatible change is the best way to ensure stability for your customers in case the API makes backward-incompatible changes. This will allow you to track the changes that have been made to the API and allow your customers to easily identify the latest version of the API."
      },
      {
        "date": "2023-01-31T20:31:00.000Z",
        "voteCount": 1,
        "content": "See @seafranek comment"
      },
      {
        "date": "2022-12-22T03:04:00.000Z",
        "voteCount": 1,
        "content": "If 'A' is the correct answer, one would wonder why.\nThe new version might be back-ward incompatible but does not necessarily mean its a breaking one. In that case A might be sufficient, although C remains mandatoryBest Practice imho."
      },
      {
        "date": "2022-12-14T04:00:00.000Z",
        "voteCount": 2,
        "content": "C is the correct answer"
      },
      {
        "date": "2022-11-16T03:15:00.000Z",
        "voteCount": 1,
        "content": "C is ok"
      },
      {
        "date": "2022-10-26T21:54:00.000Z",
        "voteCount": 1,
        "content": "option C"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 138,
    "url": "https://www.examtopics.com/discussions/google/view/56976-exam-professional-cloud-architect-topic-1-question-138/",
    "body": "Your company has developed a monolithic, 3-tier application to allow external users to upload and share files. The solution cannot be easily enhanced and lacks reliability. The development team would like to re-architect the application to adopt microservices and a fully managed service approach, but they need to convince their leadership that the effort is worthwhile. Which advantage(s) should they highlight to leadership?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe new approach will be significantly less costly, make it easier to manage the underlying infrastructure, and automatically manage the CI/CD pipelines.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe monolithic solution can be converted to a container with Docker. The generated container can then be deployed into a Kubernetes cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe new approach will make it easier to decouple infrastructure from application, develop and release new features, manage the underlying infrastructure, manage CI/CD pipelines and perform A/B testing, and scale the solution if necessary.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe process can be automated with Migrate for Compute Engine."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-07-02T19:36:00.000Z",
        "voteCount": 23,
        "content": "decoupling, new features, CI/CD, A/B testing, scaling is the advantage so C"
      },
      {
        "date": "2021-07-14T21:20:00.000Z",
        "voteCount": 12,
        "content": "hey guys new Qs posted as of July 12th, 2021, All 21 new Qs in Question #152"
      },
      {
        "date": "2021-11-08T05:03:00.000Z",
        "voteCount": 4,
        "content": "Hey man do you know if in second attempt as well, we get these same questions?"
      },
      {
        "date": "2022-06-05T06:19:00.000Z",
        "voteCount": 2,
        "content": "+ 1 Please let me know as well."
      },
      {
        "date": "2022-05-06T00:45:00.000Z",
        "voteCount": 2,
        "content": "+1. Please let me know as well."
      },
      {
        "date": "2024-01-16T21:56:00.000Z",
        "voteCount": 1,
        "content": "Almost questions ware replaced\u3001when i 2nd try.Good luck."
      },
      {
        "date": "2023-03-03T03:20:00.000Z",
        "voteCount": 1,
        "content": "Here they need to convince why the effort is worthwhile - to prove that you will talk on enhancements hence C. All of these lead to cost reduction/saving eventually."
      },
      {
        "date": "2022-12-14T04:02:00.000Z",
        "voteCount": 1,
        "content": "C is the correct answer"
      },
      {
        "date": "2022-11-18T01:28:00.000Z",
        "voteCount": 2,
        "content": "I had the same question on the exam, my choice was A, passed"
      },
      {
        "date": "2022-11-16T03:17:00.000Z",
        "voteCount": 1,
        "content": "C is ok"
      },
      {
        "date": "2022-10-28T12:51:00.000Z",
        "voteCount": 1,
        "content": "C most definitely. Following the best practice."
      },
      {
        "date": "2022-07-17T21:26:00.000Z",
        "voteCount": 2,
        "content": "C is better: Management like to understand business benefit in simple language. They don't  bother about the technology"
      },
      {
        "date": "2023-09-19T06:52:00.000Z",
        "voteCount": 1,
        "content": "Agree with you. Isn't A the simplest answer. It says all the same things as C, just without any technicality."
      },
      {
        "date": "2022-07-03T09:41:00.000Z",
        "voteCount": 3,
        "content": "C clearly states all of the benefits, C is right."
      },
      {
        "date": "2022-05-21T05:16:00.000Z",
        "voteCount": 2,
        "content": "There is very less discussion for this question.\n.I thing we are forgetting reliablity or HA.\nI am voting for B"
      },
      {
        "date": "2022-06-02T22:05:00.000Z",
        "voteCount": 1,
        "content": "HA is not mentioned in the question, reliability is, but decoupling the architecture improves reliability."
      },
      {
        "date": "2022-05-04T00:42:00.000Z",
        "voteCount": 2,
        "content": "Devs will talk to their managers - so more technically, then the managers will talk to their managers (maybe CxO level) and they will talk 'money'. So I think the correct answer is C but A is still quite possible in very flat organizations. Again the question is asked in some context which is not expressed :("
      },
      {
        "date": "2022-05-04T00:45:00.000Z",
        "voteCount": 1,
        "content": "and CxOs are not interested in 'easier' or 'automatically' which are parts of A answer, the job to make things easier/less cumbersome are on the low level managers or leads of the streams :)"
      },
      {
        "date": "2022-04-21T19:20:00.000Z",
        "voteCount": 2,
        "content": "C  to decouple infrastructure from application,   the development team want a fully managed service approach"
      },
      {
        "date": "2022-04-14T09:52:00.000Z",
        "voteCount": 2,
        "content": "C. \"develop and release new features\". Work as a dev in any environment. All leadership cares about is \"first to market\" and differentiators."
      },
      {
        "date": "2022-02-11T13:48:00.000Z",
        "voteCount": 5,
        "content": "I got similar question on my exam. Answered C."
      },
      {
        "date": "2022-01-19T09:24:00.000Z",
        "voteCount": 2,
        "content": "Got this question in my exam, answered C"
      },
      {
        "date": "2022-01-24T07:00:00.000Z",
        "voteCount": 6,
        "content": "do you have the same response to every question"
      },
      {
        "date": "2022-01-08T00:35:00.000Z",
        "voteCount": 4,
        "content": "I vote A.\nManagement is concerned about costs.\nA is the only option that touches on costs."
      },
      {
        "date": "2021-12-08T20:52:00.000Z",
        "voteCount": 1,
        "content": "The answer should be between A or C. Choice is to be made between cost effective or Scaling    respectively. They have to convince leadership on the effort and not cost so I think we can go for C."
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 139,
    "url": "https://www.examtopics.com/discussions/google/view/56706-exam-professional-cloud-architect-topic-1-question-139/",
    "body": "Your team is developing a web application that will be deployed on Google Kubernetes Engine (GKE). Your CTO expects a successful launch and you need to ensure your application can handle the expected load of tens of thousands of users. You want to test the current deployment to ensure the latency of your application stays below a certain threshold. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a load testing tool to simulate the expected number of concurrent users and total requests to your application, and inspect the results.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable autoscaling on the GKE cluster and enable horizontal pod autoscaling on your application deployments. Send curl requests to your application, and validate if the auto scaling works.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReplicate the application over multiple GKE clusters in every Google Cloud region. Configure a global HTTP(S) load balancer to expose the different clusters over a single global IP address.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Debugger in the development environment to understand the latency between the different microservices."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 29,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-07-14T20:02:00.000Z",
        "voteCount": 22,
        "content": "21 NEw Qs -  July 12, 2021\n# 15.\tAn application development team has come to you for advice.They are planning to write and deploy an HTTP(S) API using Go 1.12. The API will have a very unpredictable workload and must remain reliable during peaks in traffic. They want to minimize operational overhead for this application. What approach should you recommend?\n\na.\tUse a Managed Instance Group when deploying to Compute Engine\nb.\tDevelop an application with containers, and deploy to Google Kubernetes Engine (GKE)\nc.\tDevelop the application for App Engine standard environment\nd.\tDevelop the application for App Engine Flexible environment using a custom runtime\n\nAnswer C, ,  please share you answers"
      },
      {
        "date": "2021-07-16T09:13:00.000Z",
        "voteCount": 5,
        "content": "C is ok."
      },
      {
        "date": "2021-08-23T17:15:00.000Z",
        "voteCount": 1,
        "content": "Answer A"
      },
      {
        "date": "2021-08-23T17:15:00.000Z",
        "voteCount": 1,
        "content": "Sorry C"
      },
      {
        "date": "2021-11-17T05:43:00.000Z",
        "voteCount": 2,
        "content": "Answer is C"
      },
      {
        "date": "2022-04-14T10:00:00.000Z",
        "voteCount": 2,
        "content": "C. Unpredictable workload. Flexible does not scale nearly as fast as standard."
      },
      {
        "date": "2021-07-26T05:19:00.000Z",
        "voteCount": 13,
        "content": "Anyone can tell please if at the new exam there are also questions from the old set(before question 115)?"
      },
      {
        "date": "2021-09-15T21:06:00.000Z",
        "voteCount": 4,
        "content": "old ones are not  removed"
      },
      {
        "date": "2021-09-29T12:40:00.000Z",
        "voteCount": 2,
        "content": "really ? the old ones are are still on exam ? from 1-100 ? how about old case study questions ?"
      },
      {
        "date": "2024-06-07T08:35:00.000Z",
        "voteCount": 1,
        "content": "Test current status, load test tools only purpouse is to test load and consider success/failure based on latency / response times."
      },
      {
        "date": "2024-01-18T19:27:00.000Z",
        "voteCount": 1,
        "content": "A is correct, jmeter, k6"
      },
      {
        "date": "2024-01-16T14:52:00.000Z",
        "voteCount": 1,
        "content": "IMO, something here should be a multiple-choice option, and the examinee should have selected A and B. Otherwise, I am picking option A."
      },
      {
        "date": "2023-06-16T04:24:00.000Z",
        "voteCount": 2,
        "content": "Admin should really deleting the wrong answers.. I think 90% of people here just copying explanations and really have no idea what they are talking about. every second question has different answer with different \"voting\"answer.."
      },
      {
        "date": "2023-06-10T05:13:00.000Z",
        "voteCount": 1,
        "content": "Using Curl seems weird, how is Curl going to inject the load?  Curl would be fine if we just wanted to test the underlying latency of the system."
      },
      {
        "date": "2023-04-18T18:57:00.000Z",
        "voteCount": 1,
        "content": "It is A.\nYou want to TEST the deployment, not changing anything (yet)."
      },
      {
        "date": "2023-03-09T06:44:00.000Z",
        "voteCount": 4,
        "content": "The correct answer is: A. Use a load testing tool to simulate the expected number of concurrent users and total requests to your application, and inspect the results.\n\nA load testing tool can be used to simulate the expected number of concurrent users and total requests to your application. This will allow you to test how your application handles the expected load and to identify any potential problems.\n\nEnabling autoscaling on the GKE cluster and enabling horizontal pod autoscaling on your application deployments will not help you to test the latency of your application. This will only help to ensure that your application can handle the expected load."
      },
      {
        "date": "2023-02-23T17:22:00.000Z",
        "voteCount": 1,
        "content": "is curl returns latency info?"
      },
      {
        "date": "2022-12-14T04:05:00.000Z",
        "voteCount": 1,
        "content": "A is the correct answer"
      },
      {
        "date": "2022-12-06T14:58:00.000Z",
        "voteCount": 2,
        "content": "Also, there is no expected numbers, the users can be tens of thousands so whatever testing you do with A may not be sufficient so it's better to keep autoscaling for whatsoever load comes in and curl test ensures autoscaling happens when required"
      },
      {
        "date": "2022-12-06T14:55:00.000Z",
        "voteCount": 2,
        "content": "The question specifically asks that your CTO expects a successful launch and you need to ensure your application can handle the expected load of tens of thousands of users. In A, you are just testing and not taking an action. In B, you are not only testing with curl commands to check for latency but also taking action to enable the cluster to acquire more resources. So, I will go with B."
      },
      {
        "date": "2022-11-16T03:21:00.000Z",
        "voteCount": 1,
        "content": "A is ok"
      },
      {
        "date": "2022-10-19T12:31:00.000Z",
        "voteCount": 1,
        "content": "A is the correct, no load ==&gt; no latency checking"
      },
      {
        "date": "2022-10-15T06:01:00.000Z",
        "voteCount": 1,
        "content": "Ans A: the question asks you want to test"
      },
      {
        "date": "2022-09-11T14:30:00.000Z",
        "voteCount": 1,
        "content": "I was going for B.\nHowever analyzing question it says \"test the current deployment to ensure the latency\"\nThe only test answer is the stress test / performance test simulating the users workload."
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 140,
    "url": "https://www.examtopics.com/discussions/google/view/60396-exam-professional-cloud-architect-topic-1-question-140/",
    "body": "Your company has a Kubernetes application that pulls messages from Pub/Sub and stores them in Filestore. Because the application is simple, it was deployed as a single pod. The infrastructure team has analyzed Pub/Sub metrics and discovered that the application cannot process the messages in real time. Most of them wait for minutes before being processed. You need to scale the elaboration process that is I/O-intensive. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse kubectl autoscale deployment APP_NAME --max 6 --min 2 --cpu-percent 50 to configure Kubernetes autoscaling deployment.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a Kubernetes autoscaling deployment based on the subscription/push_request_latencies metric.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the --enable-autoscaling flag when you create the Kubernetes cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a Kubernetes autoscaling deployment based on the subscription/num_undelivered_messages metric.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 46,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 7,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-09-04T12:46:00.000Z",
        "voteCount": 38,
        "content": "Answer is D.  num_undelivered_messages metric can indicate if subscribers are keeping up with message submissions. \n https://cloud.google.com/pubsub/docs/monitoring#monitoring_the_backlog"
      },
      {
        "date": "2021-09-21T10:10:00.000Z",
        "voteCount": 2,
        "content": "D is correct !"
      },
      {
        "date": "2022-11-26T09:05:00.000Z",
        "voteCount": 2,
        "content": "The provided link is not relevant to kubernetes, but pertains to cloud pub/sub....the num_undelivered_messages metric is not available for kubernetes autoscaling...C is correct"
      },
      {
        "date": "2024-05-24T23:11:00.000Z",
        "voteCount": 1,
        "content": "Kubernetes allow autoscaling based on external metrics"
      },
      {
        "date": "2023-01-23T21:24:00.000Z",
        "voteCount": 4,
        "content": "https://cloud.google.com/kubernetes-engine/docs/tutorials/autoscaling-metrics#pubsub"
      },
      {
        "date": "2022-09-02T22:34:00.000Z",
        "voteCount": 22,
        "content": "Direct answer - D\n\nhttps://cloud.google.com/kubernetes-engine/docs/samples/container-pubsub-horizontal-pod-autoscaler\n\napiVersion: autoscaling/v2beta2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: pubsub\nspec:\n  minReplicas: 1\n  maxReplicas: 5\n  metrics:\n  - external:\n      metric:\n       name: pubsub.googleapis.com|subscription|num_undelivered_messages\n       selector:\n         matchLabels:\n           resource.labels.subscription_id: echo-read\n      target:\n        type: AverageValue\n        averageValue: 2\n    type: External\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: pubsub"
      },
      {
        "date": "2024-07-26T18:07:00.000Z",
        "voteCount": 1,
        "content": "Subscription Metric: Scaling based on the subscription/num_undelivered_messages metric directly ties the scaling behavior to the number of unprocessed messages in Pub/Sub. This ensures that your application scales out when there are more messages to process and scales in when the queue is short.\nRelevant Metric: This metric is relevant for an I/O-intensive application that processes messages from Pub/Sub, ensuring that the scaling is directly responsive to the message processing demand."
      },
      {
        "date": "2024-03-04T07:59:00.000Z",
        "voteCount": 1,
        "content": "D is the correct answer: subscription/num_undelivered_messages directly indicates the number of messages waiting to be processed, making it a perfect indicator of the workload on the application."
      },
      {
        "date": "2023-09-28T02:19:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/kubernetes-engine/docs/tutorials/autoscaling-metrics#pubsub\nhere . so this is D"
      },
      {
        "date": "2023-09-07T04:29:00.000Z",
        "voteCount": 1,
        "content": "Option C (--enable-autoscaling flag for the entire cluster): Enabling autoscaling at the cluster level doesn't provide fine-grained control over scaling individual pods or deployments based on specific workload metrics."
      },
      {
        "date": "2023-09-01T09:36:00.000Z",
        "voteCount": 2,
        "content": "Can someone tell that these answers are right like people voted D but answer is C."
      },
      {
        "date": "2023-06-27T18:06:00.000Z",
        "voteCount": 2,
        "content": "Answer D"
      },
      {
        "date": "2023-06-10T05:52:00.000Z",
        "voteCount": 1,
        "content": "B &amp; C refer to using auto scaler with custom metrics but\n\n\"Cluster autoscaler makes these scaling decisions based on the resource requests (rather than actual resource utilization) of Pods running on that node pool's nodes. \"\n\nA makes no sense as it defines a min and max that we don't know\n\nD seems like part of the solution so D."
      },
      {
        "date": "2023-04-15T11:06:00.000Z",
        "voteCount": 1,
        "content": "is D - https://cloud.google.com/kubernetes-engine/docs/tutorials/autoscaling-metrics#pubsub"
      },
      {
        "date": "2023-01-12T03:22:00.000Z",
        "voteCount": 2,
        "content": "I vot for B. Configure a Kubernetes autoscaling deployment based on the subscription/push_request_latencies metric. as the metric should be based on latency instead of num_undelivered_messages metric in D."
      },
      {
        "date": "2023-01-31T14:03:00.000Z",
        "voteCount": 4,
        "content": "The problems says PULL request and B is related to PUSH request. I do not think it is related."
      },
      {
        "date": "2022-12-14T04:07:00.000Z",
        "voteCount": 1,
        "content": "D is the correct answer"
      },
      {
        "date": "2022-12-03T05:43:00.000Z",
        "voteCount": 1,
        "content": "D is the correct one"
      },
      {
        "date": "2022-11-16T03:23:00.000Z",
        "voteCount": 1,
        "content": "D is ok"
      },
      {
        "date": "2022-11-01T01:57:00.000Z",
        "voteCount": 2,
        "content": "D\n\nhttps://cloud.google.com/kubernetes-engine/docs/tutorials/autoscaling-metrics#pubsub"
      },
      {
        "date": "2022-08-24T02:41:00.000Z",
        "voteCount": 2,
        "content": "This seems relevant \nhttps://cloud.google.com/kubernetes-engine/docs/tutorials/autoscaling-metrics#pubsub\neven if it uses Deployment + HorizontalPodAutoscaler which is not mentioned in the context of the question/answer"
      },
      {
        "date": "2022-08-16T05:07:00.000Z",
        "voteCount": 2,
        "content": "I think it wrong for application to be in pod it should be a deployment, and deployment would scale"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 141,
    "url": "https://www.examtopics.com/discussions/google/view/60698-exam-professional-cloud-architect-topic-1-question-141/",
    "body": "Your company is developing a web-based application. You need to make sure that production deployments are linked to source code commits and are fully auditable. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMake sure a developer is tagging the code commit with the date and time of commit.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMake sure a developer is adding a comment to the commit that links to the deployment.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMake the container tag match the source code commit hash.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMake sure the developer is tagging the commits with latest."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 43,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-08-26T02:08:00.000Z",
        "voteCount": 35,
        "content": "Developer shouldn't tag or comment every commit with some specific data, like timestamps or something else. There might be an app version, but it's not mentioned. I'd go with C as it's an automated, error-less approach that answers the question."
      },
      {
        "date": "2021-12-18T13:33:00.000Z",
        "voteCount": 2,
        "content": "@Kopper2019- what do you think about ans C?"
      },
      {
        "date": "2021-08-28T02:17:00.000Z",
        "voteCount": 16,
        "content": "C. Make the container tag match the source code commit hash."
      },
      {
        "date": "2021-09-11T04:13:00.000Z",
        "voteCount": 2,
        "content": "Not sure how the container tag match with the commit will help to audit, can someone explain?"
      },
      {
        "date": "2021-11-28T08:50:00.000Z",
        "voteCount": 10,
        "content": "if you got the commit hash from the container you can check the corresponding commit in the git repository. So the change, that was made and deployed into your environment can be audited."
      },
      {
        "date": "2024-06-19T09:44:00.000Z",
        "voteCount": 1,
        "content": "Linking Deployments to Commits: By tagging the container image with the source code commit hash, you create a direct link between the deployed container and the specific state of the source code. This provides a clear and auditable trail from the deployed application back to the exact source code that was used to build it.\n\nAuditability: Using the commit hash as the container tag ensures that each deployment can be traced back to a unique and immutable source code commit. This makes it easy to audit deployments and verify which version of the code is running in production."
      },
      {
        "date": "2023-09-07T04:31:00.000Z",
        "voteCount": 1,
        "content": "Can't fathom A. This is what ChatGPT says about A - I agree to this.\nOption A (tagging with date and time): Using date and time as tags may not be precise enough to identify the exact code version associated with a deployment, especially if multiple commits occurred within the same time window."
      },
      {
        "date": "2023-06-10T05:55:00.000Z",
        "voteCount": 1,
        "content": "Really C should say image?\nWe have to seperate systems: source code repo &amp; container repo.\nHow do we link the two together?  C is the only attempt at solving the problem."
      },
      {
        "date": "2023-01-12T03:26:00.000Z",
        "voteCount": 1,
        "content": "Agreed with C instead of A with them."
      },
      {
        "date": "2022-12-14T04:09:00.000Z",
        "voteCount": 1,
        "content": "C is the correct answer"
      },
      {
        "date": "2022-11-23T05:41:00.000Z",
        "voteCount": 1,
        "content": "Answer is C"
      },
      {
        "date": "2022-11-16T03:24:00.000Z",
        "voteCount": 2,
        "content": "C is ok"
      },
      {
        "date": "2022-10-19T12:40:00.000Z",
        "voteCount": 4,
        "content": "C is correct \"By design, the Git commit hash is immutable and references a specific version of your software.\" as per https://cloud.google.com/architecture/best-practices-for-building-containers#tagging_using_the_git_commit_hash"
      },
      {
        "date": "2022-09-17T21:14:00.000Z",
        "voteCount": 5,
        "content": "C is the answer.\n\nhttps://cloud.google.com/architecture/best-practices-for-building-containers#tagging_using_the_git_commit_hash\nYou can use this commit hash as a version number for your software, but also as a tag for the Docker image built from this specific version of your software. Doing so makes Docker images traceable: because in this case the image tag is immutable, you instantly know which specific version of your software is running inside a given container."
      },
      {
        "date": "2022-07-03T09:55:00.000Z",
        "voteCount": 3,
        "content": "Every Git commit with timestamp A doesn't make since. C is right"
      },
      {
        "date": "2022-06-09T04:08:00.000Z",
        "voteCount": 4,
        "content": "No manual intervention is preferred in automatic deployments. Only automating the container tag to match the commit hash will be fully auditable with the help of the scm."
      },
      {
        "date": "2022-05-04T11:33:00.000Z",
        "voteCount": 7,
        "content": "From: https://cloud.google.com/architecture/best-practices-for-building-containers\nUnder: Tagging using the Git commit hash  (bottom of page almost)\n\n\"In this case, a common way of handling version numbers is to use the Git commit SHA-1 hash (or a short version of it) as the version number. By design, the Git commit hash is immutable and references a specific version of your software.\n\nYou can use this commit hash as a version number for your software, but also as a tag for the Docker image built from this specific version of your software. Doing so makes Docker images traceable: because in this case the image tag is immutable, you instantly know which specific version of your software is running inside a given container.\""
      },
      {
        "date": "2022-03-02T00:21:00.000Z",
        "voteCount": 4,
        "content": "It's got to be A. Option C talks about containers whereas there is no mention of containers in the question."
      },
      {
        "date": "2022-02-11T13:50:00.000Z",
        "voteCount": 5,
        "content": "I got similar question on my exam. Answered C."
      },
      {
        "date": "2022-01-22T06:25:00.000Z",
        "voteCount": 5,
        "content": "I think answer is A.\n\nIn Git, tag is used to mark release points (v1.0, v2.0 and so on). You can tag the release based on the time stamp and using  git show &lt;tag-name&gt; command, you can see the commit detailed history. \n\nReference: https://git-scm.com/book/en/v2/Git-Basics-Tagging\n\nC could be the correct answer for the case if you are going with container based solution which is not mentioned anywhere in the question."
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 142,
    "url": "https://www.examtopics.com/discussions/google/view/60437-exam-professional-cloud-architect-topic-1-question-142/",
    "body": "An application development team has come to you for advice. They are planning to write and deploy an HTTP(S) API using Go 1.12. The API will have a very unpredictable workload and must remain reliable during peaks in traffic. They want to minimize operational overhead for this application. Which approach should you recommend?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDevelop the application with containers, and deploy to Google Kubernetes Engine.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDevelop the application for App Engine standard environment.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a Managed Instance Group when deploying to Compute Engine.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDevelop the application for App Engine flexible environment, using a custom runtime."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 26,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-08-25T06:49:00.000Z",
        "voteCount": 21,
        "content": "B is ok"
      },
      {
        "date": "2021-08-23T21:07:00.000Z",
        "voteCount": 12,
        "content": "B is ok.\nhttps://cloud.google.com/appengine/docs/the-appengine-environments"
      },
      {
        "date": "2021-09-18T23:33:00.000Z",
        "voteCount": 4,
        "content": "Intended to run for free or at very low cost, where you pay only for what you need and when you need it. For example, your application can scale to 0 instances when there is no traffic.\n\nExperiences sudden and extreme spikes of traffic which require immediate scaling."
      },
      {
        "date": "2021-09-18T23:32:00.000Z",
        "voteCount": 7,
        "content": "Source code is written in specific versions of the supported programming languages:\nPython 2.7, Python 3.7, Python 3.8, Python 3.9\nJava 8, Java 11\nNode.js 10, Node.js 12, Node.js 14, Node.js 16 (preview)\nPHP 5.5, PHP 7.2, PHP 7.3, and PHP 7.4\nRuby 2.5, Ruby 2.6, and Ruby 2.7\nGo 1.11, Go 1.12, Go 1.13, Go 1.14, Go 1.15, and Go 1.16 (preview)"
      },
      {
        "date": "2024-03-12T21:05:00.000Z",
        "voteCount": 3,
        "content": "Both B and D are okay, however, the need for App engine Flexible environment is not required unless you want to run docker containers, have more control over the instance used and so on, hence in this case B works well. \nhttps://cloud.google.com/appengine/docs/the-appengine-environments"
      },
      {
        "date": "2023-11-28T02:43:00.000Z",
        "voteCount": 1,
        "content": "A\nGKE is much reliable compared to the other options provided here."
      },
      {
        "date": "2023-09-19T07:21:00.000Z",
        "voteCount": 1,
        "content": "B is correct. It supports Go 1.12, and can handle sudden spikes.\nhttps://cloud.google.com/appengine/docs/the-appengine-environments"
      },
      {
        "date": "2023-07-03T17:20:00.000Z",
        "voteCount": 1,
        "content": "Can someone explain why we cannot use AppEngine Flexible environment ?"
      },
      {
        "date": "2024-02-13T00:18:00.000Z",
        "voteCount": 1,
        "content": "I guess it can't scale down to 0."
      },
      {
        "date": "2023-12-14T21:45:00.000Z",
        "voteCount": 1,
        "content": "Bcs They want to minimize operational overhead for this application"
      },
      {
        "date": "2023-06-27T18:09:00.000Z",
        "voteCount": 1,
        "content": "App engine standard provides go env."
      },
      {
        "date": "2023-03-03T10:46:00.000Z",
        "voteCount": 1,
        "content": "App Engine Std. Can run this Go version and Scales to 0."
      },
      {
        "date": "2023-02-12T17:12:00.000Z",
        "voteCount": 1,
        "content": "Standard AppEngine Environment supports Go 1.2. The AppEngine can be low cost if no or low traffic. It has free quotas."
      },
      {
        "date": "2023-01-31T20:42:00.000Z",
        "voteCount": 1,
        "content": "AppEngine scales well, only dev effort. No infrastructure. go is supported in the standard distribution."
      },
      {
        "date": "2023-01-01T11:58:00.000Z",
        "voteCount": 1,
        "content": "The answer is A. B option is not correct. It is not recommended to use App Engine Standard environment for an HTTP(S) API with a very unpredictable workload because App Engine Standard environment has certain limitations and constraints that may not be suitable for an API with an unpredictable workload. For example, App Engine Standard environment has a maximum request timeout of 60 seconds, which may not be sufficient for an API with a very unpredictable workload."
      },
      {
        "date": "2022-11-16T03:27:00.000Z",
        "voteCount": 1,
        "content": "B is ok"
      },
      {
        "date": "2022-07-03T10:00:00.000Z",
        "voteCount": 2,
        "content": "B is correct ..https://cloud.google.com/appengine/docs/the-appengine-environments\n\nExperiences sudden and extreme spikes of traffic which require immediate scaling."
      },
      {
        "date": "2022-06-09T04:11:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/appengine/docs/the-appengine-environments App engine standard environment support go 1.13 and also handles the unpredictable load."
      },
      {
        "date": "2022-02-20T07:06:00.000Z",
        "voteCount": 2,
        "content": "B. Unpredictable traffic &amp; low overhead."
      },
      {
        "date": "2022-01-12T12:39:00.000Z",
        "voteCount": 6,
        "content": "App Engine standard has autoscaling out of the box, supports Go 1.12 and can scale down to 0 to save money"
      },
      {
        "date": "2022-01-10T07:01:00.000Z",
        "voteCount": 1,
        "content": "B is ok."
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 143,
    "url": "https://www.examtopics.com/discussions/google/view/60682-exam-professional-cloud-architect-topic-1-question-143/",
    "body": "Your company is designing its data lake on Google Cloud and wants to develop different ingestion pipelines to collect unstructured data from different sources.<br>After the data is stored in Google Cloud, it will be processed in several data pipelines to build a recommendation engine for end users on the website. The structure of the data retrieved from the source systems can change at any time. The data must be stored exactly as it was retrieved for reprocessing purposes in case the data structure is incompatible with the current processing pipelines. You need to design an architecture to support the use case after you retrieve the data. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSend the data through the processing pipeline, and then store the processed data in a BigQuery table for reprocessing.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the data in a BigQuery table. Design the processing pipelines to retrieve the data from the table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSend the data through the processing pipeline, and then store the processed data in a Cloud Storage bucket for reprocessing.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the data in a Cloud Storage bucket. Design the processing pipelines to retrieve the data from the bucket.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-08-25T23:43:00.000Z",
        "voteCount": 28,
        "content": "D is ok\nThe data needs to be stored as it is retrieved. This would mean that any processing should be done after it is stored."
      },
      {
        "date": "2021-10-22T22:54:00.000Z",
        "voteCount": 6,
        "content": "D, store RAW unstructured data as-is in Cloud Storage, and then define how to process it.\nClassical Data Lake ELT (Extract -&gt; Load -&gt; Transform )"
      },
      {
        "date": "2024-04-25T23:42:00.000Z",
        "voteCount": 1,
        "content": "D\nUnstructured data - GCS\nData stored axactly as it was retrieved - store before processing"
      },
      {
        "date": "2023-12-14T21:54:00.000Z",
        "voteCount": 1,
        "content": "Key word is \"The data must be stored exactly as it was retrieved for reprocessing purposes in case the data structure is incompatible with the current processing pipelines.\" and hence D"
      },
      {
        "date": "2023-08-22T13:40:00.000Z",
        "voteCount": 2,
        "content": "D. It aligns with an example in the Cloud Architecture Framework\nhttps://cloud.google.com/architecture/big-data-analytics/analytics-lakehouse"
      },
      {
        "date": "2023-03-10T12:35:00.000Z",
        "voteCount": 1,
        "content": "What is the point of data being in a lake and then being dumped into GCS without processing. What purpose is served with GCS being a copy of lake?"
      },
      {
        "date": "2023-04-02T04:27:00.000Z",
        "voteCount": 3,
        "content": "here gcs is the lake. Not a copy.\nThe data warehouse will be what comes out of the pipelines."
      },
      {
        "date": "2022-11-16T03:31:00.000Z",
        "voteCount": 2,
        "content": "D is ok"
      },
      {
        "date": "2022-11-02T12:41:00.000Z",
        "voteCount": 2,
        "content": "D is ok"
      },
      {
        "date": "2022-09-17T08:58:00.000Z",
        "voteCount": 2,
        "content": "D is ok\nThe data needs to be stored as it is retrieved. This would mean that any processing should be done after it is stored in GCS."
      },
      {
        "date": "2022-07-03T10:05:00.000Z",
        "voteCount": 1,
        "content": "storing and retrieving data in cloud storage solve the purpose of this use case. D is perfect answer."
      },
      {
        "date": "2022-03-30T10:41:00.000Z",
        "voteCount": 1,
        "content": "Although... wouldn't be Bigtable or Datastore better than GCS?"
      },
      {
        "date": "2022-07-22T05:00:00.000Z",
        "voteCount": 2,
        "content": "Both BigTable and DataStore are NoSQL Databases, qns mentioned that data structure may change anytime"
      },
      {
        "date": "2022-02-11T13:50:00.000Z",
        "voteCount": 4,
        "content": "I got similar question on my exam. Answered D."
      },
      {
        "date": "2022-01-19T09:24:00.000Z",
        "voteCount": 4,
        "content": "Got this question in my exam, answered D"
      },
      {
        "date": "2022-01-10T07:11:00.000Z",
        "voteCount": 2,
        "content": "D is ok"
      },
      {
        "date": "2021-12-05T03:27:00.000Z",
        "voteCount": 1,
        "content": "D is the correct answer"
      },
      {
        "date": "2021-11-27T07:39:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2021-11-20T07:36:00.000Z",
        "voteCount": 2,
        "content": "D - Data must be stored as it is before and after so use Cloud storage and then build pipelines as needed."
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 144,
    "url": "https://www.examtopics.com/discussions/google/view/60743-exam-professional-cloud-architect-topic-1-question-144/",
    "body": "You are responsible for the Google Cloud environment in your company. Multiple departments need access to their own projects, and the members within each department will have the same project responsibilities. You want to structure your Google Cloud environment for minimal maintenance and maximum overview of<br>IAM permissions as each department's projects start and end. You want to follow Google-recommended practices. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGrant all department members the required IAM permissions for their respective projects.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Google Group per department and add all department members to their respective groups. Create a folder per department and grant the respective group the required IAM permissions at the folder level. Add the projects under the respective folders.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a folder per department and grant the respective members of the department the required IAM permissions at the folder level. Structure all projects for each department under the respective folders.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Google Group per department and add all department members to their respective groups. Grant each group the required IAM permissions for their respective projects."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-06T20:30:00.000Z",
        "voteCount": 16,
        "content": "it's B"
      },
      {
        "date": "2021-08-28T02:10:00.000Z",
        "voteCount": 10,
        "content": "B. Create a Google Group per department and add all department members to their respective groups. Create a folder per department and grant the respective group the required IAM permissions at the folder level. Add the projects under the respective folders."
      },
      {
        "date": "2024-05-10T14:22:00.000Z",
        "voteCount": 1,
        "content": "it's B"
      },
      {
        "date": "2022-11-16T03:34:00.000Z",
        "voteCount": 1,
        "content": "B is ok"
      },
      {
        "date": "2022-09-17T23:02:00.000Z",
        "voteCount": 4,
        "content": "B is the answer.\n\nhttps://cloud.google.com/resource-manager/docs/access-control-folders#best-practices-folders-iam\nUse groups whenever possible to manage principals.\n\nhttps://cloud.google.com/resource-manager/docs/creating-managing-folders\nA folder can contain projects, other folders, or a combination of both. Organizations can use folders to group projects under the organization node in a hierarchy. For example, your organization might contain multiple departments, each with its own set of Google Cloud resources. Folders allow you to group these resources on a per-department basis."
      },
      {
        "date": "2022-09-17T08:59:00.000Z",
        "voteCount": 1,
        "content": "B is most appropriate for the use case and principle of least privilege."
      },
      {
        "date": "2022-07-03T10:10:00.000Z",
        "voteCount": 1,
        "content": "B is most appropriate for the use case and principle of least privilege."
      },
      {
        "date": "2022-05-16T10:54:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2022-01-10T07:20:00.000Z",
        "voteCount": 1,
        "content": "B is ok"
      },
      {
        "date": "2021-12-22T00:44:00.000Z",
        "voteCount": 5,
        "content": "B is ideal for minimal maintenance and maximum overview of IAM permissions as each department's projects start and end.\nManage the users inside Groups will turn it easer."
      },
      {
        "date": "2021-12-08T21:32:00.000Z",
        "voteCount": 1,
        "content": "Voted B"
      },
      {
        "date": "2021-12-05T03:32:00.000Z",
        "voteCount": 1,
        "content": "B is the correct answer"
      },
      {
        "date": "2021-11-30T03:16:00.000Z",
        "voteCount": 2,
        "content": "Vote B"
      },
      {
        "date": "2021-10-21T00:59:00.000Z",
        "voteCount": 2,
        "content": "I would select B."
      },
      {
        "date": "2021-10-01T21:02:00.000Z",
        "voteCount": 4,
        "content": "B is correct, folder restructure per department and IAM permission for Group is recommended."
      },
      {
        "date": "2021-09-19T18:12:00.000Z",
        "voteCount": 2,
        "content": "Answer is B"
      },
      {
        "date": "2021-09-03T09:33:00.000Z",
        "voteCount": 4,
        "content": "Yes, B"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 145,
    "url": "https://www.examtopics.com/discussions/google/view/60438-exam-professional-cloud-architect-topic-1-question-145/",
    "body": "Your company has an application running as a Deployment in a Google Kubernetes Engine (GKE) cluster. You have separate clusters for development, staging, and production. You have discovered that the team is able to deploy a Docker image to the production cluster without first testing the deployment in development and then staging. You want to allow the team to have autonomy but want to prevent this from happening. You want a Google Cloud solution that can be implemented quickly with minimal effort. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a Kubernetes lifecycle hook to prevent the container from starting if it is not approved for usage in the given environment.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement a corporate policy to prevent teams from deploying Docker images to an environment unless the Docker image was tested in an earlier environment.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure binary authorization policies for the development, staging, and production clusters. Create attestations as part of the continuous integration pipeline.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Kubernetes admissions controller to prevent the container from starting if it is not approved for usage in the given environment."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 13,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-03T09:37:00.000Z",
        "voteCount": 15,
        "content": "C is s fine."
      },
      {
        "date": "2022-02-11T13:52:00.000Z",
        "voteCount": 10,
        "content": "I got similar question on my exam. Answered C."
      },
      {
        "date": "2023-03-11T16:54:00.000Z",
        "voteCount": 2,
        "content": "C it is"
      },
      {
        "date": "2022-12-25T23:20:00.000Z",
        "voteCount": 10,
        "content": "A good option for quickly implementing a solution to prevent deployments to the production cluster without first testing in development and staging would be to configure binary authorization policies for the development, staging, and production clusters. You can then create attestations as part of the continuous integration pipeline.\n\nOption C, \"Configure binary authorization policies for the development, staging, and production clusters. Create attestations as part of the continuous integration pipeline,\" would be the correct choice for this scenario.\n\nBinary authorization is a feature of Google Kubernetes Engine that allows you to enforce policies on the images that are deployed to your clusters. By configuring binary authorization policies for the development, staging, and production clusters, you can ensure that only images that have been attested by an authorized entity are allowed to be deployed to those clusters. You can create the attestations as part of the continuous integration pipeline, which will allow you to verify that the image has been tested before it is deployed to the next environment."
      },
      {
        "date": "2022-12-25T23:20:00.000Z",
        "voteCount": 1,
        "content": "Option A, \"Configure a Kubernetes lifecycle hook to prevent the container from starting if it is not approved for usage in the given environment,\" would not be a good choice because it would not prevent the deployment of the container to the cluster in the first place.\n\nOption D, \"Create a Kubernetes admissions controller to prevent the container from starting if it is not approved for usage in the given environment,\" would also not be a good choice because it would not prevent the deployment of the container to the cluster in the first place.\n\nOption B, \"Implement a corporate policy to prevent teams from deploying Docker images to an environment unless the Docker image was tested in an earlier environment,\" would be a good option, but it would not be as effective as using binary authorization policies, as it would rely on the team following the policy rather than enforcing it automatically."
      },
      {
        "date": "2022-11-16T03:44:00.000Z",
        "voteCount": 1,
        "content": "C is ok"
      },
      {
        "date": "2022-07-30T14:39:00.000Z",
        "voteCount": 1,
        "content": "Why not A? Need something to be implemented quickly is what the q asks."
      },
      {
        "date": "2022-07-03T10:15:00.000Z",
        "voteCount": 4,
        "content": "C is right.. \n\nBinary Authorization implements a policy model, where a policy is a set of rules that governs the deployment of container images. Rules in a policy provide specific criteria that an image must satisfy before it can be deployed.\n\nFor more information about the Binary Authorization policy model and other concepts, see Key concepts."
      },
      {
        "date": "2022-07-03T10:16:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/binary-authorization/docs/overview#policy_model"
      },
      {
        "date": "2021-12-20T03:39:00.000Z",
        "voteCount": 1,
        "content": "where the case study questions are  available in this website?"
      },
      {
        "date": "2021-12-05T03:43:00.000Z",
        "voteCount": 6,
        "content": "C is the correct answer\nhttps://cloud.google.com/binary-authorization/docs/overview"
      },
      {
        "date": "2021-10-26T06:16:00.000Z",
        "voteCount": 1,
        "content": "C is fine"
      },
      {
        "date": "2021-10-21T01:00:00.000Z",
        "voteCount": 1,
        "content": "I think C is the correct answer."
      },
      {
        "date": "2021-10-01T21:03:00.000Z",
        "voteCount": 2,
        "content": "C is correct, binary authorization is the solution."
      },
      {
        "date": "2021-08-28T02:08:00.000Z",
        "voteCount": 2,
        "content": "C. Configure binary authorization policies for the development, staging, and production clusters. Create attestations as part of the continuous integration pipeline."
      },
      {
        "date": "2021-08-27T22:32:00.000Z",
        "voteCount": 2,
        "content": "C is ok"
      },
      {
        "date": "2021-08-25T07:05:00.000Z",
        "voteCount": 2,
        "content": "C is ok"
      },
      {
        "date": "2021-08-23T21:11:00.000Z",
        "voteCount": 3,
        "content": "Sorry, it's C : Configure binary authorization policies for the development, staging, and production clusters. Create attestations as part of the continuous integration pipeline."
      },
      {
        "date": "2021-08-23T21:10:00.000Z",
        "voteCount": 1,
        "content": "D is ok.\nhttps://cloud.google.com/binary-authorization/docs/overview"
      },
      {
        "date": "2021-09-18T23:38:00.000Z",
        "voteCount": 1,
        "content": "You meant C I guess"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 146,
    "url": "https://www.examtopics.com/discussions/google/view/60720-exam-professional-cloud-architect-topic-1-question-146/",
    "body": "Your company wants to migrate their 10-TB on-premises database export into Cloud Storage. You want to minimize the time it takes to complete this activity, the overall cost, and database load. The bandwidth between the on-premises environment and Google Cloud is 1 Gbps. You want to follow Google-recommended practices. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDevelop a Dataflow job to read data directly from the database and write it into Cloud Storage.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Data Transfer appliance to perform an offline migration.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a commercial partner ETL solution to extract the data from the on-premises database and upload it into Cloud Storage.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCompress the data and upload it with gsutil -m to enable multi-threaded copy."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 76,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 50,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 11,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-09-08T19:08:00.000Z",
        "voteCount": 98,
        "content": "This is pretty simple. \nTime to transfer using Transfer Appliance: 1-3 weeks (I've used it twice and had a 2-3 week turnaround total)\nTime to transfer using 1Gbps : 30 hours (https://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets) \n\nAnswer is D, using gsutil"
      },
      {
        "date": "2022-04-07T01:09:00.000Z",
        "voteCount": 3,
        "content": "Will that not increase the Database load?, one of the requirement is to reduce the load of the DB during this operation."
      },
      {
        "date": "2021-12-29T04:12:00.000Z",
        "voteCount": 3,
        "content": "If I can do it in 30hrs, why choose 1  week? i'd go with B"
      },
      {
        "date": "2021-12-29T07:32:00.000Z",
        "voteCount": 2,
        "content": "I mean I'd go with A rather...questions says to spend minimum time and we have 1Gbps to do 10Tb in 30hrs"
      },
      {
        "date": "2022-01-11T18:13:00.000Z",
        "voteCount": 2,
        "content": "Transfer appliance -A"
      },
      {
        "date": "2023-03-11T16:57:00.000Z",
        "voteCount": 5,
        "content": "Go home you are drunk"
      },
      {
        "date": "2021-12-01T06:39:00.000Z",
        "voteCount": 8,
        "content": "Not about time but \"Google-recommended practices\""
      },
      {
        "date": "2021-09-13T02:28:00.000Z",
        "voteCount": 3,
        "content": "This is the correct article to support this question but the article proves the transfer appliance is the correct answer. Right below the transfer calc chart is recommended amount of data for gsutil. Gsutil should be used for data transfer under 1 tb\n\n\u201cYour private data center to Google Cloud\tEnough bandwidth to meet your project deadline\nfor less than 1 TB of data\tgsutil\u201d"
      },
      {
        "date": "2021-09-27T23:27:00.000Z",
        "voteCount": 21,
        "content": "No perfect answer as B and D both have flaws. B is time latency as transfer appliance usually takes weeks; D gsutil applies for less than 1TB. The answer should be storage transfer service for on-premises data, which is not available here. \n\nIf have to choose one I go for B"
      },
      {
        "date": "2022-08-19T23:18:00.000Z",
        "voteCount": 1,
        "content": "Storage transfer service is for online data. It can't serve the purpose if you don't have the connectivity established between on prem and gcp. Which is what we can't assume ourselves in this question."
      },
      {
        "date": "2024-06-08T03:18:00.000Z",
        "voteCount": 1,
        "content": "D says compress data, \u00bfin a single file? it will be more than the limit 5 TB of gsutil, so it is B."
      },
      {
        "date": "2024-05-24T23:30:00.000Z",
        "voteCount": 1,
        "content": "I think the answer is B.\n\nThe main consideration is between B and D. Just thinking if they want the answer to be online transfer, they should have added Online Transfer Service instead of gsutils. Just guessing Google must want us to choose B :)"
      },
      {
        "date": "2024-05-19T02:39:00.000Z",
        "voteCount": 1,
        "content": "B fo sho"
      },
      {
        "date": "2024-05-10T14:26:00.000Z",
        "voteCount": 2,
        "content": "D will be most cost effective where as B will incur cost (question asking to consider cost effective solution as well) so D is my answer"
      },
      {
        "date": "2024-05-01T18:37:00.000Z",
        "voteCount": 2,
        "content": "Option B (Data Transfer appliance) is the best choice for efficient and cost-effective data migration while minimizing database load and transfer time. This solution bypasses network limitations and reduces the impact on the on-premises environment, making it ideal for migrating large data sets to the cloud."
      },
      {
        "date": "2024-04-23T21:50:00.000Z",
        "voteCount": 2,
        "content": "Compressing the data and uploading it with gsutil -m can be a good optimization for your transfer, but it has limitations to consider:\n\nCompression Overhead: While compressing the data can reduce upload size and potentially speed up transfer, the compression and decompression processes themselves take time and resources. Depending on your data type, the benefit of reduced size might be offset by the processing overhead.\nTransfer Appliance: The recommended approach with the Transfer Appliance already utilizes parallel transfers for faster uploads, potentially making gsutil -m less impactful.\nI will go with B"
      },
      {
        "date": "2024-04-23T21:49:00.000Z",
        "voteCount": 1,
        "content": "Compressing the data and uploading it with gsutil -m can be a good optimization for your transfer, but it has limitations to consider:\n\nCompression Overhead: While compressing the data can reduce upload size and potentially speed up transfer, the compression and decompression processes themselves take time and resources. Depending on your data type, the benefit of reduced size might be offset by the processing overhead.\nTransfer Appliance: The recommended approach with the Transfer Appliance already utilizes parallel transfers for faster uploads, potentially making gsutil -m less impactful.\nI will go with B"
      },
      {
        "date": "2024-04-01T02:32:00.000Z",
        "voteCount": 1,
        "content": "with 1 Gbps it will take only 30 hrs so best option is D"
      },
      {
        "date": "2024-03-16T16:07:00.000Z",
        "voteCount": 1,
        "content": "Option B and D are most feasible options\nOption B will be okay if the size of the data is too huge\nOption D will be good for a few TBs of data. I am assuming 10 TB will fit in this case.\n\nhttps://cloud.google.com/blog/topics/developers-practitioners/how-transfer-your-data-google-cloud"
      },
      {
        "date": "2024-03-01T10:56:00.000Z",
        "voteCount": 3,
        "content": "Answer B. Cp limit is 5 TB max"
      },
      {
        "date": "2024-02-07T01:44:00.000Z",
        "voteCount": 2,
        "content": "I chose D.\nAccording to the link below, 10TB of data can be transferred in 30h. The light blue area is the acceptable line for online transfer.\nhttps://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets?hl=ja#online_versus_offline_transfer"
      },
      {
        "date": "2024-06-08T03:17:00.000Z",
        "voteCount": 1,
        "content": "D says compress data, in a single file? it will be more than the limit 5 TB of gsutil"
      },
      {
        "date": "2024-02-01T23:44:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets"
      },
      {
        "date": "2024-01-28T07:09:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets#online_versus_offline_transfer"
      },
      {
        "date": "2024-01-19T11:35:00.000Z",
        "voteCount": 2,
        "content": "It is B"
      },
      {
        "date": "2024-01-04T14:02:00.000Z",
        "voteCount": 1,
        "content": "it is D. it is far more faster to send this 10TB data over network, than 'call' for Transfer Applience."
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 147,
    "url": "https://www.examtopics.com/discussions/google/view/60583-exam-professional-cloud-architect-topic-1-question-147/",
    "body": "Your company has an enterprise application running on Compute Engine that requires high availability and high performance. The application has been deployed on two instances in two zones in the same region in active-passive mode. The application writes data to a persistent disk. In the case of a single zone outage, that data should be immediately made available to the other instance in the other zone. You want to maximize performance while minimizing downtime and data loss.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Attach a persistent SSD disk to the first instance. 2. Create a snapshot every hour. 3. In case of a zone outage, recreate a persistent SSD disk in the second instance where data is coming from the created snapshot.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a Cloud Storage bucket. 2. Mount the bucket into the first instance with gcs-fuse. 3. In case of a zone outage, mount the Cloud Storage bucket to the second instance with gcs-fuse.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Attach a regional SSD persistent disk to the first instance. 2. In case of a zone outage, force-attach the disk to the other instance.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Attach a local SSD to the first instance disk. 2. Execute an rsync command every hour where the target is a persistent SSD disk attached to the second instance. 3. In case of a zone outage, use the second instance."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 30,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-08-25T00:25:00.000Z",
        "voteCount": 43,
        "content": "Answer C\nhttps://cloud.google.com/compute/docs/disks/repd-failover"
      },
      {
        "date": "2021-10-23T07:47:00.000Z",
        "voteCount": 42,
        "content": "C is right answer. \nC. 1. Attach a regional SSD persistent disk to the first instance. 2. In case of a zone outage, force-attach the disk to the other instance.\ngcs-fuse is slower than of regional SSD PD. \n\n**** Admin: You need to correct lots of questions. Some of the marked answers are nonsense, these must be revisited based on experts comments."
      },
      {
        "date": "2024-06-19T11:02:00.000Z",
        "voteCount": 1,
        "content": "BigQuery cannot use customer supplied KMs keys only customer managed keys. The other options add too much complexity to the problem."
      },
      {
        "date": "2024-06-19T11:03:00.000Z",
        "voteCount": 1,
        "content": "Somehow I commented on the wrong answer please delete."
      },
      {
        "date": "2024-05-10T14:31:00.000Z",
        "voteCount": 1,
        "content": "C makes a better sense than any other option"
      },
      {
        "date": "2024-04-19T11:50:00.000Z",
        "voteCount": 1,
        "content": "Agree with Regional SSD persistent"
      },
      {
        "date": "2023-12-29T02:00:00.000Z",
        "voteCount": 2,
        "content": "C\n\nIn the event that the primary zone fails, you can fail over your regional Persistent Disk volume to a VM in another zone by using a force-attach operation. When there's a failure in the primary zone, you might not be able to detach the disk from the VM because the VM can't be reached to perform the detach operation. Force-attach operation lets you attach a regional Persistent Disk volume to a VM even if that volume is attached to another VM"
      },
      {
        "date": "2023-09-07T04:56:00.000Z",
        "voteCount": 1,
        "content": "I don't get why B has been given as answer... GCS-FUSE brings in additional complexity and it also doesn't serve the same purpose as effectively as regional SSD does."
      },
      {
        "date": "2023-06-15T22:33:00.000Z",
        "voteCount": 1,
        "content": "Ans: C, please check - https://cloud.google.com/compute/docs/disks/high-availability-regional-persistent-disk"
      },
      {
        "date": "2023-03-29T21:43:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/compute/docs/disks/repd-failover\n\nSeems C is correct"
      },
      {
        "date": "2023-01-07T04:02:00.000Z",
        "voteCount": 1,
        "content": "C is the correct answer,\n\nhttps://cloud.google.com/compute/docs/disks/repd-failover#zonal_failures"
      },
      {
        "date": "2022-12-14T23:46:00.000Z",
        "voteCount": 1,
        "content": "Answer is C"
      },
      {
        "date": "2022-12-12T11:14:00.000Z",
        "voteCount": 7,
        "content": "Admins please take some time and redo the answers, put them to match at least the most voted ones, would help a lot."
      },
      {
        "date": "2022-11-19T07:15:00.000Z",
        "voteCount": 2,
        "content": "Regional persistent disk is a storage option that provides synchronous replication of data between two zones in a region. Regional persistent disks can be a good building block to use when you implement HA services in Compute Engine."
      },
      {
        "date": "2022-11-16T03:55:00.000Z",
        "voteCount": 1,
        "content": "C is ok"
      },
      {
        "date": "2022-10-19T13:03:00.000Z",
        "voteCount": 1,
        "content": "C is the right answer"
      },
      {
        "date": "2022-09-18T00:33:00.000Z",
        "voteCount": 1,
        "content": "You want to maximize performance while minimizing downtime and data loss"
      },
      {
        "date": "2022-08-19T23:29:00.000Z",
        "voteCount": 1,
        "content": "Inline with the current architecture itself \"The application writes data to a persistent disk.\""
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 148,
    "url": "https://www.examtopics.com/discussions/google/view/60439-exam-professional-cloud-architect-topic-1-question-148/",
    "body": "You are designing a Data Warehouse on Google Cloud and want to store sensitive data in BigQuery. Your company requires you to generate the encryption keys outside of Google Cloud. You need to implement a solution. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGenerate a new key in Cloud Key Management Service (Cloud KMS). Store all data in Cloud Storage using the customer-managed key option and select the created key. Set up a Dataflow pipeline to decrypt the data and to store it in a new BigQuery dataset.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGenerate a new key in Cloud KMS. Create a dataset in BigQuery using the customer-managed key option and select the created key.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImport a key in Cloud KMS. Store all data in Cloud Storage using the customer-managed key option and select the created key. Set up a Dataflow pipeline to decrypt the data and to store it in a new BigQuery dataset.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImport a key in Cloud KMS. Create a dataset in BigQuery using the customer-supplied key option and select the created key.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 44,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 13,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-09-11T15:48:00.000Z",
        "voteCount": 17,
        "content": "The answer is easy. It says keys must be left outside of Google Cloud.\nThis automatically eliminates A / B.\nNow the C option says decrypts before storing it in BigQuery which the point is to encrypt the data while been in BigQuery, D is the only possible answer."
      },
      {
        "date": "2024-06-19T11:05:00.000Z",
        "voteCount": 2,
        "content": "Except that BigQuery doesn't support customer supplied keys outside of GCP."
      },
      {
        "date": "2021-08-23T21:15:00.000Z",
        "voteCount": 17,
        "content": "D is OK"
      },
      {
        "date": "2023-06-15T20:11:00.000Z",
        "voteCount": 2,
        "content": "But CSEK is not supported in BigQuery"
      },
      {
        "date": "2023-08-24T14:53:00.000Z",
        "voteCount": 2,
        "content": "It is a tricky distinction because of the term collision.\nHowever, \"import key to KMS\" does not mean CSEK.\nCSEK does not get imported or stored in KMS at all.  CSEK \"customer supplied\" is per-transaction uploaded by every API call by the user/client (no KMS).  \nThis situation \"customer supplied\" means created from non-GCP KMS (could be on-prem or EKM).  Once a key is imported to KMS it is treated as CMEK.  The API client calling GCS doesn't need to upload the key.  It lives in KMS.  That is not the same \"per-transaction\" upload as CSEK."
      },
      {
        "date": "2023-08-24T14:58:00.000Z",
        "voteCount": 1,
        "content": "I mean after being imported to KMS you key is handled like a CMEK and available to BQ service."
      },
      {
        "date": "2024-09-28T04:13:00.000Z",
        "voteCount": 1,
        "content": "C - won't encrypt data in BQ with customer key.\nA,B - you will generate key inside the GCP (what is also wrong by requirements)\nD - looks good, but say to select CSEK... but after importing the key to KMS it becomes a customer-managed.\n\nI would select the D"
      },
      {
        "date": "2024-06-19T11:04:00.000Z",
        "voteCount": 1,
        "content": "The answer cannot be D since BigQuery does not support customer provided keys, only customer managed keys generated in Cloud KMS. So B is the only viable option that doesn't add complexity."
      },
      {
        "date": "2024-06-19T11:01:00.000Z",
        "voteCount": 1,
        "content": "It cannot be D, BigQuery does not support customer supplied KMS keys, only customer managed keys, B."
      },
      {
        "date": "2023-11-18T10:21:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/bigquery/docs/customer-managed-encryption"
      },
      {
        "date": "2023-11-11T07:43:00.000Z",
        "voteCount": 2,
        "content": "A, B, C are ruled out as  they say Customer Managed keys.\nHence, D."
      },
      {
        "date": "2023-08-23T11:48:00.000Z",
        "voteCount": 4,
        "content": "GCP docu says \"BigQuery and BigLake tables don't support Customer-Supplied Encryption Keys (CSEK).\" \n\nHowever, I just tested it and it worked:\n\n1. Create Key\nopenssl rand 32 &gt; ./key2\n\n2. Import into KMS\ngcloud kms keys versions import --import-job csek1 --location us-west1 --keyring csek --key csek --algorithm google-symmetric-encryption --target-key-file ./key2\n\n3. In Cloud Console: select the key when creating a new data set and table in BigQuery"
      },
      {
        "date": "2023-08-24T14:54:00.000Z",
        "voteCount": 2,
        "content": "Right, term collision with \"customer supplied\" key.  However, \"import key to KMS\" does not mean CSEK."
      },
      {
        "date": "2023-04-18T12:55:00.000Z",
        "voteCount": 2,
        "content": "C - as BigQuery doesn't support Customer Supplier Keys."
      },
      {
        "date": "2023-03-13T03:47:00.000Z",
        "voteCount": 1,
        "content": "BigQuery doesn't support CSEK"
      },
      {
        "date": "2023-04-22T09:27:00.000Z",
        "voteCount": 1,
        "content": "BG DOES support CSEK."
      },
      {
        "date": "2023-03-13T03:51:00.000Z",
        "voteCount": 1,
        "content": "Sorry even C is not correct, why to store the data in bq without encryption.\ndata should be passed encrypted from storage to bq.\nthen Answer is B"
      },
      {
        "date": "2023-11-01T15:12:00.000Z",
        "voteCount": 1,
        "content": "I would go with C.\n\nhttps://cloud.google.com/bigquery/docs/customer-managed-encryption\nRead that document in the link carefully.\n\n1st paragraph: \"By Default, BigQuery  encrypts your content stored at rest\";\n1st bullet point, 2nd paragraph under the [Before you Begin] section: \"BigQuery and BigLake tables don't support Customer-Supplied Encryption Keys (CSEK)\"\n\nThere is also a difference between CMEK and CSEK.\nCMEK: you can create and manage a key using Cloud KMS;\nCSEK: you specify the contents of the key;\n\nRef for CMEK vs CSEK:\nhttps://cloud.google.com/sql/docs/mysql/cmek#:~:text=Note%3A%20Customer%2Dmanaged%20encryption%20keys,specific%20resources%20across%20Google%20Cloud."
      },
      {
        "date": "2023-11-01T15:19:00.000Z",
        "voteCount": 1,
        "content": "Even though I'll chose C for the answer over D, because of the terminology in \"BQ using customer-supplied key\", I have an issue with this: \n\nTo me it does not make any sense.\nThe data is being encrypted by some key say K1 to store in Cloud Storage, then Decrypted, to be Re-Encrypted (automatically by say K2 [a google created key]) by BigQuery when being stored. This negates the use of K1 on your Data Storage in BigQuery. \n\nIt makes no sense. If someone sees this differently, I'd love to hear it. Thanks."
      },
      {
        "date": "2023-05-02T00:15:00.000Z",
        "voteCount": 1,
        "content": "If you want to control encryption yourself, you can use customer-managed encryption keys (CMEK) for BigQuery. \nhttps://cloud.google.com/bigquery/docs/customer-managed-encryption"
      },
      {
        "date": "2023-06-15T20:03:00.000Z",
        "voteCount": 1,
        "content": "There is a difference between customer managed and customer supplied. Link that you have shared talks about customer managed and not customer supplied"
      },
      {
        "date": "2023-03-06T07:17:00.000Z",
        "voteCount": 1,
        "content": "Key work: \"keys outside of Google Cloud\" so you have to import the key. between C and D I go with D."
      },
      {
        "date": "2023-01-16T04:49:00.000Z",
        "voteCount": 2,
        "content": "D is correct. I had this question on the exam toaday and I go with D.\nExplanation is - Generate the key outside the GCP so C and D are correct.\n\"Set up a Dataflow pipeline to decrypt the data and to store it in a new BigQuery dataset\" is not correct becuase it means that data exist on GCP what is not correct. Only D is correct."
      },
      {
        "date": "2023-01-07T04:57:00.000Z",
        "voteCount": 2,
        "content": "Yes, BigQuery and BigLake tables don't support Customer-Supplied Encryption Keys (CSEK). Answer must be either A or C, since the say generate key outside Google Cloud, import the key, hence I go for the answer C.\nhttps://cloud.google.com/bigquery/docs/customer-managed-encryption#before_you_begin\n\nhttps://cloud.google.com/kms/docs/importing-a-key"
      },
      {
        "date": "2023-01-01T12:05:00.000Z",
        "voteCount": 4,
        "content": "Answer D is incorrect because BigQuery does not support the use of customer-supplied keys to encrypt data at rest. Instead, you can use customer-managed encryption keys in Cloud KMS to encrypt the data in BigQuery. To do this, you can either generate a new key in Cloud KMS (answer A) or import an existing key (answer C). Once you have a key in Cloud KMS, you can create a BigQuery dataset and select the key as the customer-managed key for the dataset. This will enable BigQuery to use the key to encrypt the data in the dataset."
      },
      {
        "date": "2023-01-07T04:44:00.000Z",
        "voteCount": 1,
        "content": "Yes, BigQuery and BigLake tables don't support Customer-Supplied Encryption Keys (CSEK). Answer must be either A or C, since the say generate key outside Google Cloud, import the key, hence I go for the answer C."
      },
      {
        "date": "2023-01-07T04:45:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/bigquery/docs/customer-managed-encryption#before_you_begin"
      },
      {
        "date": "2023-01-07T04:53:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/kms/docs/importing-a-key"
      },
      {
        "date": "2023-08-24T14:57:00.000Z",
        "voteCount": 1,
        "content": "You have to know the difference between CSEK and \"imported keys to KMS\".  Those are not the same things.  CSEK is never stored in KMS, obviously an imported key is.  It is then as available as any CMEK to BQ."
      },
      {
        "date": "2022-12-14T23:49:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is D"
      },
      {
        "date": "2022-12-04T11:17:00.000Z",
        "voteCount": 1,
        "content": "answer  is D\nhttps://cloud.google.com/bigquery/docs/customer-managed-encryption"
      },
      {
        "date": "2022-12-02T06:24:00.000Z",
        "voteCount": 1,
        "content": "Answer D.\nQuestions says \"...design data warehouse...\" - would prefer BigQuery"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 149,
    "url": "https://www.examtopics.com/discussions/google/view/60440-exam-professional-cloud-architect-topic-1-question-149/",
    "body": "Your organization has stored sensitive data in a Cloud Storage bucket. For regulatory reasons, your company must be able to rotate the encryption key used to encrypt the data in the bucket. The data will be processed in Dataproc. You want to follow Google-recommended practices for security. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a key with Cloud Key Management Service (KMS). Encrypt the data using the encrypt method of Cloud KMS.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a key with Cloud Key Management Service (KMS). Set the encryption key on the bucket to the Cloud KMS key.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGenerate a GPG key pair. Encrypt the data using the GPG key. Upload the encrypted data to the bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGenerate an AES-256 encryption key. Encrypt the data in the bucket using the customer-supplied encryption keys feature."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 25,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-09-05T06:23:00.000Z",
        "voteCount": 32,
        "content": "B. Create a key with Cloud Key Management Service (KMS). Set the encryption key on the bucket to the Cloud KMS key."
      },
      {
        "date": "2021-08-23T21:17:00.000Z",
        "voteCount": 9,
        "content": "B is OK\nhttps://cloud.google.com/storage/docs/encryption/using-customer-managed-keys#add-object-key"
      },
      {
        "date": "2024-02-01T23:53:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/storage/docs/encryption/customer-managed-keys#key-rotation"
      },
      {
        "date": "2023-12-12T08:06:00.000Z",
        "voteCount": 1,
        "content": "It's B, off course"
      },
      {
        "date": "2023-11-18T10:28:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/storage/docs/encryption/customer-managed-keys#key-rotation"
      },
      {
        "date": "2023-10-14T03:20:00.000Z",
        "voteCount": 2,
        "content": "The following restrictions apply when using customer-managed encryption keys:\n\nYou cannot encrypt an object with a customer-managed encryption key by updating the object's metadata. Include the key as part of a rewrite of the object instead.\n\ngcloud storage uses the objects update command to set encryption keys on objects, but the command rewrites the object as part of the request.\nthis makes rotating keys difficult"
      },
      {
        "date": "2023-08-30T06:33:00.000Z",
        "voteCount": 1,
        "content": "Probably B:\nhttps://cloud.google.com/storage/docs/encryption/customer-managed-keys#key-replacement"
      },
      {
        "date": "2023-06-11T01:22:00.000Z",
        "voteCount": 1,
        "content": "It says customer wants to manage the rotation not the supplying of key.  Hence B not D.  Seen some people say with customer managed you cannot rotate but this document suggests you can https://cloud.google.com/storage/docs/encryption/customer-managed-keys#key-rotation."
      },
      {
        "date": "2023-04-03T13:11:00.000Z",
        "voteCount": 1,
        "content": "B does not allow to rotate assymetric key.\nhttps://cloud.google.com/kms/docs/key-rotation\n=&gt; Cloud Key Management Service does not support automatic rotation of asymmetric keys. See Considerations for asymmetric keys below.\n\nI go for D."
      },
      {
        "date": "2023-04-22T09:31:00.000Z",
        "voteCount": 1,
        "content": "GC uses symmetric key."
      },
      {
        "date": "2023-03-22T20:29:00.000Z",
        "voteCount": 4,
        "content": "B. Create a key with Cloud Key Management Service (KMS). Set the encryption key on the bucket to the Cloud KMS key.\n\nTo rotate the encryption key used to encrypt data in a Cloud Storage bucket, it is recommended to use Cloud KMS. You can create a new key version, set it as the primary version, and update the bucket's default KMS key to the new key version. This allows you to rotate the encryption key while still allowing access to the data. You can then process the data in Dataproc while the encryption key is being rotated. This approach provides security and compliance with regulations, as well as easy key rotation without disrupting access to data."
      },
      {
        "date": "2023-03-22T20:29:00.000Z",
        "voteCount": 1,
        "content": "Your organization has stored sensitive data in a Cloud Storage bucket. For regulatory reasons, your company must be able to rotate the encryption key used to encrypt the data in the bucket. The data will be processed in Dataproc. You want to follow Google-recommended practices for security. What should you do?\nA. Create a key with Cloud Key Management Service (KMS). Encrypt the data using the encrypt method of Cloud KMS.\nB. Create a key with Cloud Key Management Service (KMS). Set the encryption key on the bucket to the Cloud KMS key.\nC. Generate a GPG key pair. Encrypt the data using the GPG key. Upload the encrypted data to the bucket.\nD. Generate an AES-256 encryption key. Encrypt the data in the bucket using the customer-supplied encryption keys feature."
      },
      {
        "date": "2023-01-07T05:20:00.000Z",
        "voteCount": 1,
        "content": "B is the correct answer, we can encrypt the data in the bucket using CMEK. And the key can be rotated as per requirement.\nhttps://cloud.google.com/storage/docs/encryption/using-customer-managed-keys#add-object-key\n\nhttps://cloud.google.com/storage/docs/samples/storage-rotate-encryption-key#storage_rotate_encryption_key-python"
      },
      {
        "date": "2022-11-20T02:57:00.000Z",
        "voteCount": 1,
        "content": "B is ok"
      },
      {
        "date": "2022-11-16T05:50:00.000Z",
        "voteCount": 1,
        "content": "B is ok"
      },
      {
        "date": "2022-11-01T02:14:00.000Z",
        "voteCount": 1,
        "content": "B. rotation and dataproc ... trendmicro talk about this in https://www.trendmicro.com/cloudoneconformity/knowledge-base/gcp/Dataproc/enable-encryption-with-cmks-for-dataproc-clusters.html\n\nEnsure that your Google Cloud Dataproc clusters on Compute Engine are encrypted with Customer-Managed Keys (CMKs) in order to control the cluster data encryption/decryption process. You can create and manage your own Customer-Managed Keys (CMKs) with Cloud Key Management Service (Cloud KMS). Cloud KMS provides secure and efficient encryption key management, controlled key rotation, and revocation mechanisms.\nThis rule resolution is part of the Conformity Security &amp; Compliance tool for GCP."
      },
      {
        "date": "2022-08-19T23:47:00.000Z",
        "voteCount": 3,
        "content": "As per question: \" your company must be able to rotate the encryption key\"\nIt is easily possible with KMS:  https://cloud.google.com/kms/docs/rotating-keys#kms-create-key-rotation-schedule-gcloud"
      },
      {
        "date": "2022-08-04T10:15:00.000Z",
        "voteCount": 2,
        "content": "\"Your company must be able to rotate the encryption key\" is the requirement which eliminates CMEK and why you need a CSEK. You have to use a boto config file to do this and is part of one of the labs."
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 150,
    "url": "https://www.examtopics.com/discussions/google/view/60441-exam-professional-cloud-architect-topic-1-question-150/",
    "body": "Your team needs to create a Google Kubernetes Engine (GKE) cluster to host a newly built application that requires access to third-party services on the internet.<br>Your company does not allow any Compute Engine instance to have a public IP address on Google Cloud. You need to create a deployment strategy that adheres to these guidelines. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the GKE cluster as a private cluster, and configure Cloud NAT Gateway for the cluster subnet.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the GKE cluster as a private cluster. Configure Private Google Access on the Virtual Private Cloud (VPC).",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the GKE cluster as a route-based cluster. Configure Private Google Access on the Virtual Private Cloud (VPC).",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Compute Engine instance, and install a NAT Proxy on the instance. Configure all workloads on GKE to pass through this proxy to access third-party services on the Internet."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 63,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-09-11T10:02:00.000Z",
        "voteCount": 32,
        "content": "Cloud NAT is the correct answer"
      },
      {
        "date": "2022-08-19T23:56:00.000Z",
        "voteCount": 26,
        "content": "** Admins: More than 60% of the answers you have selected are wrong. Please correct them ASAP. I must appreciate community here for taking out time to share their perspective and help fellow learners.\n\n\"B\" can never be an answer here as the Private Google Access enables internal access to Google APIs only whereas in question the ask is \"access to third-party services on the internet\""
      },
      {
        "date": "2023-11-16T08:50:00.000Z",
        "voteCount": 7,
        "content": "If they provide the correct answer, you will never see this website any more"
      },
      {
        "date": "2024-06-19T11:13:00.000Z",
        "voteCount": 1,
        "content": "True, but then if it were shut down literally nobody could pass this ridiculous test where half the questions are so badly worded and confusing with debatable options."
      },
      {
        "date": "2023-04-03T13:15:00.000Z",
        "voteCount": 13,
        "content": "This is most likely on purpose. Otherwise google will do something in order for the exam dump to be shutdown."
      },
      {
        "date": "2024-05-20T10:36:00.000Z",
        "voteCount": 1,
        "content": "Cloud NAT, Private Service Connect is for Google API Access."
      },
      {
        "date": "2024-03-10T03:17:00.000Z",
        "voteCount": 1,
        "content": "Cloud NAT to access to the internet"
      },
      {
        "date": "2024-01-19T11:42:00.000Z",
        "voteCount": 1,
        "content": "It is A"
      },
      {
        "date": "2023-11-30T20:27:00.000Z",
        "voteCount": 1,
        "content": "Needs Nat to connect to 3rd party apps"
      },
      {
        "date": "2023-11-29T19:48:00.000Z",
        "voteCount": 1,
        "content": "B is only part of the solution, but needs Cloud Nat to get access on the internet with third-party services, then the correct answer is A . See doc:\nhttps://cloud.google.com/kubernetes-engine/docs/concepts/private-cluster-concept"
      },
      {
        "date": "2023-10-18T18:14:00.000Z",
        "voteCount": 1,
        "content": "go for Cloud NAT"
      },
      {
        "date": "2023-09-07T05:00:00.000Z",
        "voteCount": 2,
        "content": "I am not sure who's writing these answers\nPrivate Google Access is useful for allowing Google Cloud resources, including GKE clusters, to access Google services without public IPs, but it doesn't provide access to third-party services on the internet."
      },
      {
        "date": "2023-06-27T15:55:00.000Z",
        "voteCount": 1,
        "content": "Cloud NAT A"
      },
      {
        "date": "2023-05-29T07:10:00.000Z",
        "voteCount": 1,
        "content": "Cloud NAT allows the resources in private subnet to access the internet\u2014for updates, patching, config management, and more\u2014in a controlled and efficient manner."
      },
      {
        "date": "2023-06-02T00:42:00.000Z",
        "voteCount": 1,
        "content": "Yeah agree as GKE admin"
      },
      {
        "date": "2023-05-29T07:04:00.000Z",
        "voteCount": 1,
        "content": "Selected Answer: A. Cloud NAT allows the resources in private subnet to access the internet\u2014for updates, patching, config management, and more\u2014in a controlled and efficient manner."
      },
      {
        "date": "2023-04-04T21:18:00.000Z",
        "voteCount": 3,
        "content": "A.\n\nhttps://cloud.google.com/kubernetes-engine/docs/how-to/private-clusters#workloads_on_private_clusters_unable_to_access_internet"
      },
      {
        "date": "2023-03-22T20:38:00.000Z",
        "voteCount": 1,
        "content": "Private Google Access allows resources in a VPC network to access Google Cloud services without an external IP address. By configuring the GKE cluster as a private cluster, the nodes and services inside the cluster will not have a public IP address, and only resources within the VPC network will be able to communicate with them. With Private Google Access enabled, the GKE cluster can access third-party services on the internet via Google APIs and services without requiring a public IP address.\n\nTherefore, the correct option is:\n\nB. Configure the GKE cluster as a private cluster. Configure Private Google Access on the Virtual Private Cloud (VPC)."
      },
      {
        "date": "2023-02-19T13:06:00.000Z",
        "voteCount": 2,
        "content": "answer should be \"B\"\nhttps://cloud.google.com/vpc/docs/private-access-options"
      },
      {
        "date": "2023-01-07T05:38:00.000Z",
        "voteCount": 3,
        "content": "A is the correct answer,\n\nGranting private nodes outbound internet access\nTo provide outbound internet access for your private nodes, such as to pull images from an external registry, use Cloud NAT to create and configure a Cloud Router. Cloud NAT lets private clusters establish outbound connections over the internet to send and receive packets.\n\nThe Cloud Router allows all your nodes in the region to use Cloud NAT for all primary and alias IP ranges. It also automatically allocates the external IP addresses for the NAT gateway.\n\nFor instructions to create and configure a Cloud Router, refer to Create a Cloud NAT configuration using Cloud Router in the Cloud NAT documentation.\n\nhttps://cloud.google.com/kubernetes-engine/docs/how-to/private-clusters#private-nodes-outbound"
      },
      {
        "date": "2022-12-14T23:56:00.000Z",
        "voteCount": 2,
        "content": "A is the correct answer"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 151,
    "url": "https://www.examtopics.com/discussions/google/view/60436-exam-professional-cloud-architect-topic-1-question-151/",
    "body": "Your company has a support ticketing solution that uses App Engine Standard. The project that contains the App Engine application already has a Virtual Private<br>Cloud (VPC) network fully connected to the company's on-premises environment through a Cloud VPN tunnel. You want to enable the App Engine application to communicate with a database that is running in the company's on-premises environment. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure private Google access for on-premises hosts only.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure private Google access.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure private services access.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure serverless VPC access.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 58,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-09-29T02:45:00.000Z",
        "voteCount": 41,
        "content": "D is right , refer to https://cloud.google.com/vpc/docs/serverless-vpc-access#use_cases"
      },
      {
        "date": "2021-12-01T17:14:00.000Z",
        "voteCount": 10,
        "content": "D) is correct. Use case example: Your serverless environment needs to access data from your on-premises database through Cloud VPN."
      },
      {
        "date": "2021-09-18T07:58:00.000Z",
        "voteCount": 18,
        "content": "D. Configuring serverless VPC access App Engine can connect to the VPC and then through VPN tunnel to the on-prem DB"
      },
      {
        "date": "2023-11-23T07:22:00.000Z",
        "voteCount": 3,
        "content": "The answer is D.\nThe option B is for the other way. The option B is for the on-premise services to be able to use Google API through VPN"
      },
      {
        "date": "2023-11-18T20:37:00.000Z",
        "voteCount": 10,
        "content": "Answer is D. Here the explanation since I didn't see any good answer:\n1-  We have a VPC.\n2-  We have an onpremisses DB.\n3- We have App Engine (that runs on a isolated network that does not belong to the VPC).\n4- We can connect the VPC to the onpremisses network using Cloud VPN, which is the main purpose of Cloud VPN (let's say to simplify this answer).\n5 - Now how we connect the AppEngine that is isolated from the VPC and needs to use \"something\" to reach out the onpremisses DB directly (no public ip, only private ip)? Here we will have to have somehow access to the VPC and then the VPN and then the on premisses DB. That is the serverless vpc access.\n6-  So flow can be something like app engine --&gt; serverless vpc access --&gt; cloud VPN ---&gt; on premessises db through private ip."
      },
      {
        "date": "2023-11-18T10:37:00.000Z",
        "voteCount": 1,
        "content": "D:\nUse cases\n...\n    Your serverless environment needs to access data from your on-premises database through Cloud VPN.\nhttps://cloud.google.com/vpc/docs/serverless-vpc-access#use_cases"
      },
      {
        "date": "2023-11-12T22:12:00.000Z",
        "voteCount": 1,
        "content": "Read this article: https://cloud.google.com/vpc/docs/serverless-vpc-access\nThat makes me conclude for D."
      },
      {
        "date": "2023-10-03T22:28:00.000Z",
        "voteCount": 1,
        "content": "It's App Engine Standard and VPN cannot be used with Standard version"
      },
      {
        "date": "2023-09-07T05:04:00.000Z",
        "voteCount": 1,
        "content": "That's the whole purpose of serverless google access"
      },
      {
        "date": "2023-06-28T03:20:00.000Z",
        "voteCount": 1,
        "content": "Private google service and private google access seem to provide same level of access: https://googlecloudarchitect.us/private-service-access-vs-google-private-access/ Based on elimination both can be eliminated. Hence D."
      },
      {
        "date": "2023-04-17T10:41:00.000Z",
        "voteCount": 3,
        "content": "D is Right . You can use a Serverless VPC Access connector to let Cloud Run, App Engine standard, and Cloud Functions environments send packets to the internal IPv4 addresses of resources in a VPC network. Serverless VPC Access also supports sending packets to other networks connected to the selected VPC network."
      },
      {
        "date": "2023-03-22T20:54:00.000Z",
        "voteCount": 6,
        "content": "Private Google Access (option B) is used to enable VM instances in a VPC network to reach Google APIs and services using an internal IP address, but it does not allow communication to on-premises resources.\n\nPrivate Services Access (option C) allows you to access supported Google Cloud services through private IP addresses rather than public IP addresses, but it does not help in communicating with on-premises resources.\n\nConfiguring Private Google Access for on-premises hosts only (option A) is not a valid option as this configuration is not available."
      },
      {
        "date": "2023-02-22T08:07:00.000Z",
        "voteCount": 1,
        "content": "C. Configure private services access.\nTo enable an App Engine application to communicate with a database running in the company's on-premises environment over a VPC network that is fully connected to the company's on-premises environment through a Cloud VPN tunnel, the recommended approach is to use Private Service Access (PSA). Therefore, the correct answer is C. Configure private services access.\n\nPrivate Service Access (PSA) allows you to create private connections between your VPC network and services like Cloud SQL, Cloud Storage, and other Google APIs and services. With PSA, you can access these services using their private IP addresses, which are only accessible from within your VPC network, and not over the public internet. This provides better security and reduces the risk of data exfiltration or unauthorized access."
      },
      {
        "date": "2023-02-10T18:49:00.000Z",
        "voteCount": 1,
        "content": "D is right. Configuring serverless VPC access is the option for app engine to have Google private access"
      },
      {
        "date": "2023-02-05T14:23:00.000Z",
        "voteCount": 1,
        "content": "You can use a Serverless VPC Access connector to let Cloud Run, App Engine standard, and Cloud Functions environments send packets to the internal IPv4 addresses of resources in a VPC network. Serverless VPC Access also supports sending packets to other networks connected to the selected VPC network.\n\nhttps://cloud.google.com/vpc/docs/private-access-options"
      },
      {
        "date": "2023-02-02T15:03:00.000Z",
        "voteCount": 9,
        "content": "Upvote if there was no mention of \"serverless VPC access\" in the training videos and study guides you used to prepare for this exam."
      },
      {
        "date": "2023-01-24T16:19:00.000Z",
        "voteCount": 3,
        "content": "I am surprised 95% selected option D without understanding the use case.  Very basic ask\n\nAppEngine -&gt;Private Google Access-&gt;On-Prem DB \nGoogle Private Access to so enable any Services no matter running in VPC  to connect to on-prem DB via VPN tunnel. \nhttps://cloud.google.com/vpc/docs/private-google-access-hybrid\nhttps://cloud.google.com/vpc/docs/configure-private-google-access-hybrid\n\nAppEngine -&gt; Serveless-VPV-Access -&gt; Any GCP Resources/Services(with private IPs)"
      },
      {
        "date": "2023-01-31T04:36:00.000Z",
        "voteCount": 3,
        "content": "Private Google Access provides access to Google Services via Private IP and this can be used to call the App Engine from On-Prem. Here the usecase is exactly the opposite. Here only option to set up the Serverless VPC access to allow Serverless components to access Private resources (including on-Prem if proper VPN is already setup)"
      },
      {
        "date": "2023-01-30T01:34:00.000Z",
        "voteCount": 2,
        "content": "I still think it is D - the example you linked to for Private Google Access is to allow on-prem resources to contact Google Services, not the other way round.  \nhttps://cloud.google.com/vpc/docs/private-google-access-hybrid\n\nBut the example others link to explicitly says a use case is \"Your serverless environment needs to access data from your on-premises database through Cloud VPN\nhttps://cloud.google.com/vpc/docs/serverless-vpc-access#use_cases"
      },
      {
        "date": "2023-01-07T05:50:00.000Z",
        "voteCount": 1,
        "content": "D is the correct answer,\n\nServerless VPC Access\n\nbookmark_border\nServerless VPC Access makes it possible for you to connect directly to your Virtual Private Cloud network from serverless environments such as Cloud Run, App Engine, or Cloud Functions. Configuring Serverless VPC Access allows your serverless environment to send requests to your VPC network using internal DNS and internal IP addresses (as defined by RFC 1918 and RFC 6598). The responses to these requests also use your internal network.\n\nThere are two main benefits to using Serverless VPC Access:\n\nRequests sent to your VPC network are never exposed to the internet.\nCommunication through Serverless VPC Access can have less latency compared to the internet.\n\nhttps://cloud.google.com/vpc/docs/serverless-vpc-access#use_case"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 152,
    "url": "https://www.examtopics.com/discussions/google/view/60415-exam-professional-cloud-architect-topic-1-question-152/",
    "body": "Your company is planning to upload several important files to Cloud Storage. After the upload is completed, they want to verify that the uploaded content is identical to what they have on-premises. You want to minimize the cost and effort of performing this check. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Use Linux shasum to compute a digest of files you want to upload. 2. Use gsutil -m to upload all the files to Cloud Storage. 3. Use gsutil cp to download the uploaded files. 4. Use Linux shasum to compute a digest of the downloaded files. 5. Compare the hashes.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Use gsutil -m to upload the files to Cloud Storage. 2. Develop a custom Java application that computes CRC32C hashes. 3. Use gsutil ls -L gs://[YOUR_BUCKET_NAME] to collect CRC32C hashes of the uploaded files. 4. Compare the hashes.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Use gsutil -m to upload all the files to Cloud Storage. 2. Use gsutil cp to download the uploaded files. 3. Use Linux diff to compare the content of the files.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Use gsutil -m to upload the files to Cloud Storage. 2. Use gsutil hash -c FILE_NAME to generate CRC32C hashes of all on-premises files. 3. Use gsutil ls -L gs://[YOUR_BUCKET_NAME] to collect CRC32C hashes of the uploaded files. 4. Compare the hashes.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 41,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-08-25T07:01:00.000Z",
        "voteCount": 39,
        "content": "D is ok .\nhttps://cloud.google.com/storage/docs/gsutil/commands/hash"
      },
      {
        "date": "2022-09-11T02:05:00.000Z",
        "voteCount": 17,
        "content": "Seems most of the questions are having wrong answers.. If there is no discussion , its highly difficult to get the right answers."
      },
      {
        "date": "2023-10-18T18:20:00.000Z",
        "voteCount": 2,
        "content": "created hash and compare after is way to go."
      },
      {
        "date": "2023-09-07T05:07:00.000Z",
        "voteCount": 1,
        "content": "I am losing faith on the answers given... Option C is downright absurd."
      },
      {
        "date": "2023-08-03T06:05:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/storage/docs/gsutil/commands/hash\nCalculate hashes on local files, which can be used to compare with gsutil ls -L output.\n-c\nCalculate a CRC32c hash for the specified files."
      },
      {
        "date": "2023-06-11T02:30:00.000Z",
        "voteCount": 1,
        "content": "Downloading before hashing cannot be right.  The upload might be fine but if the download could corrupt"
      },
      {
        "date": "2023-05-21T13:29:00.000Z",
        "voteCount": 1,
        "content": "The correct answer should be D: https://cloud.google.com/storage/docs/gsutil/commands/hash"
      },
      {
        "date": "2022-12-15T00:01:00.000Z",
        "voteCount": 2,
        "content": "D is the correct answer"
      },
      {
        "date": "2022-11-16T07:04:00.000Z",
        "voteCount": 1,
        "content": "D is ok"
      },
      {
        "date": "2022-10-22T19:08:00.000Z",
        "voteCount": 5,
        "content": "Calculate hashes on local files, which can be used to compare with gsutil ls -L output. If a specific hash option is not provided, this command calculates all gsutil-supported hashes for the files.\n\nNote that gsutil automatically performs hash validation when uploading or downloading files, so this command is only needed if you want to write a script that separately checks the hash.\n\nIf you calculate a CRC32c hash for files without a precompiled crcmod installation, hashing will be very slow. See gsutil help crcmod for details.\nhttps://cloud.google.com/storage/docs/gsutil/commands/hash"
      },
      {
        "date": "2022-10-19T13:53:00.000Z",
        "voteCount": 1,
        "content": "D is the right answer per this doc https://cloud.google.com/storage/docs/gsutil/commands/hash"
      },
      {
        "date": "2022-09-08T04:18:00.000Z",
        "voteCount": 2,
        "content": "All those who answered D.. can one of you if you're genuine tell how is this even possible - The second step in the option D?\n2. Use gsutil hash -c FILE_NAME to generate CRC32C hashes of all on-premises files."
      },
      {
        "date": "2022-09-08T04:22:00.000Z",
        "voteCount": 2,
        "content": "Reading again it's probably not C because it talks about Linux commands but what if the environment is Windows.. \nbut I still have my doubts on D if someone could clarify?"
      },
      {
        "date": "2022-07-10T16:19:00.000Z",
        "voteCount": 2,
        "content": "Correct Answer C\nA- digest comparison does not gurantee file contents are same. moreover lot of extra steps. - not correct\nB - custom Java code - lot of effort - not correct\nD - gs util cannot be used for creating hash for on prem files stored on on prem filestore/database. Not correct\nC - not the best option but right answer for the options available."
      },
      {
        "date": "2022-08-01T10:38:00.000Z",
        "voteCount": 1,
        "content": "D is correct. you only need gs util to generate hash for cloud storage. you would use your own utility to create ash for on prem and then compare the two."
      },
      {
        "date": "2022-08-12T01:44:00.000Z",
        "voteCount": 2,
        "content": "there is a hash options in gsutil for local files.\nhttps://cloud.google.com/storage/docs/gsutil/commands/hash"
      },
      {
        "date": "2022-10-09T04:23:00.000Z",
        "voteCount": 1,
        "content": "dowload file has cost, C no is a option"
      },
      {
        "date": "2022-07-03T10:46:00.000Z",
        "voteCount": 2,
        "content": "D is correct , there is no need to build custom java script."
      },
      {
        "date": "2022-05-29T04:44:00.000Z",
        "voteCount": 1,
        "content": "D seems valid"
      },
      {
        "date": "2022-05-21T08:15:00.000Z",
        "voteCount": 1,
        "content": "I am eliminating tedious approaches that is downloading and doing custom coding so A B C are eliminated.\nD is the solution."
      },
      {
        "date": "2022-04-15T11:39:00.000Z",
        "voteCount": 2,
        "content": "D makes sense"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 153,
    "url": "https://www.examtopics.com/discussions/google/view/60624-exam-professional-cloud-architect-topic-1-question-153/",
    "body": "You have deployed an application on Anthos clusters (formerly Anthos GKE). According to the SRE practices at your company, you need to be alerted if request latency is above a certain threshold for a specified amount of time. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall Anthos Service Mesh on your cluster. Use the Google Cloud Console to define a Service Level Objective (SLO), and create an alerting policy based on this SLO.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable the Cloud Trace API on your project, and use Cloud Monitoring Alerts to send an alert based on the Cloud Trace metrics.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Profiler to follow up the request latency. Create a custom metric in Cloud Monitoring based on the results of Cloud Profiler, and create an Alerting policy in case this metric exceeds the threshold.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Anthos Config Management on your cluster, and create a yaml file that defines the SLO and alerting policy you want to deploy in your cluster."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 13,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-08-25T06:47:00.000Z",
        "voteCount": 24,
        "content": "A is ok. \nhttps://cloud.google.com/service-mesh/docs/observability/slo-overview"
      },
      {
        "date": "2023-10-26T18:24:00.000Z",
        "voteCount": 4,
        "content": "\"Google Cloud Console to define a Service Level Objective (SLO)\" seems odd, B doesn't seem wrong"
      },
      {
        "date": "2023-10-18T18:23:00.000Z",
        "voteCount": 1,
        "content": "Answer A looks correct"
      },
      {
        "date": "2023-01-07T06:04:00.000Z",
        "voteCount": 4,
        "content": "Cloud Monitoring can trigger an alert when a Service is on track to violate an SLO. You can create an alerting policy based on the rate of consumption of your error budget. All alerts on error budgets have the same basic condition: a specified percentage of the error budget for the compliance period is consumed in a lookback period, which is a time period, such as the previous 60 minutes. When you create the alerting policy, Anthos Service Mesh automatically sets most of the conditions for the alert based on the settings in the SLO. You specify the lookback period and the consumption percentage.\n\nhttps://cloud.google.com/service-mesh/docs/observability/alert-policy-slo"
      },
      {
        "date": "2022-11-16T07:07:00.000Z",
        "voteCount": 2,
        "content": "A is ok"
      },
      {
        "date": "2022-09-08T04:24:00.000Z",
        "voteCount": 2,
        "content": "A seems correct"
      },
      {
        "date": "2022-08-20T00:18:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/service-mesh/docs/observability/alert-policy-slo"
      },
      {
        "date": "2022-08-06T05:46:00.000Z",
        "voteCount": 1,
        "content": "Use the Google Cloud Console to define a Service Level Objective (SLO)\nWAAAAT ?\nHow Console help you to define SLO?"
      },
      {
        "date": "2022-08-05T06:24:00.000Z",
        "voteCount": 2,
        "content": "Specific Purpose of Cloud Trace API is to get info regarding Latency.\nWould go with B."
      },
      {
        "date": "2022-07-03T10:46:00.000Z",
        "voteCount": 1,
        "content": "A is right"
      },
      {
        "date": "2022-04-02T08:18:00.000Z",
        "voteCount": 3,
        "content": "Why not B....\nCloud Trace is a distributed tracing system that collects latency data from the applications and displays it in near real-time. It allows you to follow a sample request through your distributed system, observe the network calls and profile your system end to end.\nNote that Cloud Trace is disabled by default.\nThe Anthos Service Mesh pages provide a link to the traces in the Cloud Trace page in the Cloud Console.\nhttps://cloud.google.com/service-mesh/docs/observability/accessing-traces \nIn Anthos clusters you need to install Anthos service mesh? From this link you need to install it only on GKE and on-premises platforms\nhttps://cloud.google.com/service-mesh/docs/observability/accessing-traces"
      },
      {
        "date": "2022-06-07T03:32:00.000Z",
        "voteCount": 2,
        "content": "Can you create an Alert when you use Cloud Trace?"
      },
      {
        "date": "2023-03-15T23:44:00.000Z",
        "voteCount": 2,
        "content": "yep, you can"
      },
      {
        "date": "2022-06-08T01:49:00.000Z",
        "voteCount": 3,
        "content": "I think A is about monitoring and alerting without any further investigation, while Trace is for finding the root cause/detective purposes, when you look into a call and track this call step by step through each endpoint, the call is going through."
      },
      {
        "date": "2022-02-11T14:01:00.000Z",
        "voteCount": 2,
        "content": "I got same question on my exam."
      },
      {
        "date": "2022-02-09T23:44:00.000Z",
        "voteCount": 1,
        "content": "Go for A"
      },
      {
        "date": "2022-01-19T09:25:00.000Z",
        "voteCount": 4,
        "content": "Got this question in my exam, answered A"
      },
      {
        "date": "2021-12-14T19:31:00.000Z",
        "voteCount": 2,
        "content": "A is the correct answer"
      },
      {
        "date": "2021-11-30T01:29:00.000Z",
        "voteCount": 1,
        "content": "Vote A"
      },
      {
        "date": "2021-09-30T05:53:00.000Z",
        "voteCount": 1,
        "content": "A, https://cloud.google.com/service-mesh/docs/observability/alert-policy-slo"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 154,
    "url": "https://www.examtopics.com/discussions/google/view/60627-exam-professional-cloud-architect-topic-1-question-154/",
    "body": "Your company has a stateless web API that performs scientific calculations. The web API runs on a single Google Kubernetes Engine (GKE) cluster. The cluster is currently deployed in us-central1. Your company has expanded to offer your API to customers in Asia. You want to reduce the latency for users in Asia.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a second GKE cluster in asia-southeast1, and expose both APIs using a Service of type LoadBalancer. Add the public IPs to the Cloud DNS zone.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a global HTTP(s) load balancer with Cloud CDN enabled.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a second GKE cluster in asia-southeast1, and use kubemci to create a global HTTP(s) load balancer.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the memory and CPU allocated to the application in the cluster."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 19,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 5,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-08-25T06:58:00.000Z",
        "voteCount": 36,
        "content": "C is ok .\nhttps://cloud.google.com/blog/products/gcp/how-to-deploy-geographically-distributed-services-on-kubernetes-engine-with-kubemci"
      },
      {
        "date": "2021-10-10T06:54:00.000Z",
        "voteCount": 3,
        "content": "After going through the link I feel its C"
      },
      {
        "date": "2021-10-24T10:10:00.000Z",
        "voteCount": 6,
        "content": "Mee too. \nCDN does not make sense"
      },
      {
        "date": "2023-12-31T08:03:00.000Z",
        "voteCount": 1,
        "content": "Indeed. We don't know if the API is authenticated, reveals private data, static or not."
      },
      {
        "date": "2021-09-10T01:17:00.000Z",
        "voteCount": 11,
        "content": "I'm not sure about C. kubemci is deprecated and is not part anymore of cloud sdk in favor of ingress for anthos. I'll go with A"
      },
      {
        "date": "2021-09-23T12:39:00.000Z",
        "voteCount": 14,
        "content": "Problem with A is that a service load bancer is not l7 https.  The question is outdated, the answer will have been C.  Now it would be Anthos multi cluster ingress -https://cloud.google.com/kubernetes-engine/docs/concepts/multi-cluster-ingress"
      },
      {
        "date": "2024-04-20T03:04:00.000Z",
        "voteCount": 1,
        "content": "Agree with you"
      },
      {
        "date": "2021-10-25T03:22:00.000Z",
        "voteCount": 5,
        "content": "That's actually not true. Service of type: LoadBalancer, is a service from \"K8s\" point of view, which creates  L7 HTTP(S) Load Balancer."
      },
      {
        "date": "2024-05-25T00:02:00.000Z",
        "voteCount": 1,
        "content": "Nope, service is L4 Network/Internal load balancer"
      },
      {
        "date": "2021-09-13T02:42:00.000Z",
        "voteCount": 1,
        "content": "I think either a or c is correct. I chose c base on the article ref in the chat. Do u have supporting article ref kubemci is deprecated?  I also found some chatter about kubemci being deprecated but couldn\u2019t find anything offical"
      },
      {
        "date": "2021-10-08T09:15:00.000Z",
        "voteCount": 1,
        "content": "It is hee -- https://github.com/GoogleCloudPlatform/k8s-multicluster-ingress"
      },
      {
        "date": "2024-05-11T00:30:00.000Z",
        "voteCount": 2,
        "content": "To reduce latency for users in Asia while maintaining high availability and scalability, the most appropriate option would be:\n\nB. Use a global HTTP(s) load balancer with Cloud CDN enabled."
      },
      {
        "date": "2024-04-01T17:55:00.000Z",
        "voteCount": 2,
        "content": "B is answer"
      },
      {
        "date": "2024-01-28T07:25:00.000Z",
        "voteCount": 3,
        "content": "question is old but it should be c. however currently should be multi cluster ingress\nhttps://cloud.google.com/kubernetes-engine/docs/concepts/multi-cluster-ingress"
      },
      {
        "date": "2023-12-24T07:54:00.000Z",
        "voteCount": 1,
        "content": "really powefull \nkubernetes-engine-with-kubemci"
      },
      {
        "date": "2023-11-19T18:31:00.000Z",
        "voteCount": 2,
        "content": "Well it should be C, but it is deprecated in favor of ingress for Anthos as can be read here https://github.com/GoogleCloudPlatform/k8s-multicluster-ingress"
      },
      {
        "date": "2023-10-18T18:24:00.000Z",
        "voteCount": 1,
        "content": "go for C"
      },
      {
        "date": "2023-09-07T05:15:00.000Z",
        "voteCount": 2,
        "content": "B is funny"
      },
      {
        "date": "2023-07-09T03:05:00.000Z",
        "voteCount": 1,
        "content": "If it is an API performing scientific calculations then its customer base is a very specific targeted group.  It should not be considered as a mass market app that is used by lots of people all over the world.  Considering the business purpose of the API, option B would be more than sufficient to serve the need of customers anywhere in the world."
      },
      {
        "date": "2023-07-09T00:19:00.000Z",
        "voteCount": 3,
        "content": "IDK if this question will be in the exam bc the answer should be C.\nbut kubemci has now been deprecated in favor of Ingress for Anthos. \nIngress for Anthos is the recommended way to deploy multi-cluster ingress."
      },
      {
        "date": "2023-06-11T05:41:00.000Z",
        "voteCount": 1,
        "content": "The problem with B is the question very much infers we are dealing with dynamic content &amp; not static."
      },
      {
        "date": "2023-06-11T05:19:00.000Z",
        "voteCount": 1,
        "content": "It's A or C but I think A might be better.\nA is a simpler solution.  Cloud DNS allows you to add multiple targets and part of its decision making is the latency.  https://cloud.google.com/dns/docs/zones/zones-overview\nTough but I think A because it's only one API being exposed.  Ingress comes into its own when exposing many services."
      },
      {
        "date": "2023-02-19T13:23:00.000Z",
        "voteCount": 4,
        "content": "kubemci - deprecated\nhttps://github.com/GoogleCloudPlatform/k8s-multicluster-ingress"
      },
      {
        "date": "2022-12-25T23:49:00.000Z",
        "voteCount": 2,
        "content": "A good option for reducing latency for users in Asia accessing the web API would be to create a second GKE cluster in asia-southeast1 and use kubemci to create a global HTTP(s) load balancer.\n\nOption C, \"Create a second GKE cluster in asia-southeast1, and use kubemci to create a global HTTP(s) load balancer,\" would be the correct choice for this scenario.\n\nBy creating a second GKE cluster in asia-southeast1, you can reduce latency for users in Asia by serving the API from a closer location. You can then use kubemci, a command-line tool that simplifies the process of creating a global HTTP(s) load balancer, to expose the APIs from both clusters through a single global IP address. This allows users to access the API with low latency, regardless of their location."
      },
      {
        "date": "2022-12-25T23:49:00.000Z",
        "voteCount": 2,
        "content": "Option A, \"Create a second GKE cluster in asia-southeast1, and expose both APIs using a Service of type LoadBalancer. Add the public IPs to the Cloud DNS zone,\" would not be a good choice because it would not provide a single global IP address for users to access the API, which would increase latency and complexity.\n\nOption B, \"Use a global HTTP(s) load balancer with Cloud CDN enabled,\" would not be a good choice because it would not allow you to serve the API from a closer location for users in Asia.\n\nOption D, \"Increase the memory and CPU allocated to the application in the cluster,\" would not be a good choice because it would not address the issue of latency for users in Asia accessing the API."
      },
      {
        "date": "2022-12-05T02:25:00.000Z",
        "voteCount": 5,
        "content": "Answer is C but kubemci is deprecated, now you have to go with:\nMulti Cluster Ingress is a cloud-hosted controller for Google Kubernetes Engine (GKE) clusters. It's a Google-hosted service that supports deploying shared load balancing resources across clusters and across regions. To deploy Multi Cluster Ingress across multiple clusters, complete Setting up Multi Cluster Ingress then see Deploying Ingress across multiple clusters.\nhttps://cloud.google.com/kubernetes-engine/docs/concepts/multi-cluster-ingress"
      },
      {
        "date": "2022-10-23T11:17:00.000Z",
        "voteCount": 2,
        "content": "C is correct, however, this question is an old question and need to be updated to use the ingress for global HTTPS LB"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 155,
    "url": "https://www.examtopics.com/discussions/google/view/60494-exam-professional-cloud-architect-topic-1-question-155/",
    "body": "You are migrating third-party applications from optimized on-premises virtual machines to Google Cloud. You are unsure about the optimum CPU and memory options. The applications have a consistent usage pattern across multiple weeks. You want to optimize resource usage for the lowest cost. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an instance template with the smallest available machine type, and use an image of the third-party application taken from a current on-premises virtual machine. Create a managed instance group that uses average CPU utilization to autoscale the number of instances in the group. Modify the average CPU utilization threshold to optimize the number of instances running.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an App Engine flexible environment, and deploy the third-party application using a Dockerfile and a custom runtime. Set CPU and memory options similar to your application's current on-premises virtual machine in the app.yaml file.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate multiple Compute Engine instances with varying CPU and memory options. Install the Cloud Monitoring agent, and deploy the third-party application on each of them. Run a load test with high traffic levels on the application, and use the results to determine the optimal settings.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Compute Engine instance with CPU and memory options similar to your application's current on-premises virtual machine. Install the Cloud Monitoring agent, and deploy the third-party application. Run a load test with normal traffic levels on the application, and follow the Rightsizing Recommendations in the Cloud Console.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 39,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 13,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-09-08T19:46:00.000Z",
        "voteCount": 55,
        "content": "Answer is D. \n\nhttps://cloud.google.com/migrate/compute-engine/docs/4.9/concepts/planning-a-migration/cloud-instance-rightsizing?hl=en\n\n\"Rightsizing provides two types of recommendations:\n\n1. Performance-based recommendations: Recommends Compute Engine instances based on the CPU and RAM currently allocated to the on-premises VM. This recommendation is the default.\n\n2. Cost-based recommendations: Recommends Compute Engine instances based on:\n- The current CPU and RAM configuration of the on-premises VM.\n- The average usage of this VM during a given period. To use this option, you must activate rightsizing monitoring with vSphere for this group of VMs and allow time for Migrate for Compute Engine to analyze usage."
      },
      {
        "date": "2022-10-20T03:44:00.000Z",
        "voteCount": 1,
        "content": "The point:\n2. Cost-based recommendations: Recommends Compute Engine instances based on:\n    The current CPU and RAM configuration of the on-premises VM."
      },
      {
        "date": "2022-04-09T12:01:00.000Z",
        "voteCount": 9,
        "content": "It's definitely D. See the reference at the following link that says \"The recommendation algorithm is suited to workloads that follow weekly patterns\", which matches the part of the questions that says \"consistent usage pattern over multiple weeks\":\nhttps://cloud.google.com/compute/docs/instances/apply-machine-type-recommendations-for-instances\n\nOption A also has two problems;\n1. It only focuses on CPU, but the question says \"CPU and memory\"\n2. The question does not mention anything about horizontal scalability"
      },
      {
        "date": "2022-04-09T12:06:00.000Z",
        "voteCount": 4,
        "content": "Another (less obvious) reason for choosing D: I've noticed a pattern in these exams that the cloud provider wants to advertise and promote anything that they consider to be a cool feature of their platform. In this case, they are promoting their recommendation engine. If there's even an option that sounds like it's advertising a relevant managed service from the cloud provider, then that's usually one to consider."
      },
      {
        "date": "2022-04-09T12:08:00.000Z",
        "voteCount": 1,
        "content": "I also find the following wording in option A to be a bit iffy: \"an image of the third-party application taken from a current on-premises virtual machine\". That seems a bit vague in terms of what the image format would be."
      },
      {
        "date": "2023-12-27T22:48:00.000Z",
        "voteCount": 2,
        "content": "I choose A at first, because I thought that the Rightsizing Recommendations took various days to offer the estimate stats. But according to this article:\nhttps://cloud.google.com/migrate/compute-engine/docs/4.11/concepts/planning-a-migration/cloud-instance-rightsizing\nWhile it needs a week to give a proper estimate, it can give an estimate with less time too (But the accuracy decreases)\n\"For better recommendations, Migrate for Compute Engine recommends monitoring the migrated workloads for at least seven consecutive days (or one typical business week). Migrate for Compute Engine warns you when the monitoring period is insufficient for an adequate recommendation.\n\nEven if the monitoring period is insufficient, Migrate for Compute Engine still offers a cost-optimized recommendation based on the data available.\""
      },
      {
        "date": "2023-12-27T22:48:00.000Z",
        "voteCount": 2,
        "content": "Also note that Migrate to Virtual Machines v4.11 (Which the info is from) is no longer the latest version. V5 is already out and, strangely, lacks any article about rightsizing recommendations..."
      },
      {
        "date": "2023-10-18T18:29:00.000Z",
        "voteCount": 1,
        "content": "D make sense."
      },
      {
        "date": "2023-09-07T05:17:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2023-03-15T01:34:00.000Z",
        "voteCount": 2,
        "content": "Option D would be the best option to optimize resource usage for the lowest cost when migrating third-party applications from optimized on-premises virtual machines to Google Cloud."
      },
      {
        "date": "2023-03-06T11:22:00.000Z",
        "voteCount": 1,
        "content": "Answer is D. Similar than third-party convince me..."
      },
      {
        "date": "2023-01-04T14:28:00.000Z",
        "voteCount": 7,
        "content": "why most of the answers selected by host is INCORRECT? Is it intentional to misguide the folks?"
      },
      {
        "date": "2022-12-25T14:37:00.000Z",
        "voteCount": 1,
        "content": "i choose D as it's best practice create an instance with similar configuration as on premise and check metrics"
      },
      {
        "date": "2022-12-17T15:23:00.000Z",
        "voteCount": 1,
        "content": "D is the right one because of Rightsizing option from GCP"
      },
      {
        "date": "2022-12-15T00:12:00.000Z",
        "voteCount": 1,
        "content": "D is the correct answer"
      },
      {
        "date": "2022-11-16T07:23:00.000Z",
        "voteCount": 1,
        "content": "D is ok"
      },
      {
        "date": "2022-10-19T17:18:00.000Z",
        "voteCount": 1,
        "content": "I agree with D is the most accurate"
      },
      {
        "date": "2022-10-15T11:46:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2022-09-28T11:51:00.000Z",
        "voteCount": 3,
        "content": "A, application may not support horizontal scaling and may not run in instances whith small cpu\nB, dockerize third-party applications is not a requirement....Complex and costly\nC, too expensive\nD, simple and works"
      },
      {
        "date": "2022-09-07T22:40:00.000Z",
        "voteCount": 4,
        "content": "A, the benefit of moving to cloud is scaling based on load, start with min infra and scale-up based on usage."
      },
      {
        "date": "2022-05-21T08:25:00.000Z",
        "voteCount": 3,
        "content": "A - you cannot expect application to behavior similar in 2 different envior met without a test.\nB - App Engine is costly\nC- Varing cpu and memory cannot be doone.\nD- correa."
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 156,
    "url": "https://www.examtopics.com/discussions/google/view/60416-exam-professional-cloud-architect-topic-1-question-156/",
    "body": "Your company has a Google Cloud project that uses BigQuery for data warehousing. They have a VPN tunnel between the on-premises environment and Google<br>Cloud that is configured with Cloud VPN. The security team wants to avoid data exfiltration by malicious insiders, compromised code, and accidental oversharing.<br>What should they do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Private Google Access for on-premises only.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPerform the following tasks: 1. Create a service account. 2. Give the BigQuery JobUser role and Storage Reader role to the service account. 3. Remove all other IAM access from the project.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure VPC Service Controls and configure Private Google Access.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Private Google Access."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 28,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-11-26T14:13:00.000Z",
        "voteCount": 71,
        "content": "Without the discussion this site would be useless, many thanks to all that participate.  Majority of answers are wrong..."
      },
      {
        "date": "2023-06-03T10:16:00.000Z",
        "voteCount": 3,
        "content": "you can used chatGPT now"
      },
      {
        "date": "2023-10-03T06:35:00.000Z",
        "voteCount": 11,
        "content": "Then you are definitely bound to fail :-)"
      },
      {
        "date": "2021-09-08T10:22:00.000Z",
        "voteCount": 31,
        "content": "C is the recommended one https://cloud.google.com/vpc-service-controls/docs/overview"
      },
      {
        "date": "2023-12-14T16:51:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is C.\nSecurity benefits of VPC Service Controls\nAccess from unauthorized networks using stolen credentials\nData exfiltration by malicious insiders or compromised code\nhttps://cloud.google.com/vpc-service-controls/docs/overview#benefits"
      },
      {
        "date": "2023-11-11T08:37:00.000Z",
        "voteCount": 2,
        "content": "VPC Service Controls is required to stop data exfiltration. Hence C"
      },
      {
        "date": "2023-10-19T06:32:00.000Z",
        "voteCount": 1,
        "content": "C, VPC Service controls is need for the solution"
      },
      {
        "date": "2023-02-25T13:56:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2023-01-07T19:43:00.000Z",
        "voteCount": 2,
        "content": "C is the correct answer,\n\nTo secure data from exfiltration by malicious insiders, compromised code or accidental oversharing, we use VPC Service controls\n\nhttps://cloud.google.com/vpc-service-controls/docs/overview\n\nFor private access options, connect to services in VPC networks we use private service endpoints or VPC network peering.\n\nhttps://cloud.google.com/vpc/docs/private-access-options#connect-services"
      },
      {
        "date": "2022-12-15T00:16:00.000Z",
        "voteCount": 2,
        "content": "C is the correct answer"
      },
      {
        "date": "2022-11-16T07:47:00.000Z",
        "voteCount": 2,
        "content": "C is ok"
      },
      {
        "date": "2022-10-19T17:22:00.000Z",
        "voteCount": 1,
        "content": "C is the right answer"
      },
      {
        "date": "2022-10-15T11:48:00.000Z",
        "voteCount": 1,
        "content": "I will go with C"
      },
      {
        "date": "2022-04-23T07:29:00.000Z",
        "voteCount": 7,
        "content": "Going by definition- VPC Service Controls improves your ability to mitigate the risk of data exfiltration from Google Cloud services such as Cloud Storage and BigQuery. \n\nhence C is correct"
      },
      {
        "date": "2022-04-17T20:46:00.000Z",
        "voteCount": 2,
        "content": "C is the recommended \nhttps://cloud.google.com/vpc-service-controls/docs/overview"
      },
      {
        "date": "2022-02-27T08:50:00.000Z",
        "voteCount": 2,
        "content": "I don't get it , C is correct because of the \"VPC service Control\", But Privet Google access is not for on On-premises, A is for On-premises = https://cloud.google.com/vpc/docs/private-access-options"
      },
      {
        "date": "2022-01-09T00:26:00.000Z",
        "voteCount": 1,
        "content": "I agree C.\nThe link that wroted in Reveral Solution means C."
      },
      {
        "date": "2021-12-14T20:13:00.000Z",
        "voteCount": 2,
        "content": "C is the correct answer\nhttps://cloud.google.com/vpc-service-controls/docs/overview"
      },
      {
        "date": "2021-12-03T10:41:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/vpc-service-controls/docs/overview"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 157,
    "url": "https://www.examtopics.com/discussions/google/view/60495-exam-professional-cloud-architect-topic-1-question-157/",
    "body": "You are working at an institution that processes medical data. You are migrating several workloads onto Google Cloud. Company policies require all workloads to run on physically separated hardware, and workloads from different clients must also be separated. You created a sole-tenant node group and added a node for each client. You need to deploy the workloads on these dedicated hosts. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd the node group name as a network tag when creating Compute Engine instances in order to host each workload on the correct node group.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd the node name as a network tag when creating Compute Engine instances in order to host each workload on the correct node.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse node affinity labels based on the node group name when creating Compute Engine instances in order to host each workload on the correct node group.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse node affinity labels based on the node name when creating Compute Engine instances in order to host each workload on the correct node.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 11,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-09-08T20:02:00.000Z",
        "voteCount": 58,
        "content": "Answer is D. \n\nY'all not reading the fine details. The question is about aligning EACH client to their dedicated nodes (D), not to a node group (C). \n\nhttps://cloud.google.com/compute/docs/nodes/sole-tenant-nodes#default_affinity_labels\n\nThe above reference clearly articulates the default affinity label for node group and node name. Unless we're thinking about growing each client to their own dedicated node groups (not in the current requirement), then the answer is not C, rather D. \n\nCompute Engine assigns two default affinity labels to each node:\n\nA label for the node group name:\nKey: compute.googleapis.com/node-group-name\nValue: Name of the node group.\nA label for the node name:\nKey: compute.googleapis.com/node-name\nValue: Name of the individual node."
      },
      {
        "date": "2024-06-20T07:29:00.000Z",
        "voteCount": 1,
        "content": "Except that sole tenant nodes can also be grouped, and wouldn't it be a best practice to design for scaling?"
      },
      {
        "date": "2021-08-31T01:34:00.000Z",
        "voteCount": 18,
        "content": "D. Afinity should be set at node level, not node-group as every client has its own node in the group"
      },
      {
        "date": "2021-09-08T02:55:00.000Z",
        "voteCount": 7,
        "content": "That\u2019s what  i thought too"
      },
      {
        "date": "2024-09-15T08:26:00.000Z",
        "voteCount": 1,
        "content": "Question already says node-group created \"You created a sole-tenant node group and added a node for each client\""
      },
      {
        "date": "2024-09-13T12:05:00.000Z",
        "voteCount": 1,
        "content": "KV Affinity Label &gt; Node template &gt; Node Group &gt; CE\nQuestion already said Node group is created. Then why tag it to node ?"
      },
      {
        "date": "2024-07-31T01:04:00.000Z",
        "voteCount": 1,
        "content": "Using node affinity labels based on the node group name when creating Compute Engine instances is the appropriate method to ensure workloads are hosted on the correct node group. This approach aligns with Google Cloud's recommended practices for provisioning sole-tenant VMs and provides the required isolation for client workloads."
      },
      {
        "date": "2023-11-28T00:22:00.000Z",
        "voteCount": 2,
        "content": "D\nAs per the documentation: https://cloud.google.com/compute/docs/nodes/provisioning-sole-tenant-vms#provision_a_sole-tenant_vm"
      },
      {
        "date": "2023-10-19T06:38:00.000Z",
        "voteCount": 1,
        "content": "D, the question ask \u201cYou created a sole-tenant node group and added a node for each client.\u201d\uff0cso node affinity labels based on the node name is need it."
      },
      {
        "date": "2023-01-31T01:34:00.000Z",
        "voteCount": 3,
        "content": "I had this question recently (end of jan 2023) and went with answer D. After doing some investigation, that seems to be the right answer to me."
      },
      {
        "date": "2023-05-24T10:13:00.000Z",
        "voteCount": 1,
        "content": "Preparing for exam and gone through the concept make sense the answer is D"
      },
      {
        "date": "2023-01-06T14:04:00.000Z",
        "voteCount": 1,
        "content": "Answer is D.\nRef: you can't specify node affinity labels on a node group.&gt;&gt; https://cloud.google.com/compute/docs/nodes/sole-tenant-nodes#node_templates"
      },
      {
        "date": "2022-12-15T00:19:00.000Z",
        "voteCount": 1,
        "content": "D is the correct answer"
      },
      {
        "date": "2022-11-16T08:05:00.000Z",
        "voteCount": 1,
        "content": "D is ok"
      },
      {
        "date": "2022-10-19T17:53:00.000Z",
        "voteCount": 1,
        "content": "D is the correct answer, VMs must be associated to a specific node within the node-group, so you must use the node name label to provision the VM."
      },
      {
        "date": "2022-10-15T11:52:00.000Z",
        "voteCount": 1,
        "content": "D is right, Node is right choice instead of node group"
      },
      {
        "date": "2022-07-20T02:16:00.000Z",
        "voteCount": 4,
        "content": "D :   https://cloud.google.com/compute/docs/nodes/sole-tenant-nodes\nNode affinity labels are key-value pairs assigned to nodes, and are inherited from a node template. Affinity labels let you:\n\nControl how individual VM instances are assigned to nodes.\nControl how VM instances created from a template, such as those created by a managed instance group, are assigned to nodes.\nGroup sensitive VM instances on specific nodes or node groups, separate from other VMs."
      },
      {
        "date": "2022-03-31T03:31:00.000Z",
        "voteCount": 3,
        "content": "I go with C as I believe single-tenant node group meant for only one client"
      },
      {
        "date": "2022-03-21T16:33:00.000Z",
        "voteCount": 1,
        "content": "Answer is D since it is clearly documented as \nWhen you create a VM, you request sole-tenancy by specifying node affinity or anti-affinity, referencing one or more node affinity labels. You specify custom node affinity labels when you create a node template, and Compute Engine automatically includes some default affinity labels on each node. By specifying affinity when you create a VM, you can schedule VMs together on a specific node or nodes in a node group. By specifying anti-affinity when you create a VM, you can ensure that certain VMs are not scheduled together on the same node or nodes in a node group.\n\nNode affinity labels are key-value pairs assigned to nodes, and are inherited from a node template. Affinity labels let you:\n\nControl how individual VM instances are assigned to nodes.\nControl how VM instances created from a template, such as those created by a managed instance group, are assigned to nodes.\nGroup sensitive VM instances on specific nodes or node groups, separate from other VMs."
      },
      {
        "date": "2022-02-15T14:25:00.000Z",
        "voteCount": 1,
        "content": "2/15/21"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 158,
    "url": "https://www.examtopics.com/discussions/google/view/8340-exam-professional-cloud-architect-topic-1-question-158/",
    "body": "Your company's test suite is a custom C++ application that runs tests throughout each day on Linux virtual machines. The full test suite takes several hours to complete, running on a limited number of on-premises servers reserved for testing. Your company wants to move the testing infrastructure to the cloud, to reduce the amount of time it takes to fully test a change to the system, while changing the tests as little as possible.<br>Which cloud infrastructure should you recommend?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGoogle Compute Engine unmanaged instance groups and Network Load Balancer",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGoogle Compute Engine managed instance groups with auto-scaling\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGoogle Cloud Dataproc to run Apache Hadoop jobs to process each test",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGoogle App Engine with Google StackDriver for logging"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2019-11-16T07:07:00.000Z",
        "voteCount": 24,
        "content": "B, https://cloud.google.com/compute/docs/autoscaler/"
      },
      {
        "date": "2020-08-05T01:28:00.000Z",
        "voteCount": 7,
        "content": "B is ok"
      },
      {
        "date": "2023-02-06T05:20:00.000Z",
        "voteCount": 3,
        "content": "Changing the tests as little as possible rules out C &amp; D.\nTest takes several hours and you need to improve perfromace.  Autocaling with MIG will do it\nUnmanaged group cannot autosacle.  Load balancer will not improve perfromance"
      },
      {
        "date": "2022-11-01T09:59:00.000Z",
        "voteCount": 2,
        "content": "Why not A? the custom APP may be not supporto autoscaling...."
      },
      {
        "date": "2023-02-06T05:21:00.000Z",
        "voteCount": 1,
        "content": "Changing the tests as little as possible rules out C &amp; D.\nTest takes several hours and you need to improve perfromace.  Autocaling with MIG will do it\nUnmanaged group cannot autosacle.  Load balancer will not improve perfromance"
      },
      {
        "date": "2023-12-27T23:19:00.000Z",
        "voteCount": 1,
        "content": "I second the question. The App might not support horizontal scaling.\nBut I also admit that no other answer is valid. \nA: The Load Balancer offers no benefit.\nC: Hadoop doesn't process C++\nD: App Engine is for web apps."
      },
      {
        "date": "2022-10-19T17:55:00.000Z",
        "voteCount": 1,
        "content": "B is the right answer"
      },
      {
        "date": "2022-10-15T11:54:00.000Z",
        "voteCount": 2,
        "content": "B is right"
      },
      {
        "date": "2022-01-23T07:55:00.000Z",
        "voteCount": 1,
        "content": "choose b"
      },
      {
        "date": "2021-12-14T20:28:00.000Z",
        "voteCount": 1,
        "content": "B is the correct answer"
      },
      {
        "date": "2021-11-26T01:24:00.000Z",
        "voteCount": 3,
        "content": "vote B"
      },
      {
        "date": "2021-08-19T22:34:00.000Z",
        "voteCount": 2,
        "content": "New Question \nYour company has a Kubernetes application that pulls messages from Pub/Sub and stores them in Filestore. Because the application is simple, it was deployed as a single pod. The infrastructure team has analyzed Pub/Sub metrics and discovered that the application cannot process the messages in real time. Most of them wait for minutes before being processed. You need to scale the elaboration process that is 1/0-intensive. What should you do? \n\nA.\tUsekubectl autoscale deployment APP_NAME --max 6 --min 2 --cpu-percent 50 to \nconfigure Kubernetes autoscaling deployment. \nB. Configure a Kubemetes autoscaling deployment based on the \nsubscription/push_request_latencies metric. \nC. Use the --enable-autoscaling flag when you create the Kubernetes cluster. \nD. Configure a Kubernetes autoscaling deployment based on the subscription/num_undelivered_messages metric."
      },
      {
        "date": "2021-08-21T22:39:00.000Z",
        "voteCount": 4,
        "content": "D is the answer"
      },
      {
        "date": "2021-09-16T00:37:00.000Z",
        "voteCount": 2,
        "content": "D is the correct answer \nhttps://cloud.google.com/kubernetes-engine/docs/tutorials/autoscaling-metrics#pubsub"
      },
      {
        "date": "2021-08-19T22:09:00.000Z",
        "voteCount": 2,
        "content": "New Question\nYour organization has stored sensitive data in a Cloud Storage bucket. For regulatory reasons, your company must be able to rotate the encryption key used to encrypt the data in the bucket. The data will be processed in Dataproc. You want to follow Google-recommended practices for security What should you do? \n\nA. Create a key with Cloud Key Management Service (KMS) Encrypt the data using the encrypt method of Cloud KMS. \nB. Create a key with Cloud Key Management Service (KMS). Set the encryption key on the bucket to the Cloud KMS key. \nC. Generate a GPG key pair. Encrypt the data using the GPG key. Upload the encrypted data to the bucket. \nD. Generate an AES-256 encryption key. Encrypt the data in the bucket using the customer-supplied encryption keys feature.\n\nAnswer Please"
      },
      {
        "date": "2021-08-24T06:48:00.000Z",
        "voteCount": 2,
        "content": "it is B\nhttps://cloud.google.com/storage/docs/encryption/using-customer-managed-keys"
      },
      {
        "date": "2021-10-10T07:11:00.000Z",
        "voteCount": 2,
        "content": "why are you repeating questions from this series as comments?"
      },
      {
        "date": "2021-07-26T17:45:00.000Z",
        "voteCount": 2,
        "content": "Are we getting questions from 1-100 in the exam?"
      },
      {
        "date": "2021-11-05T11:05:00.000Z",
        "voteCount": 2,
        "content": "Yeah I have the same question in mind."
      },
      {
        "date": "2021-07-19T07:16:00.000Z",
        "voteCount": 3,
        "content": "Google Compute Engine and with MIG for auto-scaling"
      },
      {
        "date": "2021-07-06T17:09:00.000Z",
        "voteCount": 2,
        "content": "Agree with Option B.Google Compute Managed instance groups with auto-scaling"
      },
      {
        "date": "2021-06-04T14:30:00.000Z",
        "voteCount": 3,
        "content": "agree with B"
      },
      {
        "date": "2021-05-17T23:22:00.000Z",
        "voteCount": 2,
        "content": "B. Google Compute Engine managed instance groups with auto-scaling"
      },
      {
        "date": "2021-05-03T09:58:00.000Z",
        "voteCount": 2,
        "content": "B is  correct"
      },
      {
        "date": "2021-04-18T19:59:00.000Z",
        "voteCount": 2,
        "content": "B should be the one"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 159,
    "url": "https://www.examtopics.com/discussions/google/view/8341-exam-professional-cloud-architect-topic-1-question-159/",
    "body": "A lead software engineer tells you that his new application design uses websockets and HTTP sessions that are not distributed across the web servers. You want to help him ensure his application will run properly on Google Cloud Platform.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tHelp the engineer to convert his websocket code to use HTTP streaming",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReview the encryption requirements for websocket connections with the security team",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMeet with the cloud operations team and the engineer to discuss load balancer options\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tHelp the engineer redesign the application to use a distributed user session service that does not rely on websockets and HTTP sessions."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 11,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2019-11-16T07:08:00.000Z",
        "voteCount": 16,
        "content": "I agree with C"
      },
      {
        "date": "2021-01-10T07:18:00.000Z",
        "voteCount": 6,
        "content": "https://cloud.google.com/load-balancing/docs/https#websocket_support"
      },
      {
        "date": "2020-08-05T01:30:00.000Z",
        "voteCount": 8,
        "content": "C is ok"
      },
      {
        "date": "2021-10-15T13:12:00.000Z",
        "voteCount": 4,
        "content": "The key line from the link above: \n\nSession affinity for WebSockets works the same as for any other request. For information, see Session affinity."
      },
      {
        "date": "2021-03-24T00:44:00.000Z",
        "voteCount": 7,
        "content": "IMO C is ok.\nBeside the reasons mentioned above regarding why A, B and D are wrong, there are also:\nA and D are wrong because are abot changing the app - whereas in the task \"You want to help him ensure his application will run properly on GCP\" (not REDESIGN/CHANGE).\nB is wrong because you don't have to \"Review the encryption requirements for websocket connections with the security team\"..."
      },
      {
        "date": "2022-11-19T23:18:00.000Z",
        "voteCount": 1,
        "content": "thanks"
      },
      {
        "date": "2023-12-27T23:31:00.000Z",
        "voteCount": 1,
        "content": "I think that, since the app is in design stage, it's totally valid to change its design to adapt to work better in the cloud. Websockets and HTTP session, while supported, are not the optimal choice for apps in the cloud.\nSome user said that the question asks for help ensure his app runs on GCP, so we shouldn't change it. But I don't think that's the case. As architects we should oversee any design that developers and engineers are introducing to the organization's architecture."
      },
      {
        "date": "2023-01-07T20:45:00.000Z",
        "voteCount": 3,
        "content": "C is the correct answer,\nGoogle Cloud HTTP(S)-based load balancers have native support for the WebSocket protocol when you use HTTP or HTTPS as the protocol to the backend. The load balancer does not need any configuration to proxy WebSocket connections.\n\nhttps://cloud.google.com/load-balancing/docs/https#websocket_support"
      },
      {
        "date": "2023-01-01T15:34:00.000Z",
        "voteCount": 1,
        "content": "The answer is D. C. is not the best answer because it does not address the issue of websockets and HTTP sessions not being distributed across the web servers. While load balancer options may be relevant to the overall operation of the application, they do not address the specific issue of ensuring that the websockets and HTTP sessions are properly distributed. A better solution would be to help the engineer redesign the application to use a distributed user session service that does not rely on websockets and HTTP sessions, as this would address the issue of session distribution. Alternatively, the engineer could consider converting their websocket code to use HTTP streaming, which could potentially help with session distribution."
      },
      {
        "date": "2023-12-27T23:26:00.000Z",
        "voteCount": 1,
        "content": "Totally agree. But you didn't vote to change the C hegemony :P"
      },
      {
        "date": "2022-10-15T11:57:00.000Z",
        "voteCount": 2,
        "content": "C is fine."
      },
      {
        "date": "2022-08-14T03:32:00.000Z",
        "voteCount": 2,
        "content": "I agree with C"
      },
      {
        "date": "2021-12-14T20:38:00.000Z",
        "voteCount": 2,
        "content": "C is the correct answer"
      },
      {
        "date": "2021-11-26T01:25:00.000Z",
        "voteCount": 4,
        "content": "vote C"
      },
      {
        "date": "2021-08-21T10:37:00.000Z",
        "voteCount": 2,
        "content": "New Case Study Question- TerramEarth\n\nFor this question, refer to the TerramEarth case study. \n\nYou are building a microservice-based application for TerramEarth. \nThe application is based on Docker containers. You want to follow Google-recommended practices to build the application continuously and store the build artifacts. What should you do?"
      },
      {
        "date": "2021-08-21T10:38:00.000Z",
        "voteCount": 2,
        "content": "A) Configure a trigger in Cloud Build for new source changes. Invoke Cloud Build to build container images for each microservice, and tag them using the code commit hash. Push the images to the Container Registry. \n\nB)Configure a trigger in Cloud Build for new source changes. The trigger invokes build jobs and build container images for the microservices. Tag the images with a version number, and push them to Cloud Storage. \n\nC) Create a Scheduler job to check the repo every minute. For any new change, invoke Cloud Build to build container images for the microservices. Tag the images using the current timestamp, and push them to the Container Registry. \n\nD) Configure a trigger in Cloud Build for new source changes. Invoke Cloud Build to build one container image, and tag the image with the label 'latest' Push the image to the Container Registry"
      },
      {
        "date": "2021-08-26T14:41:00.000Z",
        "voteCount": 5,
        "content": "A is ok"
      },
      {
        "date": "2022-10-15T11:57:00.000Z",
        "voteCount": 2,
        "content": "A looks good to me."
      },
      {
        "date": "2021-07-24T18:42:00.000Z",
        "voteCount": 1,
        "content": "Why D is wrong is the wording \"doesn't rely on\". This means the application needs to use other protocols instead of http or websocket. This is not realistic and requires too much application refactoring. Actually a distributed session service is possible with http or websocket as long as the session information is stored in shared storage such as nosql database or redis that can be accessed by all web servers. In this sense, D is wrong answer."
      },
      {
        "date": "2023-12-27T23:34:00.000Z",
        "voteCount": 1,
        "content": "There is no refactor since the app is only in design stage. Read the question carefully:\n''A lead software engineer tells you that his NEW APPLICATION DESIGN...''"
      },
      {
        "date": "2021-07-19T07:23:00.000Z",
        "voteCount": 1,
        "content": "C is fine. Global HTTP(S) load Balancer supports webSockets."
      },
      {
        "date": "2021-07-14T20:15:00.000Z",
        "voteCount": 3,
        "content": "hey guys new Qs posted as of July 12th, 2021, All 21 new Qs in Question #152"
      },
      {
        "date": "2021-07-13T23:51:00.000Z",
        "voteCount": 1,
        "content": "I also agree with C. the answer D could be ok but the question says \"his new application design \" so it means that the app has just been developed and deployed so there's no convenience to redesign it from scratch to avoid use of sessions and websocket."
      },
      {
        "date": "2022-11-19T23:16:00.000Z",
        "voteCount": 1,
        "content": "from where you got the \"new\" word? ha ha"
      },
      {
        "date": "2023-12-27T23:37:00.000Z",
        "voteCount": 1,
        "content": "From the question, ninth word from the start.\nAlso, ''design'' is an important word. It means the app hasn't been built yet. Else it would have said ''his new application'' only."
      },
      {
        "date": "2021-05-17T23:21:00.000Z",
        "voteCount": 1,
        "content": "C. Meet with the cloud operations team and the engineer to discuss load balancer options"
      },
      {
        "date": "2021-05-03T09:59:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2021-03-28T22:39:00.000Z",
        "voteCount": 1,
        "content": "Answer is C"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 160,
    "url": "https://www.examtopics.com/discussions/google/view/54369-exam-professional-cloud-architect-topic-1-question-160/",
    "body": "The application reliability team at your company this added a debug feature to their backend service to send all server events to Google Cloud Storage for eventual analysis. The event records are at least 50 KB and at most 15 MB and are expected to peak at 3,000 events per second. You want to minimize data loss.<br>Which process should you implement?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\u05d2\u20ac\u00a2 Append metadata to file body \u05d2\u20ac\u00a2 Compress individual files \u05d2\u20ac\u00a2 Name files with serverName \u05d2\u20ac\" Timestamp \u05d2\u20ac\u00a2 Create a new bucket if bucket is older than 1 hour and save individual files to the new bucket. Otherwise, save files to existing bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\u05d2\u20ac\u00a2 Batch every 10,000 events with a single manifest file for metadata \u05d2\u20ac\u00a2 Compress event files and manifest file into a single archive file \u05d2\u20ac\u00a2 Name files using serverName \u05d2\u20ac\" EventSequence \u05d2\u20ac\u00a2 Create a new bucket if bucket is older than 1 day and save the single archive file to the new bucket. Otherwise, save the single archive file to existing bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\u05d2\u20ac\u00a2 Compress individual files \u05d2\u20ac\u00a2 Name files with serverName \u05d2\u20ac\" EventSequence \u05d2\u20ac\u00a2 Save files to one bucket \u05d2\u20ac\u00a2 Set custom metadata headers for each object after saving",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\u05d2\u20ac\u00a2 Append metadata to file body \u05d2\u20ac\u00a2 Compress individual files \u05d2\u20ac\u00a2 Name files with a random prefix pattern \u05d2\u20ac\u00a2 Save files to one bucket\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 9,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-06-04T09:25:00.000Z",
        "voteCount": 36,
        "content": "answer is definitely D\nhttps://cloud.google.com/storage/docs/request-rate#naming-convention\n \"A longer randomized prefix provides more effective auto-scaling when ramping to very high read and write rates. For example, a 1-character prefix using a random hex value provides effective auto-scaling from the initial 5000/1000 reads/writes per second up to roughly 80000/16000 reads/writes per second, because the prefix has 16 potential values. If your use case does not need higher rates than this, a 1-character randomized prefix is just as effective at ramping up request rates as a 2-character or longer randomized prefix.\"\n Example: \nmy-bucket/2fa764-2016-05-10-12-00-00/file1 \nmy-bucket/5ca42c-2016-05-10-12-00-00/file2 \nmy-bucket/6e9b84-2016-05-10-12-00-01/file3"
      },
      {
        "date": "2021-06-29T20:16:00.000Z",
        "voteCount": 6,
        "content": "- New Q, 06/2021\nHelicopter Racing League Testlet 1\nCompany overview\nQUESTION 6\nFor this question, refer to the Helicopter Racing League (HRL) case study. A recent finance audit of cloud infrastructure noted an exceptionally high number of Compute Engine instances are allocated to do video encoding and transcoding. You suspect that these Virtual Machines are zombie machines that were not deleted after their workloads completed. You need to quickly get a list of which VM instances are idle. What should you do?\nA.\tLog into each Compute Engine instance and collect disk, CPU, memory, and network usage statistics for analysis.\nB.\tUse the gcloud compute instances list to list the virtual machine instances that have the idle: true label set.\nC.\tUse the gcloud recommender command to list the idle virtual machine instances.\nD.\tFrom the Google Console, identify which Compute Engine instances in the managed instance groups are no longer responding to health check probes."
      },
      {
        "date": "2021-08-06T05:59:00.000Z",
        "voteCount": 2,
        "content": "answer C"
      },
      {
        "date": "2021-07-01T06:23:00.000Z",
        "voteCount": 1,
        "content": "is it C?"
      },
      {
        "date": "2021-07-02T11:39:00.000Z",
        "voteCount": 4,
        "content": "this is not 100% accurate. you should investigate if you doubt if is incorrect\nhttps://cloud.google.com/compute/docs/instances/viewing-and-applying-idle-vm-recommendations"
      },
      {
        "date": "2021-07-14T23:03:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is A"
      },
      {
        "date": "2021-11-17T02:04:00.000Z",
        "voteCount": 2,
        "content": "Absulatly C"
      },
      {
        "date": "2023-12-14T04:38:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is C based on the URL you shared.                                                  gcloud recommender recommendations list \\\n  --project=PROJECT_ID \\\n  --location=ZONE \\\n  --recommender=google.compute.instance.IdleResourceRecommender \\\n  --format=yaml"
      },
      {
        "date": "2021-07-16T00:15:00.000Z",
        "voteCount": 3,
        "content": "I have my exam scheduled after 3 days. Would there be more questions coming on ExamTopics?"
      },
      {
        "date": "2021-07-01T10:05:00.000Z",
        "voteCount": 8,
        "content": "answer: C"
      },
      {
        "date": "2024-06-20T08:39:00.000Z",
        "voteCount": 1,
        "content": "This question is messed up. The formatting, the discussion, everything. I have no idea what to choose here. Chat GPT thinks the answer is C but most think it is D and there's not much difference between the two answers."
      },
      {
        "date": "2023-12-14T04:48:00.000Z",
        "voteCount": 2,
        "content": "The question is how to reduce the data loss, the answer should be something like separation of duty, data lost prevention, but answer D is for reducing latency retrieving data. I'm baffled by this question."
      },
      {
        "date": "2023-10-07T12:38:00.000Z",
        "voteCount": 1,
        "content": "I agree with D, but then, using a random prefix wouldn't it make more difficult the file retrieve?"
      },
      {
        "date": "2023-05-28T08:16:00.000Z",
        "voteCount": 3,
        "content": "Why not option B??"
      },
      {
        "date": "2023-01-25T23:34:00.000Z",
        "voteCount": 1,
        "content": "I was thinking correct answer was A, because we should have some kind of bucket rotation in order to avoid hiting the max size of a bucket.\nHowever it seems there is no size limit for a GCP cloud bucket, so I will have to agree with community and stick to answer D."
      },
      {
        "date": "2022-10-19T17:59:00.000Z",
        "voteCount": 2,
        "content": "D is the correct answer\nhttps://cloud.google.com/storage/docs/request-rate#naming-convention"
      },
      {
        "date": "2022-01-23T07:57:00.000Z",
        "voteCount": 2,
        "content": "D: https://cloud.google.com/storage/docs/request-rate#naming-convention"
      },
      {
        "date": "2021-12-12T19:47:00.000Z",
        "voteCount": 2,
        "content": "D is the correct answer"
      },
      {
        "date": "2021-11-26T17:10:00.000Z",
        "voteCount": 5,
        "content": "vote D"
      },
      {
        "date": "2021-09-11T21:44:00.000Z",
        "voteCount": 4,
        "content": "Request admin to intervene and delete the hijacking of the question by  kopper2019"
      },
      {
        "date": "2021-09-16T08:45:00.000Z",
        "voteCount": 5,
        "content": "Use the material for study dude! Hello? Anyone home?"
      },
      {
        "date": "2021-11-26T12:49:00.000Z",
        "voteCount": 1,
        "content": "it looks like this website does not have any admin"
      },
      {
        "date": "2021-06-29T20:15:00.000Z",
        "voteCount": 3,
        "content": "- New Q, 06/2021\nHelicopter Racing League Testlet 1\nCompany overview\nQUESTION 5\nFor this question, refer to the Helicopter Racing League (HRL) case study. HRL is looking for a cost- effective approach for storing their race data such as telemetry. They want to keep all historical records, train models using only the previous season's data, and plan for data growth in terms of volume and information collected. You need to propose a data solution. Considering HRL business requirements and the goals expressed by CEO S. Hawke, what should you do?\nA.\tUse Firestore for its scalable and flexible document-based database. Use collections to aggregate race data by season and event.\nB.\tUse Cloud Spanner for its scalability and ability to version schemas with zero downtime. Split race data using season as a primary key.\nC.\tUse BigQuery for its scalability and ability to add columns to a schema. Partition race data based on season.\nD.\tUse Cloud SQL for its ability to automatically manage storage increases and compatibility with MySQL. Use separate database instances for each season."
      },
      {
        "date": "2021-07-01T10:06:00.000Z",
        "voteCount": 2,
        "content": "answer: C"
      },
      {
        "date": "2021-07-14T23:05:00.000Z",
        "voteCount": 2,
        "content": "Yes answer is C"
      },
      {
        "date": "2021-07-01T06:27:00.000Z",
        "voteCount": 2,
        "content": "is it C?\nall these questions are from the new exam? why they are here in the comments and not as questions in the list?"
      },
      {
        "date": "2021-07-02T19:24:00.000Z",
        "voteCount": 4,
        "content": "because exam was not updated so I added the Qs but they added this new Qs as normal now we have 218 Qs"
      },
      {
        "date": "2021-09-30T19:35:00.000Z",
        "voteCount": 1,
        "content": "Hey Kopper, when would you provide the new set of questions ?"
      },
      {
        "date": "2021-08-06T06:04:00.000Z",
        "voteCount": 1,
        "content": "answer: C"
      },
      {
        "date": "2021-06-29T20:15:00.000Z",
        "voteCount": 3,
        "content": "- New Q, 06/2021\nHelicopter Racing League Testlet 1\nCompany overview\nQUESTION 4\nFor this question, refer to the Helicopter Racing League (HRL) case study. HRL wants better prediction accuracy from their ML prediction models. They want you to use Google\u2019s AI Platform so HRL can understand and interpret the predictions. What should you do?\n\nA.\tUse Explainable AI.\nB.\tUse Vision AI.\nC.\tUse Google Cloud\u2019s operations suite.\nD.\tUse Jupyter Notebooks."
      },
      {
        "date": "2024-06-20T08:38:00.000Z",
        "voteCount": 1,
        "content": "what does this have to do with the cloud storage question?"
      },
      {
        "date": "2021-07-01T10:07:00.000Z",
        "voteCount": 4,
        "content": "answer: A"
      },
      {
        "date": "2021-07-01T06:28:00.000Z",
        "voteCount": 2,
        "content": "is it A?"
      },
      {
        "date": "2021-07-14T23:06:00.000Z",
        "voteCount": 1,
        "content": "Yes answer is A"
      },
      {
        "date": "2021-08-06T06:04:00.000Z",
        "voteCount": 1,
        "content": "answer A"
      },
      {
        "date": "2021-06-29T20:15:00.000Z",
        "voteCount": 2,
        "content": "- New Q, 06/2021\nHelicopter Racing League Testlet 1\nCompany overview\nQUESTION 3\nFor this question, refer to the Helicopter Racing League (HRL) case study. The HRL development team releases a new version of their predictive capability application every Tuesday evening at 3 a.m. UTC to a repository. The security team at HRL has developed an in-house penetration test Cloud Function called Airwolf. The security team wants to run Airwolf against the predictive capability application as soon as it is released every Tuesday. You need to set up Airwolf to run at the recurring weekly cadence. What should you do?\nA.\tSet up Cloud Tasks and a Cloud Storage bucket that triggers a Cloud Function.\nB.\tSet up a Cloud Logging sink and a Cloud Storage bucket that triggers a Cloud Function.\nC.\tConfigure the deployment job to notify a Pub/Sub queue that triggers a Cloud Function.\nD.\tSet up Identity and Access Management (IAM) and Confidential Computing to trigger a Cloud Function."
      },
      {
        "date": "2021-10-20T20:24:00.000Z",
        "voteCount": 2,
        "content": "C, is the right answer. The scheduler would run without a trigger even though the release has not been done. If you read (application as soon as it is released ), the time is not certain. So, the answer is C. Check out the last 30 questions, would give a better idea as there is a separate discussion"
      },
      {
        "date": "2021-07-03T21:40:00.000Z",
        "voteCount": 5,
        "content": "answer : A"
      },
      {
        "date": "2021-11-29T17:46:00.000Z",
        "voteCount": 1,
        "content": "why A? Does Cloud Storage make sense ?"
      },
      {
        "date": "2021-09-22T11:50:00.000Z",
        "voteCount": 3,
        "content": "in option A what is the use of Cloud storage bucket? In my opinion answer is C."
      },
      {
        "date": "2021-07-14T23:07:00.000Z",
        "voteCount": 2,
        "content": "Answer is A"
      },
      {
        "date": "2022-04-06T14:59:00.000Z",
        "voteCount": 1,
        "content": "I would go with C\nhttps://cloud.google.com/source-repositories/docs/code-change-notification"
      },
      {
        "date": "2023-06-12T11:15:00.000Z",
        "voteCount": 1,
        "content": "It's probably C due to pub sub on Cloud Deploy rather than source repos\nhttps://cloud.google.com/deploy/docs/subscribe-deploy-notifications"
      },
      {
        "date": "2021-06-29T20:13:00.000Z",
        "voteCount": 1,
        "content": "- New Q, 06/2021\nHelicopter Racing League Testlet 1\nCompany overview\n\nQUESTION 2\nFor this question, refer to the Helicopter Racing League (HRL) case study. Recently HRL started a new regional racing league in Cape Town, South Africa. In an effort to give customers in Cape Town a better user experience, HRL has partnered with the Content Delivery Network provider, Fastly. HRL needs to allow traffic coming from all of the Fastly IP address ranges into their Virtual Private Cloud network (VPC network). You are a member of the HRL security team and you need to configure the update that will allow only the Fastly IP address ranges through the External HTTP(S) load balancer. Which command should you use?"
      },
      {
        "date": "2021-06-29T20:13:00.000Z",
        "voteCount": 1,
        "content": "A.\tgcloud compute security-policies rules update 1000 \\\n--security-policy from-fastly \\\n--src-ip-ranges * \\\n--action \u201callow\u201d\nB.\tgcloud compute firewall rules update sourceiplist-fastly \\\n--priority 100 \\\n--allow tcp:443\nC.\tgcloud compute firewall rules update hir-policy \\\n--priority 100 \\\n--target-tags=sourceiplist-fastly \\\n--allow tcp:443\nD.\tgcloud compute security-policies rules update 1000 \\\n--security-policy hir-policy \\\n--expression \u201cevaluatePreconfiguredExpr(\u2018sourceiplist-fastly\u2019)\u201d \\\n--action \u201callow\u201d"
      },
      {
        "date": "2021-07-01T10:11:00.000Z",
        "voteCount": 6,
        "content": "answer: D"
      },
      {
        "date": "2021-07-14T23:08:00.000Z",
        "voteCount": 2,
        "content": "Answer is A"
      },
      {
        "date": "2021-11-18T11:21:00.000Z",
        "voteCount": 1,
        "content": "A is incorrect : To match all IPs specify *\nhttps://cloud.google.com/sdk/gcloud/reference/compute/security-policies/rules/update"
      },
      {
        "date": "2021-08-06T06:05:00.000Z",
        "voteCount": 4,
        "content": "answer D"
      },
      {
        "date": "2021-08-16T01:14:00.000Z",
        "voteCount": 5,
        "content": "both A and D have correct syntax, but src-ip-ranges cannot be \"*\", correct is D"
      },
      {
        "date": "2022-04-06T15:21:00.000Z",
        "voteCount": 1,
        "content": "I agree"
      },
      {
        "date": "2021-06-29T20:09:00.000Z",
        "voteCount": 1,
        "content": "- New Q, 06/2021 \nHelicopter Racing League Testlet 1\nCompany overview\n\nHelicopter Racing League (HRL) is a global sports league for competitive helicopter racing. Each year HRL holds the world championship and several regional league competitions where teams compete to earn a spot in the world championship. HRL offers a paid service to stream the races all over the world with live telemetry and predictions throughout each race.\n\nSolution concept\n\nHRL wants to migrate their existing service to a new platform to expand their use of managed AI and ML services to facilitate race predictions. Additionally, as new fans engage with the sport, particularly in emerging regions, they want to move the serving of their content, both real-time and recorded, closer to their users."
      },
      {
        "date": "2021-06-29T20:10:00.000Z",
        "voteCount": 1,
        "content": "Existing technical environment\n\nHRL is a public cloud-first company; the core of their mission-critical applications runs on their current public cloud provider. Video recording and editing is performed at the race tracks, and the content is encoded and transcoded, where needed, in the cloud. Enterprise-grade connectivity and local compute is provided by truck-mounted mobile data centers. Their race prediction services are hosted exclusively on their existing public cloud provider. Their existing technical environment is as follows:\n\n- Existing content is stored in an object storage service on their existing public cloud provider. \nVideo encoding and transcoding is performed on VMs created for each job.\nRace predictions are performed using TensorFlow running on VMs in the current public cloud provider."
      },
      {
        "date": "2021-06-29T20:11:00.000Z",
        "voteCount": 1,
        "content": "Business requirements\nHRL\u2019s owners want to expand their predictive capabilities and reduce latency for their viewers in emerging markets. Their requirements are:\n\nSupport ability to expose the predictive models to partners. Increase predictive capabilities during and before races:\n\u25cb\tRace results\n\u25cb\tMechanical failures\n\u25cb\tCrowd sentiment\nIncrease telemetry and create additional insights. Measure fan engagement with new predictions. Enhance global availability and quality of the broadcasts. Increase the number of concurrent viewers.\nMinimize operational complexity. Ensure compliance with regulations.\nCreate a merchandising revenue stream.\n\nTechnical requirements\nMaintain or increase prediction throughput and accuracy. Reduce viewer latency.\nIncrease transcoding performance.\nCreate real-time analytics of viewer consumption patterns and engagement. Create a data mart to enable processing of large volumes of race data."
      },
      {
        "date": "2021-06-29T20:11:00.000Z",
        "voteCount": 1,
        "content": "Executive statement\n\nOur CEO, S. Hawke, wants to bring high-adrenaline racing to fans all around the world. We listen to our fans, and they want enhanced video streams that include predictions of events within the race (e.g., overtaking). Our current platform allows us to predict race outcomes but lacks the facility to support real- time predictions during races and the capacity to process season-long results."
      },
      {
        "date": "2021-06-29T20:11:00.000Z",
        "voteCount": 2,
        "content": "QUESTION 1\nFor this question, refer to the Helicopter Racing League (HRL) case study. Your team is in charge of creating a payment card data vault for card numbers used to bill tens of thousands of viewers, merchandise consumers, and season ticket holders. You need to implement a custom card tokenization service that meets the following requirements:\n\u2022\tIt must provide low latency at minimal cost.\n\u2022\tIt must be able to identify duplicate credit cards and must not store plaintext card numbers.\n\u2022\tIt should support annual key rotation.\n\nWhich storage approach should you adopt for your tokenization service?\n\nA.\tStore the card data in Secret Manager after running a query to identify duplicates.\nB.\tEncrypt the card data with a deterministic algorithm stored in Firestore using Datastore mode.\nC.\tEncrypt the card data with a deterministic algorithm and shard it across multiple Memorystore instances.\nD.\tUse column-level encryption to store the data in Cloud SQL."
      },
      {
        "date": "2021-08-03T08:59:00.000Z",
        "voteCount": 1,
        "content": "Why D ?"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 161,
    "url": "https://www.examtopics.com/discussions/google/view/7016-exam-professional-cloud-architect-topic-1-question-161/",
    "body": "A recent audit revealed that a new network was created in your GCP project. In this network, a GCE instance has an SSH port open to the world. You want to discover this network's origin.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSearch for Create VM entry in the Stackdriver alerting console",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNavigate to the Activity page in the Home section. Set category to Data Access and search for Create VM entry",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the Logging section of the console, specify GCE Network as the logging section. Search for the Create Insert entry\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConnect to the GCE instance using project SSH keys. Identify previous logins in system logs, and match these with the project owners list"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2019-10-22T10:26:00.000Z",
        "voteCount": 17,
        "content": "When you search for Create Insert, it displays a JSON code string that contains the creators e-mail"
      },
      {
        "date": "2020-08-05T01:48:00.000Z",
        "voteCount": 14,
        "content": "C is ok"
      },
      {
        "date": "2020-05-09T11:33:00.000Z",
        "voteCount": 16,
        "content": "I am going to go with C.  Answer A doesn't seem to fit because the matter of when a VM was created.\nAnswer B focuses on Data Access logs which doesn't seem to fit since the matter of creating a network firewall rule\nis an Admin activity, not a data access activity.\nD focuses on who logged in which is good to know but doesn't answer the question of how the network was created.\nC focuses on logging, the selection of network events, and the Create/Insert entry."
      },
      {
        "date": "2023-02-28T10:27:00.000Z",
        "voteCount": 1,
        "content": "C is ok to me!"
      },
      {
        "date": "2023-01-01T15:47:00.000Z",
        "voteCount": 1,
        "content": "Option C is incorrect because the GCE Network logs are not the correct place to search for the creation of a VM instance. The correct place to search for this information is the Activity page, as specified in option B."
      },
      {
        "date": "2023-01-12T02:35:00.000Z",
        "voteCount": 3,
        "content": "Question is asking about network origin creation not VM creation. that's why is C"
      },
      {
        "date": "2022-11-10T02:39:00.000Z",
        "voteCount": 1,
        "content": "C is ok"
      },
      {
        "date": "2022-10-19T18:02:00.000Z",
        "voteCount": 1,
        "content": "C is the right answer"
      },
      {
        "date": "2022-10-15T12:03:00.000Z",
        "voteCount": 2,
        "content": "C is right"
      },
      {
        "date": "2022-04-06T11:50:00.000Z",
        "voteCount": 11,
        "content": "Sorry to gripe again, but why on Earth would anybody need to remember this from the top of their mind. You will never be in a situation in which you need to remember this without looking at the available options in the console (or simply Googling it, lol)."
      },
      {
        "date": "2021-12-14T20:44:00.000Z",
        "voteCount": 1,
        "content": "C is the correct answer"
      },
      {
        "date": "2021-12-09T02:31:00.000Z",
        "voteCount": 1,
        "content": "Vote C"
      },
      {
        "date": "2021-11-26T01:38:00.000Z",
        "voteCount": 2,
        "content": "vote C"
      },
      {
        "date": "2021-08-08T14:28:00.000Z",
        "voteCount": 2,
        "content": "In Logs Explorer , Filter \"resource.type=\"gce_firewall_rule\" and Query insert Create\n\nYou would see below and email address\n    \"methodName\": \"v1.compute.firewalls.insert\",\n    \"authorizationInfo\": [\n      {\n        \"permission\": \"compute.firewalls.create\","
      },
      {
        "date": "2021-07-06T17:25:00.000Z",
        "voteCount": 2,
        "content": "Option C is correct, because logging section is the correct choice to get this details"
      },
      {
        "date": "2021-05-17T23:19:00.000Z",
        "voteCount": 2,
        "content": "C - In the Logging section of the console, specify GCE Network as the logging section. Search for the Create Insert entry"
      },
      {
        "date": "2021-05-04T09:51:00.000Z",
        "voteCount": 2,
        "content": "C is correct"
      },
      {
        "date": "2021-03-28T22:44:00.000Z",
        "voteCount": 2,
        "content": "Answer is C"
      },
      {
        "date": "2021-01-14T10:15:00.000Z",
        "voteCount": 3,
        "content": "Agree..C"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 162,
    "url": "https://www.examtopics.com/discussions/google/view/7018-exam-professional-cloud-architect-topic-1-question-162/",
    "body": "You want to make a copy of a production Linux virtual machine in the US-Central region. You want to manage and replace the copy easily if there are changes on the production virtual machine. You will deploy the copy as a new instance in a different project in the US-East region.<br>What steps must you take?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Linux dd and netcat commands to copy and stream the root disk contents to a new virtual machine instance in the US-East region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a snapshot of the root disk and select the snapshot as the root disk when you create a new virtual machine instance in the US-East region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an image file from the root disk with Linux dd command, create a new virtual machine instance in the US-East region",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a snapshot of the root disk, create an image file in Google Cloud Storage from the snapshot, and create a new virtual machine instance in the US-East region using the image file the root disk.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 17,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 7,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2019-10-22T10:55:00.000Z",
        "voteCount": 30,
        "content": "D is correct. A and B are talking about appending the file system to a new VM, not setting it at the root in a new VM set. Option C is not offered within the GCP because the image must be on the GCP platform to run the gcloud of Google Console instructions to create a VM with the image."
      },
      {
        "date": "2024-06-07T09:33:00.000Z",
        "voteCount": 1,
        "content": "you are incorrect. It is D"
      },
      {
        "date": "2020-08-05T01:44:00.000Z",
        "voteCount": 10,
        "content": "D is ok"
      },
      {
        "date": "2020-01-14T03:32:00.000Z",
        "voteCount": 15,
        "content": "Why Not B.\nhttps://cloud.google.com/compute/docs/instances/create-start-instance#createsnapshot\nThis clearly tells we can use snapshot to create a VM instance, and only need a custom image if we need to create many instances. Here we are creating only one."
      },
      {
        "date": "2020-08-24T04:04:00.000Z",
        "voteCount": 13,
        "content": "You can't use the snapshot created by another project"
      },
      {
        "date": "2020-09-22T14:00:00.000Z",
        "voteCount": 7,
        "content": "According to the documentation we can now https://cloud.google.com/compute/docs/disks/create-snapshots"
      },
      {
        "date": "2020-12-07T22:35:00.000Z",
        "voteCount": 11,
        "content": "I think the question has 2 different answers now as Google improve the snapshot function.\nQuoted from the link:\n'You can create snapshots from disks even while they are attached to running instances. Snapshots are global resources, so you can use them to restore data to a new disk or instance within the same project. You can also share snapshots across projects.'"
      },
      {
        "date": "2023-02-03T14:21:00.000Z",
        "voteCount": 4,
        "content": "B would have been the answer in the current context. But as I read carefully, it doesnt mention the step of sharing snapshot across projects. It directly expects to use the snapshot. Hence D may be the right answer!"
      },
      {
        "date": "2021-05-23T00:34:00.000Z",
        "voteCount": 16,
        "content": "Only if its in the same zone: https://cloud.google.com/compute/docs/disks/create-snapshots#sharing_snapshots\n\"Note: The disk must be in the same zone as the instance.\"\n\nBut this is not the case here, we have:\nDifferent zones and different project hence, you must use a bucket."
      },
      {
        "date": "2024-07-26T09:30:00.000Z",
        "voteCount": 1,
        "content": "In the current state of GCP ,\nBoth B and D can be done in 3 gcloud commands each, (and both require equal amount of snapshot handling). \nD is better if you have more than one instance to create but in this case , they are both equally valid."
      },
      {
        "date": "2024-07-26T09:34:00.000Z",
        "voteCount": 1,
        "content": "Edit, \nB might even be easier , you can directly create a snapshot in a target project and when creating a new vm, simply choose it as a root disk, just tried and verified it myself now"
      },
      {
        "date": "2024-06-07T09:33:00.000Z",
        "voteCount": 1,
        "content": "We neet to move from one region o another, so we need snapshot of the root disk and send it to Cloud Storage. In the scenario of using in the same region, it is better tu use instance image, not snapshots of root disk."
      },
      {
        "date": "2024-07-28T10:55:00.000Z",
        "voteCount": 1,
        "content": "U can directly create the snapshot in any target project"
      },
      {
        "date": "2023-12-28T07:54:00.000Z",
        "voteCount": 2,
        "content": "B isn't correct because you have to create a disk first. You cannot create a VM from a snapshot directly.\ngcloud compute disks create DISK_NAME \\\n      --source-snapshot SNAPSHOT_NAME \\\n      --project SOURCE_PROJECT_ID \\ --zone ZONE"
      },
      {
        "date": "2023-11-19T01:55:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/compute/docs/instances/copy-vm-between-projects"
      },
      {
        "date": "2023-10-09T04:36:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/compute/docs/instances/copy-vm-between-projects\nyou have to create Image from Snapshot and share it to the destination project."
      },
      {
        "date": "2023-09-21T06:56:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2023-07-30T00:35:00.000Z",
        "voteCount": 1,
        "content": "B is the correct answer and It is straight forward."
      },
      {
        "date": "2024-06-07T09:35:00.000Z",
        "voteCount": 1,
        "content": "No, B is not correct, try your answer in GCP. Yo will not see your snapshot, because in different regions you don't have visibility of snapshots, that's why we need to move it to Cloud Storage first."
      },
      {
        "date": "2023-03-10T04:42:00.000Z",
        "voteCount": 2,
        "content": "B is incorrect as it doesnt create an image uses the snapshot and hence D is the only corret option"
      },
      {
        "date": "2023-02-28T12:36:00.000Z",
        "voteCount": 1,
        "content": "D seems better, but B actually works too."
      },
      {
        "date": "2023-02-28T11:50:00.000Z",
        "voteCount": 4,
        "content": "https://cloud.google.com/compute/docs/instances/copy-vm-between-projects#zonal-boot-disk"
      },
      {
        "date": "2023-01-26T00:33:00.000Z",
        "voteCount": 2,
        "content": "B is the right one as of 01/2023"
      },
      {
        "date": "2023-01-12T08:18:00.000Z",
        "voteCount": 2,
        "content": "Currently , It is possible to create VM from snapshot within same project, different project or even different organisation. so answer B is more straight forward."
      },
      {
        "date": "2023-01-12T08:19:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/compute/docs/disks/create-snapshots#sharing_snapshots\nhttps://cloud.google.com/compute/docs/disks/create-snapshots#sharing_snapshots_across_orgs"
      },
      {
        "date": "2023-01-08T03:22:00.000Z",
        "voteCount": 1,
        "content": "B is the correct answer,\n\nWe can create VM from snapshot across zones and regions, please read through the link,\n\nhttps://cloud.google.com/compute/docs/instances/moving-instance-across-zones#moving-an-instance-manually"
      },
      {
        "date": "2022-12-15T04:13:00.000Z",
        "voteCount": 1,
        "content": "D is the correct answer"
      },
      {
        "date": "2022-11-13T23:05:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 163,
    "url": "https://www.examtopics.com/discussions/google/view/7020-exam-professional-cloud-architect-topic-1-question-163/",
    "body": "Your company runs several databases on a single MySQL instance. They need to take backups of a specific database at regular intervals. The backup activity needs to complete as quickly as possible and cannot be allowed to impact disk performance.<br>How should you configure the storage?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a cron job to use the gcloud tool to take regular backups using persistent disk snapshots.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMount a Local SSD volume as the backup location. After the backup is complete, use gsutil to move the backup to Google Cloud Storage.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse gcsfise to mount a Google Cloud Storage bucket as a volume directly on the instance and write backups to the mounted location using mysqldump.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMount additional persistent disk volumes onto each virtual machine (VM) instance in a RAID10 array and use LVM to create snapshots to send to Cloud Storage"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 27,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 10,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2019-11-29T06:30:00.000Z",
        "voteCount": 52,
        "content": "I think it's B. If you use a tool like GCFUSE it will write immediatly to GCS which is a cost benefit because you don't need intermediate storage. In this case however \"Quickly as possible\" key for understanding. GCFUSE will write to GCS which is much slower than writing directly to an added SSD. During the write to GCS it would also execute reads for a longer period on the production database. Therefor writing to the extra SSD would be my recommended solution. Offloading from the SSD to GCS would not impact the running database because the data is already separated."
      },
      {
        "date": "2021-11-25T12:08:00.000Z",
        "voteCount": 1,
        "content": "Thanks!"
      },
      {
        "date": "2021-11-30T03:38:00.000Z",
        "voteCount": 3,
        "content": "We cannot attach and mount a local SSD to a running instance. I think it's C (GCFUSE)"
      },
      {
        "date": "2021-08-31T17:51:00.000Z",
        "voteCount": 4,
        "content": "Point for Discussion\nCan local SSD be mounted in a running instance."
      },
      {
        "date": "2021-10-11T07:01:00.000Z",
        "voteCount": 2,
        "content": "Good point, Because Local SSDs are located on the physical machine where your virtual machine instance is running, they can be created only during the instance creation process"
      },
      {
        "date": "2021-09-08T20:12:00.000Z",
        "voteCount": 2,
        "content": "Yes they can. That's precisely why it makes Local SSD a good scratch / temp storage with  very high IOPS. \n\nhttps://cloud.google.com/compute/docs/disks/local-ssd#formatandmount"
      },
      {
        "date": "2022-01-11T09:35:00.000Z",
        "voteCount": 3,
        "content": "No, you cannot attach a local SSD after the instance is created. \n\"Because Local SSDs are located on the physical machine where your virtual machine instance is running, they can be created only during the instance creation process.\" \nThe above is from https://cloud.google.com/compute/docs/disks/local-ssd#formatandmount"
      },
      {
        "date": "2022-09-29T12:10:00.000Z",
        "voteCount": 2,
        "content": "The local SSD can be created only during the VM creation process.\nAfter than you can mount disk for in the destination path for export mysqldump. gsutil is the supported tool that you may used to migrate the dump to bucket."
      },
      {
        "date": "2020-02-25T03:12:00.000Z",
        "voteCount": 15,
        "content": "Ans: B\nPersistent Disk snapshot not required: \"They need to take backups of a specific database at regular intervals.\"\n\n\"The backup activity needs to complete as quickly as possible and cannot be allowed to impact disk performance.\"\n\nThis can be achieved by using both Local SSD &amp; GCS Fuse (mounting GCS as directory), but as the question stats needs to complete as quickly as possible.\n\nGeneral Rule: Any addition of components introduce a latency. I could not get write throughput of GCS &amp; Local SSD, even if we consider both provides same throughput, streaming data through network to GCS Bucket introduce latency. Attached Local SSD has advantage in this case, since there is no network involved.\n\nFrom Local SSD to GCS bucket - copy job does not impact the mysql data disk."
      },
      {
        "date": "2024-06-19T01:55:00.000Z",
        "voteCount": 2,
        "content": "\"A\" is the only one which gives an automated way to do it. All the rest involves a person action"
      },
      {
        "date": "2024-01-20T06:30:00.000Z",
        "voteCount": 2,
        "content": "I'll go with B"
      },
      {
        "date": "2024-01-16T05:59:00.000Z",
        "voteCount": 2,
        "content": "B,https://cloud.google.com/compute/docs/instances/sql-server/best-practices#backing_up"
      },
      {
        "date": "2024-01-11T11:20:00.000Z",
        "voteCount": 1,
        "content": "It says a specific database, not all of it. Otherwise, why not just use the snapshots? They are no cost."
      },
      {
        "date": "2023-12-23T08:57:00.000Z",
        "voteCount": 1,
        "content": "You can only add local ssd to a VM during creation.Hence going with option C"
      },
      {
        "date": "2023-12-23T08:58:00.000Z",
        "voteCount": 1,
        "content": "Refer to link -  https://cloud.google.com/compute/docs/disks/local-ssd#:~:text=You%20can%20only%20add%20Local,the%20types%20that%20do%20not."
      },
      {
        "date": "2023-03-23T17:50:00.000Z",
        "voteCount": 2,
        "content": "Option B would be the best choice for this scenario. Mounting a Local SSD volume as the backup location would ensure high performance and minimal impact on disk performance, while also allowing for quick backups. After the backup is complete, using gsutil to move the backup to Google Cloud Storage would provide a reliable and secure storage location for the backups. This approach is also cost-effective, as Local SSD volumes are less expensive than persistent disks."
      },
      {
        "date": "2023-10-03T07:51:00.000Z",
        "voteCount": 1,
        "content": "Local SSD are considered ephermeral and they are the most cost-effective and they are fast"
      },
      {
        "date": "2023-01-01T15:52:00.000Z",
        "voteCount": 1,
        "content": "B is incorrect. The Local SSD volumes are only available on certain instance types, and they are not suitable for long-term storage as they are ephemeral and are deleted when the instance is deleted or stopped. For long-term storage, it is recommended to use persistent disks or Google Cloud Storage."
      },
      {
        "date": "2023-02-06T19:27:00.000Z",
        "voteCount": 1,
        "content": "I guess umissed the second paryt of the answer B whic says \"After the backup is complete, use gsutil to move the backup to Google Cloud Storage\""
      },
      {
        "date": "2022-12-26T01:04:00.000Z",
        "voteCount": 4,
        "content": "Option B is the most appropriate solution in this case. Mounting a Local SSD volume as the backup location will allow the backups to be taken quickly and efficiently, as Local SSDs have very high I/O performance and low latencies. Additionally, using gsutil to move the backups to Google Cloud Storage after they have been taken will provide a secure and durable storage location for the backups.\n\nA, configuring a cron job to use the gcloud tool to take regular backups using persistent disk snapshots, may not be the most efficient option because persistent disks have relatively lower I/O performance compared to Local SSDs.\n\nC, using gcsfuse to mount a Google Cloud Storage bucket as a volume directly on the instance and writing the backups to the mounted location using mysqldump, may not be the most efficient option because the backups would need to be transferred over the network, which could impact the performance of the backups."
      },
      {
        "date": "2022-12-26T01:04:00.000Z",
        "voteCount": 1,
        "content": "D, mounting additional persistent disk volumes onto each VM instance in a RAID10 array and using LVM to create snapshots to send to Cloud Storage, may not be the most efficient option because it would require additional disk space and setup, and LVM snapshots may not be as fast as Local SSDs for taking backups."
      },
      {
        "date": "2022-10-31T07:20:00.000Z",
        "voteCount": 1,
        "content": "Gcsfuse needs local storage for caching, usually local/non-persistent disks are used for this purpose. With gcsfuse you can have the backend storage mounted as a filesystem on the server. Mysqldump allows for hot database backups.\nOption C provides the automated solution needed to backup and store the database.\nOption B is the manual version where you need to mount the local SSD, run the backup and then transfer it to a bucket manually."
      },
      {
        "date": "2022-10-15T12:07:00.000Z",
        "voteCount": 3,
        "content": "I will go with B"
      },
      {
        "date": "2022-09-17T22:56:00.000Z",
        "voteCount": 10,
        "content": "B is the answer.\n\nhttps://cloud.google.com/compute/docs/instances/sql-server/best-practices#backing_up\nWhen taking regular database backups, be careful not to consume too many persistent disk IOPS. Use the local SSD to stage your backups and then push them to a Cloud Storage bucket."
      },
      {
        "date": "2022-11-20T00:28:00.000Z",
        "voteCount": 1,
        "content": "best answer thank you"
      },
      {
        "date": "2022-08-13T08:36:00.000Z",
        "voteCount": 4,
        "content": "https://cloud.google.com/compute/docs/instances/sql-server/best-practices#backing_up\n\nWhen taking regular database backups, be careful not to consume too many persistent disk IOPS. Use the local SSD to stage your backups and then push them to a Cloud Storage bucket.\n\nThough it is mentioned for SQL Server, the best practices are common for most of the databases. Also it is assumed that the Local SSD are already mounted while creating the VM"
      },
      {
        "date": "2022-07-20T13:04:00.000Z",
        "voteCount": 2,
        "content": "&gt;backups of a specific database"
      },
      {
        "date": "2022-07-17T12:34:00.000Z",
        "voteCount": 5,
        "content": "B - I think this will clear things up. Local SSD is ATTACHED when CREATING the VM. The local SSDs are just LOCATED (on the physical host) where the VM is running. See here. \nhttps://cloud.google.com/compute/docs/disks/add-local-ssd#create_local_ssd\n\nYou can have a VM with locally attached SSD in an unformatted and unmounted state or just not mounted! Maybe it was umounted and now needs to be re-mounted? Answer B says to MOUNT the local SSD. MOUNTING the SSD is done when the VM is RUNNING! We need to assume the VM was built with locally attached SSD but not formatted and mounted yet. See here.\nhttps://cloud.google.com/compute/docs/disks/add-local-ssd#format_and_mount_a_local_ssd_device!"
      },
      {
        "date": "2023-01-12T05:12:00.000Z",
        "voteCount": 1,
        "content": "This clear confusion, Thank you."
      },
      {
        "date": "2022-07-17T12:41:00.000Z",
        "voteCount": 3,
        "content": "Also, When taking regular database backups, be careful not to consume too many persistent disk IOPS. Use the local SSD to stage your backups and then push them to a Cloud Storage bucket. See here under \"formatting secondary disks, backing up.\"\nhttps://cloud.google.com/compute/docs/instances/sql-server/best-practices#formatting_secondary_disks"
      },
      {
        "date": "2022-07-09T06:34:00.000Z",
        "voteCount": 1,
        "content": "It is B. Writing to Local SSD and the fasted method. (Expansive too) Coping from SSD to GCS is slow, yet not affecting  the database."
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 164,
    "url": "https://www.examtopics.com/discussions/google/view/54371-exam-professional-cloud-architect-topic-1-question-164/",
    "body": "You are helping the QA team to roll out a new load-testing tool to test the scalability of your primary cloud services that run on Google Compute Engine with Cloud<br>Bigtable.<br>Which three requirements should they include? (Choose three.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure that the load tests validate the performance of Cloud Bigtable\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a separate Google Cloud project to use for the load-testing environment\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSchedule the load-testing tool to regularly run against the production environment",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure all third-party systems your services use is capable of handling high load",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstrument the production services to record every transaction for replay by the load-testing tool",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstrument the load-testing tool and the target services with detailed logging and metrics collection\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "ABF",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ABF",
        "count": 7,
        "isMostVoted": true
      },
      {
        "answer": "BDF",
        "count": 5,
        "isMostVoted": false
      },
      {
        "answer": "BEF",
        "count": 5,
        "isMostVoted": false
      },
      {
        "answer": "ADF",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-06-04T09:36:00.000Z",
        "voteCount": 30,
        "content": "after reading link:  https://cloud.google.com/bigtable/docs/performance\nA:Run your typical workloads against Bigtable :Always run your own typical workloads against a Bigtable cluster when doing capacity planning, so you can figure out the best resource allocation for your applications.\nB. Create a separate Google Cloud project to use for the load-testing environment\nF : The most important/standard factor of testing, you gather logs and metrics in TEST environment for further scaling."
      },
      {
        "date": "2021-06-17T08:39:00.000Z",
        "voteCount": 4,
        "content": "There is no relevance to D here. So ABF"
      },
      {
        "date": "2021-10-25T01:25:00.000Z",
        "voteCount": 1,
        "content": "I agree. It is important to verity that current BitTable cluster can deal with incoming traffic: \nA cluster must have enough nodes to support its current workload and the amount of data it stores. Otherwise, the cluster might not be able to handle incoming requests, and latency could go up.\nSo although it is a managed service, it does not auto-scale."
      },
      {
        "date": "2021-08-20T00:16:00.000Z",
        "voteCount": 9,
        "content": "AB&amp;F \nCreating a separate project is highly recommended. It gives you total isolation from your product environment, and make sure it will not share the resources with your product env such as service quota"
      },
      {
        "date": "2021-08-20T00:18:00.000Z",
        "voteCount": 1,
        "content": "You won't want load testing to consume the service quotas in your product project\nhttps://cloud.google.com/docs/quota"
      },
      {
        "date": "2024-02-21T19:28:00.000Z",
        "voteCount": 1,
        "content": "It's ABF"
      },
      {
        "date": "2023-12-28T10:49:00.000Z",
        "voteCount": 5,
        "content": "A: No. Not needed since it's a managed GCP product. It'll scale to satisfy demand.\nB: Yes. You could leave it in the same project as the app, but it'll eventually be deployed to production and be a risk if anyone accidentally runs it against prod.\nC: No. You musn't run load testing against prod.\nD: Yes. The capability of the third party systems should be tested. They are another link in the chain and if they are not up to the task, they may be replaced.\nE: No. There is no need to use real data in the requests, this is a load test, not a behavior one.\nF: Yes. Having detailed logs and metrics helps diagnosing problems during the tests."
      },
      {
        "date": "2023-12-05T07:32:00.000Z",
        "voteCount": 1,
        "content": "The quota impact of not using an isolated project in a region with higher quota make me think about ABF"
      },
      {
        "date": "2023-06-28T07:08:00.000Z",
        "voteCount": 1,
        "content": "Don't think it would be this option: Ensure all third-party systems your services use is capable of handling high load. This is some extra information and we are not sure if the application in this question is even using any third party tools."
      },
      {
        "date": "2023-04-04T13:06:00.000Z",
        "voteCount": 5,
        "content": "Here is my take, I respectfully disagree with ya all :)\n\nA. Ensure that the load tests validate the performance of Cloud Bigtable Most Voted\n=&gt; not the requirement\nB. Create a separate Google Cloud project to use for the load-testing environment Most Voted\n=&gt; yes, you don't want to use production quota.\nC. Schedule the load-testing tool to regularly run against the production environment\n=&gt; yes please kill the prod ! \nD. Ensure all third-party systems your services use is capable of handling high load\n=&gt; well, that is what we shall test, so, it was more the task of the development team, not the QA team.\nE. Instrument the production services to record every transaction for replay by the load-testing tool\n=&gt; yes, this way you can build your test dataset with realistic behavior.\nF. Instrument the load-testing tool and the target services with detailed logging and metrics collection Most Vo\n=&gt; yes, otherwise you test for nothing, you have no data at the end to evaluate the system's performance."
      },
      {
        "date": "2023-09-28T04:57:00.000Z",
        "voteCount": 1,
        "content": "E. Instrument the production services to record every transaction for replay by the load-testing tool\n=&gt; yes, this way you can build your test dataset with realistic behavior.\nlol you dont test on prd but still ned prd's records .... how can bro"
      },
      {
        "date": "2022-12-26T01:10:00.000Z",
        "voteCount": 2,
        "content": "Answer ADF\nA: It is important to ensure that the load-testing tool is able to accurately test the performance of Cloud Bigtable in order to ensure that it can handle the expected load.\n\nD: It is important to ensure that all third-party systems that your primary cloud services rely on are able to handle the expected load in order to avoid any potential bottlenecks or failures.\n\nF: Instrumenting the load-testing tool and the target services with detailed logging and metrics collection can provide valuable insights into the performance and behavior of the system under test, allowing the QA team to identify any potential issues or bottlenecks."
      },
      {
        "date": "2022-12-26T01:11:00.000Z",
        "voteCount": 2,
        "content": "Why not B, C and E:\nB: creating a separate Google Cloud project to use for the load-testing environment, could also be a good idea by not necessary in order to ensure that the load tests do not impact the performance of the production environment.\n\nC: scheduling the load-testing tool to regularly run against the production environment, is not recommended, as this could potentially impact the performance of the production environment and could lead to unexpected behavior or issues.\n\nE: instrumenting the production services to record every transaction for replay by the load-testing tool, could also be a useful requirement, as it would allow the QA team to accurately replay real-world workloads during the load tests in order to more accurately simulate the expected production environment."
      },
      {
        "date": "2022-12-15T04:23:00.000Z",
        "voteCount": 1,
        "content": "ABF is the correct answer"
      },
      {
        "date": "2022-10-19T18:16:00.000Z",
        "voteCount": 1,
        "content": "A, B, F are the correct answer"
      },
      {
        "date": "2022-10-19T06:19:00.000Z",
        "voteCount": 4,
        "content": "why testing Bigtable... it's per definition of Google would absorb practically any load... don't you trust Google? :-)"
      },
      {
        "date": "2022-10-15T12:09:00.000Z",
        "voteCount": 2,
        "content": "ABF is right"
      },
      {
        "date": "2022-09-11T16:36:00.000Z",
        "voteCount": 1,
        "content": "There is no necessary reason for running it in a separate project.\nA we have to test Bigtable.\nF Important to record all the outputs and be able to review it.\nD Important to stress test third party solutions or change it."
      },
      {
        "date": "2022-09-17T23:57:00.000Z",
        "voteCount": 2,
        "content": "hi, in the sentence it's underline that you what to test the scalability of your primary CLOUD SERVICS, so I think that D is not required. For me it's ABF"
      },
      {
        "date": "2022-10-08T10:17:00.000Z",
        "voteCount": 2,
        "content": "It is Google best practice to create a separate project for testing"
      },
      {
        "date": "2021-12-14T22:47:00.000Z",
        "voteCount": 1,
        "content": "ABF is the correct answer"
      },
      {
        "date": "2021-11-26T01:44:00.000Z",
        "voteCount": 2,
        "content": "Vote ABF"
      },
      {
        "date": "2021-10-31T00:57:00.000Z",
        "voteCount": 2,
        "content": "BCF\n1) B - you need to have a separate project for Load-Testing tool. That would at least separate role based access - dev and test. Also, test will have their own code/project/config for testing, so no any chance of collision.\n2) C - testing on production? because per Google recommendation, BigTable should be tested on production instances (not on development) and for at least 10 min / 300 GB of data. Check \"Testing Performance with Cloud Bigtable\" here. \nI understand this as a requirement for integration test for projects using BigTable. Testing of BigTable on Dev instances won't give proper results.\n3) F - collecting of metrics would be useful anyway....\nWhy not A, D, E?\n1) A - isolation testing of BigTable likely doesn't make sense, if anyway integration test will need to run. That's covered in C.\n2) D - testing of 3rd party tools in isolated mode likely is a one time effort (only useful when upgrading these tools). No point to run them regularly.\n3) E - collecting metrics on production env just to replay them on load-testing tool? What's a point? We need test max load anyway..."
      },
      {
        "date": "2021-07-18T06:40:00.000Z",
        "voteCount": 1,
        "content": "hey guys new Qs posted as of July 12th, 2021, All 21 new Qs in Question #152"
      },
      {
        "date": "2021-07-26T02:25:00.000Z",
        "voteCount": 1,
        "content": "Hi Kopper please send to giodionisi@yahoo.it"
      },
      {
        "date": "2021-07-19T05:24:00.000Z",
        "voteCount": 1,
        "content": "Hi Kopper please send to thecloudit@outlook.com"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 165,
    "url": "https://www.examtopics.com/discussions/google/view/7068-exam-professional-cloud-architect-topic-1-question-165/",
    "body": "Your customer is moving their corporate applications to Google Cloud Platform. The security team wants detailed visibility of all projects in the organization. You provision the Google Cloud Resource Manager and set up yourself as the org admin.<br>What Google Cloud Identity and Access Management (Cloud IAM) roles should you give to the security team?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOrg viewer, project owner",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOrg viewer, project viewer\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOrg admin, project browser",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tProject owner, network admin"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2019-11-26T10:24:00.000Z",
        "voteCount": 30,
        "content": "A is not correct because Project owner is too broad. The security team does not need to be able to make changes to projects.\n\nB is correct because:-Org viewer grants the security team permissions to view the organization's display name.\n-Project viewer grants the security team permissions to see the resources within projects.\n\nC is not correct because Org admin is too broad. The security team does not need to be able to make changes to the organization.\n\nD is not correct because Project owner is too broad. The security team does not need to be able to make changes to projects."
      },
      {
        "date": "2022-12-20T08:10:00.000Z",
        "voteCount": 1,
        "content": "I agree."
      },
      {
        "date": "2019-10-23T05:51:00.000Z",
        "voteCount": 12,
        "content": "B is the best answer because according to Google documentation i is best to use predefined roles and give the every team the least amount of access. (https://cloud.google.com/iam/docs/using-iam-securely) The question states the security must be able to view things, and the viewer role allows just that."
      },
      {
        "date": "2020-08-05T02:28:00.000Z",
        "voteCount": 6,
        "content": "B is ok"
      },
      {
        "date": "2023-10-19T07:51:00.000Z",
        "voteCount": 1,
        "content": "B, security team want to have visibility to all the project, so viewer to Org and Project is sufficiency."
      },
      {
        "date": "2022-12-15T04:24:00.000Z",
        "voteCount": 1,
        "content": "B is the correct answer"
      },
      {
        "date": "2022-11-26T01:36:00.000Z",
        "voteCount": 1,
        "content": "Agree B as security team does not need Project owner permission, but why need to grant project viewer after granting organization viewer?"
      },
      {
        "date": "2022-09-17T10:49:00.000Z",
        "voteCount": 1,
        "content": "B. Org viewer, project viewer!"
      },
      {
        "date": "2022-07-18T00:33:00.000Z",
        "voteCount": 3,
        "content": "Very similar question was presented on 15 July 2022 exam"
      },
      {
        "date": "2022-07-19T17:27:00.000Z",
        "voteCount": 3,
        "content": "Are the 260 exam topic questions enough to pass the exam?"
      },
      {
        "date": "2022-04-16T05:31:00.000Z",
        "voteCount": 1,
        "content": "B is the answer!"
      },
      {
        "date": "2021-12-23T02:37:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2021-12-14T23:57:00.000Z",
        "voteCount": 1,
        "content": "B is the correct answer"
      },
      {
        "date": "2021-12-02T08:54:00.000Z",
        "voteCount": 1,
        "content": "Vote B"
      },
      {
        "date": "2021-11-27T04:09:00.000Z",
        "voteCount": 2,
        "content": "A is not correct because Project owner is too broad. The security team does not need to be able to make changes to projects.\n\nB is correct because:\n-Organization viewer grants the security team permissions to view the organization's display name.\n-Project viewer grants the security team permissions to see the resources within projects.\n\nC is not correct because Organization Administrator is too broad. The security team does not need to be able to make changes to the organization.\n\nD is not correct because Project Owner is too broad. The security team does not need to be able to make changes to projects."
      },
      {
        "date": "2021-07-06T18:07:00.000Z",
        "voteCount": 2,
        "content": "Option B is correct as per Least Privilege"
      },
      {
        "date": "2021-05-17T23:34:00.000Z",
        "voteCount": 2,
        "content": "B. Org viewer, project viewer"
      },
      {
        "date": "2021-05-05T10:24:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2021-03-30T00:16:00.000Z",
        "voteCount": 1,
        "content": "B is ok"
      },
      {
        "date": "2021-03-28T22:56:00.000Z",
        "voteCount": 1,
        "content": "Answer is B"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 166,
    "url": "https://www.examtopics.com/discussions/google/view/54372-exam-professional-cloud-architect-topic-1-question-166/",
    "body": "Your company places a high value on being responsive and meeting customer needs quickly. Their primary business objectives are release speed and agility. You want to reduce the chance of security errors being accidentally introduced.<br>Which two actions can you take? (Choose two.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure every code check-in is peer reviewed by a security SME",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse source code security analyzers as part of the CI/CD pipeline\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure you have stubs to unit test all interfaces between components",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable code signing and a trusted binary repository integrated with your CI/CD pipeline",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun a vulnerability security scanner as part of your continuous-integration /continuous-delivery (CI/CD) pipeline\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "BE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BE",
        "count": 29,
        "isMostVoted": true
      },
      {
        "answer": "DE",
        "count": 26,
        "isMostVoted": false
      },
      {
        "answer": "BD",
        "count": 7,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "AB",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-08-20T00:23:00.000Z",
        "voteCount": 50,
        "content": "B&amp;E\nCode signing only verifies the author. In other words it only check who you are, but not what have you done"
      },
      {
        "date": "2021-11-14T05:45:00.000Z",
        "voteCount": 2,
        "content": "I understand that would be a requirement for security"
      },
      {
        "date": "2022-09-08T20:36:00.000Z",
        "voteCount": 2,
        "content": "But when we select E , it might auto include B . SOme VA scanning tools also do SAST.\nSo why choose B and E in that case. \nD makes more sense with E .\nAuthorised repo will add an additional layer of security with verified images and artifacts in it."
      },
      {
        "date": "2023-12-12T16:36:00.000Z",
        "voteCount": 1,
        "content": "At work, we do B and E."
      },
      {
        "date": "2021-06-03T01:28:00.000Z",
        "voteCount": 36,
        "content": "I think answer is D &amp; E."
      },
      {
        "date": "2021-06-14T00:42:00.000Z",
        "voteCount": 3,
        "content": "Agree with this. https://cloud.google.com/container-registry/docs/container-analysis"
      },
      {
        "date": "2021-11-30T06:58:00.000Z",
        "voteCount": 10,
        "content": "Here the question is to provide solution for \"Speed and Agility\". The Binary authorization prevent unauthorized deployments in production for GKE, Anthos Servicemesh and Cloud run, however will add delay in deployment process. So D may not be suitable in this scenario. Answer is B&amp;E."
      },
      {
        "date": "2022-09-08T20:38:00.000Z",
        "voteCount": 2,
        "content": "Speed will nit get hampered if the images are verified and attested. Checks need to be there. If you argument would be true than why to introduce VA scanner , as that will also induce delay in deployment.\nwhen we select E , it might auto include B . Some VA scanning tools also do SAST.\nSo why choose B and E in that case.\nD makes more sense with E .\nAuthorised repo will add an additional layer of security with verified images and artifacts in it.\nAnswer - D &amp; E"
      },
      {
        "date": "2024-09-28T12:01:00.000Z",
        "voteCount": 1,
        "content": "Option D does not directly address the primary concern of reducing the chance of security errors being accidentally introduced. Here\u2019s why:\n\nFocus on Integrity: Code signing and using a trusted binary repository primarily ensure that the code and binaries have not been tampered with and are from a trusted source. While this is important for security, it doesn\u2019t specifically target the detection and prevention of security vulnerabilities within the code itself.\n\nIndirect Impact on Security Errors: While code signing can help prevent the introduction of malicious code, it doesn\u2019t directly scan for or identify security vulnerabilities that might be accidentally introduced by developers."
      },
      {
        "date": "2024-05-05T00:49:00.000Z",
        "voteCount": 1,
        "content": "why the other options aren't as ideal:\n\nA. Ensure every code check-in is peer reviewed by a security SME: Manual reviews can become a bottleneck in agile environments and are less scalable than automated tools.\nC. Ensure you have stubs to unit test all interfaces between components: Good practice, but primarily focuses on functional rather than security testing.\nD. Enable code signing and a trusted binary repository...: Integrity checks are essential but don't directly prevent the introduction of the security errors themselves."
      },
      {
        "date": "2024-03-13T21:03:00.000Z",
        "voteCount": 4,
        "content": "Cyber Sec professional here. Question asks to reduce chance of security errors accidentally introduced. This means to integrate Static Application Security Tests (SAST) and Dynamic Application Security Tests (DAST) as part of CI/CD pipeline. Hence B and E are the right match. D is to ensure only trusted code is deployed to production, not reduce 'security error accidentally introduced'."
      },
      {
        "date": "2024-01-19T22:50:00.000Z",
        "voteCount": 1,
        "content": "I guess A and C are both time consuming and labor intensive. Also, aren't C stubs supposed to be used for unit tests?\n\nWhat remains is BDE.\nB is source code inspection.\nDoing D ensures that the repository is not contaminated.\nE's vulnerability scan detects whether there are any CVEs.\nI think all of them are correct. If you had to choose two, what would it be?\nIsn't it really slow if you do B and E?"
      },
      {
        "date": "2023-11-30T13:53:00.000Z",
        "voteCount": 5,
        "content": "https://cloud.google.com/blog/products/devops-sre/devsecops-and-cicd-using-google-cloud-built-in-services"
      },
      {
        "date": "2023-10-28T07:38:00.000Z",
        "voteCount": 2,
        "content": "The thing that makes me think D makes sense is that it ensures that only images that have passed though the configured CI/CD pipeline (with vulnerability checks) will be able to be deployed. This is better explained here: https://cloud.google.com/blog/products/containers-kubernetes/guard-against-security-vulnerabilities-with-container-registry-vulnerability-scanning"
      },
      {
        "date": "2023-10-28T07:33:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/blog/products/containers-kubernetes/guard-against-security-vulnerabilities-with-container-registry-vulnerability-scanning"
      },
      {
        "date": "2023-10-14T07:10:00.000Z",
        "voteCount": 1,
        "content": "Code signing only verifies the author not content"
      },
      {
        "date": "2023-09-29T01:19:00.000Z",
        "voteCount": 2,
        "content": "DE\nhttps://cloud.google.com/blog/products/containers-kubernetes/guard-against-security-vulnerabilities-with-container-registry-vulnerability-scanning"
      },
      {
        "date": "2023-06-28T07:16:00.000Z",
        "voteCount": 1,
        "content": "trusted binary repository option seems a static thing. For a release if we haven not used any new packages, trusted binary repository would not add any extra value. So B&amp;E which will are needed for every checking/release."
      },
      {
        "date": "2023-06-19T12:40:00.000Z",
        "voteCount": 1,
        "content": "B and E is the answer for me also."
      },
      {
        "date": "2023-05-13T13:26:00.000Z",
        "voteCount": 4,
        "content": "here you can find why:  https://cloud.google.com/blog/products/devops-sre/devsecops-and-cicd-using-google-cloud-built-in-services"
      },
      {
        "date": "2023-03-23T18:32:00.000Z",
        "voteCount": 1,
        "content": "B) Using source code security analyzers as part of the CI/CD pipeline can help identify security vulnerabilities and issues early in the development process. This can help reduce the risk of security errors being accidentally introduced and ensure that security is integrated into the development process from the beginning.\n\nE) Running a vulnerability security scanner as part of the CI/CD pipeline can help identify vulnerabilities and issues in the code and infrastructure before they are deployed to production. This can help reduce the risk of security errors being accidentally introduced and ensure that security is integrated into the development process from the beginning."
      },
      {
        "date": "2023-03-15T01:42:00.000Z",
        "voteCount": 1,
        "content": "B. Use source code security analyzers as part of the CI/CD pipeline\nE. Run a vulnerability security scanner as part of your continuous-integration /continuous-delivery (CI/CD) pipeline\n\nThese actions ensure that security is integrated into the development and deployment processes and helps catch security issues early in the software development lifecycle."
      },
      {
        "date": "2023-03-10T04:59:00.000Z",
        "voteCount": 3,
        "content": "ChatGPT says B &amp; E :-)"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 167,
    "url": "https://www.examtopics.com/discussions/google/view/7073-exam-professional-cloud-architect-topic-1-question-167/",
    "body": "You want to enable your running Google Kubernetes Engine cluster to scale as demand for your application changes.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd additional nodes to your Kubernetes Engine cluster using the following command: gcloud container clusters resize CLUSTER_Name \u05d2\u20ac\" -size 10",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a tag to the instances in the cluster with the following command: gcloud compute instances add-tags INSTANCE - -tags enable- autoscaling max-nodes-10",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the existing Kubernetes Engine cluster with the following command: gcloud alpha container clusters update mycluster - -enable- autoscaling - -min-nodes=1 - -max-nodes=10\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new Kubernetes Engine cluster with the following command: gcloud alpha container clusters create mycluster - -enable- autoscaling - -min-nodes=1 - -max-nodes=10 and redeploy your application"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 12,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-01-11T10:48:00.000Z",
        "voteCount": 24,
        "content": "Agree C"
      },
      {
        "date": "2019-10-23T07:15:00.000Z",
        "voteCount": 11,
        "content": "A is incorrect because there is supposed to be two hypens \"--\" not one before size (https://cloud.google.com/sdk/gcloud/reference/container/clusters/resize). B is incorrect because it just adds a string to the cluster (https://cloud.google.com/sdk/gcloud/reference/compute/instances/add-tags). \"C\" is just as wrong as \"A\" because the documentation says it should be \"--max-nodes\" followed by \"--min-nodes\" (https://cloud.google.com/sdk/gcloud/reference/alpha/container/clusters/update), also the alpha command no longer works but it used to and is still up on google docs. This goes for \"D\" as well but D talks about making another, which doesn't have to be done because one it already up. So the debate is between A and C, and C used to work so C was chosen, although C also has spaces which never worked... So this question is an absolute thug tactic by a Google team to steal from the Google kingdom preventing the establishment of their library by failing people that actually know the science behind the technology. When you see this question at a test center I'd select C."
      },
      {
        "date": "2020-12-09T07:35:00.000Z",
        "voteCount": 4,
        "content": "You didn't check the documentation."
      },
      {
        "date": "2020-08-05T02:39:00.000Z",
        "voteCount": 9,
        "content": "C is ok"
      },
      {
        "date": "2020-08-14T01:15:00.000Z",
        "voteCount": 9,
        "content": "To enable autoscaling for an existing node pool, run the following command:\n\ngcloud container clusters update cluster-name --enable-autoscaling \\\n    --min-nodes 1 --max-nodes 10 --zone compute-zone --node-pool default-pool\n\nhttps://cloud.google.com/kubernetes-engine/docs/how-to/cluster-autoscaler"
      },
      {
        "date": "2023-08-12T08:23:00.000Z",
        "voteCount": 1,
        "content": "Agree C"
      },
      {
        "date": "2023-02-28T12:59:00.000Z",
        "voteCount": 4,
        "content": "no need to create a new one, just update!"
      },
      {
        "date": "2023-02-02T19:44:00.000Z",
        "voteCount": 1,
        "content": "See the cli docs"
      },
      {
        "date": "2022-12-15T04:31:00.000Z",
        "voteCount": 2,
        "content": "C is the correct answer"
      },
      {
        "date": "2022-12-13T11:36:00.000Z",
        "voteCount": 1,
        "content": "It's C"
      },
      {
        "date": "2022-11-05T02:44:00.000Z",
        "voteCount": 1,
        "content": "ok for C"
      },
      {
        "date": "2022-10-15T12:15:00.000Z",
        "voteCount": 2,
        "content": "I agree with C"
      },
      {
        "date": "2021-12-15T01:56:00.000Z",
        "voteCount": 2,
        "content": "C is the correct answer"
      },
      {
        "date": "2021-12-09T02:23:00.000Z",
        "voteCount": 1,
        "content": "C looks OK"
      },
      {
        "date": "2021-11-20T18:31:00.000Z",
        "voteCount": 6,
        "content": "C - cluster is already running so use update instead of create new cluster."
      },
      {
        "date": "2021-10-17T09:05:00.000Z",
        "voteCount": 5,
        "content": "Answer should  be C. Now alpha command is not needed. seems question is older and now  kubernets command is not with alpha.\ngcloud container clusters update cluster-name --enable-autoscaling  ...."
      },
      {
        "date": "2021-09-13T05:10:00.000Z",
        "voteCount": 1,
        "content": "This couldn\u2019t be C, you shouldn\u2019t use alpha commands in a production(app) workload."
      },
      {
        "date": "2021-07-04T21:31:00.000Z",
        "voteCount": 2,
        "content": "C is the way to go min and max and done"
      },
      {
        "date": "2021-05-17T23:28:00.000Z",
        "voteCount": 3,
        "content": "C. Update the existing Kubernetes Engine cluster with the following command: gcloud alpha container clusters update mycluster - -enable- autoscaling - -min-nodes=1 - -max-nodes=10"
      },
      {
        "date": "2021-05-17T18:27:00.000Z",
        "voteCount": 3,
        "content": "Answer- C.\nUpdate command and autoscaling tag will update existing running kubernetes cluster."
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 168,
    "url": "https://www.examtopics.com/discussions/google/view/54373-exam-professional-cloud-architect-topic-1-question-168/",
    "body": "Your marketing department wants to send out a promotional email campaign. The development team wants to minimize direct operation management. They project a wide range of possible customer responses, from 100 to 500,000 click-through per day. The link leads to a simple website that explains the promotion and collects user information and preferences.<br>Which infrastructure should you recommend? (Choose two.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Google App Engine to serve the website and Google Cloud Datastore to store user data.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a Google Container Engine cluster to serve the website and store data to persistent disk.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a managed instance group to serve the website and Google Cloud Bigtable to store user data.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a single Compute Engine virtual machine (VM) to host a web server, backend by Google Cloud SQL."
    ],
    "answer": "AC",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AC",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-06-03T01:32:00.000Z",
        "voteCount": 27,
        "content": "A &amp; C seems to be the correct answer."
      },
      {
        "date": "2021-07-03T07:59:00.000Z",
        "voteCount": 9,
        "content": "A. Use Google App Engine to serve the website and Google Cloud Datastore to store user data.\nC. Use a managed instance group to serve the website and Google Cloud Bigtable to store user data."
      },
      {
        "date": "2021-10-12T18:09:00.000Z",
        "voteCount": 1,
        "content": "Why not D?"
      },
      {
        "date": "2021-12-09T06:57:00.000Z",
        "voteCount": 6,
        "content": "Because a single GCE instance might not be able to handle the unpredictable load"
      },
      {
        "date": "2024-06-20T09:47:00.000Z",
        "voteCount": 3,
        "content": "Both bigtable and datastore seem like overkill solutions but A&amp;C are the only options that make sense here. In the real world use BigQuery and either App Engine or Cloud Run."
      },
      {
        "date": "2023-08-12T08:38:00.000Z",
        "voteCount": 1,
        "content": "Why not B, GKE si best fit and preferred over MI"
      },
      {
        "date": "2023-02-02T19:47:00.000Z",
        "voteCount": 3,
        "content": "Cloud Data store and Big Table are the only solutions that can handle 500000 clicks"
      },
      {
        "date": "2022-12-26T01:29:00.000Z",
        "voteCount": 5,
        "content": "A: Google App Engine is a fully managed platform for building and running web applications and APIs. It can automatically scale to meet high traffic demands, making it a good choice for serving the website for the promotional email campaign. Google Cloud Datastore can also scale automatically to meet high traffic demands, making it a good choice for storing user data.\n\nC: A managed instance group are managed as a single entity and can automatically scale up or down based on demand. This makes it a good choice for serving the website for the promotional email campaign. Google Cloud Bigtable is a fully managed, high-performance NoSQL database that can store and serve large amounts of structured data with low latency. It is designed to scale horizontally and can handle high traffic demands, making it a good choice for storing user data."
      },
      {
        "date": "2022-12-26T01:29:00.000Z",
        "voteCount": 1,
        "content": "B, using a Google Container Engine cluster to serve the website and store data to persistent disk, could be a valid solution as well. However, persistent disks may not be able to scale horizontally to meet high traffic demands, which could impact the performance of the website.\n\nD, using a single Compute Engine VM to host a web server, backed by Google Cloud SQL, would not be a good choice for this scenario. A single VM would not be able to scale to meet the wide range of possible traffic levels for the promotional email campaign, and Google Cloud SQL may not be able to scale horizontally to meet high traffic demands."
      },
      {
        "date": "2022-10-15T12:18:00.000Z",
        "voteCount": 3,
        "content": "A and C is right choice, D is saying single VM"
      },
      {
        "date": "2022-09-18T02:04:00.000Z",
        "voteCount": 2,
        "content": "AC (100%) !!!"
      },
      {
        "date": "2022-09-17T11:00:00.000Z",
        "voteCount": 1,
        "content": "A &amp; C seems to be the correct answer."
      },
      {
        "date": "2021-12-15T02:02:00.000Z",
        "voteCount": 1,
        "content": "AC is the correct answer."
      },
      {
        "date": "2021-10-23T06:01:00.000Z",
        "voteCount": 1,
        "content": "A only, choose two - App Engine + Datastore\n\nUse GAE to serve the website and Google Datastore to store user data.\n\nGCE \u2013 is too complex solution with specific OS to maintain.\nGKE \u2013 is for microservices apps, and Persistent Disk is not good solution for relational data storage;\nGAE \u2013 is fast and reliable solution, you write just code and run it on fully managed service. DataStore also matches perfectly since intended for storing user profiles, key-value pairs."
      },
      {
        "date": "2021-06-12T16:50:00.000Z",
        "voteCount": 3,
        "content": "A &amp; B with less operations management. Also Containers and App Engine as the clicks varies."
      },
      {
        "date": "2021-07-25T13:02:00.000Z",
        "voteCount": 1,
        "content": "\"Google Container Engine\" does not exist, only \"GKE\", but operating a Kubernetes cluster is not easy, in that case, an option could be Cloud Run."
      },
      {
        "date": "2021-06-14T02:44:00.000Z",
        "voteCount": 3,
        "content": "But user data storing in persistent disks? Not correct to me. Seems A &amp; C"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 169,
    "url": "https://www.examtopics.com/discussions/google/view/54374-exam-professional-cloud-architect-topic-1-question-169/",
    "body": "Your company just finished a rapid lift and shift to Google Compute Engine for your compute needs. You have another 9 months to design and deploy a more cloud-native solution. Specifically, you want a system that is no-ops and auto-scaling.<br>Which two compute products should you choose? (Choose two.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCompute Engine with containers",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGoogle Kubernetes Engine with containers\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGoogle App Engine Standard Environment\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCompute Engine with custom instance types",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCompute Engine with managed instance groups"
    ],
    "answer": "BC",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BC",
        "count": 19,
        "isMostVoted": true
      },
      {
        "answer": "CE",
        "count": 10,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-07-30T11:30:00.000Z",
        "voteCount": 18,
        "content": "I would go with B&amp;C\nCloud-native, less-ops and auto-scaling all get addressed"
      },
      {
        "date": "2022-04-03T01:47:00.000Z",
        "voteCount": 9,
        "content": "Why E is incorrect? can't MIG also perform autoscaling? Also it needs fewer administration as GKE"
      },
      {
        "date": "2022-09-20T03:04:00.000Z",
        "voteCount": 8,
        "content": "No ops = Serverless / Almost Serverless MIG is not."
      },
      {
        "date": "2024-06-13T01:18:00.000Z",
        "voteCount": 2,
        "content": "No Ops -&gt; Kubernetes?\nThis question is too generic to choose between MIG and GKE. In the context of this exam, I would choose B (GKE), specally with current options as Autopilot. But in my day to day I would consider way more factors."
      },
      {
        "date": "2023-04-13T20:13:00.000Z",
        "voteCount": 5,
        "content": "Option B, Google Kubernetes Engine (GKE) with containers, is a managed Kubernetes service that automatically manages and scales containerized applications. GKE handles cluster management tasks like scaling, upgrades, and security patches, allowing you to focus on the application itself.\n\nOption C, Google App Engine Standard Environment, is a fully managed platform for building and deploying applications. It automatically scales applications based on demand and provides a no-ops experience. With App Engine Standard Environment, you don't need to worry about infrastructure management, as Google handles it for you."
      },
      {
        "date": "2023-04-04T13:36:00.000Z",
        "voteCount": 2,
        "content": "B: GKE with autopilot mode for workload not requiring ingress or egress. Otherwise you will need some ops work IMHO.\nC: app engine for workload requiring ingress. It comes with autoscaling features and rolling update features without being as heavy as gke."
      },
      {
        "date": "2023-03-14T15:42:00.000Z",
        "voteCount": 5,
        "content": "I would still go for C &amp; E. My take is GKE still requires some operational overhead for managing the Kubernetes cluster and ensuring high availability of the workloads.\nHence C &amp; E would be most suitable one."
      },
      {
        "date": "2023-02-15T10:18:00.000Z",
        "voteCount": 1,
        "content": "No ops: use container or gcp product without mangement.\nSo not VM possible in the answer"
      },
      {
        "date": "2022-12-02T07:24:00.000Z",
        "voteCount": 2,
        "content": "App Engine standard = container based (can even go to zero)\nApp Engine flexible = VM based (minimum 1)\nNo ops: container &gt; VM"
      },
      {
        "date": "2022-10-19T18:41:00.000Z",
        "voteCount": 3,
        "content": "B &amp; C seems right to me, E needs lots of Ops to build image, instance template and instance group, ... maintain your image always"
      },
      {
        "date": "2022-10-15T12:20:00.000Z",
        "voteCount": 2,
        "content": "B. Google Kubernetes Engine with containers\nC. Google App Engine Standard Environmen"
      },
      {
        "date": "2022-09-11T16:50:00.000Z",
        "voteCount": 2,
        "content": "No ops = Serverless / Almost Serverless, less operational management overhead.\nKubernetes and App Engine are the only one that gives us that flexibility, plus is modernizing apps"
      },
      {
        "date": "2022-09-08T09:49:00.000Z",
        "voteCount": 5,
        "content": "C and E\nGKE is absolutely nor no-ops.\nMIG can be closest to no-ops among the other options"
      },
      {
        "date": "2022-09-04T04:07:00.000Z",
        "voteCount": 2,
        "content": "B&amp;C seem to be right for this question. In reality, whoever really proposes B as an option never ran Kubernetes in production."
      },
      {
        "date": "2022-05-15T02:23:00.000Z",
        "voteCount": 1,
        "content": "Vote A and B\nHowever I think option B should address more specifically, like GKE - autopilot mode."
      },
      {
        "date": "2022-05-15T02:24:00.000Z",
        "voteCount": 1,
        "content": "Correct the answer for B and C"
      },
      {
        "date": "2021-12-15T02:06:00.000Z",
        "voteCount": 1,
        "content": "BC are the correct answers"
      },
      {
        "date": "2021-12-09T02:26:00.000Z",
        "voteCount": 1,
        "content": "Agree B and C"
      },
      {
        "date": "2021-10-31T01:06:00.000Z",
        "voteCount": 3,
        "content": "Correct Answer: BC\nB: With Container Engine, Google will automatically deploy your cluster for you, update, patch, secure the nodes.\nKubernetes Engine's cluster autoscaler automatically resizes clusters based on the demands of the workloads you want to run.\nC: Solutions like Datastore, BigQuery, AppEngine, etc are truly NoOps.\nApp Engine by default scales the number of instances running up and down to match the load, thus providing consistent performance for your app at all times while minimizing idle instances and thus reducing cost."
      },
      {
        "date": "2021-10-31T01:07:00.000Z",
        "voteCount": 2,
        "content": "Note: At a high level, NoOps means that there is no infrastructure to build out and manage during usage of the platform. Typically, the compromise you make with NoOps is that you lose control of the underlying infrastructure.\nhttps://www.quora.com/How-well-does-Google-Container-Engine-support-Google-Cloud-Platform%E2%80%99s-NoOps-claim\nB \u2013 Google Container Engine (autoscaling)\nC \u2013 Google AppEngine Standard Environment (no ops)\nYou should understand this Q as following: after Lift-n-Shift parts of the monolith should be moved to managed services (e.g. REST API) running on GAE; and other micro-services will run in containers / pods."
      },
      {
        "date": "2021-09-07T12:36:00.000Z",
        "voteCount": 3,
        "content": "B &amp; C.  Although GKE standard is definitely not no-ops!"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 170,
    "url": "https://www.examtopics.com/discussions/google/view/7082-exam-professional-cloud-architect-topic-1-question-170/",
    "body": "One of your primary business objectives is being able to trust the data stored in your application. You want to log all changes to the application data.<br>How can you design your logging system to verify authenticity of your logs?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite the log concurrently in the cloud and on premises",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a SQL database and limit who can modify the log table",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDigitally sign each timestamp and log entry and store the signature\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a JSON dump of each log entry and store it in Google Cloud Storage"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-02-19T11:48:00.000Z",
        "voteCount": 31,
        "content": "Correct answer is C (verified from Question Bank in Whizlabs.com) \n\nFeedback\nC (Correct answer) - Digitally sign each timestamp and log entry and store the signature.\nAnswer A, B, and D don\u2019t have any added value to verify the authenticity of your logs. Besides, Logs are mostly suitable for exporting to Cloud storage, BigQuery, and PubSub. SQL database is not the best way to be exported to nor store log data.\nSimplified Explanation\nTo verify the authenticity of your logs if they are tampered with or forged, you can use a certain algorithm to generate digest by hashing each timestamp or log entry and then digitally sign the digest with a private key to generate a signature. Anybody with your public key can verify that signature to confirm that it was made with your private key and they can tell if the timestamp or log entry was modified. You can put the signature files into a folder separate from the log files. This separation enables you to enforce granular security policies."
      },
      {
        "date": "2019-11-14T07:38:00.000Z",
        "voteCount": 24,
        "content": "C is correct and common practice"
      },
      {
        "date": "2024-04-25T15:04:00.000Z",
        "voteCount": 1,
        "content": "Digitally sign each timestamp and log entry and store the signature"
      },
      {
        "date": "2022-12-26T01:36:00.000Z",
        "voteCount": 3,
        "content": "I would recommend option C, digitally signing each timestamp and log entry and storing the signature. Digitally signing a log entry involves creating a cryptographic hash of the log entry and a timestamp, and then encrypting the hash using a private key. The encrypted hash, known as the signature, can be stored along with the log entry in a secure manner. To verify the authenticity of the log entry, you can use the public key associated with the private key used to create the signature to decrypt the signature and recreate the hash. If the recreated hash matches the original hash, it indicates that the log entry has not been tampered with and is authentic."
      },
      {
        "date": "2022-12-26T01:37:00.000Z",
        "voteCount": 1,
        "content": "Writing the log concurrently in the cloud and on premises, would not necessarily help to verify the authenticity of the logs, so A is not an option\n\nB, using a SQL database and limiting who can modify the log table, could help to prevent unauthorized modification of the logs, but it would not necessarily provide a way to verify the authenticity of the logs if they are modified by an authorized user.\n\nOption D, creating a JSON dump of each log entry and storing it in Google Cloud Storage, would not necessarily help to verify the authenticity of the logs."
      },
      {
        "date": "2022-10-15T12:48:00.000Z",
        "voteCount": 2,
        "content": "Digitally signing is correct. C is right option!"
      },
      {
        "date": "2021-12-30T23:04:00.000Z",
        "voteCount": 1,
        "content": "C is correct.You can use deterministic algorithm to validate hash values."
      },
      {
        "date": "2021-12-15T02:22:00.000Z",
        "voteCount": 1,
        "content": "C is the correct answer"
      },
      {
        "date": "2021-11-26T19:08:00.000Z",
        "voteCount": 2,
        "content": "vote C"
      },
      {
        "date": "2021-10-23T06:12:00.000Z",
        "voteCount": 5,
        "content": "C \u2013 Digitally sign each timestamp and log entry and store the signature.\nThis is fun Q where all options are technically correct. But, the point is to find most efficient. Since, Q asks about verification of log entry - then you don't need to dub it. Using of much shorter timestamp-hash pair will address the request. So, when reading log from original source, you also read hash for this timestamp and then verify the entry's body. \nBTW, this is one of general purpose questions, which is not directly related to GCP. Just checks your attentiveness\nA - is about duplication, can work, but redundant;\nB / D - both have similar design, but don\u2019t allow verification of entry. No cross-checking of entry. E.g. person having access to log can change it in one place.\nC - storing log in one place, and hash-code in another. So, even if \"trusted\" person has modified original log, then it will break correspondence with hash code in other storage. That storage should be available only for authentication program (via service account)."
      },
      {
        "date": "2022-01-18T10:49:00.000Z",
        "voteCount": 1,
        "content": "@MaxNRG, very clearly articulated elimination technique. BTW are these questions appearing in actual exam?"
      },
      {
        "date": "2021-10-24T22:11:00.000Z",
        "voteCount": 2,
        "content": "If you attended recently, Please update some new questions too. It would be great help"
      },
      {
        "date": "2021-06-25T22:16:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2021-05-25T08:19:00.000Z",
        "voteCount": 1,
        "content": "C seems like the right answer"
      },
      {
        "date": "2021-05-17T23:52:00.000Z",
        "voteCount": 1,
        "content": "C. Digitally sign each timestamp and log entry and store the signature"
      },
      {
        "date": "2021-05-17T18:37:00.000Z",
        "voteCount": 2,
        "content": "C (Correct answer) - Digitally sign each timestamp and log entry and store the signature.\n\nOther options are possible to export logs but won't be able to verify authenticity of logs"
      },
      {
        "date": "2021-05-06T10:27:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2021-04-07T15:18:00.000Z",
        "voteCount": 1,
        "content": "I'm on the fence between C and D. C is a good practice but D can do the job as well as versioned objects might be able to do job at some level... Now, C tells that only the signature would be stored which is obviously not enough, but the owner of versioned objects might be tampered too... IDK"
      },
      {
        "date": "2021-03-30T00:11:00.000Z",
        "voteCount": 1,
        "content": "IMO - C is ok"
      },
      {
        "date": "2021-03-29T22:23:00.000Z",
        "voteCount": 1,
        "content": "Answers is C"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 171,
    "url": "https://www.examtopics.com/discussions/google/view/68682-exam-professional-cloud-architect-topic-1-question-171/",
    "body": "Your company has a Google Workspace account and Google Cloud Organization. Some developers in the company have created Google Cloud projects outside of the Google Cloud Organization.<br>You want to create an Organization structure that allows developers to create projects, but prevents them from modifying production projects. You want to manage policies for all projects centrally and be able to set more restrictive policies for production projects.<br>You want to minimize disruption to users and developers when business needs change in the future. You want to follow Google-recommended practices. Now should you design the Organization structure?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a second Google Workspace account and Organization. 2. Grant all developers the Project Creator IAM role on the new Organization. 3. Move the developer projects into the new Organization. 4. Set the policies for all projects on both Organizations. 5. Additionally, set the production policies on the original Organization.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a folder under the Organization resource named \u05d2\u20acProduction.\u05d2\u20ac 2. Grant all developers the Project Creator IAM role on the new Organization. 3. Move the developer projects into the new Organization. 4. Set the policies for all projects on the Organization. 5. Additionally, set the production policies on the \u05d2\u20acProduction\u05d2\u20ac folder.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create folders under the Organization resource named \u05d2\u20acDevelopment\u05d2\u20ac and \u05d2\u20acProduction.\u05d2\u20ac 2. Grant all developers the Project Creator IAM role on the \u05d2\u20acDevelopment\u05d2\u20ac folder. 3. Move the developer projects into the \u05d2\u20acDevelopment\u05d2\u20ac folder. 4. Set the policies for all projects on the Organization. 5. Additionally, set the production policies on the \u05d2\u20acProduction\u05d2\u20ac folder.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Designate the Organization for production projects only. 2. Ensure that developers do not have the Project Creator IAM role on the Organization. 3. Create development projects outside of the Organization using the developer Google Workspace accounts. 4. Set the policies for all projects on the Organization. 5. Additionally, set the production policies on the individual production projects."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 31,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-04-07T11:51:00.000Z",
        "voteCount": 14,
        "content": "C, because managing multiple organizations is not a Google best practice"
      },
      {
        "date": "2023-08-31T12:39:00.000Z",
        "voteCount": 1,
        "content": "the requirement is \"...You want to manage policies for all projects centrally...\" With multiple organizations that wont be possible as you would have to set policies on multiple organizations. Therefore I opt for \"C\"."
      },
      {
        "date": "2023-07-07T18:31:00.000Z",
        "voteCount": 3,
        "content": "for everyone commenting that multiple organizations is bad practice according to google check https://cloud.google.com/architecture/identity/best-practices-for-planning"
      },
      {
        "date": "2023-12-05T14:57:00.000Z",
        "voteCount": 1,
        "content": "\"The right number of organizations to use depends on the number of independent groups of administrative users in your company:\n- If your company is organized by function, you might have a single department that's in charge of overseeing all Google Cloud deployments.\n- If your company is organized by division or owns a number of autonomously-run subsidiaries, then there might not be a single department that's in charge.\n\nNo divisions is mentioned in the questions. Developer is a function."
      },
      {
        "date": "2023-02-28T14:17:00.000Z",
        "voteCount": 1,
        "content": "C, Bcuz manage multiple organizations is not a Google best practice"
      },
      {
        "date": "2023-01-15T08:57:00.000Z",
        "voteCount": 1,
        "content": "clearly C. Two orgs is a BAD practice"
      },
      {
        "date": "2022-12-26T01:39:00.000Z",
        "voteCount": 2,
        "content": "I would recommend option C, creating two folders under the Organization resource named \"Development\" and \"Production\" and placing developer and production projects in the respective folders. This approach would allow you to centrally manage policies for all projects, while also being able to set more restrictive policies for production projects. It would also allow you to easily move projects between the Development and Production folders as business needs change, without disrupting users or developers.\n\nOption D, designating the Organization for production projects only, would not allow developers to create projects within the Organization and could lead to confusion around project ownership and management. It would also make it more difficult to move projects between development and production environments."
      },
      {
        "date": "2022-12-26T01:40:00.000Z",
        "voteCount": 1,
        "content": "Option A, creating a second Google Workspace account and Organization, would not be a recommended practice as it would create unnecessary complexity and make it more difficult to manage policies and move projects between environments.\n\nOption B, creating a single folder under the Organization resource and placing all projects in that folder, would not allow you to set different policies for development and production projects."
      },
      {
        "date": "2022-12-16T03:03:00.000Z",
        "voteCount": 1,
        "content": "C Is the Correct Answer"
      },
      {
        "date": "2022-11-20T01:25:00.000Z",
        "voteCount": 2,
        "content": "all 4 answers seems stupid"
      },
      {
        "date": "2022-11-17T08:42:00.000Z",
        "voteCount": 1,
        "content": "C is ok"
      },
      {
        "date": "2022-10-15T12:50:00.000Z",
        "voteCount": 2,
        "content": "C is the best option"
      },
      {
        "date": "2022-09-08T10:02:00.000Z",
        "voteCount": 2,
        "content": "C is Ok"
      },
      {
        "date": "2022-08-19T05:06:00.000Z",
        "voteCount": 2,
        "content": "I don't think anyone can create projects outside the organization using the workspace account as it redirects the users into the organization."
      },
      {
        "date": "2022-07-24T06:46:00.000Z",
        "voteCount": 1,
        "content": "Answer is C\nA - you only want to create and Organization structure not Google Workspace\nB - best practice is to move your projects to a folders\nD - developers are allowed to create projects"
      },
      {
        "date": "2022-07-13T11:19:00.000Z",
        "voteCount": 1,
        "content": "C makes most sense in this scenario"
      },
      {
        "date": "2022-05-22T04:52:00.000Z",
        "voteCount": 1,
        "content": "D is better than C."
      },
      {
        "date": "2022-01-18T19:25:00.000Z",
        "voteCount": 3,
        "content": "C seems to be more organized solution than D."
      },
      {
        "date": "2022-01-03T22:38:00.000Z",
        "voteCount": 2,
        "content": "I would go with C"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 172,
    "url": "https://www.examtopics.com/discussions/google/view/68683-exam-professional-cloud-architect-topic-1-question-172/",
    "body": "Your company has an application running on Compute Engine that allows users to play their favorite music. There are a fixed number of instances. Files are stored in Cloud Storage, and data is streamed directly to users. Users are reporting that they sometimes need to attempt to play popular songs multiple times before they are successful. You need to improve the performance of the application. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Mount the Cloud Storage bucket using gcsfuse on all backend Compute Engine instances. 2. Serve music files directly from the backend Compute Engine instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a Cloud Filestore NFS volume and attach it to the backend Compute Engine instances. 2. Download popular songs in Cloud Filestore. 3. Serve music files directly from the backend Compute Engine instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Copy popular songs into CloudSQL as a blob. 2. Update application code to retrieve data from CloudSQL when Cloud Storage is overloaded.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a managed instance group with Compute Engine instances. 2. Create a global load balancer and configure it with two backends: \u05d2\u2014\u2039 Managed instance group \u05d2\u2014\u2039 Cloud Storage bucket 3. Enable Cloud CDN on the bucket backend.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 37,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-03-09T12:30:00.000Z",
        "voteCount": 9,
        "content": "The correct answer is: D. Create a managed instance group with Compute Engine instances. Create a global load balancer and configure it with two backends: Managed instance group, Cloud Storage bucket. Enable Cloud CDN on the bucket backend.\n\nThis solution will improve the performance of the application by:\n\nAutomatically scaling the number of Compute Engine instances to meet demand.\nDistributing traffic across multiple instances to reduce load on each instance.\nCaching popular songs in memory to reduce the number of times that they need to be loaded from Cloud Storage.\nUsing a global load balancer to distribute traffic evenly across all regions.\nUsing Cloud CDN to deliver files to users from a location that is closer to them.\nThis solution is the most efficient and cost-effective way to improve the performance of the application."
      },
      {
        "date": "2023-12-03T05:41:00.000Z",
        "voteCount": 6,
        "content": "A is Ridiculous."
      },
      {
        "date": "2023-12-07T12:06:00.000Z",
        "voteCount": 3,
        "content": "Most of the \"official\" answers are, unfortunately.  I've pretty much defaulted to the community answer distributions exclusively."
      },
      {
        "date": "2023-09-11T01:38:00.000Z",
        "voteCount": 2,
        "content": "The correct answer is D.\n\nhttps://cloud.google.com/cdn?hl=en#static-content"
      },
      {
        "date": "2023-09-11T01:37:00.000Z",
        "voteCount": 1,
        "content": "D. Absolutely"
      },
      {
        "date": "2023-09-10T04:46:00.000Z",
        "voteCount": 1,
        "content": "Not agree on A , why pay unnecessary for same data for VM disk size."
      },
      {
        "date": "2023-02-02T19:51:00.000Z",
        "voteCount": 3,
        "content": "The Cloud CDN is the best practice for the content caching."
      },
      {
        "date": "2022-12-26T01:41:00.000Z",
        "voteCount": 2,
        "content": "I would recommend option D, creating a managed instance group with Compute Engine instances and a global load balancer with two backends: the managed instance group and the Cloud Storage bucket, and enabling Cloud CDN on the bucket backend. This approach would allow you to scale the number of instances in the managed instance group as needed to handle the demand for the application, and would also use the Cloud CDN to improve the performance of the application by caching the music files closer to the users.\n\nOption A, mounting the Cloud Storage bucket using gcsfuse on all backend Compute Engine instances, would not provide a way to scale the number of instances to handle increased demand for the application."
      },
      {
        "date": "2022-12-26T01:42:00.000Z",
        "voteCount": 2,
        "content": "Option B, creating a Cloud Filestore NFS volume and attaching it to the backend Compute Engine instances, would not provide a way to scale the number of instances to handle increased demand for the application.\n\nOption C, copying popular songs into CloudSQL as a blob and updating the application code to retrieve data from CloudSQL when Cloud Storage is overloaded, would not provide a way to scale the number of instances to handle increased demand for the application. Additionally, using CloudSQL to store and serve music files may not be the most appropriate use case for the service, as it is designed for storing and querying structured data, rather than serving large files."
      },
      {
        "date": "2022-12-16T03:05:00.000Z",
        "voteCount": 2,
        "content": "D  Is the Correct Answer"
      },
      {
        "date": "2022-11-17T08:47:00.000Z",
        "voteCount": 1,
        "content": "D is ok"
      },
      {
        "date": "2022-10-21T08:01:00.000Z",
        "voteCount": 1,
        "content": "I think it should be A,"
      },
      {
        "date": "2022-10-21T08:00:00.000Z",
        "voteCount": 2,
        "content": "Why not A, I think it should be A, since it mentioned popular songs and file store is faster then the cloud storage, I vote for A"
      },
      {
        "date": "2022-10-15T12:52:00.000Z",
        "voteCount": 2,
        "content": "I will go with D"
      },
      {
        "date": "2022-09-04T04:11:00.000Z",
        "voteCount": 4,
        "content": "Do not trust the official answers here, D is correct. In special for this question, never use gcsfuse in production. Performance is bad and reliability is trashy - Google states it themselves."
      },
      {
        "date": "2022-07-13T11:21:00.000Z",
        "voteCount": 2,
        "content": "D is correct"
      },
      {
        "date": "2022-05-17T00:20:00.000Z",
        "voteCount": 3,
        "content": "Remember Gcsfuse performance is not good, reference\nhttps://cloud.google.com/storage/docs/gcs-fuse#notes"
      },
      {
        "date": "2022-04-23T07:51:00.000Z",
        "voteCount": 4,
        "content": "A is wrong because you can't be serving files directly from Compute Engine instance.\nGCS + CDN is best option"
      },
      {
        "date": "2022-04-07T11:55:00.000Z",
        "voteCount": 1,
        "content": "D for sure"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 173,
    "url": "https://www.examtopics.com/discussions/google/view/68684-exam-professional-cloud-architect-topic-1-question-173/",
    "body": "The operations team in your company wants to save Cloud VPN log events for one year. You need to configure the cloud infrastructure to save the logs. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up a filter in Cloud Logging and a Cloud Storage bucket as an export target for the logs you want to save.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable the Compute Engine API, and then enable logging on the firewall rules that match the traffic you want to save.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up a Cloud Logging Dashboard titled Cloud VPN Logs, and then add a chart that queries for the VPN metrics over a one-year time period.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up a filter in Cloud Logging and a topic in Pub/Sub to publish the logs."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 16,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-22T06:07:00.000Z",
        "voteCount": 1,
        "content": "It is A"
      },
      {
        "date": "2023-11-25T17:38:00.000Z",
        "voteCount": 1,
        "content": "I guess a similar approach to this shuld be followed https://cloud.google.com/architecture/exporting-stackdriver-logging-for-compliance-requirements"
      },
      {
        "date": "2023-03-14T16:53:00.000Z",
        "voteCount": 4,
        "content": "Archival storage: Cloud Storage is the best"
      },
      {
        "date": "2023-01-14T18:43:00.000Z",
        "voteCount": 1,
        "content": "A is ok"
      },
      {
        "date": "2022-11-17T08:50:00.000Z",
        "voteCount": 1,
        "content": "A is ok"
      },
      {
        "date": "2022-10-15T12:55:00.000Z",
        "voteCount": 2,
        "content": "I would like to go with Option A"
      },
      {
        "date": "2022-09-08T10:12:00.000Z",
        "voteCount": 3,
        "content": "Logs needed for a year. Coldline or Archive storage classes available.\nA seems fine"
      },
      {
        "date": "2022-07-21T01:36:00.000Z",
        "voteCount": 1,
        "content": "A is ok"
      },
      {
        "date": "2022-03-27T17:38:00.000Z",
        "voteCount": 1,
        "content": "A should be right."
      },
      {
        "date": "2022-01-03T22:40:00.000Z",
        "voteCount": 1,
        "content": "It should be D, CDN serves the purpose."
      },
      {
        "date": "2022-01-03T22:42:00.000Z",
        "voteCount": 1,
        "content": "Ignore A is correct. CDN was an answer to a diff question."
      },
      {
        "date": "2021-12-31T21:36:00.000Z",
        "voteCount": 1,
        "content": "A. Set up a filter in Cloud Logging and a Cloud Storage bucket as an export target for the logs you want to save."
      },
      {
        "date": "2021-12-28T04:31:00.000Z",
        "voteCount": 4,
        "content": "A Is correct.\nSet up a filter in Cloud Logging and a Cloud Storage bucket as an export target for the logs you want to save.\n\nFilter in Cloud Loggin for specific content, and Cloud Storage for storage."
      },
      {
        "date": "2021-12-27T19:25:00.000Z",
        "voteCount": 4,
        "content": "Option-A is correct. Need cloud storage bucket for long time storage."
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 174,
    "url": "https://www.examtopics.com/discussions/google/view/68685-exam-professional-cloud-architect-topic-1-question-174/",
    "body": "You are working with a data warehousing team that performs data analysis. The team needs to process data from external partners, but the data contains personally identifiable information (PII). You need to process and store the data without storing any of the PIIE data. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Dataflow pipeline to retrieve the data from the external sources. As part of the pipeline, use the Cloud Data Loss Prevention (Cloud DLP) API to remove any PII data. Store the result in BigQuery.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Dataflow pipeline to retrieve the data from the external sources. As part of the pipeline, store all non-PII data in BigQuery and store all PII data in a Cloud Storage bucket that has a retention policy set.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAsk the external partners to upload all data on Cloud Storage. Configure Bucket Lock for the bucket. Create a Dataflow pipeline to read the data from the bucket. As part of the pipeline, use the Cloud Data Loss Prevention (Cloud DLP) API to remove any PII data. Store the result in BigQuery.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAsk the external partners to import all data in your BigQuery dataset. Create a dataflow pipeline to copy the data into a new table. As part of the Dataflow bucket, skip all data in columns that have PII data"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 56,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 19,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-12-27T19:27:00.000Z",
        "voteCount": 50,
        "content": "Option-A is correct. Although Option-C sounds good, ultimately we should not store PI data at all as per question says."
      },
      {
        "date": "2021-12-28T04:41:00.000Z",
        "voteCount": 14,
        "content": "The correct answer is A.\nOption C seems to be an option, but there are two non-conformities there. In addition to storing personal data in the GCS, it is being improperly retained."
      },
      {
        "date": "2024-04-10T00:18:00.000Z",
        "voteCount": 1,
        "content": "Agree with A"
      },
      {
        "date": "2024-03-10T13:45:00.000Z",
        "voteCount": 1,
        "content": "This best option."
      },
      {
        "date": "2023-10-19T13:42:00.000Z",
        "voteCount": 1,
        "content": "A is best answer, C seems be an extract step and security risk to upload to a bucket first"
      },
      {
        "date": "2023-06-15T09:27:00.000Z",
        "voteCount": 3,
        "content": "The problem with C is the data is stored in the bucket with the PII data even though the BigQuery data has it removed?"
      },
      {
        "date": "2023-09-05T11:09:00.000Z",
        "voteCount": 1,
        "content": "exactly"
      },
      {
        "date": "2023-02-28T14:26:00.000Z",
        "voteCount": 1,
        "content": "option A, the question say dont store data..."
      },
      {
        "date": "2023-02-20T07:13:00.000Z",
        "voteCount": 1,
        "content": "A is correct."
      },
      {
        "date": "2023-02-15T10:25:00.000Z",
        "voteCount": 1,
        "content": "Answer A. The question say do not store PII data so need to remove it before storing."
      },
      {
        "date": "2023-02-11T20:51:00.000Z",
        "voteCount": 1,
        "content": "Answer should be A because the question emphasises on processing the data without storing it. That rules out C."
      },
      {
        "date": "2023-02-07T06:27:00.000Z",
        "voteCount": 2,
        "content": "C -- is wrong because PII data is uploaded and the bucket is locked which means the data cannot be deleted\nB and D are wron as they do not use Data loss prevention to protect data"
      },
      {
        "date": "2023-02-05T17:12:00.000Z",
        "voteCount": 1,
        "content": "PII --&gt; Cloud DLP.  So that narrows the choices down to A or C.  C says \"Ask the external partners to upload all data on Cloud Storage\" which is not generally a feasible or recommended practice. Also, we cannot store PII anywhere, including in GCS.  Answer is A."
      },
      {
        "date": "2022-12-28T01:26:00.000Z",
        "voteCount": 1,
        "content": "A i s correct, C sounds good but storing the data in GCS is already a violation of the PII requirements"
      },
      {
        "date": "2022-12-26T01:45:00.000Z",
        "voteCount": 4,
        "content": "I would recommend option A, creating a Dataflow pipeline to retrieve the data from the external sources and using the Cloud Data Loss Prevention (Cloud DLP) API to remove any PII data. Storing the result in BigQuery would allow the data warehousing team to easily perform analysis on the data.\n\nOption C, using Bucket Lock to protect the data and using the Cloud DLP API to remove PII data, would protect the data from unauthorized access, but would not allow the data warehousing team to easily perform analysis on the data."
      },
      {
        "date": "2022-12-26T01:45:00.000Z",
        "voteCount": 2,
        "content": "Option B, storing non-PII data in BigQuery and PII data in a Cloud Storage bucket with a retention policy set, would not fully protect the PII data and could potentially lead to data breaches.\n\nOption D, copying the data into a new table and skipping columns with PII data, would not fully protect the PII data and could potentially lead to data breaches. It would also require the data warehousing team to manually skip certain columns when performing analysis, which could be time-consuming and error-prone."
      },
      {
        "date": "2022-12-16T03:09:00.000Z",
        "voteCount": 1,
        "content": "A  Is the Correct Answer"
      },
      {
        "date": "2022-12-08T04:34:00.000Z",
        "voteCount": 1,
        "content": "A is the right one."
      },
      {
        "date": "2022-11-29T13:46:00.000Z",
        "voteCount": 2,
        "content": "Of course the correct answer is A, not sure how some people think C is valid, probably trolling trying to confuse some here."
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 175,
    "url": "https://www.examtopics.com/discussions/google/view/68686-exam-professional-cloud-architect-topic-1-question-175/",
    "body": "You want to allow your operations team to store logs from all the production projects in your Organization, without including logs from other projects. All of the production projects are contained in a folder. You want to ensure that all logs for existing and new production projects are captured automatically. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an aggregated export on the Production folder. Set the log sink to be a Cloud Storage bucket in an operations project.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an aggregated export on the Organization resource. Set the log sink to be a Cloud Storage bucket in an operations project.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate log exports in the production projects. Set the log sinks to be a Cloud Storage bucket in an operations project.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate log exports in the production projects. Set the log sinks to be BigQuery datasets in the production projects, and grant IAM access to the operations team to run queries on the datasets."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 42,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-12-28T03:11:00.000Z",
        "voteCount": 16,
        "content": "The correct answer is A"
      },
      {
        "date": "2023-05-30T23:11:00.000Z",
        "voteCount": 9,
        "content": "The admin must have failed this exam multiple times. How can one select option B here."
      },
      {
        "date": "2024-07-31T01:16:00.000Z",
        "voteCount": 1,
        "content": "Creating an aggregated export at the organization level and setting the log sink to a Cloud Storage bucket in an operations project ensures comprehensive, automatic, and centralized log capture for all existing and new production projects. Therefore, Option B is the correct choice."
      },
      {
        "date": "2024-02-10T17:53:00.000Z",
        "voteCount": 2,
        "content": "Correct Answer is A.Looks like most of tje Admin answers are all incorrect to avoid sharing PDF to others."
      },
      {
        "date": "2023-03-09T12:46:00.000Z",
        "voteCount": 4,
        "content": "The correct answer is: A. Create an aggregated export on the Production folder. Set the log sink to be a Cloud Storage bucket in an operations project.\n\nThis solution will allow the operations team to store logs from all the production projects in your Organization, without including logs from other projects. All of the production projects are contained in a folder, so you can create an aggregated export on the Production folder. You can then set the log sink to be a Cloud Storage bucket in an operations project. This will allow the operations team to store all of the logs from the production projects in one place."
      },
      {
        "date": "2022-12-26T01:48:00.000Z",
        "voteCount": 2,
        "content": "Option B is not the correct solution because it creates an aggregated export on the Organization resource, which will capture logs from all projects in the Organization, including those outside the Production folder.\n\nThe best option to achieve the desired result is to create an aggregated export on the Production folder. Set the log sink to be a Cloud Storage bucket in an operations project. This will allow the operations team to store logs from all the production projects in the Organization, without including logs from other projects. Additionally, this setup will automatically capture logs for existing and new production projects.\n\nOption A is the correct solution because it allows you to create an aggregated export on the Production folder, which will capture logs from all the production projects contained in the folder. Setting the log sink to a Cloud Storage bucket in an operations project will allow the operations team to store the logs in a central location."
      },
      {
        "date": "2022-12-26T01:48:00.000Z",
        "voteCount": 3,
        "content": "Option C is not the correct solution because it requires you to create log exports in each production project, which can be time-consuming and error-prone. Additionally, setting the log sink to a Cloud Storage bucket in an operations project will not automatically capture logs for new production projects.\n\nOption D is not the correct solution because it requires you to create log exports in each production project, which can be time-consuming and error-prone. Additionally, storing the logs in BigQuery datasets in the production projects will not allow the operations team to easily access the logs. Instead, they would need to be granted IAM access to run queries on the datasets."
      },
      {
        "date": "2022-11-18T01:01:00.000Z",
        "voteCount": 1,
        "content": "A is ok"
      },
      {
        "date": "2022-10-19T19:38:00.000Z",
        "voteCount": 6,
        "content": "A is the right answer https://cloud.google.com/logging/docs/export/aggregated_sinks"
      },
      {
        "date": "2022-10-15T13:02:00.000Z",
        "voteCount": 1,
        "content": "I will choose A as right answer"
      },
      {
        "date": "2022-09-14T08:47:00.000Z",
        "voteCount": 3,
        "content": "A is the answer.\n\nhttps://cloud.google.com/logging/docs/export/aggregated_sinks\nAggregated sinks combine and route log entries from the Google Cloud resources contained by an organization or folder."
      },
      {
        "date": "2022-09-11T17:04:00.000Z",
        "voteCount": 3,
        "content": "A, is the only one that creates the policy in the Production Folder.\nHence doing what the question says \"make sure existing / all future projects gets automatically logs sabed\""
      },
      {
        "date": "2022-07-21T01:42:00.000Z",
        "voteCount": 3,
        "content": "A is ok"
      },
      {
        "date": "2022-04-07T12:18:00.000Z",
        "voteCount": 3,
        "content": "The correct answer is A"
      },
      {
        "date": "2022-04-03T02:26:00.000Z",
        "voteCount": 1,
        "content": "Anyone can explain the difference between A and C? Both options look similar..."
      },
      {
        "date": "2022-06-07T18:13:00.000Z",
        "voteCount": 10,
        "content": "Projects vs. folder. IF you create the export on the folder it will apply to all new projects under that folder."
      },
      {
        "date": "2022-09-04T04:18:00.000Z",
        "voteCount": 2,
        "content": "Now this is a really cool feature, thanks for the explanation!"
      },
      {
        "date": "2022-02-03T23:17:00.000Z",
        "voteCount": 2,
        "content": "\"without including logs from other projects. \" &lt;&lt;  Chose A as answer"
      },
      {
        "date": "2022-01-15T06:09:00.000Z",
        "voteCount": 5,
        "content": "Don't understand why we need to choose at organization level, Please explain if B is correct"
      },
      {
        "date": "2022-01-13T20:00:00.000Z",
        "voteCount": 9,
        "content": "A is the right answer."
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 176,
    "url": "https://www.examtopics.com/discussions/google/view/68688-exam-professional-cloud-architect-topic-1-question-176/",
    "body": "Your company has an application that is running on multiple instances of Compute Engine. It generates 1 TB per day of logs. For compliance reasons, the logs need to be kept for at least two years. The logs need to be available for active query for 30 days. After that, they just need to be retained for audit purposes. You want to implement a storage solution that is compliant, minimizes costs, and follows Google-recommended practices. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Install a Cloud Logging agent on all instances. 2. Create a sink to export logs into a regional Cloud Storage bucket. 3. Create an Object Lifecycle rule to move files into a Coldline Cloud Storage bucket after one month. 4. Configure a retention policy at the bucket level using bucket lock.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Write a daily cron job, running on all instances, that uploads logs into a Cloud Storage bucket. 2. Create a sink to export logs into a regional Cloud Storage bucket. 3. Create an Object Lifecycle rule to move files into a Coldline Cloud Storage bucket after one month.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Install a Cloud Logging agent on all instances. 2. Create a sink to export logs into a partitioned BigQuery table. 3. Set a time_partitioning_expiration of 30 days.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a daily cron job, running on all instances, that uploads logs into a partitioned BigQuery table. 2. Set a time_partitioning_expiration of 30 days."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 35,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-01-04T04:26:00.000Z",
        "voteCount": 23,
        "content": "The answer is A.\n\nThe practice for managing logs generated on Compute Engine on Google Cloud is to install the Cloud Logging agent and send them to Cloud Logging.\n\nThe sent logs will be aggregated into a Cloud Logging sink and exported to Cloud Storage.\nThe reason for using Cloud Storage as the destination for the logs is that the requirement in question requires setting up a lifecycle based on the storage period.\nIn this case, the log will be used for active queries for 30 days after it is saved, but after that, it needs to be stored for a longer period of time for auditing purposes.\n\nIf the data is to be used for active queries, we can use BigQuery's Cloud Storage data query feature and move the data past 30 days to Coldline to build a cost-optimal solution.\n\nTherefore, the correct answer is as follows\n1. Install the Cloud Logging agent on all instances.\nCreate a sync that exports the logs to the region's Cloud Storage bucket.\n3. Create an Object Lifecycle rule to move the files to the Coldline Cloud Storage bucket after one month. 4.\n4. set up a bucket-level retention policy using bucket locking.\""
      },
      {
        "date": "2022-01-04T04:27:00.000Z",
        "voteCount": 4,
        "content": "https://cloud.google.com/logging/docs/agent/logging/installation\nhttps://cloud.google.com/logging/docs/export/configure_export_v2\nhttps://cloud.google.com/bigquery/external-data-cloud-storage"
      },
      {
        "date": "2023-10-18T23:36:00.000Z",
        "voteCount": 5,
        "content": "None of the options are correct:\n\nA - It should be archive (&gt;= 365 days) and not coldline ( &gt;= 90 days) - Proposed solution is more expensive that what is possible. Also no way to query unless you use BigQuery external tables.\nB &amp; C - Wrong because CRON is not the way to do this.\nD - Wrong because data is deleted after 30 days and not retained for 2 years."
      },
      {
        "date": "2024-04-29T04:27:00.000Z",
        "voteCount": 1,
        "content": "The question comes from the times when there was no Archive storage class in GCS yet"
      },
      {
        "date": "2024-02-20T07:05:00.000Z",
        "voteCount": 2,
        "content": "No need to look at any other options."
      },
      {
        "date": "2023-08-31T13:26:00.000Z",
        "voteCount": 1,
        "content": "Answer A is misleading/confusing: \"3. Create an Object Lifecycle rule to move files into a Coldline Cloud Storage bucket after one month.\" \n\nA lifecycle rule will NOT move files in another bucket (coldline bucket etc). It will just change the storage class of the file."
      },
      {
        "date": "2023-02-23T03:34:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2023-02-20T07:18:00.000Z",
        "voteCount": 1,
        "content": "A is correct."
      },
      {
        "date": "2022-12-26T02:20:00.000Z",
        "voteCount": 2,
        "content": "A. 1. Install a Cloud Logging agent on all instances. 2. Create a sink to export logs into a regional Cloud Storage bucket. 3. Create an Object Lifecycle rule to move files into a Coldline Cloud Storage bucket after one month. 4. Configure a retention policy at the bucket level using bucket lock.\n\nThis approach would allow you to use Cloud Logging to collect and export the logs from the Compute Engine instances into a Cloud Storage bucket. You can then use an Object Lifecycle rule to automatically move the logs from the regional bucket to a Coldline bucket after one month, which will reduce storage costs for logs that are not actively being queried. By configuring a retention policy using bucket lock, you can ensure that the logs are retained for at least two years for audit purposes. This approach follows Google-recommended practices for storing logs and minimizing costs."
      },
      {
        "date": "2022-12-16T03:23:00.000Z",
        "voteCount": 1,
        "content": "A  Is the Correct Answer"
      },
      {
        "date": "2022-12-02T07:45:00.000Z",
        "voteCount": 1,
        "content": "A is perfect answer\u2026 the rest doesn\u2019t sound rational"
      },
      {
        "date": "2022-11-18T01:04:00.000Z",
        "voteCount": 1,
        "content": "A is ok"
      },
      {
        "date": "2022-10-19T19:41:00.000Z",
        "voteCount": 1,
        "content": "A is the correct answer"
      },
      {
        "date": "2022-10-15T13:03:00.000Z",
        "voteCount": 2,
        "content": "I agree with A, There is no need of BigQuery."
      },
      {
        "date": "2022-10-13T05:42:00.000Z",
        "voteCount": 1,
        "content": "A is fine"
      },
      {
        "date": "2022-08-12T21:01:00.000Z",
        "voteCount": 1,
        "content": "For compliance reasons, the logs need to be kept for at least two years... In Bigquery time partitioned after 30 days ..how the logs be present for 2 years ..Hence going with A"
      },
      {
        "date": "2022-08-05T02:40:00.000Z",
        "voteCount": 1,
        "content": "A is a no-brainer"
      },
      {
        "date": "2022-07-21T01:46:00.000Z",
        "voteCount": 1,
        "content": "A is ok"
      },
      {
        "date": "2022-06-12T05:14:00.000Z",
        "voteCount": 2,
        "content": "when a partition expires, the data in the partition is no longer available"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 177,
    "url": "https://www.examtopics.com/discussions/google/view/68690-exam-professional-cloud-architect-topic-1-question-177/",
    "body": "Your company has just recently activated Cloud Identity to manage users. The Google Cloud Organization has been configured as well. The security team needs to secure projects that will be part of the Organization. They want to prohibit IAM users outside the domain from gaining permissions from now on. What should they do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an organization policy to restrict identities by domain.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an organization policy to block creation of service accounts.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Cloud Scheduler to trigger a Cloud Function every hour that removes all users that don't belong to the Cloud Identity domain from all projects.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a technical user (e.g., crawler@yourdomain.com), and give it the project owner role at root organization level. Write a bash script that: \u05d2\u20ac\u00a2 Lists all the IAM rules of all projects within the organization. \u05d2\u20ac\u00a2 Deletes all users that do not belong to the company domain. Create a Compute Engine instance in a project within the Organization and configure gcloud to be executed with technical user credentials. Configure a cron job that executes the bash script every hour."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 34,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-05-04T05:28:00.000Z",
        "voteCount": 16,
        "content": "LOL , if we give this question to someone who know nothing about GCP they will select A"
      },
      {
        "date": "2022-01-04T18:41:00.000Z",
        "voteCount": 15,
        "content": "https://cloud.google.com/resource-manager/docs/organization-policy/restricting-domains"
      },
      {
        "date": "2024-02-18T02:16:00.000Z",
        "voteCount": 3,
        "content": "Whoever wrote option D was high af."
      },
      {
        "date": "2023-12-03T06:03:00.000Z",
        "voteCount": 1,
        "content": "D is Ridiculous."
      },
      {
        "date": "2023-05-30T23:20:00.000Z",
        "voteCount": 1,
        "content": "Option D is just to create confusion only."
      },
      {
        "date": "2023-03-09T13:10:00.000Z",
        "voteCount": 2,
        "content": "The correct answer is: A. Configure an organization policy to restrict identities by domain.\n\nThis solution will allow the security team to secure projects that will be part of the Organization by prohibiting IAM users outside the domain from gaining permissions.\n\nThe other options are not as efficient or effective. Option B would not be efficient, as it would block the creation of all service accounts, which are necessary for some applications. Option C would not be effective, as it would not prevent IAM users from gaining permissions, as it would only remove users that do not belong to the Cloud Identity domain from all projects. Option D would not be efficient, as it would require a Compute Engine instance to be created and a cron job to be configured, which would add complexity and cost to the solution."
      },
      {
        "date": "2023-02-23T03:36:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2022-12-26T02:22:00.000Z",
        "voteCount": 2,
        "content": "The security team should configure an organization policy to restrict identities by domain. This will allow them to specify a list of allowed domains, and prevent users from outside those domains from gaining permissions in the Organization.\n\nAlternatively, the security team could configure an organization policy to block creation of service accounts. This would prevent the creation of new service accounts, which could be used to grant permissions to users outside the domain.\n\nThe other options are not recommended. Option C involves manually removing users every hour, which could be time-consuming and error-prone. Option D involves creating a technical user and writing a bash script to delete users, which is not a recommended approach. It would be more secure and efficient to use an organization policy to restrict identities by domain."
      },
      {
        "date": "2022-12-16T03:29:00.000Z",
        "voteCount": 1,
        "content": "A  Is the Correct Answer"
      },
      {
        "date": "2022-11-18T01:08:00.000Z",
        "voteCount": 1,
        "content": "A is ok"
      },
      {
        "date": "2022-10-19T19:42:00.000Z",
        "voteCount": 1,
        "content": "A is the correct answer\nhttps://cloud.google.com/resource-manager/docs/organization-policy/restricting-domains"
      },
      {
        "date": "2022-10-15T13:07:00.000Z",
        "voteCount": 1,
        "content": "A is right"
      },
      {
        "date": "2022-10-13T05:45:00.000Z",
        "voteCount": 1,
        "content": "A is OK"
      },
      {
        "date": "2022-07-21T01:48:00.000Z",
        "voteCount": 3,
        "content": "A is ok"
      },
      {
        "date": "2022-02-15T14:29:00.000Z",
        "voteCount": 5,
        "content": "2/15/21 exam"
      },
      {
        "date": "2022-01-06T13:01:00.000Z",
        "voteCount": 3,
        "content": "must restrict the access, not clean up every hour. see reference from Fotofilico"
      },
      {
        "date": "2022-01-06T06:49:00.000Z",
        "voteCount": 1,
        "content": "Go for A"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 178,
    "url": "https://www.examtopics.com/discussions/google/view/68691-exam-professional-cloud-architect-topic-1-question-178/",
    "body": "Your company has an application running on Google Cloud that is collecting data from thousands of physical devices that are globally distributed. Data is published to Pub/Sub and streamed in real time into an SSD Cloud Bigtable cluster via a Dataflow pipeline. The operations team informs you that your Cloud<br>Bigtable cluster has a hotspot, and queries are taking longer than expected. You need to resolve the problem and prevent it from happening in the future. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdvise your clients to use HBase APIs instead of NodeJS APIs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDelete records older than 30 days.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReview your RowKey strategy and ensure that keys are evenly spread across the alphabet.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDouble the number of nodes you currently have."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-12-27T19:44:00.000Z",
        "voteCount": 13,
        "content": "Option-C is correct: https://cloud.google.com/bigtable/docs/schema-design#row-keys"
      },
      {
        "date": "2022-12-26T02:25:00.000Z",
        "voteCount": 9,
        "content": "C. Review your RowKey strategy and ensure that keys are evenly spread across the alphabet.\n\nThe RowKey is used to sort data within a Cloud Bigtable cluster. If the keys are not evenly spread across the alphabet, it can result in a hotspot and slow down queries. To prevent this from happening in the future, you should review your RowKey strategy and ensure that keys are evenly spread across the alphabet. This will help to distribute the data evenly across the cluster and improve query performance. Other potential solutions to consider include adding more nodes to the cluster or optimizing your query patterns. However, deleting records older than 30 days or advising clients to use HBase APIs instead of NodeJS APIs would not address the issue of a hotspot in the cluster."
      },
      {
        "date": "2023-03-01T06:08:00.000Z",
        "voteCount": 1,
        "content": "C in the only answer that make sense."
      },
      {
        "date": "2022-12-16T03:31:00.000Z",
        "voteCount": 1,
        "content": "C  Is the Correct Answer"
      },
      {
        "date": "2022-12-12T11:03:00.000Z",
        "voteCount": 1,
        "content": "C\nhttps://cloud.google.com/bigtable/docs/overview#load-balancing"
      },
      {
        "date": "2022-11-18T01:10:00.000Z",
        "voteCount": 1,
        "content": "C is ok"
      },
      {
        "date": "2022-10-23T12:32:00.000Z",
        "voteCount": 1,
        "content": "C is the right answer"
      },
      {
        "date": "2022-10-15T13:10:00.000Z",
        "voteCount": 1,
        "content": "C is better option"
      },
      {
        "date": "2022-08-01T21:43:00.000Z",
        "voteCount": 1,
        "content": "Option-C is the better one."
      },
      {
        "date": "2022-07-09T09:03:00.000Z",
        "voteCount": 2,
        "content": "The issue described is with \"querying\"  meaning reading. Not writing. C: distributing across the Alphabet is good for Writing."
      },
      {
        "date": "2022-05-17T20:45:00.000Z",
        "voteCount": 7,
        "content": "Suggest to study the following reference, it's important to design the row key pattern in Bigtable.\nhttps://cloud.google.com/bigtable/docs/overview#architecture\nhttps://cloud.google.com/bigtable/docs/overview#load-balancing"
      },
      {
        "date": "2022-01-10T15:16:00.000Z",
        "voteCount": 3,
        "content": "C looks good, I dont think we have to control number of nodes in Big table"
      },
      {
        "date": "2022-01-06T06:50:00.000Z",
        "voteCount": 2,
        "content": "Vote C"
      },
      {
        "date": "2021-12-31T21:50:00.000Z",
        "voteCount": 1,
        "content": "C. Review your RowKey strategy and ensure that keys are evenly spread across the alphabet."
      },
      {
        "date": "2021-12-30T22:30:00.000Z",
        "voteCount": 2,
        "content": "C is answer.Hot key/partitions are created due to improper row key design."
      },
      {
        "date": "2021-12-30T00:18:00.000Z",
        "voteCount": 1,
        "content": "correct row-key strategy improves performance"
      },
      {
        "date": "2021-12-28T12:06:00.000Z",
        "voteCount": 1,
        "content": "I think the ans is D"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 179,
    "url": "https://www.examtopics.com/discussions/google/view/68692-exam-professional-cloud-architect-topic-1-question-179/",
    "body": "Your company has a Google Cloud project that uses BigQuery for data warehousing. There are some tables that contain personally identifiable information (PII).<br>Only the compliance team may access the PII. The other information in the tables must be available to the data science team. You want to minimize cost and the time it takes to assign appropriate access to the tables. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. From the dataset where you have the source data, create views of tables that you want to share, excluding PII. 2. Assign an appropriate project-level IAM role to the members of the data science team. 3. Assign access controls to the dataset that contains the view.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. From the dataset where you have the source data, create materialized views of tables that you want to share, excluding PII. 2. Assign an appropriate project-level IAM role to the members of the data science team. 3. Assign access controls to the dataset that contains the view.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a dataset for the data science team. 2. Create views of tables that you want to share, excluding PII. 3. Assign an appropriate project-level IAM role to the members of the data science team. 4. Assign access controls to the dataset that contains the view. 5. Authorize the view to access the source dataset.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a dataset for the data science team. 2. Create materialized views of tables that you want to share, excluding PII. 3. Assign an appropriate project-level IAM role to the members of the data science team. 4. Assign access controls to the dataset that contains the view. 5. Authorize the view to access the source dataset."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 52,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 31,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-05-15T07:39:00.000Z",
        "voteCount": 17,
        "content": "Materialized view is too costly for the requirement. So B &amp; D is out.\n\nTo protect PII, there is no need to create another dataset. Creating a view on the original dataset should be sufficient. In addition, according to https://cloud.google.com/bigquery/docs/view-access-controls, view access can be granted at the 'dataset' level."
      },
      {
        "date": "2022-11-20T12:36:00.000Z",
        "voteCount": 4,
        "content": "Sorry I mean C"
      },
      {
        "date": "2022-09-18T06:02:00.000Z",
        "voteCount": 8,
        "content": "Authorized views should be created in a different dataset from the source data. That way, data owners can give users access to the authorized view without simultaneously granting access to the underlying data."
      },
      {
        "date": "2022-11-20T12:34:00.000Z",
        "voteCount": 4,
        "content": "A is correct \n\nGiving a view access to a dataset is also known as creating an authorized view in BigQuery. An authorized view lets you share query results with particular users and groups without giving them access to the underlying tables. You can also use the view's SQL query to restrict the columns (fields) the users are able to query. In this tutorial, you create an authorized view.\n\nhttps://cloud.google.com/bigquery/docs/share-access-views"
      },
      {
        "date": "2022-01-03T09:56:00.000Z",
        "voteCount": 13,
        "content": "C is correct here. You need view to avoid PII data. So materialized view is not needed."
      },
      {
        "date": "2022-10-20T13:36:00.000Z",
        "voteCount": 1,
        "content": "also can't query data from a view, so A not."
      },
      {
        "date": "2022-10-20T13:45:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/bigquery/docs/share-access-views"
      },
      {
        "date": "2024-09-15T10:15:00.000Z",
        "voteCount": 1,
        "content": "Devil is in the details. Question mentioned table\"s\". Which mean we have more than 1 table which will result in more than 1 view. You can authorize each view on \"same\" dataset or group all view\"s\" in a dataset then authorize the dataset. This does not answer the question (may be wording) but this is the concept behind the question."
      },
      {
        "date": "2023-09-03T23:03:00.000Z",
        "voteCount": 6,
        "content": "C: view in a different dataset (https://cloud.google.com/bigquery/docs/share-access-views: \"Authorized views should be created in a different dataset from the source data. That way, data owners can give users access to the authorized view without simultaneously granting access to the underlying data.\")"
      },
      {
        "date": "2023-10-28T08:17:00.000Z",
        "voteCount": 1,
        "content": "Yes, good source"
      },
      {
        "date": "2023-08-06T00:40:00.000Z",
        "voteCount": 6,
        "content": "I went with C. A will prevent data scientists from viewing PII in the view, it doesn't stop them from viewing it in the table however."
      },
      {
        "date": "2023-07-17T11:11:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/bigquery/docs/share-access-views#create_a_dataset_where_you_can_store_your_view"
      },
      {
        "date": "2023-07-11T08:53:00.000Z",
        "voteCount": 1,
        "content": "It's no needed to create a new dataset and for sure is not cost-effective. A is best option"
      },
      {
        "date": "2023-07-04T21:54:00.000Z",
        "voteCount": 1,
        "content": "It should be option A the question states minimize cost and time, option C though has better security requires additional step."
      },
      {
        "date": "2023-04-05T23:09:00.000Z",
        "voteCount": 2,
        "content": "Option A is not the best choice because it doesn't involve creating a separate dataset for the data science team. Creating a separate dataset provides better organization and access control management for different teams.\n\nIn option C, you create a separate dataset specifically for the data science team and then create views that exclude PII. This allows for more granular access controls and a better separation of concerns. By authorizing the view to access the source dataset, you ensure that the data science team can only access the non-PII data through the views, maintaining privacy and compliance."
      },
      {
        "date": "2023-03-24T01:17:00.000Z",
        "voteCount": 1,
        "content": "Option C  are not appropriate because creating a new dataset is not necessary in this scenario. Creating views of the tables that exclude PII is a simpler and more cost-effective solution. Additionally, authorizing the view to access the source dataset is not necessary because the view already contains the relevant data."
      },
      {
        "date": "2024-07-26T15:23:00.000Z",
        "voteCount": 1,
        "content": "Wrong , because you then have to give access at view level to prevent spillover , C is better.\nAlso, creating another dataset costs nothing, BQ charges for data stored and processed only, you can have a thousand datasets, it won't matter"
      },
      {
        "date": "2023-03-11T16:46:00.000Z",
        "voteCount": 1,
        "content": "C = A + One additional step to create the dataset which is not necessary so the answer is A"
      },
      {
        "date": "2023-03-11T16:44:00.000Z",
        "voteCount": 1,
        "content": "No need for materialized view which is an operational overhead."
      },
      {
        "date": "2023-03-09T13:22:00.000Z",
        "voteCount": 1,
        "content": "A. From the dataset where you have the source data, create views of tables that you want to share, excluding PII. 2. Assign an appropriate project-level IAM role to the members of the data science team. 3. Assign access controls to the dataset that contains the view. This solution will minimize cost and the time it takes to assign appropriate access to the tables. The other options are not as efficient or effective."
      },
      {
        "date": "2023-03-01T06:12:00.000Z",
        "voteCount": 2,
        "content": "I vote for C. Option C provides better security option."
      },
      {
        "date": "2023-02-19T12:34:00.000Z",
        "voteCount": 1,
        "content": "A is my answer."
      },
      {
        "date": "2023-02-15T10:34:00.000Z",
        "voteCount": 4,
        "content": "Agree with C from the link with google best practice\nhttps://cloud.google.com/bigquery/docs/share-access-views#create_a_dataset_where_you_can_store_your_view\nCreate a dataset where you can store your view\nAfter creating your source dataset, you create a new, separate dataset to store the authorized view that you share with your data analysts. In a later step, you grant the authorized view access to the data in the source dataset. Your data analysts then have access to the authorized view, but not direct access to the source data."
      },
      {
        "date": "2023-02-09T16:51:00.000Z",
        "voteCount": 2,
        "content": "Materialized views costs more than normal ones. Creating a new dataset is not cost-effective. You can use authorized views to restrict data access.\nFrom Google doc:\n\nGiving a view access to a dataset is also known as creating an authorized view in BigQuery. An authorized view lets you share query results with particular users and groups without giving them access to the underlying tables. You can also use the view's SQL query to restrict the columns (fields) the users are able to query.\n\n\nUsers need the bigquery.tables.getData permission on all tables and views that their query references. In addition, when querying a view users need this permission on all underlying tables and views. However, if you are using authorized views or authorized datasets, you don't need to give users access to the underlying source data.\n\nReference:\n\nhttps://cloud.google.com/bigquery/docs/share-access-views\n\nhttps://cloud.google.com/bigquery/docs/table-access-controls#required_permission_to_query_tables_and_views"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 180,
    "url": "https://www.examtopics.com/discussions/google/view/68693-exam-professional-cloud-architect-topic-1-question-180/",
    "body": "Your operations team currently stores 10 TB of data in an object storage service from a third-party provider. They want to move this data to a Cloud Storage bucket as quickly as possible, following Google-recommended practices. They want to minimize the cost of this data migration. Which approach should they use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the gsutil mv command to move the data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Storage Transfer Service to move the data.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDownload the data to a Transfer Appliance, and ship it to Google.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDownload the data to the on-premises data center, and upload it to the Cloud Storage bucket."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 14,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-05-15T08:50:00.000Z",
        "voteCount": 10,
        "content": "I would have voted C looking at 10 TB at first glance. But the senario other cloud storge changes the look out. You cannot wre master applica here. As you wont be able to remove and store data like on prem. Hence is or Starge Transfer service."
      },
      {
        "date": "2022-04-08T10:38:00.000Z",
        "voteCount": 7,
        "content": "B\nhttps://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets#transfer-options"
      },
      {
        "date": "2024-02-07T02:37:00.000Z",
        "voteCount": 1,
        "content": "I chose B.\nBandwidth specification is not included in the question.If the bandwidth is 10G or more, it is A.\nThe guideline based on the gcloud command is 1TB. Since the data is in an online service, you can use TransferService."
      },
      {
        "date": "2023-02-20T07:34:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is B"
      },
      {
        "date": "2022-11-18T01:33:00.000Z",
        "voteCount": 1,
        "content": "B is ok"
      },
      {
        "date": "2022-10-15T13:21:00.000Z",
        "voteCount": 2,
        "content": "B is right, please refer this link\nhttps://cloud.google.com/storage-transfer-service"
      },
      {
        "date": "2022-03-26T11:09:00.000Z",
        "voteCount": 3,
        "content": "Current storage is object store so mostly a cloud provider\u2026 thus storage transfer service"
      },
      {
        "date": "2022-01-19T09:34:00.000Z",
        "voteCount": 5,
        "content": "Got this question in my exam, answered B"
      },
      {
        "date": "2022-01-10T15:28:00.000Z",
        "voteCount": 1,
        "content": "B \nhttps://cloud.google.com/storage-transfer-service"
      },
      {
        "date": "2022-01-09T02:56:00.000Z",
        "voteCount": 1,
        "content": "I think B or C.\nA&amp;D are out. Becaus tha data size is 10TB."
      },
      {
        "date": "2022-01-09T03:04:00.000Z",
        "voteCount": 1,
        "content": "Can the third-party provider use B StrageTransferService? \nIf can,B is correct.\nThere is not enough information."
      },
      {
        "date": "2022-01-03T10:00:00.000Z",
        "voteCount": 1,
        "content": "B  Storage Transfer Service is correct."
      },
      {
        "date": "2021-12-31T21:39:00.000Z",
        "voteCount": 1,
        "content": "B. Use the Storage Transfer Service to move the data."
      },
      {
        "date": "2021-12-30T00:23:00.000Z",
        "voteCount": 1,
        "content": "@StelSen: re. B. Being a cloud provider is not a requirement for using STS"
      },
      {
        "date": "2021-12-28T05:21:00.000Z",
        "voteCount": 1,
        "content": "B: Storage Transfer Service\nhttps://cloud.google.com/storage-transfer-service"
      },
      {
        "date": "2021-12-27T20:00:00.000Z",
        "voteCount": 5,
        "content": "Very Tricky and don't have enough details to answer the question. Use this Guide (https://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets#transfer-options) and let's try to eliminate options.\n\nA. Use the gsutil mv command to move the data. (We have 10TB, hence rejected)\nB. Use the Storage Transfer Service to move the data. (Source might not be Cloud provider. Hence rejecting it. If source is AWS/Azure then this is the answer)\nC. Download the data to a Transfer Appliance, and ship it to Google. (I don't think we can use Transfer Appliance at Third party service providers DC. Assuming this 3rd party is not a cloud provider)\nD. Download the data to the on-premises data center, and upload it to the Cloud Storage bucket. (This seems better assuming DC has good bandwidth such as 1 Gbbs)"
      },
      {
        "date": "2022-06-15T11:07:00.000Z",
        "voteCount": 2,
        "content": "B is the answer"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 181,
    "url": "https://www.examtopics.com/discussions/google/view/80034-exam-professional-cloud-architect-topic-1-question-181/",
    "body": "You have a Compute Engine managed instance group that adds and removes Compute Engine instances from the group in response to the load on your application. The instances have a shutdown script that removes REDIS database entries associated with the instance. You see that many database entries have not been removed, and you suspect that the shutdown script is the problem. You need to ensure that the commands in the shutdown script are run reliably every time an instance is shut down. You create a Cloud Function to remove the database entries. What should you do next?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the shutdown script to wait for 30 seconds before triggering the Cloud Function.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDo not use the Cloud Function. Modify the shutdown script to restart if it has not completed in 30 seconds.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up a Cloud Monitoring sink that triggers the Cloud Function after an instance removal log message arrives in Cloud Logging.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the shutdown script to wait for 30 seconds and then publish a message to a Pub/Sub queue."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 20,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 4,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-10-28T05:15:00.000Z",
        "voteCount": 11,
        "content": "Actually C is correct but Wrong also in a way .. Sink cannot trigger a cloud function directly. It need Pub/Sub which then will trigger Cloud Function."
      },
      {
        "date": "2022-12-26T05:47:00.000Z",
        "voteCount": 7,
        "content": "The correct answer is C: Set up a Cloud Monitoring sink that triggers the Cloud Function after an instance removal log message arrives in Cloud Logging.\n\nIn this scenario, you want to ensure that the commands in the shutdown script are run reliably every time an instance is shut down. One way to do this is by setting up a Cloud Monitoring sink that triggers a Cloud Function after an instance removal log message arrives in Cloud Logging. This will allow you to use the Cloud Function to perform the necessary tasks (such as removing database entries) when an instance is shut down, and it will ensure that these tasks are performed reliably and consistently.\n\nOption A: Modifying the shutdown script to wait for 30 seconds before triggering the Cloud Function is not a reliable solution, as it relies on the shutdown script being able to run for at least 30 seconds before the instance is shut down."
      },
      {
        "date": "2022-12-26T05:47:00.000Z",
        "voteCount": 3,
        "content": "Option B: Modifying the shutdown script to restart if it has not completed in 30 seconds is also not a reliable solution, as it may not be feasible to restart the script if the instance has already been shut down.\n\nOption D: Modifying the shutdown script to wait for 30 seconds and then publish a message to a Pub/Sub queue is not a reliable solution, as it relies on the shutdown script being able to run for at least 30 seconds before the instance is shut down, and it also requires additional infrastructure (a Pub/Sub queue) to be set up and maintained."
      },
      {
        "date": "2024-09-29T03:13:00.000Z",
        "voteCount": 1,
        "content": "I'll go for C, even though this scenario should be handled in another way.\nThe reason why REDIS entries are still persisted is possibly caused by the application shutdown being not handled correctly. The shutdown script should first wait for the APP shutdown and only after that trigger the REDIS removal (cloud function). \n\nUsing cloud monitoring to intercept this situation is an option, but a bit of overkill in my opinion. I would better modify the shutdown script and the app shutdown procedure and only then triggering a CF. Too bad there is no option for this :)"
      },
      {
        "date": "2024-06-21T06:29:00.000Z",
        "voteCount": 2,
        "content": "You cannot trigger a Cloud Function directly from a Cloud Monitoring sink. Instead, you can set up a Cloud Monitoring alert that sends notifications to a Pub/Sub topic, and then trigger the Cloud Function from that Pub/Sub topic."
      },
      {
        "date": "2023-09-19T17:57:00.000Z",
        "voteCount": 1,
        "content": "Option C is not an efficient way as it causes delay and complexity. on top of that, you wont be able to trigger a cloud function. Correct answer is option D."
      },
      {
        "date": "2023-02-01T16:36:00.000Z",
        "voteCount": 1,
        "content": "cCorrect Ans is C"
      },
      {
        "date": "2022-12-30T01:47:00.000Z",
        "voteCount": 2,
        "content": "Correct Ans is C"
      },
      {
        "date": "2022-12-16T04:11:00.000Z",
        "voteCount": 2,
        "content": "C  Is the Correct Answer"
      },
      {
        "date": "2022-11-21T02:28:00.000Z",
        "voteCount": 2,
        "content": "C is ok"
      },
      {
        "date": "2022-10-23T12:43:00.000Z",
        "voteCount": 1,
        "content": "C is the right answer"
      },
      {
        "date": "2022-10-19T19:54:00.000Z",
        "voteCount": 1,
        "content": "c is correct"
      },
      {
        "date": "2022-10-15T13:25:00.000Z",
        "voteCount": 3,
        "content": "C is right"
      },
      {
        "date": "2022-09-22T18:18:00.000Z",
        "voteCount": 4,
        "content": "use pub/sub trigger cloud function"
      },
      {
        "date": "2022-09-14T08:20:00.000Z",
        "voteCount": 5,
        "content": "C is the answer as shutdown script is run based on best effort and not a reliable method.\n\nhttps://cloud.google.com/compute/docs/shutdownscript#limitations\nCompute Engine executes shutdown scripts only on a best-effort basis. In rare cases, Compute Engine cannot guarantee that the shutdown script will complete."
      },
      {
        "date": "2022-09-07T02:12:00.000Z",
        "voteCount": 4,
        "content": "C would be the cleanest solution. Although at this time, Cloud Monitoring sink cannot trigger a cloud function directly, it can be done via Pub/Sub. Still better than solution D."
      },
      {
        "date": "2022-09-05T21:09:00.000Z",
        "voteCount": 1,
        "content": "C is the correct answer"
      },
      {
        "date": "2022-09-05T12:04:00.000Z",
        "voteCount": 1,
        "content": "C looks not a professional way, but can make sure the work being done."
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 182,
    "url": "https://www.examtopics.com/discussions/google/view/80035-exam-professional-cloud-architect-topic-1-question-182/",
    "body": "You are managing several projects on Google Cloud and need to interact on a daily basis with BigQuery, Bigtable, and Kubernetes Engine using the gcloud CL tool. You are travelling a lot and work on different workstations during the week. You want to avoid having to manage the gcloud CLI manually. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Google Cloud Shell in the Google Cloud Console to interact with Google Cloud.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Compute Engine instance and install gcloud on the instance. Connect to this instance via SSH to always use the same gcloud installation when interacting with Google Cloud.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall gcloud on all of your workstations. Run the command gcloud components auto-update on each workstation",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a package manager to install gcloud on your workstations instead of installing it manually."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 21,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-09-12T12:41:00.000Z",
        "voteCount": 8,
        "content": "First discard: \nC / D are totally not it. Since they will do so much work and it says \"do not want to manage gcoud CLI manually\"\nThen B is not a good cost option, besides at the end you are managing your gcloud cli manually. So A is the only left correct answer. \nIt even saves your $HOME files."
      },
      {
        "date": "2022-09-05T04:36:00.000Z",
        "voteCount": 5,
        "content": "If you're using Cloud Shell, the gcloud CLI is available automatically and you don't need to install it. commadn gcloud components auto-update doesn't exist but the command gcloud components update ensure that you are using updated version of your components."
      },
      {
        "date": "2024-07-21T16:27:00.000Z",
        "voteCount": 1,
        "content": "It is the simplest way!"
      },
      {
        "date": "2024-01-20T02:51:00.000Z",
        "voteCount": 1,
        "content": "With B, you can keep your terminal working from any workstation. Why not?\nOkay, so the question is saying that you don't want to manage gcloud cli itself."
      },
      {
        "date": "2023-11-19T03:14:00.000Z",
        "voteCount": 1,
        "content": "I will say A, except if some VPC restriction prevent using cloud shell.."
      },
      {
        "date": "2022-12-16T04:14:00.000Z",
        "voteCount": 1,
        "content": "A  Is the Correct Answer"
      },
      {
        "date": "2022-11-21T02:31:00.000Z",
        "voteCount": 1,
        "content": "A is ok"
      },
      {
        "date": "2022-10-15T13:27:00.000Z",
        "voteCount": 1,
        "content": "A is right"
      },
      {
        "date": "2022-09-04T04:41:00.000Z",
        "voteCount": 4,
        "content": "A is the correct solution, as the only requirement for your workstation is to have a browser installed."
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 183,
    "url": "https://www.examtopics.com/discussions/google/view/80075-exam-professional-cloud-architect-topic-1-question-183/",
    "body": "Your company recently acquired a company that has infrastructure in Google Cloud. Each company has its own Google Cloud organization. Each company is using a Shared Virtual Private Cloud (VPC) to provide network connectivity for its applications. Some of the subnets used by both companies overlap. In order for both businesses to integrate, the applications need to have private network connectivity. These applications are not on overlapping subnets. You want to provide connectivity with minimal re-engineering. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up VPC peering and peer each Shared VPC together.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the projects from the acquired company into your company's Google Cloud organization. Re-launch the instances in your companies Shared VPC.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up a Cloud VPN gateway in each Shared VPC and peer Cloud VPNs.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure SSH port forwarding on each application to provide connectivity between applications in the different Shared VPCs."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 48,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 6,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-09-09T09:55:00.000Z",
        "voteCount": 26,
        "content": "VPC peering cannot be established between VPCs if there is IP range overlap. C is ok since you can establish VPN across these VPCs and only include the applications required IP ranges as its mentioned that they do not overlap"
      },
      {
        "date": "2023-05-16T12:51:00.000Z",
        "voteCount": 8,
        "content": "looks like the following best practice: https://cloud.google.com/architecture/best-practices-vpc-design#shared-service\n\nCloud VPN is another alternative. Because Cloud VPN establishes reachability through managed IPsec tunnels, it doesn't have the aggregate limits of VPC Network Peering. Cloud VPN uses a VPN Gateway for connectivity and doesn't consider the aggregate resource use of the IPsec peer. The drawbacks of Cloud VPN include increased costs (VPN tunnels and traffic egress), management overhead required to maintain tunnels, and the performance overhead of IPsec."
      },
      {
        "date": "2024-02-04T02:54:00.000Z",
        "voteCount": 1,
        "content": "c -&gt; https://cloud.google.com/vpc/docs/using-vpc-peering#no_subnet_ip_range_overlap_across_peered_networks"
      },
      {
        "date": "2023-11-25T09:15:00.000Z",
        "voteCount": 3,
        "content": "No subnet IP range overlap across peered VPC networks\nhttps://cloud.google.com/vpc/docs/using-vpc-peering"
      },
      {
        "date": "2023-04-13T00:27:00.000Z",
        "voteCount": 3,
        "content": "It seems to be C because VPN peering use BGP protocol that manages the overlaps.\nhttps://cloud.google.com/network-connectivity/docs/vpn/how-to/configuring-peer-gateway"
      },
      {
        "date": "2023-04-03T13:57:00.000Z",
        "voteCount": 1,
        "content": "omermahgoub said it best. C"
      },
      {
        "date": "2023-03-13T12:55:00.000Z",
        "voteCount": 1,
        "content": "B to reorg it all under one org cos it's a mess. \nYou cant have shared RFC1918 ranges between peered networks OR VPNs... Don't know why everyone thinks VPNs avoid that problem. https://cloud.google.com/network-connectivity/docs/vpn/how-to/creating-ha-vpn2 \"You can connect two VPC networks together as long as the primary and secondary subnet IP address ranges in each network don't overlap.\""
      },
      {
        "date": "2023-06-17T01:42:00.000Z",
        "voteCount": 1,
        "content": "This applies to static routes only.  \"A dynamic route can overlap with a subnet route in a peer network. For dynamic routes, the destination ranges that overlap with a subnet route from the peer network are silently dropped. Google Cloud uses the subnet route.\".  https://cloud.google.com/vpc/docs/vpc-peering"
      },
      {
        "date": "2023-02-07T07:50:00.000Z",
        "voteCount": 1,
        "content": "Please check answer from  BalaGCPArch \n\n\"https://cloud.google.com/vpc/docs/vpc-peering#overlapping_subnets_at_time_of_peering\nOverlapping subnets at time of peering\nAt the time of peering, Google Cloud checks to see if there are any subnets with overlapping IP ranges between the two VPC networks or any of their peered networks. If there is an overlap, peering is not established. Since a full mesh connectivity is created between VM instances, subnets in the peered VPC networks can't have overlapping IP ranges as this would cause routing issues.\""
      },
      {
        "date": "2022-12-26T06:04:00.000Z",
        "voteCount": 7,
        "content": "VPC peering is generally possible even if there are overlapping subnets between the two VPCs. However, there are some considerations to keep in mind if there's overlapping subnets:\n1. You will not be able to route traffic between the overlapping subnets. If needed, you will have to use a different method (such as a Cloud VPN connection or a Cloud Router) to connect the VPCs.\n2. You will need to ensure that the overlapping subnets are not used by any resources in either VPC. This means that you will need to either modify the existing network configuration to avoid using the overlapping subnets, or you will need to create new subnets that do not overlap.\n3. You may need to update any existing firewall rules or routes that refer to the overlapping subnets to ensure that they are still valid after the VPCs are peered.\n\nIn the question, you want to provide private network connectivity between the two companies' applications, which are not on overlapping subnets. However, there is overlap in the subnets used by both companies, which means that you will not be able to use VPC peering to connect the two VPCs."
      },
      {
        "date": "2022-12-26T06:04:00.000Z",
        "voteCount": 8,
        "content": "One solution in this case would be to set up a Cloud VPN gateway in each Shared VPC and peer Cloud VPNs. This will allow you to create a secure, private network connection between the two VPCs, and it will allow the applications in each company's Shared VPC to communicate with each other over the private connection.\n\nThe correct answer is C: Set up a Cloud VPN gateway in each Shared VPC and peer Cloud VPNs.\n\n\nOption A: Setting up VPC peering and peering each Shared VPC together would not be a viable solution in this case, because the subnets used by both companies overlap, and VPC peering does not support overlapping subnets."
      },
      {
        "date": "2022-12-15T15:43:00.000Z",
        "voteCount": 1,
        "content": "Answer is C\nA is wrong because you cannot peer VPCs with overlapping subnets:\nhttps://cloud.google.com/vpc/docs/vpc-peering#interaction-subnet-subnet\nIPv4 subnet routes in peered VPC networks can't overlap:\n- Peering prohibits identical IPv4 subnet routes. For example, two peered VPC networks can't both have an IPv4 subnet route whose destination is 100.64.0.0/10.\n- Peering prohibits a subnet route from being contained within a peering subnet route. For example, if the local VPC network has a subnet route whose destination is 100.64.0.0/24, then none of the peered VPC networks can have a subnet route whose destination is 100.64.0.0/10.\n\nB and D are ruled out because it breaks the requirement \"with minimal re-engineering\" to the applications"
      },
      {
        "date": "2022-11-21T02:20:00.000Z",
        "voteCount": 4,
        "content": "A is ok: The applications are not on overlapping subnets. So use VPC peering.\nYou want to provide connectivity with minimal re-engineering. VPC Network Peering\naccomplishes this.\nhttps://cloud.google.com/vpc/docs/vpc-peering"
      },
      {
        "date": "2022-12-03T14:42:00.000Z",
        "voteCount": 6,
        "content": "C should be the Answer : same explaination goes here \n\n\"https://cloud.google.com/vpc/docs/vpc-peering#overlapping_subnets_at_time_of_peering\n\nOverlapping subnets at time of peering\nAt the time of peering, Google Cloud checks to see if there are any subnets with overlapping IP ranges between the two VPC networks or any of their peered networks. If there is an overlap, peering is not established. Since a full mesh connectivity is created between VM instances, subnets in the peered VPC networks can't have overlapping IP ranges as this would cause routing issues.\""
      },
      {
        "date": "2023-07-06T22:56:00.000Z",
        "voteCount": 1,
        "content": "Thanks for this"
      },
      {
        "date": "2022-10-23T12:52:00.000Z",
        "voteCount": 1,
        "content": "C is correct answer"
      },
      {
        "date": "2022-10-15T13:31:00.000Z",
        "voteCount": 1,
        "content": "C is best option"
      },
      {
        "date": "2022-09-22T18:29:00.000Z",
        "voteCount": 2,
        "content": "These applications ARE NOT on overlapping subnets"
      },
      {
        "date": "2022-10-02T05:45:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/vpc/docs/vpc-peering#overlapping_subnets_at_time_of_peering\n\nOverlapping subnets at time of peering\nAt the time of peering, Google Cloud checks to see if there are any subnets with overlapping IP ranges between the two VPC networks or any of their peered networks. If there is an overlap, peering is not established. Since a full mesh connectivity is created between VM instances, subnets in the peered VPC networks can't have overlapping IP ranges as this would cause routing issues."
      },
      {
        "date": "2022-09-21T02:02:00.000Z",
        "voteCount": 1,
        "content": "the answer is A"
      },
      {
        "date": "2022-09-15T04:34:00.000Z",
        "voteCount": 1,
        "content": "Can someone explain my why not B?\nIt's fine to eliminate A due to overlapping and also D because is out of discussion, but why C is better than B?"
      },
      {
        "date": "2022-09-15T08:36:00.000Z",
        "voteCount": 5,
        "content": "you need minimal re-engineering. migrating projects and relaunching instances will be a significant effort."
      },
      {
        "date": "2022-09-12T12:45:00.000Z",
        "voteCount": 1,
        "content": "Google Documentation \" When a VPC subnet is created or a subnet IP range is expanded, Google Cloud performs a check to make sure the new subnet range does not overlap with IP ranges \" \n I know the subnets where the application is hosted, does not overlap, however it will not allow a VPC peering because of that overlap, so the only possible answer is C.\n\nhttps://cloud.google.com/vpc/docs/vpc-peering"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 184,
    "url": "https://www.examtopics.com/discussions/google/view/79697-exam-professional-cloud-architect-topic-1-question-184/",
    "body": "You are managing several internal applications that are deployed on Compute Engine. Business users inform you that an application has become very slow over the past few days. You want to find the underlying cause in order to solve the problem. What should you do first?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInspect the logs and metrics from the instances in Cloud Logging and Cloud Monitoring.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the Compute Engine Instances behind the application to a machine type with more CPU and memory.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRestore a backup of the application database from a time before the application became slow.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the applications on a managed instance group with autoscaling enabled. Add a load balancer in front of the managed instance group, and have the users connect to the IP of the load balancer."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 39,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-09-03T00:54:00.000Z",
        "voteCount": 22,
        "content": "First thing to do is to inspect logs and monitoring to see what is happening"
      },
      {
        "date": "2022-12-26T06:10:00.000Z",
        "voteCount": 6,
        "content": "When an application becomes slow, the first step you should take is to gather information about the underlying cause of the problem. One way to do this is by inspecting the logs and metrics from the instances where the application is deployed. Google Cloud Platform (GCP) provides tools such as Cloud Logging and Cloud Monitoring that can help you to collect and analyze this information.\n\nBy reviewing the logs and metrics from the instances, you may be able to identify issues such as resource shortages (e.g. CPU, memory, or disk), network problems, or application errors that are causing the performance issues. Once you have identified the underlying cause of the problem, you can take steps to resolve it.\n\nThe correct answer is A: Inspect the logs and metrics from the instances in Cloud Logging and Cloud Monitoring."
      },
      {
        "date": "2022-12-26T06:10:00.000Z",
        "voteCount": 2,
        "content": "Option D: Deploying the applications on a managed instance group with autoscaling enabled and adding a load balancer in front of the managed instance group may help to improve the performance of the application, but it is not necessarily the first step you should take. You should first try to understand the underlying cause of the performance issues before making changes to the deployment architecture."
      },
      {
        "date": "2022-12-26T06:10:00.000Z",
        "voteCount": 2,
        "content": "Option B: Changing the Compute Engine instances behind the application to a machine type with more CPU and memory may help to improve the performance of the application, but it is not necessarily the first step you should take. You should first try to understand the underlying cause of the performance issues before making changes to the instances.\n\nOption C: Restoring a backup of the application database from a time before the application became slow may help to resolve the performance issues if the problem is related to the database. However, it is not necessarily the first step you should take, as there may be other issues causing the performance problems."
      },
      {
        "date": "2023-12-30T03:52:00.000Z",
        "voteCount": 1,
        "content": "A is the only inpection. \nYou want to inspect and find the underlying cause in order to solve the problem.\n\nB &amp; D are possible solutions, not inspection. \nC is neither solution nor inspection. C will just lead to the issue again."
      },
      {
        "date": "2023-11-14T07:26:00.000Z",
        "voteCount": 2,
        "content": "The admin has lost it"
      },
      {
        "date": "2023-03-01T06:40:00.000Z",
        "voteCount": 4,
        "content": "Key Word \"find the underlying cause\", so the answer is A."
      },
      {
        "date": "2023-02-07T07:52:00.000Z",
        "voteCount": 5,
        "content": "Question mentions \"You want to find the underlying cause in order to solve the problem\"\nB, C and D  are  attempt to solve the problem without finding the cause"
      },
      {
        "date": "2022-12-28T01:10:00.000Z",
        "voteCount": 4,
        "content": "this has nothing to do with \"gcp\" in real, this is SRE instinct"
      },
      {
        "date": "2022-12-16T04:29:00.000Z",
        "voteCount": 2,
        "content": "A  Is the Correct Answer"
      },
      {
        "date": "2022-10-15T13:32:00.000Z",
        "voteCount": 3,
        "content": "This is no brainer question, I would choose A"
      },
      {
        "date": "2022-09-12T12:52:00.000Z",
        "voteCount": 3,
        "content": "Agree with A.\nFirst remove any non possible answers: B / C.\nThen we have A or D left. \nBut D does a good action / recommended action but it says \"what do we do first\" which is always troubleshoot."
      },
      {
        "date": "2022-09-05T11:37:00.000Z",
        "voteCount": 1,
        "content": "First thing would be to inspect the logs"
      },
      {
        "date": "2022-09-04T06:28:00.000Z",
        "voteCount": 1,
        "content": "Answe is A.  I would agree the question is stating \"You want to find the underlying cause in order to solve the problem.\"  \nEverything else is making changes without understanding the issues at hand"
      },
      {
        "date": "2022-09-04T04:43:00.000Z",
        "voteCount": 2,
        "content": "A is the only answer that is really caring about **analyzing** the underlying problem before **touching** anything."
      },
      {
        "date": "2022-09-04T02:16:00.000Z",
        "voteCount": 1,
        "content": "key word is \"You want to find the underlying cause in order to solve the problem\""
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 185,
    "url": "https://www.examtopics.com/discussions/google/view/79702-exam-professional-cloud-architect-topic-1-question-185/",
    "body": "Your company has an application running as a Deployment in a Google Kubernetes Engine (GKE) cluster. When releasing new versions of the application via a rolling deployment, the team has been causing outages. The root cause of the outages is misconfigurations with parameters that are only used in production. You want to put preventive measures for this in the platform to prevent outages. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure liveness and readiness probes in the Pod specification.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure health checks on the managed instance group.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Scheduled Task to check whether the application is available.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an uptime alert in Cloud Monitoring."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 36,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-09-04T04:46:00.000Z",
        "voteCount": 19,
        "content": "A: Configuring the right liveness and readiness probes prevents outages when rolling out a new ReplicaSet of a Deployment, because Pods are only getting traffic when they are considered ready.\nB: With GKE, you do not deal with MIGs.\nC: Does not use GKE tools and is therefore not the best option.\nD: Does alert you but does not prevent the outage."
      },
      {
        "date": "2022-09-09T10:20:00.000Z",
        "voteCount": 8,
        "content": "more explanation in the below link..https://cloud.google.com/blog/products/containers-kubernetes/kubernetes-best-practices-setting-up-health-checks-with-readiness-and-liveness-probes"
      },
      {
        "date": "2024-10-16T05:27:00.000Z",
        "voteCount": 1,
        "content": "B. We are talking about GKE, not Compute Engine instances\nC. The task will not check in real-time\nD. Uptime alerts do not apply to GKE pods"
      },
      {
        "date": "2024-04-10T09:24:00.000Z",
        "voteCount": 2,
        "content": "There is not such a thing as Managed compute instances in GKE\nhttps://cloud.google.com/blog/products/containers-kubernetes/kubernetes-best-practices-setting-up-health-checks-with-readiness-and-liveness-probes"
      },
      {
        "date": "2024-03-03T13:56:00.000Z",
        "voteCount": 1,
        "content": "Right answer is D. Liveness and readiness probes (option A) are essential for overall application health but might not directly detect misconfigurations during deployments. They focus on ensuring pods are healthy and responsive, not necessarily catching configuration issues."
      },
      {
        "date": "2024-06-09T09:30:00.000Z",
        "voteCount": 1,
        "content": "uptime and monitoring will not stop outages in application, howver you will be informed on time to respond to the issue.\n\nA. Configuring liveness and readiness probe in each pod will stop starting pods from receiving traffic before they are declared ready and available. hence before taking down a working pod."
      },
      {
        "date": "2023-08-09T21:53:00.000Z",
        "voteCount": 2,
        "content": "Who answered aside from A never read/implementes kubernetes best practices. Link https://cloud.google.com/blog/products/containers-kubernetes/best-practices-for-creating-a-highly-available-gke-cluster"
      },
      {
        "date": "2023-06-19T22:58:00.000Z",
        "voteCount": 2,
        "content": "A without any doubts"
      },
      {
        "date": "2023-06-17T10:29:00.000Z",
        "voteCount": 2,
        "content": "D. Configure an uptime alert in Cloud Monitoring.\nConfiguring an uptime alert in Cloud Monitoring will notify the team when the application becomes unavailable. This will help in detecting outages before they occur and mitigate the risks of releasing new versions with misconfigurations.\nWhile configuring liveness and readiness probes in the Pod specification and configuring health checks on the managed instance group are important for ensuring that the application is running, they do not prevent outages caused by misconfigurations with production parameters.\nCreating a Scheduled Task to check whether the application is available is also useful, but it is not preventive in nature. By the time a scheduled task detects an outage, the damage may have already been done."
      },
      {
        "date": "2023-06-17T10:30:00.000Z",
        "voteCount": 1,
        "content": "why not A ?  \n* Configuring liveness and readiness probes in the Pod specification is important to detect when a container in a Pod becomes unresponsive or starts experiencing problems. However, it does not directly prevent outages caused by misconfigurations with parameters that are only used in production.\n\nLiveness and readiness probes can help to detect issues with the application, but they do not provide information about the health of the underlying infrastructure. Misconfigurations with parameters that are only used in production can cause problems with the infrastructure itself, which may not be detected by liveness and readiness probes.\n\nIn summary, while configuring liveness and readiness probes is important, it should be done in addition to other preventive measures such as configuring an uptime alert in Cloud Monitoring to ensure timely detection of outages and reduce their impact on the application."
      },
      {
        "date": "2023-03-09T13:52:00.000Z",
        "voteCount": 4,
        "content": "Configure liveness and readiness probes in the Pod specification.\n\nThis will help to prevent outages by ensuring that only healthy Pods are serving traffic. The liveness probe will check that the Pod is running and responding to requests. The readiness probe will check that the Pod is ready to serve traffic, such as by checking that the application is installed and configured."
      },
      {
        "date": "2022-12-26T06:12:00.000Z",
        "voteCount": 3,
        "content": "Liveness and readiness probes are used to determine the health of a Pod. Liveness probes are used to determine whether a Pod is running, and readiness probes are used to determine whether a Pod is able to receive traffic.\n\nBy configuring liveness and readiness probes in the Pod specification, you can help to prevent outages when releasing new versions of the application via a rolling deployment. If a Pod fails a liveness or readiness probe, it will be restarted, which can help to prevent issues caused by misconfigured parameters or other problems.\nThe correct answer is A: Configure liveness and readiness probes in the Pod specification."
      },
      {
        "date": "2022-12-26T06:12:00.000Z",
        "voteCount": 2,
        "content": "Option B: Configuring health checks on the managed instance group is not relevant in this scenario, as the application is running in a GKE cluster, not on a managed instance group.\n\nOption C: Creating a Scheduled Task to check whether the application is available may help to detect outages, but it will not prevent them from occurring. To prevent outages, you should focus on identifying and addressing the root cause of the problem.\n\nOption D: Configuring an uptime alert in Cloud Monitoring may help to detect outages, but it will not prevent them from occurring. To prevent outages, you should focus on identifying and addressing the root cause of the problem."
      },
      {
        "date": "2022-12-16T04:32:00.000Z",
        "voteCount": 1,
        "content": "A  Is the Correct Answer"
      },
      {
        "date": "2022-11-21T02:33:00.000Z",
        "voteCount": 1,
        "content": "A is ok"
      },
      {
        "date": "2022-10-15T13:34:00.000Z",
        "voteCount": 1,
        "content": "A is best answer"
      },
      {
        "date": "2022-09-14T08:11:00.000Z",
        "voteCount": 2,
        "content": "A is the answer.\n\nKubernetes Health Checks with Readiness and Liveness Probes\nhttps://www.youtube.com/watch?v=mxEvAPQRwhw"
      },
      {
        "date": "2022-09-04T02:22:00.000Z",
        "voteCount": 1,
        "content": "B is out since MIGs relate to compute engine. D and C are both not preventive measures."
      },
      {
        "date": "2022-09-03T03:42:00.000Z",
        "voteCount": 2,
        "content": "Option A is correct. Since it is regarding GKE and the application deployed in GKE cluster. Therefore, managed instance group does not have anything to do.\nSo, right answer is:\nA. Configure liveness and readiness probes in the Pod specification."
      },
      {
        "date": "2022-09-03T00:58:00.000Z",
        "voteCount": 4,
        "content": "A. \nThere are no MIGs in GKE. Only thing that makes sense is to have good readiness probes"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 186,
    "url": "https://www.examtopics.com/discussions/google/view/79736-exam-professional-cloud-architect-topic-1-question-186/",
    "body": "Your company uses Google Kubernetes Engine (GKE) as a platform for all workloads. Your company has a single large GKE cluster that contains batch, stateful, and stateless workloads. The GKE cluster is configured with a single node pool with 200 nodes. Your company needs to reduce the cost of this cluster but does not want to compromise availability. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a second GKE cluster for the batch workloads only. Allocate the 200 original nodes across both clusters.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure CPU and memory limits on the namespaces in the cluster. Configure all Pods to have a CPU and memory limits.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a HorizontalPodAutoscaler for all stateless workloads and for all compatible stateful workloads. Configure the cluster to use node auto scaling.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the node pool to use preemptible VMs."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 30,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-09-04T04:51:00.000Z",
        "voteCount": 13,
        "content": "A: Is not necessary because you can have multiple node pools with different configurations.\nB: Optimizes resource usage of CPU/memory in your existing node pool but does not necessarily improve cost - still an option that should be considered.\nC: This looks really good. Autoscaling workloads and the node pools makes your whole infrastructure more elastic and gives you the option to rely on the same node pool.\nD: This might not be a good option for every type of workload. Batch and stateless workloads can often handle this quite well, but stateful workloads are not well-suited for operation on preemptible VMs.\n\nSince only one answer is accepted, I'll choose C."
      },
      {
        "date": "2022-09-05T21:15:00.000Z",
        "voteCount": 8,
        "content": "C is the correct answer as it doesn't involve major changes to the current Kubernetes configuration"
      },
      {
        "date": "2024-10-16T05:14:00.000Z",
        "voteCount": 1,
        "content": "It uses Google features"
      },
      {
        "date": "2024-04-30T02:41:00.000Z",
        "voteCount": 1,
        "content": "Vote for C. In B limits could compromise availability."
      },
      {
        "date": "2023-03-01T06:47:00.000Z",
        "voteCount": 1,
        "content": "Answer C. Use HorizontalPodAutoscaler."
      },
      {
        "date": "2023-02-02T20:12:00.000Z",
        "voteCount": 1,
        "content": "HorizontalPodAutoscaler is the way"
      },
      {
        "date": "2023-01-15T10:40:00.000Z",
        "voteCount": 1,
        "content": "c is good"
      },
      {
        "date": "2022-12-26T06:37:00.000Z",
        "voteCount": 4,
        "content": "The correct answer is C: Configure a HorizontalPodAutoscaler for all stateless workloads and for all compatible stateful workloads. Configure the cluster to use node auto scaling.\n\nOne way to reduce the cost of a Google Kubernetes Engine (GKE) cluster without compromising availability is to use horizontal pod autoscalers (HPA) and node auto scaling.\n\nHPA allows you to automatically scale the number of Pods in a deployment based on the resource usage of the Pods. By configuring HPA for stateless workloads and for compatible stateful workloads, you can ensure that the number of Pods is automatically adjusted based on the actual resource usage, which can help to reduce costs.\n\nNode auto scaling allows you to automatically add or remove nodes from the node pool based on the resource usage of the cluster. By configuring node auto scaling, you can ensure that the cluster has the minimum number of nodes needed to meet the resource requirements of the workloads, which can also help to reduce costs."
      },
      {
        "date": "2022-12-26T06:37:00.000Z",
        "voteCount": 2,
        "content": "A: Creating a second GKE cluster for the batch workloads only and allocating the 200 original nodes across both clusters would not necessarily help to reduce costs, as the total number of nodes in the clusters would remain the same.\n\nB: Configuring CPU and memory limits on the namespaces in the cluster and configuring all Pods to have CPU and memory limits may help to reduce costs, but it is not sufficient on its own. You should also use HPA and node auto scaling to ensure that the cluster is properly sized based on the actual resource usage.\n\nD: Changing the node pool to use preemptible VMs may help to reduce costs, but it is not sufficient on its own. Preemptible VMs can be terminated at any time, which may not be suitable for all workloads. You should also use HPA and node auto scaling to ensure that the cluster is properly sized based on the actual resource usage."
      },
      {
        "date": "2022-12-20T07:50:00.000Z",
        "voteCount": 1,
        "content": "C is the correct answer as it doesn't involve major changes to the current Kubernetes configuration"
      },
      {
        "date": "2022-11-21T02:37:00.000Z",
        "voteCount": 1,
        "content": "C is ok"
      },
      {
        "date": "2022-10-15T13:37:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2022-09-05T15:28:00.000Z",
        "voteCount": 1,
        "content": "C is correc"
      },
      {
        "date": "2022-09-05T13:16:00.000Z",
        "voteCount": 2,
        "content": "Configure a HorizontalPodAutoscaler for all stateless workloads and for all compatible stateful workloads. Configure the cluster to use node auto scaling"
      },
      {
        "date": "2022-09-03T03:50:00.000Z",
        "voteCount": 3,
        "content": "Option C is correct. Since, the company does not want to compromise availability of the application so, HPA is suitable option for autoscaling pods. Keeping the cost optimization in mind, nodes of the GKE cluster also needs to be autoscaled. Therefore, the correct option is, \nC. Configure a HorizontalPodAutoscaler for all stateless workloads and for all compatible stateful workloads. Configure the cluster to use node auto scaling."
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 187,
    "url": "https://www.examtopics.com/discussions/google/view/80112-exam-professional-cloud-architect-topic-1-question-187/",
    "body": "Your company has a Google Cloud project that uses BigQuery for data warehousing on a pay-per-use basis. You want to monitor queries in real time to discover the most costly queries and which users spend the most. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. In the BigQuery dataset that contains all the tables to be queried, add a label for each user that can launch a query. 2. Open the Billing page of the project. 3. Select Reports. 4. Select BigQuery as the product and filter by the user you want to check.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a Cloud Logging sink to export BigQuery data access logs to BigQuery. 2. Perform a BigQuery query on the generated table to extract the information you need.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a Cloud Logging sink to export BigQuery data access logs to Cloud Storage. 2. Develop a Dataflow pipeline to compute the cost of queries split by users.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Activate billing export into BigQuery. 2. Perform a BigQuery query on the billing table to extract the information you need."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 43,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 12,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-09-07T02:02:00.000Z",
        "voteCount": 13,
        "content": "I choose B because of \"real-time\". Otherwise, D seems to be the most relevant and flexible."
      },
      {
        "date": "2023-02-03T15:53:00.000Z",
        "voteCount": 9,
        "content": "D also can be continuous https://cloud.google.com/billing/docs/how-to/export-data-bigquery#setup. I think D is the right answer."
      },
      {
        "date": "2022-10-20T06:11:00.000Z",
        "voteCount": 8,
        "content": "B is the correct answer https://cloud.google.com/blog/products/data-analytics/taking-a-practical-approach-to-bigquery-cost-monitoring\nA is incorrect as there is not billing page for a project, its billing account that handles all org billing."
      },
      {
        "date": "2023-05-16T13:01:00.000Z",
        "voteCount": 1,
        "content": "\"details about the query that was executed, like the SQL code, the job ID and, most important, the user who executed the query and the amount of data that was processed. With that information, you can compute the total cost of the query using a simple multiplication equation: cost per TB processed * numbers of TB processed\" means it will be an estimation, not the real numbers."
      },
      {
        "date": "2023-08-24T17:40:00.000Z",
        "voteCount": 1,
        "content": "That blog page says it shows DAILY results, not real time..."
      },
      {
        "date": "2024-08-27T10:21:00.000Z",
        "voteCount": 1,
        "content": "Google recommends to export cloud billing data to bigquery to control cost in real-time: https://cloud.google.com/billing/docs/how-to/export-data-bigquery"
      },
      {
        "date": "2024-04-10T10:34:00.000Z",
        "voteCount": 1,
        "content": "B is correct!"
      },
      {
        "date": "2024-03-04T14:07:00.000Z",
        "voteCount": 1,
        "content": "B is the correct option. Why not A:  While the Billing page offers reports with user-level cost breakdowns, it doesn't provide real-time information or detailed query data. Why not D: Billing export can provide cost data in BigQuery, but it doesn't capture details about individual queries or users, making it insufficient for the specific needs of identifying costly queries and high-spending users."
      },
      {
        "date": "2024-02-04T02:49:00.000Z",
        "voteCount": 1,
        "content": "b -&gt; real time"
      },
      {
        "date": "2024-01-24T05:19:00.000Z",
        "voteCount": 1,
        "content": "I tend to agree with the GPT4 summary:\nIn summary, \n\nOption B is more focused on analyzing specific BigQuery usage patterns and costs down to the level of individual queries and users. It's better for real-time analysis of query activities. \n\nOption D, on the other hand, provides a broader overview of all costs associated with the Google Cloud project, which is beneficial for general cost management but less so for in-depth analysis of specific BigQuery queries and user activities. \n\nFor the specific need to discover the most costly queries and which users are responsible, Option B is more targeted and appropriate"
      },
      {
        "date": "2024-01-09T02:42:00.000Z",
        "voteCount": 1,
        "content": "I took PCA exam today and  I passed PCA exam. In my test, I choose D on this question."
      },
      {
        "date": "2024-01-24T05:14:00.000Z",
        "voteCount": 2,
        "content": "My congrats! Did the exam results show you that you answered correctly to this particular  question?"
      },
      {
        "date": "2023-11-27T19:11:00.000Z",
        "voteCount": 1,
        "content": "It can't be better explained https://cloud.google.com/blog/products/data-analytics/taking-a-practical-approach-to-bigquery-cost-monitoring"
      },
      {
        "date": "2023-09-29T00:29:00.000Z",
        "voteCount": 1,
        "content": "B: because of\" https://cloud.google.com/blog/products/data-analytics/taking-a-practical-approach-to-bigquery-cost-monitoring\nAnd because https://cloud.google.com/billing/docs/how-to/export-data-bigquery#example-queries. is not mentioning anything about query per user.\nB"
      },
      {
        "date": "2023-09-26T08:10:00.000Z",
        "voteCount": 2,
        "content": "B don't provide the cost"
      },
      {
        "date": "2023-09-29T00:36:00.000Z",
        "voteCount": 2,
        "content": "Yes it can:\nhttps://cloud.google.com/blog/products/data-analytics/taking-a-practical-approach-to-bigquery-cost-monitoring"
      },
      {
        "date": "2023-09-02T19:32:00.000Z",
        "voteCount": 2,
        "content": "The answer is D. 1. Activate billing export into BigQuery. 2. Perform a BigQuery query on the billing table to extract the information you need.\n\nExplanation:\n\nA. This option is not correct because adding a label for each user in the BigQuery dataset will not allow you to monitor the cost of queries or find out which users spend the most.\n\nB. This option is not correct because BigQuery data access logs do not include billing information or query costs.\n\nC. This option is not correct because BigQuery data access logs stored in Cloud Storage do not include billing information or query costs, and developing a Dataflow pipeline to compute the cost of queries would be unnecessarily complex.\n\nD. This is the correct option because activating billing export into BigQuery will allow you to query the billing data in real-time to discover the most costly queries and which users spend the most."
      },
      {
        "date": "2023-08-05T04:20:00.000Z",
        "voteCount": 2,
        "content": "D is the easiest"
      },
      {
        "date": "2023-07-17T11:38:00.000Z",
        "voteCount": 1,
        "content": "1. Activate billing export into BigQuery. \n2. Perform a BigQuery query on the billing table to extract the information you need."
      },
      {
        "date": "2023-07-06T08:41:00.000Z",
        "voteCount": 1,
        "content": "both B and D do not meet the exact requirements while B does not give the cost D does not provide user level details. Answer A seems to better suited though complicated"
      },
      {
        "date": "2023-06-19T23:13:00.000Z",
        "voteCount": 1,
        "content": "For me, most simple and also available in real time is D"
      },
      {
        "date": "2023-04-06T00:24:00.000Z",
        "voteCount": 3,
        "content": "Option D might seem like a reasonable choice, but it doesn't provide real-time monitoring of queries, which is the requirement mentioned in the question.\n\nActivating billing export to BigQuery provides detailed billing information for your Google Cloud project. However, this method doesn't provide real-time insights into query costs and user expenditures, as billing data is typically updated once per day.\n\nOn the other hand, option B allows you to monitor queries in real-time by exporting BigQuery data access logs directly to another BigQuery table, enabling you to analyze the most costly queries and user expenses as they happen."
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 188,
    "url": "https://www.examtopics.com/discussions/google/view/80000-exam-professional-cloud-architect-topic-1-question-188/",
    "body": "Your company and one of its partners each have a Google Cloud project in separate organizations. Your company's project (prj-a) runs in Virtual Private Cloud<br>(vpc-a). The partner's project (prj-b) runs in vpc-b. There are two instances running on vpc-a and one instance running on vpc-b. Subnets defined in both VPCs are not overlapping. You need to ensure that all instances communicate with each other via internal IPs, minimizing latency and maximizing throughput. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up a network peering between vpc-a and vpc-b.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up a VPN between vpc-a and vpc-b using Cloud VPN.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure IAP TCP forwarding on the instance in vpc-b, and then launch the following gcloud command from one of the instances in vpc-a gcloud: gcloud compute start-iap-tunnel INSTANCE_NAME_IN_VPC_8 22 \\ --local-host-port=localhost:22",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create an additional instance in vpc-a. 2. Create an additional instance in vpc-b. 3. Install OpenVPN in newly created instances. 4. Configure a VPN tunnel between vpc-a and vpc-b with the help of OpenVPN."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 27,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-09-12T09:14:00.000Z",
        "voteCount": 11,
        "content": "definitely A.\n\nhttps://cloud.google.com/vpc/docs/vpc-peering\nGoogle Cloud VPC Network Peering allows internal IP address connectivity across two Virtual Private Cloud (VPC) networks regardless of whether they belong to the same project or the same organization."
      },
      {
        "date": "2024-04-30T03:15:00.000Z",
        "voteCount": 1,
        "content": "VPC peering: https://cloud.google.com/vpc/docs/vpc-peering\nPeered VPC networks can be in the same project, different projects of the same organization, or different projects of different organizations.\n\nIPv4 subnet routes in peered VPC networks can't overlap"
      },
      {
        "date": "2024-01-20T03:44:00.000Z",
        "voteCount": 1,
        "content": "A is ok!"
      },
      {
        "date": "2023-12-30T04:06:00.000Z",
        "voteCount": 2,
        "content": "Since it's mentioned that the subnets do not overlap, A is the best way to go. \nIf the subnets overlapped, you would go with B. \nhttps://cloud.google.com/vpc/docs/vpc-peering#interaction-subnet-subnet"
      },
      {
        "date": "2022-11-21T02:15:00.000Z",
        "voteCount": 2,
        "content": "A is ok"
      },
      {
        "date": "2022-11-18T10:39:00.000Z",
        "voteCount": 2,
        "content": "Clearly A as the IPs do not overlap"
      },
      {
        "date": "2022-10-20T06:23:00.000Z",
        "voteCount": 2,
        "content": "A is the correct answer as per https://cloud.google.com/vpc/docs/vpc-peering"
      },
      {
        "date": "2022-10-15T13:41:00.000Z",
        "voteCount": 2,
        "content": "A is right answer"
      },
      {
        "date": "2022-09-09T10:09:00.000Z",
        "voteCount": 2,
        "content": "Clearly A"
      },
      {
        "date": "2022-09-05T15:45:00.000Z",
        "voteCount": 1,
        "content": "A - VPC peering should be good"
      },
      {
        "date": "2022-09-05T13:21:00.000Z",
        "voteCount": 1,
        "content": "It should be A"
      },
      {
        "date": "2022-09-05T05:12:00.000Z",
        "voteCount": 1,
        "content": "network peering is fine"
      },
      {
        "date": "2022-09-04T11:11:00.000Z",
        "voteCount": 1,
        "content": "It should be A"
      },
      {
        "date": "2022-09-04T02:35:00.000Z",
        "voteCount": 1,
        "content": "peering is better as both orgs are in GCP"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 189,
    "url": "https://www.examtopics.com/discussions/google/view/80304-exam-professional-cloud-architect-topic-1-question-189/",
    "body": "You want to store critical business information in Cloud Storage buckets. The information is regularly changed, but previous versions need to be referenced on a regular basis. You want to ensure that there is a record of all changes to any information in these buckets. You want to ensure that accidental edits or deletions can be easily rolled back. Which feature should you enable?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBucket Lock",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tObject Versioning\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tObject change notification",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tObject Lifecycle Management"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-03-11T16:51:00.000Z",
        "voteCount": 5,
        "content": "Object Versioning is a feature that allows you to store multiple versions of an object in Cloud Storage. Hence, answer should be B"
      },
      {
        "date": "2022-11-21T02:24:00.000Z",
        "voteCount": 1,
        "content": "B is ok"
      },
      {
        "date": "2022-10-29T13:20:00.000Z",
        "voteCount": 1,
        "content": "Most definitely B"
      },
      {
        "date": "2022-10-20T06:28:00.000Z",
        "voteCount": 1,
        "content": "B is the correct answer as per https://cloud.google.com/storage/docs/object-versioning"
      },
      {
        "date": "2022-10-15T13:43:00.000Z",
        "voteCount": 2,
        "content": "B - Object versioning"
      },
      {
        "date": "2022-09-12T13:05:00.000Z",
        "voteCount": 4,
        "content": "Object versioning, super important to be able to rollback in case of any deletion."
      },
      {
        "date": "2022-09-05T05:12:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/storage/docs/object-versioning"
      },
      {
        "date": "2022-09-10T00:44:00.000Z",
        "voteCount": 7,
        "content": "I too got this question in 10-09-22 exam with similar option and result is pass"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 190,
    "url": "https://www.examtopics.com/discussions/google/view/80040-exam-professional-cloud-architect-topic-1-question-190/",
    "body": "You have a Compute Engine application that you want to autoscale when total memory usage exceeds 80%. You have installed the Cloud Monitoring agent and configured the autoscaling policy as follows:<br>\u2711 Metric identifier: agent.googleapis.com/memory/percent_used<br>\u2711 Filter: metric.label.state = 'used'<br>\u2711 Target utilization level: 80<br>\u2711 Target type: GAUGE<br>You observe that the application does not scale under high load. You want to resolve this. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the Target type to DELTA_PER_MINUTE.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the Metric identifier to agent.googleapis.com/memory/bytes_used.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the filter to metric.label.state = 'used' AND metric.label.state = 'buffered' AND metric.label.state = 'cached' AND metric.label.state = 'slab'.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the filter to metric.label.state = 'free' and the Target utilization to 20."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 68,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 29,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 15,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-10-20T07:30:00.000Z",
        "voteCount": 33,
        "content": "C is correct answer:\nA. Change the Target type to DELTA_PER_MINUTE. (in this case the utlization tagret need to be in minutes which is not the case its percentage % and not time based.\nB. Change the Metric identifier to agent.googleapis.com/memory/bytes_used. (not applicable) \nC. Change the filter to metric.label.state = 'used' AND metric.label.state = 'buffered' AND metric.label.state = 'cached' AND metric.label.state = 'slab'. (this gives total memory used)\nD. Change the filter to metric.label.state = 'free' and the Target utilization to 20. (you would still need to change the the percent_used to percent_free)\n\nhttps://stackoverflow.com/questions/69267526/what-is-disk-data-cached-in-the-memory-usage-chart-metrics-of-gcp-compute-in\n\nhttps://cloud.google.com/compute/docs/autoscaler/scaling-cloud-monitoring-metrics"
      },
      {
        "date": "2022-09-05T05:13:00.000Z",
        "voteCount": 14,
        "content": "TARGET_TYPE: the value type for the metric.\ngauge: the autoscaler computes the average value of the data collected in the last couple of minutes and compares that to the utilization target.\ndelta-per-minute: the autoscaler calculates the average rate of growth per minute and compares that to the utilization target.\ndelta-per-second: the autoscaler calculates the average rate of growth per second and compares that to the utilization target. For accurate comparisons, if you set the utilization target in seconds, use delta-per-second as the target type. Likewise, use delta-per-minute for a utilization target in minutes."
      },
      {
        "date": "2024-10-16T04:57:00.000Z",
        "voteCount": 1,
        "content": "There should always be a time reference when having a metric"
      },
      {
        "date": "2024-07-30T00:29:00.000Z",
        "voteCount": 1,
        "content": "exam done today, This question has been changed in the exam and the filter in the text of the question is actually \"metric.label.state = 'used' AND metric.label.state = 'buffered' AND metric.label.state = 'cached' AND metric.label.state = 'slab'.\""
      },
      {
        "date": "2024-04-30T03:46:00.000Z",
        "voteCount": 3,
        "content": "...but....propbably thwre is a mistake in the question. I assume that metric.label.state can't have many values in the same time so instead AND operator OR should be used ??????"
      },
      {
        "date": "2024-05-25T02:15:00.000Z",
        "voteCount": 1,
        "content": "Have same thought as you. Misunderstood this point thus selected wrong answer. All answers seem wrong..."
      },
      {
        "date": "2024-04-16T17:44:00.000Z",
        "voteCount": 2,
        "content": "To configure autoscaling based on the percent of used memory, specify the percent_used metric provided by the memory Ops Agent metrics. You should filter the metric by state to use only the used memory state. If you do not specify the filter, then the autoscaler takes the sum of memory usage by all memory states labeled as buffered, cached, free, slab, and used."
      },
      {
        "date": "2024-04-11T05:53:00.000Z",
        "voteCount": 1,
        "content": "I'm preparing for a test and see that questions from 115 onwards are considered valid. Can anyone who's taken the test offer any insights or advice? Thank you!"
      },
      {
        "date": "2024-04-10T10:14:00.000Z",
        "voteCount": 1,
        "content": "C is the right option ( use used and gauge as options as in the guide listed here\nhttps://cloud.google.com/compute/docs/autoscaler/scaling-cloud-monitoring-metrics#autoscale_based_on_memory_usage"
      },
      {
        "date": "2024-03-16T07:38:00.000Z",
        "voteCount": 2,
        "content": "In the real exam \n\"Filter: metric.label.state = 'used' AND metric.label.state = 'buffered' AND metric.label.state = 'cached' AND metric.label.state = 'slab'\" is in the QUESTION.\n\nThe answer should be:\nC. Change the filter to metric.label.state = 'used'"
      },
      {
        "date": "2024-03-03T14:01:00.000Z",
        "voteCount": 2,
        "content": "C is the correct approach. The current filter only considers memory in the \"used\" state. However, the operating system also uses memory for caching, buffering, and other purposes. By modifying the filter we ensure the autoscaling policy considers all memory states, providing a more accurate representation of total memory usage."
      },
      {
        "date": "2024-03-01T12:05:00.000Z",
        "voteCount": 1,
        "content": "This question came in recent exam and default state already have all metric.label.state . Went with DELTE per minute option A"
      },
      {
        "date": "2024-02-17T01:05:00.000Z",
        "voteCount": 2,
        "content": "Question has a mistake\n\"Filter: metric.label.state = 'used'\" is in option C\n\n\"Change the filter to metric.label.state = 'used' AND metric.label.state = 'buffered' AND metric.label.state = 'cached' AND metric.label.state = 'slab'.\"\nis actually in the queston.\n\n\nhttps://cloud.google.com/compute/docs/autoscaler/scaling-cloud-monitoring-metrics#autoscale_based_on_memory_usage\nYou should use: Filter: metric.label.state = 'used'"
      },
      {
        "date": "2024-01-28T06:41:00.000Z",
        "voteCount": 4,
        "content": "The question in actual exam is reverse. The filter in the question is metric.label.state = 'used' AND metric.label.state = 'buffered' AND metric.label.state = 'cached' AND metric.label.state = 'slab'.\n\nand the option C is:\nFilter: metric.label.state = 'used'\n\nC is the correct answer in that case\nhttps://cloud.google.com/compute/docs/autoscaler/scaling-cloud-monitoring-metrics#autoscale_based_on_memory_usage"
      },
      {
        "date": "2023-12-28T20:23:00.000Z",
        "voteCount": 1,
        "content": "Actually this question is kinda weird\nWe can discard A &amp; B right away:\nA: If you change to DELTA_PER_MINUTE it'll calculate the difference in memory used from one minute to the other, and if that difference is bigger* than 80%, it'll trigger. Not what we want.\nB: If we change the metric to bytes_used, we must change the value of the gauge too. Not an option.\nNow comes the messy part.\nFollowing what is said in this page: https://cloud.google.com/monitoring/api/metrics_opsagent#agent-memory\nThe metric.label.state should be ONE of these: [buffered, cached, free, slab, used]\nAnd it also states that: 'Summing the values of all states yields the total memory on the machine'. So, using a simple equation, if we remove the 'free' one from them, then that would give us the total memory that is being used. But remember, it said ONE of them, so that would discard it."
      },
      {
        "date": "2023-12-28T20:24:00.000Z",
        "voteCount": 1,
        "content": "Now D, for me is the closest one to being true. If you ask only for the free percentage_used and change the target to 20, you should be done.\nBut a question I read here was very interesting, and connects with the * used above...\nHow does it know that it should scale when the metric is above or below? We don't set that filter. We can hope that the autoscaling is smart enough to know that when we use 'used' we mean more than and when using 'free' we mean less than.\nI couldn't find any information about that, so if anyone gets any additional info, please share it."
      },
      {
        "date": "2023-12-15T06:20:00.000Z",
        "voteCount": 1,
        "content": "C is the correct one"
      },
      {
        "date": "2023-10-31T19:33:00.000Z",
        "voteCount": 7,
        "content": "In the real exam, questions metric label state was mentioned as \n\"metric.label.state = 'used' AND metric.label.state = 'buffered' AND metric.label.state = 'cached' AND metric.label.state = 'slab'\"\nand \"metric.label.state = 'used'\" was given in answer C."
      },
      {
        "date": "2024-01-24T05:39:00.000Z",
        "voteCount": 1,
        "content": "How does it work for autoscaling? \n\"AND\" is considered as \"OR\"? \nWhen you already have metric.label.state = 'used' in the problem statement and have an issue, then you trying to add more conditions and hope that this will solve the problem?!?\nStrange...."
      },
      {
        "date": "2023-10-29T06:30:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/monitoring/api/metrics_agent#agent-memory\npercent_used\nCurrent percentage of memory used by memory state. Summing percentages over all states yields 100 percent. Sampled every 60 seconds.\nstate: One of [buffered, cached, free, slab, used]."
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 191,
    "url": "https://www.examtopics.com/discussions/google/view/80419-exam-professional-cloud-architect-topic-1-question-191/",
    "body": "You are deploying an application to Google Cloud. The application is part of a system. The application in Google Cloud must communicate over a private network with applications in a non-Google Cloud environment. The expected average throughput is 200 kbps. The business requires:<br>\u2711 as close to 100% system availability as possible<br>\u2711 cost optimization<br>You need to design the connectivity between the locations to meet the business requirements. What should you provision?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAn HA Cloud VPN gateway connected with two tunnels to an on-premises VPN gateway\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTwo Classic Cloud VPN gateways connected to two on-premises VPN gateways Configure each Classic Cloud VPN gateway to have two tunnels, each connected to different on-premises VPN gateways",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTwo HA Cloud VPN gateways connected to two on-premises VPN gateways Configure each HA Cloud VPN gateway to have two tunnels, each connected to different on-premises VPN gateways",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA single Cloud VPN gateway connected to an on-premises VPN gateway"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 29,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 4,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-10-21T01:43:00.000Z",
        "voteCount": 6,
        "content": "A is true only if the on-prem (peer) gateway has two separate external P addresses. The HA VPN gateway uses two tunnels, one tunnel to each external IP address on the peer device as described in https://cloud.google.com/network-connectivity/docs/vpn/concepts/topologies#configurations_that_support_9999_availability\n\nC is a complete solution that provides full redundancy of the on-prem gateway. This is probably more expensive and having two HA VPN Gateways is an unusual configuration as the online documentation only describes using one HA VPN Gateway\n\nA appears to be correct with assumptions...!"
      },
      {
        "date": "2024-04-22T04:34:00.000Z",
        "voteCount": 1,
        "content": "D is correct. \nA. Single HA Cloud VPN with Two Tunnels: This offers redundancy, but a single point of failure exists at the Cloud VPN gateway itself. A second HA Cloud VPN gateway provides additional fault tolerance.\nB. Two Classic Cloud VPNs with Multiple Tunnels: Classic Cloud VPNs are being phased out and might be less cost-effective than HA Cloud VPNs. Additionally, managing multiple Classic Cloud VPN gateways can be more complex.\nD. Single Cloud VPN Gateway: This offers a single point of failure and wouldn't achieve the desired level of high availability."
      },
      {
        "date": "2024-04-22T04:36:00.000Z",
        "voteCount": 1,
        "content": "Sorry, I meant C is correct."
      },
      {
        "date": "2023-10-25T12:03:00.000Z",
        "voteCount": 1,
        "content": "why not B? See https://cloud.google.com/network-connectivity/docs/vpn/concepts/classic-topologies#option-3\n\nDon't you need redundant VPN gateways on the other side (on-prem) to reach close to 100% availability? A) has a single VPN gateway on-prem."
      },
      {
        "date": "2024-04-30T05:01:00.000Z",
        "voteCount": 1,
        "content": "Because B i about increasing throughput and load balancing not about availability ??? Expected avarage throughput is \"only\" 200 kbps.\nA) has a single VPN...right but I think the clue is in \"You are deploying an application to Google Cloud. The application is part of a system.\"....so probably you have no leverage on on-premises solutions ?????? ....but in fact I'm not sure wchich answer is right :("
      },
      {
        "date": "2023-06-17T02:24:00.000Z",
        "voteCount": 3,
        "content": "A: looks an exact match for the requirements, 99.99% availability\n\nB: Is a manual implementation of HA, not optimizing cost\n\nC: Is behoynd HA, no longer optimizing cost.\n\nD: Does not provide close to 100% as possible"
      },
      {
        "date": "2023-03-06T07:13:00.000Z",
        "voteCount": 3,
        "content": "Use HA (High Availability) VPN as required in the question. A is better aswer."
      },
      {
        "date": "2022-12-21T07:36:00.000Z",
        "voteCount": 2,
        "content": "both A and C are possible solutions but A is cheaper."
      },
      {
        "date": "2022-11-21T02:48:00.000Z",
        "voteCount": 2,
        "content": "A is ok"
      },
      {
        "date": "2022-11-19T08:28:00.000Z",
        "voteCount": 2,
        "content": "Correct Answer is D,\nYou cannot migrate an existing Cloud VPN tunnel or tunnels on a Classic VPN gateway to an HA VPN gateway. Instead, you need to create new tunnels and delete the old ones.\nhttps://cloud.google.com/network-connectivity/docs/vpn/how-to/moving-to-ha-vpn#general_guidelines"
      },
      {
        "date": "2024-04-30T05:04:00.000Z",
        "voteCount": 1,
        "content": "You cannot migrate.....but....\"You are deploying an application to Google Cloud\"....nothing to migrate...."
      },
      {
        "date": "2022-10-20T08:16:00.000Z",
        "voteCount": 3,
        "content": "A satisfty both requriements"
      },
      {
        "date": "2022-10-20T08:18:00.000Z",
        "voteCount": 1,
        "content": "Satisfy both requirements for close to 100% availability and cost containment"
      },
      {
        "date": "2022-10-15T13:49:00.000Z",
        "voteCount": 1,
        "content": "A is right"
      },
      {
        "date": "2022-09-18T07:25:00.000Z",
        "voteCount": 2,
        "content": "best explained in https://jayendrapatil.com/tag/classic-vpn-vs-ha-vpn/\nHA VPN provides an SLA of 99.99% service availability, when configured with two interfaces and two external IP addresses."
      },
      {
        "date": "2022-09-17T04:13:00.000Z",
        "voteCount": 1,
        "content": "To meet the 99.99% SLA on the Google Cloud side, there must be a tunnel from each of the two interfaces on the HA VPN gateway to the corresponding interfaces on the peer gateway."
      },
      {
        "date": "2022-09-12T13:13:00.000Z",
        "voteCount": 4,
        "content": "A can provide 99.99% availability as well, and no need for C which will be more expensive.\n\nhttps://cloud.google.com/network-connectivity/docs/vpn/concepts/topologies#1-peer-1-address"
      },
      {
        "date": "2022-09-12T09:05:00.000Z",
        "voteCount": 3,
        "content": "A can provide 99.99% availability as well, and no need for C which will be more expensive.\n\nhttps://cloud.google.com/network-connectivity/docs/vpn/concepts/topologies#1-peer-1-address"
      },
      {
        "date": "2022-09-07T06:27:00.000Z",
        "voteCount": 1,
        "content": "C  is full mash. real HR with  redundancy on the on premises site"
      },
      {
        "date": "2022-09-07T02:21:00.000Z",
        "voteCount": 3,
        "content": "I choose A. Gives you 99.99% availability, and is certainly cheaper than B, C and is more reliable than D.\n\nhttps://cloud.google.com/network-connectivity/docs/vpn/concepts/topologies"
      },
      {
        "date": "2022-09-05T13:38:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/static/network-connectivity/docs/vpn/images/ha-vpn-gcp-to-on-prem-2-a.svg"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 192,
    "url": "https://www.examtopics.com/discussions/google/view/121240-exam-professional-cloud-architect-topic-1-question-192/",
    "body": "Your company has an application running on App Engine that allows users to upload music files and share them with other people. You want to allow users to upload files directly into Cloud Storage from their browser session. The payload should not be passed through the backend. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Set a CORS configuration in the target Cloud Storage bucket where the base URL of the App Engine application is an allowed origin.<br>2. Use the Cloud Storage Signed URL feature to generate a POST URL.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Set a CORS configuration in the target Cloud Storage bucket where the base URL of the App Engine application is an allowed origin.<br>2. Assign the Cloud Storage WRITER role to users who upload files.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Use the Cloud Storage Signed URL feature to generate a POST URL.<br>2. Use App Engine default credentials to sign requests against Cloud Storage.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Assign the Cloud Storage WRITER role to users who upload files.<br>2. Use App Engine default credentials to sign requests against Cloud Storage."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-09-23T07:34:00.000Z",
        "voteCount": 10,
        "content": "It should be A. Since it is stated that the payload should not passed from the backend and be send directly to the bucket, then a CORS configuration should be set to the bucket."
      },
      {
        "date": "2024-06-26T07:27:00.000Z",
        "voteCount": 3,
        "content": "A. 1. Set a CORS configuration in the target Cloud Storage bucket where the base URL of the App Engine application is an allowed origin. 2. Use the Cloud Storage Signed URL feature to generate a POST URL.\nHere's why this approach is most suitable:\n\t\u2022 CORS configuration: This allows cross-origin requests from your App Engine application to access the Cloud Storage bucket for uploads. Setting the App Engine base URL as an allowed origin ensures secure communication.\n\t\u2022 Cloud Storage Signed URL: This feature generates a temporary URL with specific permissions and expiration time. You can provide this signed URL to the user's browser for uploading files directly to Cloud Storage. The payload (music file) doesn't pass through your backend, reducing server load."
      },
      {
        "date": "2024-03-03T08:02:00.000Z",
        "voteCount": 1,
        "content": "I would go for A as per its definition and it works good that way"
      },
      {
        "date": "2024-02-10T05:46:00.000Z",
        "voteCount": 1,
        "content": "It's between A and C.\n\nBut if you select C then you have to justify the use of \"Use App Engine default credentials to sign requests against Cloud Storage. \" hence go with option A."
      },
      {
        "date": "2024-01-26T21:02:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/blog/products/storage-data-transfer/uploading-images-directly-to-cloud-storage-by-using-signed-url"
      },
      {
        "date": "2024-01-09T04:32:00.000Z",
        "voteCount": 1,
        "content": "I'm also not sure either A or C. But in my PCA exam today, I choose C. And I have passed."
      },
      {
        "date": "2023-12-10T19:37:00.000Z",
        "voteCount": 2,
        "content": "C is the Answer. https://cloud.google.com/storage/docs/cross-origin"
      },
      {
        "date": "2023-12-13T19:22:00.000Z",
        "voteCount": 2,
        "content": "if its cross-origin. Then why C is answer? Shouldn't it be A"
      },
      {
        "date": "2023-10-04T04:54:00.000Z",
        "voteCount": 3,
        "content": "Signed URL is for TIme-Based access. This needs access all the time."
      },
      {
        "date": "2023-09-28T23:39:00.000Z",
        "voteCount": 3,
        "content": "A:\nhttps://cloud.google.com/storage/docs/cross-origin#server-side-support\n\"Cloud Storage supports this specification by allowing you to configure your buckets to support CORS. Continuing the above example, you can configure the example.storage.googleapis.com bucket so that a browser can share its resources with scripts from example.appspot.com.\""
      },
      {
        "date": "2023-09-24T09:55:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is A"
      },
      {
        "date": "2023-09-23T00:10:00.000Z",
        "voteCount": 3,
        "content": "Not sure is A or C. I will go with C.\n\nhttps://cloud.google.com/blog/products/storage-data-transfer/uploading-images-directly-to-cloud-storage-by-using-signed-url"
      },
      {
        "date": "2023-10-24T01:40:00.000Z",
        "voteCount": 2,
        "content": "There is no any relationship between App engine application and cloud storage. \nYou need bind them."
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 193,
    "url": "https://www.examtopics.com/discussions/google/view/121322-exam-professional-cloud-architect-topic-1-question-193/",
    "body": "You are configuring the cloud network architecture for a newly created project in Google Cloud that will host applications in Compute Engine. Compute Engine virtual machine instances will be created in two different subnets (sub-a and sub-b) within a single region:<br>\u2022\tInstances in sub-a will have public IP addresses.<br>\u2022\tInstances in sub-b will have only private IP addresses.<br><br>To download updated packages, instances must connect to a public repository outside the boundaries of Google Cloud. You need to allow sub-b to access the external repository. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Private Google Access on sub-b.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Cloud NAT and select sub-b in the NAT mapping section.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a bastion host instance in sub-a to connect to instances in sub-b.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Identity-Aware Proxy for TCP forwarding for instances in sub-b."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-10T05:53:00.000Z",
        "voteCount": 1,
        "content": "https://www.youtube.com/watch?v=4uskhIk7LdM"
      },
      {
        "date": "2023-10-29T07:42:00.000Z",
        "voteCount": 3,
        "content": "IMHO\n\nA -&gt; It doesn't make sense, Public Google Access allows you to access Google APIs without an external IP, which doesnt solve the problem\nC -&gt; Bastion host is for the opposite purpose; accessing a machine administratively from the outside without an external IP, not a machine without an external IP accessing the outside.\nD -&gt; It doesn't make sense.\nB -&gt; It's the recommended solution for GCP"
      },
      {
        "date": "2023-10-26T06:16:00.000Z",
        "voteCount": 1,
        "content": "Answer is B. Cloud Nat is the right service to use when you want to connect to reach services on internet without exposing the vm with an external IP"
      },
      {
        "date": "2023-09-30T12:35:00.000Z",
        "voteCount": 1,
        "content": "I will Select C. As there will many Instances will require internet access to update the OS."
      },
      {
        "date": "2023-09-30T12:42:00.000Z",
        "voteCount": 1,
        "content": "Changed answer to B, Cloud NAT"
      },
      {
        "date": "2023-09-28T22:45:00.000Z",
        "voteCount": 1,
        "content": "nat is what you need for non-external vm can reach the internet\nB is the only 1"
      },
      {
        "date": "2023-09-27T01:34:00.000Z",
        "voteCount": 2,
        "content": "Cloud NAT allows the resources in a private subnet to access the internet\u2014for updates, patching, config management, and more\u2014in a controlled and efficient manner."
      },
      {
        "date": "2023-09-24T10:06:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is B you will need NAT to access repositories hosted on the public internet"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 194,
    "url": "https://www.examtopics.com/discussions/google/view/121314-exam-professional-cloud-architect-topic-1-question-194/",
    "body": "Your company is planning to migrate their Windows Server 2022 from their on-premises data center to Google Cloud. You need to bring the licenses that are currently in use in on-premises virtual machines into the target cloud environment. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create an image of the on-premises virtual machines and upload into Cloud Storage.<br>2. Import the image as a virtual disk on Compute Engine.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create standard instances on Compute Engine.<br>2. Select as the OS the same Microsoft Windows version that is currently in use in the on-premises environment.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create an image of the on-premises virtual machine.<br>2. Import the image as a virtual disk on Compute Engine.<br>3. Create a standard instance on Compute Engine, selecting as the OS the same Microsoft Windows version that is currently in use in the on-premises environment.<br>4. Attach a data disk that includes data that matches the created image.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create an image of the on-premises virtual machines.<br>2. Import the image as a virtual disk on Compute Engine using --os=windows-2022-dc-v<timestamp>.<br>3. Create a sole-tenancy instance on Compute Engine that uses the imported disk as a boot disk.\n\t\t\t\t\t\t\t\t\t\t</timestamp><span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 27,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 6,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-12-28T21:16:00.000Z",
        "voteCount": 16,
        "content": "Well, yes, you actually need a sole-tenant instance to install a windows server with your licence.\nA lot of links were pasted here, but after reading a lot of them, I reached this one who explicitly states:\n'To create a VM instance that uses the custom BYOL image, you must provision the VM instance on a sole-tenant node.'\nhttps://cloud.google.com/compute/docs/images/creating-custom-windows-byol-images#use_the_custom_image"
      },
      {
        "date": "2024-10-14T09:49:00.000Z",
        "voteCount": 1,
        "content": "No need to have a sole-tenancy instance"
      },
      {
        "date": "2024-03-02T23:15:00.000Z",
        "voteCount": 1,
        "content": "D is answer due to BYOL conditions of MS Mobility Licenses"
      },
      {
        "date": "2024-02-29T23:30:00.000Z",
        "voteCount": 1,
        "content": "A. is a valid and used strategy to migrate Windows Server virtual machines from an on-premises environment to Google Cloud Platform (GCP), especially when you want to carry over existing licenses using the Bring Your Own License (BYOL) approach."
      },
      {
        "date": "2024-02-10T06:03:00.000Z",
        "voteCount": 1,
        "content": "To create a VM instance that uses the custom BYOL image, you must provision the VM instance on a sole-tenant node."
      },
      {
        "date": "2024-01-20T11:24:00.000Z",
        "voteCount": 1,
        "content": "It shouod be c"
      },
      {
        "date": "2023-12-11T16:14:00.000Z",
        "voteCount": 4,
        "content": "Sole tenant purpose is to facilitate importing licenses BYOL\n\nSole-tenant nodes can help you meet dedicated hardware requirements for bring your own license (BYOL) scenarios that require per-core or per-processor licenses. When you use sole-tenant nodes, you have some visibility into the underlying hardware, which lets you track core and processor usage. To track this usage, Compute Engine reports the ID of the physical server on which a VM is scheduled. Then, by using Cloud Logging, you can view the historical server usage of a VM. To optimize the use of the host hardware, you can overcommit sole-tenant VM CPUs, share sole-tenant node groups and manually live migrate VMs."
      },
      {
        "date": "2023-12-04T19:02:00.000Z",
        "voteCount": 1,
        "content": "Answer is A: \nSteps:\n1 - Export an existing VM in .OVA format\n2 - Install and Authorize the gCloud SDK\n3 - Copy the .OVA file to a Google Storage Bucket\n4 - Import the .OVA file to Google Cloud from the bucket. \nhttps://www.youtube.com/watch?v=NG38am3Y8hM \ngcloud compute instances import gcpinstanename --os=windows-10-x64-byol"
      },
      {
        "date": "2023-12-03T07:36:00.000Z",
        "voteCount": 1,
        "content": "Eligible Products: License Mobility typically applies to Microsoft software such as SQL Server and other Microsoft applications, but it's important to note that Windows Server licenses are generally not eligible for License Mobility. So, D."
      },
      {
        "date": "2023-11-30T21:36:00.000Z",
        "voteCount": 2,
        "content": "Licensing scenarios such as licenses related to Linux BYOS with RHEL or SLES, as well as Microsoft applications don't require sole-tenant nodes. If you are considering bringing licenses from Microsoft applications such as SharePoint Server and SQL Server, use Microsoft License Mobility.\n\nhttps://cloud.google.com/compute/docs/nodes/bringing-your-own-licenses#importing_and_creating_an_image_from_an_offline_virtual_disk"
      },
      {
        "date": "2023-11-21T10:23:00.000Z",
        "voteCount": 1,
        "content": "either c or d but c seems right too"
      },
      {
        "date": "2023-11-14T10:00:00.000Z",
        "voteCount": 2,
        "content": "No need to have a sole-tenancy instance"
      },
      {
        "date": "2023-10-08T09:38:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/compute/docs/import/importing-virtual-disks"
      },
      {
        "date": "2023-09-28T08:53:00.000Z",
        "voteCount": 2,
        "content": "D for sure with BOYL"
      },
      {
        "date": "2023-09-24T10:22:00.000Z",
        "voteCount": 1,
        "content": "If the customer truly wants to BYOL, then sole-tenant nodes are required which is a requirement for this question \nYes it is required for BYOL because that's a requirement from Microsoft themselves\nhttps://www.microsoft.com/en-us/licensing/news/updated-licensing-rights-for-dedicated-cloud"
      },
      {
        "date": "2023-09-24T10:18:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is D\nSole-tenant nodes can help you meet dedicated hardware requirements for bring your own license (BYOL) scenarios that require per-core or per-processor licenses. When you use sole-tenant nodes, you have some visibility into the underlying hardware, which lets you track core and processor usage."
      },
      {
        "date": "2023-09-24T05:25:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/migrate/compute-engine/docs/4.11/how-to/organizing-migrations/sole-tenant-nodes"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 195,
    "url": "https://www.examtopics.com/discussions/google/view/121313-exam-professional-cloud-architect-topic-1-question-195/",
    "body": "You are deploying an application to Google Cloud. The application is part of a system. The application in Google Cloud must communicate over a private network with applications in a non-Google Cloud environment. The expected average throughput is 200 kbps. The business requires:<br><br>\u2022\t99.99% system availability<br>\u2022\tcost optimization<br><br>You need to design the connectivity between the locations to meet the business requirements. What should you provision?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAn HA Cloud VPN gateway connected with two tunnels to an on-premises VPN gateway.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA Classic Cloud VPN gateway connected with two tunnels to an on-premises VPN gateway.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTwo HA Cloud VPN gateways connected to two on-premises VPN gateways. Configure each HA Cloud VPN gateway to have two tunnels, each connected to different on-premises VPN gateways.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA Classic Cloud VPN gateway connected with one tunnel to an on-premises VPN gateway."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-12-28T21:22:00.000Z",
        "voteCount": 4,
        "content": "Yep, this question is a duplicate of #191."
      },
      {
        "date": "2023-09-25T06:22:00.000Z",
        "voteCount": 4,
        "content": "Duplicated question in somewhere. Answer is A."
      },
      {
        "date": "2023-09-24T10:34:00.000Z",
        "voteCount": 4,
        "content": "HA VPN supports two tunnels to achieve 99.99%. Classic VPN does not. Any more than 2 tunnels is excessive cost."
      },
      {
        "date": "2023-09-24T10:09:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is A"
      },
      {
        "date": "2023-09-24T05:12:00.000Z",
        "voteCount": 3,
        "content": "A, I think the question is duplicated"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 196,
    "url": "https://www.examtopics.com/discussions/google/view/121324-exam-professional-cloud-architect-topic-1-question-196/",
    "body": "Your company wants to migrate their 10-TB on-premises database export into Cloud Storage. You want to minimize the time it takes to complete this activity and the overall cost. The bandwidth between the on-premises environment and Google Cloud is 1 Gbps. You want to follow Google-recommended practices. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDevelop a Dataflow job to read data directly from the database and write it into Cloud Storage.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Data Transfer appliance to perform an offline migration.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a commercial partner ETL solution to extract the data from the on-premises database and upload it into Cloud Storage.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpload the data with gcloud storage cp.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 21,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 20,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-10-29T10:07:00.000Z",
        "voteCount": 10,
        "content": "This Transfer Appliance docs says it is suitable when \"It would take more than one week to upload your data over the network\"\nSince 10TB would take way less than a week for that bandwidth, I would go for D\nhttps://cloud.google.com/transfer-appliance/docs/4.0/overview#suitability"
      },
      {
        "date": "2023-12-06T14:03:00.000Z",
        "voteCount": 6,
        "content": "maximum object size in GCS is 5TB"
      },
      {
        "date": "2024-07-04T08:05:00.000Z",
        "voteCount": 2,
        "content": "So how would it work with B ? The data needs to still end up in GCS. Also, who says the export is 1 large file ?"
      },
      {
        "date": "2024-01-16T08:06:00.000Z",
        "voteCount": 2,
        "content": "so no answer is possible according to you ?..."
      },
      {
        "date": "2024-02-10T03:27:00.000Z",
        "voteCount": 1,
        "content": "you could solve that using split command"
      },
      {
        "date": "2024-06-15T07:14:00.000Z",
        "voteCount": 2,
        "content": "B. Use the Data Transfer Appliance to perform an offline migration.\n\nusing the Data Transfer Appliance aligns with Google's recommended practice for large-scale migrations where bandwidth limitations are a concern, ensuring efficient, secure, and cost-effective transfer of your on-premises database export into Google Cloud Storage."
      },
      {
        "date": "2024-05-10T05:46:00.000Z",
        "voteCount": 1,
        "content": "The current maximum object size supported by GCS is 5 TB, so it should be B"
      },
      {
        "date": "2024-04-11T19:16:00.000Z",
        "voteCount": 1,
        "content": "thanks for sharing it &lt;a href=\"https://www.qualitybacklink.net\"&gt;Link building SEO&lt;/a&gt;"
      },
      {
        "date": "2024-04-10T10:47:00.000Z",
        "voteCount": 2,
        "content": "Option B.  Cloud Storage object limit is 5 TB.  https://cloud.google.com/storage/quotas?hl=en#objects\n\n\nhttps://cloud.google.com/storage/quotas?hl=en#objects"
      },
      {
        "date": "2024-03-01T10:53:00.000Z",
        "voteCount": 3,
        "content": "Answer is B. Max cp limit of file is 5 TB"
      },
      {
        "date": "2024-01-13T04:17:00.000Z",
        "voteCount": 3,
        "content": "Since we would want to do it in the shortest time possible, using gsutil cp would take only 30 hours to move 10TB. So answer is D.\n\nhttps://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets?hl=en#online_versus_offline_transfer"
      },
      {
        "date": "2023-12-26T09:49:00.000Z",
        "voteCount": 3,
        "content": "As we have 1Ggigabit network we can transfer this with CLI command quicker than Transfer appliance. which takes time like more than week. Incase of lesser band width may be we use transfer appliance"
      },
      {
        "date": "2023-12-16T02:47:00.000Z",
        "voteCount": 1,
        "content": "ill go for b"
      },
      {
        "date": "2023-12-13T02:55:00.000Z",
        "voteCount": 3,
        "content": "Because there is 10 TB of data, it's B"
      },
      {
        "date": "2023-12-11T16:26:00.000Z",
        "voteCount": 1,
        "content": "Appliance is overkilling for 30 hours cli command, the appliance could take more than a week with shipping, how many times you run jobs which take hours to completed? Many times I would go with D"
      },
      {
        "date": "2023-11-16T05:21:00.000Z",
        "voteCount": 5,
        "content": "option B (Use the Data Transfer appliance to perform an offline migration) seems to be the most appropriate. It addresses the need for a speedy transfer of a large amount of data and is a cost-effective solution recommended by Google for large-scale data migrations. This option circumvents potential network bandwidth limitations and provides a reliable way to transfer large datasets.\n\nWhy option D is not a good choice : \nD. Upload the data with gcloud storage cp: This method uses the gcloud command-line tool to copy files to Cloud Storage. While simple, it might not be the most efficient for a 10-TB migration given the 1 Gbps bandwidth. The process could be slow and may require additional handling for potential interruptions and resuming uploads."
      },
      {
        "date": "2023-12-11T14:05:00.000Z",
        "voteCount": 2,
        "content": "Respectfully, I feel option D is the better choice.  According to https://cloud.google.com/transfer-appliance/docs/4.0/overview#suitability, Google recommends using a transfer appliance if the data transfer would take longer than 1 week.  At 1 Gbps and 10TB of data, the transfer would take 30 hours (https://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets#online_versus_offline_transfer).  We also need to take into account shipping time for the appliance itself, to and from. That would take a couple weeks.\n\nThe question mentions to minimize time and cost, and follow Google best practices.  In this case, D checks those boxes. The question does not mention a customer concern about network interruptions. If they did, then B could be argued as the more appropriate answer."
      },
      {
        "date": "2023-10-04T05:21:00.000Z",
        "voteCount": 3,
        "content": "Duplicate so review question #146 and the choice over there is offline transfer appliance.This is a tricky question"
      },
      {
        "date": "2024-04-30T23:57:00.000Z",
        "voteCount": 1,
        "content": "No exactly duplicate. In #146  choice betwee Data Transfer Appliance and gsutil -m. here between DTA and gcloud storage cp....but stilll a tricky question"
      },
      {
        "date": "2023-11-19T00:55:00.000Z",
        "voteCount": 1,
        "content": "Answer has to be D."
      },
      {
        "date": "2023-09-27T23:45:00.000Z",
        "voteCount": 1,
        "content": "D is the answer"
      },
      {
        "date": "2023-09-27T05:17:00.000Z",
        "voteCount": 3,
        "content": "The google math is 1TB data, 1GB bandwidth will take 3 hrs. We have 10TB of data so we are looking at 30 HRS hence the storage appliance"
      },
      {
        "date": "2023-12-10T23:26:00.000Z",
        "voteCount": 4,
        "content": "1 Gbps and not 1GB BW, this means it will take 23 hours approx, still faster than the appliance."
      },
      {
        "date": "2023-09-27T05:16:00.000Z",
        "voteCount": 1,
        "content": "age\n  Dataset size\n1\nTB\n12\nnetwork_check\n  Bandwidth\n1\nGbps\n9\ntimer\n  Transfer time\n3\nhr\n4"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 197,
    "url": "https://www.examtopics.com/discussions/google/view/138677-exam-professional-cloud-architect-topic-1-question-197/",
    "body": "You are working at a financial institution that stores mortgage loan approval documents on Cloud Storage. Any change to these approval documents must be uploaded as a separate approval file. You need to ensure that these documents cannot be deleted or overwritten for the next 5 years. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a retention policy on the bucket for the duration of 5 years. Create a lock on the retention policy.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a retention policy organizational constraint constraints/storage.retentionPolicySeconds at the organization level. Set the duration to 5 years.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a customer-managed key for the encryption of the bucket. Rotate the key after 5 years.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a retention policy organizational constraint constraints/storage.retentionPolicySeconds at the project level. Set the duration to 5 years."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-05-09T04:06:00.000Z",
        "voteCount": 1,
        "content": "Yes, its A - same question as 125 :)"
      },
      {
        "date": "2024-05-01T00:20:00.000Z",
        "voteCount": 1,
        "content": "A is correct.\n - retention policy must be locked\n- the is now need for retention policy for all buckets in organization or all buckets in the project"
      },
      {
        "date": "2024-04-26T04:56:00.000Z",
        "voteCount": 2,
        "content": "Create a retention policy on the bucket for the duration of 5 years. Create a lock on the retention policy"
      },
      {
        "date": "2024-04-14T07:18:00.000Z",
        "voteCount": 3,
        "content": "A. Create a retention policy on the bucket for the duration of 5 years. Create a lock on the retention policy"
      },
      {
        "date": "2024-04-14T03:31:00.000Z",
        "voteCount": 2,
        "content": "I think a ist correct"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 198,
    "url": "https://www.examtopics.com/discussions/google/view/146571-exam-professional-cloud-architect-topic-1-question-198/",
    "body": "Your company has decided to make a major revision of their API in order to create better experiences for their developers. They need to keep the old version of the API available and deployable, while allowing new customers and testers to try out the new API. They want to keep the same SSL and DNS records in place to serve both APIs.<br><br>What should they do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a new load balancer for the new version of the API",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReconfigure old clients to use a new endpoint for the new API",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tHave the old API forward traffic to the new API based on the path",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse separate backend pools for each API path behind the load balancer\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-09-09T15:34:00.000Z",
        "voteCount": 2,
        "content": "D is the correct answer as two versions (Old and New) have to be maintained with a single end point exposed to the developers."
      },
      {
        "date": "2024-08-27T11:07:00.000Z",
        "voteCount": 3,
        "content": "This solution allows you to meet all the requirements in a clean and maintainable way"
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 199,
    "url": "https://www.examtopics.com/discussions/google/view/146572-exam-professional-cloud-architect-topic-1-question-199/",
    "body": "You have a Compute Engine application that you want to autoscale when total memory usage exceeds 80%. You have installed the Cloud Monitoring agent and configured the autoscaling policy as follows:<br><br><img src=\"https://img.examtopics.com/professional-cloud-architect/image1.png\"><br><br>You observe that the application does not scale under high load. You want to resolve this. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the Target type to DELTA_PER_MINUTE.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the Metric identifier to agent.googleapis.com/memory/bytes_used.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the filter to metric.label.state = \u2018used\u2019.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the filter to metric.label.state = \u2018free\u2019 and the Target utilization to 20."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 3,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-08-27T11:09:00.000Z",
        "voteCount": 5,
        "content": "C. Change the filter to metric.label.state = \u2018used\u2019. The current filter is set up with multiple AND conditions, which means it's looking for a metric that simultaneously has all these states: 'used', 'buffered', 'cached', and 'slab'. This is logically impossible, as a memory location can't be in multiple states at once. Therefore, the filter will never match any metrics, and the autoscaling policy won't trigger."
      },
      {
        "date": "2024-08-28T04:23:00.000Z",
        "voteCount": 3,
        "content": "as it is clearly indicated in the public documentation https://cloud.google.com/compute/docs/autoscaler/scaling-cloud-monitoring-metrics#gcloud_5\nyou have change the filter to metric.label.state=\"used\""
      }
    ],
    "examNameCode": "professional-cloud-architect",
    "topicNumber": "1"
  }
]