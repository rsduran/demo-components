[
  {
    "topic": 1,
    "index": 1,
    "url": "https://www.examtopics.com/discussions/google/view/54296-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You are building an ML model to detect anomalies in real-time sensor data. You will use Pub/Sub to handle incoming requests. You want to store the results for analytics and visualization. How should you configure the pipeline?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1 = Dataflow, 2 = AI Platform, 3 = BigQuery\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1 = DataProc, 2 = AutoML, 3 = Cloud Bigtable",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1 = BigQuery, 2 = AutoML, 3 = Cloud Functions",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1 = BigQuery, 2 = AI Platform, 3 = Cloud Storage"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 23,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-06-03T16:55:00.000Z",
        "voteCount": 22,
        "content": "Definitely A. Dataflow is must."
      },
      {
        "date": "2021-06-02T13:45:00.000Z",
        "voteCount": 11,
        "content": "Even if I follow the link, it should be dataflow, AI-Platform and Bigquery.\nReal answer should be A"
      },
      {
        "date": "2024-09-20T11:27:00.000Z",
        "voteCount": 2,
        "content": "To preprocess data you will use Dataflow, and then you can use the Vertex AI platform for training and serving. Since it's a recommendation use case, Cloud BigQuery is the recommended NoSQL store to manage this use case storage at scale and reduce latency."
      },
      {
        "date": "2024-09-17T10:38:00.000Z",
        "voteCount": 1,
        "content": "PubSub -&gt; Dataflow -&gt; AI Platform -&gt; BiqQuery"
      },
      {
        "date": "2024-07-08T06:27:00.000Z",
        "voteCount": 1,
        "content": "BigQuery for analytics 100%"
      },
      {
        "date": "2024-06-05T06:38:00.000Z",
        "voteCount": 1,
        "content": "Big Query is ideal for analytics"
      },
      {
        "date": "2024-04-25T08:10:00.000Z",
        "voteCount": 1,
        "content": "Verified Answer"
      },
      {
        "date": "2024-04-04T23:41:00.000Z",
        "voteCount": 1,
        "content": "Option A"
      },
      {
        "date": "2023-12-01T03:52:00.000Z",
        "voteCount": 1,
        "content": "A - Dataflow is the only correct option for this case."
      },
      {
        "date": "2023-11-01T17:30:00.000Z",
        "voteCount": 1,
        "content": "AutoML is useful for labeled data. So either A or D. Dataflow is must for pipeline so A is correct"
      },
      {
        "date": "2023-09-26T00:50:00.000Z",
        "voteCount": 1,
        "content": "A. Definitely it's the correct answer"
      },
      {
        "date": "2023-09-03T08:35:00.000Z",
        "voteCount": 1,
        "content": "This use case similar to anomaly detection also points to A only. \nhttps://cloud.google.com/blog/products/data-analytics/anomaly-detection-using-streaming-analytics-and-ai"
      },
      {
        "date": "2023-08-07T20:15:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is D. 1 = BigQuery, 2 = AI Platform, 3 = Cloud Storage."
      },
      {
        "date": "2023-07-14T05:17:00.000Z",
        "voteCount": 1,
        "content": "right answer i A. Dataflow is a must."
      },
      {
        "date": "2023-04-28T21:27:00.000Z",
        "voteCount": 1,
        "content": "Dataflow is required"
      },
      {
        "date": "2023-04-28T21:26:00.000Z",
        "voteCount": 1,
        "content": "A, data flow is required"
      },
      {
        "date": "2023-03-08T13:29:00.000Z",
        "voteCount": 1,
        "content": "Definitely A."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 2,
    "url": "https://www.examtopics.com/discussions/google/view/55686-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "Your organization wants to make its internal shuttle service route more efficient. The shuttles currently stop at all pick-up points across the city every 30 minutes between 7 am and 10 am. The development team has already built an application on Google Kubernetes Engine that requires users to confirm their presence and shuttle station one day in advance. What approach should you take?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Build a tree-based regression model that predicts how many passengers will be picked up at each shuttle station. 2. Dispatch an appropriately sized shuttle and provide the map with the required stops based on the prediction.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Build a tree-based classification model that predicts whether the shuttle should pick up passengers at each shuttle station. 2. Dispatch an available shuttle and provide the map with the required stops based on the prediction.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Define the optimal route as the shortest route that passes by all shuttle stations with confirmed attendance at the given time under capacity constraints. 2. Dispatch an appropriately sized shuttle and indicate the required stops on the map.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Build a reinforcement learning model with tree-based classification models that predict the presence of passengers at shuttle stops as agents and a reward function around a distance-based metric. 2. Dispatch an appropriately sized shuttle and provide the map with the required stops based on the simulated outcome."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 16,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-06-20T09:17:00.000Z",
        "voteCount": 24,
        "content": "C:  for all confirmed."
      },
      {
        "date": "2024-09-20T11:29:00.000Z",
        "voteCount": 13,
        "content": "I agree with this, because it mentioned that they now \"require users to confirm their presence\". I think this is an example of when a classical routing algorithm is a better fit compare to ML-approach."
      },
      {
        "date": "2024-09-20T11:30:00.000Z",
        "voteCount": 2,
        "content": "Answer is C. This is a case where machine learning would be terrible, as it would not be 100% accurate and some passengers would not get picked up. A simple algorith works better here, and the question confirms customers will be indicating when they are at the stop so no ML required."
      },
      {
        "date": "2024-06-05T06:41:00.000Z",
        "voteCount": 1,
        "content": "C is the option that covers the scenario."
      },
      {
        "date": "2023-12-01T03:54:00.000Z",
        "voteCount": 3,
        "content": "C - Since we have the attendance list in advance. Tree-based classification, regression and reinforced learning sounds useless in this case."
      },
      {
        "date": "2023-11-14T06:13:00.000Z",
        "voteCount": 1,
        "content": "you do not need to predict how many people will be at each station as the requirement mentions they have to register a day in advance"
      },
      {
        "date": "2023-05-08T22:52:00.000Z",
        "voteCount": 1,
        "content": "Went with C"
      },
      {
        "date": "2023-04-03T22:57:00.000Z",
        "voteCount": 1,
        "content": "I think it should be C. I can easily eliminate D, this is not a case for reinforcement learning. Moreover, it seems like a Route Optimization rather than finding out best sized shuttle as mentioned in A or whether the shuttle should stop at a point as per point B."
      },
      {
        "date": "2023-03-14T11:25:00.000Z",
        "voteCount": 1,
        "content": "This is a route optimization problem"
      },
      {
        "date": "2022-11-23T06:31:00.000Z",
        "voteCount": 3,
        "content": "No need to predict the presences since they are already confirmed, best thing we can do is optimize the route"
      },
      {
        "date": "2022-10-31T02:57:00.000Z",
        "voteCount": 1,
        "content": "C. route more efficient is an optimization model"
      },
      {
        "date": "2022-08-15T04:56:00.000Z",
        "voteCount": 1,
        "content": "C is looks correct for me"
      },
      {
        "date": "2022-08-07T05:24:00.000Z",
        "voteCount": 1,
        "content": "Confirmed C"
      },
      {
        "date": "2022-07-28T13:34:00.000Z",
        "voteCount": 2,
        "content": "C. route more efficient is an optimization model"
      },
      {
        "date": "2022-05-09T13:48:00.000Z",
        "voteCount": 1,
        "content": "Answer is C"
      },
      {
        "date": "2022-04-12T22:05:00.000Z",
        "voteCount": 1,
        "content": "correct answer is C"
      },
      {
        "date": "2022-03-01T11:09:00.000Z",
        "voteCount": 1,
        "content": "C. Why would you want to predict anything here? The info on how many passengers will be and at which stations are already given by passengers themselves."
      },
      {
        "date": "2022-01-14T16:21:00.000Z",
        "voteCount": 1,
        "content": "It needs to be C. No use of ML here"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 3,
    "url": "https://www.examtopics.com/discussions/google/view/54298-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You were asked to investigate failures of a production line component based on sensor readings. After receiving the dataset, you discover that less than 1% of the readings are positive examples representing failure incidents. You have tried to train several classification models, but none of them converge. How should you resolve the class imbalance problem?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the class distribution to generate 10% positive examples.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a convolutional neural network with max pooling and softmax activation.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDownsample the data with upweighting to create a sample with 10% positive examples.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRemove negative examples until the numbers of positive and negative examples are equal."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 13,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-07-19T21:01:00.000Z",
        "voteCount": 31,
        "content": "ANS: C\nhttps://developers.google.com/machine-learning/data-prep/construct/sampling-splitting/imbalanced-data#downsampling-and-upweighting\n\n - less than 1% of the readings are positive \n-  none of them converge. \n\nDownsampling (in this context) means training on a disproportionately low subset of the majority class examples."
      },
      {
        "date": "2021-10-18T05:21:00.000Z",
        "voteCount": 2,
        "content": "Agree, C is correct"
      },
      {
        "date": "2024-09-24T22:56:00.000Z",
        "voteCount": 9,
        "content": "=New Question3=\nYou are going to train a DNN regression model with Keras APJs using this code:\n\n\tmodel - tf.keras.Sequential() model.add(tf.keras.layers.Oense(\n\t\t256,\n\t\tuse_bias-True,\n\t\tactivation-\u2022relu',\n\t\tkernel_initializer-None,\n\t\tkernel_regularizer-None,\n\t\tinput_shape-(500,)))\n\tmodel.add(tf.keras.layers.Oropout(rate-0.25)) \n\tmodel.add(tf.keras.layers.Oense( \n\t\t128, use_bias-True, \n\t\tactivation-\u2022relu',\n\t\tkernel_initializer-'uniform', \n\t\tkernel_regularizer-'12'))\n\tmodel.add(tf.keras.layers.Oropout(rate-0.25)) \n\tmodel.add(tf.keras.layers.Oense( \n\t\t2, use_bias-False,\n\t\tactivation-\u2022softriax')) \n\tmodel.cornpile(loss-\u2022mse')\n\t\n\tHow many trainable weights does your model have? (The arithmetic below is correct.)\n\t\nA.\t 501*256+257*128+2 = 161154\nB.\t 500*256+256*128+128*2 = 161024\nC.\t 501*256+257*128+128*2 = 161408\nD.\t 500*256*0(?)25+256*128*0(?)25+128*2 = 4044"
      },
      {
        "date": "2022-01-02T07:34:00.000Z",
        "voteCount": 1,
        "content": "B: Dense layers with 100 % trainable weigts, the dropout rate at 0.25 will randomly drop 25 % for the regularization's sake - still training for 100 % of the weights."
      },
      {
        "date": "2022-01-24T06:12:00.000Z",
        "voteCount": 5,
        "content": "Correct answer is C. Do not forget about bias term which is also trainable parameter."
      },
      {
        "date": "2022-04-14T09:08:00.000Z",
        "voteCount": 1,
        "content": "Why 128 for the last layer is correct and not 129 X 2?"
      },
      {
        "date": "2022-08-15T23:14:00.000Z",
        "voteCount": 1,
        "content": "because of use_bias = False"
      },
      {
        "date": "2022-08-15T23:23:00.000Z",
        "voteCount": 3,
        "content": "C is correct. 2nd Layer with use_bias = True"
      },
      {
        "date": "2021-12-25T19:47:00.000Z",
        "voteCount": 4,
        "content": "Why do you post new questions in every existing question rather than post them as a new question?"
      },
      {
        "date": "2021-12-28T05:46:00.000Z",
        "voteCount": 4,
        "content": "Only moderator can post new questions.  Thus, I am left with this format.  I have emailed the additional questions to the moderator, but he/she has not added them to the site.  These questions were received off of other practice tests, but answers were not provided."
      },
      {
        "date": "2021-12-22T07:31:00.000Z",
        "voteCount": 1,
        "content": "Answer?"
      },
      {
        "date": "2022-06-12T03:46:00.000Z",
        "voteCount": 1,
        "content": "D , is the only option that takes care of the dropout factor"
      },
      {
        "date": "2022-06-12T04:06:00.000Z",
        "voteCount": 2,
        "content": "my bad , this was tricky \"The Dropout Layer randomly disables neurons during training. They still are present in your model and therefore aren\u00b4t discounted from the number of parameters in your model summary.\" , so D is wrong , C and A takes care of the bias , but C is correct"
      },
      {
        "date": "2024-09-24T22:58:00.000Z",
        "voteCount": 5,
        "content": "The answer is C.\n\nWe have to note that\n(1) Downsampling on major class\n(2) Upsampling on minor class\n(3) Upweighting on minor class\nall work for imbalanced data.\n\nHowever, the key assumption in the question is that \"You have tried to train several classification models, but none of them converge\".\nYou are not asked to tackle imbalanced data but asked to handle the non-convergence problem (due to the limited resources or the poorness of the algorithm).\n\nIn the official document, it says: \"If you have an imbalanced data set, first try training on the true distribution. If the model works well and generalizes, you're done! If not, try the following downsampling and upweighting technique.\"\nhttps://developers.google.com/machine-learning/data-prep/construct/sampling-splitting/imbalanced-data#downsampling-and-upweighting\nIn other words, the \"downsampling and upweighting technique\" is the technique for the non-convergence problem (not for for the imbalanced data)."
      },
      {
        "date": "2024-09-24T22:56:00.000Z",
        "voteCount": 2,
        "content": "C. Downsample the data with upweighting to create a sample with 10% positive examples.\n\nDealing with class imbalance can be challenging for machine learning models. One common approach to resolving the problem is to downsample the data, either by removing examples from the majority class or by oversampling the minority class. In this case, since you have very few positive examples, you would want to oversample the positive examples to create a sample that better represents the underlying distribution of the data. This could involve using upweighting, where positive examples are given a higher weight in the loss function to compensate for their relative scarcity in the data. This can help the model to better focus on the positive examples and improve its performance in classifying failure incidents."
      },
      {
        "date": "2024-06-05T06:49:00.000Z",
        "voteCount": 1,
        "content": "This approach involves downsampling the majority class (negative examples) and upweighting the minority class (positive examples) to create a balanced dataset.\nBy doing so, the model can learn from both classes effectively.\nReference: How to Handle Imbalanced Classes in Machine Learning [https://elitedatascience.com/imbalanced-classes]"
      },
      {
        "date": "2023-12-01T03:57:00.000Z",
        "voteCount": 2,
        "content": "C - Downsample the majority and add weights to it."
      },
      {
        "date": "2023-11-14T11:34:00.000Z",
        "voteCount": 1,
        "content": "Max Pooling is a pooling operation that calculates the maximum value for patches of a feature map, and uses it to create a downsampled (pooled) feature map. It is usually used after a convolutional layer."
      },
      {
        "date": "2023-05-08T22:53:00.000Z",
        "voteCount": 1,
        "content": "Went with C"
      },
      {
        "date": "2023-04-28T22:19:00.000Z",
        "voteCount": 1,
        "content": "https://developers.google.com/machine-learning/data-prep/construct/sampling-splitting/imbalanced-data#downsampling-and-upweighting"
      },
      {
        "date": "2023-02-05T09:08:00.000Z",
        "voteCount": 1,
        "content": "https://developers.google.com/machine-learning/data-prep/construct/sampling-splitting/imbalanced-data"
      },
      {
        "date": "2023-01-31T13:17:00.000Z",
        "voteCount": 1,
        "content": "C. Downsample the data with upweighting to create a sample with 10% positive examples.\n\nDealing with class imbalance can be challenging for machine learning models. One common approach to resolving the problem is to downsample the data, either by removing examples from the majority class or by oversampling the minority class. In this case, since you have very few positive examples, you would want to oversample the positive examples to create a sample that better represents the underlying distribution of the data. This could involve using upweighting, where positive examples are given a higher weight in the loss function to compensate for their relative scarcity in the data. This can help the model to better focus on the positive examples and improve its performance in classifying failure incidents."
      },
      {
        "date": "2022-12-31T03:04:00.000Z",
        "voteCount": 1,
        "content": "Answer would obviously be C\nAs the dataset is imbalanced and you need to resolve this issue in order to obtain desired result the best approach will be to downsample the data."
      },
      {
        "date": "2022-11-23T06:32:00.000Z",
        "voteCount": 1,
        "content": "Best practice for imbalanced dataset is to downsample with upweight\nhttps://developers.google.com/machine-learning/data-prep/construct/sampling-splitting/imbalanced-data#downsampling-and-upweighting"
      },
      {
        "date": "2022-08-15T06:07:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is \"C\""
      },
      {
        "date": "2022-07-28T13:35:00.000Z",
        "voteCount": 1,
        "content": "C. because regardless of the model you use, you should always try to transform or adapt your dataset so that it is more balanced"
      },
      {
        "date": "2022-06-12T02:24:00.000Z",
        "voteCount": 1,
        "content": "https://developers.google.com/machine-learning/data-prep/construct/sampling-splitting/imbalanced-data"
      },
      {
        "date": "2022-06-12T02:24:00.000Z",
        "voteCount": 1,
        "content": "https://developers.google.com/machine-learning/data-prep/construct/sampling-splitting/imbalanced-data"
      },
      {
        "date": "2022-06-12T02:25:00.000Z",
        "voteCount": 1,
        "content": "sorry , mean C"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 4,
    "url": "https://www.examtopics.com/discussions/google/view/57450-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You want to rebuild your ML pipeline for structured data on Google Cloud. You are using PySpark to conduct data transformations at scale, but your pipelines are taking over 12 hours to run. To speed up development and pipeline run time, you want to use a serverless tool and SQL syntax. You have already moved your raw data into Cloud Storage. How should you build the pipeline on Google Cloud while meeting the speed and processing requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Data Fusion's GUI to build the transformation pipelines, and then write the data into BigQuery.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConvert your PySpark into SparkSQL queries to transform the data, and then run your pipeline on Dataproc to write the data into BigQuery.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIngest your data into Cloud SQL, convert your PySpark commands into SQL queries to transform the data, and then use federated queries from BigQuery for machine learning.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIngest your data into BigQuery using BigQuery Load, convert your PySpark commands into BigQuery SQL queries to transform the data, and then write the transformations to a new table.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 24,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 7,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-09-20T11:31:00.000Z",
        "voteCount": 21,
        "content": "It should be D .... Data Fusion is not SQL syntax ...."
      },
      {
        "date": "2021-09-08T07:54:00.000Z",
        "voteCount": 4,
        "content": "Agree, BQ is the only serverless that support SQL"
      },
      {
        "date": "2022-01-19T22:22:00.000Z",
        "voteCount": 1,
        "content": "Needs to be D as the most suitable answer given the req's in question Datafusion is more of a no code Data transformation tool"
      },
      {
        "date": "2021-07-18T16:17:00.000Z",
        "voteCount": 12,
        "content": "ANS: A\nhttps://cloud.google.com/data-fusion#section-1\n- Data Fusion is a serverless approach leveraging the scalability and reliability of Google services like Dataproc means Data Fusion offers the best of data integration capabilities with a lower total cost of ownership.\n- BigQuery is serverless and supports SQL. \n- Dataproc is not serverless, you have to manage clusters. \n- Cloud SQL is not serverless, you have to manage instances."
      },
      {
        "date": "2024-08-16T02:01:00.000Z",
        "voteCount": 1,
        "content": "By your logic it should be D, because BQ is fully serverless and supports SQL"
      },
      {
        "date": "2021-09-08T07:54:00.000Z",
        "voteCount": 1,
        "content": "Data Fusion is not serverless, it create dataproc to execute the job .... I think the answer is C"
      },
      {
        "date": "2021-10-18T05:30:00.000Z",
        "voteCount": 3,
        "content": "Data Fusion is serverless: https://cloud.google.com/data-fusion#all-features"
      },
      {
        "date": "2023-03-06T01:02:00.000Z",
        "voteCount": 2,
        "content": "I think you're only viewing the sentence \"A serverless approach leveraging the scalability and reliability of Google services like Dataproc means Data Fusion offers the best of data integration capabilities with a lower total cost of ownership\", The sentence implies that Data Fusion leverages a serverless approach, but it does not explicitly state that Data Fusion itself is serverless. It states that Data Fusion offers the best of data integration capabilities by using a serverless approach that leverages the scalability and reliability of Google services like Dataproc. So, while Data Fusion may not be fully serverless, it is designed to take advantage of serverless capabilities through its integration with Google services."
      },
      {
        "date": "2021-10-18T05:29:00.000Z",
        "voteCount": 2,
        "content": "Agree, A is correct"
      },
      {
        "date": "2024-09-17T11:23:00.000Z",
        "voteCount": 1,
        "content": "B. You need Cloud Dataproc to transform the data from PySpark to Spark SQL"
      },
      {
        "date": "2024-08-16T01:57:00.000Z",
        "voteCount": 2,
        "content": "Serverless, SQL syntax -&gt; BigQuery, simple as that"
      },
      {
        "date": "2024-08-02T14:03:00.000Z",
        "voteCount": 2,
        "content": "I am very curious. Why are the solutions (when I click Reveal Solution) generally WRONG?"
      },
      {
        "date": "2024-07-16T05:59:00.000Z",
        "voteCount": 2,
        "content": "option D because needs a serveless solution and sql sintax and BigQuery offer this. Datarproc is not serverless, so B is incorrect, D is correct option."
      },
      {
        "date": "2024-07-08T06:49:00.000Z",
        "voteCount": 2,
        "content": "There's an updated version of this question in the official Google Cloud certified PMLE study guide. Option D is marked as correct"
      },
      {
        "date": "2024-08-16T02:02:00.000Z",
        "voteCount": 1,
        "content": "Can you link the updated version? On Amazon it's still 1st version and marked B"
      },
      {
        "date": "2024-06-05T11:01:00.000Z",
        "voteCount": 2,
        "content": "The best approach is option D: Ingest data into BigQuery and use SQL queries for transformations. This leverages BigQuery\u2019s serverless capabilities, efficient processing, and seamless integration with other Google Cloud services."
      },
      {
        "date": "2023-12-01T04:02:00.000Z",
        "voteCount": 1,
        "content": "D - BigQuery is the only serverless and SQL-syntax option."
      },
      {
        "date": "2023-11-14T06:27:00.000Z",
        "voteCount": 2,
        "content": "D - as BQ is server less and supports SQL\nnone of the other options match both criteria"
      },
      {
        "date": "2023-07-07T17:14:00.000Z",
        "voteCount": 1,
        "content": "I'll go with D."
      },
      {
        "date": "2023-05-08T22:53:00.000Z",
        "voteCount": 3,
        "content": "Went with D"
      },
      {
        "date": "2023-03-14T11:29:00.000Z",
        "voteCount": 3,
        "content": "BQ is the serverless solution"
      },
      {
        "date": "2024-08-16T02:03:00.000Z",
        "voteCount": 1,
        "content": "But using dataproc is not serverless, so answer should be D"
      },
      {
        "date": "2023-01-13T23:48:00.000Z",
        "voteCount": 1,
        "content": "Correct option is D"
      },
      {
        "date": "2023-01-11T04:48:00.000Z",
        "voteCount": 1,
        "content": "Vote D"
      },
      {
        "date": "2023-01-06T03:59:00.000Z",
        "voteCount": 1,
        "content": "It should be A."
      },
      {
        "date": "2022-11-23T06:37:00.000Z",
        "voteCount": 4,
        "content": "Data Fusion is not in SQL syntax, so no A;\nDataproc is not serverless, so no B;\nPassing through Cloud SQL is uselss, just go with BigQuery, so no C;\nD is correct"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 5,
    "url": "https://www.examtopics.com/discussions/google/view/54653-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You manage a team of data scientists who use a cloud-based backend system to submit training jobs. This system has become very difficult to administer, and you want to use a managed service instead. The data scientists you work with use many different frameworks, including Keras, PyTorch, theano, Scikit-learn, and custom libraries. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the AI Platform custom containers feature to receive training jobs using any framework.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Kubeflow to run on Google Kubernetes Engine and receive training jobs through TF Job.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a library of VM images on Compute Engine, and publish these images on a centralized repository.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up Slurm workload manager to receive jobs that can be scheduled to run on your cloud infrastructure."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 18,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-06-05T09:40:00.000Z",
        "voteCount": 24,
        "content": "the answer is A"
      },
      {
        "date": "2021-07-23T23:34:00.000Z",
        "voteCount": 10,
        "content": "A, because AI platform supported all the frameworks mentioned. And Kubeflow is not managed service in GCP. https://cloud.google.com/ai-platform/training/docs/getting-started-pytorch"
      },
      {
        "date": "2024-07-08T06:53:00.000Z",
        "voteCount": 1,
        "content": "A. Now it's called Vertex AI"
      },
      {
        "date": "2024-06-05T11:03:00.000Z",
        "voteCount": 1,
        "content": "The best approach is option A: Use AI Platform custom containers. It provides flexibility, scalability, and support for various frameworks, making it an ideal choice for your team\u2019s needs."
      },
      {
        "date": "2023-12-01T04:20:00.000Z",
        "voteCount": 1,
        "content": "Chose A"
      },
      {
        "date": "2023-11-14T06:29:00.000Z",
        "voteCount": 2,
        "content": "A is the only Google managed service solution \nB,C - are not managed \nD- is a 3rd party"
      },
      {
        "date": "2023-05-08T22:54:00.000Z",
        "voteCount": 2,
        "content": "Went with A"
      },
      {
        "date": "2023-01-12T06:08:00.000Z",
        "voteCount": 1,
        "content": "The answer must be D as nowhere in the question has GCP been mention https://aadityachapagain.com/2020/09/distributed-training-with-slurm-on-gcp/"
      },
      {
        "date": "2023-03-06T01:47:00.000Z",
        "voteCount": 2,
        "content": "D is incorrect, this is more far from a managed service based solution."
      },
      {
        "date": "2023-01-12T06:07:00.000Z",
        "voteCount": 1,
        "content": "The Answer is D. As no where in the answer has GCP been mentioned. https://aadityachapagain.com/2020/09/distributed-training-with-slurm-on-gcp/"
      },
      {
        "date": "2023-01-06T03:46:00.000Z",
        "voteCount": 2,
        "content": "It's A"
      },
      {
        "date": "2022-12-08T09:50:00.000Z",
        "voteCount": 1,
        "content": "Here the question is on workload management not on supporting frameworks slurm is a managed solution for workloads"
      },
      {
        "date": "2022-11-23T06:50:00.000Z",
        "voteCount": 4,
        "content": "Now it's Vertex AI (instead of AI Platform), but it's the best solution, no need to do anything more complicated"
      },
      {
        "date": "2022-10-31T03:03:00.000Z",
        "voteCount": 3,
        "content": "A - Vertex AI now"
      },
      {
        "date": "2022-08-15T06:08:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is \"A\""
      },
      {
        "date": "2022-02-27T19:26:00.000Z",
        "voteCount": 2,
        "content": "A is correct"
      },
      {
        "date": "2022-01-22T04:33:00.000Z",
        "voteCount": 2,
        "content": "the answer is A"
      },
      {
        "date": "2022-01-06T17:47:00.000Z",
        "voteCount": 2,
        "content": "A AI Platform should be correct\nYou can build pipelines in Airflow, Kubeflow, Dataflow but they need to be managed over AI platform or Vertex AI."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 6,
    "url": "https://www.examtopics.com/discussions/google/view/54652-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You work for an online retail company that is creating a visual search engine. You have set up an end-to-end ML pipeline on Google Cloud to classify whether an image contains your company's product. Expecting the release of new products in the near future, you configured a retraining functionality in the pipeline so that new data can be fed into your ML models. You also want to use AI Platform's continuous evaluation service to ensure that the models have high accuracy on your test dataset. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tKeep the original test dataset unchanged even if newer products are incorporated into retraining.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExtend your test dataset with images of the newer products when they are introduced to retraining.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReplace your test dataset with images of the newer products when they are introduced to retraining.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate your test dataset with images of the newer products when your evaluation metrics drop below a pre-decided threshold."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 19,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-06-05T18:44:00.000Z",
        "voteCount": 32,
        "content": "I think B is the right answer.\n\nA: Doesn't make sense. If you don't use the new product, it becomes useless.\nC: Conventional products are also necessary as data.\nD: I don't understand the need to wait until the threshold is exceeded."
      },
      {
        "date": "2021-10-18T05:40:00.000Z",
        "voteCount": 1,
        "content": "Agree with you, B is correct"
      },
      {
        "date": "2021-09-09T01:21:00.000Z",
        "voteCount": 1,
        "content": "Agree, B as it extends to new products."
      },
      {
        "date": "2023-05-10T07:56:00.000Z",
        "voteCount": 2,
        "content": "D could have sense considering that is mentioned the intention to use AI Platform's continuous evaluation service"
      },
      {
        "date": "2023-11-15T01:46:00.000Z",
        "voteCount": 2,
        "content": "it's D for two reasons:\n- explicitly required in the question to leverage Continuous evaluation service\n- the threshod check allows to decide when perform the retrain avoiding making it for every single new data arrived."
      },
      {
        "date": "2021-06-05T09:39:00.000Z",
        "voteCount": 11,
        "content": "answer is B"
      },
      {
        "date": "2024-10-12T17:21:00.000Z",
        "voteCount": 1,
        "content": "D:\nIts definitely not a clear choice. B is the most obvious answer - you know you've got new data coming in, so why not incorporate it immediately into training. EXCEPT the question clearly states that Vertex continual evaluation should feature."
      },
      {
        "date": "2024-09-24T23:01:00.000Z",
        "voteCount": 4,
        "content": "=New Question6=\nYou work for a global footwear retailer and need to predict when an item will be out of stock based on historical inventory dat a. Customer behavior is highly dynamic since footwear demand is influenced by many different factors. You want to serve models that are trained on all available data, but track your performance on specific subsets of data before pushing to production. What is the most streamlined and reliable way to perform this validation? \n\nA.   Use the TFX Mode!Validator tools to specify performance metrics for production readiness\nB.\t Use k-fold cross-validation as a validation strategy to ensure that your model is ready for production. \nC.\t Use the last relevant week of data as a validation set to ensure that your model is performing accurately on current data.\nD.\t Use the entire dataset and treat the area under the receiver operating characteristics curve (AUC ROC) as the main metric."
      },
      {
        "date": "2024-09-12T18:24:00.000Z",
        "voteCount": 1,
        "content": "Option A\nYou can define specific performance metrics for different subsets of your data"
      },
      {
        "date": "2024-06-02T19:45:00.000Z",
        "voteCount": 1,
        "content": "option A is correct"
      },
      {
        "date": "2024-06-02T19:46:00.000Z",
        "voteCount": 1,
        "content": "TFX ModelValidator tools are designed to integrate performance tracking into the ML pipeline, providing robust validation on specific subsets of data before deploying models to production."
      },
      {
        "date": "2022-02-07T17:54:00.000Z",
        "voteCount": 6,
        "content": "A is the correct"
      },
      {
        "date": "2022-01-14T16:35:00.000Z",
        "voteCount": 2,
        "content": "B looks to be ok as using cross validation testing results are more even"
      },
      {
        "date": "2024-09-24T23:01:00.000Z",
        "voteCount": 1,
        "content": "A. Keep the original test dataset unchanged even if newer products are incorporated into retraining. : This would not test on new products.\nB. Extend your test dataset with images of the newer products when they are introduced to retraining. Most Voted : old+new products testing. Great\nC. Replace your test dataset with images of the newer products when they are introduced to retraining. : No need of old product to be tested? old product recognition might change when new products are added in training. Option Not good.\nD. Update your test dataset with images of the newer products when your evaluation metrics drop below a pre-decided threshold.: why wait? no need"
      },
      {
        "date": "2024-09-24T23:01:00.000Z",
        "voteCount": 2,
        "content": "You need to correctly classify newer products, so you need the new training data ==&gt; A is wrong;\nYou need to keep doing a good job on older dataset, you can't just ignore it ==&gt; C is wrong;\nYou know when you are introducing new products, there is no need to wait for a drop in preformaces ==&gt; D is wrong;\nB is correct"
      },
      {
        "date": "2024-09-12T18:14:00.000Z",
        "voteCount": 1,
        "content": "B correct"
      },
      {
        "date": "2024-06-05T11:07:00.000Z",
        "voteCount": 1,
        "content": "The best approach is option B: Extend your test dataset with images of the newer products. This ensures accurate evaluation as your product catalog evolves."
      },
      {
        "date": "2024-01-21T11:03:00.000Z",
        "voteCount": 1,
        "content": "My initial confusion with option B arose from the phrase \"with images of the newer products when they are introduced to retraining.\" Initially, I mistakenly interpreted it as recommending the use of the same images in both training and testing, which is incorrect. However, upon further reflection, I realized that using the same product does not necessarily mean using identical images. Therefore, I now believe that option B is the most suitable choice."
      },
      {
        "date": "2023-12-06T07:04:00.000Z",
        "voteCount": 1,
        "content": "A and C make no sense - you don't want to lose any of the performance on existing products.\nD - Why would you wait for your performance to drop in the first place? That's a reactive rather than proactive approach.\nThe answer is B"
      },
      {
        "date": "2023-12-01T04:34:00.000Z",
        "voteCount": 1,
        "content": "B for sure"
      },
      {
        "date": "2023-11-14T06:31:00.000Z",
        "voteCount": 1,
        "content": "B is the only thing we do in practice"
      },
      {
        "date": "2023-05-08T22:54:00.000Z",
        "voteCount": 2,
        "content": "Went with B"
      },
      {
        "date": "2023-03-06T23:56:00.000Z",
        "voteCount": 2,
        "content": "you can't just replace the old product data with just new product, until you don't sell old product anymore"
      },
      {
        "date": "2022-12-31T03:08:00.000Z",
        "voteCount": 1,
        "content": "Ans: B\nA would not use the newer data hence not a ideal option\nC Replacing will not be a good option as it will replace older data with newer data which in turn hampers accuracy\nD waiting for threshold is not a better option"
      },
      {
        "date": "2022-11-30T00:11:00.000Z",
        "voteCount": 2,
        "content": "B is the most plausible answer. The key principle is that test set should represent ground truth distribution to infer credible model evaluation. So once new products become available, test set should be updated to reflect the new product distribution"
      },
      {
        "date": "2022-11-01T10:50:00.000Z",
        "voteCount": 1,
        "content": "it should be B as its inclusive"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 7,
    "url": "https://www.examtopics.com/discussions/google/view/54654-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You need to build classification workflows over several structured datasets currently stored in BigQuery. Because you will be performing the classification several times, you want to complete the following steps without writing code: exploratory data analysis, feature selection, model building, training, and hyperparameter tuning and serving. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure AutoML Tables to perform the classification task.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun a BigQuery ML task to perform logistic regression for the classification.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AI Platform Notebooks to run the classification model with pandas library.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AI Platform to run the classification model job configured for hyperparameter tuning."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-07-24T00:13:00.000Z",
        "voteCount": 27,
        "content": "A. Because BigQuery ML need to write code."
      },
      {
        "date": "2024-07-16T07:04:00.000Z",
        "voteCount": 1,
        "content": "create a model without doing literally anything, logo AutoML. A."
      },
      {
        "date": "2024-06-05T22:28:00.000Z",
        "voteCount": 1,
        "content": "A) Auto ML Tables doesn\u2019t require code."
      },
      {
        "date": "2024-03-31T16:55:00.000Z",
        "voteCount": 3,
        "content": "The question says 'over several structured datasets' means large/multiple datasets and 'several times' means frequently use of data. Though BigQuery ML is not an absolute 'NO Code' solution but all it needs is very simple SQL query to train ML model So 'B' could be the correct answer here but it is asking for Hyperparameter tuning which is not available in BigQuery ML so correct answer is 'A'"
      },
      {
        "date": "2023-12-01T04:40:00.000Z",
        "voteCount": 1,
        "content": "A - AutoML is no code"
      },
      {
        "date": "2023-07-08T19:51:00.000Z",
        "voteCount": 1,
        "content": "requirement : No code\nA. Configure AutoML Tables to perform the classification task. : No code\nB. Run a BigQuery ML task to perform logistic regression for the classification. : coding LR model\nC. Use AI Platform Notebooks to run the classification model with pandas library. : Notebooks include codes\nD. Use AI Platform to run the classification model job configured for hyperparameter tuning.: job needs to be written what to execute"
      },
      {
        "date": "2023-05-08T22:54:00.000Z",
        "voteCount": 1,
        "content": "Went with A"
      },
      {
        "date": "2022-12-06T11:18:00.000Z",
        "voteCount": 1,
        "content": "Because BigQueryML doesn't have lots of steps that mentioned in question"
      },
      {
        "date": "2022-11-23T07:01:00.000Z",
        "voteCount": 1,
        "content": "\"without writing code\" ==&gt; AutoML\nA is correct"
      },
      {
        "date": "2022-11-01T10:53:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is \"A\""
      },
      {
        "date": "2022-08-15T06:09:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is \"A\""
      },
      {
        "date": "2022-08-05T23:13:00.000Z",
        "voteCount": 1,
        "content": "Because BigQuery ML need to write code, so A is the correct one"
      },
      {
        "date": "2022-06-12T05:32:00.000Z",
        "voteCount": 1,
        "content": "\"without writing code\" only A option complies with this statment , all other options requires writing code"
      },
      {
        "date": "2022-02-27T19:25:00.000Z",
        "voteCount": 2,
        "content": "A is correct"
      },
      {
        "date": "2022-01-06T17:57:00.000Z",
        "voteCount": 3,
        "content": "A is correct \nhttps://cloud.google.com/automl-tables/docs/beginners-guide"
      },
      {
        "date": "2021-12-22T07:30:00.000Z",
        "voteCount": 3,
        "content": "=New Question7=\nYou recently designed and built a custom neural network that uses critical dependencies specific to your organization's framework. You need to train the model using a managed training service on Google Cloud. However, the ML framework and related dependencies are not supported by Al Platform Training. Also, both your model and your data are too large to fit in memory on a single machine. Your ML framework of choice uses the scheduler, workers, and servers distribution structure. What should you do?\n\nA.\t Build your custom container to run jobs on Al Platform Training\nB.\t Use a built-in model available on Al Platform Training\nC.\t Build your custom containers to run distributed training jobs on Al Platform Training\nD.\t Reconfigure your code to a ML framework with dependencies that are supported by Al Platform Training"
      },
      {
        "date": "2022-01-05T12:13:00.000Z",
        "voteCount": 4,
        "content": "C custom container and distributed system"
      },
      {
        "date": "2022-01-20T21:53:00.000Z",
        "voteCount": 3,
        "content": "Answer - C\nIt's between A &amp; C\nC - Because the questions states data too large to fit in memory hence distributed training is relevant"
      },
      {
        "date": "2022-04-05T22:29:00.000Z",
        "voteCount": 3,
        "content": "C is the answer without doubt.\nA: Distributed? Nope\nB: Built-in? Nope\nD: Reconfig? Nope"
      },
      {
        "date": "2021-12-22T07:33:00.000Z",
        "voteCount": 1,
        "content": "Answer?"
      },
      {
        "date": "2021-12-12T07:43:00.000Z",
        "voteCount": 1,
        "content": "You have to export out BQ trained ML model to set it up for inference. Inference is not natively offered in BQ. \nYou can perform EDA in autoML tables."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 8,
    "url": "https://www.examtopics.com/discussions/google/view/54655-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You work for a public transportation company and need to build a model to estimate delay times for multiple transportation routes. Predictions are served directly to users in an app in real time. Because different seasons and population increases impact the data relevance, you will retrain the model every month. You want to follow Google-recommended best practices. How should you configure the end-to-end architecture of the predictive model?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Kubeflow Pipelines to schedule your multi-step workflow from training to deploying your model.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a model trained and deployed on BigQuery ML, and trigger retraining with the scheduled query feature in BigQuery.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite a Cloud Functions script that launches a training and deploying job on AI Platform that is triggered by Cloud Scheduler.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Composer to programmatically schedule a Dataflow job that executes the workflow from training to deploying your model."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 13,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-06-22T01:12:00.000Z",
        "voteCount": 39,
        "content": "Answer: A\nA. Kubeflow Pipelines can form an end-to-end architecture (https://www.kubeflow.org/docs/components/pipelines/overview/pipelines-overview/) and deploy models.\nB. BigQuery ML can't offer an end-to-end architecture because it must use another tool, like AI Platform, for serving models at the end of the process (https://cloud.google.com/bigquery-ml/docs/export-model-tutorial#online_deployment_and_serving).\nC. Cloud Scheduler can trigger the first step in a pipeline, but then some orchestrator is needed to continue the remaining steps. Besides, having Cloud Scheduler alone can't ensure failure handling during pipeline execution.\nD. A Dataflow job can't deploy models, it must use AI Platform at the end instead."
      },
      {
        "date": "2021-09-09T01:58:00.000Z",
        "voteCount": 1,
        "content": "Dataflow can deploy model .... this is how you do stream inference on stream"
      },
      {
        "date": "2022-02-17T08:33:00.000Z",
        "voteCount": 2,
        "content": "yes you can but it is not supposed to do that. DF is for data processing and transformation. you would loose all shenanigans kubeflow provide as native.\nAmong the two answers, i think A is the most correct"
      },
      {
        "date": "2021-10-18T06:08:00.000Z",
        "voteCount": 1,
        "content": "Please send a source link?"
      },
      {
        "date": "2021-10-18T06:08:00.000Z",
        "voteCount": 3,
        "content": "I guess it's A"
      },
      {
        "date": "2021-06-05T10:30:00.000Z",
        "voteCount": 11,
        "content": "the answer is D. found similar explaination in this course. open for discussion. I found B could also work, but the question asked for end-to end, thus I choose D in stead of B https://www.coursera.org/lecture/ml-pipelines-google-cloud/what-is-cloud-composer-CuXTQ"
      },
      {
        "date": "2023-03-06T02:13:00.000Z",
        "voteCount": 1,
        "content": "D is incorrect. Cloud Composer is a fully managed workflow orchestration service built on Apache Airflow. It is a recommended way by Google to schedule continuous training jobs. But it isn\u2019t used to run the training jobs. AI Platform is used for training and deployment."
      },
      {
        "date": "2024-09-24T23:02:00.000Z",
        "voteCount": 1,
        "content": "Req:  retrain the model every month+ Google-recommended best practice+ end-to-end architecture\nA. Configure Kubeflow Pipelines to schedule your multi-step workflow from training to deploying your model. : Supports all above\nB. Use a model trained and deployed on BigQuery ML, and trigger retraining with the scheduled query feature in BigQuery : Why BigQuery ML when vertexAI/kubflow can handle end to end. BigQuery ML+ traigger only initiate the code run.\nC. Write a Cloud Functions script that launches a training and deploying job on AI Platform that is triggered by Cloud Scheduler. : Not recommended by google for end to end ML\nD. Use Cloud Composer to programmatically schedule a Dataflow job that executes the workflow from training to deploying your model. : Not recommended by google for end to end ML. what if model fails? matrix monitor?"
      },
      {
        "date": "2024-06-05T22:30:00.000Z",
        "voteCount": 1,
        "content": "A) Kubeflow Pipelines is the answer."
      },
      {
        "date": "2023-12-01T04:42:00.000Z",
        "voteCount": 1,
        "content": "Chose A"
      },
      {
        "date": "2023-11-14T06:39:00.000Z",
        "voteCount": 1,
        "content": "D - Dataflow job can't deploy models\nB,C are not - are not complete solutions\nleaving A to be the correct one"
      },
      {
        "date": "2023-09-22T03:13:00.000Z",
        "voteCount": 1,
        "content": "Answer is A"
      },
      {
        "date": "2023-05-08T22:55:00.000Z",
        "voteCount": 1,
        "content": "Went with A"
      },
      {
        "date": "2023-02-24T01:36:00.000Z",
        "voteCount": 1,
        "content": "A :  Yet  the newer is Vertext-AI Pipeline built on Kubeflow"
      },
      {
        "date": "2023-02-08T01:39:00.000Z",
        "voteCount": 1,
        "content": "A : In this case, it would be a good fit as you need to retrain your model every month, which can be automated with Kubeflow Pipelines. This makes it easier to manage the entire process, from training to deploying, in a streamlined and scalable manner."
      },
      {
        "date": "2022-11-23T07:15:00.000Z",
        "voteCount": 1,
        "content": "A is correct\nAll the options get you to the required result, but only A follows the Google-recommended best practices"
      },
      {
        "date": "2022-11-01T10:59:00.000Z",
        "voteCount": 1,
        "content": "Answer is A:  Kubeflow Pipelines can form an end-to-end architecture"
      },
      {
        "date": "2022-08-15T06:25:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is \"A\""
      },
      {
        "date": "2022-02-27T19:29:00.000Z",
        "voteCount": 2,
        "content": "Community vote"
      },
      {
        "date": "2022-02-17T08:33:00.000Z",
        "voteCount": 2,
        "content": "A for me too. KF provides all the end2end tools to perform what is asked"
      },
      {
        "date": "2021-09-10T23:35:00.000Z",
        "voteCount": 2,
        "content": "A\n\nKubeflow can handle all of those things, including deploying to a model endpoint for real-time serving."
      },
      {
        "date": "2021-07-18T05:17:00.000Z",
        "voteCount": 6,
        "content": "ANS: A\nhttps://medium.com/google-cloud/how-to-build-an-end-to-end-propensity-to-purchase-solution-using-bigquery-ml-and-kubeflow-pipelines-cd4161f734d9#75c7\n\nTo automate this model-building process, you will orchestrate the pipeline using Kubeflow Pipelines, \u2018a platform for building and deploying portable, scalable machine learning (ML) workflows based on Docker containers.\u2019"
      },
      {
        "date": "2021-09-09T01:58:00.000Z",
        "voteCount": 1,
        "content": "I think both A and D are correct because it is just different fashion of doing ML ..."
      },
      {
        "date": "2021-10-09T20:36:00.000Z",
        "voteCount": 2,
        "content": "But D doesn't follow Google best practices"
      },
      {
        "date": "2021-10-24T03:23:00.000Z",
        "voteCount": 3,
        "content": "Answer seems to be A really. Here is a link from Google-recommended best practices. They are talking about Vertex AI Pipelines, which are essentially Kubeflow. \n\nhttps://cloud.google.com/architecture/ml-on-gcp-best-practices?hl=en#machine-learning-workflow-orchestration"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 9,
    "url": "https://www.examtopics.com/discussions/google/view/55580-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You are developing ML models with AI Platform for image segmentation on CT scans. You frequently update your model architectures based on the newest available research papers, and have to rerun training on the same dataset to benchmark their performance. You want to minimize computation costs and manual intervention while having version control for your code. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Functions to identify changes to your code in Cloud Storage and trigger a retraining job.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the gcloud command-line tool to submit training jobs on AI Platform when you update your code.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Build linked with Cloud Source Repositories to trigger retraining when new code is pushed to the repository.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an automated workflow in Cloud Composer that runs daily and looks for changes in code in Cloud Storage using a sensor."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-07-18T05:34:00.000Z",
        "voteCount": 26,
        "content": "ANS:C\n\nCI/CD for Kubeflow pipelines.\n\nAt the heart of this architecture is Cloud Build, infrastructure. Cloud Build can import source from Cloud Source Repositories, GitHub, or Bitbucket, and then execute a build to your specifications, and produce artifacts such as Docker containers or Python tar files."
      },
      {
        "date": "2021-09-09T02:22:00.000Z",
        "voteCount": 3,
        "content": "I think B might be make sense if they have compute concern, there might be many version change but not all that you want to trigger compute"
      },
      {
        "date": "2021-06-18T11:28:00.000Z",
        "voteCount": 10,
        "content": "Should be C\nhttps://cloud.google.com/architecture/architecture-for-mlops-using-tfx-kubeflow-pipelines-and-cloud-build#cicd_architecture"
      },
      {
        "date": "2024-09-24T23:03:00.000Z",
        "voteCount": 1,
        "content": "Req :frequently rerun training +  minimise computation costs + 0 manual intervention + version control for your code\nA. Use Cloud Functions to identify changes to your code in Cloud Storage and trigger a retraining job. : No version control\nB. Use the gcloud command-line tool to submit training jobs on AI Platform when you update your code. : Needs manual intervention to gcloud cli code submission\nC. Use Cloud Build linked with Cloud Source Repositories to trigger retraining when new code is pushed to the repository.  Yes, connects to github like Vcontrols, automated=0 manual intervention + can initiate upon code changes + cost(not sure compared to other options)\nD. Create an automated workflow in Cloud Composer that runs daily and looks for changes in code in Cloud Storage using a sensor. : Sensor?? too much . also none of req meets."
      },
      {
        "date": "2024-06-05T22:32:00.000Z",
        "voteCount": 1,
        "content": "C) It is the only answer with version control."
      },
      {
        "date": "2024-01-16T07:10:00.000Z",
        "voteCount": 1,
        "content": "I mean C is indeed the most logical, but i do not see anything relevant to cost concern. Anyone has any explanation?"
      },
      {
        "date": "2023-05-08T22:55:00.000Z",
        "voteCount": 1,
        "content": "Went with C"
      },
      {
        "date": "2023-03-30T06:59:00.000Z",
        "voteCount": 1,
        "content": "C follows a best practice, B is a manual step"
      },
      {
        "date": "2022-11-23T07:30:00.000Z",
        "voteCount": 1,
        "content": "C is the correct answer, it's the Google recommended approach;\nChecking for changes in code without using Cloud Source Repository is a bad choice, so no A and B;\nCloud Composer is an overkill, so no D."
      },
      {
        "date": "2022-11-01T11:01:00.000Z",
        "voteCount": 1,
        "content": "Answer is C"
      },
      {
        "date": "2022-08-15T06:26:00.000Z",
        "voteCount": 2,
        "content": "Correct answer is \"C\""
      },
      {
        "date": "2022-06-12T06:54:00.000Z",
        "voteCount": 1,
        "content": "C is the best answer because \"having version control for your code\""
      },
      {
        "date": "2022-02-27T19:30:00.000Z",
        "voteCount": 3,
        "content": "Community vote"
      },
      {
        "date": "2022-01-06T18:46:00.000Z",
        "voteCount": 1,
        "content": "C cloudbuild"
      },
      {
        "date": "2021-12-12T07:48:00.000Z",
        "voteCount": 1,
        "content": "B is definitely wrong because it will require manual intervention.Question specifically states the objective of minimal manual intervention. C is the way to go."
      },
      {
        "date": "2021-12-06T03:41:00.000Z",
        "voteCount": 1,
        "content": "My answer is C.\n\nCi/CD/CT is executed in Cloud Build."
      },
      {
        "date": "2021-10-18T06:20:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2021-09-10T23:38:00.000Z",
        "voteCount": 2,
        "content": "C\n\nCloud Build + Source Repository triggers for CI/CD"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 10,
    "url": "https://www.examtopics.com/discussions/google/view/54830-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "Your team needs to build a model that predicts whether images contain a driver's license, passport, or credit card. The data engineering team already built the pipeline and generated a dataset composed of 10,000 images with driver's licenses, 1,000 images with passports, and 1,000 images with credit cards. You now have to train a model with the following label map: [`\u02dcdrivers_license', `\u02dcpassport', `\u02dccredit_card']. Which loss function should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCategorical hinge",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBinary cross-entropy",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCategorical cross-entropy\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSparse categorical cross-entropy"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 18,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 18,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-06-24T12:47:00.000Z",
        "voteCount": 20,
        "content": "Answer is C"
      },
      {
        "date": "2021-06-30T20:07:00.000Z",
        "voteCount": 9,
        "content": "Use sparse categorical crossentropy when your classes are mutually exclusive (e.g. when each sample belongs exactly to one class) and categorical crossentropy when one sample can have multiple classes or labels are soft probabilities (like [0.5, 0.3, 0.2])."
      },
      {
        "date": "2021-10-17T04:38:00.000Z",
        "voteCount": 8,
        "content": "Definitely C - the target variable label formulated in the question requires a categorical cross entropy loss function i.e. 3 columns 'drivers_license' ,  'passport', 'credit_card' that can take values 1, 0. Meanwhile sparse categorical cross entropy would require the labels to be integer encoded in a single vector, for example, 'drivers_license' = 1,  'passport' = 2, 'credit_card' = 3."
      },
      {
        "date": "2023-07-08T07:32:00.000Z",
        "voteCount": 1,
        "content": "Actually it is exactly the opposite. Your label map has 3 options which are mutually exclusive. A document cannot be both - a driver license and a passport. There is a SPARSE vector as output - only one of the categorical outputs is valid for a one example."
      },
      {
        "date": "2023-07-08T08:14:00.000Z",
        "voteCount": 1,
        "content": "No, I'm sorry, I wrote it before checking - You were right. We use sparse categorical cross entropy when we have just an index (integer) as a label. The only difference is that it decodes the integer into one hot representation that suites to out DNN output."
      },
      {
        "date": "2021-06-07T10:11:00.000Z",
        "voteCount": 10,
        "content": "answer is D \nhttps://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/"
      },
      {
        "date": "2021-08-11T05:36:00.000Z",
        "voteCount": 3,
        "content": "Use sparse categorical crossentropy when your classes are mutually exclusive (e.g. when each sample belongs exactly to one class) and categorical crossentropy when one sample can have multiple classes or labels are soft probabilities (like [0.5, 0.3, 0.2])."
      },
      {
        "date": "2022-03-28T03:40:00.000Z",
        "voteCount": 2,
        "content": "Literally from the link you posted: \n\"A possible cause of frustration when using cross-entropy with classification problems with a large number of labels is the one hot encoding process. [...] This can mean that the target element of each training example may require a one hot encoded vector with tens or hundreds of thousands of zero values, requiring significant memory. Sparse cross-entropy addresses this by performing the same cross-entropy calculation of error, without requiring that the target variable be one hot encoded prior to training\".\nHere we have 3 categories...No problem doing one-hot encoding. Answer: C"
      },
      {
        "date": "2024-07-03T00:03:00.000Z",
        "voteCount": 1,
        "content": "C needs the target to be One hot encoded already. Since it is not, the answer is D"
      },
      {
        "date": "2024-06-05T22:40:00.000Z",
        "voteCount": 1,
        "content": "C) Multi-Class Classification (Three or More Classes):\nSince you have three classes, you should use a multi-class loss function.\nThe most common choice for multi-class image classification is categorical cross-entropy2.\nCategorical cross-entropy is designed for scenarios where each input belongs to exactly one class (i.e., mutually exclusive classes).\nTherefore, the correct answer is C. Categorical cross-entropy. It\u2019s well-suited for multi-class classification tasks like this one.\nReferences:\nHow to Choose Loss Functions When Training Deep Learning Neural Networks (https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/)\nStack Exchange: How to know which loss function is suitable for image classification? (https://datascience.stackexchange.com/questions/58138/how-to-know-which-loss-function-is-suitable-for-image-classification)"
      },
      {
        "date": "2024-04-21T00:35:00.000Z",
        "voteCount": 1,
        "content": "I'd go with C. Categorical cross entropy is used when classes are mutually exclusive. If the number of classes was very high, then we could use sparse categorical cross entropy."
      },
      {
        "date": "2024-04-12T07:37:00.000Z",
        "voteCount": 2,
        "content": "Use sparse categorical crossentropy when your classes are mutually exclusive (e.g. when each sample belongs exactly to one class) and categorical crossentropy when one sample can have multiple classes or labels are soft probabilities (like [0.5, 0.3, 0.2])."
      },
      {
        "date": "2024-04-21T10:59:00.000Z",
        "voteCount": 2,
        "content": "A. Categorical hinge : Mainly for SVM soft margins\nB. Binary cross-entropy : for 2 class only\nC. Categorical cross-entropy: Multi class but not necessarily Mutually exclusive\nD. Sparse categorical cross-entropy : Multi class + Mutually exclusive only , saves memory too"
      },
      {
        "date": "2024-04-21T11:02:00.000Z",
        "voteCount": 1,
        "content": "https://www.tensorflow.org/api_docs/python/tf/keras/losses/categorical_crossentropy\nhttps://www.tensorflow.org/api_docs/python/tf/keras/metrics/sparse_categorical_crossentropy"
      },
      {
        "date": "2024-04-04T00:56:00.000Z",
        "voteCount": 1,
        "content": "C\nD is for integer value instead of one-hot encoded vectors, in our question, it is 'drivers_license', 'passport', 'credit_card' one-hot."
      },
      {
        "date": "2024-02-29T14:15:00.000Z",
        "voteCount": 1,
        "content": "It depends on how the labels are encoded. If onehot use CCE. If its a single integer representing the class use SCCE (Source: same as in the official (wrong) answer)\nFrom the question it's not clear how the labels are encoded. But for just 3 classes there is no doubt it's better to go with one-hot encoding. Memory restrictions or a huge number of classes might point to SCCE"
      },
      {
        "date": "2024-01-24T01:11:00.000Z",
        "voteCount": 2,
        "content": "You now HAVE TO to train a model with the following label map: [`\u02dcdrivers_license', `\u02dcpassport', `\u02dccredit_card']."
      },
      {
        "date": "2023-11-14T07:01:00.000Z",
        "voteCount": 1,
        "content": "If you are wondering between C &amp; D - think about what \"sparse\" means\nIt is used when dealing with hundreds of categories"
      },
      {
        "date": "2023-10-29T09:13:00.000Z",
        "voteCount": 1,
        "content": "mutually exclusive classes"
      },
      {
        "date": "2023-10-09T22:51:00.000Z",
        "voteCount": 2,
        "content": "In this case, we have a multi-class classification problem with three classes: driver's license, passport, and credit card. Therefore, we should use the categorical cross-entropy loss function to train our model.\n\nSparse categorical cross-entropy is used for multi-class classification problems where the labels are represented in a sparse matrix format. This is not the case in this problem."
      },
      {
        "date": "2023-09-23T16:28:00.000Z",
        "voteCount": 1,
        "content": "Only 3 categories of values being either T or F. They don't really need to be integer encoded, which differs sparse cross-entropy from categorical."
      },
      {
        "date": "2023-09-02T12:00:00.000Z",
        "voteCount": 1,
        "content": "https://fmorenovr.medium.com/sparse-categorical-cross-entropy-vs-categorical-cross-entropy-ea01d0392d28"
      },
      {
        "date": "2023-09-02T12:01:00.000Z",
        "voteCount": 1,
        "content": "categorical_crossentropy (cce) produces a one-hot array containing the probable match for each category,\n\nsparse_categorical_crossentropy (scce) produces a category index of the most likely matching category."
      },
      {
        "date": "2023-08-12T09:26:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is: C. Categorical cross-entropy.\n\nyou are dealing with a multi-class classification problem where each image can belong to one of three classes: \"driver's license,\" \"passport,\" or \"credit card.\" Categorical cross-entropy is the appropriate loss function for multi-class classification tasks. It measures the dissimilarity between the predicted class probabilities and the true class labels. It's designed to penalize larger errors in predicted probabilities and help the model converge towards more accurate predictions."
      },
      {
        "date": "2023-07-08T22:42:00.000Z",
        "voteCount": 1,
        "content": "Req : Multi class + mutually exclusive labels\nA. Categorical hinge : Mainly for SVM soft margins\nB. Binary cross-entropy : for 2 class only\nC. Categorical cross-entropy: Multi class but not necessarily Mutually exclusive\nD. Sparse categorical cross-entropy : Multi class + Mutually exclusive  only , saves memory too"
      },
      {
        "date": "2023-06-23T08:19:00.000Z",
        "voteCount": 1,
        "content": "it's C"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 11,
    "url": "https://www.examtopics.com/discussions/google/view/54832-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You are designing an ML recommendation model for shoppers on your company's ecommerce website. You will use Recommendations AI to build, test, and deploy your system. How should you develop recommendations that increase revenue while following best practices?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the \u05d2\u20acOther Products You May Like\u05d2\u20ac recommendation type to increase the click-through rate.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the \u05d2\u20acFrequently Bought Together\u05d2\u20ac recommendation type to increase the shopping cart size for each order.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImport your user events and then your product catalog to make sure you have the highest quality event stream.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBecause it will take time to collect and record product data, use placeholder values for the product catalog to test the viability of the model."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 10,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-06-15T10:50:00.000Z",
        "voteCount": 19,
        "content": "Answer should be B\nhttps://cloud.google.com/recommendations-ai/docs/placements#rps"
      },
      {
        "date": "2021-07-18T17:02:00.000Z",
        "voteCount": 7,
        "content": "ANS:B\nhttps://cloud.google.com/recommendations-ai/docs/placements#fbt\nFrequently bought together (shopping cart expansion)\nThe \"Frequently bought together\" recommendation predicts items frequently bought together for a specific product within the same shopping session. If a list of products is being viewed, then it predicts items frequently bought with that product list.\n\nThis recommendation is useful when the user has indicated an intent to purchase a particular product (or list of products) already, and you are looking to recommend complements (as opposed to substitutes). This recommendation is commonly displayed on the \"add to cart\" page, or on the \"shopping cart\" or \"registry\" pages (for shopping cart expansion)."
      },
      {
        "date": "2024-08-24T09:48:00.000Z",
        "voteCount": 1,
        "content": "Answer is B\n\nFrequently Bought Together (shopping cart expansion) model is recommended to increase revenue\nThe option C is wrong because we should import the catalog first, then bring the user events\notherwise the events will be unjoined\nhttps://cloud.google.com/retail/docs/user-events#retail-reqs"
      },
      {
        "date": "2024-06-24T03:55:00.000Z",
        "voteCount": 1,
        "content": "ans is B"
      },
      {
        "date": "2024-06-05T22:43:00.000Z",
        "voteCount": 1,
        "content": "B) To increase revenue, expand shopping cart with other items frequently bought together."
      },
      {
        "date": "2024-06-05T22:43:00.000Z",
        "voteCount": 1,
        "content": "B) To increase revenue, expand shopping cart with other items frequently bought together."
      },
      {
        "date": "2023-07-08T22:49:00.000Z",
        "voteCount": 1,
        "content": "Req:  ML Recommendations + increase revenue + best practices\nA. Use the \u05d2\u20acOther Products You May Like\u05d2\u20ac recommendation type to increase the click-through rate. : You may like ? No\nB. Use the \u05d2\u20acFrequently Bought Together\u05d2\u20ac recommendation type to increase the shopping cart size for each order. : Viable with companies purchase information. Also this is the basic recommendation to get started with : cross sell and upsell\nC. Import your user events and then your product catalog to make sure you have the highest quality event stream. : Ensuring quality? This makes sure the data quality. Not bringing more sales much\nD. Because it will take time to collect and record product data, use placeholder values for the product catalog to test the viability of the model. : dummy values to replace for now? No value added to sales."
      },
      {
        "date": "2023-05-08T22:56:00.000Z",
        "voteCount": 1,
        "content": "Went with B"
      },
      {
        "date": "2023-03-23T09:42:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/recommendations-ai/docs/overview"
      },
      {
        "date": "2022-11-23T08:41:00.000Z",
        "voteCount": 1,
        "content": "B directly impact the revenue"
      },
      {
        "date": "2022-08-15T06:28:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is \"B\""
      },
      {
        "date": "2022-02-27T19:39:00.000Z",
        "voteCount": 2,
        "content": "Community vote"
      },
      {
        "date": "2022-01-06T19:04:00.000Z",
        "voteCount": 1,
        "content": "Event Data is important along with product data but I am not sure if there is a catch here, what goes first\nhttps://github.com/GoogleCloudPlatform/analytics-componentized-patterns/blob/master/retail/recommendation-system/bqml/bqml_retail_recommendation_system.ipynb"
      },
      {
        "date": "2021-11-05T04:37:00.000Z",
        "voteCount": 4,
        "content": "I don't know the correct answer, but it seems C and D are not correct:\n- \"Do not record user events for product items that have not been imported yet.\"; i.e., import your product catalog first and then your user events.  \n- \"Make sure that all required catalog information is included and correct. Do not use dummy or placeholder values.\"\nhttps://cloud.google.com/retail/recommendations-ai/docs/upload-catalog#catalog_import_best_practices\n\nI think the correct answer is B, because the \"default optimization objective\" for FBT is \"revenue per order\", whereas the \"default optimization objective\" for OYML is \"click-through rate\".\nhttps://cloud.google.com/retail/recommendations-ai/docs/placements#fbt"
      },
      {
        "date": "2021-10-18T10:48:00.000Z",
        "voteCount": 1,
        "content": "Sense is B"
      },
      {
        "date": "2021-06-07T10:19:00.000Z",
        "voteCount": 5,
        "content": "the correct answer should be C \nthere is a diagram on the webpage, discuss how it works https://cloud.google.com/recommendations"
      },
      {
        "date": "2021-07-28T11:15:00.000Z",
        "voteCount": 2,
        "content": "I think B is the correct answer instead of C, since B directly contributes to increasing revenue."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 12,
    "url": "https://www.examtopics.com/discussions/google/view/54658-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You are designing an architecture with a serverless ML system to enrich customer support tickets with informative metadata before they are routed to a support agent. You need a set of models to predict ticket priority, predict ticket resolution time, and perform sentiment analysis to help agents make strategic decisions when they process support requests. Tickets are not expected to have any domain-specific terms or jargon.<br>The proposed architecture has the following flow:<br><img src=\"/assets/media/exam-media/03841/0000800001.png\" class=\"in-exam-image\"><br>Which endpoints should the Enrichment Cloud Functions call?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1 = AI Platform, 2 = AI Platform, 3 = AutoML Vision",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1 = AI Platform, 2 = AI Platform, 3 = AutoML Natural Language",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1 = AI Platform, 2 = AI Platform, 3 = Cloud Natural Language API\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1 = Cloud Natural Language API, 2 = AI Platform, 3 = Cloud Vision API"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 21,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-07-18T16:53:00.000Z",
        "voteCount": 28,
        "content": "ANS: C\n\nhttps://cloud.google.com/architecture/architecture-of-a-serverless-ml-model#architecture\nThe architecture has the following flow:\nA user writes a ticket to Firebase, which triggers a Cloud Function.\n-The Cloud Function calls 3 different endpoints to enrich the ticket:\n-An AI Platform endpoint, where the function can predict the priority.\n-An AI Platform endpoint, where the function can predict the resolution time.\n-The Natural Language API to do sentiment analysis and word salience.\n-For each reply, the Cloud Function updates the Firebase real-time database.\n-The Cloud Function then creates a ticket into the helpdesk platform using the RESTful API."
      },
      {
        "date": "2021-06-07T10:32:00.000Z",
        "voteCount": 17,
        "content": "the answer should be C. The tickets do not include specific terms , which means, it doesn't need to be custom built. thus, we can use cloud NLP API instead of automl NLP."
      },
      {
        "date": "2024-09-24T06:35:00.000Z",
        "voteCount": 1,
        "content": "ANS: C\nTickets are not expected to have any domain-specific terms or jargon. Therefore we can use the Natural Language API, and we don't need to train our own model."
      },
      {
        "date": "2024-06-05T22:51:00.000Z",
        "voteCount": 2,
        "content": "C) Eliminate A and D as not vision or images required. From B (Auto ML Natural Language) requires custom training and C) NLP API gives you sentiment analysis out of the box."
      },
      {
        "date": "2023-11-14T07:08:00.000Z",
        "voteCount": 3,
        "content": "C - as Natural Language API has sentiment analysis \nand using the API over a custom model is always preferred"
      },
      {
        "date": "2023-07-08T23:00:00.000Z",
        "voteCount": 3,
        "content": "Req :  serverless ML system +  models to (predict ticket priority -predict ticket resolution time- perform sentiment analysis )\nThe proposed architecture has the following flow:\n\nA. 1 = AI Platform, 2 = AI Platform, 3 = AutoML Vision. : No image data as input here. Only text (NLP)\nB. 1 = AI Platform, 2 = AI Platform, 3 = AutoML Natural Language : Only sentiment for 3rd endpoint. No custom model needed : https://cloud.google.com/natural-language/automl/docs/beginners-guide . So autoML not required\nC. 1 = AI Platform, 2 = AI Platform, 3 = Cloud Natural Language API : 1- for classification(priority :high low medium), 2- ticket time-regression -3- sentiment analysis the CNL api is enough \nD. 1 = Cloud Natural Language API, 2 = AI Platform, 3 = Cloud Vision API : No image data"
      },
      {
        "date": "2023-05-08T22:56:00.000Z",
        "voteCount": 1,
        "content": "Went with C"
      },
      {
        "date": "2022-12-15T14:08:00.000Z",
        "voteCount": 2,
        "content": "ANS: C\n\nThis is the exact solution by Google: https://web.archive.org/web/20210618072649/https://cloud.google.com/architecture/architecture-of-a-serverless-ml-model#architecture"
      },
      {
        "date": "2022-12-15T01:13:00.000Z",
        "voteCount": 1,
        "content": "ANS: B As you need to train custom regression models (Auto ML), as NLP API is not going to be able to rank your Priority and eval the Time."
      },
      {
        "date": "2022-12-15T01:11:00.000Z",
        "voteCount": 1,
        "content": "ANS: C as NLP API is not able to perform custom Regression Models (predict time) and Priority. You need Auto ML o train your own"
      },
      {
        "date": "2022-11-23T08:51:00.000Z",
        "voteCount": 2,
        "content": "AI Platform (now Vertex AI) for both the predictions and Natural Language API for sentiment analysis since there are no specific terms (so no need to custom build something with an AutoML), so C"
      },
      {
        "date": "2022-08-15T06:31:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is \"C\""
      },
      {
        "date": "2022-06-12T07:36:00.000Z",
        "voteCount": 2,
        "content": "- by options eliminations A,D must be dropped we have no vision tasks in this system\n- answer between B,C , question stated \"no specific domain or jargon\"  so natural laguage api is prefered over automl since there no custom entinites or custom training , so I vote for C"
      },
      {
        "date": "2022-02-27T19:41:00.000Z",
        "voteCount": 4,
        "content": "Community vote"
      },
      {
        "date": "2021-12-06T04:03:00.000Z",
        "voteCount": 2,
        "content": "Mine is C.\n\nPriority prediction is categorical. Resolution time is linear regression. Sentiment is a NLP problem."
      },
      {
        "date": "2021-06-15T11:01:00.000Z",
        "voteCount": 1,
        "content": "Should be B, don't forget the domain specific terms and jargons\nhttps://medium.com/google-cloud/analyzing-sentiment-of-text-with-domain-specific-vocabulary-and-topics-726b8f287aef"
      },
      {
        "date": "2021-07-01T06:24:00.000Z",
        "voteCount": 7,
        "content": "the question said \"Tickets are not expected to have any domain-specific terms or jargon.\""
      },
      {
        "date": "2021-06-05T12:40:00.000Z",
        "voteCount": 1,
        "content": "not sure if I agree with b, I think D is a better choice"
      },
      {
        "date": "2021-06-06T15:48:00.000Z",
        "voteCount": 2,
        "content": "predict ticket priority (AI plateform : classification), predict ticket resolution time  (AI plateform : regression), and perform sentiment analysis ( Cloud NLP API )"
      },
      {
        "date": "2021-07-28T11:20:00.000Z",
        "voteCount": 2,
        "content": "D is wrong since Cloud Vision API is not needed."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 13,
    "url": "https://www.examtopics.com/discussions/google/view/54300-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You have trained a deep neural network model on Google Cloud. The model has low loss on the training data, but is performing worse on the validation data. You want the model to be resilient to overfitting. Which strategy should you use when retraining the model?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tApply a dropout parameter of 0.2, and decrease the learning rate by a factor of 10.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tApply a L2 regularization parameter of 0.4, and decrease the learning rate by a factor of 10.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun a hyperparameter tuning job on AI Platform to optimize for the L2 regularization and dropout parameters.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun a hyperparameter tuning job on AI Platform to optimize for the learning rate, and increase the number of neurons by a factor of 2."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 26,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-06-15T16:35:00.000Z",
        "voteCount": 26,
        "content": "Should be C\nhttps://machinelearningmastery.com/introduction-to-regularization-to-reduce-overfitting-and-improve-generalization-error/"
      },
      {
        "date": "2021-06-05T12:42:00.000Z",
        "voteCount": 7,
        "content": "increasing the size of the network will make the overfitting situation worse"
      },
      {
        "date": "2023-12-01T06:09:00.000Z",
        "voteCount": 1,
        "content": "Voted C"
      },
      {
        "date": "2023-11-14T07:12:00.000Z",
        "voteCount": 2,
        "content": "A,B have very specific numbers which doesn't gurantee success \nC is best \nD - increases the size - which is not helping with overfitting"
      },
      {
        "date": "2023-07-08T23:24:00.000Z",
        "voteCount": 3,
        "content": "Req: make model resilient \n\nA. Apply a dropout parameter of 0.2, and decrease the learning rate by a factor of 10. : Might / might not work . But may not find optimal parameter set since it uses random values \nB. Apply a L2 regularization parameter of 0.4, and decrease the learning rate by a factor of 10. : Might / might not work . But may not find optimal parameter set since it uses random values \nC. Run a hyperparameter tuning job on AI Platform to optimize for the L2 regularization and dropout parameters. : l2 and dropout are regularisation method which would work. Let AI find the optimal solution on how extend these parameters should regularise. Yes this would work.\nD. Run a hyperparameter tuning job on AI Platform to optimize for the learning rate, and increase the number of neurons by a factor of 2 : AIplatform would do but adding neurons would make network nore complex. So we can eliminate this option."
      },
      {
        "date": "2023-05-20T01:48:00.000Z",
        "voteCount": 1,
        "content": "It should be C as regularization (L1/L2), early stopping and drop out are some of the ways in deep learning to handle overfitting. Other options have specific values which may or may not solve overfitting as it depends on specific use case."
      },
      {
        "date": "2023-05-08T22:57:00.000Z",
        "voteCount": 2,
        "content": "Went with C"
      },
      {
        "date": "2022-12-15T14:12:00.000Z",
        "voteCount": 2,
        "content": "ANS: C\n\nA and B are random values, why they choose that values?\nD could increase even more overfitting since you're using a more complex model."
      },
      {
        "date": "2022-11-23T08:55:00.000Z",
        "voteCount": 1,
        "content": "We don't know the optimum values for the parameters, so we need to run a hyperparameter tuning job; L2 regularization and dropout parameters are great ways to avoid overfitting.\nSo C is the answer"
      },
      {
        "date": "2022-08-15T06:32:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is \"C\""
      },
      {
        "date": "2022-06-12T07:49:00.000Z",
        "voteCount": 1,
        "content": "- by options eliminations C,D are better than A,D (more automated , scalable)\n- between C,D C is better as in D \"and increase the number of neurons by a factor of 2\" will make matters worse and increase overfitting"
      },
      {
        "date": "2022-07-03T11:35:00.000Z",
        "voteCount": 1,
        "content": "also in A,D mainly learning rate has no direct relation with overfitting"
      },
      {
        "date": "2022-04-06T03:45:00.000Z",
        "voteCount": 2,
        "content": "C for sure"
      },
      {
        "date": "2022-03-03T11:18:00.000Z",
        "voteCount": 2,
        "content": "Best practice is to let a AI Platform tool run the tuning to optimize hyperparameters. Why should I trust values in answers A or B?? Plus L2 regularization and dropout are the way to go here."
      },
      {
        "date": "2022-02-27T19:43:00.000Z",
        "voteCount": 2,
        "content": "Community vote"
      },
      {
        "date": "2022-01-23T20:32:00.000Z",
        "voteCount": 3,
        "content": "it is the logical ans"
      },
      {
        "date": "2022-01-19T03:01:00.000Z",
        "voteCount": 3,
        "content": "regularization and dropout"
      },
      {
        "date": "2022-01-06T19:27:00.000Z",
        "voteCount": 2,
        "content": "Increasing Neurons or layers / network will increase overfitting, it is good for under fitting. C should be fine."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 14,
    "url": "https://www.examtopics.com/discussions/google/view/54595-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You built and manage a production system that is responsible for predicting sales numbers. Model accuracy is crucial, because the production model is required to keep up with market changes. Since being deployed to production, the model hasn't changed; however the accuracy of the model has steadily deteriorated.<br>What issue is most likely causing the steady decline in model accuracy?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPoor data quality",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLack of model retraining\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tToo few layers in the model for capturing information",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncorrect data split ratio during model training, evaluation, validation, and test"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 28,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-06-05T03:56:00.000Z",
        "voteCount": 30,
        "content": "B. Retraining is needed as the market is changing."
      },
      {
        "date": "2021-07-22T03:30:00.000Z",
        "voteCount": 11,
        "content": "I also think it is B - who is giving the \"correct\" answers to the questions? I feel like 4 out of 5 of them are incorrect."
      },
      {
        "date": "2021-12-26T19:11:00.000Z",
        "voteCount": 16,
        "content": "the biggest issue of this website is `all correct answers` are wrong"
      },
      {
        "date": "2024-09-03T00:00:00.000Z",
        "voteCount": 1,
        "content": "Model is not updated to current sales trends."
      },
      {
        "date": "2024-08-02T15:30:00.000Z",
        "voteCount": 1,
        "content": "B naturally"
      },
      {
        "date": "2024-06-05T23:04:00.000Z",
        "voteCount": 2,
        "content": "B) You require model monitoring to identify changes and at the right time retrain the model with new data to avoid model drift."
      },
      {
        "date": "2024-03-31T17:43:00.000Z",
        "voteCount": 1,
        "content": "The market can be dynamic, Sales trends, customer preferences, and even competitor strategies might evolve over time but our model hasn't changed since the deployment so our model can adapt with these changes by retraining only\nDegradation Over Time: Without retraining to adapt to these changes, the model's predictions become less accurate as the real world diverges from the data it was trained on."
      },
      {
        "date": "2024-01-23T02:03:00.000Z",
        "voteCount": 1,
        "content": "As the consistent changes in the Market data, the Model in Production should regularly retrain for better results. Option B is the right choice"
      },
      {
        "date": "2023-12-01T06:10:00.000Z",
        "voteCount": 1,
        "content": "Keeping the model up to date is crucial. So - B."
      },
      {
        "date": "2023-11-14T07:15:00.000Z",
        "voteCount": 1,
        "content": "B because the environment is changing and the model only captures past performance"
      },
      {
        "date": "2023-07-08T23:30:00.000Z",
        "voteCount": 2,
        "content": "Situation : model trained long before.\nQ : why accuracy of the model has steadily deteriorated.\n\nA. Poor data quality :  Model perfomance depens on trained model only. Quality issue should be taken care by pipeline and it do not much affect the model to cause a performance slow down over time\nB. Lack of model retraining : Very obvious\nC. Too few layers in the model for capturing information : If so model wpould not have been deployed at first stage due to low performance on unseen data\nD. Incorrect data split ratio during model training, evaluation, validation, and test : This is relevant only at training when model deployed at first place, We have way passed that. Not not the reason."
      },
      {
        "date": "2023-05-08T22:57:00.000Z",
        "voteCount": 1,
        "content": "Went with B"
      },
      {
        "date": "2023-03-22T00:52:00.000Z",
        "voteCount": 1,
        "content": "B is correct. Model needs to keep up with the market changes, implying that the underlying data distribution would be changing as well. Hence retrain the model."
      },
      {
        "date": "2023-03-07T07:57:00.000Z",
        "voteCount": 1,
        "content": "The questions says the model is required to keep up with market changes, hence retraining needed."
      },
      {
        "date": "2022-12-23T04:35:00.000Z",
        "voteCount": 1,
        "content": "B is the correct answer"
      },
      {
        "date": "2022-12-15T14:13:00.000Z",
        "voteCount": 1,
        "content": "ANS: B"
      },
      {
        "date": "2022-11-23T09:08:00.000Z",
        "voteCount": 1,
        "content": "Data distribution changes over time and so should do the model, so B is the correct answer"
      },
      {
        "date": "2022-08-15T06:33:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is \"B\""
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 15,
    "url": "https://www.examtopics.com/discussions/google/view/54659-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You have been asked to develop an input pipeline for an ML training model that processes images from disparate sources at a low latency. You discover that your input data does not fit in memory. How should you create a dataset following Google-recommended best practices?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a tf.data.Dataset.prefetch transformation.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConvert the images to tf.Tensor objects, and then run Dataset.from_tensor_slices().",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConvert the images to tf.Tensor objects, and then run tf.data.Dataset.from_tensors().",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConvert the images into TFRecords, store the images in Cloud Storage, and then use the tf.data API to read the images for training.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 24,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-06-15T18:16:00.000Z",
        "voteCount": 19,
        "content": "Should be D"
      },
      {
        "date": "2021-12-07T04:23:00.000Z",
        "voteCount": 14,
        "content": "My option is D.\n\nCite from Google Pag: to construct a Dataset from data in memory,  use tf.data.Dataset.from_tensors() or tf.data.Dataset.from_tensor_slices(). When input data is stored in a file (not in memory), the recommended TFRecord format, you can use tf.data.TFRecordDataset().\n\ntf.data.Dataset is for data in memory.\ntf.data.TFRecordDataset is for data in non-memory storage."
      },
      {
        "date": "2024-06-05T23:52:00.000Z",
        "voteCount": 2,
        "content": "D) Storing images in TFRecords optimises storage for images."
      },
      {
        "date": "2024-04-12T07:41:00.000Z",
        "voteCount": 1,
        "content": "tf.data.Dataset is for data in memory.\ntf.data.TFRecordDataset is for data in non-memory storage."
      },
      {
        "date": "2024-03-06T03:13:00.000Z",
        "voteCount": 3,
        "content": "why this website shows wrong option as answer, this is my observation from so many questions?"
      },
      {
        "date": "2023-12-01T06:11:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2023-11-14T07:17:00.000Z",
        "voteCount": 2,
        "content": "D because:\ntf.data.Dataset is for data in memory.\ntf.data.TFRecordDataset is for data in non-memory storage."
      },
      {
        "date": "2023-11-01T08:40:00.000Z",
        "voteCount": 2,
        "content": "all \"correct\" answers are wrong"
      },
      {
        "date": "2023-05-08T22:57:00.000Z",
        "voteCount": 1,
        "content": "Went with D"
      },
      {
        "date": "2023-04-08T23:53:00.000Z",
        "voteCount": 2,
        "content": "For all questions the given answers and voted answers are different. Which one should be considered for exam?"
      },
      {
        "date": "2023-04-20T05:16:00.000Z",
        "voteCount": 2,
        "content": "You should consider the voted ones."
      },
      {
        "date": "2023-02-05T15:33:00.000Z",
        "voteCount": 1,
        "content": "Converting your data into TFRecord has many advantages, such as: More efficient storage: the TFRecord data can take up less space than the original data; it can also be partitioned into multiple files. Fast I/O: the TFRecord format can be read with parallel I/O operations, which is useful for TPUs or multiple hosts"
      },
      {
        "date": "2023-02-05T15:31:00.000Z",
        "voteCount": 1,
        "content": "my option is D"
      },
      {
        "date": "2022-12-26T04:44:00.000Z",
        "voteCount": 1,
        "content": "Ans: D"
      },
      {
        "date": "2022-12-15T14:15:00.000Z",
        "voteCount": 1,
        "content": "ans: D"
      },
      {
        "date": "2022-11-24T00:29:00.000Z",
        "voteCount": 1,
        "content": "For data in memory use tf.data.Dataset, for data in non-memory storage use tf.data.TFRecordDataset.\nSince data don't fit in memory, go with option D."
      },
      {
        "date": "2022-08-15T06:33:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is \"D\""
      },
      {
        "date": "2022-06-12T08:01:00.000Z",
        "voteCount": 2,
        "content": "- by options eliminations A is the first option to be dropped , prefetch will use additional memory overhead to buffer images\n- answer in B,C,D but D is the best answer as we save the huge images dataset on gcs then load batches of data for training\n- B,C not good as they did not provide a solution to images are not fit in memory"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 16,
    "url": "https://www.examtopics.com/discussions/google/view/54591-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You are an ML engineer at a large grocery retailer with stores in multiple regions. You have been asked to create an inventory prediction model. Your model's features include region, location, historical demand, and seasonal popularity. You want the algorithm to learn from new inventory data on a daily basis. Which algorithms should you use to build the model?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tClassification",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReinforcement Learning",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRecurrent Neural Networks (RNN)\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConvolutional Neural Networks (CNN)"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-06-05T03:54:00.000Z",
        "voteCount": 28,
        "content": "The answer is C. Use RNN because it is a time series analysis."
      },
      {
        "date": "2021-10-09T07:01:00.000Z",
        "voteCount": 7,
        "content": "As Y2Data pointed out, your reasoning for choosing B does not make much sense. \n\nFurthermore, Reinforcement Learning for this question does not make much sense to me. Reinforcement Learning is basically agent - task problems. You give the agent a task i.e. get out of a maze and then through trial and error and many many iterations the agent learns the correct way to perform the task. It is called Reinforcement because you ... well ... reinforce the agent, you reward the agent for correct choices and penalize for incorrect choices. In RL you dont use many / any previous data because the data is generated with each iteration I think."
      },
      {
        "date": "2024-09-27T12:00:00.000Z",
        "voteCount": 1,
        "content": "I chose B because the model need to learn"
      },
      {
        "date": "2024-06-26T23:30:00.000Z",
        "voteCount": 1,
        "content": "I would choose A. And it is only because the features already have time-series information (like demand). And it would be way easier to train XGBoost than RNN model."
      },
      {
        "date": "2024-06-06T00:03:00.000Z",
        "voteCount": 1,
        "content": "C) The best choice for this scenario would be C. Recurrent Neural Networks (RNN).\n\nRationale:\n\nThe task at hand is a time-series prediction problem, where the goal is to predict future inventory levels based on historical data. RNNs are particularly well-suited for such tasks because they have \u201cmemory\u201d and can learn patterns in sequential data1.\nFeatures like region, location, historical demand, and seasonal popularity can be used as input to the RNN. The network can then learn the temporal dependencies between these features and the inventory levels.\nRNNs can be trained incrementally, which means the model can be updated daily with new inventory data, allowing the model to adapt to changing trends and patterns"
      },
      {
        "date": "2023-12-22T01:29:00.000Z",
        "voteCount": 1,
        "content": "go for C\nhttps://www.akkio.com/post/deep-learning-vs-reinforcement-learning-key-differences-and-use-cases#:~:text=Reinforcement%20learning%20is%20particularly%20well,of%20reinforcement%20learning%20in%20action."
      },
      {
        "date": "2023-11-14T07:22:00.000Z",
        "voteCount": 2,
        "content": "The question asks for \"prediction model\"\nclassification and RL do not fit the bill\nCNN are used for vision\nso only answer left is C"
      },
      {
        "date": "2023-07-08T20:57:00.000Z",
        "voteCount": 2,
        "content": "I'm not sure that daily basis means it is time series. It could mean updating the model daily.\nBut I'll follow collective intelligence."
      },
      {
        "date": "2023-05-08T22:58:00.000Z",
        "voteCount": 1,
        "content": "Went with C"
      },
      {
        "date": "2023-02-06T06:47:00.000Z",
        "voteCount": 1,
        "content": "Reinforcement Learning(RL) is a type of machine learning technique that enables an agent to learn in an interactive environment by trial and error using feedback from its own actions and experiences."
      },
      {
        "date": "2022-12-15T14:17:00.000Z",
        "voteCount": 1,
        "content": "ans: C"
      },
      {
        "date": "2022-11-23T09:09:00.000Z",
        "voteCount": 1,
        "content": "RNN are a fit tool to work with time-series as this one, so C"
      },
      {
        "date": "2022-08-15T06:46:00.000Z",
        "voteCount": 2,
        "content": "Correct answer is \"C\""
      },
      {
        "date": "2022-06-12T08:05:00.000Z",
        "voteCount": 1,
        "content": "\"algorithm to learn from new inventory data on a daily basis\" = time series model , best option to deal with time series is forsure RNN , vote for C"
      },
      {
        "date": "2022-04-06T03:56:00.000Z",
        "voteCount": 3,
        "content": "It's C."
      },
      {
        "date": "2022-02-02T22:50:00.000Z",
        "voteCount": 2,
        "content": "C - for time series"
      },
      {
        "date": "2021-12-07T04:32:00.000Z",
        "voteCount": 4,
        "content": "My option is B.\n\n\"You want the algorithm to learn from new inventory data on a daily basis\". The implication is a feedback with reward or punishment, which can optimise the mode. But, all other options can only practice prediction against new data rather than learning knowledge from new data automatically."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 17,
    "url": "https://www.examtopics.com/discussions/google/view/55437-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You are building a real-time prediction engine that streams files which may contain Personally Identifiable Information (PII) to Google Cloud. You want to use the<br>Cloud Data Loss Prevention (DLP) API to scan the files. How should you ensure that the PII is not accessible by unauthorized individuals?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStream all files to Google Cloud, and then write the data to BigQuery. Periodically conduct a bulk scan of the table using the DLP API.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStream all files to Google Cloud, and write batches of the data to BigQuery. While the data is being written to BigQuery, conduct a bulk scan of the data using the DLP API.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate two buckets of data: Sensitive and Non-sensitive. Write all data to the Non-sensitive bucket. Periodically conduct a bulk scan of that bucket using the DLP API, and move the sensitive data to the Sensitive bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate three buckets of data: Quarantine, Sensitive, and Non-sensitive. Write all data to the Quarantine bucket. Periodically conduct a bulk scan of that bucket using the DLP API, and move the data to either the Sensitive or Non-Sensitive bucket.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 17,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 6,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-06-16T05:58:00.000Z",
        "voteCount": 25,
        "content": "Should be D\nhttps://cloud.google.com/architecture/automating-classification-of-data-uploaded-to-cloud-storage#building_the_quarantine_and_classification_pipeline"
      },
      {
        "date": "2021-09-01T12:27:00.000Z",
        "voteCount": 1,
        "content": "All PII should be Sensitive data, that's why I think the answer is A."
      },
      {
        "date": "2022-07-13T22:17:00.000Z",
        "voteCount": 1,
        "content": "Option D, as documented in that link (a fully automated process, using Cloud Functions - rather than a \"periodic\" scan as worded in the question), would be my choice.\n\nIt's easier than B, which would work for a real-time scenario - but would require loads more custom work to implement (things like batching, segmentation, triggering).\n\nA and C are 'reactive' / periodic, and so not appropriate for the given scenario."
      },
      {
        "date": "2021-06-22T01:55:00.000Z",
        "voteCount": 5,
        "content": "D; others pose risks"
      },
      {
        "date": "2024-06-06T00:11:00.000Z",
        "voteCount": 1,
        "content": "D) The best choice for this scenario would be D. Create three buckets of data: Quarantine, Sensitive, and Non-sensitive. Write all data to the Quarantine bucket. Periodically conduct a bulk scan of that bucket using the DLP API, and move the data to either the Sensitive or Non-Sensitive bucket."
      },
      {
        "date": "2023-12-04T01:48:00.000Z",
        "voteCount": 2,
        "content": "D - Quarantine bucket is the google reccomended approach"
      },
      {
        "date": "2023-11-03T12:51:00.000Z",
        "voteCount": 1,
        "content": "Option B does not provide a clear separation between sensitive and non-sensitive data before it is written to BigQuery, which means that PII might be exposed during the process. \n\nBut, in D  offers a better level of security by writing all the data to a Quarantine bucket first. This way, the DLP API can scan and categorize the data into Sensitive or Non-sensitive buckets before it is further processed or stored. This ensures that PII is not accessible by unauthorized individuals, as the sensitive data is identified and separated from the non-sensitive data before any further actions are taken."
      },
      {
        "date": "2023-07-09T04:31:00.000Z",
        "voteCount": 1,
        "content": "real-time prediction engine,  that streams files to Google Cloud. PII is not accessible by unauthorized individuals.\nD"
      },
      {
        "date": "2023-07-07T07:06:00.000Z",
        "voteCount": 1,
        "content": "D should be the correct answer"
      },
      {
        "date": "2023-05-08T22:58:00.000Z",
        "voteCount": 2,
        "content": "Went with D"
      },
      {
        "date": "2023-04-20T05:32:00.000Z",
        "voteCount": 1,
        "content": "B is real time"
      },
      {
        "date": "2023-04-01T01:40:00.000Z",
        "voteCount": 1,
        "content": "It's D"
      },
      {
        "date": "2023-02-06T15:08:00.000Z",
        "voteCount": 2,
        "content": "A, D, C they do not apply to a realtime case, all three say that the scan is applied periodically\nThen it's B"
      },
      {
        "date": "2023-03-07T08:10:00.000Z",
        "voteCount": 1,
        "content": "Never mentioned periodically in the question, if I'm not wrong?"
      },
      {
        "date": "2023-01-11T12:36:00.000Z",
        "voteCount": 1,
        "content": "I think that is the correct because of the \"real time\" application."
      },
      {
        "date": "2022-11-24T01:20:00.000Z",
        "voteCount": 1,
        "content": "D is the right answer: you can temporarily store the sensitive data in a Quarantine bucket with restricted access, then move the data to the relative buckets once the PII have been protected."
      },
      {
        "date": "2022-08-15T06:47:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is \"D\""
      },
      {
        "date": "2022-05-24T01:00:00.000Z",
        "voteCount": 1,
        "content": "Answer is D : Question says that there MAY be sensitive data, so not all data is sensitive. This is why we need 3 buckets : Quarantine as a landing bucket, sensitive for sensitive data after DLP scan, non-sensitive for non-sensitive after DLP scan.\nhttps://cloud.google.com/architecture/automating-classification-of-data-uploaded-to-cloud-storage"
      },
      {
        "date": "2022-03-30T05:01:00.000Z",
        "voteCount": 2,
        "content": "Reason being \"Real Time' DLP scanning. Option A would scan all the data again and again. For others - Buckets etc is overkill and offline process."
      },
      {
        "date": "2022-03-23T08:34:00.000Z",
        "voteCount": 1,
        "content": "But what about the real-time element, how would that work with the quarantine?"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 18,
    "url": "https://www.examtopics.com/discussions/google/view/56782-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You work for a large hotel chain and have been asked to assist the marketing team in gathering predictions for a targeted marketing strategy. You need to make predictions about user lifetime value (LTV) over the next 20 days so that marketing can be adjusted accordingly. The customer dataset is in BigQuery, and you are preparing the tabular data for training with AutoML Tables. This data has a time signal that is spread across multiple columns. How should you ensure that<br>AutoML fits the best model to your data?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tManually combine all columns that contain a time signal into an array. AIlow AutoML to interpret this array appropriately. Choose an automatic data split across the training, validation, and testing sets.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSubmit the data for training without performing any manual transformations. AIlow AutoML to handle the appropriate transformations. Choose an automatic data split across the training, validation, and testing sets.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSubmit the data for training without performing any manual transformations, and indicate an appropriate column as the Time column. AIlow AutoML to split your data based on the time signal provided, and reserve the more recent data for the validation and testing sets.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSubmit the data for training without performing any manual transformations. Use the columns that have a time signal to manually split your data. Ensure that the data in your validation set is from 30 days after the data in your training set and that the data in your testing sets from 30 days after your validation set.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 21,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 10,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-07-25T04:03:00.000Z",
        "voteCount": 24,
        "content": "Should be D. As time signal that is spread across multiple columns so manual split is required."
      },
      {
        "date": "2021-07-28T11:35:00.000Z",
        "voteCount": 4,
        "content": "Also think it is D, since it mentioned that the time signal is spread across multiple columns."
      },
      {
        "date": "2021-10-17T14:07:00.000Z",
        "voteCount": 5,
        "content": "Correct answer is C - AutoML handles training, validation, test splits automatically for you when you specify a Time column. There is no requirement to do this manually."
      },
      {
        "date": "2021-10-24T03:16:00.000Z",
        "voteCount": 9,
        "content": "Correct answer is D. It clearly says the time signal data is spread across different columns. If it weren't then C would be correct and your point would be valid. However, in this case the answer is D 100%. \n\nhttps://cloud.google.com/automl-tables/docs/data-best-practices#time"
      },
      {
        "date": "2022-01-20T10:22:00.000Z",
        "voteCount": 1,
        "content": "this comment is only about time information in different columns, not about time itself. C is correct as for me"
      },
      {
        "date": "2022-01-20T10:24:00.000Z",
        "voteCount": 1,
        "content": "but if time signal means time mark not the business signal the D is the correct - very controversial"
      },
      {
        "date": "2024-02-26T04:06:00.000Z",
        "voteCount": 4,
        "content": "I think the answer is C. In this case I am interpreting time signal as the features that hold predictive power as a function of time i.e. time signal. There is no indication to how much data is available so using the 30 days after mark is not wise. You only have 30 days worth of data for validation set. If you have a few years worth of data this seems like a unnecessary small validation set."
      },
      {
        "date": "2021-07-01T22:49:00.000Z",
        "voteCount": 14,
        "content": "C \nYou use the Time column to tell AutoML Tables that time matters for your data; it is not randomly distributed over time. When you specify the Time column, AutoML Tables use the earliest 80% of the rows for training, the next 10% of rows for validation, and the latest 10% of rows for testing.\nAutoML Tables treats each row as an independent and identically distributed training example; setting the Time column does not change this. The Time column is used only to split the data set.\nYou must include a value for the Time column for every row in your dataset. Make sure that the Time column has enough distinct values, so that the evaluation and test sets are non-empty. Usually, having at least 20 distinct values should be sufficient.\nhttps://cloud.google.com/automl-tables/docs/prepare#time"
      },
      {
        "date": "2021-08-02T08:16:00.000Z",
        "voteCount": 2,
        "content": "From the link you provided, I think it's A :\n\nThe Time column must have a data type of Timestamp.\n\nDuring schema review, you select this column as the Time column. (In the API, you use the timeColumnSpecId field.) This selection takes effect only if you have not specified the data split column.\n\nIf you have a time-related column that you do not want to use to split your data, set the data type for that column to Timestamp but do not set it as the Time column."
      },
      {
        "date": "2024-09-30T09:34:00.000Z",
        "voteCount": 1,
        "content": "D could work, but I'm still leaning towards C"
      },
      {
        "date": "2024-08-04T19:00:00.000Z",
        "voteCount": 1,
        "content": "AutoML handles training, validation, test splits automatically for you when you specify a Time column. There is no requirement to do this manually."
      },
      {
        "date": "2024-06-06T05:48:00.000Z",
        "voteCount": 1,
        "content": "D)D is correct, as this would satisfy the days criteria mentioned in the question. 30 days is more than 20 days, and the prediction model can be used on a validation dataset to validate the results for the next 20 days."
      },
      {
        "date": "2024-01-21T11:55:00.000Z",
        "voteCount": 2,
        "content": "thinking that \"spread across multiple columns\" seems like \"columns with redundant information,\" and considering how AutoML can deal with correlated columns, I think option C is the best choice, with no need for a manual split.\n\nHowever, \"time information is not contained in a single column\" is the same thing as \"time signal that is spread across multiple columns.\" I agree that D could be the best option.\n\nThen, I tend to think that D is the best choice because the text could be more clearly expressed in redundant options."
      },
      {
        "date": "2023-11-17T09:09:00.000Z",
        "voteCount": 2,
        "content": "Either C or D but leaning towards C as not get the 30 days in D"
      },
      {
        "date": "2023-11-14T07:31:00.000Z",
        "voteCount": 2,
        "content": "\"data has a time signal that is spread across multiple columns\" - I interpret as having &gt; 1 timeseries column.\nAutoML knows how to deal with a single column but not multiple\nhence answer is D"
      },
      {
        "date": "2023-11-06T14:52:00.000Z",
        "voteCount": 1,
        "content": "Since AutoML is good enough to perform the splits, C appears to be the right answer. Moreover, time information across multiple columns which requires manual split as per option D is different from the question's scenario where the time signal is spread across multiple columns which can be hours, months, days, etc. if we can define in AutoML the right time signal column, its enojugh to split the data and pick most recent data as test data and earliest data as test data"
      },
      {
        "date": "2023-08-22T04:33:00.000Z",
        "voteCount": 2,
        "content": "A Wrong, Even if columns are combines into a 1D-array(column), the time signal should be noticed to autoML anyway. Automatic split cannot work since we need more than 20 days history\nB Wrong, Without indicating time signal to AutoML, data would leak in (time leakage) in training/validation/test sets\nC Wrong, but might be possible if time signal wouldn't have bee spread across multiple columns\nD True, because time signal is spread accross multiple columns require to manually split the data. Since we want to predict LTV over the next 20 days, it is necessary to have at least 20 days history between the splits (30 seems okay: 10 days predictions) Validating and testing on the last 2 months seems reasonable for marketing purpose (usually seasonal)."
      },
      {
        "date": "2023-07-08T21:23:00.000Z",
        "voteCount": 1,
        "content": "Why 30 days after each data sets, even though we need to predict only for 20 days?"
      },
      {
        "date": "2023-07-07T07:07:00.000Z",
        "voteCount": 1,
        "content": "Agree with kkd14. D should be the correct answer."
      },
      {
        "date": "2023-07-07T00:41:00.000Z",
        "voteCount": 1,
        "content": "As far as I understand, that AutoML table can handle time-signal column full automatically. Thus, I went to C."
      },
      {
        "date": "2023-05-08T22:59:00.000Z",
        "voteCount": 1,
        "content": "Went with D"
      },
      {
        "date": "2023-03-26T11:17:00.000Z",
        "voteCount": 1,
        "content": "C. Submit the data for training without performing any manual transformations, and indicate an appropriate column as the Time column. Allow AutoML to split your data based on the time signal provided, and reserve the more recent data for the validation and testing sets.\n\nThis approach ensures that AutoML can handle the time-based nature of the data properly. By providing the Time column, AutoML can automatically split the data in a way that respects the time-based structure, using more recent data for validation and testing. This approach is especially important for time-series data, as it helps prevent leakage of future information into the training set, ensuring a more accurate and reliable model."
      },
      {
        "date": "2023-02-06T17:26:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/automl-tables/docs/data-best-practices#time\n\n- If the time information is not contained in a single column, you can use a manual data split to use the most recent data as the test data, and the earliest data as the training data."
      },
      {
        "date": "2023-01-12T08:04:00.000Z",
        "voteCount": 1,
        "content": "I go with D: https://cloud.google.com/automl-tables/docs/data-best-practices#time  \nRead it carefully at the last paragraph of the topic: If the time information is not contained in a single column, you can use a manual data split to use the most recent data as the test data, and the earliest data as the training data."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 19,
    "url": "https://www.examtopics.com/discussions/google/view/55816-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You have written unit tests for a Kubeflow Pipeline that require custom libraries. You want to automate the execution of unit tests with each new push to your development branch in Cloud Source Repositories. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite a script that sequentially performs the push to your development branch and executes the unit tests on Cloud Run.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUsing Cloud Build, set an automated trigger to execute the unit tests when changes are pushed to your development branch.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up a Cloud Logging sink to a Pub/Sub topic that captures interactions with Cloud Source Repositories. Configure a Pub/Sub trigger for Cloud Run, and execute the unit tests on Cloud Run.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up a Cloud Logging sink to a Pub/Sub topic that captures interactions with Cloud Source Repositories. Execute the unit tests using a Cloud Function that is triggered when messages are sent to the Pub/Sub topic."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 12,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-06-22T02:14:00.000Z",
        "voteCount": 16,
        "content": "B. GCP recommends to use Cloud Build when building KubeFlow Pipelines. It's possible to run unit tests in Cloud Build. And, the others seems overly complex/unnecessary"
      },
      {
        "date": "2021-10-19T03:15:00.000Z",
        "voteCount": 7,
        "content": "B makes sense because of this: https://cloud.google.com/architecture/architecture-for-mlops-using-tfx-kubeflow-pipelines-and-cloud-build#cicd_architecture"
      },
      {
        "date": "2021-10-19T03:17:00.000Z",
        "voteCount": 2,
        "content": "The image explains a lot"
      },
      {
        "date": "2024-06-26T23:46:00.000Z",
        "voteCount": 1,
        "content": "B: No need of any Pub/Sub stuff"
      },
      {
        "date": "2024-06-06T05:54:00.000Z",
        "voteCount": 1,
        "content": "B. Cloud Build."
      },
      {
        "date": "2023-11-14T07:33:00.000Z",
        "voteCount": 2,
        "content": "B is the only sensible answer as its a feature of CloudBuild \neverything else is the delusions of a madmen"
      },
      {
        "date": "2023-07-07T00:55:00.000Z",
        "voteCount": 1,
        "content": "A, C, D need addiontal maunal tasks.\nB is correct."
      },
      {
        "date": "2023-05-18T06:33:00.000Z",
        "voteCount": 1,
        "content": "Cloud Build is the best choice but the other answers are feasible."
      },
      {
        "date": "2023-05-08T22:59:00.000Z",
        "voteCount": 1,
        "content": "Went with B"
      },
      {
        "date": "2023-02-06T17:29:00.000Z",
        "voteCount": 1,
        "content": "Because it is the most automatic of the options"
      },
      {
        "date": "2022-12-15T14:31:00.000Z",
        "voteCount": 1,
        "content": "ans: B"
      },
      {
        "date": "2022-11-24T01:53:00.000Z",
        "voteCount": 1,
        "content": "B is the Google-recommended best practice."
      },
      {
        "date": "2022-08-15T06:48:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is \"B\""
      },
      {
        "date": "2022-04-06T19:46:00.000Z",
        "voteCount": 2,
        "content": "B it is."
      },
      {
        "date": "2021-09-08T14:55:00.000Z",
        "voteCount": 5,
        "content": "Easy one, B, Cloud Build is the tool for CI/CD."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 20,
    "url": "https://www.examtopics.com/discussions/google/view/54305-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You are training an LSTM-based model on AI Platform to summarize text using the following job submission script: gcloud ai-platform jobs submit training $JOB_NAME \\<br>--package-path $TRAINER_PACKAGE_PATH \\<br>--module-name $MAIN_TRAINER_MODULE \\<br>--job-dir $JOB_DIR \\<br>--region $REGION \\<br>--scale-tier basic \\<br>-- \\<br>--epochs 20 \\<br>--batch_size=32 \\<br>--learning_rate=0.001 \\<br>You want to ensure that training time is minimized without significantly compromising the accuracy of your model. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the 'epochs' parameter.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the 'scale-tier' parameter.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the 'batch size' parameter.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the 'learning rate' parameter."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 15,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-06-22T02:20:00.000Z",
        "voteCount": 31,
        "content": "B. Changing the scale tier does not impact performance\u2013only speeds up training time. Epochs, Batch size, and learning rate all are hyperparameters that might impact model accuracy."
      },
      {
        "date": "2023-07-07T01:02:00.000Z",
        "voteCount": 1,
        "content": "A, C, D could impact the accuracy. But B not."
      },
      {
        "date": "2023-05-08T22:59:00.000Z",
        "voteCount": 1,
        "content": "Went with B"
      },
      {
        "date": "2023-02-06T17:35:00.000Z",
        "voteCount": 2,
        "content": "A is incorrect, less training iteration will affect model performance.\n\nB is correct, cost is not a concern as it is not mentioned in the question, the scale tier can be upgraded to significantly minimize the training time.\n\nC is incorrect, wouldn\u2019t affect training time, but would affect model performance.\n\nD is incorrect, the model might converge faster with higher learning rate, but this would affect the training routine and might cause exploding gradients."
      },
      {
        "date": "2023-01-11T07:24:00.000Z",
        "voteCount": 1,
        "content": "It's B!"
      },
      {
        "date": "2022-11-24T01:56:00.000Z",
        "voteCount": 2,
        "content": "A, C, D are all about hyperparameters that might impact model accuracy, while B is just about computing speed; so upgrading the scale tier will make the model faster with no chance of reducing accuracy."
      },
      {
        "date": "2022-08-15T06:48:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is \"B\""
      },
      {
        "date": "2022-06-12T14:05:00.000Z",
        "voteCount": 3,
        "content": "- using options elimination all options except B can harm the accuracy"
      },
      {
        "date": "2022-04-06T19:52:00.000Z",
        "voteCount": 2,
        "content": "B for sure."
      },
      {
        "date": "2022-02-22T12:35:00.000Z",
        "voteCount": 1,
        "content": "Might be hrlpfull https://cloud.google.com/ai-platform/training/docs/machine-types#scale_tiers\nGoogle may optimize the configuration of the scale tiers for different jobs over time, based on customer feedback and the availability of cloud resources. Each scale tier is defined in terms of its suitability for certain types of jobs. Generally, the more advanced the tier, the more machines are allocated to the cluster, and the more powerful the specifications of each virtual machine. As you increase the complexity of the scale tier, the hourly cost of training jobs, measured in training units, also increases. See the pricing page to calculate the cost of your job."
      },
      {
        "date": "2021-12-12T08:40:00.000Z",
        "voteCount": 3,
        "content": "A,C and D all point to hyper parameter tuning which is not the objective in the question.\n\nAs others have said - B is only way to improve the time to training the model."
      },
      {
        "date": "2021-11-20T08:30:00.000Z",
        "voteCount": 1,
        "content": "examtopics , Can we attach releveant docs why C ?"
      },
      {
        "date": "2021-10-19T05:59:00.000Z",
        "voteCount": 3,
        "content": "Correct is B, scale-tier is the definition of what GPU will be used: https://cloud.google.com/ai-platform/training/docs/using-gpus"
      },
      {
        "date": "2021-09-13T20:20:00.000Z",
        "voteCount": 3,
        "content": "Should be B.\nQuestion didn't say anything about cost, so while B would increase cost with more computation time, it would save real-world time."
      },
      {
        "date": "2021-09-08T14:59:00.000Z",
        "voteCount": 3,
        "content": "Go with B, all the other options could affect the accuracy."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 21,
    "url": "https://www.examtopics.com/discussions/google/view/54840-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You have deployed multiple versions of an image classification model on AI Platform. You want to monitor the performance of the model versions over time. How should you perform this comparison?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCompare the loss performance for each model on a held-out dataset.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCompare the loss performance for each model on the validation data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCompare the receiver operating characteristic (ROC) curve for each model using the What-If Tool.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCompare the mean average precision across the models using the Continuous Evaluation feature.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 15,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 8,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-06-17T16:31:00.000Z",
        "voteCount": 13,
        "content": "Answer is D"
      },
      {
        "date": "2021-09-08T15:04:00.000Z",
        "voteCount": 6,
        "content": "D is correct. Choose the feature / capability GCP provides is always a good bet. :)"
      },
      {
        "date": "2024-06-26T23:52:00.000Z",
        "voteCount": 2,
        "content": "The answer is A. I am not sure why people choose B vs A as you may overfit your validation set. And you are using your held-out set really rare == no option to overfit."
      },
      {
        "date": "2024-06-04T22:06:00.000Z",
        "voteCount": 1,
        "content": "Continuous Evaluation feature is deprecated."
      },
      {
        "date": "2024-06-05T10:17:00.000Z",
        "voteCount": 1,
        "content": "so is the what if tool"
      },
      {
        "date": "2024-06-05T10:17:00.000Z",
        "voteCount": 2,
        "content": "so it looks like that B is the best answer"
      },
      {
        "date": "2024-06-01T08:35:00.000Z",
        "voteCount": 3,
        "content": "In the official study guide, this was the explanation given for answer B : \n\"The image classification model is a deep learning model. You minimize the loss of deep learning models to get the best model. So comparing loss performance for each model on validation data is the correct answer.\""
      },
      {
        "date": "2023-11-15T07:48:00.000Z",
        "voteCount": 4,
        "content": "D - because you are using a Google provided feature.\nremember in this exam its important to always choose the google services over anything else"
      },
      {
        "date": "2023-10-05T04:49:00.000Z",
        "voteCount": 1,
        "content": "mAP is for object detection, so the answer should be B"
      },
      {
        "date": "2023-07-07T07:09:00.000Z",
        "voteCount": 1,
        "content": "Went with D, using continuous evaluation feature seems correct to me."
      },
      {
        "date": "2023-07-07T01:33:00.000Z",
        "voteCount": 1,
        "content": "I choose by myself D. But as I read the post here https://www.v7labs.com/blog/mean-average-precision, I was not sure about D. \nIt wrote mAP is commonly used for object detection or instance segmentation tasks. \nValidation Dataset in GCP context: not trained dataset and not seen dataset"
      },
      {
        "date": "2023-05-30T10:45:00.000Z",
        "voteCount": 1,
        "content": "D. Compare the mean average precision across the models using the Continuous Evaluation feature\nhttps://cloud.google.com/vertex-ai/docs/evaluation/introduction\nVertex AI provides model evaluation metrics, such as precision and recall, to help you determine the performance of your models...\nVertex AI supports evaluation of the following model types:\nAuPRC: The area under the precision-recall (PR) curve, also referred to as average precision. This value ranges from zero to one, where a higher value indicates a higher-quality model."
      },
      {
        "date": "2023-05-08T23:00:00.000Z",
        "voteCount": 1,
        "content": "Went with D"
      },
      {
        "date": "2023-04-18T20:24:00.000Z",
        "voteCount": 1,
        "content": "I go for B. Option D is good when we are already in production"
      },
      {
        "date": "2023-03-21T11:12:00.000Z",
        "voteCount": 1,
        "content": "o monitor the performance of the model versions over time, you should compare the loss performance for each model on the validation data. Therefore, option B is the correct answer."
      },
      {
        "date": "2023-05-09T15:26:00.000Z",
        "voteCount": 3,
        "content": "Please, How? B is not monitoring. It is a validation. The definition of monitoring states:\n\"observe and check the progress or quality of (something) over a period of time\"\nSo it is a continuous process. Each option A,B,C are just one time check, not monitoring."
      },
      {
        "date": "2023-02-28T06:31:00.000Z",
        "voteCount": 4,
        "content": "The best option to monitor the performance of multiple versions of an image classification model on AI Platform over time is to compare the loss performance for each model on the validation data.\n\nOption B is the best approach because comparing the loss performance of each model on the validation data is a common method to monitor machine learning model performance over time. The validation data is a subset of the data that is not used for model training, but is used to evaluate its performance during training and to compare different versions of the model. By comparing the loss performance of each model on the same validation data, you can determine which version of the model has better performance."
      },
      {
        "date": "2023-02-06T17:53:00.000Z",
        "voteCount": 1,
        "content": "If you have multiple model versions in a single model and have created an evaluation job for each one, you can view a chart comparing the mean average precision of the model versions over time"
      },
      {
        "date": "2023-01-31T07:00:00.000Z",
        "voteCount": 3,
        "content": "Guys, I not sure about the answer D ... And maybe you could help me in my arguments. \n\nI think choose loss to compare the model performance is better than see for metrics. For example, when can build an image model classification that has good precision metrics, because the class in unbalanced, but the loss could be terrible because of kind of loss choose that penalizes classes. \n\nso, losses are better than metrics to available models, and the answer is in A or B.\n\nI thought that the A could be the answer because I see validation as a part of the training process. So, If we want to test the model performance over time, we have to use new data, which I suppose to be the held-out data."
      },
      {
        "date": "2022-12-15T14:37:00.000Z",
        "voteCount": 1,
        "content": "ans: D"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 22,
    "url": "https://www.examtopics.com/discussions/google/view/54308-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You trained a text classification model. You have the following SignatureDefs:<br><img src=\"/assets/media/exam-media/03841/0001300001.png\" class=\"in-exam-image\"><br>You started a TensorFlow-serving component server and tried to send an HTTP request to get a prediction using: headers = {\"content-type\": \"application/json\"} json_response = requests.post('http: //localhost:8501/v1/models/text_model:predict', data=data, headers=headers)<br>What is the correct way to write the predict request?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tdata = json.dumps({\u05d2\u20acsignature_name\u05d2\u20ac: \u05d2\u20acseving_default\u05d2\u20ac, \u05d2\u20acinstances\u05d2\u20ac [['ab', 'bc', 'cd']]})",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tdata = json.dumps({\u05d2\u20acsignature_name\u05d2\u20ac: \u05d2\u20acserving_default\u05d2\u20ac, \u05d2\u20acinstances\u05d2\u20ac [['a', 'b', 'c', 'd', 'e', 'f']]})",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tdata = json.dumps({\u05d2\u20acsignature_name\u05d2\u20ac: \u05d2\u20acserving_default\u05d2\u20ac, \u05d2\u20acinstances\u05d2\u20ac [['a', 'b', 'c'], ['d', 'e', 'f']]})",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tdata = json.dumps({\u05d2\u20acsignature_name\u05d2\u20ac: \u05d2\u20acserving_default\u05d2\u20ac, \u05d2\u20acinstances\u05d2\u20ac [['a', 'b'], ['c', 'd'], ['e', 'f']]})\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-06-02T14:38:00.000Z",
        "voteCount": 27,
        "content": "Options:\n\nA. data = json.dumps({\u201csignature_name\u201d: \u201cseving_default\u201d, \u201cinstances\u201d [[\u2018ab\u2019, \u2018bc\u2019, \u2018cd\u2019]]})\nB. data = json.dumps({\u201csignature_name\u201d: \u201cserving_default\u201d, \u201cinstances\u201d [[\u2018a\u2019, \u2018b\u2019, \u2018c\u2019, \u2018d\u2019, \u2018e\u2019, \u2018f\u2019]]})\nC. data = json.dumps({\u201csignature_name\u201d: \u201cserving_default\u201d, \u201cinstances\u201d [[\u2018a\u2019, \u2018b\u2019, \u2018c\u2019], [\u2018d\u2019, \u2018e\u2019, \u2018f\u2019]]})\nD. data = json.dumps({\u201csignature_name\u201d: \u201cserving_default\u201d, \u201cinstances\u201d [[\u2018a\u2019, \u2018b\u2019], [\u2018c\u2019, \u2018d\u2019], [\u2018e\u2019, \u2018f\u2019]]})"
      },
      {
        "date": "2021-06-22T02:37:00.000Z",
        "voteCount": 21,
        "content": "Most likely D. A negative number in the shape enables auto expand (https://stackoverflow.com/questions/37956197/what-is-the-negative-index-in-shape-arrays-used-for-tensorflow). \n\nThen the first number -1 out of the shape (-1, 2) speaks the number of 1 dimensional arrays within the tensor (and it can autoexpand) while the second numer (2) sets the number of elements in the inner array at 2. Hence D."
      },
      {
        "date": "2024-06-06T06:31:00.000Z",
        "voteCount": 1,
        "content": "D) Any rows, 2 columns."
      },
      {
        "date": "2023-05-08T23:00:00.000Z",
        "voteCount": 2,
        "content": "Went with D"
      },
      {
        "date": "2022-12-15T14:40:00.000Z",
        "voteCount": 1,
        "content": "ans: D"
      },
      {
        "date": "2022-11-24T02:34:00.000Z",
        "voteCount": 1,
        "content": "Having \"shape=[-1,2]\", the input can have as many rows as we want, but each row needs to be of 2 elements. The only option satisfying this requirement is D."
      },
      {
        "date": "2022-08-15T06:50:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is \"D\""
      },
      {
        "date": "2022-06-12T14:59:00.000Z",
        "voteCount": 1,
        "content": "will vote for D , as the data shape in instances matches the shape in signature def"
      },
      {
        "date": "2022-03-23T20:03:00.000Z",
        "voteCount": 2,
        "content": "shape is (-1,2) indicating any no of rows, 2 columns only."
      },
      {
        "date": "2021-10-19T06:23:00.000Z",
        "voteCount": 3,
        "content": "D is correct if shape(-1,2) means 2 columns for each row"
      },
      {
        "date": "2021-10-19T06:24:00.000Z",
        "voteCount": 1,
        "content": "Link to explanation: https://stackoverflow.com/questions/37956197/what-is-the-negative-index-in-shape-arrays-used-for-tensorflow"
      },
      {
        "date": "2021-09-08T15:13:00.000Z",
        "voteCount": 5,
        "content": "D: (-1, 2) represents a vector with any number of rows but only 2 columns."
      },
      {
        "date": "2021-06-09T04:52:00.000Z",
        "voteCount": 4,
        "content": "Correct answer is D, the shapes otherwise don't matter"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 23,
    "url": "https://www.examtopics.com/discussions/google/view/54825-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "Your organization's call center has asked you to develop a model that analyzes customer sentiments in each call. The call center receives over one million calls daily, and data is stored in Cloud Storage. The data collected must not leave the region in which the call originated, and no Personally Identifiable Information (PII) can be stored or analyzed. The data science team has a third-party tool for visualization and access which requires a SQL ANSI-2011 compliant interface. You need to select components for data processing and for analytics. How should the data pipeline be designed?<br><img src=\"/assets/media/exam-media/03841/0001400001.png\" class=\"in-exam-image\"><br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1= Dataflow, 2= BigQuery\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1 = Pub/Sub, 2= Datastore",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1 = Dataflow, 2 = Cloud SQL",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1 = Cloud Function, 2= Cloud SQL"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 12,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-06-07T13:31:00.000Z",
        "voteCount": 18,
        "content": "The correct answer is A"
      },
      {
        "date": "2021-10-01T10:05:00.000Z",
        "voteCount": 7,
        "content": "Evidence here https://github.com/GoogleCloudPlatform/dataflow-contact-center-speech-analysis"
      },
      {
        "date": "2021-06-07T08:56:00.000Z",
        "voteCount": 7,
        "content": "Should be A"
      },
      {
        "date": "2024-06-06T06:35:00.000Z",
        "voteCount": 1,
        "content": "A) Dataflow &amp; BigQuery (Analytics)"
      },
      {
        "date": "2023-11-15T07:54:00.000Z",
        "voteCount": 3,
        "content": "A - because it has BigQuery.\nAlmost never would you see an answer that prefers CloudSQL over BQ"
      },
      {
        "date": "2023-05-08T23:00:00.000Z",
        "voteCount": 2,
        "content": "Went with A"
      },
      {
        "date": "2022-12-19T22:03:00.000Z",
        "voteCount": 1,
        "content": "correct answer is A"
      },
      {
        "date": "2022-12-07T04:07:00.000Z",
        "voteCount": 1,
        "content": "we need a dataflow to process data from cloud storage and data is unstructured and if we want to perform analysis on unstructured with SQL interface BIgQuery is the only option"
      },
      {
        "date": "2022-11-24T02:42:00.000Z",
        "voteCount": 1,
        "content": "You need to do analytics, so the answer needs to contain BigQuery and only option A does.\nMoreover, BigQuery is fine with SQL and Dataflow is the right tool for the processing pipline."
      },
      {
        "date": "2022-08-15T06:51:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is \"A\""
      },
      {
        "date": "2022-06-01T17:25:00.000Z",
        "voteCount": 2,
        "content": "D - to call API you need Cloud Functions. Dataflow would be for ETL"
      },
      {
        "date": "2022-06-01T17:32:00.000Z",
        "voteCount": 2,
        "content": "Sorry incorrect - Dataflow can call external API so stand corrected . Answer : A"
      },
      {
        "date": "2022-03-25T12:52:00.000Z",
        "voteCount": 2,
        "content": "Dataflow &amp; BigQuery"
      },
      {
        "date": "2021-11-19T11:59:00.000Z",
        "voteCount": 1,
        "content": "A, https://cloud.google.com/architecture/architecture-for-mlops-using-tfx-kubeflow-pipelines-and-cloud-build Fig.6"
      },
      {
        "date": "2021-10-19T06:41:00.000Z",
        "voteCount": 3,
        "content": "A is correct\nDataflow - Unified stream and batch data processing that's serverless, fast, and cost-effective\nBigQuery - Good for analytics and dashboards"
      },
      {
        "date": "2021-09-27T03:11:00.000Z",
        "voteCount": 1,
        "content": "BQ is SQL ANSI-2011 compliant"
      },
      {
        "date": "2021-09-08T15:24:00.000Z",
        "voteCount": 2,
        "content": "A or C. Not sure how many third-party tool supports BigQuery. If not, then the answer is C."
      },
      {
        "date": "2022-05-07T12:56:00.000Z",
        "voteCount": 1,
        "content": "wrong. cloud sql is not for analytics."
      },
      {
        "date": "2021-08-24T23:37:00.000Z",
        "voteCount": 3,
        "content": "it's def A"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 24,
    "url": "https://www.examtopics.com/discussions/google/view/55818-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You are an ML engineer at a global shoe store. You manage the ML models for the company's website. You are asked to build a model that will recommend new products to the user based on their purchase behavior and similarity with other users. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBuild a classification model",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBuild a knowledge-based filtering model",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBuild a collaborative-based filtering model\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBuild a regression model using the features as predictors"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 14,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-06-22T02:41:00.000Z",
        "voteCount": 20,
        "content": "C. Collaborative filtering is about user similarity and product recommendations. Other models won't work"
      },
      {
        "date": "2024-06-06T06:43:00.000Z",
        "voteCount": 1,
        "content": "C) Collaborative filtering model"
      },
      {
        "date": "2023-11-15T07:58:00.000Z",
        "voteCount": 2,
        "content": "Chat gPT:\nCollaborative filtering models are specifically designed for recommendation systems. They work by analyzing the interactions and behaviors of users and items, then making predictions about what users will like based on similarities with other users. In this case, since you're looking at purchase behavior and user similarities, a collaborative filtering approach is well-suited to identify and recommend products that users with similar behaviors have liked or purchased.\n\nClassification models (Option A) and regression models (Option D) are generally used for different types of predictive modeling tasks, not specifically for recommendations. A knowledge-based filtering model (Option B), while useful in recommendation systems, relies more on explicit knowledge about users and items, rather than on user interaction patterns and similarities, which seems to be the focus in this scenario."
      },
      {
        "date": "2023-08-21T12:57:00.000Z",
        "voteCount": 1,
        "content": "C. Collaborative filtering is apt amongst the answers"
      },
      {
        "date": "2023-05-08T23:01:00.000Z",
        "voteCount": 2,
        "content": "Went with C"
      },
      {
        "date": "2022-12-15T14:45:00.000Z",
        "voteCount": 1,
        "content": "ans: C"
      },
      {
        "date": "2022-12-08T03:34:00.000Z",
        "voteCount": 1,
        "content": "C\nhttps://cloud.google.com/blog/topics/developers-practitioners/looking-build-recommendation-system-google-cloud-leverage-following-guidelines-identify-right-solution-you-part-i"
      },
      {
        "date": "2022-11-24T02:44:00.000Z",
        "voteCount": 1,
        "content": "This is a textbook application of collaborative filtering, C is the correct answer"
      },
      {
        "date": "2022-08-15T06:51:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is \"C\""
      },
      {
        "date": "2022-06-12T15:16:00.000Z",
        "voteCount": 1,
        "content": "https://developers.google.com/machine-learning/recommendation/collaborative/basics"
      },
      {
        "date": "2022-03-06T11:33:00.000Z",
        "voteCount": 2,
        "content": "Definitely C"
      },
      {
        "date": "2022-02-28T00:06:00.000Z",
        "voteCount": 2,
        "content": "Community vote"
      },
      {
        "date": "2022-01-27T18:59:00.000Z",
        "voteCount": 2,
        "content": "should be C"
      },
      {
        "date": "2021-10-19T07:03:00.000Z",
        "voteCount": 4,
        "content": "C - https://cloud.google.com/architecture/recommendations-using-machine-learning-on-compute-engine#filtering_the_data"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 25,
    "url": "https://www.examtopics.com/discussions/google/view/54314-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You work for a social media company. You need to detect whether posted images contain cars. Each training example is a member of exactly one class. You have trained an object detection neural network and deployed the model version to AI Platform Prediction for evaluation. Before deployment, you created an evaluation job and attached it to the AI Platform Prediction model version. You notice that the precision is lower than your business requirements allow. How should you adjust the model's final layer softmax threshold to increase precision?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the recall.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDecrease the recall.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the number of false positives.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDecrease the number of false negatives."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 15,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-06-24T06:23:00.000Z",
        "voteCount": 30,
        "content": "Decreasing FN increases recall (D). So D and A are the same.\nIncreasing FP decreases precision (C). \n\nAnswer: B (\"improving precision typically reduces recall and vice versa\", https://developers.google.com/machine-learning/crash-course/classification/precision-and-recall)"
      },
      {
        "date": "2021-09-01T13:23:00.000Z",
        "voteCount": 3,
        "content": "I do believe B is the right answer.\nBut D and A aren't exactly the same.\nA. Increase recall can be either  \n       1. keeping TP + FN the same but increase TP and decrease FN. Which isn't sure how that's gonna affect Precision since both TP and TP+FP increase.\n       2. keeping TP the same but increase (TP + FN), which is increasing FN (Same as D), not sure how that will affect Precision as well."
      },
      {
        "date": "2021-09-08T15:41:00.000Z",
        "voteCount": 21,
        "content": "Precision = TruePositives / (TruePositives + FalsePositives)\nRecall = TruePositives / (TruePositives + FalseNegatives)\nA. Increase recall -&gt; will decrease precision\nB. Decrease recall -&gt; will increase precision\nC. Increase the false positives -&gt; will decrease precision\nD. Decrease the false negatives -&gt; will increase recall, reduce precision\nThe correct answer is B."
      },
      {
        "date": "2024-06-06T07:12:00.000Z",
        "voteCount": 1,
        "content": "B) Decrease Recall (increases precision)"
      },
      {
        "date": "2023-07-07T05:03:00.000Z",
        "voteCount": 2,
        "content": "To increase precision, you have to decrese recall, increse true positives, increse false negatives and decrease false positives"
      },
      {
        "date": "2023-05-08T23:01:00.000Z",
        "voteCount": 3,
        "content": "Went with B"
      },
      {
        "date": "2023-02-28T07:21:00.000Z",
        "voteCount": 1,
        "content": "Option B is the best approach because decreasing the threshold will increase the precision by reducing the number of false positives."
      },
      {
        "date": "2023-01-12T23:09:00.000Z",
        "voteCount": 1,
        "content": "A , C , D   they are the same.  So I go with B , it is threshold adjustment from 0.5 +-"
      },
      {
        "date": "2023-01-13T00:45:00.000Z",
        "voteCount": 1,
        "content": "WE want to increase Precision, it is the same as decreasing recall.  Both are opposed each other.\nhttps://developers.google.com/machine-learning/crash-course/classification/precision-and-recall"
      },
      {
        "date": "2022-12-15T14:49:00.000Z",
        "voteCount": 1,
        "content": "ans: B.\nA: should decrease even more the precission.\nC: will decrease precision\nD: will increase recall (precision would be the same)"
      },
      {
        "date": "2022-11-24T03:07:00.000Z",
        "voteCount": 2,
        "content": "Precision and recall are negatively correlated, when one goes up the other goes down and vice-versa; to increase precidion we need to decrease recall, therefore answer B.\n(To be more complete, answer C and D are wrong because they both would increase recall, according to the recall formula)"
      },
      {
        "date": "2022-08-15T06:52:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is \"C\""
      },
      {
        "date": "2022-08-15T06:53:00.000Z",
        "voteCount": 1,
        "content": "sorry correct ans is \" B\""
      },
      {
        "date": "2022-08-13T11:46:00.000Z",
        "voteCount": 2,
        "content": "Answer is D\nIf the dataset does not change, TP + FN is constant.\nFN goes down then TP goes up.\nHence Precision = TP / TP + FP goes up."
      },
      {
        "date": "2022-06-12T15:19:00.000Z",
        "voteCount": 1,
        "content": "precision and recall have negative proportion , so to increase precision reduce recall"
      },
      {
        "date": "2022-04-07T00:50:00.000Z",
        "voteCount": 1,
        "content": "It's B.\nC,D is basically ruining your model."
      },
      {
        "date": "2022-03-08T07:35:00.000Z",
        "voteCount": 2,
        "content": "Answer: D\nBecause of Precision should respond the answer how many retrieved items are relevant? In the relation of  False Negative / true positives  an optima  precision need a high number of true positives. If your model is precision is lower than your business requirement is because the model has a high number of false negatives. Check it in: https://en.wikipedia.org/wiki/Precision_and_recall"
      },
      {
        "date": "2022-01-27T19:01:00.000Z",
        "voteCount": 1,
        "content": "definitely B"
      },
      {
        "date": "2022-01-11T09:27:00.000Z",
        "voteCount": 1,
        "content": "I think this should be C. The reason is, for one to increase precision, the classification threshold for whether the car is there or not should be kept low. That way, even when the model is not very confident (say only 60% confident), it will say, yes, car is there. What this does is it will crease the times the model says car is present, driving up precision (when it says car is there, car is really there). The consequence of this is, False positives will increase too, reducing recall.\nSo C is my choice.\n\nChoices A and B are not really right, as precision and recall are after-effects, not something you will control ahead."
      },
      {
        "date": "2021-12-10T08:39:00.000Z",
        "voteCount": 3,
        "content": "Answer is B . 100% sure . The only way to affect precision and recall is by adjusting threshold. FN and FP go in opposite direction so C &amp; D are the same.  A increasing recall decreases precision ."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 26,
    "url": "https://www.examtopics.com/discussions/google/view/54316-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You are responsible for building a unified analytics environment across a variety of on-premises data marts. Your company is experiencing data quality and security challenges when integrating data across the servers, caused by the use of a wide range of disconnected tools and temporary solutions. You need a fully managed, cloud-native data integration service that will lower the total cost of work and reduce repetitive work. Some members on your team prefer a codeless interface for building Extract, Transform, Load (ETL) process. Which service should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDataflow",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDataprep",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tApache Flink",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Data Fusion\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 16,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-06-02T15:00:00.000Z",
        "voteCount": 14,
        "content": "D. correct.\nReference: https://cloud.google.com/data-fusion"
      },
      {
        "date": "2024-06-06T07:16:00.000Z",
        "voteCount": 1,
        "content": "D) Cloud Data Function"
      },
      {
        "date": "2024-04-14T07:35:00.000Z",
        "voteCount": 2,
        "content": "codeless interface -&gt; D"
      },
      {
        "date": "2023-11-15T08:34:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2023-07-07T05:08:00.000Z",
        "voteCount": 1,
        "content": "I think D is correct."
      },
      {
        "date": "2023-05-08T23:01:00.000Z",
        "voteCount": 1,
        "content": "Went with D"
      },
      {
        "date": "2023-03-06T03:20:00.000Z",
        "voteCount": 1,
        "content": "Answer is B"
      },
      {
        "date": "2023-02-28T08:01:00.000Z",
        "voteCount": 2,
        "content": "Cloud Data Fusion is a fully managed, cloud-native data integration service provided by Google Cloud Platform. It is designed to simplify the process of building and managing ETL pipelines across a variety of data sources and targets."
      },
      {
        "date": "2022-11-24T03:18:00.000Z",
        "voteCount": 3,
        "content": "\"codeless interface\" ==&gt; Data Fusion"
      },
      {
        "date": "2022-08-15T06:54:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is \"D\""
      },
      {
        "date": "2022-07-19T10:00:00.000Z",
        "voteCount": 1,
        "content": "D is correct as it is codeless"
      },
      {
        "date": "2022-06-12T15:23:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/data-fusion/docs/concepts/overview#using_the_code-free_web_ui"
      },
      {
        "date": "2022-04-07T00:55:00.000Z",
        "voteCount": 2,
        "content": "D without any doubt"
      },
      {
        "date": "2022-01-27T19:06:00.000Z",
        "voteCount": 3,
        "content": "D.\nDatafusion is more designed for data ingestion from one source to another one, with few transformation. Dataprep is more designed for data preparation (as its name means), data cleaning, new column creation, splitting column. Dataprep also provide insight of the data for helping you in your recipes."
      },
      {
        "date": "2021-10-25T07:36:00.000Z",
        "voteCount": 2,
        "content": "D. Dataprep would also work but Data Fusion is better suited.\n(See https://stackoverflow.com/questions/58175386/can-google-data-fusion-make-the-same-data-cleaning-than-dataprep)"
      },
      {
        "date": "2021-10-19T12:07:00.000Z",
        "voteCount": 4,
        "content": "D is correct\n\nVisual point-and-click interface enabling code-free deployment of ETL/ELT data pipelines and Operate high-volumes of data pipelines periodically\n\nsource: https://cloud.google.com/data-fusion#all-features"
      },
      {
        "date": "2021-09-12T01:33:00.000Z",
        "voteCount": 2,
        "content": "B. Dataprep makes use of Apache beam, which can process streaming and batch, and thus prevent training-serving skew."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 27,
    "url": "https://www.examtopics.com/discussions/google/view/54826-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You are an ML engineer at a regulated insurance company. You are asked to develop an insurance approval model that accepts or rejects insurance applications from potential customers. What factors should you consider before building the model?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRedaction, reproducibility, and explainability",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTraceability, reproducibility, and explainability\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFederated learning, reproducibility, and explainability",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDifferential privacy, federated learning, and explainability"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 25,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 9,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-07-16T08:02:00.000Z",
        "voteCount": 32,
        "content": "I think the answer should be B. as I review the OECD document on impact of AI on insurance, the document mention explainability, traceable. However, open for discussion. https://www.oecd.org/finance/Impact-Big-Data-AI-in-the-Insurance-Sector.pdf"
      },
      {
        "date": "2021-06-07T09:04:00.000Z",
        "voteCount": 13,
        "content": "Should be B"
      },
      {
        "date": "2021-06-30T23:08:00.000Z",
        "voteCount": 2,
        "content": "I think it should be A, as it is regulated, so need to have PII"
      },
      {
        "date": "2024-08-15T21:51:00.000Z",
        "voteCount": 1,
        "content": "As per ChatGPT, anwser is B"
      },
      {
        "date": "2024-07-05T12:44:00.000Z",
        "voteCount": 1,
        "content": "Agree with B"
      },
      {
        "date": "2024-06-06T07:19:00.000Z",
        "voteCount": 1,
        "content": "B) Traceability, Reproducibility and Explainability"
      },
      {
        "date": "2024-06-05T11:54:00.000Z",
        "voteCount": 1,
        "content": "A kinda makes sense here, because redaction means remove sensitive or private information before sharing it.."
      },
      {
        "date": "2024-04-19T05:41:00.000Z",
        "voteCount": 2,
        "content": "went with B"
      },
      {
        "date": "2023-11-15T08:37:00.000Z",
        "voteCount": 4,
        "content": "B. Traceability, reproducibility, and explainability.\n\nTraceability: This involves maintaining records of the data, decisions, and processes used in the model. This is crucial in regulated industries for audit purposes and to ensure compliance with regulatory standards. It helps in understanding how the model was developed and how it makes decisions.\n\nReproducibility: Ensuring that the results of the model can be reproduced using the same data and methods is vital for validating the model's reliability and for future development or debugging.\n\nExplainability: Given the significant impact of the model\u2019s decisions on individuals' lives, it's crucial that the model's decisions can be explained in understandable terms. This is not just a best practice in AI ethics; in many jurisdictions, it's a legal requirement under regulations that mandate transparency in automated decision-making."
      },
      {
        "date": "2024-05-23T05:14:00.000Z",
        "voteCount": 1,
        "content": "you are a lifesaver, sum sum. thank you"
      },
      {
        "date": "2023-07-02T02:48:00.000Z",
        "voteCount": 4,
        "content": "B. Traceability, reproducibility, and explainability are the most important factors to consider before building an insurance approval model. \nTraceability ensures that the data used in the model is reliable and can be traced back to its source. \nReproducibility ensures that the model can be replicated and tested to ensure its accuracy and fairness. \nExplainability ensures that the model's decisions can be explained to customers and regulators in a transparent manner. These factors are crucial for building a trustworthy and compliant model for an insurance company.\nRedaction is also important for protecting sensitive customer information, but it is not as critical as the other factors listed. Federated learning and differential privacy are techniques used to protect data privacy, but they are not necessarily required for building an insurance approval model."
      },
      {
        "date": "2023-05-08T23:01:00.000Z",
        "voteCount": 1,
        "content": "Went with B"
      },
      {
        "date": "2023-02-20T03:58:00.000Z",
        "voteCount": 4,
        "content": "B. Traceability, reproducibility, and explainability\n\nWhen developing an insurance approval model, it's crucial to consider several factors to ensure that the model is fair, accurate, and compliant with regulations. The factors to consider include:\n\nTraceability: It's important to be able to trace the data used to build the model and the decisions made by the model. This is important for transparency and accountability.\n\nReproducibility: The model should be built in a way that allows for its reproducibility. This means that other researchers should be able to reproduce the same results using the same data and methods.\n\nExplainability: The model should be able to provide clear and understandable explanations for its decisions. This is important for building trust with customers and ensuring compliance with regulations.\n\nOther factors that may also be important to consider, depending on the specific context of the insurance company and its customers, include data privacy and security, fairness, and bias mitigation."
      },
      {
        "date": "2023-02-20T03:57:00.000Z",
        "voteCount": 1,
        "content": "B. Traceability, reproducibility, and explainability\n\nWhen developing an insurance approval model, it's crucial to consider several factors to ensure that the model is fair, accurate, and compliant with regulations. The factors to consider include:\n\nTraceability: It's important to be able to trace the data used to build the model and the decisions made by the model. This is important for transparency and accountability.\n\nReproducibility: The model should be built in a way that allows for its reproducibility. This means that other researchers should be able to reproduce the same results using the same data and methods.\n\nExplainability: The model should be able to provide clear and understandable explanations for its decisions. This is important for building trust with customers and ensuring compliance with regulations.\n\nOther factors that may also be important to consider, depending on the specific context of the insurance company and its customers, include data privacy and security, fairness, and bias mitigation."
      },
      {
        "date": "2023-01-05T08:52:00.000Z",
        "voteCount": 2,
        "content": "Checking Google documents, it seems D."
      },
      {
        "date": "2023-03-08T07:12:00.000Z",
        "voteCount": 1,
        "content": "Please mention the links"
      },
      {
        "date": "2022-12-15T14:54:00.000Z",
        "voteCount": 1,
        "content": "ans: B"
      },
      {
        "date": "2022-08-15T06:54:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is \"B\""
      },
      {
        "date": "2022-07-19T10:01:00.000Z",
        "voteCount": 3,
        "content": "should be D as all of the techniques abide to any problems related to insurance"
      },
      {
        "date": "2022-07-17T00:53:00.000Z",
        "voteCount": 1,
        "content": "B should be True"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 28,
    "url": "https://www.examtopics.com/discussions/google/view/55546-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You are training a Resnet model on AI Platform using TPUs to visually categorize types of defects in automobile engines. You capture the training profile using the<br>Cloud TPU profiler plugin and observe that it is highly input-bound. You want to reduce the bottleneck and speed up your model training process. Which modifications should you make to the tf.data dataset? (Choose two.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the interleave option for reading data.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReduce the value of the repeat parameter.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the buffer size for the shuttle option.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the prefetch option equal to the training batch size.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDecrease the batch size argument in your transformation."
    ],
    "answer": "AD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AD",
        "count": 10,
        "isMostVoted": true
      },
      {
        "answer": "DE",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-07-10T04:57:00.000Z",
        "voteCount": 39,
        "content": "AD - please weigh in guys"
      },
      {
        "date": "2021-11-10T14:24:00.000Z",
        "voteCount": 25,
        "content": "A. Use the interleave option for reading data. - Yes, that helps to parallelize data reading.\nB. Reduce the value of the repeat parameter. - No, this is only to repeat rows of the dataset.\nC. Increase the buffer size for the shuttle option. - No, there is only a shuttle option.\nD. Set the prefetch option equal to the training batch size. - Yes, this will pre-load the data.\nE. Decrease the batch size argument in your transformation. - No, could be even slower due to more I/Os.\n\nhttps://www.tensorflow.org/guide/data_performance"
      },
      {
        "date": "2024-06-06T07:21:00.000Z",
        "voteCount": 1,
        "content": "A) and D) are the right answers!"
      },
      {
        "date": "2023-07-12T03:57:00.000Z",
        "voteCount": 2,
        "content": "A and D : https://www.tensorflow.org/guide/data_performance , interleave and prefetch"
      },
      {
        "date": "2023-05-08T23:02:00.000Z",
        "voteCount": 2,
        "content": "Went with A &amp; D"
      },
      {
        "date": "2022-12-19T02:34:00.000Z",
        "voteCount": 1,
        "content": "yes AD"
      },
      {
        "date": "2022-08-29T05:43:00.000Z",
        "voteCount": 1,
        "content": "Yes AD"
      },
      {
        "date": "2022-08-15T06:57:00.000Z",
        "voteCount": 1,
        "content": "YES.....AD - agree with danielp1"
      },
      {
        "date": "2022-07-22T10:21:00.000Z",
        "voteCount": 2,
        "content": "AD - agree with danielp1\n\nBy the way, this is handy to understand the significance of shuffle buffer_size: https://stackoverflow.com/a/48096625/1933315"
      },
      {
        "date": "2022-07-11T22:35:00.000Z",
        "voteCount": 1,
        "content": "I think D &amp; E are correct."
      },
      {
        "date": "2022-07-02T14:15:00.000Z",
        "voteCount": 3,
        "content": "AD should be the right answer."
      },
      {
        "date": "2022-06-18T22:30:00.000Z",
        "voteCount": 1,
        "content": "Answers?"
      },
      {
        "date": "2021-10-21T02:26:00.000Z",
        "voteCount": 2,
        "content": "For me it should be D and E as well. Prefetching will help reading data while training is performed, which helps with the bottleneck, D is for sure right. I think decreasing batch size would help too, because less records will be read in each training step (reading a lot of records would lead to the bottleneck described, as reading data is costly).\n\nI'm not 100% sure on A, personally I don't think processing many input files concurrently would help in this case because the reading operation is precisely the problem. However, I'm no expert in this topic so I might be wrong."
      },
      {
        "date": "2022-02-20T04:46:00.000Z",
        "voteCount": 1,
        "content": "D is not correct answer. Instead of decrising batch size, incrising may help. (https://cloud.google.com/tpu/docs/performance-guide - \"TPU model performance\" section)"
      },
      {
        "date": "2024-06-05T12:33:00.000Z",
        "voteCount": 1,
        "content": "you mean E, not D, right?"
      },
      {
        "date": "2021-07-20T09:48:00.000Z",
        "voteCount": 3,
        "content": "I think it should be DE. I found this article https://towardsdatascience.com/overcoming-data-preprocessing-bottlenecks-with-tensorflow-data-service-nvidia-dali-and-other-d6321917f851"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 29,
    "url": "https://www.examtopics.com/discussions/google/view/55005-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You have trained a model on a dataset that required computationally expensive preprocessing operations. You need to execute the same preprocessing at prediction time. You deployed the model on AI Platform for high-throughput online prediction. Which architecture should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tValidate the accuracy of the model that you trained on preprocessed data. Create a new model that uses the raw data and is available in real time. Deploy the new model onto AI Platform for online prediction.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSend incoming prediction requests to a Pub/Sub topic. Transform the incoming data using a Dataflow job. Submit a prediction request to AI Platform using the transformed data. Write the predictions to an outbound Pub/Sub queue.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStream incoming prediction request data into Cloud Spanner. Create a view to abstract your preprocessing logic. Query the view every second for new records. Submit a prediction request to AI Platform using the transformed data. Write the predictions to an outbound Pub/Sub queue.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSend incoming prediction requests to a Pub/Sub topic. Set up a Cloud Function that is triggered when messages are published to the Pub/Sub topic. Implement your preprocessing logic in the Cloud Function. Submit a prediction request to AI Platform using the transformed data. Write the predictions to an outbound Pub/Sub queue."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 16,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 6,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-07-13T18:23:00.000Z",
        "voteCount": 29,
        "content": "Supporting B ..https://cloud.google.com/architecture/data-preprocessing-for-ml-with-tf-transform-pt1#where_to_do_preprocessing"
      },
      {
        "date": "2021-06-09T13:00:00.000Z",
        "voteCount": 13,
        "content": "I think it should b B"
      },
      {
        "date": "2021-09-22T07:18:00.000Z",
        "voteCount": 3,
        "content": "I also agree with B, this is how I would advise clients to do it as well"
      },
      {
        "date": "2024-06-27T00:17:00.000Z",
        "voteCount": 1,
        "content": "D. The issue with B is that DataFlow does not work well with high throughput"
      },
      {
        "date": "2024-06-06T07:25:00.000Z",
        "voteCount": 1,
        "content": "B) Pub/Sub + Dataflow"
      },
      {
        "date": "2023-07-07T07:15:00.000Z",
        "voteCount": 3,
        "content": "Went with B, using dataflow for large amount data transformation is the best option"
      },
      {
        "date": "2023-07-07T05:48:00.000Z",
        "voteCount": 2,
        "content": "I went to B. \nA is completely wrong. C: 1st cloud spanner is not designed for high throughput, also it is not for preprocessing. D: cloud function could not be get enough resource to do the high computational transformation."
      },
      {
        "date": "2023-06-10T10:40:00.000Z",
        "voteCount": 1,
        "content": "Because the concern here is high throughput and not specifically the latency so better to go with option B"
      },
      {
        "date": "2023-05-30T11:09:00.000Z",
        "voteCount": 1,
        "content": "B. Send incoming prediction requests to a Pub/Sub topic. Transform the incoming data using a Dataflow job. Submit a prediction request to AI Platform using the transformed data. Write the predictions to an outbound Pub/Sub queue\nhttps://dataintegration.info/building-streaming-data-pipelines-on-google-cloud"
      },
      {
        "date": "2023-05-08T23:02:00.000Z",
        "voteCount": 1,
        "content": "Went with B"
      },
      {
        "date": "2023-04-27T00:56:00.000Z",
        "voteCount": 3,
        "content": "I think it's D as B is not a good choice because it requires you to run a Dataflow job for each prediction request. This is inefficient and can lead to latency issues."
      },
      {
        "date": "2023-04-27T20:14:00.000Z",
        "voteCount": 2,
        "content": "Yes i agree Dataflow can introduce latency"
      },
      {
        "date": "2023-04-12T18:43:00.000Z",
        "voteCount": 1,
        "content": "I go for D. Option B has Dataflow that it is more suitable for batch"
      },
      {
        "date": "2023-03-24T06:12:00.000Z",
        "voteCount": 1,
        "content": "It's B"
      },
      {
        "date": "2022-12-19T02:35:00.000Z",
        "voteCount": 1,
        "content": "yes ans B"
      },
      {
        "date": "2022-12-08T04:06:00.000Z",
        "voteCount": 1,
        "content": "B\nPubsub + DataFlow + Vertex AI (AI Platform)"
      },
      {
        "date": "2022-08-10T07:36:00.000Z",
        "voteCount": 2,
        "content": "Should be B. Dataflow is BEST option for preprocessing training , testing data both"
      },
      {
        "date": "2022-08-06T00:08:00.000Z",
        "voteCount": 1,
        "content": "Answer should be  B"
      },
      {
        "date": "2022-06-13T10:23:00.000Z",
        "voteCount": 2,
        "content": "- using options eliminatios , A totally wrong , D also not valid as cloud functions is not sutiable for heavy data workflows\n- answer between B,D will vote for B as dataflow is the best solution while dealing with heavy data workflows"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 30,
    "url": "https://www.examtopics.com/discussions/google/view/54319-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "Your team trained and tested a DNN regression model with good results. Six months after deployment, the model is performing poorly due to a change in the distribution of the input data. How should you address the input differences in production?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate alerts to monitor for skew, and retrain the model.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPerform feature selection on the model, and retrain the model with fewer features.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRetrain the model, and select an L2 regularization parameter with a hyperparameter tuning service.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPerform feature selection on the model, and retrain the model on a monthly basis with fewer features."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 12,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-07-20T07:46:00.000Z",
        "voteCount": 33,
        "content": "A\n\nData values skews: These skews are significant changes in the\nstatistical properties of data, which means that data patterns are\nchanging, and you need to trigger a retraining of the model to capture\nthese changes.\nhttps://developers.google.com/machine-learning/guides/rules-of-ml/#rule_37_measure_trainingserving_skew"
      },
      {
        "date": "2021-10-20T02:29:00.000Z",
        "voteCount": 2,
        "content": "I agree, A is correct"
      },
      {
        "date": "2021-11-23T10:23:00.000Z",
        "voteCount": 2,
        "content": "Rule #37:\nThe difference between the performance on the holdout data and the \"next\u00adday\" data. Again, this will always exist. You should tune your regularization to maximize the next-day performance. However, large drops in performance between holdout and next-day data may indicate that some features are time-sensitive and possibly degrading model performance.\n\nMaybe it should be C"
      },
      {
        "date": "2021-06-24T20:22:00.000Z",
        "voteCount": 5,
        "content": "A\nData drift doesn't necessarily require feature reselection (e.g. by L2 regularization). \nhttps://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning#challenges"
      },
      {
        "date": "2024-06-06T07:26:00.000Z",
        "voteCount": 1,
        "content": "A) Monitor the model and set alerts"
      },
      {
        "date": "2023-07-02T03:07:00.000Z",
        "voteCount": 1,
        "content": "When the distribution of input data changes, the model may not perform as well as it did during training. It is important to monitor the performance of the model in production and identify any changes in the distribution of input data. By creating alerts to monitor for skew, you can detect when the input data distribution has changed and take action to retrain the model using more recent data that reflects the new distribution. This will help ensure that the model continues to perform well in production."
      },
      {
        "date": "2023-05-08T23:02:00.000Z",
        "voteCount": 2,
        "content": "Went with A"
      },
      {
        "date": "2023-03-24T06:13:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2023-03-08T07:28:00.000Z",
        "voteCount": 1,
        "content": "Its A, as the model itself is performing well, neither overfitting nor performing poorly suddenly, it's a gradual change so regularization on the original model would not help. C is incorrect."
      },
      {
        "date": "2023-02-28T08:49:00.000Z",
        "voteCount": 1,
        "content": "Creating alerts to monitor for skew in the input data can help to detect when the distribution of the data has changed and the model's performance is affected. Once a skew is detected, retraining the model with the new data can improve its performance."
      },
      {
        "date": "2023-02-06T19:05:00.000Z",
        "voteCount": 1,
        "content": "Skew &amp; drift monitoring: Production data tends to constantly change in different dimensions (i.e. time and system wise). And this causes the performance of the model to drop.\nhttps://cloud.google.com/vertex-ai/docs/model-monitoring/using-model-monitoring"
      },
      {
        "date": "2022-12-08T04:09:00.000Z",
        "voteCount": 2,
        "content": "A\nYou don't need to do feature selection again"
      },
      {
        "date": "2022-07-01T04:06:00.000Z",
        "voteCount": 1,
        "content": "A very obvious , no need for explanation"
      },
      {
        "date": "2022-06-13T10:26:00.000Z",
        "voteCount": 1,
        "content": "abviously A no tricks here , no too much thinking"
      },
      {
        "date": "2022-01-19T08:07:00.000Z",
        "voteCount": 1,
        "content": "A\nas celia explained"
      },
      {
        "date": "2021-11-12T16:27:00.000Z",
        "voteCount": 1,
        "content": "Colleagues that said (C) keep attention for the question: They said the model was good, so for skewness is only necessary the (A) solution."
      },
      {
        "date": "2021-09-08T16:34:00.000Z",
        "voteCount": 2,
        "content": "A. It is well documented in Google model monitoring docs."
      },
      {
        "date": "2021-07-31T12:12:00.000Z",
        "voteCount": 2,
        "content": "should be C. as L2 regularization prevent overfitting - can potential maintain model performance if data distribution is little skewed."
      },
      {
        "date": "2021-06-09T13:02:00.000Z",
        "voteCount": 2,
        "content": "A model learns the distribution of the data, if it has done its job well any change in the distribution will lead to underperformance not by virtue of poor model performance but by very definition."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 31,
    "url": "https://www.examtopics.com/discussions/google/view/55825-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You need to train a computer vision model that predicts the type of government ID present in a given image using a GPU-powered virtual machine on Compute<br>Engine. You use the following parameters:<br>\u2711 Optimizer: SGD<br>\u2711 Image shape = 224\u05b3\u2014224<br>\u2711 Batch size = 64<br>\u2711 Epochs = 10<br>\u2711 Verbose =2<br>During training you encounter the following error: ResourceExhaustedError: Out Of Memory (OOM) when allocating tensor. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the optimizer.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReduce the batch size.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the learning rate.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReduce the image shape."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 10,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-06-22T04:21:00.000Z",
        "voteCount": 23,
        "content": "B. I think you want to reduce batch size. Learning rate and optimizer shouldn't really impact memory utilisation. Decreasing image size (A) would work, but might be costly in terms final performance"
      },
      {
        "date": "2021-07-24T07:09:00.000Z",
        "voteCount": 9,
        "content": "B. https://stackoverflow.com/questions/59394947/how-to-fix-resourceexhaustederror-oom-when-allocating-tensor/59395251#:~:text=OOM%20stands%20for%20%22out%20of,in%20your%20Dense%20%2C%20Conv2D%20layers"
      },
      {
        "date": "2024-06-06T07:29:00.000Z",
        "voteCount": 1,
        "content": "B) Reduce the batch size."
      },
      {
        "date": "2023-07-07T05:55:00.000Z",
        "voteCount": 1,
        "content": "no doubt went to B"
      },
      {
        "date": "2023-05-08T23:03:00.000Z",
        "voteCount": 2,
        "content": "Went with B"
      },
      {
        "date": "2023-03-24T06:14:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2023-02-28T08:57:00.000Z",
        "voteCount": 1,
        "content": "By reducing the batch size, the amount of memory required for each iteration of the training process is reduced"
      },
      {
        "date": "2023-02-28T08:48:00.000Z",
        "voteCount": 1,
        "content": "Creating alerts to monitor for skew in the input data can help to detect when the distribution of the data has changed and the model's performance is affected. Once a skew is detected, retraining the model with the new data can improve its performance."
      },
      {
        "date": "2023-02-28T08:50:00.000Z",
        "voteCount": 1,
        "content": "Sorry it's not the response for this question. it's the response for the previous question."
      },
      {
        "date": "2023-02-28T01:57:00.000Z",
        "voteCount": 1,
        "content": "Reduce the image shape != Reduce the image Size."
      },
      {
        "date": "2022-11-19T20:03:00.000Z",
        "voteCount": 2,
        "content": "The answer is B\nSince you are using an SGD, you can use a batch size of 1\nref: https://stackoverflow.com/questions/63139072/batch-size-for-stochastic-gradient-descent-is-length-of-training-data-and-not-1"
      },
      {
        "date": "2022-06-13T10:29:00.000Z",
        "voteCount": 3,
        "content": "to fix memory overflow you need to reduce batch size also reduce input resolution is valid\nbut reducing image size can harm model performance , so answer is B"
      },
      {
        "date": "2021-12-07T13:03:00.000Z",
        "voteCount": 2,
        "content": "B is my option. But, D seems not wrong.\n\nReducing batch size or reducing image size bot can reduce memory usage. But, the former seems much easier."
      },
      {
        "date": "2021-11-12T16:41:00.000Z",
        "voteCount": 3,
        "content": "B is correct.\n\nLetter D can be used, as we reduced the image size but this will directly impact the model's performance. Another point is that when doing this, if you are using a model via Keras's `Functional API` you need to change the definition of the input and also apply pre-processing on the image to reduce its size . In other words: much more work than the letter B."
      },
      {
        "date": "2021-10-20T02:33:00.000Z",
        "voteCount": 3,
        "content": "B is correct, it uses less memory.\n\nA works too but depending on what you need you will loose perfomance (just like maartenalexander said) so I think it is not recommended."
      },
      {
        "date": "2021-10-05T05:45:00.000Z",
        "voteCount": 2,
        "content": "Initially, I though D. ,decreasing image size, would be the correct one, but now that I am reviewing the test I think maartenalexander is correct in saying reduced image size might decrease final performance, so I'd go with B eventually."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 32,
    "url": "https://www.examtopics.com/discussions/google/view/56675-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You developed an ML model with AI Platform, and you want to move it to production. You serve a few thousand queries per second and are experiencing latency issues. Incoming requests are served by a load balancer that distributes them across multiple Kubeflow CPU-only pods running on Google Kubernetes Engine<br>(GKE). Your goal is to improve the serving latency without changing the underlying infrastructure. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSignificantly increase the max_batch_size TensorFlow Serving parameter.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSwitch to the tensorflow-model-server-universal version of TensorFlow Serving.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSignificantly increase the max_enqueued_batches TensorFlow Serving parameter.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRecompile TensorFlow Serving using the source to support CPU-specific optimizations. Instruct GKE to choose an appropriate baseline minimum CPU platform for serving nodes.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 12,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 10,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 5,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-09-14T19:00:00.000Z",
        "voteCount": 27,
        "content": "D is correct since this question is focusing on server performance which development env is higher than production env. It's already throttling so increase the pressure on them won't help. Both A and C is essentially doing this. B is a bit mysterious, but we definitely know that D would work."
      },
      {
        "date": "2021-10-20T02:38:00.000Z",
        "voteCount": 2,
        "content": "I think it's D too"
      },
      {
        "date": "2024-08-17T13:49:00.000Z",
        "voteCount": 1,
        "content": "I think the correct is D, because the question is about reducing latency. As for A, increasing the batch size might event hurt latency if the system is overwhelmed to serve more multiple requests"
      },
      {
        "date": "2024-06-24T23:49:00.000Z",
        "voteCount": 1,
        "content": "it is D"
      },
      {
        "date": "2024-06-06T07:34:00.000Z",
        "voteCount": 1,
        "content": "C) Batch enqueued"
      },
      {
        "date": "2024-04-14T07:49:00.000Z",
        "voteCount": 1,
        "content": "increasing the max_batch_size TensorFlow Serving parameter, is not the best choice because increasing the batch size may not necessarily improve latency. In fact, it may even lead to higher latency for individual requests, as they will have to wait for the batch to be filled before processing. This may be useful when optimizing for throughput, but not for serving latency, which is the primary goal in this scenario."
      },
      {
        "date": "2023-11-13T09:42:00.000Z",
        "voteCount": 4,
        "content": "https://github.com/tensorflow/serving/blob/master/tensorflow_serving/batching/README.md#batch-scheduling-parameters-and-tuning\n\nA may help to some extent, but it primarily affects how many requests are processed in a single batch. It might not directly address latency issues.\n\nD is a valid approach for optimizing TensorFlow Serving for CPU-specific optimizations, but it's a more involved process and might not be the quickest way to address latency issues."
      },
      {
        "date": "2023-11-11T11:19:00.000Z",
        "voteCount": 1,
        "content": "I think A is correct, as D implies changes to the infrastructure (question says you must not do that)."
      },
      {
        "date": "2024-03-04T07:23:00.000Z",
        "voteCount": 1,
        "content": "This is purely a software optimization and on how GKE handles requests. GKE should be able to choose different CPU types for nodes within the same cluster, which doesn't represent a change in architecture."
      },
      {
        "date": "2023-08-10T07:46:00.000Z",
        "voteCount": 1,
        "content": "increasing the max_batch_size TensorFlow Serving parameter, is not the best choice because increasing the batch size may not necessarily improve latency. In fact, it may even lead to higher latency for individual requests, as they will have to wait for the batch to be filled before processing. This may be useful when optimizing for throughput, but not for serving latency, which is the primary goal in this scenario."
      },
      {
        "date": "2023-07-12T04:32:00.000Z",
        "voteCount": 1,
        "content": "max_batch_size parameter controls the maximum number of requests that can be batched together by TensorFlow Serving. Increasing this parameter can help reduce the number of round trips between the client and server, which can improve serving latency. However, increasing the batch size too much can lead to higher memory usage and longer processing times for each batch."
      },
      {
        "date": "2023-07-07T07:19:00.000Z",
        "voteCount": 1,
        "content": "Definetely D\nto improve the serving latency of an ML model on AI Platform, you can recompile TensorFlow Serving using the source to support CPU-specific optimizations and instruct GKE to choose an appropriate baseline minimum CPU platform for serving nodes, this way GKE will schedule the pods on nodes with at least that CPU platform."
      },
      {
        "date": "2023-05-08T23:03:00.000Z",
        "voteCount": 1,
        "content": "Went with D"
      },
      {
        "date": "2023-03-24T06:15:00.000Z",
        "voteCount": 2,
        "content": "A is correct. max_batch_size TensorFlow Serving parameter"
      },
      {
        "date": "2023-03-23T23:19:00.000Z",
        "voteCount": 3,
        "content": "CPU-only: One Approach\nIf your system is CPU-only (no GPU), then consider starting with the following values: num_batch_threads equal to the number of CPU cores; max_batch_size to a really high value; batch_timeout_micros to 0. Then experiment with batch_timeout_micros values in the 1-10 millisecond (1000-10000 microsecond) range, while keeping in mind that 0 may be the optimal value.\n\nhttps://github.com/tensorflow/serving/tree/master/tensorflow_serving/batching"
      },
      {
        "date": "2023-04-24T12:08:00.000Z",
        "voteCount": 4,
        "content": "In that very link, what it says is that max_batch_size is the parameter that governs the latency/troughput tradeoff, and as I understand, the higher the batch size, the higher the throughput, but that doesn't assure that latency will be lower.\nI would go with D"
      },
      {
        "date": "2022-12-26T12:01:00.000Z",
        "voteCount": 1,
        "content": "Answer: D\nhttps://www.youtube.com/watch?v=fnZTVQ1SnDg"
      },
      {
        "date": "2022-12-15T15:09:00.000Z",
        "voteCount": 1,
        "content": "ans: D"
      },
      {
        "date": "2022-08-06T04:27:00.000Z",
        "voteCount": 1,
        "content": "D is the right one"
      },
      {
        "date": "2022-08-05T05:59:00.000Z",
        "voteCount": 1,
        "content": "D is the correct one"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 33,
    "url": "https://www.examtopics.com/discussions/google/view/55827-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You have a demand forecasting pipeline in production that uses Dataflow to preprocess raw data prior to model training and prediction. During preprocessing, you employ Z-score normalization on data stored in BigQuery and write it back to BigQuery. New training data is added every week. You want to make the process more efficient by minimizing computation time and manual intervention. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNormalize the data using Google Kubernetes Engine.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTranslate the normalization algorithm into SQL for use with BigQuery.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the normalizer_fn argument in TensorFlow's Feature Column API.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNormalize the data with Apache Spark using the Dataproc connector for BigQuery."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 15,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-06-22T04:27:00.000Z",
        "voteCount": 21,
        "content": "B. I think. BiqQuery definitely minimizes computational time for normalization. I think it would also minimize manual intervention. For data normalization in dataflow you'd have to pass in values of mean and standard deviation as a side-input. That seems more work than a simple SQL query"
      },
      {
        "date": "2021-10-21T05:58:00.000Z",
        "voteCount": 2,
        "content": "I agree that B would definitely get the job done. But wouldn't D work as well and keep all the data pre-processing in Dataflow?"
      },
      {
        "date": "2021-11-13T13:25:00.000Z",
        "voteCount": 4,
        "content": "Dataflow uses Beam, different from  dataproc that uses Spark.\n\nI think that D would be wrong because we would add one more service into the pipeline for a simple transformation (minus the mean and divide by std)."
      },
      {
        "date": "2024-06-06T07:35:00.000Z",
        "voteCount": 1,
        "content": "B) Using BigQuery"
      },
      {
        "date": "2023-11-15T12:32:00.000Z",
        "voteCount": 1,
        "content": "z-scores is very easy to do in BQ - no need for more complex solutions"
      },
      {
        "date": "2023-09-11T23:58:00.000Z",
        "voteCount": 2,
        "content": "B. All that maartenalexander said, + BigQuery already has a function for that: https://cloud.google.com/bigquery/docs/reference/standard-sql/bigqueryml-syntax-standard-scaler , we could even schedule the query for calculating this automatically :)"
      },
      {
        "date": "2023-07-14T03:27:00.000Z",
        "voteCount": 1,
        "content": "Every week when new data is loaded mean and standard deviation is calculated for it and passed as parameter to calculate z score at serving \nhttps://towardsdatascience.com/how-to-normalize-features-in-tensorflow-5b7b0e3a4177"
      },
      {
        "date": "2023-07-20T09:30:00.000Z",
        "voteCount": 1,
        "content": "owever, in the given scenario, you are using Dataflow for preprocessing and BigQuery for storing data.\n\nTo make the process more efficient by minimizing computation time and manual intervention, you should still opt for option B: Translate the normalization algorithm into SQL for use with BigQuery. This way, you can perform the normalization directly in BigQuery, which will save time and resources compared to using an external tool."
      },
      {
        "date": "2023-07-07T10:38:00.000Z",
        "voteCount": 1,
        "content": "A, D usually need additional configuration, which could cost much more time."
      },
      {
        "date": "2023-05-08T23:04:00.000Z",
        "voteCount": 2,
        "content": "Went with B"
      },
      {
        "date": "2023-03-24T06:18:00.000Z",
        "voteCount": 2,
        "content": "Best way is B"
      },
      {
        "date": "2023-02-28T09:17:00.000Z",
        "voteCount": 2,
        "content": "Option D is the best solution because Apache Spark provides a distributed computing platform that can handle large-scale data processing with ease. By using the Dataproc connector for BigQuery, Spark can read data directly from BigQuery and perform the normalization process in a distributed manner. This can significantly reduce computation time and manual intervention. Option A is not a good solution because Kubernetes is a container orchestration platform that does not directly provide data normalization capabilities. Option B is not a good solution because Z-score normalization is a data transformation technique that cannot be easily translated into SQL. Option C is not a good solution because the normalizer_fn argument in TensorFlow's Feature Column API is only applicable for feature normalization during model training, not for data preprocessing."
      },
      {
        "date": "2023-01-11T07:26:00.000Z",
        "voteCount": 2,
        "content": "Best way to proceed is B."
      },
      {
        "date": "2023-02-28T09:19:00.000Z",
        "voteCount": 1,
        "content": "SQL is not as flexible as other programming languages like Python, which can limit the ability to customize the normalization process or incorporate new features in the future."
      },
      {
        "date": "2022-06-13T10:47:00.000Z",
        "voteCount": 4,
        "content": "B is the most efficient as you will not load --&gt; process --&gt; save , no you will only write some sql in bigquery and voila :D"
      },
      {
        "date": "2022-03-17T06:59:00.000Z",
        "voteCount": 2,
        "content": "It's B, bigquery can do this internally, no need for dataflow"
      },
      {
        "date": "2023-02-28T09:19:00.000Z",
        "voteCount": 1,
        "content": "SQL is not as flexible as other programming languages like Python, which can limit the ability to customize the normalization process or incorporate new features in the future."
      },
      {
        "date": "2022-01-31T21:01:00.000Z",
        "voteCount": 2,
        "content": "I agree with B."
      },
      {
        "date": "2021-07-06T00:03:00.000Z",
        "voteCount": 3,
        "content": "B. I agree with B as well."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 34,
    "url": "https://www.examtopics.com/discussions/google/view/54964-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You need to design a customized deep neural network in Keras that will predict customer purchases based on their purchase history. You want to explore model performance using multiple model architectures, store training data, and be able to compare the evaluation metrics in the same dashboard. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate multiple models using AutoML Tables.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAutomate multiple training runs using Cloud Composer.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun multiple training jobs on AI Platform with similar job names.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an experiment in Kubeflow Pipelines to organize multiple runs.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 16,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-07-09T00:18:00.000Z",
        "voteCount": 12,
        "content": "D - https://www.kubeflow.org/docs/about/use-cases/"
      },
      {
        "date": "2021-06-09T03:38:00.000Z",
        "voteCount": 6,
        "content": "Should be D"
      },
      {
        "date": "2024-06-06T07:37:00.000Z",
        "voteCount": 1,
        "content": "D) Experiments is the way forward"
      },
      {
        "date": "2023-11-23T07:22:00.000Z",
        "voteCount": 2,
        "content": "I would vote for D but if C had said instead \"different job names\" .. would that have been a better option?"
      },
      {
        "date": "2023-11-15T12:36:00.000Z",
        "voteCount": 1,
        "content": "D - everything else is just nonsense"
      },
      {
        "date": "2023-07-07T10:43:00.000Z",
        "voteCount": 2,
        "content": "D should be correct"
      },
      {
        "date": "2023-07-07T07:21:00.000Z",
        "voteCount": 1,
        "content": "C has similar job name, which make it wrong\nSo correct answer should be D"
      },
      {
        "date": "2023-07-02T03:26:00.000Z",
        "voteCount": 5,
        "content": "The best approach is to create an experiment in Kubeflow Pipelines to organize multiple runs. \n\nOption A is incorrect because AutoML Tables is a managed machine learning service that automates the process of building machine learning models from tabular data. It does not provide the flexibility to customize the model architecture or explore multiple model architectures.\n\nOption B is incorrect because Cloud Composer is a managed workflow orchestration service that can be used to automate machine learning workflows. However, it does not provide the same level of flexibility or scalability as Kubeflow Pipelines.\n\nOption C is incorrect because running multiple training jobs on AI Platform with similar job names will not allow you to easily organize and compare the results."
      },
      {
        "date": "2023-05-08T23:04:00.000Z",
        "voteCount": 1,
        "content": "Went with D"
      },
      {
        "date": "2023-02-28T09:21:00.000Z",
        "voteCount": 1,
        "content": "With Kubeflow Pipelines, you can create experiments that help you keep track of multiple training runs with different model architectures and hyperparameters."
      },
      {
        "date": "2022-12-29T03:52:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/vertex-ai/docs/experiments/user-journey/uj-compare-models"
      },
      {
        "date": "2022-08-22T21:23:00.000Z",
        "voteCount": 1,
        "content": "D\noption C does not work since CAIP have updated to VertexAI"
      },
      {
        "date": "2022-07-09T07:54:00.000Z",
        "voteCount": 1,
        "content": "https://www.kubeflow.org/docs/components/pipelines/concepts/experiment/\nhttps://www.kubeflow.org/docs/components/pipelines/concepts/run/"
      },
      {
        "date": "2022-04-14T07:50:00.000Z",
        "voteCount": 3,
        "content": "D- we need to use experiments feature to comapre models,having different jobnames is not going to help track experiments."
      },
      {
        "date": "2022-01-21T22:41:00.000Z",
        "voteCount": 2,
        "content": "C for me. It only talks about experimentation .. thats where AI platform fits better."
      },
      {
        "date": "2022-01-01T05:27:00.000Z",
        "voteCount": 1,
        "content": "Similar job names is a bit of a confusion creator as we can not use same job names for sure.  D sounds better but better in vertex AI during experiment phase only."
      },
      {
        "date": "2021-10-29T11:15:00.000Z",
        "voteCount": 4,
        "content": "C anyone? D seems to me like an overkill."
      },
      {
        "date": "2021-11-13T13:46:00.000Z",
        "voteCount": 3,
        "content": "(C) presents the most specific solution for what the question asks for: experimenting with models with their due comparisons. All of this is possible with the AI Platform. Furthermore, the question only speaks of experimentation. Kubeflow would be more powerfull if was a necessity for end-to-end pipeline."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 35,
    "url": "https://www.examtopics.com/discussions/google/view/55564-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You are developing a Kubeflow pipeline on Google Kubernetes Engine. The first step in the pipeline is to issue a query against BigQuery. You plan to use the results of that query as the input to the next step in your pipeline. You want to achieve this in the easiest way possible. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the BigQuery console to execute your query, and then save the query results into a new BigQuery table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite a Python script that uses the BigQuery API to execute queries against BigQuery. Execute this script as the first step in your Kubeflow pipeline.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Kubeflow Pipelines domain-specific language to create a custom component that uses the Python BigQuery client library to execute queries.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLocate the Kubeflow Pipelines repository on GitHub. Find the BigQuery Query Component, copy that component's URL, and use it to load the component into your pipeline. Use the component to execute queries against BigQuery.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 11,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 10,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-06-22T04:33:00.000Z",
        "voteCount": 21,
        "content": "D. Kubeflow pipelines have different types of components, ranging from low- to high-level. They have a ComponentStore that allows you to access prebuilt functionality from GitHub."
      },
      {
        "date": "2021-08-01T15:03:00.000Z",
        "voteCount": 6,
        "content": "agree, links: https://github.com/kubeflow/pipelines/blob/master/components/gcp/bigquery/query/sample.ipynb;  https://v0-5.kubeflow.org/docs/pipelines/reusable-components/"
      },
      {
        "date": "2022-01-01T05:23:00.000Z",
        "voteCount": 6,
        "content": "Not sure what is the reason behind putting A as it is manual and manual steps can not be part of automation. I would say Answer is D as it just require a clone of the component from github. Using a Python and import bigquery component may sounds good too, but ask was what is easiest. It depends how word \"easy\" is taken by individuals but definitely not A."
      },
      {
        "date": "2024-08-17T13:53:00.000Z",
        "voteCount": 1,
        "content": "D is the correct answer, as reusing an existing component is the most streamlined way to interact with BigQuery."
      },
      {
        "date": "2024-08-04T20:53:00.000Z",
        "voteCount": 1,
        "content": "much simpler to just write a couple of lines of python"
      },
      {
        "date": "2024-08-03T02:03:00.000Z",
        "voteCount": 1,
        "content": "Clearly B"
      },
      {
        "date": "2024-06-06T07:41:00.000Z",
        "voteCount": 3,
        "content": "B) Python API"
      },
      {
        "date": "2024-05-05T13:41:00.000Z",
        "voteCount": 2,
        "content": "from kfp.components import load_component_from_url\n\nbigquery_query_op = load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/master/components/gcp/bigquery/query/component.yaml')\n\ndef my_pipeline():\n    query_result = bigquery_query_op(\n        project_id='my-project',\n        query='SELECT * FROM my_dataset.my_table'\n    )\n    # Use the query_result as input to the next step in the pipeline"
      },
      {
        "date": "2023-12-05T00:54:00.000Z",
        "voteCount": 3,
        "content": "Im going \"against the flow\" and chosing B. It just sounds a lot easier option than D."
      },
      {
        "date": "2023-06-20T06:31:00.000Z",
        "voteCount": 2,
        "content": "Very confused as to why D is the correct answer. To me it seems a) much simpler to just write a couple of lines of python (https://cloud.google.com/bigquery/docs/reference/libraries#client-libraries-install-python) and b) the documentation for the BigQuery reusable component (https://v0-5.kubeflow.org/docs/pipelines/reusable-components/) states that the data is written to Google Cloud Storage, which means we have to write the fetching logic in the next pipeline step, going against the \"as simple as possible\" requirement. Would be interested to hear why I am wrong."
      },
      {
        "date": "2023-06-21T22:45:00.000Z",
        "voteCount": 2,
        "content": "Actually, the problem statement even says that the query result has to be used as input to the next step, meaning with answer D) we would have to download the results before passing them to the next step. Additionally, we would have to handle potentially existing files in Google Cloud Storage if the pipeline is either executed multiple times or even in parallel. (I will die on this hill \ud83d\ude06 )."
      },
      {
        "date": "2023-11-05T10:11:00.000Z",
        "voteCount": 2,
        "content": "Yup, you raised valid points. Depending on your specific requirements and familiarity with Python, writing a custom script using the BigQuery API (Option B) can be a simpler and more flexible approach.\n\nWith Option B, you can write a Python script that uses the BigQuery API to execute queries against BigQuery and fetch the data directly into your pipeline. This way, you can process the data as needed and pass it to the next step in the pipeline without the need to fetch it from Google Cloud Storage.\n\nWhile using the reusable BigQuery Query Component (Option D) provides a pre-built solution, it does require additional steps to fetch the data from Google Cloud Storage for the next step in the pipeline, which might not be the simplest approach."
      },
      {
        "date": "2023-05-08T23:04:00.000Z",
        "voteCount": 1,
        "content": "Went with D"
      },
      {
        "date": "2022-07-09T08:02:00.000Z",
        "voteCount": 1,
        "content": "https://linuxtut.com/en/f4771efee37658c083cc/"
      },
      {
        "date": "2022-07-09T08:04:00.000Z",
        "voteCount": 1,
        "content": "answer between C,D but above link has an article which uses a ready .yml file for bigquery component on official kubeflow pipelines repo"
      },
      {
        "date": "2022-05-09T19:16:00.000Z",
        "voteCount": 2,
        "content": "Answer is D."
      },
      {
        "date": "2022-03-29T18:14:00.000Z",
        "voteCount": 1,
        "content": "A. it says the easiest way possible so it sounds like just running the query on the console should be enogh. It doesn't says that the data will need to be uploaded again anytime soon, so we can asume that its just a one time query to be run."
      },
      {
        "date": "2022-05-09T19:15:00.000Z",
        "voteCount": 3,
        "content": "A is wrong. Answer is D. It's a pipeline which means you will run it multiple times? Do you always want to make the query manually each time you run your pipeline?"
      },
      {
        "date": "2022-01-31T21:11:00.000Z",
        "voteCount": 2,
        "content": "D is good."
      },
      {
        "date": "2021-11-29T05:11:00.000Z",
        "voteCount": 2,
        "content": "The result of D is just the path to the Cloud Storage where the result is stored not the data itself. So the input to the next step is this path, where you still have to load the data? So i would guess B. Can anyone explain if i am wrong?"
      },
      {
        "date": "2021-11-13T13:51:00.000Z",
        "voteCount": 2,
        "content": "D. The easiest way possible in developer's world: copy code from stackoverflow or github hahaha. Jokes a part, I think D is the correct. (A) is manual, so you have to do always. (B) could be, but is not the easiest one because you need to write a script for this. (C) uses Kubeflow intern solution, but you need to work to create a custom component. (D) is the (C) solution, but easier using a component created previously to do the job."
      },
      {
        "date": "2021-07-20T08:07:00.000Z",
        "voteCount": 1,
        "content": "ans: c\nhttps://medium.com/google-cloud/using-bigquery-and-bigquery-ml-from-kubeflow-pipelines-991a2fa4bea8\nhttps://cloud.google.com/architecture/architecture-for-mlops-using-tfx-kubeflow-pipelines-and-cloud-build#kubeflow-piplines-components\nKubeflow Pipelines, a containerized task can invoke other services such as BigQuery jobs, AI Platform (distributed) training jobs, and Dataflow jobs."
      },
      {
        "date": "2021-09-04T23:50:00.000Z",
        "voteCount": 6,
        "content": "why create a custom component when a big query's reusable component is already present. Answer is D."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 36,
    "url": "https://www.examtopics.com/discussions/google/view/55829-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You are building a model to predict daily temperatures. You split the data randomly and then transformed the training and test datasets. Temperature data for model training is uploaded hourly. During testing, your model performed with 97% accuracy; however, after deploying to production, the model's accuracy dropped to 66%. How can you make your production model more accurate?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNormalize the data for the training, and test datasets as two separate steps.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSplit the training and test data based on time rather than a random split to avoid leakage.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd more data to your test set to ensure that you have a fair distribution and sample for testing.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tApply data transformations before splitting, and cross-validate to make sure that the transformations are applied to both the training and test sets."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 15,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-06-22T04:46:00.000Z",
        "voteCount": 34,
        "content": "B. If you do time series prediction, you can't borrow information from the future to predict the future. If you do, you are artificially increasing your accuracy."
      },
      {
        "date": "2024-09-07T21:33:00.000Z",
        "voteCount": 1,
        "content": "It's D"
      },
      {
        "date": "2024-09-07T21:34:00.000Z",
        "voteCount": 1,
        "content": "B I mean. Sorry I wrote that comment very early and there is no delete key!"
      },
      {
        "date": "2024-08-03T07:00:00.000Z",
        "voteCount": 1,
        "content": "temporal split is a must in time series forecasting evaluation"
      },
      {
        "date": "2024-06-06T08:13:00.000Z",
        "voteCount": 1,
        "content": "B) Time split to avoid leaking data."
      },
      {
        "date": "2023-12-05T00:55:00.000Z",
        "voteCount": 1,
        "content": "Definetely B"
      },
      {
        "date": "2023-11-15T12:40:00.000Z",
        "voteCount": 1,
        "content": "they did not explicitly say forecasting, but splitting by time is the number one rule you learn"
      },
      {
        "date": "2023-05-08T23:04:00.000Z",
        "voteCount": 1,
        "content": "Went with B"
      },
      {
        "date": "2023-03-24T06:34:00.000Z",
        "voteCount": 2,
        "content": "D is correct. cross-validate"
      },
      {
        "date": "2022-06-13T11:27:00.000Z",
        "voteCount": 2,
        "content": "train accuracy 97% , production accuracy 66% ---&gt; time series data ---&gt; random split ---&gt; cause leakage , answer is B"
      },
      {
        "date": "2022-05-09T13:31:00.000Z",
        "voteCount": 3,
        "content": "You don't split data randomly for time series prediction."
      },
      {
        "date": "2022-04-14T07:58:00.000Z",
        "voteCount": 2,
        "content": "B should be the answer. D is incorrect as normalize before split is going to do data leak https://community.rapidminer.com/discussion/32592/normalising-data-before-data-split-or-after"
      },
      {
        "date": "2022-03-08T07:08:00.000Z",
        "voteCount": 3,
        "content": "If you do random split in a time series, your risk that training data will contain information about the target (definition of leakage), but similar data won't be available when the model is used for prediction. Leakage causes the model to look accurate until you start making actual predictions with it."
      },
      {
        "date": "2022-01-31T21:14:00.000Z",
        "voteCount": 2,
        "content": "agree B as well"
      },
      {
        "date": "2021-12-20T13:05:00.000Z",
        "voteCount": 2,
        "content": "I think is B"
      },
      {
        "date": "2021-09-08T17:39:00.000Z",
        "voteCount": 3,
        "content": "B. D doesn't improve anything at all. Split and Transform is no different than Transform and Split if the transform logic is the same."
      },
      {
        "date": "2021-08-25T17:30:00.000Z",
        "voteCount": 1,
        "content": "seems like D"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 37,
    "url": "https://www.examtopics.com/discussions/google/view/54966-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You are developing models to classify customer support emails. You created models with TensorFlow Estimators using small datasets on your on-premises system, but you now need to train the models using large datasets to ensure high performance. You will port your models to Google Cloud and want to minimize code refactoring and infrastructure overhead for easier migration from on-prem to cloud. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AI Platform for distributed training.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a cluster on Dataproc for training.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Managed Instance Group with autoscaling.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Kubeflow Pipelines to train on a Google Kubernetes Engine cluster."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-06-22T04:51:00.000Z",
        "voteCount": 28,
        "content": "A. AI platform provides lower infrastructure overhead and allows you to not have to refactor your code too much (no containerization and such, like in KubeFlow)."
      },
      {
        "date": "2024-06-06T08:17:00.000Z",
        "voteCount": 1,
        "content": "A) AI Platform"
      },
      {
        "date": "2024-05-26T09:49:00.000Z",
        "voteCount": 2,
        "content": "The most suitable option for minimizing code refactoring and infrastructure overhead while enabling large-scale training on Google Cloud is:\n\nA. Use AI Platform for distributed training.\n* **Simplified Workflow:** AI Platform offers a managed service for training machine learning models. You can train your existing TensorFlow Estimator code with minimal changes, reducing the need for extensive code refactoring.\n* **Distributed Training:** AI Platform automatically handles distributing your training job across multiple machines, allowing you to leverage the power of Google's cloud infrastructure to train on large datasets efficiently.\n* **Reduced Infrastructure Overhead:** You don't need to manage the underlying infrastructure (e.g., setting up and maintaining a cluster) yourself. AI Platform takes care of all the infrastructure provisioning and management, minimizing the workload on your team."
      },
      {
        "date": "2023-12-05T01:00:00.000Z",
        "voteCount": 2,
        "content": "I chose A. Even though D is a working option, it requires us to create a GKE cluster, which requires more work."
      },
      {
        "date": "2023-11-15T12:42:00.000Z",
        "voteCount": 1,
        "content": "A - because it has native support for TF"
      },
      {
        "date": "2023-07-13T08:03:00.000Z",
        "voteCount": 2,
        "content": "A. Use AI Platform for distributed training. : Managed , low infra change migration: yes , although need code refactoring to bigquery sql\nB. Create a cluster on Dataproc for training. : only cluster ? what about training?\nC. Create a Managed Instance Group with autoscaling.  : Same Q?\nD. Use Kubeflow Pipelines to train on a Google Kubernetes Engine cluster :  only training?"
      },
      {
        "date": "2023-05-08T23:05:00.000Z",
        "voteCount": 1,
        "content": "Went with A"
      },
      {
        "date": "2023-02-28T09:33:00.000Z",
        "voteCount": 1,
        "content": "Option A is the best choice as AI Platform provides a distributed training framework, enabling you to train large-scale models faster and with less effort"
      },
      {
        "date": "2022-06-13T11:34:00.000Z",
        "voteCount": 1,
        "content": "using options eliminations answer between A,D will vote for A as it is easier"
      },
      {
        "date": "2022-06-13T11:34:00.000Z",
        "voteCount": 1,
        "content": "- using options eliminations answer between A,D will vote for A as it is easier"
      },
      {
        "date": "2022-05-09T13:28:00.000Z",
        "voteCount": 2,
        "content": "The answer is A. AI platform also contains kubeflow pipelines. you don't need to set up infrastructure to use it. For D you need to set up a kubernetes  cluster engine. The question asks us to minimize infrastructure overheard."
      },
      {
        "date": "2022-04-14T08:02:00.000Z",
        "voteCount": 1,
        "content": "D- Kubeflow pipelines with Vertex ai provides you ability to reuse existing code using a TF conatiner in a pipeline. it helps automate the process. there is a qwiklab walking through this. \nA-incorrect, question is asking resuse existing code with minimum changes. distributed deployment does not address that."
      },
      {
        "date": "2022-05-09T13:26:00.000Z",
        "voteCount": 2,
        "content": "The answer is A. AI platform also contains kubeflow pipelines. you don't need to set up infrastructure to use it. For D you need to set up a kubernetes  cluster engine. The question asks us to minimize infrastructure overheard."
      },
      {
        "date": "2022-01-24T22:51:00.000Z",
        "voteCount": 2,
        "content": "A - better to go with managed service and distributed"
      },
      {
        "date": "2022-01-16T05:27:00.000Z",
        "voteCount": 1,
        "content": "I am 100% sure that the answer is D.\nKubeflow pipelines were designed keeping:\n\nA) Portability.\nB) Composability.\nC) Flexibility in mind. \n\nThis is the pain point that the kubeflow pipelines address"
      },
      {
        "date": "2022-05-09T13:27:00.000Z",
        "voteCount": 2,
        "content": "The answer is A. AI platform also contains kubeflow pipelines. you don't need to set up infrastructure to use it. For D you need to set up a kubernetes  cluster engine. The question asks us to minimize infrastructure overheard."
      },
      {
        "date": "2022-01-01T05:57:00.000Z",
        "voteCount": 3,
        "content": "TensorFlow Estimators means require distributed and that is key feature for AI platform or later Vertex AI."
      },
      {
        "date": "2021-12-20T13:10:00.000Z",
        "voteCount": 1,
        "content": "I think is A"
      },
      {
        "date": "2021-09-22T08:20:00.000Z",
        "voteCount": 3,
        "content": "I think the answer is either A or B, but personally think it is likely B because dataproc is a common tool box on GCP used for ML while AI platform might require refactoring. However, I dont really know A or B"
      },
      {
        "date": "2021-10-13T04:36:00.000Z",
        "voteCount": 5,
        "content": "Another vote for answer A. AI Platform distributed training here. \n\nHowever, I wanted to share my logic why its not B as well. Dataproc is a managed Hadoop and as such needs a processing engine for ML tasks. Most likely Spark and SparkML. Now Spark code is quite different than pure Python and SparkML is even more different than TFcode. I imagine there might me a way to convert TF code to run on SparkML, but this seems a lot of work. And besides the question specifically wants us to minimize refactoring, so there you have it, we can eliminate option B 100%."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 38,
    "url": "https://www.examtopics.com/discussions/google/view/54967-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You have trained a text classification model in TensorFlow using AI Platform. You want to use the trained model for batch predictions on text data stored in<br>BigQuery while minimizing computational overhead. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExport the model to BigQuery ML.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy and version the model on AI Platform.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Dataflow with the SavedModel to read the data from BigQuery.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSubmit a batch prediction job on AI Platform that points to the model location in Cloud Storage."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 15,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 9,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-06-22T04:53:00.000Z",
        "voteCount": 20,
        "content": "A. You would want to minimize computational overhead\u2013BigQuery minimizes such overhead"
      },
      {
        "date": "2021-09-22T08:31:00.000Z",
        "voteCount": 3,
        "content": "BQML doesnt support NLP model"
      },
      {
        "date": "2021-10-09T21:24:00.000Z",
        "voteCount": 9,
        "content": "you can import a TF model in BQ ML"
      },
      {
        "date": "2021-10-28T18:39:00.000Z",
        "voteCount": 5,
        "content": "agree. https://cloud.google.com/bigquery-ml/docs/making-predictions-with-imported-tensorflow-models"
      },
      {
        "date": "2023-07-13T08:10:00.000Z",
        "voteCount": 3,
        "content": "No need . This is a text classification problem. need to convert words to numbers and use a classifier."
      },
      {
        "date": "2021-06-18T06:30:00.000Z",
        "voteCount": 11,
        "content": "I think it's A\nhttps://cloud.google.com/bigquery-ml/docs/making-predictions-with-imported-tensorflow-models#importing_models"
      },
      {
        "date": "2024-06-06T08:28:00.000Z",
        "voteCount": 1,
        "content": "A) BigQuery ML"
      },
      {
        "date": "2024-05-26T09:59:00.000Z",
        "voteCount": 1,
        "content": "Use the gcloud command to submit a batch prediction job, specifying the model location in Cloud Storage and the BigQuery table as the input source."
      },
      {
        "date": "2024-08-26T05:26:00.000Z",
        "voteCount": 2,
        "content": "in the option D, it just mentioned GCS , BQ is no where to be found"
      },
      {
        "date": "2024-03-19T00:34:00.000Z",
        "voteCount": 2,
        "content": "Bquery to minimize computational overhead"
      },
      {
        "date": "2023-12-27T10:46:00.000Z",
        "voteCount": 1,
        "content": "Would go with D"
      },
      {
        "date": "2023-11-15T12:45:00.000Z",
        "voteCount": 2,
        "content": "A - you can import TF models to BQ"
      },
      {
        "date": "2023-07-13T08:14:00.000Z",
        "voteCount": 2,
        "content": "Model :  AI Platform. \npred batch data : BigQuery \nconstraint : computational overhead\n\nSame platform as data == less computation required to load and pass it to model"
      },
      {
        "date": "2023-07-07T07:24:00.000Z",
        "voteCount": 2,
        "content": "minimize computational overhead\u2013&gt;BigQuery"
      },
      {
        "date": "2023-06-08T02:10:00.000Z",
        "voteCount": 1,
        "content": "Not sure if when you have the saved model in Cloud storage that means that you don't use compute in vertex. I think that the option compute-free is bigquery"
      },
      {
        "date": "2023-06-08T01:59:00.000Z",
        "voteCount": 1,
        "content": "Not sure \nText Classification Using BigQuery ML and ML.NGRAMS\nhttps://medium.com/@jeffrey.james/text-classification-using-bigquery-ml-and-ml-ngrams-6e365f0b5505"
      },
      {
        "date": "2023-05-21T01:11:00.000Z",
        "voteCount": 2,
        "content": "I think D have extra compute on extrating data frm BQ"
      },
      {
        "date": "2023-05-17T10:10:00.000Z",
        "voteCount": 2,
        "content": "There are some drawbacks to option D.\n\nCost: Submitting a batch prediction job on AI Platform is a paid service. The cost will depend on the size of the model and the amount of data that you are predicting.\nComplexity: Submitting a batch prediction job on AI Platform requires you to write some code. This can be a challenge if you are not familiar with AI Platform.\nPerformance: Submitting a batch prediction job on AI Platform may not be as efficient as using BigQuery ML. This is because AI Platform needs to load the model into memory before it can run the predictions.\nOverall, option D is a viable option, but it may not be the best option for all situations."
      },
      {
        "date": "2023-05-08T23:05:00.000Z",
        "voteCount": 1,
        "content": "Went with D"
      },
      {
        "date": "2023-04-24T20:33:00.000Z",
        "voteCount": 1,
        "content": "why not C?"
      },
      {
        "date": "2023-04-20T06:16:00.000Z",
        "voteCount": 1,
        "content": "what about C?"
      },
      {
        "date": "2023-07-02T03:39:00.000Z",
        "voteCount": 2,
        "content": "This is an option that can be used to minimize computational overhead, but it is more complex to set up and requires you to have Dataflow installed."
      },
      {
        "date": "2023-11-26T04:25:00.000Z",
        "voteCount": 1,
        "content": "Although it's more complex, the question doesn't imply any restrictions on complexity, only computational overheard"
      },
      {
        "date": "2023-04-12T19:16:00.000Z",
        "voteCount": 1,
        "content": "D is more straightforward"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 39,
    "url": "https://www.examtopics.com/discussions/google/view/56104-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You work with a data engineering team that has developed a pipeline to clean your dataset and save it in a Cloud Storage bucket. You have created an ML model and want to use the data to refresh your model as soon as new data is available. As part of your CI/CD workflow, you want to automatically run a Kubeflow<br>Pipelines training job on Google Kubernetes Engine (GKE). How should you architect this workflow?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure your pipeline with Dataflow, which saves the files in Cloud Storage. After the file is saved, start the training job on a GKE cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse App Engine to create a lightweight python client that continuously polls Cloud Storage for new files. As soon as a file arrives, initiate the training job.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a Cloud Storage trigger to send a message to a Pub/Sub topic when a new file is available in a storage bucket. Use a Pub/Sub-triggered Cloud Function to start the training job on a GKE cluster.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Scheduler to schedule jobs at a regular interval. For the first step of the job, check the timestamp of objects in your Cloud Storage bucket. If there are no new files since the last run, abort the job."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 8,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-06-26T02:56:00.000Z",
        "voteCount": 15,
        "content": "C\nhttps://cloud.google.com/architecture/architecture-for-mlops-using-tfx-kubeflow-pipelines-and-cloud-build#triggering-and-scheduling-kubeflow-pipelines"
      },
      {
        "date": "2021-06-26T02:56:00.000Z",
        "voteCount": 7,
        "content": "C\nhttps://cloud.google.com/architecture/architecture-for-mlops-using-tfx-kubeflow-pipelines-and-cloud-build#triggering-and-scheduling-kubeflow-pipelines"
      },
      {
        "date": "2021-08-12T05:28:00.000Z",
        "voteCount": 1,
        "content": "On a schedule, using Cloud Scheduler.\nResponding to an event, using Pub/Sub and Cloud Functions. For example, the event can be the availability of new data files in a Cloud Storage bucket."
      },
      {
        "date": "2023-07-02T03:47:00.000Z",
        "voteCount": 1,
        "content": "Option D requires the job to be scheduled at regular intervals, even if there are no new files. This can waste resources and lead to unnecessary delays in the training process."
      },
      {
        "date": "2024-06-06T08:33:00.000Z",
        "voteCount": 1,
        "content": "C) PUB/sub trigger from Cloud Storage &amp; Cloud Function"
      },
      {
        "date": "2023-12-05T01:33:00.000Z",
        "voteCount": 1,
        "content": "C - This is the google reccomended method."
      },
      {
        "date": "2023-11-15T12:47:00.000Z",
        "voteCount": 1,
        "content": "C- because you don't want to re-engineer the pipeline"
      },
      {
        "date": "2023-05-08T23:05:00.000Z",
        "voteCount": 1,
        "content": "Went with C"
      },
      {
        "date": "2023-02-28T09:50:00.000Z",
        "voteCount": 1,
        "content": "The scenario involves automatically running a Kubeflow Pipelines training job on GKE as soon as new data becomes available. To achieve this, we can use Cloud Storage to store the cleaned dataset, and then configure a Cloud Storage trigger that sends a message to a Pub/Sub topic whenever a new file is added to the storage bucket. We can then create a Pub/Sub-triggered Cloud Function that starts the training job on a GKE cluster."
      },
      {
        "date": "2023-01-04T11:38:00.000Z",
        "voteCount": 1,
        "content": "The question says: As part of your CI/CD workflow, you want to automatically run a Kubeflow..\n\nC is also an option but it seems more cumbersome. \nOne thing hat could be against A is that the data engineering team is separate team so they might not access your CI/CD if any changes from their side is needed.."
      },
      {
        "date": "2023-07-02T03:47:00.000Z",
        "voteCount": 1,
        "content": "Option A requires the data engineering team to modify the pipeline, which can be time-consuming and error-prone."
      },
      {
        "date": "2022-12-08T15:38:00.000Z",
        "voteCount": 2,
        "content": "C\nPubsub is the keyword"
      },
      {
        "date": "2022-07-09T08:34:00.000Z",
        "voteCount": 1,
        "content": "event driven architecture is better than polling based architecure so I will vote for C"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 40,
    "url": "https://www.examtopics.com/discussions/google/view/56120-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You have a functioning end-to-end ML pipeline that involves tuning the hyperparameters of your ML model using AI Platform, and then using the best-tuned parameters for training. Hypertuning is taking longer than expected and is delaying the downstream processes. You want to speed up the tuning job without significantly compromising its effectiveness. Which actions should you take? (Choose two.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDecrease the number of parallel trials.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDecrease the range of floating-point values.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the early stopping parameter to TRUE.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the search algorithm from Bayesian search to random search.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDecrease the maximum number of trials during subsequent training phases.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "CE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "CE",
        "count": 12,
        "isMostVoted": true
      },
      {
        "answer": "CD",
        "count": 9,
        "isMostVoted": false
      },
      {
        "answer": "AD",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "AC",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "AB",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "DE",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-08-01T18:51:00.000Z",
        "voteCount": 19,
        "content": "I think should CE. I can't find any reference regarding B can reduce tuning time."
      },
      {
        "date": "2021-06-26T08:19:00.000Z",
        "voteCount": 16,
        "content": "Answer: B &amp; C (Ref: https://cloud.google.com/ai-platform/training/docs/using-hyperparameter-tuning)\n(A) Decreasing the number of parallel trials will increase tuning time.\n(D) Bayesian search works better and faster than random search since it's selective in points to evaluate and uses knowledge of previouls evaluated points.\n(E) maxTrials should be larger than 10*the number of hyperparameters used. And spanning the whole minimum space (10*num_hyperparams) already takes some time. So, lowering maxTrials has little effect on reducing tuning time."
      },
      {
        "date": "2024-06-06T10:43:00.000Z",
        "voteCount": 1,
        "content": "Bayesian search should cost more time, because it can converge in fewer iterations than the other algorithms but not necessarily in a faster time because trials are dependent and thus require sequentiality"
      },
      {
        "date": "2021-09-04T00:25:00.000Z",
        "voteCount": 10,
        "content": "In your link, when they mentionned maxTrials they said that \"In most cases there is a point of diminishing returns after which additional trials have little or no effect on the accuracy\"\nThey also say that it can affect time and cost\nI think i'd rather go with CE"
      },
      {
        "date": "2024-08-16T02:35:00.000Z",
        "voteCount": 1,
        "content": "In the PMLE book it's grid search instead of Bayesian search and that makes sense, but there is also marked Decrease the number of parallel trials as correct answer, which I think should be wrong."
      },
      {
        "date": "2024-08-04T21:03:00.000Z",
        "voteCount": 1,
        "content": "With Vertex AI hyperparameter tuning, you can configure the number of trials and the search algorithm as well as range of parameters."
      },
      {
        "date": "2024-06-06T08:38:00.000Z",
        "voteCount": 2,
        "content": "C) and D)"
      },
      {
        "date": "2024-04-14T08:04:00.000Z",
        "voteCount": 2,
        "content": "see  pawan94"
      },
      {
        "date": "2024-01-07T09:01:00.000Z",
        "voteCount": 4,
        "content": "C and E, if you reference the latest docs of hptune job on vertex ai :\n1. A not possible (refer: https://cloud.google.com/vertex-ai/docs/training/using-hyperparameter-tuning#:~:text=the%20benefit%20of%20reducing%20the%20time%20the) , if you reduce the number of parallel trials then the speed of overall completion gets negatively affected. \n. The question is about how to speed up the process but not changing the model params. Changing the optimization algorithm would lead to unexpected results.\n\nSo in my opinion C and E ( after carefully reading the updated docs) and please don't believe everything CHATGPT says . I encountered so many questions where the LLM's are giving completely wrong answers"
      },
      {
        "date": "2023-12-05T01:37:00.000Z",
        "voteCount": 3,
        "content": "I chose C and D"
      },
      {
        "date": "2023-11-15T12:52:00.000Z",
        "voteCount": 3,
        "content": "Chat GPT says:\n. Set the early stopping parameter to TRUE.\n\nEarly Stopping: Enabling early stopping allows the tuning process to terminate a trial if it becomes clear that it's not producing promising results. This prevents wasting time on unpromising trials and can significantly speed up the hyperparameter tuning process. It helps to focus resources on more promising parameter combinations.\nD. Change the search algorithm from Bayesian search to random search.\n\nRandom Search Algorithm: Random search, as opposed to Bayesian optimization, doesn't attempt to build a model of the objective function. While Bayesian search can be more efficient in finding the optimal parameters, random search is often faster per iteration. Random search can be particularly effective when the hyperparameter space is large, as it doesn't require as much computational power to select the next set of parameters to evaluate."
      },
      {
        "date": "2023-06-07T01:16:00.000Z",
        "voteCount": 3,
        "content": "C&amp;E\nThis video explains very well the max trials and parallel trials\nhttps://youtu.be/8hZ_cBwNOss\nThis link explains early stopping\nSee https://cloud.google.com/ai-platform/training/docs/using-hyperparameter-tuning#early-stopping"
      },
      {
        "date": "2023-05-21T01:29:00.000Z",
        "voteCount": 1,
        "content": "A increase time, B HP tuning job normally bottle neck is not at model size, D did reduce time, but might significantly hurt effectiveness"
      },
      {
        "date": "2023-05-09T02:06:00.000Z",
        "voteCount": 1,
        "content": "Running parallel trials has the benefit of reducing the time the training job takes (real time\u2014the total processing time required is not typically changed). However, running in parallel can reduce the effectiveness of the tuning job overall. That is because hyperparameter tuning uses the results of previous trials to inform the values to assign to the hyperparameters of subsequent trials. When running in parallel, some trials start without having the benefit of the results of any trials still running.\nYou can specify that AI Platform Training must automatically stop a trial that has become clearly unpromising. This saves you the cost of continuing a trial that is unlikely to be useful.\n\nTo permit stopping a trial early, set the enableTrialEarlyStopping value in the HyperparameterSpec to TRUE."
      },
      {
        "date": "2023-05-08T23:05:00.000Z",
        "voteCount": 1,
        "content": "Went with C &amp; E"
      },
      {
        "date": "2023-03-31T00:10:00.000Z",
        "voteCount": 1,
        "content": "To speed up the tuning job without significantly compromising its effectiveness, you can take the following actions:\n\nA. Decrease the number of parallel trials: By reducing the number of parallel trials, you can limit the amount of computational resources being used at a given time, which may help speed up the tuning job. However, reducing the number of parallel trials too much could limit the exploration of the parameter space and result in suboptimal results.\n\nD. Change the search algorithm from Bayesian search to random search: Bayesian optimization is a computationally intensive method that requires more time and resources than random search. By switching to a simpler method like random search, you may be able to speed up the tuning job without compromising its effectiveness. However, random search may not be as efficient in finding the best hyperparameters as Bayesian optimization."
      },
      {
        "date": "2023-03-23T23:49:00.000Z",
        "voteCount": 1,
        "content": "Early stopping is for training, not hyperparameter tuning"
      },
      {
        "date": "2023-02-28T10:02:00.000Z",
        "voteCount": 2,
        "content": "The two actions that can speed up hyperparameter tuning without compromising effectiveness are decreasing the number of parallel trials and changing the search algorithm from Bayesian search to random search."
      },
      {
        "date": "2023-02-21T04:35:00.000Z",
        "voteCount": 1,
        "content": "B. Decrease the range of floating-point values: Reducing the range of the hyperparameters will decrease the search space and the time it takes to find the optimal hyperparameters. However, if the range is too narrow, it may not be possible to find the best hyperparameters.\n\nC. Set the early stopping parameter to TRUE: Setting the early stopping parameter to true will stop the trial when the performance has stopped improving. This will help to reduce the number of trials needed and thus speed up the hypertuning job without compromising its effectiveness.\nD.Changing the search algorithm from Bayesian search to random search could also be a valid action to speed up the hypertuning job. Random search can explore the hyperparameter space more efficiently and with less computation cost compared to Bayesian search, especially when the search space is large and complex. However, it may not be as effective as Bayesian search in finding the best hyperparameters in some cases."
      },
      {
        "date": "2023-03-08T08:28:00.000Z",
        "voteCount": 1,
        "content": "D might not be the correct option, as for random search it might be faster but there might be a chance of decreased accuracy and this violates the questions as it says, to not comprise efficiency!"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 41,
    "url": "https://www.examtopics.com/discussions/google/view/54973-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "Your team is building an application for a global bank that will be used by millions of customers. You built a forecasting model that predicts customers' account balances 3 days in the future. Your team will use the results in a new feature that will notify users when their account balance is likely to drop below $25. How should you serve your predictions?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a Pub/Sub topic for each user. 2. Deploy a Cloud Function that sends a notification when your model predicts that a user's account balance will drop below the $25 threshold.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a Pub/Sub topic for each user. 2. Deploy an application on the App Engine standard environment that sends a notification when your model predicts that a user's account balance will drop below the $25 threshold.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Build a notification system on Firebase. 2. Register each user with a user ID on the Firebase Cloud Messaging server, which sends a notification when the average of all account balance predictions drops below the $25 threshold.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Build a notification system on Firebase. 2. Register each user with a user ID on the Firebase Cloud Messaging server, which sends a notification when your model predicts that a user's account balance will drop below the $25 threshold.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 21,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-06-09T04:23:00.000Z",
        "voteCount": 19,
        "content": "Should be D !\n creating a Pub/Sub topic for each user is overkill"
      },
      {
        "date": "2021-09-16T16:29:00.000Z",
        "voteCount": 3,
        "content": "Yes, create a topic is overkill but not a NOTIFICATION SYSTEM. it's totally normal.\nSeriously, the step two involves \"REGISTER EACH USER ....\", how is this better than create a topic????\n\nshould be A and it's so obvious!"
      },
      {
        "date": "2021-09-22T08:43:00.000Z",
        "voteCount": 3,
        "content": "I think A is straight forward answer but in real life, customer also consider cost, so practically, app engine will be picked in this case..... because of the large user base"
      },
      {
        "date": "2022-02-19T21:43:00.000Z",
        "voteCount": 9,
        "content": "D is correct.  Firebase is designed for exactly this sort of scenario.  Also, it would not be possible to create millions of pubsub topics due to GCP quotas\nhttps://cloud.google.com/pubsub/quotas#quotas\nhttps://firebase.google.com/docs/cloud-messaging"
      },
      {
        "date": "2024-06-06T08:48:00.000Z",
        "voteCount": 1,
        "content": "D) Firebase"
      },
      {
        "date": "2023-12-05T01:42:00.000Z",
        "voteCount": 1,
        "content": "D is correct. Firebase is used for applications."
      },
      {
        "date": "2023-07-13T08:29:00.000Z",
        "voteCount": 1,
        "content": "simple answer , use tools most mentioned during training . , cloud functions"
      },
      {
        "date": "2023-08-30T22:20:00.000Z",
        "voteCount": 3,
        "content": "Pub/Sub has a limit of 10,000 topics only and can't be increased https://cloud.google.com/pubsub/quotas#resource_limits."
      },
      {
        "date": "2023-05-08T23:06:00.000Z",
        "voteCount": 1,
        "content": "Went with D"
      },
      {
        "date": "2023-03-28T05:40:00.000Z",
        "voteCount": 2,
        "content": "\"Create a Pub/Sub topic for each user\" xD"
      },
      {
        "date": "2022-07-09T08:48:00.000Z",
        "voteCount": 3,
        "content": "\"Create a Pub/Sub topic for each user\" this is crazy , we can not imagine a system with millions of pub/sub topics , so A,B wrong\nC also wrong"
      },
      {
        "date": "2022-04-14T08:26:00.000Z",
        "voteCount": 1,
        "content": "D- is more automated compared to A. A is overkill"
      },
      {
        "date": "2022-01-18T10:54:00.000Z",
        "voteCount": 3,
        "content": "I think, D is the best answer"
      },
      {
        "date": "2022-01-16T13:50:00.000Z",
        "voteCount": 4,
        "content": "Project limit is 10,000 topics, you could have multiple projects but that does not scale well. so D.\nhttps://cloud.google.com/pubsub/quotas#resource_limits"
      },
      {
        "date": "2022-01-03T23:25:00.000Z",
        "voteCount": 3,
        "content": "D looks more relevant\nNotification messages: Simply display a message content, which is handled by the FCM SDK. Data Messages: Display a message with some set interactions"
      },
      {
        "date": "2021-11-07T15:26:00.000Z",
        "voteCount": 3,
        "content": "A doesn't work. There is a quota limit on the number of pub/sub topics you can create, also one Cloud function cannot subscribe to millions of topics. A doesn't scale at all."
      },
      {
        "date": "2021-11-07T15:20:00.000Z",
        "voteCount": 4,
        "content": "Answer is D. FCM is designed for this type of notification sent to mobile and desktop apps."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 42,
    "url": "https://www.examtopics.com/discussions/google/view/55062-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You work for an advertising company and want to understand the effectiveness of your company's latest advertising campaign. You have streamed 500 MB of campaign data into BigQuery. You want to query the table, and then manipulate the results of that query with a pandas dataframe in an AI Platform notebook.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AI Platform Notebooks' BigQuery cell magic to query the data, and ingest the results as a pandas dataframe.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExport your table as a CSV file from BigQuery to Google Drive, and use the Google Drive API to ingest the file into your notebook instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDownload your table from BigQuery as a local CSV file, and upload it to your AI Platform notebook instance. Use pandas.read_csv to ingest he file as a pandas dataframe.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrom a bash cell in your AI Platform notebook, use the bq extract command to export the table as a CSV file to Cloud Storage, and then use gsutil cp to copy the data into the notebook. Use pandas.read_csv to ingest the file as a pandas dataframe."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 21,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-06-10T07:31:00.000Z",
        "voteCount": 26,
        "content": "A: no \"CSV\" found in provided link https://cloud.google.com/bigquery/docs/bigquery-storage-python-pandas"
      },
      {
        "date": "2023-11-15T12:59:00.000Z",
        "voteCount": 5,
        "content": "A is the google recommended answer. And what you should use\nC is what the intern does ..."
      },
      {
        "date": "2024-01-04T03:59:00.000Z",
        "voteCount": 2,
        "content": "Dude, I laughed so hard"
      },
      {
        "date": "2024-06-06T08:50:00.000Z",
        "voteCount": 1,
        "content": "A) Magic command"
      },
      {
        "date": "2023-05-08T23:06:00.000Z",
        "voteCount": 2,
        "content": "Went with A"
      },
      {
        "date": "2023-03-28T05:46:00.000Z",
        "voteCount": 1,
        "content": "A, Using the command %%bigquery df"
      },
      {
        "date": "2023-02-14T16:04:00.000Z",
        "voteCount": 2,
        "content": "Why not D? using BQ notebook magic would be ok for a single time use. but usually a DS would reload the data multiple time, and every time you need to stream 500mb data to the notebook instance from BQ. Isn't it cheaper to store the data as a csv in a bucket?"
      },
      {
        "date": "2023-01-14T20:42:00.000Z",
        "voteCount": 4,
        "content": "%%bigquery df\nSELECT name, SUM(number) as count\nFROM `bigquery-public-data.usa_names.usa_1910_current`\nGROUP BY name\nORDER BY count DESC\nLIMIT 3\n\nprint(df.head())"
      },
      {
        "date": "2022-12-10T08:17:00.000Z",
        "voteCount": 2,
        "content": "A\nhttps://cloud.google.com/bigquery/docs/visualize-jupyter"
      },
      {
        "date": "2022-06-21T22:46:00.000Z",
        "voteCount": 2,
        "content": "Answer : A . Refer to this link for details: https://cloud.google.com/bigquery/docs/bigquery-storage-python-pandas\nFirst 2 points talks about querying the data. \nDownload query results to a pandas DataFrame by using the BigQuery Storage API from the IPython magics for BigQuery in a Jupyter notebook.\nDownload query results to a pandas DataFrame by using the BigQuery client library for Python.\nDownload BigQuery table data to a pandas DataFrame by using the BigQuery client library for Python.\nDownload BigQuery table data to a pandas DataFrame by using the BigQuery Storage API client library for Python."
      },
      {
        "date": "2022-06-11T08:05:00.000Z",
        "voteCount": 2,
        "content": "https://googleapis.dev/python/bigquery/latest/magics.html#ipython-magics-for-bigquery"
      },
      {
        "date": "2022-04-28T06:26:00.000Z",
        "voteCount": 3,
        "content": "this is the simplest and most straightforward way read BQ data into Pandas dataframe."
      },
      {
        "date": "2022-04-14T08:36:00.000Z",
        "voteCount": 1,
        "content": "both A and C is technically correct. C has more manual step and A has less. The question does not ask which requires least effort. so C is clear answer"
      },
      {
        "date": "2022-12-15T15:40:00.000Z",
        "voteCount": 2,
        "content": "\"A and C are valid, but C is more difficult than A. they don't ask to be easier so I will go with the more difficult\". WHAAAT?\nGoogle best practices are always: easier &gt; harder. Even they encourage you to skip ML if you don't need ML."
      },
      {
        "date": "2022-02-19T21:46:00.000Z",
        "voteCount": 1,
        "content": "C is the correct answer due to the size of the data.  It wouldn't be possible to download it all into an in memory data frame."
      },
      {
        "date": "2022-06-25T23:32:00.000Z",
        "voteCount": 2,
        "content": "500mb of data into a pandas dataframe generally isn't a problem, far from it."
      },
      {
        "date": "2022-01-19T09:56:00.000Z",
        "voteCount": 1,
        "content": "IPython magics for BigQuery\nhttps://cloud.google.com/bigquery/docs/bigquery-storage-python-pandas"
      },
      {
        "date": "2022-01-04T00:21:00.000Z",
        "voteCount": 1,
        "content": "I agree with A"
      },
      {
        "date": "2021-09-16T16:30:00.000Z",
        "voteCount": 2,
        "content": "Just load it \n\nhttps://googleapis.dev/python/bigquery/latest/magics.html"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 43,
    "url": "https://www.examtopics.com/discussions/google/view/57513-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You are an ML engineer at a global car manufacture. You need to build an ML model to predict car sales in different cities around the world. Which features or feature crosses should you use to train city-specific relationships between car type and number of sales?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThee individual features: binned latitude, binned longitude, and one-hot encoded car type.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOne feature obtained as an element-wise product between latitude, longitude, and car type.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOne feature obtained as an element-wise product between binned latitude, binned longitude, and one-hot encoded car type.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTwo feature crosses as an element-wise product: the first between binned latitude and one-hot encoded car type, and the second between binned longitude and one-hot encoded car type."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-07-31T03:22:00.000Z",
        "voteCount": 22,
        "content": "C\nhttps://developers.google.com/machine-learning/crash-course/feature-crosses/check-your-understanding"
      },
      {
        "date": "2021-07-11T21:15:00.000Z",
        "voteCount": 8,
        "content": "C should be the answer"
      },
      {
        "date": "2024-09-08T01:31:00.000Z",
        "voteCount": 2,
        "content": "While i acknowledge the answer is C, It seems wrong to elementwise combine binned lat/lon, as it means there are at least 2 places with the same number in the world, probably more. Not only but by multiplying the binned a values it implies they are ordinal, but they are not ordinal in the same direction, so the relationship on price will be lost (a good example is northern countries tend to be richer, but the east/west relationship isn't defined)"
      },
      {
        "date": "2024-06-06T08:53:00.000Z",
        "voteCount": 1,
        "content": "C) one feature"
      },
      {
        "date": "2023-11-15T13:01:00.000Z",
        "voteCount": 2,
        "content": "C - everything else is madness"
      },
      {
        "date": "2024-07-04T08:01:00.000Z",
        "voteCount": 1,
        "content": "creating this cross feature is madness from explainability standpoint"
      },
      {
        "date": "2023-05-08T23:06:00.000Z",
        "voteCount": 1,
        "content": "Went with C"
      },
      {
        "date": "2022-06-11T08:33:00.000Z",
        "voteCount": 4,
        "content": "https://developers.google.com/machine-learning/crash-course/feature-crosses/video-lecture"
      },
      {
        "date": "2022-01-25T22:32:00.000Z",
        "voteCount": 4,
        "content": "C - Answer\nwhen doing feature cross the features need to be binned"
      },
      {
        "date": "2022-01-09T22:31:00.000Z",
        "voteCount": 3,
        "content": "https://developers.google.com/machine-learning/crash-course/feature-crosses/check-your-understanding\nAnswer C: It needs a feature cross to obtain one feature."
      },
      {
        "date": "2022-01-04T00:22:00.000Z",
        "voteCount": 3,
        "content": "I got with C"
      },
      {
        "date": "2021-11-07T01:16:00.000Z",
        "voteCount": 1,
        "content": "\"element-wise product\" sounds like we are not using a feature cross but artificially creating a new column whose values is the \"element-wise product\" of other column values...; i.e., (1, 2, 3) =&gt; 1 * 2 * 3 = 6.\nI am not a native English speaker; thus, I might misunderstand the sentence."
      },
      {
        "date": "2021-07-09T00:55:00.000Z",
        "voteCount": 4,
        "content": "D - https://developers.google.com/machine-learning/crash-course/feature-crosses/video-lecture"
      },
      {
        "date": "2021-09-20T01:40:00.000Z",
        "voteCount": 13,
        "content": "Cannot be D, Despite Binning is a good idea because it enables the model to learn nonlinear relationships within a single feature; separate latitude and longitude in different feature crosses is not a good one, this separation will prevent the model from learning city-specific sales. A city is the conjunction of latitude and longitude.\n\nIn that order of Ideas Crossing binned latitude with binned longitude enables the model to learn city-specific effects of car type.\n\nI will go for C,\nhttps://developers.google.com/machine-learning/crash-course/feature-crosses/check-your-understanding"
      },
      {
        "date": "2021-10-09T07:25:00.000Z",
        "voteCount": 2,
        "content": "Damn that was a good explanation. Thank you for writing it out."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 44,
    "url": "https://www.examtopics.com/discussions/google/view/55569-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You work for a large technology company that wants to modernize their contact center. You have been asked to develop a solution to classify incoming calls by product so that requests can be more quickly routed to the correct support team. You have already transcribed the calls using the Speech-to-Text API. You want to minimize data preprocessing and development time. How should you build the model?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the AI Platform Training built-in algorithms to create a custom model.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AutoMlL Natural Language to extract custom entities for classification.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Cloud Natural Language API to extract custom entities for classification.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBuild a custom model to identify the product keywords from the transcribed calls, and then run the keywords through a classification algorithm."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 9,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 5,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-06-18T07:03:00.000Z",
        "voteCount": 22,
        "content": "Should be B\n-&gt; minimize data preprocessing and development time"
      },
      {
        "date": "2021-07-30T08:56:00.000Z",
        "voteCount": 6,
        "content": "Agree its B. A and D is incorrect since it requires more development time. C is also incorrect since the product is company specific and might not be well recognized by Cloud Natural Language API."
      },
      {
        "date": "2021-08-28T07:36:00.000Z",
        "voteCount": 2,
        "content": "I thought the answer is B too. However, after carefully reading the question and answers again, B produces entities for classification only, not a classification result.\nSo, A and D are only candidates and A is better."
      },
      {
        "date": "2024-08-15T22:56:00.000Z",
        "voteCount": 1,
        "content": "Cloud NLP API no require custom training"
      },
      {
        "date": "2024-06-06T08:56:00.000Z",
        "voteCount": 2,
        "content": "C) Cloud NLP API"
      },
      {
        "date": "2024-02-17T06:01:00.000Z",
        "voteCount": 3,
        "content": "I'm voting C here!"
      },
      {
        "date": "2024-02-12T18:58:00.000Z",
        "voteCount": 2,
        "content": "AutoML only has classification and regression"
      },
      {
        "date": "2023-09-07T08:32:00.000Z",
        "voteCount": 1,
        "content": "Key Differences:\n\nApproach: Option B (AutoML Natural Language) involves using an AutoML service to train a custom NLP model, while Option C (Cloud Natural Language API) relies on a pre-built NLP API.\n\nControl and Customization: Option B gives you more control and customization over the training process, as you train a model specific to your needs. Option C offers less control but is quicker to set up since it uses a pre-built API.\n\nComplexity: Option B might require more technical expertise to set up and configure the AutoML model, while Option C is more straightforward and user-friendly.\n\nIn summary, both options allow you to extract custom entities for classification, but Option B (AutoML) involves more manual involvement in training a custom model, while Option C (Cloud Natural Language API) provides a simpler, pre-built solution"
      },
      {
        "date": "2023-05-08T23:07:00.000Z",
        "voteCount": 2,
        "content": "Went with B"
      },
      {
        "date": "2023-04-20T06:34:00.000Z",
        "voteCount": 1,
        "content": "why not C?"
      },
      {
        "date": "2023-05-24T17:54:00.000Z",
        "voteCount": 1,
        "content": "you have to classify company products, which are custom classes"
      },
      {
        "date": "2024-08-13T15:05:00.000Z",
        "voteCount": 1,
        "content": "It seems to me that if there is a product name that needs to be learned in AutoML Natural Language, there is a possibility that it cannot be transcribed into text by the Speech-to-Text API in the first place."
      },
      {
        "date": "2023-09-07T08:34:00.000Z",
        "voteCount": 2,
        "content": "you can still use Option C (Cloud Natural Language API) even when the solution needs to classify incoming calls by company-specific products rather than general products. The Cloud Natural Language API can be customized to handle company-specific entities and classifications effectively."
      },
      {
        "date": "2023-02-28T02:25:00.000Z",
        "voteCount": 1,
        "content": "AutoML is appropriate to classify incoming calls by product (Custom) to be routed to the correct support team.\n\nCloud Natural Language API is for general case (not particular business)"
      },
      {
        "date": "2022-06-11T09:16:00.000Z",
        "voteCount": 2,
        "content": "\"minimize data preprocessing and development time\" answer will be limited to B,C\nwill choose C as Natural Language API  does not handle custom operation"
      },
      {
        "date": "2022-04-14T08:40:00.000Z",
        "voteCount": 4,
        "content": "B- automl custom classification and entity is going to help with minimum effort."
      },
      {
        "date": "2022-03-17T07:53:00.000Z",
        "voteCount": 4,
        "content": "I'm leaning towards C over B here. The question is underlining that minimal development time is required, and C is even less than B. If the information is really domain specific, then you'd need B, but it's not clear what products the company sells, so we don't have enough info to say it's too domain specific for C."
      },
      {
        "date": "2022-03-28T08:54:00.000Z",
        "voteCount": 8,
        "content": "If anything, C is wrong because it tells you something that is not true: extract custom entities with Natural Language API it's not possible. That is something you can do only with AutoML. Look at this comparison table: https://cloud.google.com/natural-language#section-6\nThat's how they subtly point you at answer B."
      },
      {
        "date": "2022-01-19T10:06:00.000Z",
        "voteCount": 4,
        "content": "AutoML Natural Language - custom entities, with least development time"
      },
      {
        "date": "2022-01-04T00:37:00.000Z",
        "voteCount": 2,
        "content": "Should be B\nBasic classification, entity extraction, and sentiment analysis are available through the Cloud Natural Language API. AutoML Natural Language enables you to define custom classification categories, entities, and sentiment scores that are relevant to your application."
      },
      {
        "date": "2022-05-09T13:18:00.000Z",
        "voteCount": 1,
        "content": "no. if you need custom entities you don't use APIs"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 45,
    "url": "https://www.examtopics.com/discussions/google/view/57011-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You are training a TensorFlow model on a structured dataset with 100 billion records stored in several CSV files. You need to improve the input/output execution performance. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLoad the data into BigQuery, and read the data from BigQuery.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLoad the data into Cloud Bigtable, and read the data from Bigtable.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConvert the CSV files into shards of TFRecords, and store the data in Cloud Storage.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConvert the CSV files into shards of TFRecords, and store the data in the Hadoop Distributed File System (HDFS)."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 13,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 6,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-07-09T01:02:00.000Z",
        "voteCount": 25,
        "content": "C - not enough info in the question, but C is the \"most correct\" one"
      },
      {
        "date": "2024-06-06T10:57:00.000Z",
        "voteCount": 1,
        "content": "C) The most suitable option for improving input/output execution performance in this scenario is C. Convert the CSV files into shards of TFRecords and store the data in Cloud Storage. This approach leverages the efficiency of TFRecords and the scalability of Cloud Storage, aligning with TensorFlow best practices."
      },
      {
        "date": "2023-12-05T02:06:00.000Z",
        "voteCount": 1,
        "content": "C is the google reccomended approach."
      },
      {
        "date": "2023-11-15T13:08:00.000Z",
        "voteCount": 1,
        "content": "C is the correct one as BQ will not help you with performance"
      },
      {
        "date": "2023-10-02T13:03:00.000Z",
        "voteCount": 2,
        "content": "C https://datascience.stackexchange.com/questions/16318/what-is-the-benefit-of-splitting-tfrecord-file-into-shards#:~:text=Splitting%20TFRecord%20files%20into%20shards,them%20through%20a%20training%20process."
      },
      {
        "date": "2023-10-02T13:03:00.000Z",
        "voteCount": 1,
        "content": "C https://datascience.stackexchange.com/questions/16318/what-is-the-benefit-of-splitting-tfrecord-file-into-shards#:~:text=Splitting%20TFRecord%20files%20into%20shards,them%20through%20a%20training%20process."
      },
      {
        "date": "2023-09-17T15:39:00.000Z",
        "voteCount": 1,
        "content": "bard: The correct answer is:\n\nC. Convert the CSV files into shards of TFRecords, and store the data in Cloud Storage.\nTFRecords is a TensorFlow-specific binary format that is optimized for performance. Converting the CSV files into TFRecords will improve the input/output execution performance. Sharding the TFRecords will allow the data to be read in parallel, which will further improve performance.\n\nThe other options are not as likely to improve performance.\n\nLoading the data into BigQuery or Cloud Bigtable will add an additional layer of abstraction, which can slow down performance.\nStoring the TFRecords in HDFS is not likely to improve performance, as HDFS is not optimized for TensorFlow."
      },
      {
        "date": "2023-08-08T11:46:00.000Z",
        "voteCount": 1,
        "content": "Using BigQuery or Bigtable may not be the most efficient option for input/output operations with TensorFlow. Storing the data in HDFS may be an option, but Cloud Storage is generally a more scalable and cost-effective solution."
      },
      {
        "date": "2023-06-09T01:20:00.000Z",
        "voteCount": 1,
        "content": "While Bigtable can offer high-performance I/O capabilities, it is important to note that it is primarily designed for structured data storage and real-time access patterns. In this scenario, the focus is on optimizing input/output execution performance, and using TFRecords in Cloud Storage aligns well with that goal."
      },
      {
        "date": "2023-06-05T01:21:00.000Z",
        "voteCount": 2,
        "content": "A. Load the data into BigQuery, and read the data from BigQuery.\nhttps://cloud.google.com/blog/products/ai-machine-learning/tensorflow-enterprise-makes-accessing-data-on-google-cloud-faster-and-easier\nPrecisely on this link provided in other comments it whos that the best shot with tfrecords is: 18752 Records per second. In the same report it shows that bigquery is morethan 40000 recors per second"
      },
      {
        "date": "2023-07-22T00:10:00.000Z",
        "voteCount": 3,
        "content": "BigQuery is designed for running large-scale analytical queries, not for serving input pipelines for machine learning models like TensorFlow. BigQuery's strength is in its ability to handle complex queries over vast amounts of data, but it may not provide the optimal performance for the specific task of feeding data into a TensorFlow model.\n\nOn the other hand, converting the CSV files into shards of TFRecords and storing them in Cloud Storage (Option C) will provide better performance because TFRecords is a format designed specifically for TensorFlow. It allows for efficient storage and retrieval of data, making it a more suitable choice for improving the input/output execution performance. Additionally, Cloud Storage provides high throughput and low-latency data access, which is beneficial for training large-scale TensorFlow models."
      },
      {
        "date": "2023-05-08T23:07:00.000Z",
        "voteCount": 2,
        "content": "Went with C"
      },
      {
        "date": "2023-02-21T23:04:00.000Z",
        "voteCount": 1,
        "content": "Cloud Bigtable is typically used to process unstructured data, such as time-series data, logs, or other types of data that do not conform to a fixed schema. However, Cloud Bigtable can also be used to store structured data if necessary, such as in the case of a key-value store or a database that does not require complex relational queries."
      },
      {
        "date": "2023-02-21T22:41:00.000Z",
        "voteCount": 1,
        "content": "Option C, converting the CSV files into shards of TFRecords and storing the data in Cloud Storage, is the most appropriate solution for improving input/output execution performance in this scenario"
      },
      {
        "date": "2023-01-06T07:30:00.000Z",
        "voteCount": 4,
        "content": "https://cloud.google.com/architecture/ml-on-gcp-best-practices#store-tabular-data-in-bigquery\nBigQuery for structured data, cloud storage for unstructed data"
      },
      {
        "date": "2023-05-17T06:58:00.000Z",
        "voteCount": 1,
        "content": "agree. BigQuery and Cloud Storage have effectively identical storage performance, where BigQuery is optimised for structured dataset and GCS for unstructured."
      },
      {
        "date": "2022-06-09T10:46:00.000Z",
        "voteCount": 1,
        "content": "\"100 billion records stored in several CSV files\" that means we deal with distributed big data problem , so HDFS is very suitable , Will choose D"
      },
      {
        "date": "2022-08-20T07:35:00.000Z",
        "voteCount": 2,
        "content": "HDFS will require more resources\n100 bil record is processed fine with Cloud Storage object"
      },
      {
        "date": "2022-05-09T19:19:00.000Z",
        "voteCount": 4,
        "content": "Answer is C. TFRecords in cloud storage for big data is the recommended practice by Google for training TF models."
      },
      {
        "date": "2022-03-25T03:35:00.000Z",
        "voteCount": 3,
        "content": "Google best practices: Use Cloud Storage buckets and directories to group the shards of data (either sharded TFRecord files if using Tensorflow, or Avro if using any other framework). Aim for files of at least 100Mb, and 100 - 10000 shards."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 46,
    "url": "https://www.examtopics.com/discussions/google/view/56134-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "As the lead ML Engineer for your company, you are responsible for building ML models to digitize scanned customer forms. You have developed a TensorFlow model that converts the scanned images into text and stores them in Cloud Storage. You need to use your ML model on the aggregated data collected at the end of each day with minimal manual intervention. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the batch prediction functionality of AI Platform.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a serving pipeline in Compute Engine for prediction.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Functions for prediction each time a new data point is ingested.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the model on AI Platform and create a version of it for online inference."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 13,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-06-26T17:20:00.000Z",
        "voteCount": 28,
        "content": "Use the model at the end of the day =&gt; Not D, C. \nMinimize manual intervention =&gt; not B\nAns: A"
      },
      {
        "date": "2024-06-06T11:00:00.000Z",
        "voteCount": 1,
        "content": "A) This a batch prediction using AI Platform"
      },
      {
        "date": "2024-03-22T18:56:00.000Z",
        "voteCount": 1,
        "content": "A is the most efficient"
      },
      {
        "date": "2023-11-15T13:10:00.000Z",
        "voteCount": 1,
        "content": "A is the only way"
      },
      {
        "date": "2023-05-08T23:07:00.000Z",
        "voteCount": 1,
        "content": "Went with A"
      },
      {
        "date": "2023-01-05T06:07:00.000Z",
        "voteCount": 1,
        "content": "There is only A, for me."
      },
      {
        "date": "2022-12-22T08:29:00.000Z",
        "voteCount": 3,
        "content": "Because aggregated data can be sent at the end of the day for batch prediction and AI platform is managed so satisfy minimal intervention requirement\nNot B as violates minimal intervention requirement\nNot C and D as real-time or online inference is not needed since data is aggregated at the end of the day"
      },
      {
        "date": "2022-12-10T08:33:00.000Z",
        "voteCount": 1,
        "content": "You need to use your ML model on the aggregated data collected at the end of each day with minimal manual intervention."
      },
      {
        "date": "2022-11-19T21:52:00.000Z",
        "voteCount": 1,
        "content": "A.\nhttps://datatonic.com/insights/vertex-ai-improving-debugging-batch-prediction/#:~:text=Vertex%20AI%20Batch%20Prediction%20provides,to%20GCS%20or%20BigQuery%2C%20respectively."
      },
      {
        "date": "2022-06-09T11:10:00.000Z",
        "voteCount": 1,
        "content": "\"You need to use your ML model on the aggregated data\" that means we need the batch prediction feature in AI platform"
      },
      {
        "date": "2022-01-19T11:59:00.000Z",
        "voteCount": 3,
        "content": "A\nhttps://cloud.google.com/ai-platform/prediction/docs/batch-predict"
      },
      {
        "date": "2021-10-05T06:26:00.000Z",
        "voteCount": 3,
        "content": "Another vote for A. Technically, through the right lens D could be correct as well, but what tipped me towards A was batch vs online predictions and the need for less manual work."
      },
      {
        "date": "2021-09-16T16:42:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/ai-platform/prediction/docs/batch-predict"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 47,
    "url": "https://www.examtopics.com/discussions/google/view/55570-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You recently joined an enterprise-scale company that has thousands of datasets. You know that there are accurate descriptions for each table in BigQuery, and you are searching for the proper BigQuery table to use for a model you are building on AI Platform. How should you find the data that you need?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Data Catalog to search the BigQuery datasets by using keywords in the table description.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTag each of your model and version resources on AI Platform with the name of the BigQuery table that was used for training.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMaintain a lookup table in BigQuery that maps the table descriptions to the table ID. Query the lookup table to find the correct table ID for the data that you need.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExecute a query in BigQuery to retrieve all the existing table names in your project using the INFORMATION_SCHEMA metadata tables that are native to BigQuery. Use the result o find the table that you need."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 17,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-06-18T07:24:00.000Z",
        "voteCount": 18,
        "content": "Should be A\nhttps://cloud.google.com/data-catalog/docs/concepts/overview"
      },
      {
        "date": "2022-04-14T09:48:00.000Z",
        "voteCount": 7,
        "content": "who is providing these answers?? Its clearly A. most of the answers are incorrect here."
      },
      {
        "date": "2024-06-06T11:01:00.000Z",
        "voteCount": 1,
        "content": "A) Data Catalog"
      },
      {
        "date": "2023-12-05T02:10:00.000Z",
        "voteCount": 1,
        "content": "A without hesitation."
      },
      {
        "date": "2023-11-15T13:11:00.000Z",
        "voteCount": 1,
        "content": "A is the only way"
      },
      {
        "date": "2023-07-07T12:38:00.000Z",
        "voteCount": 1,
        "content": "A should be correct"
      },
      {
        "date": "2023-05-08T23:07:00.000Z",
        "voteCount": 2,
        "content": "Went with A"
      },
      {
        "date": "2022-02-20T05:48:00.000Z",
        "voteCount": 1,
        "content": "Another vote for A by me."
      },
      {
        "date": "2022-01-04T01:04:00.000Z",
        "voteCount": 3,
        "content": "A should be the way to go for large datasets\n--This is also good but it is legacy way of checking:-\nNFORMATION_SCHEMA contains these views for table metadata: TABLES and TABLE_OPTIONS for metadata about tables. COLUMNS and COLUMN_FIELD_PATHS for metadata about columns and fields. PARTITIONS for metadata about table partitions (Preview)"
      },
      {
        "date": "2021-12-20T14:51:00.000Z",
        "voteCount": 1,
        "content": "I vote A"
      },
      {
        "date": "2021-10-05T06:27:00.000Z",
        "voteCount": 1,
        "content": "Another vote for answer A from me."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 48,
    "url": "https://www.examtopics.com/discussions/google/view/56140-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You started working on a classification problem with time series data and achieved an area under the receiver operating characteristic curve (AUC ROC) value of<br>99% for training data after just a few experiments. You haven't explored using any sophisticated algorithms or spent any time on hyperparameter tuning. What should your next step be to identify and fix the problem?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAddress the model overfitting by using a less complex algorithm.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAddress data leakage by applying nested cross-validation during model training.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAddress data leakage by removing features highly correlated with the target value.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAddress the model overfitting by tuning the hyperparameters to reduce the AUC ROC value."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 15,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 12,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-06-26T19:10:00.000Z",
        "voteCount": 27,
        "content": "Ans: B (Ref: https://towardsdatascience.com/time-series-nested-cross-validation-76adba623eb9)\n(C) High correlation doesn't mean leakage. The question may suggest target leakage and the defining point of this leakage is the availability of data after the target is available.(https://www.kaggle.com/dansbecker/data-leakage)"
      },
      {
        "date": "2023-07-09T02:19:00.000Z",
        "voteCount": 5,
        "content": "This ref doesn't explain WHY we should use NCV in this case - it just explains HOW to use NCV when dealing with time series.\nCross-validation, including nested cross-validation, is a powerful tool for model evaluation and hyperparameter tuning, but it does NOT DIRECTLY ADDRESS data leakage. Data leakage refers to a situation where information from the test dataset leaks into the training dataset, causing the model to have an unrealistically high performance. Nested cross-validation can indeed help provide a more accurate estimation of the model's performance on unseen data, but IT DOESN'T SOLVE the underlying issue of data leakage if it's already present."
      },
      {
        "date": "2023-03-05T23:49:00.000Z",
        "voteCount": 8,
        "content": "C: this is correct choice 1000000000%\nThis is data leakage issue on training data\nhttps://cloud.google.com/automl-tables/docs/train#analyze\nThe question is from this content.\nIf a column's Correlation with Target value is high, make sure that is expected, and not an indication of target leakage.\n\nLet 's explain on my owner way, sometime  the feature  used on training data use value to calculate something from  target value unintentionally,  it result in high correlation with each other. \nfor instance , you predict stock price by using moving average, MACD , RSI  despite the fact that 3 features have been calculated from price (target)."
      },
      {
        "date": "2023-06-02T03:59:00.000Z",
        "voteCount": 2,
        "content": "I agree. Besides, when a CV is done randomly (not split by the time point) it can make things worse."
      },
      {
        "date": "2024-10-15T02:57:00.000Z",
        "voteCount": 1,
        "content": "Select answer: C. --reason--- While B (nested cross-validation) helps improve the evaluation process and prevents over-optimistic performance estimates, it doesn't tackle the root cause of data leakage. Data leakage is often caused by features that are too closely tied to the target\u2014in this case, the unusually high AUC suggests that the model is gaining unfair information."
      },
      {
        "date": "2024-06-25T03:13:00.000Z",
        "voteCount": 1,
        "content": "B is the correct option"
      },
      {
        "date": "2024-06-06T11:10:00.000Z",
        "voteCount": 1,
        "content": "C) Is the best answer"
      },
      {
        "date": "2024-05-26T10:47:00.000Z",
        "voteCount": 1,
        "content": "Nested cross validation will not work for time series data. Time series data require the expanding widow training data set. Seems most likely the issue is high correlation in columns."
      },
      {
        "date": "2024-04-26T09:44:00.000Z",
        "voteCount": 2,
        "content": "B: correct. \nconsidering c, but why should we remove a feature of highly predictive nature?? for me, this does not explain the problem of overfitting... a highly predictive feature is also useful for good performance evaluated on the test set. \n--&gt; Decide for B!"
      },
      {
        "date": "2024-04-21T01:29:00.000Z",
        "voteCount": 1,
        "content": "agree with Paul_Dirac"
      },
      {
        "date": "2023-12-29T02:56:00.000Z",
        "voteCount": 1,
        "content": "I initially went with B- however after reading this: https://machinelearningmastery.com/nested-cross-validation-for-machine-learning-with-python/ I think C is right. Quoted from the link: \"Nested cross-validation is an approach to model hyperparameter optimization and model selection that attempts to overcome the problem of overfitting the training dataset.\". Overfitting is exactly our problem here. Correlated features in the dataset may be a sign of data leakage, but they are not necessarily."
      },
      {
        "date": "2023-11-15T13:15:00.000Z",
        "voteCount": 1,
        "content": "I think its B. GPT4 makes a good argument about C:\n While this is a valid approach to handling data leakage, it might not be sufficient if the leakage is due to reasons other than high correlation, such as temporal leakage in time-series data."
      },
      {
        "date": "2023-09-13T00:59:00.000Z",
        "voteCount": 1,
        "content": "Option A: This option is a reasonable choice. Switching to a less complex algorithm can help reduce overfitting, and using k-fold cross-validation can provide a better estimate of how well the model will generalize to unseen data. It's essential to ensure that the high performance isn't solely due to overfitting."
      },
      {
        "date": "2023-09-13T01:00:00.000Z",
        "voteCount": 2,
        "content": "Option B: Nested cross-validation is primarily used to estimate model performance accurately and select the best model hyperparameters. While it's a good practice, it doesn't directly address the overfitting issue. It helps prevent over-optimistic model performance estimates but doesn't necessarily fix the overfitting problem.\n\nOption C: Removing features highly correlated with the target value can be a valid step in feature selection or preprocessing. However, it doesn't directly address the overfitting issue or explain why the model is performing exceptionally well on the training data. It's a separate step from mitigating overfitting.\n\nOption D: This option is incorrect. Tuning hyperparameters should aim to improve model performance on the validation set, not reduce it. \n\nIn summary, the most appropriate next step is Option A:"
      },
      {
        "date": "2023-08-24T09:44:00.000Z",
        "voteCount": 1,
        "content": "B: If splits are done chronologically(as it is always advised), Nested CV should work\nC: High correlation with target means we have to check if this is strong explanatory power or data leakage. dropping the features won't help us distinguish in those cases but may help reveal independence contribution of remaining features"
      },
      {
        "date": "2023-08-08T11:41:00.000Z",
        "voteCount": 2,
        "content": "Option C is a good step to avoid overfitting, but it's not necessarily the best approach to address data leakage.\n\nData leakage occurs when information from the validation or test data leaks into the training data, leading to overly optimistic performance metrics. In time-series data, it's important to avoid using future information to predict past events.\n\nRemoving features highly correlated with the target value may help to reduce overfitting, but it does not necessarily address data leakage.\n\nTherefore, applying nested cross-validation during model training is a better approach to address data leakage in this scenario."
      },
      {
        "date": "2023-07-09T02:17:00.000Z",
        "voteCount": 1,
        "content": "https://towardsdatascience.com/avoiding-data-leakage-in-timeseries-101-25ea13fcb15f\nDirectly says: \"Dive straight into the MVP, cross-validate later!\" \nMVP stands for Minimum Viable Product"
      },
      {
        "date": "2023-07-07T07:41:00.000Z",
        "voteCount": 1,
        "content": "Agree with Paul_Dirac. Also it is recommended to use nested-cross-validation to avoid data leakage in time series data."
      },
      {
        "date": "2023-06-02T04:01:00.000Z",
        "voteCount": 1,
        "content": "There can be a feature causing data leakage which might have been overlooked. In addition, when cross-validation is done randomly, the leakage can be even bigger."
      },
      {
        "date": "2023-05-08T23:08:00.000Z",
        "voteCount": 1,
        "content": "Went with B"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 49,
    "url": "https://www.examtopics.com/discussions/google/view/55571-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You work for an online travel agency that also sells advertising placements on its website to other companies. You have been asked to predict the most relevant web banner that a user should see next. Security is important to your company. The model latency requirements are 300ms@p99, the inventory is thousands of web banners, and your exploratory analysis has shown that navigation context is a good predictor. You want to Implement the simplest solution. How should you configure the prediction pipeline?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEmbed the client on the website, and then deploy the model on AI Platform Prediction.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEmbed the client on the website, deploy the gateway on App Engine, and then deploy the model on AI Platform Prediction.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEmbed the client on the website, deploy the gateway on App Engine, deploy the database on Cloud Bigtable for writing and for reading the user's navigation context, and then deploy the model on AI Platform Prediction.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEmbed the client on the website, deploy the gateway on App Engine, deploy the database on Memorystore for writing and for reading the user's navigation context, and then deploy the model on Google Kubernetes Engine."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 18,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 15,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-07-31T18:31:00.000Z",
        "voteCount": 12,
        "content": "Security =&gt; not A.\nB: doesn't handle processing with banner inventory.\nD: deployment on GKE is less simple than on AI Platform. Besides, MemoryStore is in-memory while banners are stored persistently.\nAns: C"
      },
      {
        "date": "2024-04-25T09:00:00.000Z",
        "voteCount": 2,
        "content": "B: doesn't handle processing with banner inventory ---&gt; not true..."
      },
      {
        "date": "2021-07-18T21:15:00.000Z",
        "voteCount": 6,
        "content": "ANS: C\nGAE + IAP\nhttps://medium.com/google-cloud/secure-cloud-run-cloud-functions-and-app-engine-with-api-key-73c57bededd1\n\nBigtable at low latency\nhttps://cloud.google.com/bigtable#section-2"
      },
      {
        "date": "2024-06-21T02:46:00.000Z",
        "voteCount": 1,
        "content": "They affirm that navigation context is a good predictor for your model. Therefore you need to be able to perform the prediction and write the new context (if you get more data you will get a better model) and read (to use it for your prediction).\nOn one hand, BigQuery is a OLAP method so for writings and readings could take it around 2 seconds.\nOn the other hand, BigTable is a OLTP method and can make writings and readings in about 9 milliseconds \nConclusion: As one of the requerements is that the latency requirements have to be below 300ms your only choice is using BigTable\n\nhttps://galvarado.com.mx/post/comparaci%C3%B3n-de-bases-de-datos-en-google-cloud-datastore-vs-bigtable-vs-cloud-sql-vs-spanner-vs-bigquery/"
      },
      {
        "date": "2024-06-06T11:13:00.000Z",
        "voteCount": 1,
        "content": "C) Big Table for low latency"
      },
      {
        "date": "2024-04-26T09:51:00.000Z",
        "voteCount": 2,
        "content": "Was torn between B and C, but decided for B, because the question states how we should configure the PREDICTION pipeline! \nSince the exploratory analysis already identified navigation context as good predictor, the focus should be on the prediction model itself."
      },
      {
        "date": "2024-04-21T01:31:00.000Z",
        "voteCount": 1,
        "content": "agree with Paul_Dirac"
      },
      {
        "date": "2024-03-13T13:07:00.000Z",
        "voteCount": 3,
        "content": "look at Q80"
      },
      {
        "date": "2023-11-15T13:20:00.000Z",
        "voteCount": 3,
        "content": "I was torn between B and C.\nBut I really don't see the need for a DB"
      },
      {
        "date": "2023-11-15T03:47:00.000Z",
        "voteCount": 1,
        "content": "Embed the client on the website, deploy the gateway on App Engine, and then deploy the model on AI Platform Prediction."
      },
      {
        "date": "2023-07-13T22:45:00.000Z",
        "voteCount": 1,
        "content": "secuirity (gateway) + Simplest(ai, not DB)"
      },
      {
        "date": "2023-07-07T07:42:00.000Z",
        "voteCount": 1,
        "content": "Bigtable is recommended for storage in the case scenario."
      },
      {
        "date": "2023-07-03T09:19:00.000Z",
        "voteCount": 4,
        "content": "B is also a possible solution, but it does not include a database for storing and retrieving the user's navigation context. This means that every time a user visits a page, the gateway would need to query the website to retrieve the navigation context, which could be slow and inefficient. By using Cloud Bigtable to store the navigation context, the gateway can quickly retrieve the context from the database and pass it to the model for prediction. This makes the overall prediction pipeline more efficient and scalable. Therefore, C is a better option compared to B."
      },
      {
        "date": "2023-06-21T23:19:00.000Z",
        "voteCount": 1,
        "content": "B is correct, C introduces computational overhead, unnecessarily increasing serving latency."
      },
      {
        "date": "2023-06-05T01:37:00.000Z",
        "voteCount": 1,
        "content": "C. Embed the client on the website, deploy the gateway on App Engine, deploy the database on Cloud Bigtable for writing and for reading the user's navigation context, and then deploy the model on AI Platform Prediction\nhttps://cloud.google.com/architecture/minimizing-predictive-serving-latency-in-machine-learning#choosing_a_nosql_database\nTypical use cases for Bigtable are:\n* Ad prediction that leverages dynamically aggregated values over all ad requests and historical data."
      },
      {
        "date": "2023-05-09T01:37:00.000Z",
        "voteCount": 1,
        "content": "Bigtable is a massively scalable NoSQL database service engineered for high throughput and for low-latency workloads. It can handle petabytes of data, with millions of reads and writes per second at a latency that's on the order of milliseconds.\n\nTypical use cases for Bigtable are:\n\nFraud detection that leverages dynamically aggregated values. Applications in Fintech and Adtech are usually subject to heavy reads and writes.\nAd prediction that leverages dynamically aggregated values over all ad requests and historical data.\nBooking recommendation based on the overall customer base's recent bookings."
      },
      {
        "date": "2023-05-08T23:08:00.000Z",
        "voteCount": 1,
        "content": "Went with C"
      },
      {
        "date": "2023-04-10T09:06:00.000Z",
        "voteCount": 1,
        "content": "The volume is too low for a Bigtable scenario"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 50,
    "url": "https://www.examtopics.com/discussions/google/view/57248-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "Your team is building a convolutional neural network (CNN)-based architecture from scratch. The preliminary experiments running on your on-premises CPU-only infrastructure were encouraging, but have slow convergence. You have been asked to speed up model training to reduce time-to-market. You want to experiment with virtual machines (VMs) on Google Cloud to leverage more powerful hardware. Your code does not include any manual device placement and has not been wrapped in Estimator model-level abstraction. Which environment should you train your model on?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAVM on Compute Engine and 1 TPU with all dependencies installed manually.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAVM on Compute Engine and 8 GPUs with all dependencies installed manually.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA Deep Learning VM with an n1-standard-2 machine and 1 GPU with all libraries pre-installed.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA Deep Learning VM with more powerful CPU e2-highcpu-16 machines with all libraries pre-installed."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 11,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-07-20T05:42:00.000Z",
        "voteCount": 15,
        "content": "ANS: C\n\nto support CNN, you should use GPU. \nfor preliminary experiment, pre-installed pkgs/libs are good choice.\n\nhttps://cloud.google.com/deep-learning-vm/docs/cli#creating_an_instance_with_one_or_more_gpus\nhttps://cloud.google.com/deep-learning-vm/docs/introduction#pre-installed_packages"
      },
      {
        "date": "2021-07-31T19:34:00.000Z",
        "voteCount": 13,
        "content": "Code without manual device placement =&gt; default to CPU if TPU is present or to the lowest order GPU if multiple GPUs are present. =&gt; Not A, B.\nD: already using CPU and needing GPU for CNN.\nAns: C"
      },
      {
        "date": "2024-06-06T11:19:00.000Z",
        "voteCount": 1,
        "content": "C) GPU and all pre-installed libraries."
      },
      {
        "date": "2024-04-20T01:32:00.000Z",
        "voteCount": 1,
        "content": "Agree with celia20200410 - C"
      },
      {
        "date": "2023-11-15T13:25:00.000Z",
        "voteCount": 2,
        "content": "Agree with  celia20200410 - C"
      },
      {
        "date": "2023-11-15T09:57:00.000Z",
        "voteCount": 1,
        "content": "keyword: Your code does not include any manual device placement and has not been wrapped in Estimator model-level abstraction."
      },
      {
        "date": "2023-07-07T07:45:00.000Z",
        "voteCount": 1,
        "content": "Should use the deep learning VM with GPU. \nTPU should be selected only if necessary, coz it incurs high cost. GPU in this case is enough."
      },
      {
        "date": "2023-05-08T23:08:00.000Z",
        "voteCount": 1,
        "content": "Went with C"
      },
      {
        "date": "2023-04-19T17:12:00.000Z",
        "voteCount": 1,
        "content": "thinking in fastest way"
      },
      {
        "date": "2023-03-31T05:41:00.000Z",
        "voteCount": 1,
        "content": "You should use GPU."
      },
      {
        "date": "2023-02-27T03:47:00.000Z",
        "voteCount": 2,
        "content": "Critical sentence: Your code does not include any manual device placement and has not been wrapped in Estimator model-level abstraction.\n\nSo only answer we have. it's D."
      },
      {
        "date": "2023-02-21T23:43:00.000Z",
        "voteCount": 3,
        "content": "Critical sentece: Your code does not include any manual device placement and has not been wrapped in Estimator model-level abstraction.\n\nSo only answer we have. it's D."
      },
      {
        "date": "2023-07-03T09:21:00.000Z",
        "voteCount": 2,
        "content": "Option D provides a more powerful CPU but does not include a GPU, which may not be optimal for deep learning training."
      },
      {
        "date": "2023-01-05T05:38:00.000Z",
        "voteCount": 1,
        "content": "It's C."
      },
      {
        "date": "2022-08-24T07:08:00.000Z",
        "voteCount": 3,
        "content": "\"has not been wrapped in Estimator model-level abstraction\"\nHow you can use GPU?\nD in my opinion, E-family using for high CPU tasks"
      },
      {
        "date": "2022-07-11T02:11:00.000Z",
        "voteCount": 1,
        "content": "Answer C\n========\nExplanation\n\"speed up model training\" will make us biased towards GPU,TPU options\nby options eliminations we may need to stay away of any manual installations , so using preconfigered deep learning will speed up time to market"
      },
      {
        "date": "2022-04-14T09:58:00.000Z",
        "voteCount": 1,
        "content": "the question is asking speed up time to market which can happen if model trains fast. so TPU VM can be a solution. https://cloud.google.com/blog/products/compute/introducing-cloud-tpu-vms option A. if question asks most managed way than answer is deep learning container with everything installed. C"
      },
      {
        "date": "2023-07-03T09:22:00.000Z",
        "voteCount": 2,
        "content": "Option A with 1 TPU and option B with 8 GPUs might provide even faster training, but since the code does not include manual device placement, it may not utilize all the available resources effectively."
      },
      {
        "date": "2023-09-22T00:54:00.000Z",
        "voteCount": 2,
        "content": "Instead If you have a single GPU, TensorFlow will use this accelerator to speed up model training with no extra work on your part: https://codelabs.developers.google.com/vertex-p2p-distributed#2\nNormally you don't use just one TPU and for both GPUs and TPUs it is necessary to define a distributed training strategy: https://www.tensorflow.org/guide/distributed_training"
      },
      {
        "date": "2022-01-04T02:33:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 51,
    "url": "https://www.examtopics.com/discussions/google/view/55572-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You work on a growing team of more than 50 data scientists who all use AI Platform. You are designing a strategy to organize your jobs, models, and versions in a clean and scalable way. Which strategy should you choose?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up restrictive IAM permissions on the AI Platform notebooks so that only a single user or group can access a given instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSeparate each data scientist's work into a different project to ensure that the jobs, models, and versions created by each data scientist are accessible only to that user.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse labels to organize resources into descriptive categories. Apply a label to each created resource so that users can filter the results by label when viewing or monitoring the resources.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up a BigQuery sink for Cloud Logging logs that is appropriately filtered to capture information about AI Platform resource usage. In BigQuery, create a SQL view that maps users to the resources they are using"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-06-18T08:05:00.000Z",
        "voteCount": 14,
        "content": "I think should be C,\nAs IAM roles are given to the entire AI Notebook resource, not to a specific instance."
      },
      {
        "date": "2021-07-20T04:54:00.000Z",
        "voteCount": 10,
        "content": "ans: c \n\nhttps://cloud.google.com/ai-platform/prediction/docs/resource-labels#overview_of_labels\nYou can add labels to your AI Platform Prediction jobs, models, and model versions, then use those labels to organize resources into categories when viewing or monitoring the resources.\n\nFor example, you can label jobs by team (such as engineering or research) and development phase (prod or test), then filter the jobs based on the team and phase.\n\nLabels are also available on operations, but these labels are derived from the resource to which the operation applies. You cannot add or update labels on an operation.\n\nA label is a key-value pair, where both the key and the value are custom strings that you supp"
      },
      {
        "date": "2021-11-12T17:33:00.000Z",
        "voteCount": 1,
        "content": "I read through this page: https://cloud.google.com/ai-platform/prediction/docs/sharing-models. This one sounds more like A. Is isn't that correct? I am not quite sure."
      },
      {
        "date": "2021-11-12T17:38:00.000Z",
        "voteCount": 1,
        "content": "or maybe A is not correct because \"sharing models using IAM\" only applies to \"manage access to resource\" but this question is more like asking to \"organize jobs, models, and versions\".  not sure if my understanding is right or not."
      },
      {
        "date": "2024-09-09T08:27:00.000Z",
        "voteCount": 1,
        "content": "B. Setting up different resources in separate projects can help separate the use of resources.\nFrom the official guide book"
      },
      {
        "date": "2024-06-06T11:21:00.000Z",
        "voteCount": 1,
        "content": "C) labels"
      },
      {
        "date": "2023-11-15T13:29:00.000Z",
        "voteCount": 1,
        "content": "C\nAlthough there are some questions where setting up a logging sink to  BQ is the answer."
      },
      {
        "date": "2023-05-08T23:09:00.000Z",
        "voteCount": 1,
        "content": "Went with C"
      },
      {
        "date": "2023-02-27T04:00:00.000Z",
        "voteCount": 1,
        "content": "Restricting access is not scalable and creates silos - better to document sharable resources through tagging, hence C."
      },
      {
        "date": "2022-12-10T10:43:00.000Z",
        "voteCount": 1,
        "content": "C\nResource tagging/labeling is the best way to manage ML resources for medium/big data science teams."
      },
      {
        "date": "2022-01-19T12:45:00.000Z",
        "voteCount": 4,
        "content": "https://cloud.google.com/ai-platform/prediction/docs/resource-labels#overview_of_labels\n(A) applies only to notebooks wich is not enough"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 52,
    "url": "https://www.examtopics.com/discussions/google/view/55099-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You are training a deep learning model for semantic image segmentation with reduced training time. While using a Deep Learning VM Image, you receive the following error: The resource 'projects/deeplearning-platforn/zones/europe-west4-c/acceleratorTypes/nvidia-tesla-k80' was not found. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure that you have GPU quota in the selected region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure that the required GPU is available in the selected region.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure that you have preemptible GPU quota in the selected region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure that the selected GPU has enough GPU memory for the workload."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-07-20T04:56:00.000Z",
        "voteCount": 23,
        "content": "ANS: B \nhttps://cloud.google.com/deep-learning-vm/docs/troubleshooting#resource_not_found \n\nhttps://cloud.google.com/compute/docs/gpus/gpu-regions-zones \n\nResource not found\nSymptom: - The resource 'projects/deeplearning-platform/zones/europe-west4-c/acceleratorTypes/nvidia-tesla-k80' was not found\n\nProblem: You are trying to create an instance with one or more GPUs in a region where GPUs are not available (for example, an instance with a K80 GPU in europe-west4-c).\n\nSolution: To determine which region has the required GPU, see GPUs on Compute Engine."
      },
      {
        "date": "2021-06-11T07:09:00.000Z",
        "voteCount": 8,
        "content": "it is B, the error message relates to Quota is different:\nhttps://cloud.google.com/deep-learning-vm/docs/troubleshooting#resource_not_found"
      },
      {
        "date": "2024-06-06T11:22:00.000Z",
        "voteCount": 1,
        "content": "B) GPUs are only available in specific regions and zones"
      },
      {
        "date": "2023-12-05T03:28:00.000Z",
        "voteCount": 1,
        "content": "Not all resources can be found in any region. Therefore - B"
      },
      {
        "date": "2023-11-27T11:39:00.000Z",
        "voteCount": 1,
        "content": "It is clearly mentioned here: https://cloud.google.com/deep-learning-vm/docs/troubleshooting"
      },
      {
        "date": "2023-11-15T13:30:00.000Z",
        "voteCount": 1,
        "content": "B - because it's \"cant be found\""
      },
      {
        "date": "2023-05-08T23:09:00.000Z",
        "voteCount": 1,
        "content": "Went with B"
      },
      {
        "date": "2023-02-27T05:36:00.000Z",
        "voteCount": 2,
        "content": "The error says the resource was not found - hence B.\nIf quota was the problem (A) then you'd see a different error message."
      },
      {
        "date": "2022-12-10T10:47:00.000Z",
        "voteCount": 2,
        "content": "B obviously"
      },
      {
        "date": "2022-04-19T07:35:00.000Z",
        "voteCount": 3,
        "content": "The resource is not found because it doesn't exist in the region."
      },
      {
        "date": "2022-04-14T10:01:00.000Z",
        "voteCount": 1,
        "content": "the question is asking what should you do not why is the error.\nAnswer should be A. if you get that exception, make sure to check your limit for instance before running the job."
      },
      {
        "date": "2022-01-19T12:48:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/deep-learning-vm/docs/troubleshooting#resource_not_found"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 53,
    "url": "https://www.examtopics.com/discussions/google/view/57249-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "Your team is working on an NLP research project to predict political affiliation of authors based on articles they have written. You have a large training dataset that is structured like this:<br><img src=\"/assets/media/exam-media/03841/0002900001.png\" class=\"in-exam-image\"><br>You followed the standard 80%-10%-10% data distribution across the training, testing, and evaluation subsets. How should you distribute the training examples across the train-test-eval subsets while maintaining the 80-10-10 proportion?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDistribute texts randomly across the train-test-eval subsets: Train set: [TextA1, TextB2, ...] Test set: [TextA2, TextC1, TextD2, ...] Eval set: [TextB1, TextC2, TextD1, ...]",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDistribute authors randomly across the train-test-eval subsets: (*) Train set: [TextA1, TextA2, TextD1, TextD2, ...] Test set: [TextB1, TextB2, ...] Eval set: [TexC1,TextC2 ...]\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDistribute sentences randomly across the train-test-eval subsets: Train set: [SentenceA11, SentenceA21, SentenceB11, SentenceB21, SentenceC11, SentenceD21 ...] Test set: [SentenceA12, SentenceA22, SentenceB12, SentenceC22, SentenceC12, SentenceD22 ...] Eval set: [SentenceA13, SentenceA23, SentenceB13, SentenceC23, SentenceC13, SentenceD31 ...]",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDistribute paragraphs of texts (i.e., chunks of consecutive sentences) across the train-test-eval subsets: Train set: [SentenceA11, SentenceA12, SentenceD11, SentenceD12 ...] Test set: [SentenceA13, SentenceB13, SentenceB21, SentenceD23, SentenceC12, SentenceD13 ...] Eval set: [SentenceA11, SentenceA22, SentenceB13, SentenceD22, SentenceC23, SentenceD11 ...]"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-08-15T01:09:00.000Z",
        "voteCount": 19,
        "content": "I think since we are predicting political leaning of authors, perhaps distributing authors make more sense? (B)"
      },
      {
        "date": "2021-08-16T11:47:00.000Z",
        "voteCount": 7,
        "content": "Agree it should be B. Since every author has his/her distinct style, splitting different text from the same author across different set could result in data label leakage."
      },
      {
        "date": "2021-09-04T07:24:00.000Z",
        "voteCount": 1,
        "content": "I don't agree as we want to know the political affiliation from a text and not based on an author. I think A is better"
      },
      {
        "date": "2021-09-20T22:12:00.000Z",
        "voteCount": 2,
        "content": "it is the political affiliation from a text, but to whom belong that text?\nThe statement clearly says ... Predict political affiliation of authors based on articles they have written.  Hence the political affiliation is for each author according to the text he wrote."
      },
      {
        "date": "2021-09-20T22:46:00.000Z",
        "voteCount": 12,
        "content": "Exactly! I also consider is B\nCheck this out!\nIf we just put  inside the Training set , Validation set and Test set , randomly  Text, Paragraph or sentences the model will have the ability to learn specific qualities about The Author's use of language beyond just his own articles. Therefore the model will mixed up different opinions.\nRather if we divided things up a the author level, so that given authors were only on the training data, or only  in the test data or only in the validation data. The model will find more difficult to get a  high accuracy on the test validation (What is correct and have more sense!). Because it will need to really focus in author by author articles  rather than get a single political affiliation based on a bunch of mixed articles from different authors.\n\nhttps://developers.google.com/machine-learning/crash-course/18th-century-literature"
      },
      {
        "date": "2021-07-05T22:49:00.000Z",
        "voteCount": 8,
        "content": "Should be A, we are trying to get a label on the entire text so only A makes sense"
      },
      {
        "date": "2021-10-11T14:48:00.000Z",
        "voteCount": 5,
        "content": "Correct answer is B - https://developers.google.com/machine-learning/crash-course/18th-century-literature"
      },
      {
        "date": "2023-02-14T19:12:00.000Z",
        "voteCount": 1,
        "content": "This is a known study. if you use A, the moment a new author is given in a test set the accuracy is waay low than what your metrics might suggest. To have realistic evaluation results it should be B. Also note that the label is for the \"authour\" not a text."
      },
      {
        "date": "2024-06-06T11:27:00.000Z",
        "voteCount": 1,
        "content": "B) Authors"
      },
      {
        "date": "2024-05-26T11:16:00.000Z",
        "voteCount": 1,
        "content": "We have divide / split at author level. Other wise model will used text to author relationship but we want to find text to political affiliation relation ship. While prediction we already know text to author relation but we want to find text to political relation (and therefore author to political relation is implied."
      },
      {
        "date": "2023-07-03T09:28:00.000Z",
        "voteCount": 3,
        "content": "This is the best approach as it ensures that the data is distributed in a way that is representative of the overall population. By randomly distributing authors across the subsets, we ensure that each subset has a similar distribution of political affiliations. This helps to minimize bias and increases the likelihood that our model will generalize well to new data.\n\nDistributing texts randomly or by sentences or paragraphs may result in subsets that are biased towards a particular political affiliation. This could lead to overfitting and poor generalization performance. Therefore, it is important to distribute the data in a way that maintains the overall distribution of political affiliations across the subsets."
      },
      {
        "date": "2023-05-08T23:09:00.000Z",
        "voteCount": 1,
        "content": "Went with B"
      },
      {
        "date": "2023-02-15T23:10:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/automl-tables/docs/prepare#split\nhttps://developers.google.com/machine-learning/crash-course/18th-century-literature"
      },
      {
        "date": "2023-02-08T06:32:00.000Z",
        "voteCount": 1,
        "content": "Ans B\nThe model is to predict which political party the author belongs to, not which political party the text belongs to... You do not have the information of the political party of each text, you are assuming that the texts are associated with the political party of the author."
      },
      {
        "date": "2022-09-05T00:40:00.000Z",
        "voteCount": 1,
        "content": "label is party, feature is text"
      },
      {
        "date": "2022-08-11T00:13:00.000Z",
        "voteCount": 1,
        "content": "IMO, B is correct\nA,C,D label leakaged"
      },
      {
        "date": "2022-01-19T13:05:00.000Z",
        "voteCount": 6,
        "content": "https://developers.google.com/machine-learning/crash-course/18th-century-literature\nSplit by authors, otherwise there will be data leakage - the model will get the ability to learn author specific use of language"
      },
      {
        "date": "2022-01-04T02:44:00.000Z",
        "voteCount": 1,
        "content": "B I agree"
      },
      {
        "date": "2021-12-20T15:13:00.000Z",
        "voteCount": 2,
        "content": "I already saw the video in: https://developers.google.com/machine-learning/crash-course/18th-century-literature\n\nBased on this video I concluded that the answer is A. What answer B is saying is that you will have Author B's texts in the training set, Author A's texts in the testing set and Author C's texts in the validation set. According to the video B is incorrect.\n\nWe want to have texts from author A in the training, testing and validation set. So A is correct. I think most people are choosing B because the word \"author\" but let's be careful."
      },
      {
        "date": "2022-03-09T03:03:00.000Z",
        "voteCount": 5,
        "content": "I though the same initially, but no..We'd want texts from author A in the training, testing and validation set if the task was to predict the author from a text (meaning, if the label was the author..right? You train the model to learn the style of text and connect it to an author. You'd need new texts from the same author in the test and validation sets, to see if the model is able to recognize him/her). HERE, the task is to predict political affiliation from a text of an author. The author is given. In the test and validation sets you need new authors, to see wether the model is able to guess their political affiliation. So you would do 80 authors (and corresponding texts) for training, 10 different authors for validation, and 10 different ones for test."
      },
      {
        "date": "2021-09-27T11:08:00.000Z",
        "voteCount": 1,
        "content": "Partition by author - there is an actual example in Coursera 'Production ML systems' course"
      },
      {
        "date": "2021-09-18T15:58:00.000Z",
        "voteCount": 4,
        "content": "I think it is B.\n--\nYour test data includes data from populations that will not be represented in production.\n\nFor example, suppose you are training a model with purchase data from a number of stores. You know, however, that the model will be used primarily to make predictions for stores that are not in the training data. To ensure that the model can generalize to unseen stores, you should segregate your data sets by stores. In other words, your test set should include only stores different from the evaluation set, and the evaluation set should include only stores different from the training set.\nhttps://cloud.google.com/automl-tables/docs/prepare#ml-use"
      },
      {
        "date": "2021-09-08T20:37:00.000Z",
        "voteCount": 1,
        "content": "Should be D. Please see the dataset provided, it is based on the text / paragraphs."
      },
      {
        "date": "2021-10-14T03:40:00.000Z",
        "voteCount": 1,
        "content": "Have a look at the link the other have already provided twice. Splitting sentence by sentence is literally mentioned in said video as a bad example and something we should not do in this case."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 54,
    "url": "https://www.examtopics.com/discussions/google/view/58146-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "Your team has been tasked with creating an ML solution in Google Cloud to classify support requests for one of your platforms. You analyzed the requirements and decided to use TensorFlow to build the classifier so that you have full control of the model's code, serving, and deployment. You will use Kubeflow pipelines for the ML platform. To save time, you want to build on existing resources and use managed services instead of building a completely new model. How should you build the classifier?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Natural Language API to classify support requests.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AutoML Natural Language to build the support requests classifier.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an established text classification model on AI Platform to perform transfer learning.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an established text classification model on AI Platform as-is to classify support requests."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 11,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-07-27T09:24:00.000Z",
        "voteCount": 29,
        "content": "ANS: C as you want to have full control of the model code."
      },
      {
        "date": "2021-07-18T21:00:00.000Z",
        "voteCount": 11,
        "content": "ANS: D \n\nhttps://cloud.google.com/ai-platform/training/docs/algorithms \n- to use TensorFlow \n- to build on existing resources\n- to use managed services"
      },
      {
        "date": "2021-10-09T07:47:00.000Z",
        "voteCount": 3,
        "content": "While D is very close for me, I think there are 2 giveaways here: \n\"To save time, you want to build on existing resources\"  - transfer learning \n\"instead of building a completely new model\" - answer D leaves the model as is \n\nANS C:"
      },
      {
        "date": "2021-10-09T22:11:00.000Z",
        "voteCount": 6,
        "content": "the model cannot work as-is as the classes to predict will likely not be the same; we need to use transfer learning to retrain the last layer and adapt it to the classes we need, hence C"
      },
      {
        "date": "2024-06-06T11:29:00.000Z",
        "voteCount": 1,
        "content": "C) Transfer learning"
      },
      {
        "date": "2023-05-08T23:10:00.000Z",
        "voteCount": 3,
        "content": "Went with C"
      },
      {
        "date": "2023-02-14T19:26:00.000Z",
        "voteCount": 1,
        "content": "Usage of Tensorflow, can build a simple model by using a sentence embedding and a single layer classifier."
      },
      {
        "date": "2023-02-08T14:09:00.000Z",
        "voteCount": 1,
        "content": "you don't need transfer learning in this case"
      },
      {
        "date": "2022-06-06T11:17:00.000Z",
        "voteCount": 2,
        "content": "- \"You analyzed the requirements and decided to use TensorFlow\" this will make choices to reduce to C and D\n- \" so that you have full control of the model's code \" will make us choose C"
      },
      {
        "date": "2022-05-09T13:55:00.000Z",
        "voteCount": 1,
        "content": "Answer is C."
      },
      {
        "date": "2022-04-12T06:17:00.000Z",
        "voteCount": 1,
        "content": "According to me it is B.\nA is not correct as it uses an API call only and we won't build the system on existing resources.\nC &amp; D I do not see in AI Platform (Vertex AI) an established text classification that can be used.\nThe B answer is the right one, you have the labeled data, you need to remove the custom TF code and build a classifier with AutoML Natural Language"
      },
      {
        "date": "2022-05-09T13:54:00.000Z",
        "voteCount": 1,
        "content": "B is wrong.  question says \" you have full control of the model's code\". You don't have full control of automl code. The right answer is C."
      },
      {
        "date": "2022-03-09T03:18:00.000Z",
        "voteCount": 3,
        "content": "\"full control of the model's code, serving, and deployment\": Not A nor B.\nand \"you want to build on existing resources and use managed services\": Not D (that's \"as-is\") You want transfer learning."
      },
      {
        "date": "2022-01-04T02:47:00.000Z",
        "voteCount": 1,
        "content": "Cis correct"
      },
      {
        "date": "2021-10-09T07:43:00.000Z",
        "voteCount": 1,
        "content": "ANS: C according to me as well. As arbik said, full control, custom model are give aways."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 55,
    "url": "https://www.examtopics.com/discussions/google/view/57271-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You recently joined a machine learning team that will soon release a new project. As a lead on the project, you are asked to determine the production readiness of the ML components. The team has already tested features and data, model development, and infrastructure. Which additional readiness check should you recommend to the team?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure that training is reproducible.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure that all hyperparameters are tuned.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure that model performance is monitored.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure that feature expectations are captured in the schema."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 17,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-07-06T03:24:00.000Z",
        "voteCount": 21,
        "content": "I think it should be C"
      },
      {
        "date": "2021-07-15T21:33:00.000Z",
        "voteCount": 4,
        "content": "performance monitoring  is a continuous effort that happens all time. but reproducibility makes more sense to be added to model QA"
      },
      {
        "date": "2021-07-28T23:39:00.000Z",
        "voteCount": 4,
        "content": "The question was not about model QA but production readiness, thus I think the answer is C because monitor model performance in production is important. As regard to A, I would I argue it could fall under \"model development\", since reproducible training is already important during model development."
      },
      {
        "date": "2021-11-12T19:10:00.000Z",
        "voteCount": 7,
        "content": "To my understanding, I think A might be correct since model performance monitoring is happens \"in production\". but the question said the project \"will soon release\" which means right now is before launching, so to me testing the reproducible would make more sense. (I was confused about A and C for a long time)\nreference: \n- Testing reproducibility: https://developers.google.com/machine-learning/testing-debugging/pipeline/deploying\n- Testing in Production: https://developers.google.com/machine-learning/testing-debugging/pipeline/production"
      },
      {
        "date": "2021-12-06T11:03:00.000Z",
        "voteCount": 1,
        "content": "I also think is C:\nreference : https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/aad9f93b86b7addfea4c419b9100c6cdd26cacea.pdf"
      },
      {
        "date": "2021-07-09T21:29:00.000Z",
        "voteCount": 9,
        "content": "A - important one before moving to the production"
      },
      {
        "date": "2021-07-27T02:27:00.000Z",
        "voteCount": 5,
        "content": "Testing for Deploying Machine Learning Models:\n- Test Model Updates with Reproducible Training\nhttps://developers.google.com/machine-learning/testing-debugging/pipeline/deploying"
      },
      {
        "date": "2024-06-06T11:31:00.000Z",
        "voteCount": 1,
        "content": "C) Model monitoring"
      },
      {
        "date": "2024-05-07T10:54:00.000Z",
        "voteCount": 1,
        "content": "C is not a readiness check. Monitoring is a continuous effort. IMO A is the correct answer. If the training is not reproducible it's not ready for production. If any error happens, data drifts / skews, then there is no way to recreate the model.\n\nThis is a check BEFORE going to production. Once it's in production, then yes C is important."
      },
      {
        "date": "2023-12-05T03:36:00.000Z",
        "voteCount": 2,
        "content": "Monitoring is crucial. So - C"
      },
      {
        "date": "2023-05-08T23:10:00.000Z",
        "voteCount": 1,
        "content": "Went with C"
      },
      {
        "date": "2023-04-26T01:34:00.000Z",
        "voteCount": 1,
        "content": "I'll go with C. \nMonitoring model performance is an important aspect of production readiness. It allows the team to detect and respond to changes in performance that may affect the quality of the model. The other options are also important, but they are more focused on the development phase of the project rather than the production phase."
      },
      {
        "date": "2023-02-16T00:19:00.000Z",
        "voteCount": 4,
        "content": "Hey! all guys\nA+B+D=The team has already tested features and data, model development, and infrastructure. we are about to go live with production. \nMonitoring readiness is the last thing to account for.\n\nIt will be very rediculous if you launch model as production regardless of how we will have about monitoring.  you will lauch model as production for while and will make plan to model performance monitoring later ??? you are too reckless.\n\nPls . Read it carefully https://developers.google.com/machine-learning/testing-debugging/pipeline/production\nhttps://developers.google.com/machine-learning/testing-debugging/pipeline/overview#what-is-an-ml-pipeline.\nYou \n\nMost guys prefer A : https://developers.google.com/machine-learning/testing-debugging/pipeline/deploying   I think that it is all about model development prior to deploying ."
      },
      {
        "date": "2023-02-08T14:19:00.000Z",
        "voteCount": 1,
        "content": "I think that your team ensure that all hypermarameters were turned yet when tested features... i think that it's more important that they ensure that model performance is monitored than thaining is reproducible for best practices.\nhttps://cloud.google.com/architecture/ml-on-gcp-best-practices"
      },
      {
        "date": "2023-01-24T00:28:00.000Z",
        "voteCount": 1,
        "content": "Reproducible Training is more likely to be in the Deployment step in that it referred to the question \"The team has already tested features and data, model development\"  but the  question focuses on Production readiness  \nhttps://developers.google.com/machine-learning/testing-debugging/pipeline/production\nMonitor  section is part of this above link"
      },
      {
        "date": "2023-01-05T05:17:00.000Z",
        "voteCount": 1,
        "content": "C, for me."
      },
      {
        "date": "2022-11-08T11:11:00.000Z",
        "voteCount": 2,
        "content": "It's mentioned that the team has already tested features and data, implying that data generation is reproducible. If you have to test features data has to be reproducible to compare model outputs. ( https://developers.google.com/machine-learning/data-prep/construct/sampling-splitting/randomization). Hence C makes more sense"
      },
      {
        "date": "2022-09-05T00:51:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/ai-platform/docs/ml-solutions-overview"
      },
      {
        "date": "2022-06-26T10:31:00.000Z",
        "voteCount": 2,
        "content": "With the specific focus on \"production readiness\" as stated, I'd pick C above the others."
      },
      {
        "date": "2022-06-18T23:06:00.000Z",
        "voteCount": 1,
        "content": "I think it's C. \nA is related to infrastructure, B is related to model development and D is related to Data and features. It clearly mentioned that team has already tested for model development, data and features and infrastructure."
      },
      {
        "date": "2022-06-08T05:02:00.000Z",
        "voteCount": 1,
        "content": "\"production readiness\" means that we are still in dev-test phase , and \"performance \n monitoring\" happens in production , and what if monitoring is applied but the model re-train is difficult , so \"A\" is the best answer"
      },
      {
        "date": "2022-02-17T07:06:00.000Z",
        "voteCount": 2,
        "content": "A makes more sense than C."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 56,
    "url": "https://www.examtopics.com/discussions/google/view/57276-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You work for a credit card company and have been asked to create a custom fraud detection model based on historical data using AutoML Tables. You need to prioritize detection of fraudulent transactions while minimizing false positives. Which optimization objective should you use when training the model?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAn optimization objective that minimizes Log loss",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAn optimization objective that maximizes the Precision at a Recall value of 0.50",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAn optimization objective that maximizes the area under the precision-recall curve (AUC PR) value\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAn optimization objective that maximizes the area under the receiver operating characteristic curve (AUC ROC) value"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 16,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-07-31T21:10:00.000Z",
        "voteCount": 20,
        "content": "This is a case of imbalanced data.\nAns: C \nhttps://stats.stackexchange.com/questions/262616/roc-vs-precision-recall-curves-on-imbalanced-dataset\n\nhttps://neptune.ai/blog/f1-score-accuracy-roc-auc-pr-auc"
      },
      {
        "date": "2021-10-17T14:19:00.000Z",
        "voteCount": 2,
        "content": "C is wrong - correct answer is D. ROC basically compares True Positives against False Negative, exactly what we are trying to optimise for."
      },
      {
        "date": "2021-07-09T21:51:00.000Z",
        "voteCount": 8,
        "content": "D - https://en.wikipedia.org/wiki/Receiver_operating_characteristic"
      },
      {
        "date": "2021-07-15T22:11:00.000Z",
        "voteCount": 2,
        "content": "True. The true positive is presented by Y axis. The bigger the area the graph take, the higher TP ratio"
      },
      {
        "date": "2023-07-22T00:49:00.000Z",
        "voteCount": 2,
        "content": "A larger area under the ROC curve does indicate a better model performance in terms of correctly identifying true positives. However, it does not take into account the imbalance in the class distribution or the costs associated with false positives and false negatives.\n\nIn contrast, the AUC PR curve focuses on the trade-off between precision (Y-axis) and recall (X-axis), making it more suitable for imbalanced datasets and applications with different costs for false positives and false negatives, like credit card fraud detection."
      },
      {
        "date": "2023-07-22T00:48:00.000Z",
        "voteCount": 2,
        "content": "AUC ROC is more suitable when the class distribution is balanced and false positives and false negatives have similar costs.\n\nIn the case of credit card fraud detection, the class distribution is typically imbalanced (fewer fraudulent transactions compared to non-fraudulent ones), and the cost of false positives (incorrectly identifying a transaction as fraudulent) and false negatives (failing to detect a fraudulent transaction) are not the same.\n\nBy maximizing the AUC PR (area under the precision-recall curve), the model focuses on the trade-off between precision (proportion of true positives among predicted positives) and recall (proportion of true positives among actual positives), which is more relevant in imbalanced datasets and for applications where the costs of false positives and false negatives are not equal. This makes option C a better choice for credit card fraud detection."
      },
      {
        "date": "2024-06-06T11:38:00.000Z",
        "voteCount": 1,
        "content": "C) PR (Precision Recall)"
      },
      {
        "date": "2024-06-06T11:37:00.000Z",
        "voteCount": 1,
        "content": "C) PR ROC"
      },
      {
        "date": "2023-07-03T09:38:00.000Z",
        "voteCount": 4,
        "content": "In fraud detection, it's crucial to minimize false positives (transactions flagged as fraudulent but are actually legitimate) while still detecting as many fraudulent transactions as possible. AUC PR is a suitable optimization objective for this scenario because it provides a balanced trade-off between precision and recall, which are both important metrics in fraud detection. A high AUC PR value indicates that the model has high precision and recall, which means it can detect a large number of fraudulent transactions while minimizing false positives.\n\nLog loss (A) and AUC ROC (D) are also commonly used optimization objectives in machine learning, but they may not be as effective in this particular scenario. Precision at a Recall value of 0.50 (B) is a specific metric and not an optimization objective."
      },
      {
        "date": "2023-05-08T23:10:00.000Z",
        "voteCount": 1,
        "content": "Went with C"
      },
      {
        "date": "2023-02-16T00:45:00.000Z",
        "voteCount": 1,
        "content": "Hi Everyone\nI discover, there are some clues that  this question is likely to refer to  the last section  of https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc  \nThis is what it tries to tell us especially with the last sentence\nClassification-threshold invariance is not always desirable. In cases where there are wide disparities in the cost of false negatives vs. false positives, it may be critical to minimize one type of classification error. For example, when doing email spam detection, you likely want to prioritize minimizing false positives (even if that results in a significant increase of false negatives). AUC isn't a useful metric for this type of optimization.\n\nAdditionally, it tells me which of the following choices is the answer to this question as below.\nhttps://cloud.google.com/automl-tables/docs/train#opt-obj."
      },
      {
        "date": "2023-02-08T14:35:00.000Z",
        "voteCount": 1,
        "content": "What is different however is that ROC AUC looks at a true positive rate TPR and false positive rate FPR while PR AUC looks at positive predictive value PPV and true positive rate TPR.\n\nDetect Fraudulent transactions = Max TP\nMinimizing false positives -&gt; min FP\n\nhttps://neptune.ai/blog/f1-score-accuracy-roc-auc-pr-auc#:~:text=ROC%20AUC%20vs%20PR%20AUC&amp;text=What%20is%20different%20however%20is,and%20true%20positive%20rate%20TPR"
      },
      {
        "date": "2023-01-24T02:38:00.000Z",
        "voteCount": 3,
        "content": "Detection of fraudulent transactions seems to be imbalanced data.\nhttps://cloud.google.com/automl-tables/docs/train#opt-obj\nAUC ROC : Distinguish between classes. Default value for binary classification.\n\nAUC PR\tOptimize results for  predictions for the less common class.\nit is straightforward to answer, you just have to capture key word to get the right way. (Almost banlanced Or Imbalanced)\nhttps://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/\n\nWhen to Use ROC vs. Precision-Recall Curves?\nGenerally, the use of ROC curves and precision-recall curves are as follows:\n\nROC curves should be used when there are roughly equal numbers of observations for each class.\nPrecision-Recall curves should be used when there is a moderate to large class imbalance."
      },
      {
        "date": "2023-01-05T05:15:00.000Z",
        "voteCount": 1,
        "content": "Fraud Detection --&gt; Imbalanced Dataset ---&gt; AUC PR --&gt; C, for me"
      },
      {
        "date": "2022-12-15T16:18:00.000Z",
        "voteCount": 1,
        "content": "ans: C\nPaul_Dirac and giaZ are correct."
      },
      {
        "date": "2022-12-15T13:54:00.000Z",
        "voteCount": 2,
        "content": "C\nhttps://towardsdatascience.com/on-roc-and-precision-recall-curves-c23e9b63820c"
      },
      {
        "date": "2022-09-07T02:07:00.000Z",
        "voteCount": 2,
        "content": "\"You need to prioritize detection of fraudulent transactions while minimizing false positives.\"\nSeems that answer B fits this well. If we want to focus exactly on minimizing false positives we can do that by maximising Precision at a specific Recall value. C is about balance between these two, and D doesn't care about false positive/negatives."
      },
      {
        "date": "2022-08-22T22:03:00.000Z",
        "voteCount": 1,
        "content": "D \nhttps://en.wikipedia.org/wiki/Receiver_operating_characteristic\nC optimize precision only"
      },
      {
        "date": "2022-08-22T22:07:00.000Z",
        "voteCount": 1,
        "content": "Sorry, C is my final decision\nhttps://cloud.google.com/automl-tables/docs/train#opt-obj"
      },
      {
        "date": "2022-08-06T03:03:00.000Z",
        "voteCount": 1,
        "content": "Answer is c."
      },
      {
        "date": "2022-03-10T07:35:00.000Z",
        "voteCount": 6,
        "content": "https://icaiit.org/proceedings/6th_ICAIIT/1_3Fayzrakhmanov.pdf\nThe problem of fraudulent transactions detection, which is an imbalanced classification problem (most transactions are not fraudulent), you want to maximize both precision and recall; so the area under the PR curve. As a matter of fact, the question asks you to focus on detecting fraudulent transactions (maximize true positive rate, a.k.a. Recall) while minimizing false positives (a.k.a. maximizing Precision). Another way to see it is this: for imbalanced problems like this one you'll get a lot of true negatives even from a bad model (it's easy to guess a transaction as \"non-fraudulent\" because most of them are!), and with high TN the ROC curve goes high fast, which would be misleading. So you wanna avoid dealing with true negatives in your evaluation, which is precisely what the PR curve allows you to do."
      },
      {
        "date": "2021-12-07T05:29:00.000Z",
        "voteCount": 3,
        "content": "The following is the official document for the list of optimization objectives for AutoML Tables\n\"About model optimization objectives\"\nhttps://cloud.google.com/automl-tables/docs/train#opt-obj\n\nAUC PR: Optimize results for predictions for the less common class."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 57,
    "url": "https://www.examtopics.com/discussions/google/view/56170-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "Your company manages a video sharing website where users can watch and upload videos. You need to create an ML model to predict which newly uploaded videos will be the most popular so that those videos can be prioritized on your company's website. Which result should you use to determine whether the model is successful?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe model predicts videos as popular if the user who uploads them has over 10,000 likes.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe model predicts 97.5% of the most popular clickbait videos measured by number of clicks.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe model predicts 95% of the most popular videos measured by watch time within 30 days of being uploaded.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe Pearson correlation coefficient between the log-transformed number of views after 7 days and 30 days after publication is equal to 0."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-06-27T07:32:00.000Z",
        "voteCount": 17,
        "content": "Ans: C (See https://developers.google.com/machine-learning/problem-framing/framing#quantify-it; though it's just an example.)\n(A) The absolute number of likes shouldn't be used because no information about subscribers or visits to the website is provided. The number may vary.\n(B) Clickbait videos are a subset of uploaded videos. Using them is an improper criterion.\n(D) The coefficient should reach 1. (Ref:https://arxiv.org/pdf/1510.06223.pdf)"
      },
      {
        "date": "2021-07-28T23:46:00.000Z",
        "voteCount": 5,
        "content": "Thanks for the detailed unswer and reference!"
      },
      {
        "date": "2024-06-06T11:42:00.000Z",
        "voteCount": 1,
        "content": "C) Watch time"
      },
      {
        "date": "2023-05-08T23:11:00.000Z",
        "voteCount": 1,
        "content": "Went with C"
      },
      {
        "date": "2022-12-15T16:25:00.000Z",
        "voteCount": 3,
        "content": "ans: C\nIn this type of questions, I think a good idea is trying to copy already existing solutions. For this case, YouTube cares a lot about watchtime. In a previous question, Amazon implemented \"Usually buy together\" for maximizing profit."
      },
      {
        "date": "2022-12-15T13:59:00.000Z",
        "voteCount": 1,
        "content": "Must be C"
      },
      {
        "date": "2022-07-11T02:37:00.000Z",
        "voteCount": 2,
        "content": "watch time among all other options is the most KPI to rely on"
      },
      {
        "date": "2022-03-17T09:05:00.000Z",
        "voteCount": 1,
        "content": "I think this is B. The question specifies \"popular\" and also that \"newly uploaded\" videos need prioritising. C is therefore wrong because you don't have that metric until 30 days has passed from upload time. \"Click through rate\" is one measure of popularity, so it fits, and is instant."
      },
      {
        "date": "2022-01-04T21:03:00.000Z",
        "voteCount": 1,
        "content": "C looks correct."
      },
      {
        "date": "2021-07-19T23:57:00.000Z",
        "voteCount": 3,
        "content": "ANS: C\n\nD is wrong. \nPearson's Correlation Coefficient is a linear correlation coefficient that returns a value of between -1 and +1. \nA -1 means there is a strong negative correlation\n+1 means that there is a strong positive correlation\n0 means that there is no correlation"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 58,
    "url": "https://www.examtopics.com/discussions/google/view/57554-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You are working on a Neural Network-based project. The dataset provided to you has columns with different ranges. While preparing the data for model training, you discover that gradient optimization is having difficulty moving weights to a good solution. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse feature construction to combine the strongest features.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the representation transformation (normalization) technique.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImprove the data cleaning step by removing features with missing values.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the partitioning step to reduce the dimension of the test set and have a larger training set."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 16,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-07-10T03:21:00.000Z",
        "voteCount": 25,
        "content": "Vote for B. We could impute instead of remove the column to avoid loss of information"
      },
      {
        "date": "2021-09-28T01:08:00.000Z",
        "voteCount": 10,
        "content": "I also think it is B:\n\"The presence of feature value X in the formula will affect the step size of the gradient descent. The difference in ranges of features will cause different step sizes for each feature. To ensure that the gradient descent moves smoothly towards the minima and that the steps for gradient descent are updated at the same rate for all the features, we scale the data before feeding it to the model.\""
      },
      {
        "date": "2024-08-03T09:30:00.000Z",
        "voteCount": 1,
        "content": "clearly B"
      },
      {
        "date": "2024-06-06T11:46:00.000Z",
        "voteCount": 2,
        "content": "B) Option B (Use the representation transformation technique) is the most relevant choice. Normalizing the features will help gradient descent converge efficiently, leading to better weight updates and improved model performance.\nRemember that feature scaling is crucial for gradient optimization, especially when dealing with features that have different ranges. By ensuring consistent scales, you\u2019ll enhance the effectiveness of your Neural Network training process."
      },
      {
        "date": "2024-04-01T01:31:00.000Z",
        "voteCount": 2,
        "content": "Because the range needs to normalize"
      },
      {
        "date": "2023-12-05T04:25:00.000Z",
        "voteCount": 2,
        "content": "B - The key phrase is \"different ranges\", therefore we need to normalize the values."
      },
      {
        "date": "2023-05-08T23:11:00.000Z",
        "voteCount": 1,
        "content": "Went with B"
      },
      {
        "date": "2023-05-02T06:09:00.000Z",
        "voteCount": 1,
        "content": "Normalization"
      },
      {
        "date": "2023-01-05T05:01:00.000Z",
        "voteCount": 2,
        "content": "Normalization is the word."
      },
      {
        "date": "2023-01-05T05:00:00.000Z",
        "voteCount": 1,
        "content": "Normalization is the word."
      },
      {
        "date": "2022-12-15T14:03:00.000Z",
        "voteCount": 1,
        "content": "B\n \"Normalization\" is the keyword"
      },
      {
        "date": "2022-01-19T13:51:00.000Z",
        "voteCount": 4,
        "content": "normalization\nhttps://developers.google.com/machine-learning/data-prep/transform/transform-numeric"
      },
      {
        "date": "2022-01-08T01:44:00.000Z",
        "voteCount": 4,
        "content": "B. The problem does not mention anything about missing values. It needs to normalize the features with different ranges."
      },
      {
        "date": "2022-01-04T21:06:00.000Z",
        "voteCount": 1,
        "content": "Looking at explanation I would choose C as well"
      },
      {
        "date": "2021-11-17T13:36:00.000Z",
        "voteCount": 3,
        "content": "(B)\n- NN models needs features with close ranges\n- SGD converges well using features in [0, 1] scale\n- The question specifically mention \"different ranges\"\nDocumentation - https://developers.google.com/machine-learning/data-prep/transform/transform-numeric"
      },
      {
        "date": "2021-09-16T17:12:00.000Z",
        "voteCount": 2,
        "content": "When gradient descent fails, it's out of the lacking of a powerful feature. Using normalization would make it worse.\nInstead, using either A or C would increase the strength of certain feature.\nBut, C should come first since A is only feasible after at least 1 meaningful training.\nSo C."
      },
      {
        "date": "2021-07-09T22:07:00.000Z",
        "voteCount": 3,
        "content": "B - remove the outliers?"
      },
      {
        "date": "2021-07-15T23:02:00.000Z",
        "voteCount": 4,
        "content": "Normalization is more complicated than that. \n\nNormalization changes the values of dataset's numeric fields to be in a common scale, without impacting differences in the ranges of values. Normalization is required only when features have different ranges."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 59,
    "url": "https://www.examtopics.com/discussions/google/view/57555-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "Your data science team needs to rapidly experiment with various features, model architectures, and hyperparameters. They need to track the accuracy metrics for various experiments and use an API to query the metrics over time. What should they use to track and report their experiments while minimizing manual effort?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Kubeflow Pipelines to execute the experiments. Export the metrics file, and query the results using the Kubeflow Pipelines API.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AI Platform Training to execute the experiments. Write the accuracy metrics to BigQuery, and query the results using the BigQuery API.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AI Platform Training to execute the experiments. Write the accuracy metrics to Cloud Monitoring, and query the results using the Monitoring API.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AI Platform Notebooks to execute the experiments. Collect the results in a shared Google Sheets file, and query the results using the Google Sheets API."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 26,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 9,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-02-14T20:08:00.000Z",
        "voteCount": 14,
        "content": "Old answer is A. New answer (not available) would be Virtex AI experiments which comes with monitoring API inbuilt. https://cloud.google.com/blog/topics/developers-practitioners/track-compare-manage-experiments-vertex-ai-experiments"
      },
      {
        "date": "2021-07-18T17:28:00.000Z",
        "voteCount": 12,
        "content": "ANS: A\nhttps://codelabs.developers.google.com/codelabs/cloud-kubeflow-pipelines-gis\nKubeflow Pipelines (KFP) helps solve these issues by providing a way to deploy robust, repeatable machine learning pipelines along with monitoring, auditing, version tracking, and reproducibility. Cloud AI Pipelines makes it easy to set up a KFP installation."
      },
      {
        "date": "2024-08-24T14:45:00.000Z",
        "voteCount": 1,
        "content": "This is an old question, when Vertex AI didn't have Vertex AI Experiments. The old answer is A"
      },
      {
        "date": "2024-07-22T03:47:00.000Z",
        "voteCount": 1,
        "content": "Shoudlnt it be B? VAI has inbuilt VAI experiments and metadata to track metrics.."
      },
      {
        "date": "2024-06-18T06:48:00.000Z",
        "voteCount": 1,
        "content": "Should agree with A"
      },
      {
        "date": "2024-06-06T11:51:00.000Z",
        "voteCount": 1,
        "content": "A) Kubeflow pipelines"
      },
      {
        "date": "2023-11-15T10:31:00.000Z",
        "voteCount": 5,
        "content": "either A or C but going with C due to minimal effort"
      },
      {
        "date": "2023-07-07T07:57:00.000Z",
        "voteCount": 1,
        "content": "I agree with tavva_prudhvi that cloud monitoring is not the best option to do machine learning tracking, Metadata is a better option for that purpose"
      },
      {
        "date": "2023-07-03T09:47:00.000Z",
        "voteCount": 2,
        "content": "Option C suggests using AI Platform Training to execute the experiments and write the accuracy metrics to Cloud Monitoring. While Cloud Monitoring can be used to monitor and collect metrics from various services in Google Cloud, it is not specifically designed for machine learning experiments tracking.\n\nUsing Cloud Monitoring for tracking machine learning experiments may not provide the same level of functionality and flexibility as Kubeflow Pipelines or AI Platform Training. Additionally, querying the results from Cloud Monitoring may not be as straightforward as using the APIs provided by Kubeflow Pipelines or AI Platform Training.\n\nTherefore, while Cloud Monitoring can be used as a general-purpose monitoring solution, it may not be the best option for tracking and reporting machine learning experiments."
      },
      {
        "date": "2023-06-19T07:28:00.000Z",
        "voteCount": 1,
        "content": "Cloud monitoring may not be the   most suitable option for tracking and reporting experiments, only because of this option C is out &amp; I stick to A"
      },
      {
        "date": "2023-05-08T23:11:00.000Z",
        "voteCount": 2,
        "content": "Went with A"
      },
      {
        "date": "2023-04-28T00:52:00.000Z",
        "voteCount": 1,
        "content": "It is B"
      },
      {
        "date": "2023-02-16T07:23:00.000Z",
        "voteCount": 1,
        "content": "This is the question, Try out and choose what is the closet to this lab.Last updated Jan 21, 2023\nhttps://codelabs.developers.google.com/vertex_experiments_pipelines_intro#0"
      },
      {
        "date": "2023-02-16T09:24:00.000Z",
        "voteCount": 1,
        "content": "As  The lab walk me through how to create pipe line to experiment ,   it use Kubeflow and apply experiment SDK"
      },
      {
        "date": "2023-01-05T03:35:00.000Z",
        "voteCount": 3,
        "content": "Vertex AI Experiments + Cloud Monitoring for the metrics. It's C!"
      },
      {
        "date": "2022-12-29T04:21:00.000Z",
        "voteCount": 1,
        "content": "I like C\nhttps://cloud.google.com/monitoring/mql"
      },
      {
        "date": "2022-12-16T02:23:00.000Z",
        "voteCount": 1,
        "content": "C: Google has already provided inhouse monitoring mechanism so no need to query or use any other tool. https://cloud.google.com/bigquery/docs/monitoring"
      },
      {
        "date": "2022-06-04T14:33:00.000Z",
        "voteCount": 1,
        "content": "https://www.kubeflow.org/docs/components/pipelines/introduction/#what-is-kubeflow-pipelines"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 60,
    "url": "https://www.examtopics.com/discussions/google/view/57556-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You work for a bank and are building a random forest model for fraud detection. You have a dataset that includes transactions, of which 1% are identified as fraudulent. Which data transformation strategy would likely improve the performance of your classifier?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite your data in TFRecords.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tZ-normalize all the numeric features.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOversample the fraudulent transaction 10 times.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse one-hot encoding on all categorical features."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 16,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-07-09T22:24:00.000Z",
        "voteCount": 13,
        "content": "C - https://swarit.medium.com/detecting-fraudulent-consumer-transactions-through-machine-learning-25b1f2cabbb4"
      },
      {
        "date": "2022-01-04T21:13:00.000Z",
        "voteCount": 5,
        "content": "C is the answer"
      },
      {
        "date": "2024-06-18T06:50:00.000Z",
        "voteCount": 1,
        "content": "Agree with C"
      },
      {
        "date": "2024-06-06T11:52:00.000Z",
        "voteCount": 1,
        "content": "C) Oversample"
      },
      {
        "date": "2024-04-01T01:39:00.000Z",
        "voteCount": 3,
        "content": "Oversampling increases the number of fraudulent transaction in the training data to enable the machine to learn how to predict them"
      },
      {
        "date": "2023-12-05T04:29:00.000Z",
        "voteCount": 1,
        "content": "C - Even though most similar questions propose to downsample the majority (not fraudulent) and add weights to it."
      },
      {
        "date": "2023-05-08T23:12:00.000Z",
        "voteCount": 2,
        "content": "Went with C"
      },
      {
        "date": "2022-12-15T16:34:00.000Z",
        "voteCount": 1,
        "content": "ans: C\n\nA, B, D =&gt; wouldnt help with imbalance"
      },
      {
        "date": "2022-12-15T14:17:00.000Z",
        "voteCount": 1,
        "content": "C\nhttps://medium.com/analytics-vidhya/credit-card-fraud-detection-how-to-handle-imbalanced-dataset-1f18b6f881"
      },
      {
        "date": "2022-07-11T02:47:00.000Z",
        "voteCount": 1,
        "content": "the best option is C"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 61,
    "url": "https://www.examtopics.com/discussions/google/view/90424-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You are using transfer learning to train an image classifier based on a pre-trained EfficientNet model. Your training dataset has 20,000 images. You plan to retrain the model once per day. You need to minimize the cost of infrastructure. What platform components and configuration environment should you use?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA Deep Learning VM with 4 V100 GPUs and local storage.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA Deep Learning VM with 4 V100 GPUs and Cloud Storage.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA Google Kubernetes Engine cluster with a V100 GPU Node Pool and an NFS Server",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAn AI Platform Training job using a custom scale tier with 4 V100 GPUs and Cloud Storage\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 24,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 4,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-15T16:38:00.000Z",
        "voteCount": 12,
        "content": "ans: D\n\nA, C =&gt; local storage, NFS... discarded. Google encourages you to use Cloud Storage.\nB =&gt; could do the job, but here I would focus on the \"daily training\" thing, because Vertex AI Training jobs are better for this. Also I think that Google usually encourages to use Vertex AI over VMs."
      },
      {
        "date": "2024-10-12T04:05:00.000Z",
        "voteCount": 1,
        "content": "Answer: D\nauto scaling"
      },
      {
        "date": "2024-07-22T03:49:00.000Z",
        "voteCount": 1,
        "content": "D because automatic scaling"
      },
      {
        "date": "2024-06-06T11:55:00.000Z",
        "voteCount": 1,
        "content": "D) Is the best answer"
      },
      {
        "date": "2023-11-28T11:22:00.000Z",
        "voteCount": 1,
        "content": "I'll go with D. How is C correct?"
      },
      {
        "date": "2023-11-15T11:55:00.000Z",
        "voteCount": 1,
        "content": "D as need to minimize cost"
      },
      {
        "date": "2023-08-02T08:13:00.000Z",
        "voteCount": 2,
        "content": "I think it is A. Refer to Q20 of the GCP Sample Questions - they say managed services (such as Kubeflow Pipelines / Vertex AI) are not the options for 'minimizing costs'. In this case, you should configure your own infrastructure to train the model leaving A,B. Undecided between A,B because A would minimize costs, but also result in inefficient I/O operations during training."
      },
      {
        "date": "2023-07-04T11:24:00.000Z",
        "voteCount": 2,
        "content": "The pre-trained EfficientNet model can be easily loaded from Cloud Storage, which eliminates the need for local storage or an NFS server. Using AI Platform Training allows for the automatic scaling of resources based on the size of the dataset, which can save costs compared to using a fixed-size VM or node pool. Additionally, the ability to use custom scale tiers allows for fine-tuning of resource allocation to match the specific needs of the training job."
      },
      {
        "date": "2023-05-08T23:12:00.000Z",
        "voteCount": 1,
        "content": "Went with D"
      },
      {
        "date": "2023-02-22T02:58:00.000Z",
        "voteCount": 2,
        "content": "B. A Deep Learning VM with 4 V100 GPUs and Cloud Storage.\n\nFor this scenario, a Deep Learning VM with 4 V100 GPUs and Cloud Storage is likely the most cost-effective solution while still providing sufficient computing resources for the model training. Using Cloud Storage can allow the model to be trained and the data to be stored in a scalable and cost-effective way.\n\nOption A, using a Deep Learning VM with local storage, may not provide enough storage capacity to store the training data and model checkpoints. Option C, using a Kubernetes Engine cluster, can be overkill for the size of the job and adds additional complexity. Option D, using an AI Platform Training job, is a good option as it is designed for running machine learning jobs at scale, but may be more expensive than a Deep Learning VM with Cloud Storage."
      },
      {
        "date": "2023-02-08T17:02:00.000Z",
        "voteCount": 1,
        "content": "becouse it's cheap"
      },
      {
        "date": "2022-12-16T02:38:00.000Z",
        "voteCount": 3,
        "content": "it seems D"
      },
      {
        "date": "2022-12-09T17:53:00.000Z",
        "voteCount": 2,
        "content": "I think it's D"
      },
      {
        "date": "2022-12-09T06:40:00.000Z",
        "voteCount": 2,
        "content": "It's D"
      },
      {
        "date": "2022-12-07T07:25:00.000Z",
        "voteCount": 4,
        "content": "It seems D to me."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 62,
    "url": "https://www.examtopics.com/discussions/google/view/90506-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "While conducting an exploratory analysis of a dataset, you discover that categorical feature A has substantial predictive power, but it is sometimes missing. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDrop feature A if more than 15% of values are missing. Otherwise, use feature A as-is.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCompute the mode of feature A and then use it to replace the missing values in feature A.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReplace the missing values with the values of the feature with the highest Pearson correlation with feature A.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd an additional class to categorical feature A for missing values. Create a new binary feature that indicates whether feature A is missing.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 23,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 11,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-15T16:50:00.000Z",
        "voteCount": 14,
        "content": "ans: D\n\nA =&gt; no, you don't want to drop a feature with high prediction power.\nB =&gt; i think this could confuse the model... a better solution could be to fill missing values using an algorithm like Expectation Maximization, but using the mode i think is a bad idea in this case, because if you have a significant number of missing values (for example &gt;10%)  this would modify the \"predictive power\". you don't want to lose predictive power of a feature, just guide the model to learn when to use that feature and when to ignore it.\nC =&gt; this doesn't make any sense for me. not sure what i would do that.\nD =&gt; i think this could be a really good approach, and i'm pretty sure it would work pretty well a lot of models. the model would learn that when \"is_available_feat_A\" == True, then it would use the feature A, but whenever it is missing then it would try to use other features."
      },
      {
        "date": "2023-04-12T02:00:00.000Z",
        "voteCount": 2,
        "content": "I guess I would go with D, but it confuses me the fact that in option D, it doesn't say that NaN values are replaced (only that there's a new column added) and this could lead to problems like exploding gradients.\nPlus, Google encourages to replace missing values. https://developers.google.com/machine-learning/testing-debugging/common/data-errors\nAny thoughts on this?"
      },
      {
        "date": "2024-06-06T12:00:00.000Z",
        "voteCount": 1,
        "content": "D) Good approach"
      },
      {
        "date": "2024-04-01T01:48:00.000Z",
        "voteCount": 1,
        "content": "Google encourages filling missing value and using mode is one of the examples given. D only tell the obvious - data is missing!"
      },
      {
        "date": "2023-12-05T04:54:00.000Z",
        "voteCount": 1,
        "content": "B and D are correct, but I decided to go with D."
      },
      {
        "date": "2023-11-15T12:19:00.000Z",
        "voteCount": 1,
        "content": "highly predictive"
      },
      {
        "date": "2023-11-11T12:13:00.000Z",
        "voteCount": 2,
        "content": "Definitely not D, it does not even solve the problem of NA values."
      },
      {
        "date": "2023-09-20T13:46:00.000Z",
        "voteCount": 1,
        "content": "Options B or D\nBut isnt there an inconsistency in option D? if you replace missing values with a new category (\"missing\") why would you haveto create an extra feature?"
      },
      {
        "date": "2023-07-07T08:00:00.000Z",
        "voteCount": 1,
        "content": "Agree with wish0035, answer should be D"
      },
      {
        "date": "2023-06-22T00:24:00.000Z",
        "voteCount": 2,
        "content": "By creating a new class for the missing values, you explicitly capture the absence of data, which can provide valuable information for predictive modeling. Additionally, creating a binary feature allows the model to distinguish between cases where feature A is present and cases where it is missing, which can be useful for identifying potential patterns or relationships in the data."
      },
      {
        "date": "2023-06-10T13:35:00.000Z",
        "voteCount": 1,
        "content": "By imputing the missing values with the mode (the most frequent value), you retain the original feature's predictive power while handling the missing values"
      },
      {
        "date": "2023-05-25T01:19:00.000Z",
        "voteCount": 2,
        "content": "Both B and D are possible, but the correct answer is D because of the feature high predictive power."
      },
      {
        "date": "2023-05-08T23:12:00.000Z",
        "voteCount": 1,
        "content": "Went with D"
      },
      {
        "date": "2023-03-15T09:57:00.000Z",
        "voteCount": 2,
        "content": "I think, its D. \nOption B of imputing the missing values of feature A with the mode of feature A could be a reasonable approach if the mode provides a good representation of the distribution of feature A. However, this method may lead to biased results if the mode is not representative of the missing values. This could be the case if the missing values have a different distribution than the observed values.\n\nSimilarly, When a categorical feature has substantial predictive power, it is important not to discard it. Instead, missing values can be handled by adding an additional class for missing values and creating a new binary feature that indicates whether feature A is missing or not. This approach ensures that the predictive power of feature A is retained while accounting for missing values. Computing the mode of feature A and replacing missing values may distort the distribution of the feature and create bias in the analysis. Similarly, replacing missing values with values from another feature may introduce noise and lead to incorrect results."
      },
      {
        "date": "2023-02-27T08:04:00.000Z",
        "voteCount": 1,
        "content": "If our objective was to produce a complete dataset then we might use some average value to fill in the gaps (option B) but in this case we want to predict an outcome, so inventing our own data is not going to help in my view.\n\nOption D is the most sensible approach to let the model choose the best features."
      },
      {
        "date": "2022-12-16T02:49:00.000Z",
        "voteCount": 4,
        "content": "B\n\"For categorical variables, we can usually replace missing values with mean, median, or most frequent values\"\nDr. Logan Song - Journey to Become a Google Cloud Machine Learning Engineer - Page 48"
      },
      {
        "date": "2023-11-07T10:17:00.000Z",
        "voteCount": 1,
        "content": "While this approach may seem reasonable, it can introduce bias in the dataset by over-representing the mode, especially if the missing values are not missing at random."
      },
      {
        "date": "2022-12-16T02:04:00.000Z",
        "voteCount": 2,
        "content": "B. Because the important feature is already known. By using mode, contribution of other features will not be missed"
      },
      {
        "date": "2022-12-11T05:13:00.000Z",
        "voteCount": 3,
        "content": "Mode is the way to go for categorical features. B, for me."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 63,
    "url": "https://www.examtopics.com/discussions/google/view/90507-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You work for a large retailer and have been asked to segment your customers by their purchasing habits. The purchase history of all customers has been uploaded to BigQuery. You suspect that there may be several distinct customer segments, however you are unsure of how many, and you don\u2019t yet understand the commonalities in their behavior. You want to find the most efficient solution. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a k-means clustering model using BigQuery ML. Allow BigQuery to automatically optimize the number of clusters.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new dataset in Dataprep that references your BigQuery table. Use Dataprep to identify similarities within each column.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Data Labeling Service to label each customer record in BigQuery. Train a model on your labeled data using AutoML Tables. Review the evaluation metrics to understand whether there is an underlying pattern in the data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGet a list of the customer segments from your company\u2019s Marketing team. Use the Data Labeling Service to label each customer record in BigQuery according to the list. Analyze the distribution of labels in your dataset using Data Studio."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 23,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-06-06T12:02:00.000Z",
        "voteCount": 1,
        "content": "A) K-means is ideal for unsupervised clustering"
      },
      {
        "date": "2024-04-01T01:51:00.000Z",
        "voteCount": 3,
        "content": "K-means algorithm is used for grouping/clustering data in unsupervised learning experiments."
      },
      {
        "date": "2023-05-08T23:13:00.000Z",
        "voteCount": 4,
        "content": "Went with A"
      },
      {
        "date": "2023-05-08T21:57:00.000Z",
        "voteCount": 3,
        "content": "when to use k-means : Your data may contain natural groupings or clusters of data. You may want to identify these groupings descriptively in order to make data-driven decisions. For example, a retailer may want to identify natural groupings of customers who have similar purchasing habits or locations. This process is known as customer segmentation.\nhttps://cloud.google.com/bigquery/docs/kmeans-tutorial"
      },
      {
        "date": "2023-03-15T10:01:00.000Z",
        "voteCount": 2,
        "content": "A\nThis is the most efficient solution for segmenting customers based on their purchasing habits, as it utilizes BigQuery's built-in machine learning capabilities to identify distinct clusters of customers based on their purchasing behavior. By allowing BigQuery to automatically optimize the number of clusters, you can ensure that the model identifies the most appropriate number of segments based on the data, without having to manually select the number of clusters."
      },
      {
        "date": "2023-01-05T03:07:00.000Z",
        "voteCount": 2,
        "content": "I correct myself. It's A:\nAccording to the documentation, if you omit the num_clusters option, BigQuery ML will choose a reasonable default based on the total number of rows in the training data."
      },
      {
        "date": "2022-12-16T02:52:00.000Z",
        "voteCount": 3,
        "content": "A\nhttps://cloud.google.com/bigquery-ml/docs/kmeans-tutorial\nhttps://towardsdatascience.com/how-to-use-k-means-clustering-in-bigquery-ml-to-understand-and-describe-your-data-better-c972c6f5733b"
      },
      {
        "date": "2022-12-15T16:59:00.000Z",
        "voteCount": 4,
        "content": "ans: A, pretty sure.\n\nC, D =&gt; discarded, very time consuming.\nB =&gt; yes, you can identify similarities within each column, but when i read \"you don\u2019t yet understand the commonalities in their behavior\" i understand that this job would be difficult, because there could be many columns to analyze, and i don't think that this would be efficient.\n\nA =&gt; BigQuery ML is compatible with kmeans clustering, it's easy and efficient to create, and i would automatically detect the number of clusters.\n\nAlso from the BigQuery ML docs: \"K-means clustering for data segmentation; for example, identifying customer segments.\"\n(Source: https://cloud.google.com/bigquery-ml/docs/introduction#supported_models_in)"
      },
      {
        "date": "2022-12-15T09:58:00.000Z",
        "voteCount": 3,
        "content": "K-means is a good unsupervised learning algorithm to segment a population based on similarity\n\nWe can usa K-means directly in BQ, so I think it's \"the most efficient way\"\n\nLabeling is not a good option since we don't really know what make a customer similar to another, and why dataprep if we can use directly BQ?"
      },
      {
        "date": "2022-12-13T06:21:00.000Z",
        "voteCount": 1,
        "content": "It seems B, to me."
      },
      {
        "date": "2022-12-10T12:08:00.000Z",
        "voteCount": 1,
        "content": "Its B!  Dataprep provides Data profiling functionalities"
      },
      {
        "date": "2022-12-10T07:28:00.000Z",
        "voteCount": 1,
        "content": "The question is about commonalities of clients by characteristics, no about characteristics by client. I mean with B you are looking for segments of the characteristics which define a client. But you need segments of clients defined by characteristics."
      },
      {
        "date": "2022-12-07T15:30:00.000Z",
        "voteCount": 4,
        "content": "Will go for 'A' as it is easy to build model in BQML where data is already present and optimization would be auto in case of K-mean algo"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 64,
    "url": "https://www.examtopics.com/discussions/google/view/90508-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You recently designed and built a custom neural network that uses critical dependencies specific to your organization\u2019s framework. You need to train the model using a managed training service on Google Cloud. However, the ML framework and related dependencies are not supported by AI Platform Training. Also, both your model and your data are too large to fit in memory on a single machine. Your ML framework of choice uses the scheduler, workers, and servers distribution structure. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a built-in model available on AI Platform Training.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBuild your custom container to run jobs on AI Platform Training.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBuild your custom containers to run distributed training jobs on AI Platform Training.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReconfigure your code to a ML framework with dependencies that are supported by AI Platform Training."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 18,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-12-18T04:08:00.000Z",
        "voteCount": 9,
        "content": "Answer C. By running your machine learning (ML) training job in a custom container, you can use ML frameworks, non-ML dependencies, libraries, and binaries that are not otherwise supported on Vertex AI. \nModel and your data are too large to fit in memory on a single machine hence distributed training jobs.\nhttps://cloud.google.com/vertex-ai/docs/training/containers-overview"
      },
      {
        "date": "2024-06-06T12:06:00.000Z",
        "voteCount": 1,
        "content": "C) Distributed training with customer containers"
      },
      {
        "date": "2024-04-01T01:55:00.000Z",
        "voteCount": 3,
        "content": "This allows using external dependences and distributed training will solve the memory issues"
      },
      {
        "date": "2024-02-28T22:13:00.000Z",
        "voteCount": 2,
        "content": "Critical dependencies that are not supported -&gt; Custom container\nToo large to fit in memory on a single machine -&gt; Distributed"
      },
      {
        "date": "2023-05-08T23:13:00.000Z",
        "voteCount": 1,
        "content": "Went with C"
      },
      {
        "date": "2022-12-15T17:01:00.000Z",
        "voteCount": 1,
        "content": "ans: C\n\nA, D =&gt; too much work.\nB =&gt; discarded because \"model and your data are too large to fit in memory on a single machine\""
      },
      {
        "date": "2022-12-11T05:33:00.000Z",
        "voteCount": 1,
        "content": "C, for me!"
      },
      {
        "date": "2022-12-09T06:52:00.000Z",
        "voteCount": 1,
        "content": "I think it's C"
      },
      {
        "date": "2022-12-07T15:46:00.000Z",
        "voteCount": 1,
        "content": "Will go for 'C'- Custom containers can address the env limitation and distributed processing will handle the data volume"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 65,
    "url": "https://www.examtopics.com/discussions/google/view/90582-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "While monitoring your model training\u2019s GPU utilization, you discover that you have a native synchronous implementation. The training data is split into multiple files. You want to reduce the execution time of your input pipeline. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the CPU load",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd caching to the pipeline",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the network bandwidth",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd parallel interleave to the pipeline\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 14,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-12-16T03:04:00.000Z",
        "voteCount": 7,
        "content": "It's D\nhttps://www.tensorflow.org/guide/data_performance"
      },
      {
        "date": "2024-06-06T12:08:00.000Z",
        "voteCount": 1,
        "content": "D) Parallelisation required"
      },
      {
        "date": "2024-04-01T01:57:00.000Z",
        "voteCount": 2,
        "content": "Multiple files reduce execution time through papalism"
      },
      {
        "date": "2024-02-28T22:15:00.000Z",
        "voteCount": 1,
        "content": "\"training data split into multiple files\", \"reduce the execution time of your input pipeline\" -&gt; Parallel interleave"
      },
      {
        "date": "2023-05-08T23:13:00.000Z",
        "voteCount": 1,
        "content": "Went with D"
      },
      {
        "date": "2022-12-09T14:29:00.000Z",
        "voteCount": 2,
        "content": "I think it's D"
      },
      {
        "date": "2022-12-08T03:42:00.000Z",
        "voteCount": 1,
        "content": "D for me"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 66,
    "url": "https://www.examtopics.com/discussions/google/view/90591-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "Your data science team is training a PyTorch model for image classification based on a pre-trained RestNet model. You need to perform hyperparameter tuning to optimize for several parameters. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConvert the model to a Keras model, and run a Keras Tuner job.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun a hyperparameter tuning job on AI Platform using custom containers.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Kuberflow Pipelines instance, and run a hyperparameter tuning job on Katib.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConvert the model to a TensorFlow model, and run a hyperparameter tuning job on AI Platform."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 19,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-12-09T18:10:00.000Z",
        "voteCount": 8,
        "content": "B because Vertex AI supports custom models hyperparameter tuning"
      },
      {
        "date": "2024-06-06T12:10:00.000Z",
        "voteCount": 1,
        "content": "B) Customer containers"
      },
      {
        "date": "2023-05-08T23:13:00.000Z",
        "voteCount": 1,
        "content": "Went with B"
      },
      {
        "date": "2023-02-16T18:38:00.000Z",
        "voteCount": 1,
        "content": "This is a question sourced from google blog\n pre-trained BERT model  \nhttps://cloud.google.com/blog/topics/developers-practitioners/pytorch-google-cloud-how-train-and-tune-pytorch-models-vertex-ai\nhttps://cloud.google.com/blog/topics/developers-practitioners/pytorch-google-cloud-how-deploy-pytorch-models-vertex-ai"
      },
      {
        "date": "2023-01-25T04:56:00.000Z",
        "voteCount": 4,
        "content": "C:\nDon't wast your time to convert to other framework, you can use it on custom container absolutely.\nhttps://cloud.google.com/blog/topics/developers-practitioners/pytorch-google-cloud-how-train-and-tune-pytorch-models-vertex-ai"
      },
      {
        "date": "2023-01-25T05:22:00.000Z",
        "voteCount": 3,
        "content": "I insist on B, At the present, it seem like we can use prebuilt container instead of custom container, but none of the 4 choice, so B is the most likely  way out of this question."
      },
      {
        "date": "2022-12-15T17:05:00.000Z",
        "voteCount": 4,
        "content": "ans: B\n\nA, D =&gt; too much work.\nC =&gt; not sure why you would complicate so much when Vertex AI has this feature in custom containers."
      },
      {
        "date": "2022-12-08T04:11:00.000Z",
        "voteCount": 1,
        "content": "C seems to correct- https://www.kubeflow.org/docs/components/katib/overview/"
      },
      {
        "date": "2022-12-10T06:28:00.000Z",
        "voteCount": 5,
        "content": "Why use a thrid-party tool when Vertex AI already let you tuning hyperparameters in custom containers? I think it's B"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 67,
    "url": "https://www.examtopics.com/discussions/google/view/90589-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You have a large corpus of written support cases that can be classified into 3 separate categories: Technical Support, Billing Support, or Other Issues. You need to quickly build, test, and deploy a service that will automatically classify future written requests into one of the categories. How should you configure the pipeline?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Cloud Natural Language API to obtain metadata to classify the incoming cases.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AutoML Natural Language to build and test a classifier. Deploy the model as a REST API.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse BigQuery ML to build and test a logistic regression model to classify incoming requests. Use BigQuery ML to perform inference.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a TensorFlow model using Google\u2019s BERT pre-trained model. Build and test a classifier, and deploy the model using Vertex AI."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 18,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-12-15T17:08:00.000Z",
        "voteCount": 8,
        "content": "ans: B\n\nA =&gt; no, you need customization.\nC, B =&gt; more work and complexity\n\nB =&gt; AutoML is easier and faster and \"you need to quickly build, test, and deploy\". Also the REST API part fits our use case."
      },
      {
        "date": "2024-06-06T22:55:00.000Z",
        "voteCount": 1,
        "content": "B) AutoML NLP"
      },
      {
        "date": "2024-04-13T01:34:00.000Z",
        "voteCount": 1,
        "content": "Went with B"
      },
      {
        "date": "2024-04-01T02:03:00.000Z",
        "voteCount": 1,
        "content": "AutoML is faster and offers the requisite REST API"
      },
      {
        "date": "2024-02-28T22:19:00.000Z",
        "voteCount": 1,
        "content": "\"quickly build, test and deploy\" + custom categories -&gt; AutoML"
      },
      {
        "date": "2023-05-08T23:14:00.000Z",
        "voteCount": 1,
        "content": "Went with B"
      },
      {
        "date": "2023-04-20T02:48:00.000Z",
        "voteCount": 1,
        "content": "I think it's B, but I don't understand why it doesn't suggest to deploy the model on Vertex AI instead of as a REST API."
      },
      {
        "date": "2023-02-08T19:11:00.000Z",
        "voteCount": 1,
        "content": "ans B becouse es more fast"
      },
      {
        "date": "2022-12-16T03:09:00.000Z",
        "voteCount": 3,
        "content": "B\nwish0035 explained"
      },
      {
        "date": "2022-12-11T07:44:00.000Z",
        "voteCount": 1,
        "content": "Quickly: AutoML: B."
      },
      {
        "date": "2022-12-09T18:14:00.000Z",
        "voteCount": 1,
        "content": "I think it's B because of the deployment"
      },
      {
        "date": "2022-12-08T04:07:00.000Z",
        "voteCount": 1,
        "content": "B will give quick result on classification"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 68,
    "url": "https://www.examtopics.com/discussions/google/view/90596-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You need to quickly build and train a model to predict the sentiment of customer reviews with custom categories without writing code. You do not have enough data to train a model from scratch. The resulting model should have high predictive performance. Which service should you use?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAutoML Natural Language\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Natural Language API",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAI Hub pre-made Jupyter Notebooks",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAI Platform Training built-in algorithms"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 18,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 8,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-06-06T22:56:00.000Z",
        "voteCount": 1,
        "content": "A) AutoML - Codeless"
      },
      {
        "date": "2024-05-27T03:23:00.000Z",
        "voteCount": 1,
        "content": "\"Quickly build\" &gt;&gt; usually go with the low-code/no-code options of autoML"
      },
      {
        "date": "2024-04-20T07:22:00.000Z",
        "voteCount": 1,
        "content": "AutoML does not have transfer learning capabilities as of now. Given that there are\n not enough data to train from scratch, B is the only option that makes sense."
      },
      {
        "date": "2024-04-22T05:43:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/vertex-ai/docs/text-data/sentiment-analysis/prepare-data"
      },
      {
        "date": "2024-04-01T02:24:00.000Z",
        "voteCount": 1,
        "content": "This suitable job for AutoML, it used transfer learning when there is small data for training."
      },
      {
        "date": "2024-09-03T02:54:00.000Z",
        "voteCount": 1,
        "content": "AutoML now supports Transfer learning, I checked it."
      },
      {
        "date": "2024-02-23T05:43:00.000Z",
        "voteCount": 3,
        "content": "AutoML Natural Language is designed to work well even with relatively small datasets. It uses transfer learning and other techniques to train models effectively on limited data, which is crucial since there's enough data to train a model from scratch."
      },
      {
        "date": "2023-11-11T12:44:00.000Z",
        "voteCount": 1,
        "content": "Custom models and custom categories and hence AutoML natural language, It would still work with less data"
      },
      {
        "date": "2023-10-29T10:38:00.000Z",
        "voteCount": 2,
        "content": "NO DATA TO TRAIN THE MODEL FROM SCRACH"
      },
      {
        "date": "2024-05-27T06:37:00.000Z",
        "voteCount": 1,
        "content": "\"You do not have enough data to train a model from scratch\" - I think this means that there is SOME data but not a lot, something which AutoML can handle."
      },
      {
        "date": "2023-05-08T23:14:00.000Z",
        "voteCount": 1,
        "content": "Went with A"
      },
      {
        "date": "2023-03-29T20:59:00.000Z",
        "voteCount": 2,
        "content": "It's A. \"Custom categories\" means B can't be correct"
      },
      {
        "date": "2023-03-15T10:15:00.000Z",
        "voteCount": 3,
        "content": "Its A, Check this document, https://cloud.google.com/natural-language/automl/docs/beginners-guide\nThe Natural Language API discovers syntax, entities, and sentiment in text, and classifies text into a predefined set of categories."
      },
      {
        "date": "2023-02-22T03:31:00.000Z",
        "voteCount": 4,
        "content": "If you do not have enough data to train a model from scratch, then it may be more appropriate to use a pre-trained model or a pre-made Jupyter Notebook.\n\nOption B, the Cloud Natural Language API, could still be a viable option if you have access to labeled data for sentiment analysis. The API provides pre-trained models for sentiment analysis that you can use to classify text. However, if you have custom categories or labels, then you would need to train a custom model, which may not be feasible with limited data."
      },
      {
        "date": "2023-02-08T19:16:00.000Z",
        "voteCount": 2,
        "content": "https://www.toptal.com/machine-learning/google-nlp-tutorial#:~:text=Google%20Natural%20Language%20API%20vs.&amp;text=Google%20AutoML%20Natural%20Language%20is,t%20require%20machine%20learning%20knowledge.\nIn this case need custom categories without writing code"
      },
      {
        "date": "2023-01-25T05:43:00.000Z",
        "voteCount": 2,
        "content": "Quickly ==&gt; A and B and custom categories + you do not have enough data to train a model (it doesn't mean no data to train) it will probably have a few samples Let's say 10 samples)  as this link https://cloud.google.com/natural-language/automl/docs/beginners-guide#include-enough-labeled-examples-in-each-category\n==&gt; A"
      },
      {
        "date": "2023-01-25T05:41:00.000Z",
        "voteCount": 1,
        "content": "Quickly ==&gt; A and  B  and  custom categories + you do not have enough data to train a model (it doesn't mean no data to train)  it will probably have a few samples Let's say 10 samples) \n ==&gt; B"
      },
      {
        "date": "2023-01-25T05:43:00.000Z",
        "voteCount": 3,
        "content": "Sorry, I go with A A A A A A"
      },
      {
        "date": "2023-01-25T05:41:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/natural-language/automl/docs/beginners-guide#include-enough-labeled-examples-in-each-category"
      },
      {
        "date": "2022-12-27T03:41:00.000Z",
        "voteCount": 2,
        "content": "B is the correct answer\nAutoML needs data for training and its clearly mentioned we don't have any data."
      },
      {
        "date": "2023-03-15T10:10:00.000Z",
        "voteCount": 1,
        "content": "they said, \"do not have enough data\"!!!!"
      },
      {
        "date": "2022-12-16T03:10:00.000Z",
        "voteCount": 1,
        "content": "A\nwish0035 explained"
      },
      {
        "date": "2022-12-16T01:59:00.000Z",
        "voteCount": 1,
        "content": "B is the correct answer. The API connects with the prebuilt Google NLP model for prediction"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 69,
    "url": "https://www.examtopics.com/discussions/google/view/90750-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You need to build an ML model for a social media application to predict whether a user\u2019s submitted profile photo meets the requirements. The application will inform the user if the picture meets the requirements. How should you build a model to ensure that the application does not falsely accept a non-compliant picture?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AutoML to optimize the model\u2019s recall in order to minimize false negatives.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AutoML to optimize the model\u2019s F1 score in order to balance the accuracy of false positives and false negatives.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Vertex AI Workbench user-managed notebooks to build a custom model that has three times as many examples of pictures that meet the profile photo requirements.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Vertex AI Workbench user-managed notebooks to build a custom model that has three times as many examples of pictures that do not meet the profile photo requirements."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 28,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 20,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 15,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-10T06:40:00.000Z",
        "voteCount": 17,
        "content": "I think it's B, since we want to reduce false positives"
      },
      {
        "date": "2023-01-08T18:47:00.000Z",
        "voteCount": 3,
        "content": "B\nyes, A is incorrect as minimize false negatives does not help"
      },
      {
        "date": "2024-09-18T04:46:00.000Z",
        "voteCount": 1,
        "content": "False negative: Non-compliant, but did not alert. That is what we want to minimize."
      },
      {
        "date": "2024-09-18T04:49:00.000Z",
        "voteCount": 1,
        "content": "Upon reading further it seems like the model predicts compliance, so a positive means the picture is compliant. Then B seems more appropriate"
      },
      {
        "date": "2023-06-25T07:18:00.000Z",
        "voteCount": 9,
        "content": "a non-compliant profile image = positive\nfalse negatives = didn't alert the non-compliant profile image\nso the objective is to minimize false nagatives"
      },
      {
        "date": "2023-07-23T03:28:00.000Z",
        "voteCount": 13,
        "content": "The answer is A. The negative event is usually labeled as positive (e.g., fraud detection, customer default prediction, and here non-compliant picture identification). The question explicitly says, \"ensure that the application does not falsely accept a non-compliant picture.\" So we should avoid falsely labeling a non-compliant image as compliant (negative). \n\nIt is never mentioned in the question that false positives are also a concern. So, recall is better than F1-score for this problem."
      },
      {
        "date": "2024-09-08T05:00:00.000Z",
        "voteCount": 1,
        "content": "The question explicitly states that this isn't the case, it's identifying compliant images, it is compliance that is the positive, so F1 is the only sensible metric."
      },
      {
        "date": "2024-09-08T05:01:00.000Z",
        "voteCount": 1,
        "content": "A is wrong because we are trying to minimise false positives, not false negatives. The question states that the model identifies compliance (rather than non-compliance) so a positive means compliant.\nB is correct, though one would usually say \"we are trying to optimise precision\", optimising F1 is the only answer that addresses this, albiet not as directly as I'd like.\nC and D are nonsense."
      },
      {
        "date": "2024-06-07T00:14:00.000Z",
        "voteCount": 1,
        "content": "A) Minimise False Negatives"
      },
      {
        "date": "2024-05-27T09:19:00.000Z",
        "voteCount": 1,
        "content": "D. Cost of Misclassification: In this scenario, falsely accepting a non-compliant picture (false positive) is more critical than rejecting a compliant picture (false negative). A user with a non-compliant picture could violate the platform's terms or negatively impact the user experience.\nTraining Data Imbalance: Social media applications might receive many compliant pictures and far fewer non-compliant ones. A standard training dataset might be imbalanced, with the model learning more from the majority class (compliant pictures)."
      },
      {
        "date": "2024-03-27T06:07:00.000Z",
        "voteCount": 1,
        "content": "Gonna go with B on this one, tricky question but since reducing false positives is the goal here only B fits that requirement"
      },
      {
        "date": "2024-04-27T03:50:00.000Z",
        "voteCount": 1,
        "content": "a non-compliant profile image = positive\nfalse negatives = didn't alert the non-compliant profile image\nso the objective is to minimize false nagatives"
      },
      {
        "date": "2024-03-04T05:04:00.000Z",
        "voteCount": 2,
        "content": "I went with A."
      },
      {
        "date": "2024-01-23T07:40:00.000Z",
        "voteCount": 2,
        "content": "B.\nA non-compliant picture is the positive and not the negative. What the question is asking is to decrease the number of false positives (\"falsely labeled as non compliant\"), which is achieved through optimizing for precision and not recall. Since C and D sound a bit overkill, I would go for the one that prioritizes false positives which is B."
      },
      {
        "date": "2023-11-17T01:23:00.000Z",
        "voteCount": 1,
        "content": "Think is B since we need to optimize for percision"
      },
      {
        "date": "2023-11-15T22:29:00.000Z",
        "voteCount": 1,
        "content": "Minimize False positive. Hence percision. D is the closest."
      },
      {
        "date": "2023-11-11T12:42:00.000Z",
        "voteCount": 1,
        "content": "Optimising for false positives is the goal here which should have been precision. Since precision is not available in options, the next best is F1 score which is harmonic mean of precision and recall. Although it wont fully satisfy the false positives it atleast wont skew towards recall which is more false positives that deviates from the goal. Hence B"
      },
      {
        "date": "2023-10-22T04:23:00.000Z",
        "voteCount": 1,
        "content": "We should optimize for precision to minimize false positives, so optimizing for recall should be incorrect. F1 Score will balance both precision and recall. Both B and C might not necessarily meet the goal"
      },
      {
        "date": "2023-10-10T07:46:00.000Z",
        "voteCount": 1,
        "content": "I vote B"
      },
      {
        "date": "2023-09-27T08:25:00.000Z",
        "voteCount": 2,
        "content": "A. Let me explain why. You may have 3 times more examples of images, however, the total number of images can be small, which lead to poor model performace, so C and D are not the for definite answer. The target is the detection of abnormal photo, so  falsely accept a non-compliant picture is false negative. So A."
      },
      {
        "date": "2023-09-27T08:22:00.000Z",
        "voteCount": 1,
        "content": "A. Let me explain why. You may have 3 times more examples of images, however, the total number of images can be small, which lead to poor model performace, so C and D are not the for definite answer. The target is the detection of abnormal photo, so  falsely accept a non-compliant picture is false negative. So A."
      },
      {
        "date": "2023-09-24T07:58:00.000Z",
        "voteCount": 1,
        "content": "I was thinking B but after reading the comments I think it should be A. \nI was thinking a non-compliant profile image = negative but actually it should be the positive case we do want to flag out. So minimising false negative fits the requirement \"ensure that the application does not falsely accept a non-compliant picture.\""
      },
      {
        "date": "2023-07-07T14:24:00.000Z",
        "voteCount": 1,
        "content": "B should be correct. It covers not only the recall but also the precision"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 70,
    "url": "https://www.examtopics.com/discussions/google/view/90751-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You lead a data science team at a large international corporation. Most of the models your team trains are large-scale models using high-level TensorFlow APIs on AI Platform with GPUs. Your team usually takes a few weeks or months to iterate on a new version of a model. You were recently asked to review your team\u2019s spending. How should you reduce your Google Cloud compute costs without impacting the model\u2019s performance?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AI Platform to run distributed training jobs with checkpoints.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AI Platform to run distributed training jobs without checkpoints.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate to training with Kuberflow on Google Kubernetes Engine, and use preemptible VMs with checkpoints.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate to training with Kuberflow on Google Kubernetes Engine, and use preemptible VMs without checkpoints."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 25,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 6,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-16T03:48:00.000Z",
        "voteCount": 10,
        "content": "https://cloud.google.com/blog/products/ai-machine-learning/reduce-the-costs-of-ml-workflows-with-preemptible-vms-and-gpus?hl=en"
      },
      {
        "date": "2024-06-07T01:01:00.000Z",
        "voteCount": 1,
        "content": "C) Preemptible VMs with Check points"
      },
      {
        "date": "2024-04-01T02:57:00.000Z",
        "voteCount": 3,
        "content": "Pre-emptive VMs are cheaper and checkpoints will enable termination if the result is acceptable"
      },
      {
        "date": "2023-09-27T08:31:00.000Z",
        "voteCount": 1,
        "content": "I guess distributed training is not cheap. So C."
      },
      {
        "date": "2023-09-19T12:34:00.000Z",
        "voteCount": 3,
        "content": "C is the best approach because it allows you to reduce your compute costs without impacting the model's performance. Preemptible VMs are much cheaper than standard VMs, but they can be terminated at any time. By using checkpoints, you can ensure that your training job can be resumed if a preemptible VM is terminated. \nAlso, even if training takes days, the checkpoints will prevent lossing the progress if preemtible VM are down."
      },
      {
        "date": "2023-07-07T08:05:00.000Z",
        "voteCount": 2,
        "content": "Optimize cost then should use kubeflow"
      },
      {
        "date": "2023-05-08T23:15:00.000Z",
        "voteCount": 1,
        "content": "Went with C"
      },
      {
        "date": "2023-05-08T03:30:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/ai-platform/prediction/docs/ai-explanations/overview\nAI Explanations helps you understand your model's outputs for classification and regression tasks. Whenever you request a prediction on AI Platform, AI Explanations tells you how much each feature in the data contributed to the predicted result. You can then use this information to verify that the model is behaving as expected, recognize bias in your models, and get ideas for ways to improve your model and your training data."
      },
      {
        "date": "2023-05-04T22:12:00.000Z",
        "voteCount": 2,
        "content": "preemtible vm are valid for 24hrs. Hence training needs months to complete which is mentioned in question that makes A is answer."
      },
      {
        "date": "2023-03-17T10:53:00.000Z",
        "voteCount": 1,
        "content": "Additionally, AI Platform's autoscaling feature can automatically adjust the number of resources used based on the workload, further optimizing costs."
      },
      {
        "date": "2023-03-17T10:53:00.000Z",
        "voteCount": 1,
        "content": "I think it\u2019s a.\nBy using distributed training jobs with checkpoints, you can train your models on multiple GPUs simultaneously, which reduces the training time. Checkpoints allow you to save the progress of your training jobs regularly, so if the training job gets interrupted or fails, you can restart it from the last checkpoint instead of starting from scratch. This saves time and resources, which reduces costs. Additionally, AI Platform's autoscaling feature can automatically adjust the number of resources used based on the workload, further optimizing costs."
      },
      {
        "date": "2023-01-25T06:15:00.000Z",
        "voteCount": 1,
        "content": "C is out of date ? AI Platform is Vertex-AI ,so , this is a simple scenario that would accommodate infrastructure for this case."
      },
      {
        "date": "2023-01-04T07:07:00.000Z",
        "voteCount": 2,
        "content": "It's A."
      },
      {
        "date": "2022-12-16T03:28:00.000Z",
        "voteCount": 4,
        "content": "It's seem C\n- https://www.kubeflow.org/docs/distributions/gke/pipelines/preemptible/\n- https://cloud.google.com/optimization/docs/guide/checkpointing"
      },
      {
        "date": "2022-12-11T08:03:00.000Z",
        "voteCount": 3,
        "content": "\"A Preemptible VM (PVM) is a Google Compute Engine (GCE) virtual machine (VM) instance that can be purchased for a steep discount as long as the customer accepts that the instance will terminate after 24 hours.\"\nThis excludes C and D. Checkpoints are needed for long processing, so A."
      },
      {
        "date": "2022-12-10T12:53:00.000Z",
        "voteCount": 3,
        "content": "C -  Reduce cost with preemptive instances and add checkpoints to snapshot intermediate results"
      },
      {
        "date": "2022-12-10T08:38:00.000Z",
        "voteCount": 2,
        "content": "Saving checkpoints avoids re-run from scratch"
      },
      {
        "date": "2022-12-08T19:52:00.000Z",
        "voteCount": 1,
        "content": "I think it should be A\nhttps://cloud.google.com/ai-platform/training/docs/overview"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 71,
    "url": "https://www.examtopics.com/discussions/google/view/90752-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You need to train a regression model based on a dataset containing 50,000 records that is stored in BigQuery. The data includes a total of 20 categorical and numerical features with a target variable that can include negative values. You need to minimize effort and training time while maximizing model performance. What approach should you take to train this regression model?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a custom TensorFlow DNN model",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse BQML XGBoost regression to train the model.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AutoML Tables to train the model without early stopping.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AutoML Tables to train the model with RMSLE as the optimization objective."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 12,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-04-14T06:46:00.000Z",
        "voteCount": 1,
        "content": "Went with B"
      },
      {
        "date": "2023-05-08T23:15:00.000Z",
        "voteCount": 1,
        "content": "Went with B"
      },
      {
        "date": "2023-02-11T05:04:00.000Z",
        "voteCount": 4,
        "content": "Ans B.\nC --&gt; No early stopping means longer training time\nD --&gt; RMSLE metric need non-negative Y values"
      },
      {
        "date": "2023-01-27T06:19:00.000Z",
        "voteCount": 1,
        "content": "B and C is the most likely because of regression approach, But  RMSLE it not allow you to take negative label to train  as https://cloud.google.com/automl-tables/docs/evaluate#evaluation_metrics_for_regression_models \n\nRMSLE: The root-mean-squared logarithmic error metric is similar to RMSE, except that it uses the natural logarithm of the predicted and actual values plus 1. RMSLE penalizes under-prediction more heavily than over-prediction. It can also be a good metric when you don't want to penalize differences for large prediction values more heavily than for small prediction values. This metric ranges from zero to infinity; a lower value indicates a higher quality model.\n The RMSLE evaluation metric is returned only if all label and predicted values are non-negative."
      },
      {
        "date": "2023-01-25T07:08:00.000Z",
        "voteCount": 2,
        "content": "BQML XGBoost ==&gt; you have to take sql knowlege to write statement and  B didn't mention how to get mx performance.    Meanwhile \nAutoML you just click and select, click and select, click and select to get it done.  and D refers to measurement to get maximizing model performance.  you can minimize effort literally"
      },
      {
        "date": "2023-01-27T06:20:00.000Z",
        "voteCount": 2,
        "content": "To john pongthorn , You are wrong  55555\nit must be B genuinely"
      },
      {
        "date": "2023-01-05T03:21:00.000Z",
        "voteCount": 2,
        "content": "I recommend option D, Use AutoML Tables to train the model with RMSLE as the optimization objective.\n\nUsing AutoML Tables to train the model can be a convenient and efficient way to minimize effort and training time while still maximizing model performance. In this case, using RMSLE as the optimization objective can be a good choice because it is a good fit for regression models with negative values in the target variable."
      },
      {
        "date": "2022-12-21T17:00:00.000Z",
        "voteCount": 3,
        "content": "B is correct"
      },
      {
        "date": "2022-12-16T13:20:00.000Z",
        "voteCount": 1,
        "content": "Its seen B for me"
      },
      {
        "date": "2022-12-11T14:13:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2022-12-11T08:04:00.000Z",
        "voteCount": 1,
        "content": "It's B."
      },
      {
        "date": "2022-12-08T19:56:00.000Z",
        "voteCount": 2,
        "content": "B. BigQuery is a keyword for me"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 72,
    "url": "https://www.examtopics.com/discussions/google/view/91026-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You are building a linear model with over 100 input features, all with values between \u20131 and 1. You suspect that many features are non-informative. You want to remove the non-informative features from your model while keeping the informative ones in their original form. Which technique should you use?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse principal component analysis (PCA) to eliminate the least informative features.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse L1 regularization to reduce the coefficients of uninformative features to 0.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAfter building your model, use Shapley values to determine which features are the most informative.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an iterative dropout technique to identify which features do not degrade the model when removed."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-18T03:35:00.000Z",
        "voteCount": 7,
        "content": "L1 regularization it's good for feature selection\nhttps://www.quora.com/How-does-the-L1-regularization-method-help-in-feature-selection\nhttps://developers.google.com/machine-learning/crash-course/regularization-for-sparsity/l1-regularization"
      },
      {
        "date": "2023-02-22T13:57:00.000Z",
        "voteCount": 1,
        "content": "but this is not a sparse input vector, just a high dimensional vector where many features are not relevant."
      },
      {
        "date": "2022-12-11T08:15:00.000Z",
        "voteCount": 5,
        "content": "A. PCA reconfigures the features, so no. \nC. After building your model, so no.\nD. Dropout should be in the model and it doesn't tell us which features are informative or not. Big No!\nFor me, it's B."
      },
      {
        "date": "2024-06-07T02:17:00.000Z",
        "voteCount": 1,
        "content": "B) L1 Regularisation"
      },
      {
        "date": "2023-07-07T08:07:00.000Z",
        "voteCount": 1,
        "content": "Went with B"
      },
      {
        "date": "2023-05-08T23:15:00.000Z",
        "voteCount": 1,
        "content": "Went with B"
      },
      {
        "date": "2023-03-30T14:14:00.000Z",
        "voteCount": 1,
        "content": "L1 regularization penalises weights in proportion to the sum of the absolute value of the weights. L1 regularization helps drive the weights of irrelevant or barely relevant features to exactly 0. A feature with a weight of 0 is effectively removed from the model. https://developers.google.com/machine-learning/glossary#L1_regularization"
      },
      {
        "date": "2023-03-17T11:03:00.000Z",
        "voteCount": 1,
        "content": "Its B. See my explanations under the comments why its not C."
      },
      {
        "date": "2023-02-08T19:43:00.000Z",
        "voteCount": 1,
        "content": "it's a best way, becouse you reduce features non relevant in this case non-informatives"
      },
      {
        "date": "2023-01-26T07:45:00.000Z",
        "voteCount": 3,
        "content": "Answer C:\nIn the official sample questions, there's a similar question, the explanation is that L! is for reducing overfitting while explainability (shapely) is for feature selection, hence C.\nhttps://docs.google.com/forms/d/e/1FAIpQLSeYmkCANE81qSBqLW0g2X7RoskBX9yGYQu-m1TtsjMvHabGqg/viewform"
      },
      {
        "date": "2023-03-17T11:00:00.000Z",
        "voteCount": 1,
        "content": "Its wrong. Using Shapley values to determine feature importance can be a useful technique, but it requires building a complete model and can be computationally expensive, especially with over 100 input features. Additionally, it may not be practical to use this method for every model iteration or update. On the other hand, L1 regularization can be used during the model building process to effectively reduce the impact of non-informative features by shrinking their coefficients to 0, making it a more efficient and effective approach."
      },
      {
        "date": "2023-01-26T07:46:00.000Z",
        "voteCount": 1,
        "content": "It cannot be A either because PCA modifies the features, and it says you should keep them in their original form.\nand D cannot be because again dropout is for generalizing and avoiding overfitting, and it's done on the NN model not on the data."
      },
      {
        "date": "2023-01-06T13:45:00.000Z",
        "voteCount": 2,
        "content": "The features must be removed from the model. They are not removed when doing L1 regularization. PCA is used prior to training."
      },
      {
        "date": "2023-01-12T18:29:00.000Z",
        "voteCount": 3,
        "content": "should be A\nas keeping the informative ones in their original form"
      },
      {
        "date": "2023-09-26T07:28:00.000Z",
        "voteCount": 1,
        "content": "How PCA can keep the original form?"
      },
      {
        "date": "2023-03-17T11:01:00.000Z",
        "voteCount": 1,
        "content": "That is a good point. PCA is a technique used to reduce the dimensionality of the dataset by transforming the original features into a new set of uncorrelated features. This can help to eliminate the least informative features and reduce the computational burden of building a model with many input features. However, it is important to note that PCA does not necessarily remove the original features from the model, but rather transforms them into a new set of features. On the other hand, L1 regularization can effectively remove the impact of non-informative features by setting their coefficients to 0 during the model building process. Therefore, both techniques can be useful for addressing the issue of non-informative features in a linear model, depending on the specific needs of the problem."
      },
      {
        "date": "2022-12-13T06:47:00.000Z",
        "voteCount": 2,
        "content": "Agree with B"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 73,
    "url": "https://www.examtopics.com/discussions/google/view/91004-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You work for a global footwear retailer and need to predict when an item will be out of stock based on historical inventory data Customer behavior is highly dynamic since footwear demand is influenced by many different factors. You want to serve models that are trained on all available data, but track your performance on specific subsets of data before pushing to production. What is the most streamlined and reliable way to perform this validation?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse then TFX ModelValidator tools to specify performance metrics for production readiness.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse k-fold cross-validation as a validation strategy to ensure that your model is ready for production.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the last relevant week of data as a validation set to ensure that your model is performing accurately on current data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the entire dataset and treat the area under the receiver operating characteristics curve (AUC ROC) as the main metric."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 26,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 21,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-01-25T08:01:00.000Z",
        "voteCount": 13,
        "content": "https://www.tensorflow.org/tfx/guide/evaluator"
      },
      {
        "date": "2022-12-18T03:43:00.000Z",
        "voteCount": 9,
        "content": "it's seem C for me\nB is wrong cuz \"Many machine learning techniques don\u2019t work well here due to the sequential nature and temporal correlation of time series. For example, k-fold cross validation can cause data leakage; models need to be retrained to generate new forecasts\" \n- https://cloud.google.com/learn/what-is-time-series"
      },
      {
        "date": "2024-06-07T02:20:00.000Z",
        "voteCount": 1,
        "content": "A) TFX ModelValidator is designed to handle the exact needs described in the scenario: training on all data, validating on specific subsets, and ensuring production readiness with comprehensive performance metrics.\nThis makes it the most streamlined and reliable method compared to other options, which either lack specificity in production readiness (B), are too narrow in scope (C), or risk overfitting and inadequate validation (D)."
      },
      {
        "date": "2024-04-20T02:11:00.000Z",
        "voteCount": 2,
        "content": "Evaluator TFX lets you evaluate the performance on different subsets of data https://www.tensorflow.org/tfx/guide/evaluator"
      },
      {
        "date": "2024-04-14T12:26:00.000Z",
        "voteCount": 2,
        "content": "The Evaluator TFX pipeline component performs deep analysis on the training results for your models, to help you understand how your model performs on subsets of your data."
      },
      {
        "date": "2024-03-05T08:46:00.000Z",
        "voteCount": 3,
        "content": "I prefer A to C because 1 week of data may be insufficient to generalize the model and could lead to overfitting on the validation subset."
      },
      {
        "date": "2024-02-28T08:12:00.000Z",
        "voteCount": 1,
        "content": "option C provides a streamlined and reliable approach that focuses on evaluating the model's performance on the most relevant and recent data, which is essential for predicting out-of-stock events in a dynamic retail setting."
      },
      {
        "date": "2023-11-15T23:34:00.000Z",
        "voteCount": 1,
        "content": "Either A or C but C is only last week which is not specific data sets"
      },
      {
        "date": "2023-09-22T05:51:00.000Z",
        "voteCount": 1,
        "content": "Answer should be C, we are dealing with dynamic data and the \"last\" data is more relevant to have an idea about the future performance"
      },
      {
        "date": "2023-09-19T12:53:00.000Z",
        "voteCount": 1,
        "content": "Option C,  because it allows you to track your model's performance on the most *recent* data, which is the most relevant data for predicting stockout risk. Given that the preferences are dynamic, the most important thing is that the model WORKS correctly with the newest data"
      },
      {
        "date": "2023-07-23T03:44:00.000Z",
        "voteCount": 1,
        "content": "The answer is A. Performance on specific subsets of data before pushing to production == TFX ModelValidator with custom performance metrics for production readiness.\n\nC is wrong because performance in the last relevant week of data != performance on specific subsets of data."
      },
      {
        "date": "2023-08-03T12:49:00.000Z",
        "voteCount": 2,
        "content": "The ModelValidator TFX Pipeline Component (Deprecated)"
      },
      {
        "date": "2023-07-19T07:42:00.000Z",
        "voteCount": 2,
        "content": "I will go for A. I don't think the aim of the question is to test if the candidates know whether or not a component is deprecated . Note that ModelValidator has been fused with Evaluator. So we can imagine, the question would have been updated in recent exams. Evaluator enables testing on specific subsets with the metrics we want, then indicates to Pusher component to push the new model to production if \"model is good enough\". This would make the pipeline quite streamlined (https://www.tensorflow.org/tfx/guide/evaluator)\n\nB: wrong: using historical data, one should watch data leakage\nC: wrong: We want to track performance on specific subsets of data (not necessarily the last week) maybe to do some targeting/segmentation ? who knows. \nD: wrong because we want to track performance on specific subsets of data not the entire dataset"
      },
      {
        "date": "2023-07-23T05:52:00.000Z",
        "voteCount": 1,
        "content": "Bro, thats not TFXModelValidator its Evaluator, are both the same?"
      },
      {
        "date": "2023-10-25T15:29:00.000Z",
        "voteCount": 1,
        "content": "TFXModelValidator is deprecated, but its behaviour can be replicated using the Evaluator object - which is the point he tried to make. See the docs here: https://www.tensorflow.org/tfx/guide/modelval"
      },
      {
        "date": "2023-07-07T08:08:00.000Z",
        "voteCount": 1,
        "content": "Went with C"
      },
      {
        "date": "2023-06-08T04:12:00.000Z",
        "voteCount": 1,
        "content": "I think that it should be C for the following key point\n\", but track your performance on specific subsets of data before pushing to production\"\nSo the ask is which subset of data you should use."
      },
      {
        "date": "2023-06-01T18:32:00.000Z",
        "voteCount": 1,
        "content": "Could someone explain why A is better option than C? C is correct one in terms of evaluation overall, no doubt. But do we choose TFX because it understands we are dealing with time series? Or is it the \"specific subset\" in the Q that makes us thinking we have already chosen the data of last period and just need to push it into the TFX?"
      },
      {
        "date": "2023-05-20T03:26:00.000Z",
        "voteCount": 1,
        "content": "A is deprecated.. so C"
      },
      {
        "date": "2023-05-08T23:16:00.000Z",
        "voteCount": 2,
        "content": "Went with A"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 74,
    "url": "https://www.examtopics.com/discussions/google/view/90994-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You have deployed a model on Vertex AI for real-time inference. During an online prediction request, you get an \u201cOut of Memory\u201d error. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse batch prediction mode instead of online mode.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSend the request again with a smaller batch of instances.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse base64 to encode your data before using it for prediction.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tApply for a quota increase for the number of prediction requests."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 29,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-18T03:54:00.000Z",
        "voteCount": 22,
        "content": "B is the answer\n429 - Out of Memory\nhttps://cloud.google.com/ai-platform/training/docs/troubleshooting"
      },
      {
        "date": "2023-03-20T09:55:00.000Z",
        "voteCount": 3,
        "content": "Upvote this comment, its the right answer!"
      },
      {
        "date": "2024-06-07T02:51:00.000Z",
        "voteCount": 1,
        "content": "B) Use smaller set of tokens"
      },
      {
        "date": "2024-02-28T08:14:00.000Z",
        "voteCount": 1,
        "content": "By reducing the batch size of instances sent for prediction, you decrease the memory footprint of each request, potentially alleviating the out-of-memory issue. However, be mindful that excessively reducing the batch size might impact the efficiency of your prediction process."
      },
      {
        "date": "2023-05-08T23:16:00.000Z",
        "voteCount": 1,
        "content": "Went with B"
      },
      {
        "date": "2023-03-20T09:57:00.000Z",
        "voteCount": 2,
        "content": "B. Send the request again with a smaller batch of instances.\n\nIf you are getting an \"Out of Memory\" error during an online prediction request, it suggests that the amount of data you are sending in each request is too large and is exceeding the available memory. To resolve this issue, you can try sending the request again with a smaller batch of instances. This reduces the amount of data being sent in each request and helps avoid the out-of-memory error. If the problem persists, you can also try increasing the machine type or the number of instances to provide more resources for the prediction service."
      },
      {
        "date": "2023-02-28T03:17:00.000Z",
        "voteCount": 1,
        "content": "This question is about prediction not training - and specifically it's about _online_ prediction (aka realtime serving).\n\nAll the answers are about batch workloads apart from C."
      },
      {
        "date": "2023-02-28T03:21:00.000Z",
        "voteCount": 1,
        "content": "Okay, option D is also about online serving, but the error message indicates a problem for individual predictions, which will not be fixed by increasing the number of predictions per second."
      },
      {
        "date": "2023-03-30T14:32:00.000Z",
        "voteCount": 1,
        "content": "@BenMS this feels like a trick question.... makes on to zone to the word batch. https://cloud.google.com/ai-platform/training/docs/troubleshooting .... states then when an error occurs with an online prediction request, you usually get an HTTP status code back from the service. These are some commonly encountered codes and their meaning in the context of online prediction:\n\n429 - Out of Memory\nThe processing node ran out of memory while running your model. There is no way to increase the memory allocated to prediction nodes at this time. You can try these things to get your model to run:\n\nReduce your model size by:\n1. Using less precise variables.\n2. Quantizing your continuous data.\n3. Reducing the size of other input features (using smaller vocab sizes, for example).\n4. Send the request again with a smaller batch of instances."
      },
      {
        "date": "2022-12-29T13:23:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/ai-platform/training/docs/troubleshooting"
      },
      {
        "date": "2022-12-11T08:38:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is B."
      },
      {
        "date": "2022-12-11T05:22:00.000Z",
        "voteCount": 1,
        "content": "answer B as reported here: https://cloud.google.com/ai-platform/training/docs/troubleshooting"
      },
      {
        "date": "2022-12-11T02:05:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/ai-platform/training/docs/troubleshooting#http_status_codes"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 75,
    "url": "https://www.examtopics.com/discussions/google/view/91029-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You work at a subscription-based company. You have trained an ensemble of trees and neural networks to predict customer churn, which is the likelihood that customers will not renew their yearly subscription. The average prediction is a 15% churn rate, but for a particular customer the model predicts that they are 70% likely to churn. The customer has a product usage history of 30%, is located in New York City, and became a customer in 1997. You need to explain the difference between the actual prediction, a 70% churn rate, and the average prediction. You want to use Vertex Explainable AI. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTrain local surrogate models to explain individual predictions.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure sampled Shapley explanations on Vertex Explainable AI.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure integrated gradients explanations on Vertex Explainable AI.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMeasure the effect of each feature as the weight of the feature multiplied by the feature value."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 21,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-06-07T02:55:00.000Z",
        "voteCount": 1,
        "content": "B) Shapley"
      },
      {
        "date": "2024-02-28T08:21:00.000Z",
        "voteCount": 3,
        "content": "Sampled Shapley explanations offer a more sophisticated and model-agnostic method for understanding feature importance and contributions to predictions."
      },
      {
        "date": "2023-11-07T04:50:00.000Z",
        "voteCount": 1,
        "content": "I agree, it seems like B"
      },
      {
        "date": "2023-06-25T07:53:00.000Z",
        "voteCount": 2,
        "content": "B\nrefer: \nhttps://cloud.google.com/vertex-ai/docs/explainable-ai/overview#compare-methods"
      },
      {
        "date": "2023-05-08T23:16:00.000Z",
        "voteCount": 2,
        "content": "Went with B"
      },
      {
        "date": "2023-05-08T22:33:00.000Z",
        "voteCount": 2,
        "content": "Assigns credit for the outcome to each feature, and considers different permutations of the features. This method provides a sampling approximation of exact Shapley values.\nshampled shapely recommended Model Type: Non-differentiable models, such as ensembles of trees and neural networks.\nhttps://cloud.google.com/ai-platform/prediction/docs/ai-explanations/overview"
      },
      {
        "date": "2023-02-08T19:50:00.000Z",
        "voteCount": 2,
        "content": "Sampled Shapley works well for these models, which are meta-ensembles of trees and neural networks.\nhttps://cloud.google.com/vertex-ai/docs/explainable-ai/overview#sampled-shapley"
      },
      {
        "date": "2023-01-26T01:58:00.000Z",
        "voteCount": 2,
        "content": "B is optimal for tabular data Tree or DNN\n\nC integrated gradients explanations on Vertex Explainable AI.\nIt is used for image."
      },
      {
        "date": "2023-01-26T01:59:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/vertex-ai/docs/explainable-ai/overview#compare-methods"
      },
      {
        "date": "2023-01-04T06:53:00.000Z",
        "voteCount": 1,
        "content": "It should be B."
      },
      {
        "date": "2022-12-28T08:15:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/vertex-ai/docs/explainable-ai/overview#sampled-shapley"
      },
      {
        "date": "2022-12-23T23:04:00.000Z",
        "voteCount": 1,
        "content": "B - For sure as per GCP Docs here: https://cloud.google.com/vertex-ai/docs/explainable-ai/overview"
      },
      {
        "date": "2022-12-18T04:06:00.000Z",
        "voteCount": 2,
        "content": "B\n- https://christophm.github.io/interpretable-ml-book/shapley.html\n- https://cloud.google.com/vertex-ai/docs/explainable-ai/overview"
      },
      {
        "date": "2022-12-13T07:32:00.000Z",
        "voteCount": 3,
        "content": "Agree with B :  individual instance prediction + ensemble of trees and neural networks (recommended model types for Sampled Shapley : \"Non-differentiable models, such as ensembles of trees and neural networks \" ). Check out the link below :\nhttps://cloud.google.com/vertex-ai/docs/explainable-ai/overview"
      },
      {
        "date": "2022-12-12T20:39:00.000Z",
        "voteCount": 2,
        "content": "it is about a individual instance prediction. I think use integrated gradient method"
      },
      {
        "date": "2022-12-11T08:42:00.000Z",
        "voteCount": 1,
        "content": "It seems D."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 76,
    "url": "https://www.examtopics.com/discussions/google/view/91030-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You are working on a classification problem with time series data. After conducting just a few experiments using random cross-validation, you achieved an Area Under the Receiver Operating Characteristic Curve (AUC ROC) value of 99% on the training data. You haven\u2019t explored using any sophisticated algorithms or spent any time on hyperparameter tuning. What should your next step be to identify and fix the problem?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAddress the model overfitting by using a less complex algorithm and use k-fold cross-validation.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAddress data leakage by applying nested cross-validation during model training.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAddress data leakage by removing features highly correlated with the target value.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAddress the model overfitting by tuning the hyperparameters to reduce the AUC ROC value."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 15,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-04-21T11:10:00.000Z",
        "voteCount": 2,
        "content": "random cross-validation \ntime series data\n\n-&gt; B"
      },
      {
        "date": "2024-04-14T06:53:00.000Z",
        "voteCount": 2,
        "content": "B with nested cross validation."
      },
      {
        "date": "2024-04-20T03:10:00.000Z",
        "voteCount": 1,
        "content": "can you explain me why?"
      },
      {
        "date": "2024-02-28T23:58:00.000Z",
        "voteCount": 3,
        "content": "\"99% on training data\" -&gt; Data leakage\n\"random cross-validation\" -&gt; Not suitable for time series, use \"nested cross-validation\""
      },
      {
        "date": "2024-02-28T09:01:00.000Z",
        "voteCount": 1,
        "content": "Options B and C (Address data leakage by applying nested cross-validation during model training; Address data leakage by removing features highly correlated with the target value) are less relevant in this scenario because the primary concern appears to be overfitting rather than data leakage. Data leakage typically involves inadvertent inclusion of information from the test set in the training process, which may lead to overly optimistic performance metrics. However, there is no indication that data leakage is the cause of the high AUC ROC value in this case."
      },
      {
        "date": "2023-11-14T07:36:00.000Z",
        "voteCount": 3,
        "content": "Options A and B also address overfitting, but they involve different strategies. Option A suggests using a less complex algorithm and k-fold cross-validation. While this can be effective, it might be premature to change the algorithm without first exploring hyperparameter tuning. Option B suggests addressing data leakage, which is a different issue and may not be the primary cause of overfitting in this scenario."
      },
      {
        "date": "2023-09-28T04:04:00.000Z",
        "voteCount": 1,
        "content": "B with nested cross validation."
      },
      {
        "date": "2023-05-08T23:17:00.000Z",
        "voteCount": 2,
        "content": "Went with B"
      },
      {
        "date": "2023-02-28T03:34:00.000Z",
        "voteCount": 1,
        "content": "Nested cross-validation to reduce data leakage - same as a previous question."
      },
      {
        "date": "2023-02-27T06:47:00.000Z",
        "voteCount": 1,
        "content": "It`s B"
      },
      {
        "date": "2022-12-18T04:10:00.000Z",
        "voteCount": 3,
        "content": "B (same question 48)\n- https://towardsdatascience.com/time-series-nested-cross-validation-76adba623eb9"
      },
      {
        "date": "2022-12-11T08:45:00.000Z",
        "voteCount": 1,
        "content": "To say overfitting, I should have results on testing data, so it's data leakage. Common sense excludes C, so it's B."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 77,
    "url": "https://www.examtopics.com/discussions/google/view/91031-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You need to execute a batch prediction on 100 million records in a BigQuery table with a custom TensorFlow DNN regressor model, and then store the predicted results in a BigQuery table. You want to minimize the effort required to build this inference pipeline. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImport the TensorFlow model with BigQuery ML, and run the ml.predict function.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the TensorFlow BigQuery reader to load the data, and use the BigQuery API to write the results to BigQuery.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Dataflow pipeline to convert the data in BigQuery to TFRecords. Run a batch inference on Vertex AI Prediction, and write the results to BigQuery.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLoad the TensorFlow SavedModel in a Dataflow pipeline. Use the BigQuery I/O connector with a custom function to perform the inference within the pipeline, and write the results to BigQuery."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 17,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-18T04:21:00.000Z",
        "voteCount": 10,
        "content": "A should work with less effort\n- https://cloud.google.com/bigquery-ml/docs/making-predictions-with-imported-tensorflow-models#api\n- https://towardsdatascience.com/how-to-do-batch-predictions-of-tensorflow-models-directly-in-bigquery-ffa843ebdba6"
      },
      {
        "date": "2024-06-07T19:26:00.000Z",
        "voteCount": 1,
        "content": "BigQuery ML might not support custom TensorFlow DNN models directly."
      },
      {
        "date": "2024-03-02T10:26:00.000Z",
        "voteCount": 1,
        "content": "Went with C"
      },
      {
        "date": "2024-01-09T06:26:00.000Z",
        "voteCount": 2,
        "content": "Simplest doesn't mean it is the most effecient/optimal. If I follow the Best practices offered by Google for Serving / Inference Pipeline I would go with Vertex AI predictions. Read More for correct details : https://cloud.google.com/architecture/ml-on-gcp-best-practices#machine-learning-development"
      },
      {
        "date": "2024-03-02T10:26:00.000Z",
        "voteCount": 1,
        "content": "Agreed, i'll also go with C."
      },
      {
        "date": "2023-05-08T23:17:00.000Z",
        "voteCount": 2,
        "content": "Went with A"
      },
      {
        "date": "2023-03-28T03:53:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/bigquery-ml/docs/making-predictions-with-imported-tensorflow-models"
      },
      {
        "date": "2023-02-08T19:59:00.000Z",
        "voteCount": 2,
        "content": "for this:\nhttps://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-inference-overview\nPredict the label, either a numerical value for regression tasks or a categorical value for classification tasks on DNN regresion"
      },
      {
        "date": "2022-12-11T10:51:00.000Z",
        "voteCount": 1,
        "content": "ml.predict: https://cloud.google.com/bigquery-ml/docs/making-predictions-with-imported-tensorflow-models#api --&gt; A"
      },
      {
        "date": "2022-12-11T08:54:00.000Z",
        "voteCount": 1,
        "content": "Answer A as the simplest"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 78,
    "url": "https://www.examtopics.com/discussions/google/view/91033-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You are creating a deep neural network classification model using a dataset with categorical input values. Certain columns have a cardinality greater than 10,000 unique values. How should you encode these categorical values as input into the model?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConvert each categorical value into an integer value.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConvert the categorical string data to one-hot hash buckets.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMap the categorical variables into a vector of boolean values.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConvert each categorical value into a run-length encoded string."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 17,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 4,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-06-07T03:25:00.000Z",
        "voteCount": 1,
        "content": "B) Hash buckets"
      },
      {
        "date": "2024-03-02T10:29:00.000Z",
        "voteCount": 1,
        "content": "went with A"
      },
      {
        "date": "2023-05-08T23:17:00.000Z",
        "voteCount": 2,
        "content": "Went with B"
      },
      {
        "date": "2023-05-08T04:20:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/ai-platform/training/docs/algorithms/wide-and-deep\nIf the column is categorical with high cardinality, then the column is treated with hashing, where the number of hash buckets equals to the square root of the number of unique values in the column."
      },
      {
        "date": "2023-03-28T03:56:00.000Z",
        "voteCount": 1,
        "content": "B.\nThe other options solves nada."
      },
      {
        "date": "2023-02-08T20:12:00.000Z",
        "voteCount": 1,
        "content": "https://towardsdatascience.com/getting-deeper-into-categorical-encodings-for-machine-learning-2312acd347c8\nWhen you have  millions uniques values try to do: Hash Encoding"
      },
      {
        "date": "2023-01-26T07:28:00.000Z",
        "voteCount": 2,
        "content": "B unconditoinally\nhttps://cloud.google.com/ai-platform/training/docs/algorithms/xgboost#analysis\n\nIf the column is categorical with high cardinality, then the column is treated with hashing, where the number of hash buckets equals to the square root of the number of unique values in the column.\nA categorical column is considered to have high cardinality if the number of unique values is greater than the square root of the number of rows in the dataset."
      },
      {
        "date": "2022-12-21T17:09:00.000Z",
        "voteCount": 2,
        "content": "I think C as it has 10000 categorical values"
      },
      {
        "date": "2022-12-18T04:43:00.000Z",
        "voteCount": 4,
        "content": "I think B is correct\nRef.:\"\n- https://cloud.google.com/ai-platform/training/docs/algorithms/xgboost\n- https://stackoverflow.com/questions/26473233/in-preprocessing-data-with-high-cardinality-do-you-hash-first-or-one-hot-encode"
      },
      {
        "date": "2022-12-23T13:21:00.000Z",
        "voteCount": 1,
        "content": "- https://cloud.google.com/ai-platform/training/docs/algorithms/xgboost#analysis"
      },
      {
        "date": "2022-12-17T10:18:00.000Z",
        "voteCount": 1,
        "content": "Answer is B. When cardinality of the categorical column is very large best choice is binary encoding however it not here hence one-hot hash option."
      },
      {
        "date": "2022-12-17T10:23:00.000Z",
        "voteCount": 1,
        "content": "https://www.analyticsvidhya.com/blog/2020/08/types-of-categorical-data-encoding/"
      },
      {
        "date": "2022-12-13T07:44:00.000Z",
        "voteCount": 1,
        "content": "Ans : B"
      },
      {
        "date": "2022-12-11T15:45:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2022-12-11T10:54:00.000Z",
        "voteCount": 1,
        "content": "It should be B"
      },
      {
        "date": "2022-12-11T09:00:00.000Z",
        "voteCount": 3,
        "content": "Answer A since with 10.000 unique values one-hot shouldn't be a good solution\nhttps://machinelearningmastery.com/how-to-prepare-categorical-data-for-deep-learning-in-python/"
      },
      {
        "date": "2024-03-02T10:28:00.000Z",
        "voteCount": 1,
        "content": "I agree with A"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 79,
    "url": "https://www.examtopics.com/discussions/google/view/91035-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You need to train a natural language model to perform text classification on product descriptions that contain millions of examples and 100,000 unique words. You want to preprocess the words individually so that they can be fed into a recurrent neural network. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a hot-encoding of words, and feed the encodings into your model.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIdentify word embeddings from a pre-trained model, and use the embeddings in your model.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSort the words by frequency of occurrence, and use the frequencies as the encodings in your model.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAssign a numerical value to each word from 1 to 100,000 and feed the values as inputs in your model."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 6,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-05-08T23:17:00.000Z",
        "voteCount": 1,
        "content": "Went with B"
      },
      {
        "date": "2023-01-26T08:14:00.000Z",
        "voteCount": 2,
        "content": "B\nhttps://developers.google.com/machine-learning/guides/text-classification/step-3\nhttps://developers.google.com/machine-learning/guides/text-classification/step-4\n\ni"
      },
      {
        "date": "2023-01-04T06:20:00.000Z",
        "voteCount": 1,
        "content": "Answer is B"
      },
      {
        "date": "2022-12-23T23:28:00.000Z",
        "voteCount": 4,
        "content": "Answer is B: According to Google Docs here: - https://developers.google.com/machine-learning/guides/text-classification/ it is a Word Embedding case"
      },
      {
        "date": "2022-12-18T05:30:00.000Z",
        "voteCount": 2,
        "content": "B (I'm not sure)\n- https://developers.google.com/machine-learning/guides/text-classification/step-3#label_vectorization\n- https://developers.google.com/machine-learning/guides/text-classification/step-4\n- https://towardsai.net/p/deep-learning/text-classification-with-rnn\n - https://towardsdatascience.com/pre-trained-word-embedding-for-text-classification-end2end-approach-5fbf5cd8aead"
      },
      {
        "date": "2022-12-23T13:26:00.000Z",
        "voteCount": 1,
        "content": "- https://developers.google.com/machine-learning/crash-course/embeddings/translating-to-a-lower-dimensional-space"
      },
      {
        "date": "2022-12-11T09:03:00.000Z",
        "voteCount": 1,
        "content": "Bag of words is a good practice to represent and feed text at a DNN \nhttps://machinelearningmastery.com/gentle-introduction-bag-words-model/"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 80,
    "url": "https://www.examtopics.com/discussions/google/view/91558-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You work for an online travel agency that also sells advertising placements on its website to other companies. You have been asked to predict the most relevant web banner that a user should see next. Security is important to your company. The model latency requirements are 300ms@p99, the inventory is thousands of web banners, and your exploratory analysis has shown that navigation context is a good predictor. You want to Implement the simplest solution. How should you configure the prediction pipeline?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEmbed the client on the website, and then deploy the model on AI Platform Prediction.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEmbed the client on the website, deploy the gateway on App Engine, deploy the database on Firestore for writing and for reading the user\u2019s navigation context, and then deploy the model on AI Platform Prediction.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEmbed the client on the website, deploy the gateway on App Engine, deploy the database on Cloud Bigtable for writing and for reading the user\u2019s navigation context, and then deploy the model on AI Platform Prediction.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEmbed the client on the website, deploy the gateway on App Engine, deploy the database on Memorystore for writing and for reading the user\u2019s navigation context, and then deploy the model on Google Kubernetes Engine."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 16,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 12,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-18T05:35:00.000Z",
        "voteCount": 8,
        "content": "C (same question 49)\nkeywords\nthe inventory is thousands of web banners -&gt; Bigtable\nYou want to Implement the simplest solution -&gt; AI Platform Prediction"
      },
      {
        "date": "2023-08-03T12:42:00.000Z",
        "voteCount": 1,
        "content": "Yes, but in that question Option B doesnt have a database.Firestore can handle thousands of web banners, right?"
      },
      {
        "date": "2023-04-27T02:03:00.000Z",
        "voteCount": 6,
        "content": "Here are some of the reasons why C is not as simple as B:\n\nCloud Bigtable is a more complex database to set up and manage than Firestore.\nCloud Bigtable is not as secure as Firestore.\nCloud Bigtable is not as well-integrated with other Google Cloud services as Firestore.\nTherefore, B is the simpler solution that meets all of the requirements."
      },
      {
        "date": "2024-08-04T04:54:00.000Z",
        "voteCount": 1,
        "content": "go for B"
      },
      {
        "date": "2024-04-09T11:59:00.000Z",
        "voteCount": 1,
        "content": "see e707"
      },
      {
        "date": "2024-04-05T02:01:00.000Z",
        "voteCount": 1,
        "content": "as Hiromi said"
      },
      {
        "date": "2024-02-27T02:26:00.000Z",
        "voteCount": 1,
        "content": "I would opt for B as we have requirement of retrieval latency"
      },
      {
        "date": "2023-11-15T03:46:00.000Z",
        "voteCount": 1,
        "content": "Embed the client on the website, deploy the gateway on App Engine, and then deploy the model on AI Platform Prediction."
      },
      {
        "date": "2023-11-11T12:07:00.000Z",
        "voteCount": 1,
        "content": "I would go with Firestore as throughput or latency requirement provided in the question are possible with Firestore and bigTable may be an overkill. Had the scenario involved super large volumes of data, CBT would have taken precedence"
      },
      {
        "date": "2023-09-10T02:20:00.000Z",
        "voteCount": 1,
        "content": "I think B, based on \"the simplest solution\" consideration."
      },
      {
        "date": "2023-08-03T12:38:00.000Z",
        "voteCount": 2,
        "content": "the primary requirement mentioned in the original question is to implement the simplest solution. Firestore is a fully managed, serverless NoSQL database that can also handle thousands of web banners and dynamically changing user browsing history. It is designed for real-time data synchronization and can quickly update the most relevant web banner as the user browses different pages of the website.\n\nWhile Cloud Bigtable offers high performance and scalability, it is more complex to manage and is better suited for large-scale, high-throughput workloads. Firestore, on the other hand, is easier to implement and maintain, making it a more suitable choice for the simplest solution in this scenario."
      },
      {
        "date": "2023-07-23T06:55:00.000Z",
        "voteCount": 2,
        "content": "The answer is C for the following reason:\n\nIf you need:\n- Submillisecond retrieval latency on a limited amount of quickly changing data, retrieved by a few thousand clients, use Memorystore.\n- Millisecond retrieval latency on slowly changing data where storage scales automatically, use Datastore.\n- Millisecond retrieval latency on dynamically changing data, using a store that can scale linearly with heavy reads and writes, use Bigtable.\nSource: https://cloud.google.com/architecture/minimizing-predictive-serving-latency-in-machine-learning#choosing_a_nosql_database\n\nC is better than B because 1) the inventory is thousands of web banners and 2) we expect the user to compare many travel destinations, dates, hotels, and tariffs during their search process. It means the user's browsing history is dynamically changing, and we need to identify \"the most relevant web banner that a user should see next\" =&gt; we will be dynamically changing the ad as the user browses different pages of the website."
      },
      {
        "date": "2023-09-10T02:24:00.000Z",
        "voteCount": 1,
        "content": "BTW, the storage solution does not mention web banners, just browsing history.\nbut what about the \"simplest solution\" consideration? that wold point into the Datastore direction.\nIt is true however that the guide you mention recommends firestore for \" slowly changing data \", which I wonder why? I expect Firectore to be able to perfectly handle many updates per second, few updates per user per second."
      },
      {
        "date": "2023-05-08T23:18:00.000Z",
        "voteCount": 1,
        "content": "Went with C"
      },
      {
        "date": "2023-04-26T02:17:00.000Z",
        "voteCount": 1,
        "content": "B for me"
      },
      {
        "date": "2023-01-04T06:16:00.000Z",
        "voteCount": 2,
        "content": "B, for me."
      },
      {
        "date": "2022-12-18T05:56:00.000Z",
        "voteCount": 3,
        "content": "I think C because of latency requirements. \nCloud BigTable has high latency feature from https://cloud.google.com/bigtable"
      },
      {
        "date": "2023-08-03T12:44:00.000Z",
        "voteCount": 1,
        "content": "correct that Cloud Bigtable can provide better latency compared to Firestore, especially when dealing with very large datasets and high-throughput workloads. However, it's important to consider the trade-offs and the specific use case.\n\nFor the given scenario, the latency requirements are 300ms@p99, which Firestore can handle effectively for thousands of web banners and dynamically changing user browsing history. Firestore is designed for real-time data synchronization and can quickly update the most relevant web banner as the user browses different pages on the website.\n\nWhile Cloud Bigtable can offer improved latency, it comes with added complexity in terms of management and configuration. If the primary goal is to implement the simplest solution while meeting the latency requirements, Firestore remains a more suitable choice for this use case."
      },
      {
        "date": "2022-12-14T02:24:00.000Z",
        "voteCount": 1,
        "content": "I need a DB to store the banners, so no A. We're talking of thousands of banners, so no C. Memorystore calls Redis, and other solutions, so no D. The answer is B, for me."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 81,
    "url": "https://www.examtopics.com/discussions/google/view/91038-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "Your data science team has requested a system that supports scheduled model retraining, Docker containers, and a service that supports autoscaling and monitoring for online prediction requests. Which platform components should you choose for this system?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tVertex AI Pipelines and App Engine",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tVertex AI Pipelines, Vertex AI Prediction, and Vertex AI Model Monitoring\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Composer, BigQuery ML, and Vertex AI Prediction",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Composer, Vertex AI Training with custom containers, and App Engine"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 17,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 4,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-01-26T23:47:00.000Z",
        "voteCount": 7,
        "content": "The Cloud Compose may be good consideration if you are involved in getting Google Data Engineer Cert\nApp enging is relevant to Dev-Op Cert\n\nPls.\nif you know a bit about ML Google Cloud, we are preparing to take Google ML  Cert, if there is no specifically particular requirement in the question.\nWe must emphasize on use of Vertext AI as much as possible."
      },
      {
        "date": "2024-06-07T03:55:00.000Z",
        "voteCount": 1,
        "content": "B) Vertex AI Pipelines"
      },
      {
        "date": "2023-05-28T02:04:00.000Z",
        "voteCount": 2,
        "content": "B. Vertext AI also supports Docker container\nhttps://cloud.google.com/vertex-ai/docs/training/containers-overview"
      },
      {
        "date": "2023-05-09T04:19:00.000Z",
        "voteCount": 2,
        "content": "A custom container is a Docker image that you create to run your training application. By running your machine learning (ML) training job in a custom container, you can use ML frameworks, non-ML dependencies, libraries, and binaries that are not otherwise supported on Vertex AI. so we need vertex ai custom container for docker container. Thus option A and B are omitted .\nApp Engine allows developers to focus on what they do best: writing code. Based on Compute Engine, the App Engine flexible environment automatically scales your app up and down while also balancing the load.\nCustomizable infrastructure - App Engine flexible environment instances are Compute Engine virtual machines, which means that you can take advantage of custom libraries, use SSH for debugging, and deploy your own Docker containers."
      },
      {
        "date": "2023-05-08T23:19:00.000Z",
        "voteCount": 2,
        "content": "Went with B"
      },
      {
        "date": "2023-04-27T02:10:00.000Z",
        "voteCount": 1,
        "content": "I think it's D. B does not support Docker containers, does it?"
      },
      {
        "date": "2023-05-10T02:44:00.000Z",
        "voteCount": 2,
        "content": "I can't change the voting but It's B."
      },
      {
        "date": "2023-04-22T17:23:00.000Z",
        "voteCount": 1,
        "content": "Shouldn't it be A?\nhttps://cloud.google.com/appengine/docs/standard/scheduling-jobs-with-cron-yaml"
      },
      {
        "date": "2023-01-06T14:42:00.000Z",
        "voteCount": 1,
        "content": "Vote for B"
      },
      {
        "date": "2022-12-18T05:47:00.000Z",
        "voteCount": 3,
        "content": "Vote for B"
      },
      {
        "date": "2022-12-17T07:18:00.000Z",
        "voteCount": 1,
        "content": "D is the only option that provides scheduled model retraining"
      },
      {
        "date": "2022-12-14T03:06:00.000Z",
        "voteCount": 1,
        "content": "Serve Vertex AI Prediction, but the monitoring in the question is not the one of the answer B. (that is connected to the modeol). The correct answer is C."
      },
      {
        "date": "2023-01-04T06:07:00.000Z",
        "voteCount": 1,
        "content": "I changed my mind. It's D."
      },
      {
        "date": "2022-12-11T09:20:00.000Z",
        "voteCount": 3,
        "content": "Everything is possible on Vetex AI"
      },
      {
        "date": "2022-12-20T03:16:00.000Z",
        "voteCount": 1,
        "content": "Scheduling is not possible without the Cloud Scheduler\nhttps://cloud.google.com/vertex-ai/docs/pipelines/schedule-cloud-scheduler"
      },
      {
        "date": "2022-12-23T13:59:00.000Z",
        "voteCount": 2,
        "content": "I think Vertex AI Pipeline includes schedule/trigger runs, so my vote is B"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 82,
    "url": "https://www.examtopics.com/discussions/google/view/91041-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You are profiling the performance of your TensorFlow model training time and notice a performance issue caused by inefficiencies in the input data pipeline for a single 5 terabyte CSV file dataset on Cloud Storage. You need to optimize the input pipeline performance. Which action should you try first to increase the efficiency of your pipeline?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPreprocess the input CSV file into a TFRecord file.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRandomly select a 10 gigabyte subset of the data to train your model.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSplit into multiple CSV files and use a parallel interleave transformation.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the reshuffle_each_iteration parameter to true in the tf.data.Dataset.shuffle method."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 9,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-07-01T21:00:00.000Z",
        "voteCount": 3,
        "content": "Preprocessing the input CSV file into a TFRecord file optimizes the input data pipeline by enabling more efficient reading and processing. TFRecord is a binary format that is faster to read and more efficient for TensorFlow to process compared to CSV, which is a text-based format. This change can significantly reduce the time spent on data input operations during model training."
      },
      {
        "date": "2024-06-07T04:00:00.000Z",
        "voteCount": 1,
        "content": "A) Convert CSV file into TFRecord is more effecient and processing CSV in parallel (C)"
      },
      {
        "date": "2024-04-20T03:14:00.000Z",
        "voteCount": 2,
        "content": "Converting a large 5 terabyte CSV file to a TFRecord can be a time-consuming process, and you would still be dealing with a single large file."
      },
      {
        "date": "2023-11-07T11:14:00.000Z",
        "voteCount": 1,
        "content": "While preprocessing the input CSV file into a TFRecord file (Option A) can improve the performance of your input pipeline, it is not the first action to try in this situation. Converting a large 5 terabyte CSV file to a TFRecord can be a time-consuming process, and you would still be dealing with a single large file."
      },
      {
        "date": "2023-09-10T02:27:00.000Z",
        "voteCount": 1,
        "content": "i think C based on the consideration: \"Which action should you try first \", meaning it should be less impactful to continue using CSV."
      },
      {
        "date": "2023-06-04T05:45:00.000Z",
        "voteCount": 2,
        "content": "https://www.tensorflow.org/guide/data_performance#best_practice_summary"
      },
      {
        "date": "2023-05-08T23:19:00.000Z",
        "voteCount": 1,
        "content": "Went with C"
      },
      {
        "date": "2023-04-27T02:13:00.000Z",
        "voteCount": 1,
        "content": "Option A, preprocess the input CSV file into a TFRecord file, is not as good because it requires additional processing time. Hence, I think C is the best choice."
      },
      {
        "date": "2023-04-25T00:47:00.000Z",
        "voteCount": 1,
        "content": "I think it could be A.\nhttps://cloud.google.com/architecture/best-practices-for-ml-performance-cost#preprocess_the_data_once_and_save_it_as_a_tfrecord_file"
      },
      {
        "date": "2023-04-20T05:50:00.000Z",
        "voteCount": 1,
        "content": "Clearly both A and C works here, but I can't find any documentation which suggests C is any better than A."
      },
      {
        "date": "2023-03-17T02:09:00.000Z",
        "voteCount": 2,
        "content": "\"Which action should you try first\" seems to be key -- C seems more intuitive as first step!\nA is valid as well (interleave works w TFRecords) &amp; definitely more efficient IMO, but maybe 2nd step!"
      },
      {
        "date": "2023-02-23T00:15:00.000Z",
        "voteCount": 3,
        "content": "Option B (randomly selecting a 10 gigabyte subset of the data) could lead to a loss of useful data and may not be representative of the entire dataset. Option C (splitting into multiple CSV files and using a parallel interleave transformation) may also improve the performance, but may be more complex to implement and maintain, and may not be as efficient as converting to TFRecord. Option D (setting the reshuffle_each_iteration parameter to true in the tf.data.Dataset.shuffle method) is not directly related to the input data format and may not provide as significant a performance improvement as converting to TFRecord."
      },
      {
        "date": "2023-03-22T11:54:00.000Z",
        "voteCount": 1,
        "content": "Please read this site https://www.tensorflow.org/tutorials/load_data/csv, its simple to implement in the same input pipeline, and we cannot judge the answer by implementation difficulties!"
      },
      {
        "date": "2023-02-14T12:09:00.000Z",
        "voteCount": 4,
        "content": "Could anyone be kind to explain why C is preferred over A? My initial guess was on A, but everyone here seems to unanimously prefer C. Is it because it is not about optimizing I/O performance, but rather the input _pipeline_, which is about processing arrived data within that TF input pipeline (non-I/O)? I just try to understand here. Thanks for reply in advance!"
      },
      {
        "date": "2023-03-22T11:51:00.000Z",
        "voteCount": 1,
        "content": "Option C, splitting into multiple CSV files and using a parallel interleave transformation, could improve the pipeline efficiency by allowing multiple workers to read the data in parallel."
      },
      {
        "date": "2023-04-20T05:51:00.000Z",
        "voteCount": 1,
        "content": "yes but how is it more efficient than converting to a TFRecord file?"
      },
      {
        "date": "2023-07-23T07:33:00.000Z",
        "voteCount": 1,
        "content": "A TFRecord file is a binary file format that is used to store TensorFlow data. It is more efficient than a CSV file because it can be read more quickly and it takes up less space. However, it is still a large file, and it would take a long time to read it into memory. Splitting the file into multiple smaller files would reduce the amount of time it takes to read the files into memory, and it would also make it easier to parallelize the reading process."
      },
      {
        "date": "2023-02-08T20:22:00.000Z",
        "voteCount": 1,
        "content": "split data it's best way in my opinion"
      },
      {
        "date": "2022-12-18T05:58:00.000Z",
        "voteCount": 2,
        "content": "C\nKeywords -&gt; You need to optimize the input pipeline performance \nhttps://www.tensorflow.org/guide/data_performance"
      },
      {
        "date": "2022-12-23T14:09:00.000Z",
        "voteCount": 1,
        "content": "- https://www.tensorflow.org/tutorials/load_data/csv"
      },
      {
        "date": "2022-12-14T03:14:00.000Z",
        "voteCount": 1,
        "content": "It seems C, to me."
      },
      {
        "date": "2022-12-11T10:03:00.000Z",
        "voteCount": 2,
        "content": "Splitting the file we can use parallel interleave to parallel load the datasets\nhttps://www.tensorflow.org/guide/data_performance"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 83,
    "url": "https://www.examtopics.com/discussions/google/view/91045-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You need to design an architecture that serves asynchronous predictions to determine whether a particular mission-critical machine part will fail. Your system collects data from multiple sensors from the machine. You want to build a model that will predict a failure in the next N minutes, given the average of each sensor\u2019s data from the past 12 hours. How should you design the architecture?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. HTTP requests are sent by the sensors to your ML model, which is deployed as a microservice and exposes a REST API for prediction<br>2. Your application queries a Vertex AI endpoint where you deployed your model.<br>3. Responses are received by the caller application as soon as the model produces the prediction.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Events are sent by the sensors to Pub/Sub, consumed in real time, and processed by a Dataflow stream processing pipeline.<br>2. The pipeline invokes the model for prediction and sends the predictions to another Pub/Sub topic.<br>3. Pub/Sub messages containing predictions are then consumed by a downstream system for monitoring.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Export your data to Cloud Storage using Dataflow.<br>2. Submit a Vertex AI batch prediction job that uses your trained model in Cloud Storage to perform scoring on the preprocessed data.<br>3. Export the batch prediction job outputs from Cloud Storage and import them into Cloud SQL.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Export the data to Cloud Storage using the BigQuery command-line tool<br>2. Submit a Vertex AI batch prediction job that uses your trained model in Cloud Storage to perform scoring on the preprocessed data.<br>3. Export the batch prediction job outputs from Cloud Storage and import them into BigQuery."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-06-07T04:12:00.000Z",
        "voteCount": 1,
        "content": "B) Pub/Sub &amp; DataFlow"
      },
      {
        "date": "2024-04-24T07:44:00.000Z",
        "voteCount": 1,
        "content": "The simplest solution that can support an eventual batch prediction (triggered by pub/sub) even the semi-real time prediction."
      },
      {
        "date": "2024-02-29T00:45:00.000Z",
        "voteCount": 1,
        "content": "Needs to be real time not batch. The data needs to be processed as a stream since multiple sensors are used. pawan94 is right. https://cloud.google.com/architecture/minimizing-predictive-serving-latency-in-machine-learning#online_real-time_prediction"
      },
      {
        "date": "2024-01-09T12:06:00.000Z",
        "voteCount": 2,
        "content": "Here you go to the answer provided by google itself. I don't understand why would people use batch prediction when they its sensor data and online prediction is as well asynchronous. \nhttps://cloud.google.com/architecture/minimizing-predictive-serving-latency-in-machine-learning#offline_batch_prediction:~:text=Predictive%20maintenance%3A%20asynchronously%20predicting%20whether%20a%20particular%20machine%20part%20will%20fail%20in%20the%20next%20N%20minutes%2C%20given%20the%20averages%20of%20the%20sensor%27s%20data%20in%20the%20past%2030%20minutes."
      },
      {
        "date": "2023-12-19T08:44:00.000Z",
        "voteCount": 1,
        "content": "it refers to asincronou prediction I' go with C"
      },
      {
        "date": "2023-05-28T02:40:00.000Z",
        "voteCount": 2,
        "content": "D.\nI think we have to query data from the past 12 hours for the prediction, and that's the reason for exporting the data to Cloud Storage.\nAlso, the predictions don't have to be real time."
      },
      {
        "date": "2023-05-08T23:19:00.000Z",
        "voteCount": 1,
        "content": "Went with B"
      },
      {
        "date": "2023-03-28T04:25:00.000Z",
        "voteCount": 2,
        "content": "B.\nOnline prediction, and need decoupling with Pub/Sub to make it asynchronous. Option A is synchronous."
      },
      {
        "date": "2023-03-22T12:00:00.000Z",
        "voteCount": 2,
        "content": "Option C may not be the best choice for this use case because it involves using a batch prediction job in Vertex AI to perform scoring on preprocessed data. Batch prediction jobs are more suitable for scenarios where data is processed in batches, and results can be generated over a longer period, such as daily or weekly.\n\nIn this use case, the requirement is to predict whether a machine part will fail in the next N minutes, given the average of each sensor's data from the past 12 hours. Therefore, real-time processing and prediction are necessary. Batch prediction jobs are not designed for real-time processing, and there may be a delay in receiving the predictions.\n\nOption B, on the other hand, is designed for real-time processing and prediction. The Pub/Sub and Dataflow components allow for real-time processing of incoming sensor data, and the trained ML model can be invoked for prediction in real-time. This makes it ideal for mission-critical applications where timely predictions are essential."
      },
      {
        "date": "2023-03-22T11:56:00.000Z",
        "voteCount": 1,
        "content": "Its B, This architecture leverages the strengths of Pub/Sub, Dataflow, and Vertex AI. The system collects data from multiple sensors, which sends events to Pub/Sub. Pub/Sub can handle the high volume of incoming data and can buffer messages to prevent data loss. A Dataflow stream processing pipeline can consume the events in real-time and perform feature engineering and data preprocessing before invoking the trained ML model for prediction. The predictions are then sent to another Pub/Sub topic, where they can be consumed by a downstream system for monitoring.\n\nThis architecture is highly scalable, resilient, and efficient, as it can handle large volumes of data and perform real-time processing and prediction. It also separates concerns by using a separate pipeline for data processing and another for prediction, making it easier to maintain and modify the system."
      },
      {
        "date": "2023-02-08T20:28:00.000Z",
        "voteCount": 1,
        "content": "if you have sensors inyour architecture.. you need pub/sub..."
      },
      {
        "date": "2023-01-27T00:41:00.000Z",
        "voteCount": 2,
        "content": "B is most likely . if you search asynchronous on this page. it appears in \nthe question wants to focus on online prediction with asynchronous mode.\nhttps://cloud.google.com/architecture/minimizing-predictive-serving-latency-in-machine-learning#online_real-time_prediction \nand the question is the same as what has been explained in this section obviously.  it is  as below.\nPredictive maintenance: asynchronously predicting whether a particular machine part will fail in the next N minutes, given the averages of the sensor's data in the past 30 minutes.\n\nafte that, you can take a closer look at figure3 and read what it try to describle\n\nC and D it is the  offline solution but you opt to use different tools.\nhttps://cloud.google.com/architecture/minimizing-predictive-serving-latency-in-machine-learning#offline_batch_prediction"
      },
      {
        "date": "2023-01-27T00:07:00.000Z",
        "voteCount": 1,
        "content": "Asycnchromoue preciction  = Batch prediction\nhttps://cloud.google.com/architecture/minimizing-predictive-serving-latency-in-machine-learning#offline_batch_prediction"
      },
      {
        "date": "2023-01-27T00:29:00.000Z",
        "voteCount": 1,
        "content": "Asynchronous prediction = Batch prediction, It is incorrect because I am reckless to read this article, Admin can delete my shitty comment above. I was mistaken"
      },
      {
        "date": "2022-12-18T06:24:00.000Z",
        "voteCount": 3,
        "content": "B\n\"Predictive maintenance: asynchronously predicting whether a particular machine part will fail in the next N minutes, given the averages of the sensor's data in the past 30 minutes.\"\nhttps://cloud.google.com/architecture/minimizing-predictive-serving-latency-in-machine-learning#offline_batch_prediction"
      },
      {
        "date": "2022-12-23T14:10:00.000Z",
        "voteCount": 1,
        "content": "- https://cloud.google.com/architecture/minimizing-predictive-serving-latency-in-machine-learning#online_real-time_prediction"
      },
      {
        "date": "2022-12-17T06:32:00.000Z",
        "voteCount": 1,
        "content": "Answer is B. \nhttps://cloud.google.com/architecture/minimizing-predictive-serving-latency-in-machine-learning#handling_dynamic_real-time_features"
      },
      {
        "date": "2022-12-14T03:48:00.000Z",
        "voteCount": 1,
        "content": "C, for me."
      },
      {
        "date": "2022-12-13T07:25:00.000Z",
        "voteCount": 1,
        "content": "ref : https://cloud.google.com/architecture/minimizing-predictive-serving-latency-in-machine-learning#offline_batch_prediction"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 84,
    "url": "https://www.examtopics.com/discussions/google/view/91053-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "Your company manages an application that aggregates news articles from many different online sources and sends them to users. You need to build a recommendation model that will suggest articles to readers that are similar to the articles they are currently reading. Which approach should you use?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a collaborative filtering system that recommends articles to a user based on the user\u2019s past behavior.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEncode all articles into vectors using word2vec, and build a model that returns articles based on vector similarity.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBuild a logistic regression model for each user that predicts whether an article should be recommended to a user.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tManually label a few hundred articles, and then train an SVM classifier based on the manually classified articles that categorizes additional articles into their respective categories."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 16,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-04-14T07:04:00.000Z",
        "voteCount": 1,
        "content": "Went with B"
      },
      {
        "date": "2023-05-08T23:19:00.000Z",
        "voteCount": 1,
        "content": "Went with B"
      },
      {
        "date": "2023-04-17T00:33:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/blog/topics/developers-practitioners/meet-ais-multitool-vector-embeddings\nAnswer B"
      },
      {
        "date": "2023-03-28T04:28:00.000Z",
        "voteCount": 2,
        "content": "Currently reading is the keyword here. Going to need B for that, A won't work since it would be based on e.g. all reading history and not the article currently being read."
      },
      {
        "date": "2023-03-22T12:03:00.000Z",
        "voteCount": 2,
        "content": "Option A, creating a collaborative filtering system, may not be ideal for this use case because it relies on user behavior data, which may not be available or sufficient for new users or for users who have not interacted with the system much.\n\nOption C, building a logistic regression model for each user, may not be scalable because it requires building a separate model for each user, which can become difficult to manage as the number of users increases.\n\nOption D, manually labeling articles and training an SVM classifier, may not be as effective as the word2vec approach because it relies on manual labeling, which can be time-consuming and may not capture the full semantic meaning of the articles. Additionally, SVMs may not be as effective as neural network-based approaches like word2vec for capturing complex relationships between words and articles."
      },
      {
        "date": "2022-12-28T07:56:00.000Z",
        "voteCount": 1,
        "content": "word2vec can easily get similar articles, but the collaborative filter isn't sure well."
      },
      {
        "date": "2022-12-18T06:29:00.000Z",
        "voteCount": 3,
        "content": "B\nhttps://towardsdatascience.com/recommending-news-articles-based-on-already-read-articles-627695221fe8"
      },
      {
        "date": "2022-12-17T06:23:00.000Z",
        "voteCount": 1,
        "content": "Answer B"
      },
      {
        "date": "2022-12-14T03:52:00.000Z",
        "voteCount": 2,
        "content": "Collaborative looks at the other users, knowledge-based at me.Answer B is the most knowledge based, among these."
      },
      {
        "date": "2022-12-12T20:23:00.000Z",
        "voteCount": 2,
        "content": "\"similar to they are currently reading\". it should be a collaborative filtering problem"
      },
      {
        "date": "2023-02-02T07:01:00.000Z",
        "voteCount": 3,
        "content": "No, Collaborative filtering recommends articles other people read that are not necessarily similar to what the person is reading. These people are chosen on being similar to the person in question, not the article."
      },
      {
        "date": "2022-12-11T11:30:00.000Z",
        "voteCount": 2,
        "content": "Answer B"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 85,
    "url": "https://www.examtopics.com/discussions/google/view/91122-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You work for a large social network service provider whose users post articles and discuss news. Millions of comments are posted online each day, and more than 200 human moderators constantly review comments and flag those that are inappropriate. Your team is building an ML model to help human moderators check content on the platform. The model scores each comment and flags suspicious comments to be reviewed by a human. Which metric(s) should you use to monitor the model\u2019s performance?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNumber of messages flagged by the model per minute",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNumber of messages flagged by the model per minute confirmed as being inappropriate by humans.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPrecision and recall estimates based on a random sample of 0.1% of raw messages each minute sent to a human for review",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPrecision and recall estimates based on a sample of messages flagged by the model as potentially inappropriate each minute\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 23,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 11,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 11,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-19T02:58:00.000Z",
        "voteCount": 11,
        "content": "D\n- https://cloud.google.com/natural-language/automl/docs/beginners-guide\n- https://cloud.google.com/vertex-ai/docs/text-data/classification/evaluate-model"
      },
      {
        "date": "2023-09-10T02:42:00.000Z",
        "voteCount": 7,
        "content": "A. Number of messages flagged by the model per minute =&gt; NO, no measure of model performance \nB. Number of messages flagged by the model per minute confirmed as being inappropriate by humans.=&gt; DONT THINK SO, because we need the total number of messages (flagged?)\nC. Precision and recall estimates based on a random sample of 0.1% of raw messages each minute sent to a human for review. =&gt; I think YES, because as I understand it that would be based on a sample of ALL messages not just the ones that have been flagged.\nD. Precision and recall estimates based on a sample of messages flagged by the model as potentially inappropriate each minute =&gt; I think NO, because the sample includes only flagged messages, meaning positives, so you cannot really measure recall."
      },
      {
        "date": "2023-11-07T11:29:00.000Z",
        "voteCount": 4,
        "content": "The main issue with option C is that it uses a random sample of only 0.1% of raw messages. This random sample might not contain enough examples of inappropriate content to accurately assess the model's performance. Since the majority of messages on the platform are likely appropriate, the random sample may not capture enough inappropriate content for a robust evaluation."
      },
      {
        "date": "2024-09-26T08:16:00.000Z",
        "voteCount": 1,
        "content": "I went with B.\nRemember how to calculate Recall: TP/(TP+FN). Since \"sample of messaged flagged by the model\" are only P cases, you won't have your F cases reviewed by a human, therefore you won't have FN, therefore it's not D.\n\nI also believe that 0.1% of raw messages is going to have too little P cases,  therefore not C.\n\nAnd then we remain with option B, which is not optimal, but it is the best we can do in this situation."
      },
      {
        "date": "2024-09-08T10:22:00.000Z",
        "voteCount": 1,
        "content": "It is absolutely not possible to calculate recall with D because we only have positives in the sample we need false negatives. Because of the high quantity of total data, 0.1% is fine, the answer is C"
      },
      {
        "date": "2024-03-25T08:51:00.000Z",
        "voteCount": 3,
        "content": "Precision and recall are critical metrics for evaluating the performance of classification models, especially in contexts where both the accuracy of positive predictions (precision) and the ability to identify all positive instances (recall) are important. In this case:\nPrecision (the proportion of messages flagged by the model as inappropriate that were actually inappropriate) helps ensure that the model minimizes the burden on human moderators by not flagging too many false positives, which could overwhelm them.\nRecall (the proportion of actual inappropriate messages that were correctly flagged by the model) ensures that the model is effective at catching as many inappropriate messages as possible, reducing the risk of harmful content being missed."
      },
      {
        "date": "2024-03-13T08:54:00.000Z",
        "voteCount": 1,
        "content": "I go with C"
      },
      {
        "date": "2024-02-29T07:33:00.000Z",
        "voteCount": 1,
        "content": "Let's consider below hypothetical scenario:\n\nTotal number of comments per minute: 10,000\nComments actually inappropriate: 500\nIf we use a random sample of only 0.1% of raw messages (10 comments) for evaluation, there's a high chance that this small sample may not include any or only a few inappropriate comments. As a result, the precision and recall estimates based on this sample may be skewed, leading to unreliable assessments of the model's performance. Thus, C is ruled out."
      },
      {
        "date": "2024-02-29T01:23:00.000Z",
        "voteCount": 1,
        "content": "C does not make sense to me since it is a very small random sample. It is also only messages that have been sent to humans for review meaning that there is bias in that result set."
      },
      {
        "date": "2024-01-02T04:03:00.000Z",
        "voteCount": 1,
        "content": "D only caring for observations flagged by the model means we don't control for false negatives (approved actually inappropriate messages). B seems like a better option to me: the wording confuses me a bit, but I understand it as the true and false positives (human flagged comments and their modelled label)"
      },
      {
        "date": "2023-11-15T08:18:00.000Z",
        "voteCount": 1,
        "content": "In favor of D"
      },
      {
        "date": "2023-11-14T08:11:00.000Z",
        "voteCount": 1,
        "content": "Given the context of content moderation, a balanced approach is often preferred. Therefore, option C, precision and recall estimates based on a random sample of raw messages, is a good choice. It provides a holistic view of the model's performance, taking into account both false positives (precision) and false negatives (recall), and it reflects how well the model is handling the entire dataset."
      },
      {
        "date": "2023-11-11T11:53:00.000Z",
        "voteCount": 1,
        "content": "A --&gt; Conveys model'a activity levels but nit accuracy\nB --&gt; Accuracy to some extend but wont give full picture as it does not account False negatives\nC --&gt; Using a random sample of the raw messages allows you to estimate precision and recall for the overall activity, not just the flagged content.\nD --&gt; Specifically measures on the subset of data that it flagged\n\nBoth C &amp; D work well in this case, but the specificity is higher in option D and hence will go with D"
      },
      {
        "date": "2023-10-30T08:15:00.000Z",
        "voteCount": 1,
        "content": "Google Cloud used to have a service called \"continuous evaluation\", where human labelers classify data to establish a ground truth. Thinking along those lines, the answer is C as it's the logical equivalent of that service.\n\nhttps://cloud.google.com/ai-platform/prediction/docs/continuous-evaluation"
      },
      {
        "date": "2023-06-28T01:53:00.000Z",
        "voteCount": 2,
        "content": "Question is to measure model performance so has to be precision &amp; recall , hence D."
      },
      {
        "date": "2023-06-08T04:28:00.000Z",
        "voteCount": 1,
        "content": "D. Precision and recall estimates based on a sample of messages flagged by the model as potentially inappropriate each minute \nYou will need precision and recall to identify fals positives and false negatives. A very small random sample doesn't help specially becasue probably you will have skewed data. So D."
      },
      {
        "date": "2023-05-08T23:20:00.000Z",
        "voteCount": 1,
        "content": "Went with D"
      },
      {
        "date": "2023-04-26T02:28:00.000Z",
        "voteCount": 1,
        "content": "we need to monitor the model, so D"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 86,
    "url": "https://www.examtopics.com/discussions/google/view/91062-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You are a lead ML engineer at a retail company. You want to track and manage ML metadata in a centralized way so that your team can have reproducible experiments by generating artifacts. Which management solution should you recommend to your team?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore your tf.logging data in BigQuery.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tManage all relational entities in the Hive Metastore.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore all ML metadata in Google Cloud\u2019s operations suite.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tManage your ML workflows with Vertex ML Metadata.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 16,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-12-19T03:01:00.000Z",
        "voteCount": 5,
        "content": "D\n- https://cloud.google.com/vertex-ai/docs/ml-metadata/tracking"
      },
      {
        "date": "2024-07-03T08:53:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      },
      {
        "date": "2024-02-14T17:25:00.000Z",
        "voteCount": 1,
        "content": "Selected Answer: D"
      },
      {
        "date": "2023-05-08T23:20:00.000Z",
        "voteCount": 1,
        "content": "Went with D"
      },
      {
        "date": "2023-02-08T20:34:00.000Z",
        "voteCount": 2,
        "content": "totally D"
      },
      {
        "date": "2022-12-14T05:06:00.000Z",
        "voteCount": 3,
        "content": "This should be an easy D."
      },
      {
        "date": "2022-12-11T11:59:00.000Z",
        "voteCount": 3,
        "content": "https://codelabs.developers.google.com/vertex-mlmd-pipelines?hl=id&amp;authuser=6#0"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 87,
    "url": "https://www.examtopics.com/discussions/google/view/91065-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You have been given a dataset with sales predictions based on your company\u2019s marketing activities. The data is structured and stored in BigQuery, and has been carefully managed by a team of data analysts. You need to prepare a report providing insights into the predictive capabilities of the data. You were asked to run several ML models with different levels of sophistication, including simple models and multilayered neural networks. You only have a few hours to gather the results of your experiments. Which Google Cloud tools should you use to complete this task in the most efficient and self-serviced way?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse BigQuery ML to run several regression models, and analyze their performance.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRead the data from BigQuery using Dataproc, and run several models using SparkML.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Vertex AI Workbench user-managed notebooks with scikit-learn code for a variety of ML algorithms and performance metrics.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTrain a custom TensorFlow model with Vertex AI, reading the data from BigQuery featuring a variety of ML algorithms."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 19,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 8,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-02-29T01:31:00.000Z",
        "voteCount": 3,
        "content": "You only have a few hours. The dataset is in BQ. The dataset is carefully managed. BQML it is."
      },
      {
        "date": "2024-02-28T03:32:00.000Z",
        "voteCount": 1,
        "content": "I agree with pico answer"
      },
      {
        "date": "2024-01-21T03:31:00.000Z",
        "voteCount": 3,
        "content": "All deep neural networks are multilayered neural networks, but not all multilayered neural networks are necessarily deep. The term \"deep\" is used to emphasize the depth of the network in the context of having many hidden layers, which has been shown to be effective for learning hierarchical representations of complex patterns in data.\n\nHence BQ allows creation of DNNs (https://cloud.google.com/bigquery/docs/reference/standard-sql/bigqueryml-syntax-create-dnn-models) it should be A."
      },
      {
        "date": "2023-09-13T02:22:00.000Z",
        "voteCount": 3,
        "content": "Vertex AI Workbench provides user-managed notebooks that allow you to run Python code using libraries like scikit-learn, TensorFlow, and more.\nYou can easily connect to your BigQuery dataset from within the notebook, extract the data, and perform data preprocessing.\nYou can then experiment with different ML algorithms available in scikit-learn and track performance metrics.\nIt provides flexibility, control, and the ability to run various models quickly."
      },
      {
        "date": "2023-09-13T02:24:00.000Z",
        "voteCount": 2,
        "content": "Not A. \nBigQuery ML is convenient for quick model training and predictions within BigQuery itself, but it has limitations in terms of the variety of ML algorithms and customization options it offers.\nIt may not be the best choice for running more sophisticated ML models or extensive experiments.\n\nand It only said regression model"
      },
      {
        "date": "2023-08-06T00:49:00.000Z",
        "voteCount": 1,
        "content": "I think multilayered neural networks need to be trained externally from BQ ML as stated here:\nhttps://cloud.google.com/bigquery/docs/bqml-introduction"
      },
      {
        "date": "2023-08-06T21:00:00.000Z",
        "voteCount": 1,
        "content": "nvm you can import DNN in BQ"
      },
      {
        "date": "2023-07-07T15:56:00.000Z",
        "voteCount": 2,
        "content": "According to the question, you don't have enough time. B, C, D need much more time to set up the service, or write the code. Also the data is already in BigQuery. BQML should be the fastest way. Besides, BQML supports xgboost, NN models as well."
      },
      {
        "date": "2023-06-27T15:28:00.000Z",
        "voteCount": 2,
        "content": "The question says that \"You were asked to run several ML models with different levels of sophistication, including simple models and multilayered neural networks\" BQ ML doesn't allow this. BQ ML provides only simple regression/categorization models. It is not about training these \"sophisticated models\" but only run them, so you can easly do it within few hours with notebooks."
      },
      {
        "date": "2023-05-08T23:20:00.000Z",
        "voteCount": 2,
        "content": "Went with A"
      },
      {
        "date": "2023-04-29T21:12:00.000Z",
        "voteCount": 1,
        "content": "C allows to execute more complex tests"
      },
      {
        "date": "2023-07-24T11:05:00.000Z",
        "voteCount": 1,
        "content": "However, given the limited time constraint of a few hours and the fact that the data is already stored in BigQuery, option A is more efficient.\n\nBigQuery ML allows you to quickly create and evaluate ML models directly within BigQuery, without the need to move the data or set up a separate environment. This makes it faster and more convenient for running several regression models and analyzing their performance within the given time frame."
      },
      {
        "date": "2023-02-20T16:12:00.000Z",
        "voteCount": 2,
        "content": "B,C,D requires coding. You only have some hours, A is the fastest."
      },
      {
        "date": "2022-12-19T03:06:00.000Z",
        "voteCount": 3,
        "content": "I vote for A"
      },
      {
        "date": "2022-12-14T05:18:00.000Z",
        "voteCount": 2,
        "content": "It's A."
      },
      {
        "date": "2022-12-11T12:07:00.000Z",
        "voteCount": 2,
        "content": "I will go with A, since it's the fastest way to do it. Custom training in Vertex AI requires time and writing scikit-learn models in notebooks too"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 88,
    "url": "https://www.examtopics.com/discussions/google/view/91131-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You are an ML engineer at a bank. You have developed a binary classification model using AutoML Tables to predict whether a customer will make loan payments on time. The output is used to approve or reject loan requests. One customer\u2019s loan request has been rejected by your model, and the bank\u2019s risks department is asking you to provide the reasons that contributed to the model\u2019s decision. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse local feature importance from the predictions.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the correlation with target values in the data summary page.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the feature importance percentages in the model evaluation page.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tVary features independently to identify the threshold per feature that changes the classification."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 25,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-02-23T04:33:00.000Z",
        "voteCount": 10,
        "content": "To access local feature importance in AutoML Tables, you can use the \"Explain\" feature, which shows the contribution of each feature to the prediction for a specific example. This will help you identify the most important features that contributed to the loan request being rejected.\n\nOption B, using the correlation with target values in the data summary page, may not provide the most accurate explanation as it looks at the overall correlation between the features and target variable, rather than the contribution of each feature to a specific prediction.\n\nOption C, using the feature importance percentages in the model evaluation page, may not provide a sufficient explanation for the specific prediction, as it shows the importance of each feature across all predictions, rather than for a specific prediction.\n\nOption D, varying features independently to identify the threshold per feature that changes the classification, is not recommended as it can be time-consuming and does not provide a clear explanation for why the loan request was rejected"
      },
      {
        "date": "2023-05-08T23:21:00.000Z",
        "voteCount": 1,
        "content": "Went with A"
      },
      {
        "date": "2023-03-28T04:45:00.000Z",
        "voteCount": 4,
        "content": "Local, not global since they asked about one specific prediction.\nCheck out that section on this blog: https://cloud.google.com/blog/products/ai-machine-learning/explaining-model-predictions-structured-data/\nCool stuff!"
      },
      {
        "date": "2023-03-23T08:11:00.000Z",
        "voteCount": 2,
        "content": "Local feature importance can provide insight into the specific features that contributed to the model's decision for a particular instance. This information can be used to explain the model's decision to the bank's risks department and potentially identify any issues or biases in the model. Option B is not applicable as the loan request has already been rejected by the model, so there are no target values to correlate with. Option C may provide some insights, but local feature importance will provide more specific information for this particular instance. Option D involves changing the features, which may not be feasible or ethical in this case."
      },
      {
        "date": "2023-03-17T04:17:00.000Z",
        "voteCount": 1,
        "content": "C seems more apt &amp; exhaustive to explain for bank's purpose; it uses various Feature Attribution methods.\nA explains how much each feature added to or subtracted from the result as compared with the baseline prediction score; indicative, but less optimal for the purpose at hand"
      },
      {
        "date": "2023-02-08T20:38:00.000Z",
        "voteCount": 2,
        "content": "it's think is more easy to explain with feature importance"
      },
      {
        "date": "2023-01-04T03:52:00.000Z",
        "voteCount": 1,
        "content": "AutoML Tables tells you how much each feature impacts this model. It is shown in the Feature importance graph. The values are provided as a percentage for each feature: the higher the percentage, the more strongly that feature impacted model training. C."
      },
      {
        "date": "2022-12-19T03:13:00.000Z",
        "voteCount": 2,
        "content": "A\nhttps://cloud.google.com/automl-tables/docs/explain#local"
      },
      {
        "date": "2022-12-17T06:06:00.000Z",
        "voteCount": 4,
        "content": "Agree with A. \n\"Local feature importance gives you visibility into how the individual features in a specific prediction request affected the resulting prediction.\nEach local feature importance value shows only how much the feature affected the prediction for that row. To understand the overall behavior of the model, use model feature importance.\"\nhttps://cloud.google.com/automl-tables/docs/explain#local"
      },
      {
        "date": "2022-12-14T05:28:00.000Z",
        "voteCount": 1,
        "content": "\"Feature importance: AutoML Tables tells you how much each feature impacts this model. It is shown in the Feature importance graph. The values are provided as a percentage for each feature: the higher the percentage, the more strongly that feature impacted model training.\" The correct answer is C."
      },
      {
        "date": "2023-03-23T08:10:00.000Z",
        "voteCount": 2,
        "content": "Can you tell the feature importance for a specific prediction?"
      },
      {
        "date": "2022-12-12T20:31:00.000Z",
        "voteCount": 2,
        "content": "Should be A. it is specific to this example. so use local feature importance"
      },
      {
        "date": "2022-12-12T00:58:00.000Z",
        "voteCount": 1,
        "content": "It seems C, to me."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 89,
    "url": "https://www.examtopics.com/discussions/google/view/91459-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You work for a magazine distributor and need to build a model that predicts which customers will renew their subscriptions for the upcoming year. Using your company\u2019s historical data as your training set, you created a TensorFlow model and deployed it to AI Platform. You need to determine which customer attribute has the most predictive power for each prediction served by the model. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AI Platform notebooks to perform a Lasso regression analysis on your model, which will eliminate features that do not provide a strong signal.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStream prediction results to BigQuery. Use BigQuery\u2019s CORR(X1, X2) function to calculate the Pearson correlation coefficient between each feature and the target variable.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the AI Explanations feature on AI Platform. Submit each prediction request with the \u2018explain\u2019 keyword to retrieve feature attributions using the sampled Shapley method.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the What-If tool in Google Cloud to determine how your model will perform when individual features are excluded. Rank the feature importance in order of those that caused the most significant performance drop when removed from the model."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 15,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-02-14T17:36:00.000Z",
        "voteCount": 4,
        "content": "Vertex AI Explanations went with C"
      },
      {
        "date": "2023-05-08T23:21:00.000Z",
        "voteCount": 2,
        "content": "Went with C"
      },
      {
        "date": "2023-05-08T03:33:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/ai-platform/prediction/docs/ai-explanations/overview\nAI Explanations helps you understand your model's outputs for classification and regression tasks. Whenever you request a prediction on AI Platform, AI Explanations tells you how much each feature in the data contributed to the predicted result."
      },
      {
        "date": "2023-03-17T02:22:00.000Z",
        "voteCount": 3,
        "content": "Key words in question \"for each prediction served\" - that make its C\nD is more of a broader analysis activity"
      },
      {
        "date": "2023-01-27T06:51:00.000Z",
        "voteCount": 1,
        "content": "You have to use a flagship native service  as much as possible."
      },
      {
        "date": "2022-12-19T03:30:00.000Z",
        "voteCount": 1,
        "content": "I vote for D\n- https://www.tensorflow.org/tensorboard/what_if_tool\n- https://pair-code.github.io/what-if-tool/\n- https://medium.com/red-buffer/tensorflows-what-if-tool-c52914ea215c\nC is wrong cuz AI Explanation dosen't work for TensorFlow models (https://cloud.google.com/vertex-ai/docs/explainable-ai/overview)"
      },
      {
        "date": "2022-12-19T10:07:00.000Z",
        "voteCount": 2,
        "content": "This is from the doc you provided:\n\"Feature attribution is supported for all types of models (both AutoML and custom-trained), frameworks (TensorFlow, scikit, XGBoost), and modalities (images, text, tabular, video).\"\nhttps://cloud.google.com/vertex-ai/docs/explainable-ai/overview#supported_model_types_2"
      },
      {
        "date": "2022-12-23T04:38:00.000Z",
        "voteCount": 1,
        "content": "Sorry, I mean Shapley method doesn't support TensorFlow Models\nSee https://cloud.google.com/vertex-ai/docs/explainable-ai/overview#compare-methods"
      },
      {
        "date": "2022-12-23T14:57:00.000Z",
        "voteCount": 2,
        "content": "Sorry, i tink C is the answer. Tks"
      },
      {
        "date": "2022-12-23T14:57:00.000Z",
        "voteCount": 2,
        "content": "Sorry, i tink C is the answer"
      },
      {
        "date": "2022-12-17T05:59:00.000Z",
        "voteCount": 3,
        "content": "AI Explanations provides feature attributions using the sampled Shapley method, which can help you understand how much each feature contributes to a model's prediction."
      },
      {
        "date": "2022-12-14T05:31:00.000Z",
        "voteCount": 2,
        "content": "AI Explanations helps you understand your model's outputs for classification and regression tasks. Whenever you request a prediction on AI Platform, AI Explanations tells you how much each feature in the data contributed to the predicted result.\" It's C!"
      },
      {
        "date": "2022-12-13T08:47:00.000Z",
        "voteCount": 2,
        "content": "Agree with C"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 90,
    "url": "https://www.examtopics.com/discussions/google/view/91150-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You are working on a binary classification ML algorithm that detects whether an image of a classified scanned document contains a company\u2019s logo. In the dataset, 96% of examples don\u2019t have the logo, so the dataset is very skewed. Which metrics would give you the most confidence in your model?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tF-score where recall is weighed more than precision\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRMSE",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tF1 score",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tF-score where precision is weighed more than recall"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 24,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 9,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 6,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-03-23T08:16:00.000Z",
        "voteCount": 10,
        "content": "In this scenario, the dataset is highly imbalanced, where most of the examples do not have the company's logo. Therefore, accuracy could be misleading as the model can have high accuracy by simply predicting that all images do not have the logo. F1 score is a good metric to consider in such cases, as it takes both precision and recall into account. However, since the dataset is highly skewed, we should weigh recall more than precision to ensure that the model is correctly identifying the images that do have the logo. Therefore, F-score where recall is weighed more than precision is the best metric to evaluate the performance of the model in this scenario. Option B (RMSE) is not applicable to this classification problem, and option D (F-score where precision is weighed more than recall) is not suitable for highly skewed datasets."
      },
      {
        "date": "2024-04-20T02:42:00.000Z",
        "voteCount": 2,
        "content": "I'd go with C. We don't know which option (less FP or less FN) is most important for business with the provided information, so we should seek a balance."
      },
      {
        "date": "2024-03-01T11:10:00.000Z",
        "voteCount": 1,
        "content": "I think it's D."
      },
      {
        "date": "2024-01-22T17:53:00.000Z",
        "voteCount": 3,
        "content": "I think it could be D, but the question does not provide enough information for this.\n\nI have this feeling: If 4% have the logo, we are looking just for these ones, right? So, the 'quality of TP,' that's it, the precision, could be more interesting because we want a model that we can rely on. So, when this model Predict a image with logo, we`ll be more certain about it. \n\nIf we use recall, for example, a model with 99% recall has more chance of getting the logo, but we won't have quality in this. This model could suggest a lot of images without logo. It is better to use any ML than this..."
      },
      {
        "date": "2023-11-14T08:24:00.000Z",
        "voteCount": 1,
        "content": "both option A (F-score with higher weight on recall) and option C (F1 score) could be suitable depending on the specific priorities and requirements of your classification problem. If missing a company's logo is considered more problematic than having false alarms, then option A might be preferred. The F1 score (option C) is a balanced measure that considers both precision and recall, which is generally a good choice in imbalanced datasets.\n\nUltimately, the choice between option A and option C depends on the specific goals and constraints of your application."
      },
      {
        "date": "2023-11-12T09:23:00.000Z",
        "voteCount": 2,
        "content": "The question not have clear preference for recall or precision hence going with C"
      },
      {
        "date": "2023-06-27T16:47:00.000Z",
        "voteCount": 4,
        "content": "Yeah, I know - everyone is voting A... To be honest I still don't understand why are you more affraid of these few FNs than FPs. In my opinion they are exactly same evil. Every documantation says that F1 is great on skewed data. You should use weighted F1 when you know what is worse for you FNs or FPs. In this case we have no any hints on it, so I would stay with ordinary F1."
      },
      {
        "date": "2023-06-08T04:40:00.000Z",
        "voteCount": 2,
        "content": "A. F-score where recall is weighed more than precision\nEven a model which always says that don't have the logo will have a good precision because is the most common. What we need is improve recall."
      },
      {
        "date": "2023-05-08T23:21:00.000Z",
        "voteCount": 2,
        "content": "Went with A"
      },
      {
        "date": "2023-02-24T12:01:00.000Z",
        "voteCount": 3,
        "content": "I think is A. The positive Class is the minority. So, it's more important to correctly detect logos in all images that have logo (recall) than correctly detect logos in images classified with logos (precision)."
      },
      {
        "date": "2023-02-08T20:45:00.000Z",
        "voteCount": 3,
        "content": "I think is D becouse u try detect TP then it's more important recall than precision"
      },
      {
        "date": "2023-01-11T07:29:00.000Z",
        "voteCount": 1,
        "content": "Answer A is my choice."
      },
      {
        "date": "2022-12-29T18:14:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2022-12-28T07:59:00.000Z",
        "voteCount": 3,
        "content": "less logo images.  Recall should be weighted more"
      },
      {
        "date": "2022-12-27T02:38:00.000Z",
        "voteCount": 4,
        "content": "I think A. \nIf D were the answer, the threshold would be set higher to increase PRECISION, but the low percentage of positives (4%) would allow RECALL to be extremely low. If the percentage of positives is low, greater weight should be given to RECALL.\nhttps://medium.com/@douglaspsteen/beyond-the-f-1-score-a-look-at-the-f-beta-score-3743ac2ef6e3"
      },
      {
        "date": "2022-12-24T00:47:00.000Z",
        "voteCount": 4,
        "content": "Answer C: F1-Score is the best for imbalanced Data like this case: https://stephenallwright.com/imbalanced-data-metric/"
      },
      {
        "date": "2022-12-23T15:20:00.000Z",
        "voteCount": 2,
        "content": "D (not sure)"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 91,
    "url": "https://www.examtopics.com/discussions/google/view/91570-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You work on the data science team for a multinational beverage company. You need to develop an ML model to predict the company\u2019s profitability for a new line of naturally flavored bottled waters in different locations. You are provided with historical data that includes product types, product sales volumes, expenses, and profits for all regions. What should you use as the input and output for your model?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse latitude, longitude, and product type as features. Use profit as model output.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse latitude, longitude, and product type as features. Use revenue and expenses as model outputs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse product type and the feature cross of latitude with longitude, followed by binning, as features. Use profit as model output.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse product type and the feature cross of latitude with longitude, followed by binning, as features. Use revenue and expenses as model outputs."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 17,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-19T14:39:00.000Z",
        "voteCount": 7,
        "content": "C (not sure)\n- https://developers.google.com/machine-learning/crash-course/feature-crosses/video-lecture\n- https://developers.google.com/machine-learning/crash-course/regularization-for-sparsity/l1-regularization"
      },
      {
        "date": "2024-01-29T04:46:00.000Z",
        "voteCount": 2,
        "content": "the question asks to predict profitability , not profit. \nprofitability is calculated from revenue and expenses. \nthe correct answer is D"
      },
      {
        "date": "2023-09-20T23:09:00.000Z",
        "voteCount": 2,
        "content": "Most people have chosen C but: \nDoes it make sense to do binning after feature cross? Isnt it the other way around?"
      },
      {
        "date": "2023-10-20T00:36:00.000Z",
        "voteCount": 1,
        "content": "I agree it is the way around. See example: \nhttps://developers.google.com/machine-learning/crash-course/feature-crosses/check-your-understanding\nOne feature cross: [binned latitude X binned longitude X binned roomsPerPerson]"
      },
      {
        "date": "2023-10-20T00:45:00.000Z",
        "voteCount": 1,
        "content": "In the following examples it is said that it is not possible to cross lat &amp; lon without bucketized them before since continous values must be converted into discrete before crossing :\nhttps://www.kaggle.com/code/vikramtiwari/feature-crosses-tensorflow-mlcc"
      },
      {
        "date": "2023-05-08T23:21:00.000Z",
        "voteCount": 1,
        "content": "Went with C"
      },
      {
        "date": "2023-03-23T08:27:00.000Z",
        "voteCount": 3,
        "content": "Option C  is the best option because it takes into account both the product type and location, which can affect profitability. Binning the feature cross of latitude and longitude can help capture the nonlinear relationship between location and profitability, and using profit as the model output is appropriate because it's the target variable we want to predict."
      },
      {
        "date": "2023-02-17T06:32:00.000Z",
        "voteCount": 1,
        "content": "Agreeing with hiromi, taxberg\nFeature cross and bucket lat and lon on geographical problems"
      },
      {
        "date": "2023-02-08T20:49:00.000Z",
        "voteCount": 1,
        "content": "your output is profit"
      },
      {
        "date": "2023-02-03T06:03:00.000Z",
        "voteCount": 2,
        "content": "Must be C. Always feature cross lat and lon on geographical problems. Also, D can not be right as we do not have revenue in the dataset."
      },
      {
        "date": "2022-12-17T05:36:00.000Z",
        "voteCount": 2,
        "content": "In this case, there is no need to reduce the number of unique values in the latitude and longitude variables, and binning would reduce information from those features hence A"
      },
      {
        "date": "2022-12-25T12:23:00.000Z",
        "voteCount": 1,
        "content": "Why no need to reduce?"
      },
      {
        "date": "2022-12-17T05:38:00.000Z",
        "voteCount": 1,
        "content": "binding and crossing*"
      },
      {
        "date": "2022-12-14T05:39:00.000Z",
        "voteCount": 2,
        "content": "Easy C."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 92,
    "url": "https://www.examtopics.com/discussions/google/view/91574-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You work as an ML engineer at a social media company, and you are developing a visual filter for users\u2019 profile photos. This requires you to train an ML model to detect bounding boxes around human faces. You want to use this filter in your company\u2019s iOS-based mobile phone application. You want to minimize code development and want the model to be optimized for inference on mobile phones. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTrain a model using AutoML Vision and use the \u201cexport for Core ML\u201d option.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTrain a model using AutoML Vision and use the \u201cexport for Coral\u201d option.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTrain a model using AutoML Vision and use the \u201cexport for TensorFlow.js\u201d option.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTrain a custom TensorFlow model and convert it to TensorFlow Lite (TFLite)."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 18,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 4,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-20T05:54:00.000Z",
        "voteCount": 14,
        "content": "https://cloud.google.com/vision/automl/docs/export-edge\nCore ML -&gt; iOS and macOS\nCoral -&gt; Edge TPU-based device\nTensorFlow.js -&gt; web"
      },
      {
        "date": "2023-10-20T01:05:00.000Z",
        "voteCount": 4,
        "content": "Updated Vertex AI link:https://cloud.google.com/vertex-ai/docs/export/export-edge-model\n\nTrained AutoML Edge image classification models can be exported in the following formats:\nTF Lite - to run your model on edge or mobile devices.\nEdge TPU TF Lite - to run your model on Edge TPU devices.\nContainer - to run on a Docker container.\nCore ML - to run your model on iOS and macOS devices.\nTensorflow.js - to run your model in the browser and in Node.js."
      },
      {
        "date": "2023-05-08T23:22:00.000Z",
        "voteCount": 1,
        "content": "Went with A"
      },
      {
        "date": "2023-04-17T00:16:00.000Z",
        "voteCount": 1,
        "content": "https://developer.apple.com/documentation/coreml\nAnswer A"
      },
      {
        "date": "2023-04-17T00:18:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/vertex-ai/docs/export/export-edge-model#export"
      },
      {
        "date": "2023-02-23T05:43:00.000Z",
        "voteCount": 1,
        "content": "AutoML Vision is a service provided by Google Cloud that enables developers to train and deploy machine learning models for image recognition tasks, such as detecting bounding boxes around human faces. The \u201cexport for Coral\u201d option generates a TFLite model that is optimized for running on Coral, a hardware platform specifically designed for edge computing, including mobile devices. The TFLite model is also compatible with iOS-based mobile phone applications, making it easy to integrate into the company's app."
      },
      {
        "date": "2023-03-23T09:05:00.000Z",
        "voteCount": 1,
        "content": "While Coral can be used to optimize machine learning models for inference on edge devices, it's not the best option for an iOS-based mobile phone application."
      },
      {
        "date": "2023-02-23T05:42:00.000Z",
        "voteCount": 1,
        "content": "Option A, using AutoML Vision and exporting for Core ML, is also a viable option. Core ML is Apple's machine learning framework that is optimized for iOS-based devices. However, using this option would require more development effort to integrate the Core ML model into the app.\n\nOption C, using AutoML Vision and exporting for TensorFlow.js, is not the best option for this scenario since it is optimized for running on web browsers, not mobile devices.\n\nOption D, training a custom TensorFlow model and converting it to TFLite, would require significant development effort and time compared to using AutoML Vision. AutoML Vision provides a simple and efficient way to train and deploy machine learning models without requiring expertise in machine learning."
      },
      {
        "date": "2023-03-23T09:06:00.000Z",
        "voteCount": 1,
        "content": "Excellent reasoning for C,D but Core ML is Apple's machine learning framework that is optimized for iOS-based devices, and exporting the model to Core ML format can help minimize inference time on mobile devices."
      },
      {
        "date": "2023-02-08T20:52:00.000Z",
        "voteCount": 1,
        "content": "https://www.tensorflow.org/lite\nhttps://medium.com/the-ai-team/step-into-on-device-inference-with-tensorflow-lite-a47242ba9130"
      },
      {
        "date": "2023-03-23T09:04:00.000Z",
        "voteCount": 1,
        "content": "Its wrong, While TFLite is a mobile-optimized version of TensorFlow, it requires more code development than using AutoML Vision and exporting for Core ML. Therefore, it's not the best option for minimizing code development time."
      },
      {
        "date": "2023-01-04T03:32:00.000Z",
        "voteCount": 1,
        "content": "I correct myself: it's A!"
      },
      {
        "date": "2022-12-24T00:52:00.000Z",
        "voteCount": 1,
        "content": "A indeed as described here: https://cloud.google.com/vision/automl/docs/export-edge"
      },
      {
        "date": "2022-12-19T15:16:00.000Z",
        "voteCount": 2,
        "content": "A\n\"You want to minimize code development\" -&gt; AutoML\n- https://cloud.google.com/vision/automl/docs/tflite-coreml-ios-tutorial\n- https://cloud.google.com/vertex-ai/docs/training-overview#image"
      },
      {
        "date": "2022-12-17T05:26:00.000Z",
        "voteCount": 2,
        "content": "TensorFlow Lite is a lightweight version of TensorFlow that is optimized for mobile and embedded devices, making it an ideal choice for use in an iOS-based mobile phone application."
      },
      {
        "date": "2022-12-14T05:55:00.000Z",
        "voteCount": 1,
        "content": "I find no answer is 100% right, but D seems closer to the truth."
      },
      {
        "date": "2023-01-04T03:33:00.000Z",
        "voteCount": 1,
        "content": "It's A."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 93,
    "url": "https://www.examtopics.com/discussions/google/view/91575-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You have been asked to build a model using a dataset that is stored in a medium-sized (~10 GB) BigQuery table. You need to quickly determine whether this data is suitable for model development. You want to create a one-time report that includes both informative visualizations of data distributions and more sophisticated statistical analyses to share with other ML engineers on your team. You require maximum flexibility to create your report. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Vertex AI Workbench user-managed notebooks to generate the report.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Google Data Studio to create the report.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the output from TensorFlow Data Validation on Dataflow to generate the report.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Dataprep to create the report."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 20,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 10,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 5,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-06-20T09:18:00.000Z",
        "voteCount": 1,
        "content": "It is a data science request that could be ended on Jupiter notebook"
      },
      {
        "date": "2024-04-14T07:18:00.000Z",
        "voteCount": 2,
        "content": "More Flexbility"
      },
      {
        "date": "2024-02-14T17:57:00.000Z",
        "voteCount": 1,
        "content": "More Flexbility"
      },
      {
        "date": "2023-11-15T08:43:00.000Z",
        "voteCount": 1,
        "content": "Max flexibility"
      },
      {
        "date": "2023-11-11T11:09:00.000Z",
        "voteCount": 2,
        "content": "Looker studio is good too but it does not give the same depth in statistical analysis of the data as using matplotlib, seaborn etc gives on a notebook. So Jupyterlab notebook a.k.a Vertex AI workbench for me"
      },
      {
        "date": "2023-10-22T04:33:00.000Z",
        "voteCount": 1,
        "content": "A as it is a one off report with maximum flexibility. Dont need a dashboard unless being reused"
      },
      {
        "date": "2023-09-24T10:08:00.000Z",
        "voteCount": 2,
        "content": "A for more sophisticated statistical analyses and maximum flexibility"
      },
      {
        "date": "2023-09-10T02:57:00.000Z",
        "voteCount": 1,
        "content": "A (AI workbench): \"sophisticated\""
      },
      {
        "date": "2023-07-23T07:13:00.000Z",
        "voteCount": 1,
        "content": "The answer is A.\n\nB is wrong because you need more sophisticated statistical analyses and maximum flexibility to create your report."
      },
      {
        "date": "2023-07-08T08:20:00.000Z",
        "voteCount": 1,
        "content": "1. one- time\n2. flexibility\ngo for A"
      },
      {
        "date": "2023-07-07T21:41:00.000Z",
        "voteCount": 1,
        "content": "went with A, because of max. flexibility"
      },
      {
        "date": "2023-06-28T07:33:00.000Z",
        "voteCount": 2,
        "content": "Correct Answer A . While Google Data Studio (Option B) is a powerful data visualization and reporting tool, it might not provide the same level of flexibility and sophistication for statistical analyses compared to a notebook environment."
      },
      {
        "date": "2023-05-09T02:29:00.000Z",
        "voteCount": 3,
        "content": "TensorFlow Data Validation(TFDV) can compute descriptive statistics that provide a quick overview of the data in terms of the features that are present and the shapes of their value distributions. Tools such as Facets Overview can provide a succinct visualization of these statistics for easy browsing."
      },
      {
        "date": "2023-04-29T21:48:00.000Z",
        "voteCount": 1,
        "content": "A. Flexibility is the key."
      },
      {
        "date": "2023-04-19T06:18:00.000Z",
        "voteCount": 2,
        "content": "I think has to be B. One of the keys is that it says quickly and BQ makes it very easy to export the query into Looker Studio. The other one is that there's maximum flexibility within the needs for this case (informative visualizations + statistical analysis), as we can develop and write custom formulas.\nA feels like overkill to use a Deep Learning VM Image to only describe data and perform some analysis.\nC also feels overkill to start developping a neural net for that.\nD although you may use Dataprep for this, it is less suited than A"
      },
      {
        "date": "2023-03-31T04:12:00.000Z",
        "voteCount": 1,
        "content": "A se\u00e7ene\u011fini \u00f6neriyorum \u00e7\u00fcnk\u00fc Vertex AI Workbench kullan\u0131c\u0131 y\u00f6netimli not defterleri (user-managed notebooks), BigQuery tablosundaki verilerin analiz edilmesi ve g\u00f6rselle\u015ftirilmesi i\u00e7in daha fazla esneklik ve \u00f6zelle\u015ftirme sa\u011flar. Python k\u00fct\u00fcphaneleri (pandas, matplotlib, seaborn vb.) kullanarak, veri da\u011f\u0131l\u0131mlar\u0131n\u0131n g\u00f6rselle\u015ftirmelerini olu\u015fturabilir ve daha karma\u015f\u0131k istatistiksel analizler ger\u00e7ekle\u015ftirebilirsiniz."
      },
      {
        "date": "2023-03-28T05:09:00.000Z",
        "voteCount": 2,
        "content": "I think it's A.One time report containing real datasets STATISTICAL measurements to tell if the data is suitable for model development. Target audience is also other ML engineers.\nGetting a whole report of exactly this with TFDV/Facets is like two lines of code: https://www.tensorflow.org/tfx/data_validation/get_started\n\nA similar data studio report for this would take lots of time and work, and there would be no benefit from reuseability since task was a one-time job."
      },
      {
        "date": "2023-03-28T05:12:00.000Z",
        "voteCount": 1,
        "content": "Depending on your definition of \"You require maximum flexibility to create your report.\", it could very well be B too."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 94,
    "url": "https://www.examtopics.com/discussions/google/view/91578-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You work on an operations team at an international company that manages a large fleet of on-premises servers located in few data centers around the world. Your team collects monitoring data from the servers, including CPU/memory consumption. When an incident occurs on a server, your team is responsible for fixing it. Incident data has not been properly labeled yet. Your management team wants you to build a predictive maintenance solution that uses monitoring data from the VMs to detect potential failures and then alerts the service desk team. What should you do first?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTrain a time-series model to predict the machines\u2019 performance values. Configure an alert if a machine\u2019s actual performance values significantly differ from the predicted performance values.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement a simple heuristic (e.g., based on z-score) to label the machines\u2019 historical performance data. Train a model to predict anomalies based on this labeled dataset.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDevelop a simple heuristic (e.g., based on z-score) to label the machines\u2019 historical performance data. Test this heuristic in a production environment.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tHire a team of qualified analysts to review and label the machines\u2019 historical performance data. Train a model based on this manually labeled dataset."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 16,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 6,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-17T04:09:00.000Z",
        "voteCount": 9,
        "content": "I would go for C, it is important to have a clear understanding of what constitutes a potential failure and how to detect it. A heuristic based on z-scores, for example, can be used to flag instances where the performance values of a machine significantly differ from its historical baseline."
      },
      {
        "date": "2023-09-13T03:06:00.000Z",
        "voteCount": 3,
        "content": "NOT C: when you have tested something directly in production??\n\nOption B involves labeling historical data using heuristics, which can be a practical and quick way to get started."
      },
      {
        "date": "2023-06-24T07:29:00.000Z",
        "voteCount": 1,
        "content": "Vote for C\nReference: Rule #1: Don\u2019t be afraid to launch a product without machine learning.\nhttps://developers.google.com/machine-learning/guides/rules-of-ml#before_machine_learning"
      },
      {
        "date": "2023-06-14T19:42:00.000Z",
        "voteCount": 2,
        "content": "simple solution goes first, more sophisticated one -- after"
      },
      {
        "date": "2023-05-08T23:22:00.000Z",
        "voteCount": 2,
        "content": "Went with C"
      },
      {
        "date": "2023-04-17T00:12:00.000Z",
        "voteCount": 2,
        "content": "Answer C \nSame as Question  number 139"
      },
      {
        "date": "2023-03-26T11:38:00.000Z",
        "voteCount": 3,
        "content": "What\u2019s the difference between B and C?"
      },
      {
        "date": "2023-05-21T20:15:00.000Z",
        "voteCount": 2,
        "content": "in B you are labeling with heuristics and still develop a model\nin C you follow the ML-rules to adopt simple solution first and later decide if, how and where you need more sophisticated model"
      },
      {
        "date": "2023-03-23T09:24:00.000Z",
        "voteCount": 1,
        "content": "This is the best option for this scenario because it's quick and inexpensive, and it can provide a baseline for labeling the historical performance data. Once we have labeled data, we can train a predictive maintenance model to detect potential failures and alert the service desk team."
      },
      {
        "date": "2023-03-12T04:03:00.000Z",
        "voteCount": 1,
        "content": "why not D ?"
      },
      {
        "date": "2023-03-23T09:28:00.000Z",
        "voteCount": 1,
        "content": "While this approach may result in accurate labeling of the historical performance data, it can be time-consuming and expensive."
      },
      {
        "date": "2023-01-27T21:54:00.000Z",
        "voteCount": 1,
        "content": "https://www.geeksforgeeks.org/z-score-for-outlier-detection-python/"
      },
      {
        "date": "2022-12-19T15:32:00.000Z",
        "voteCount": 3,
        "content": "I vote for B\n- https://developers.google.com/machine-learning/guides/rules-of-ml"
      },
      {
        "date": "2022-12-22T02:57:00.000Z",
        "voteCount": 4,
        "content": "Sorry, I think C is the answer"
      },
      {
        "date": "2023-01-04T18:17:00.000Z",
        "voteCount": 1,
        "content": "C.\nwe need detect potential failures"
      },
      {
        "date": "2023-02-24T12:35:00.000Z",
        "voteCount": 2,
        "content": "Why not B? The team wants to create a model to predict failure. So, the z-score is used to label the failure scenario, for then to use this to build a prediction model."
      },
      {
        "date": "2023-03-23T09:30:00.000Z",
        "voteCount": 2,
        "content": "While this approach may work in some cases, it's not guaranteed to work well in this scenario because we don't know the nature of the anomalies that we want to detect. Therefore, it may be difficult to come up with a heuristic that can accurately label the historical performance data."
      },
      {
        "date": "2023-05-18T06:58:00.000Z",
        "voteCount": 1,
        "content": "But testing the heuristic in a production environment without training a model could be risky and lead to false alarms or misses."
      },
      {
        "date": "2022-12-14T06:36:00.000Z",
        "voteCount": 2,
        "content": "This is really tricky, but it could be A."
      },
      {
        "date": "2023-01-07T07:35:00.000Z",
        "voteCount": 1,
        "content": "Thinking about it, it should be C."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 95,
    "url": "https://www.examtopics.com/discussions/google/view/91326-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You are developing an ML model that uses sliced frames from video feed and creates bounding boxes around specific objects. You want to automate the following steps in your training pipeline: ingestion and preprocessing of data in Cloud Storage, followed by training and hyperparameter tuning of the object model using Vertex AI jobs, and finally deploying the model to an endpoint. You want to orchestrate the entire pipeline with minimal cluster management. What approach should you use?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Kubeflow Pipelines on Google Kubernetes Engine.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Vertex AI Pipelines with TensorFlow Extended (TFX) SDK.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Vertex AI Pipelines with Kubeflow Pipelines SDK.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Composer for the orchestration."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 9,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-02-06T03:01:00.000Z",
        "voteCount": 11,
        "content": "From:\nhttps://cloud.google.com/vertex-ai/docs/pipelines/build-pipeline#sdk \n\"1. If you use TensorFlow in an ML workflow that processes terabytes of structured data or text data, we recommend that you build your pipeline using TFX.\nTo learn more about building a TFX pipeline, follow the TFX getting started tutorials.\nTo learn more about using Vertex AI Pipelines to run a TFX pipeline, follow the TFX on Google Cloud tutorials.\n2. For other use cases, we recommend that you build your pipeline using the Kubeflow Pipelines SDK. By building a pipeline with the Kubeflow Pipelines SDK, you can implement your workflow by building custom components or reusing prebuilt components, such as the Google Cloud Pipeline Components. Google Cloud Pipeline Components make it easier to use Vertex AI services like AutoML in your pipeline.\"\n\nSo I guess since it is image processing, it should be Kubeflow - answer C (TFX is for structured or text data)."
      },
      {
        "date": "2024-09-08T11:49:00.000Z",
        "voteCount": 1,
        "content": "TFX absolutely does support things other than structured and text datasets."
      },
      {
        "date": "2024-09-08T11:51:00.000Z",
        "voteCount": 1,
        "content": "This question is designed to be TFX. It would be a weird thing to do to say \"Vertex pipelines with kubeflow SDK\" because that's just one of the ways to implement stuff in vertex pipelines, which it doesn't normally specify. TFX adds the things in the question on top of the functionality of Vertex."
      },
      {
        "date": "2024-07-22T07:24:00.000Z",
        "voteCount": 2,
        "content": "minimal cluster management should rule out option c. why has everyone chosen that!it should be b"
      },
      {
        "date": "2024-05-11T05:30:00.000Z",
        "voteCount": 3,
        "content": "You have to understand the ML lifecycle and difference between TFX and KFP better here. For ML end to end life cycle TFX is a better option, you can ensure Drift Detection/ Train Serve SKew with TFDV, you can easily perform serving with Tf.Serve and easily integrate TFX with Vertex AI pipeline which runs serverless. All these features are not directly available/managed in KFP ( as its user centric and user managed library).\n\nSo I would go here with B."
      },
      {
        "date": "2024-04-20T03:26:00.000Z",
        "voteCount": 1,
        "content": "If you use TensorFlow in an ML workflow that processes terabytes of structured data or text data, should use TFX. For other use cases, Kubeflow. Link: https://cloud.google.com/vertex-ai/docs/pipelines/build-pipelin"
      },
      {
        "date": "2024-03-03T02:59:00.000Z",
        "voteCount": 2,
        "content": "Overall, using Vertex AI Pipelines with TensorFlow Extended (TFX) SDK provides a comprehensive and managed solution for handling video feed data in an ML pipeline, while minimizing the need for manual infrastructure management and maximizing scalability and efficiency."
      },
      {
        "date": "2023-12-04T06:05:00.000Z",
        "voteCount": 1,
        "content": "I vote for be. the question stated that the minumumn clustering management is required, and I found this on the google study guide\" Vertex AI Pipelines automatically provisions underlying infrastructure and managed it for you\""
      },
      {
        "date": "2023-11-15T09:07:00.000Z",
        "voteCount": 1,
        "content": "minimal managment"
      },
      {
        "date": "2023-05-08T23:22:00.000Z",
        "voteCount": 1,
        "content": "Went with C"
      },
      {
        "date": "2023-03-25T07:42:00.000Z",
        "voteCount": 1,
        "content": "Vertex AI Pipelines with Kubeflow Pipelines SDK provides a high-level interface for building end-to-end machine learning pipelines. This approach allows for easy integration with Google Cloud services, including Cloud Storage for data ingestion and preprocessing, Vertex AI for training and hyperparameter tuning, and deployment to an endpoint. The Kubeflow Pipelines SDK also allows for easy orchestration of the entire pipeline, minimizing cluster management."
      },
      {
        "date": "2023-03-22T11:17:00.000Z",
        "voteCount": 1,
        "content": "Answer is C. If you use TensorFlow in an ML workflow that processes terabytes of structured data or text data, should use TFX. For other use cases, Kubeflow. Link: https://cloud.google.com/vertex-ai/docs/pipelines/build-pipeline"
      },
      {
        "date": "2023-03-09T12:56:00.000Z",
        "voteCount": 1,
        "content": "Answer C...\nhttps://cloud.google.com/architecture/ml-on-gcp-best-practices#use-vertex-pipelines"
      },
      {
        "date": "2023-04-17T00:06:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/architecture/ml-on-gcp-best-practices#use-kubeflow-pipelines-sdk-for-flexible-pipeline-construction"
      },
      {
        "date": "2023-01-27T22:09:00.000Z",
        "voteCount": 2,
        "content": "Google want you to use core native service Pipeline, Don't overthink but , need to think it over.\nThe anwser is in\n https://cloud.google.com/architecture/architecture-for-mlops-using-tfx-kubeflow-pipelines-and-cloud-build\nhttps://cloud.google.com/vertex-ai/docs/pipelines"
      },
      {
        "date": "2023-01-05T05:27:00.000Z",
        "voteCount": 3,
        "content": "\" You want to orchestrate the entire pipeline with minimal cluster management\"\nbecause of that it cant be answer c \ni vote for b, becausse there is no cluster management with vertex ai"
      },
      {
        "date": "2023-05-11T13:20:00.000Z",
        "voteCount": 1,
        "content": "nope, not correct"
      },
      {
        "date": "2022-12-19T15:36:00.000Z",
        "voteCount": 3,
        "content": "C\n\"If you are using other frameworks, we recommend using Kubeflow Pipeline, which is very flexible and allows you to use simple code to construct pipelines. Kubeflow Pipeline also provides Google Cloud pipeline components such as Vertex AI AutoML.\"\n(Journey to Become a Google Cloud Machine Learning Engineer: Build the mind and hand of a Google Certified ML professional)"
      },
      {
        "date": "2022-12-17T03:57:00.000Z",
        "voteCount": 1,
        "content": "vote C"
      },
      {
        "date": "2022-12-17T03:57:00.000Z",
        "voteCount": 1,
        "content": "I vote C.\nhttps://cloud.google.com/vertex-ai/docs/pipelines/build-pipeline"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 96,
    "url": "https://www.examtopics.com/discussions/google/view/92135-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You are training an object detection machine learning model on a dataset that consists of three million X-ray images, each roughly 2 GB in size. You are using Vertex AI Training to run a custom training application on a Compute Engine instance with 32-cores, 128 GB of RAM, and 1 NVIDIA P100 GPU. You notice that model training is taking a very long time. You want to decrease training time without sacrificing model performance. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the instance memory to 512 GB and increase the batch size.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReplace the NVIDIA P100 GPU with a v3-32 TPU in the training job.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable early stopping in your Vertex AI Training job.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the tf.distribute.Strategy API and run a distributed training job."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 26,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 19,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 12,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-01-18T03:45:00.000Z",
        "voteCount": 7,
        "content": "I would say C.\n\nThe question asks about time, so the option \"early stopping\" looks fine because it will no impact the existent accuracy (it will maybe improve it).\n\nThe tf.distribute.Strategy reading the TF docs says that it's used when you want to split training between GPUs, but the question says that we have a single GPU.\n\nOpen to discuss. :)"
      },
      {
        "date": "2023-07-12T07:32:00.000Z",
        "voteCount": 2,
        "content": "tf.distribute.OneDeviceStrategy uses parallel training on one GPU"
      },
      {
        "date": "2024-09-18T22:44:00.000Z",
        "voteCount": 1,
        "content": "since compute engine is being used, seems like GPU upgrade makes sense"
      },
      {
        "date": "2024-09-09T04:06:00.000Z",
        "voteCount": 1,
        "content": "The difficulty of this question is it's pure ambiguity. Two of the answer DO change the hardware, so this is obviously an option. The distribute strategy is clearly the right choice (D) assuming we are allowed more hardware to distribute it over.\nPeople are saying \"we cannot change the hardware so it's B\", but B is a change of hardware to TPU anyway, which would require a code change, at which point D would be implemented anyway."
      },
      {
        "date": "2024-09-06T05:40:00.000Z",
        "voteCount": 1,
        "content": "I have seen two or even 3 of this question and there are strong debates on the answer, I want to suggest D, because Yes, distributed training can work with your setup of 32 cores, 128 GB of RAM, and 1 NVIDIA P100 GPU. However, the efficiency and performance will depend on the specific framework and strategy you use. The important thing about this answer is that it does not affect quality"
      },
      {
        "date": "2024-08-28T21:06:00.000Z",
        "voteCount": 1,
        "content": "in the question it says 3 Million xrays each with 2 GB , it will round upto 6M in size, TPU are exactly designed to accelerate ML tasks and it does massive parallelism, so i would go with B , i would directly omit A , C coz it is more about preventing and not directly aimed at reducing downtime, D is viable solution but comapring with B it is not."
      },
      {
        "date": "2024-06-20T09:22:00.000Z",
        "voteCount": 1,
        "content": "Agree with B"
      },
      {
        "date": "2024-04-24T12:27:00.000Z",
        "voteCount": 2,
        "content": "I would say B:\n\nA. Increse memory doesn't mean necessary a speed up of the process, it's not a batch-size problem\nB. It seems a image -&gt; Tensorflow situation. So transforming image into tensors means that a TPU works better and maybe faster\nC. It's not a overfitting problem\nD. Same here, it's not a memory or input-size problem"
      },
      {
        "date": "2024-04-21T10:31:00.000Z",
        "voteCount": 1,
        "content": "https://www.tensorflow.org/guide/distributed_training#onedevicestrategy"
      },
      {
        "date": "2024-04-21T10:31:00.000Z",
        "voteCount": 1,
        "content": "https://www.tensorflow.org/guide/distributed_training#onedevicestrategy\n-&gt; D"
      },
      {
        "date": "2024-02-29T03:23:00.000Z",
        "voteCount": 2,
        "content": "In my eyes the only solution is distributed training. 3 000 000 x 2GB = 6 Petabytes worth of data. No single device will get you there."
      },
      {
        "date": "2024-02-28T09:09:00.000Z",
        "voteCount": 1,
        "content": "Agree with JamesDoes"
      },
      {
        "date": "2023-11-15T09:17:00.000Z",
        "voteCount": 3,
        "content": "B as it have only one GPU hence in D distributed not efficient"
      },
      {
        "date": "2023-11-14T09:15:00.000Z",
        "voteCount": 1,
        "content": "f the question didn't specify the framework used, and you want to choose an option that is more framework-agnostic, it's important to consider the available options. \n\nGiven the context and the need for a framework-agnostic approach, you might consider a combination of options A and D. Increasing instance memory and batch size can still be beneficial, and if you're using a deep learning framework that supports distributed training (like TensorFlow or PyTorch), implementing distributed training (Option D) can further accelerate the process."
      },
      {
        "date": "2023-11-11T11:01:00.000Z",
        "voteCount": 4,
        "content": "I would go with B as v3-32 TPU offers much more computational power than a single P100 GPU, and this upgrade should provide a substantial decrease in training time.\n\nAlso tf.distributestrategy is good to perform distreibuted training on multiple GPUs or TPUs but the current setup has just one GPU which makes it the second best option provided the architecture uses multiple GPUs.\n\nIncrease in memory may allow large batch size but wont address the fundamental problem which is over utilised GPU\n\nEarly stopping is good for avoiding overfitting when model already starts performing at its best. Its good to reduce overall training time but wont improve the training speed"
      },
      {
        "date": "2023-09-13T03:20:00.000Z",
        "voteCount": 2,
        "content": "Given the options and the goal of decreasing training time, options B (using TPUs) and D (distributed training) are the most effective ways to achieve this goal\n\nC. Enable early stopping in your Vertex AI Training job:\n\nEarly stopping is a technique that can help save training time by monitoring a validation metric and stopping the training process when the metric stops improving. While it can help in terms of stopping unnecessary training runs, it may not provide as substantial a speedup as other options."
      },
      {
        "date": "2023-11-07T12:00:00.000Z",
        "voteCount": 1,
        "content": "TPUs (Tensor Processing Units) are Google's custom-developed application-specific integrated circuits (ASICs) used to accelerate machine learning workloads. They are often faster than GPUs for specific types of computations. However, not all models or training pipelines will benefit from TPUs, and they might require code modification to fully utilize the TPU capabilities."
      },
      {
        "date": "2023-09-10T03:08:00.000Z",
        "voteCount": 1,
        "content": "A. Increase the instance memory to 512 GB and increase the batch size.\n&gt; this will not necessarily decrease training time\nB. Replace the NVIDIA P100 GPU with a v3-32 TPU in the training job. Most Voted\n&gt; TPU can sacrifice performance\nC. Enable early stopping in your Vertex AI Training job. \n&gt; YES, this decreases training time without sacrificing performance, if set properly\nD. Use the tf.distribute.Strategy API and run a distributed training job. \n&gt; No idea .... But I believe the type of machine and architecture cannot be changed as per the wording of the question."
      },
      {
        "date": "2023-11-07T12:01:00.000Z",
        "voteCount": 1,
        "content": "Early stopping is a method that allows you to stop training once the model performance stops improving on a validation dataset. While it can prevent overfitting and save time by stopping unnecessary training epochs, it does not inherently speed up the training process."
      },
      {
        "date": "2023-07-31T06:57:00.000Z",
        "voteCount": 2,
        "content": "Option D, using the tf.distribute.Strategy API for distributed training, can be beneficial for improving training efficiency, but it would require additional resources and complexity to set up compared to simply using a TPU.\n\nTherefore, replacing the NVIDIA P100 GPU with a v3-32 TPU in the Vertex AI Training job would be the most effective way to decrease training time while maintaining or even improving model performance"
      },
      {
        "date": "2023-07-21T00:05:00.000Z",
        "voteCount": 4,
        "content": "I don't understand why so many people are voting for D (tf.distribute.Strategy API). If we look at our training infrastructure, we can see the bottleneck is obviously the GPU, which has 12GB or 16GB memory depending on the model (https://www.leadtek.com/eng/products/ai_hpc(37)/tesla_p100(761)/detail). This means we can afford to have a batch size of only 6-8 images (2GB each) even if we assume the GPU is utilized 100%. And remember the training size is 3M, which means each epoch will have 375-500K steps in the best case.\n\nWith 32-cores and 128GB memory, we are able to afford higher batch sizes (e.g., 32), so moving to TPU will accelerate the training.\n\nA is wrong because we can't afford a larger batch size with the current GPU. D is wrong because you don't have multiple GPUs and your current GPU is saturated. C is a viable option, but it seems less optimal than B."
      },
      {
        "date": "2023-07-25T10:04:00.000Z",
        "voteCount": 1,
        "content": "I should note that the batch size should be lower than even 6-8 images because the model weights will also take the GPU memory."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 97,
    "url": "https://www.examtopics.com/discussions/google/view/91897-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You are a data scientist at an industrial equipment manufacturing company. You are developing a regression model to estimate the power consumption in the company\u2019s manufacturing plants based on sensor data collected from all of the plants. The sensors collect tens of millions of records every day. You need to schedule daily training runs for your model that use all the data collected up to the current date. You want your model to scale smoothly and require minimal development work. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTrain a regression model using AutoML Tables.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDevelop a custom TensorFlow regression model, and optimize it using Vertex AI Training.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDevelop a custom scikit-learn regression model, and optimize it using Vertex AI Training.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDevelop a regression model using BigQuery ML.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 26,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 12,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-04-13T04:28:00.000Z",
        "voteCount": 13,
        "content": "The key is to understand the amount of data that needs to be used for training - the sensor collects tens of millions of records every day and the model needs to use all the data up to the current date. \nThere is a limitation for AutoML is 100M rows -&gt; https://cloud.google.com/vertex-ai/docs/tabular-data/classification-regression/prepare-data"
      },
      {
        "date": "2024-04-22T06:22:00.000Z",
        "voteCount": 1,
        "content": "There is a limitation for AutoML is 100M rows -&gt; https://cloud.google.com/vertex-ai/docs/tabular-data/classification-regression/prepare-data"
      },
      {
        "date": "2023-12-12T06:55:00.000Z",
        "voteCount": 2,
        "content": "I go for A"
      },
      {
        "date": "2024-04-22T06:22:00.000Z",
        "voteCount": 1,
        "content": "There is a limitation for AutoML is 100M rows -&gt; https://cloud.google.com/vertex-ai/docs/tabular-data/classification-regression/prepare-data"
      },
      {
        "date": "2023-11-15T09:26:00.000Z",
        "voteCount": 2,
        "content": "Either A or D . Since not stated where is sensor data stored . hence go for A"
      },
      {
        "date": "2023-06-28T08:15:00.000Z",
        "voteCount": 1,
        "content": "Ans D. BigQuery ML allows you to schedule daily training runs by incorporating the latest data collected up to the current date. By specifying the appropriate SQL query, you can include all the relevant data in the training process, ensuring that your model is updated regularly."
      },
      {
        "date": "2023-11-10T00:51:00.000Z",
        "voteCount": 1,
        "content": "it says  \"use all the data collected up to the current date\" not a just a selection of \"relevant\" (?!) data"
      },
      {
        "date": "2023-05-09T05:19:00.000Z",
        "voteCount": 3,
        "content": "I would go with A because it states that it requires minimal development work. Not sure tho, correct me if I\u2019m wrong"
      },
      {
        "date": "2023-05-08T23:23:00.000Z",
        "voteCount": 1,
        "content": "Went with D"
      },
      {
        "date": "2023-03-28T06:03:00.000Z",
        "voteCount": 3,
        "content": "Old question, the quotas were removed when they moved AutoML into VertexAI.\nhttps://cloud.google.com/vertex-ai/docs/quotas#model_quotas#tabular"
      },
      {
        "date": "2023-03-17T22:49:00.000Z",
        "voteCount": 1,
        "content": "Would go w A given the specifics mentioned in question. \n\nBigQuery is an unnecessary distraction IMO (e.g. why would we assume BigQuery and not BigTable!)"
      },
      {
        "date": "2023-03-09T12:41:00.000Z",
        "voteCount": 2,
        "content": "Answer D\nhttps://cloud.google.com/blog/products/data-analytics/automl-tables-now-generally-available-bigquery-ml\n\nThis legacy version of AutoML Tables is deprecated and will no longer be available on Google Cloud after January 23, 2024. All the functionality of legacy AutoML Tables and new features are available on the Vertex AI platform. See Migrate to Vertex AI to learn how to migrate your resources."
      },
      {
        "date": "2023-02-21T15:34:00.000Z",
        "voteCount": 1,
        "content": "You require minimal development work and the question doesn't mention if your data is stored in BQ"
      },
      {
        "date": "2023-01-09T00:43:00.000Z",
        "voteCount": 3,
        "content": "Answer is D, AutoML has 200M rows as limits"
      },
      {
        "date": "2023-01-08T06:49:00.000Z",
        "voteCount": 1,
        "content": "A and D seem both good, but A works better, for me."
      },
      {
        "date": "2022-12-29T05:25:00.000Z",
        "voteCount": 2,
        "content": "But BQML also has limits on training data\nhttps://cloud.google.com/bigquery-ml/quotas"
      },
      {
        "date": "2022-12-19T16:29:00.000Z",
        "voteCount": 3,
        "content": "Vote for D\nA dosen't work because AutoML has limits on training data\n- https://www.examtopics.com/exams/google/professional-machine-learning-engineer/view/10/"
      },
      {
        "date": "2023-01-07T05:47:00.000Z",
        "voteCount": 1,
        "content": "Wrong. The limit is 200 M records. We have 10M records. see:\nhttps://cloud.google.com/automl-tables/docs/quotas"
      },
      {
        "date": "2023-03-21T19:06:00.000Z",
        "voteCount": 2,
        "content": "it's more than 10M. the training needs to use all the data collected up to the current date"
      },
      {
        "date": "2022-12-17T03:12:00.000Z",
        "voteCount": 3,
        "content": "BigQuery ML can scale smoothly and requires minimal development work.\nModel can be build using SQL queries rather than writing custom code."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 98,
    "url": "https://www.examtopics.com/discussions/google/view/91896-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You built a custom ML model using scikit-learn. Training time is taking longer than expected. You decide to migrate your model to Vertex AI Training, and you want to improve the model\u2019s training time. What should you try out first?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate your model to TensorFlow, and train it using Vertex AI Training.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTrain your model in a distributed mode using multiple Compute Engine VMs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTrain your model with DLVM images on Vertex AI, and ensure that your code utilizes NumPy and SciPy internal methods whenever possible.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTrain your model using Vertex AI Training with GPUs."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 18,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 5,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-11-14T09:36:00.000Z",
        "voteCount": 2,
        "content": "Options B and C may also be relevant in certain scenarios, but they are generally more involved and might require additional considerations. Option B can be effective for large-scale training tasks, but it might add complexity and overhead. Option C could be helpful, but the impact on training time might not be as immediate and substantial as using GPUs."
      },
      {
        "date": "2023-09-13T03:32:00.000Z",
        "voteCount": 1,
        "content": "D: Training your model with GPUs can provide a substantial speedup, especially for deep learning models or models that require a lot of computation. This option is likely to have a significant impact on training time.\n\nNOT C: While optimizing code can help improve training time to some extent, it may not provide as significant a speedup as the other options. However, it's still a good practice to optimize your code."
      },
      {
        "date": "2023-09-10T03:15:00.000Z",
        "voteCount": 3,
        "content": "I dont think scikit-learn would support GPU or distribution, so based on \"What should you try out first?\" I think &gt; C. Train your model with DLVM images on Vertex AI, and ensure that your code utilizes NumPy and SciPy internal methods whenever possible."
      },
      {
        "date": "2023-07-09T13:51:00.000Z",
        "voteCount": 3,
        "content": "why not B? Vertex AI provides the ability to distribute training tasks across multiple Compute Engine VMs, which can parallelize the workload and significantly reduce the training time for large datasets and complex models."
      },
      {
        "date": "2023-06-28T08:25:00.000Z",
        "voteCount": 1,
        "content": "Option D is not the optimal choice for a scikit-learn model since scikit-learn does not have native GPU support. Option C, training with DLVM images on Vertex AI and optimizing code with NumPy and SciPy, would be more appropriate in your scenario."
      },
      {
        "date": "2023-06-28T08:23:00.000Z",
        "voteCount": 1,
        "content": "Ans - D.  quickest improvement in training time with minimal modifications to your existing scikit-learn model, trying out Option D and training your model using Vertex AI Training with GPUs is the recommended first step."
      },
      {
        "date": "2023-05-18T06:26:00.000Z",
        "voteCount": 4,
        "content": "A) Migrate your model to TensorFlow, and train it using Vertex AI Training.\nNot the first thing to do.\nB) Train your model in a distributed mode using multiple Compute Engine VMs.\nCould be not easy and fast.\nD)Train your model using Vertex AI Training with GPUs\nsklearn does not support GPUs\n\nAlso, most of scikit-learn assumes data is in NumPy arrays or SciPy sparse matrices of a single numeric dtype.\nI choose C as the correct answer."
      },
      {
        "date": "2023-05-08T23:23:00.000Z",
        "voteCount": 1,
        "content": "Went with C"
      },
      {
        "date": "2023-04-16T23:59:00.000Z",
        "voteCount": 1,
        "content": "Answer C"
      },
      {
        "date": "2023-02-24T13:56:00.000Z",
        "voteCount": 1,
        "content": "How about using sklearn's multi-core? Considering multiple jobs, could we choose item B?\nhttps://machinelearningmastery.com/multi-core-machine-learning-in-python/"
      },
      {
        "date": "2023-02-08T21:07:00.000Z",
        "voteCount": 1,
        "content": "https://scikit-learn.org/stable/faq.html#will-you-add-gpu-support"
      },
      {
        "date": "2023-01-28T06:34:00.000Z",
        "voteCount": 1,
        "content": "C is correct absolutely\nhttps://console.cloud.google.com/marketplace/details/click-to-deploy-images/deeplearning?_ga=2.139171125.787784554.1674450530-1146240914.1659613735&amp;project=quantum-hash-240404"
      },
      {
        "date": "2023-01-07T06:05:00.000Z",
        "voteCount": 4,
        "content": "Scikit learn does not support GPU:s\nhttps://scikit-learn.org/stable/faq.html#will-you-add-gpu-support"
      },
      {
        "date": "2022-12-27T01:54:00.000Z",
        "voteCount": 1,
        "content": "C\n\nNo D \nhttps://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers?hl=ko#scikit-learn"
      },
      {
        "date": "2022-12-21T01:59:00.000Z",
        "voteCount": 3,
        "content": "GPU is not useful for sciki-learn model\nhttps://scikit-learn.org/stable/faq.html#will-you-add-gpu-support\nbut DLVM did mention it is support scikit-learn framework\nhttps://cloud.google.com/deep-learning-vm"
      },
      {
        "date": "2022-12-19T17:00:00.000Z",
        "voteCount": 1,
        "content": "D (not sure)\n- https://cloud.google.com/vertex-ai/docs/training/code-requirements#gpus"
      },
      {
        "date": "2022-12-25T12:36:00.000Z",
        "voteCount": 2,
        "content": "Changind my vote to C"
      },
      {
        "date": "2022-12-17T03:09:00.000Z",
        "voteCount": 1,
        "content": "Training a machine learning model on a GPU can significantly improve the training time compared to training on a CPU."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 99,
    "url": "https://www.examtopics.com/discussions/google/view/91884-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You are an ML engineer at a travel company. You have been researching customers\u2019 travel behavior for many years, and you have deployed models that predict customers\u2019 vacation patterns. You have observed that customers\u2019 vacation destinations vary based on seasonality and holidays; however, these seasonal variations are similar across years. You want to quickly and easily store and compare the model versions and performance statistics across years. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the performance statistics in Cloud SQL. Query that database to compare the performance statistics across the model versions.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate versions of your models for each season per year in Vertex AI. Compare the performance statistics across the models in the Evaluate tab of the Vertex AI UI.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the performance statistics of each pipeline run in Kubeflow under an experiment for each season per year. Compare the results across the experiments in the Kubeflow UI.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the performance statistics of each version of your models using seasons and years as events in Vertex ML Metadata. Compare the results across the slices.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 10,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 7,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-04-26T11:19:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/vertex-ai/docs/model-registry/versioning\nModel versioning lets you create multiple versions of the same model. With model versioning, you can organize your models in a way that helps navigate and understand which changes had what effect on the models. With Vertex AI Model Registry you can view your models and all of their versions in a single view. You can drill down into specific model versions and see exactly how they performed."
      },
      {
        "date": "2024-04-21T02:27:00.000Z",
        "voteCount": 1,
        "content": "agree with pico"
      },
      {
        "date": "2023-11-15T09:38:00.000Z",
        "voteCount": 1,
        "content": "either B or D so leaning towards B"
      },
      {
        "date": "2023-09-13T03:41:00.000Z",
        "voteCount": 2,
        "content": "Vertex AI provides a managed environment for machine learning, and creating model versions for each season per year is a structured way to organize and compare models. You can use the Evaluate tab to compare performance metrics easily. This approach is well-suited for the task."
      },
      {
        "date": "2023-09-13T03:42:00.000Z",
        "voteCount": 1,
        "content": "not D: \n\nVertex ML Metadata is designed for tracking metadata and lineage in machine learning pipelines. While it can store model version information and performance statistics, it might not provide as straightforward a way to compare models across years and seasons as Vertex AI's model versioning and evaluation tools."
      },
      {
        "date": "2023-09-10T03:28:00.000Z",
        "voteCount": 2,
        "content": "I absolutely do not master this topicm but I would say correct answer is D.\nIt does not sound right to systematically create versions of a model beased on seasonality, if the model has not changed. \"Events\" in metadata sound right."
      },
      {
        "date": "2023-06-28T08:36:00.000Z",
        "voteCount": 2,
        "content": "Ans D- With Vertex ML Metadata, you can store the performance statistics of each version of your models as events. You can associate these events with specific seasons and years, making it easy to organize and retrieve the data based on the relevant time periods. By storing performance statistics as events, you can capture the necessary information for comparing model versions across years."
      },
      {
        "date": "2023-06-05T05:55:00.000Z",
        "voteCount": 1,
        "content": "D. Store the performance statistics of each version of your models using seasons and years as events in Vertex ML Metadata. Compare the results across the slices.\nhttps://cloud.google.com/vertex-ai/docs/ml-metadata/analyzing#filtering\nWhich versions of a trained model achieved a certain quality threshold?"
      },
      {
        "date": "2023-09-13T03:45:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/vertex-ai/docs/evaluation/using-model-evaluation#console"
      },
      {
        "date": "2023-05-08T23:24:00.000Z",
        "voteCount": 1,
        "content": "Went with D"
      },
      {
        "date": "2023-05-29T00:34:00.000Z",
        "voteCount": 1,
        "content": "why choose D instead of B?"
      },
      {
        "date": "2023-05-08T23:04:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/vertex-ai/docs/model-registry/versioning\nModel versioning lets you create multiple versions of the same model. With model versioning, you can organize your models in a way that helps navigate and understand which changes had what effect on the models. With Vertex AI Model Registry you can view your models and all of their versions in a single view. You can drill down into specific model versions and see exactly how they performed."
      },
      {
        "date": "2023-03-17T23:01:00.000Z",
        "voteCount": 1,
        "content": "You can compare evaluation results across different models, model versions, and evaluation jobs --&gt; https://cloud.google.com/vertex-ai/docs/evaluation/using-model-evaluation\n\nMetadata mgmt has a very different purpose"
      },
      {
        "date": "2023-03-09T12:31:00.000Z",
        "voteCount": 1,
        "content": "Answer D"
      },
      {
        "date": "2022-12-19T17:04:00.000Z",
        "voteCount": 2,
        "content": "D\n- https://cloud.google.com/vertex-ai/docs/ml-metadata/introduction"
      },
      {
        "date": "2022-12-17T03:06:00.000Z",
        "voteCount": 2,
        "content": "Vote D. It is easy to compare via Vertex ML Metadata UI the performance statistics across the different slices and see how the model performance varies over time."
      },
      {
        "date": "2022-12-17T02:00:00.000Z",
        "voteCount": 1,
        "content": "i think it is D"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 100,
    "url": "https://www.examtopics.com/discussions/google/view/91724-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You are an ML engineer at a manufacturing company. You need to build a model that identifies defects in products based on images of the product taken at the end of the assembly line. You want your model to preprocess the images with lower computation to quickly extract features of defects in products. Which approach should you use to build the model?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReinforcement learning",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRecommender system",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRecurrent Neural Networks (RNN)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConvolutional Neural Networks (CNN)\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-02T06:39:00.000Z",
        "voteCount": 2,
        "content": "CNN is commonly used for image classifications"
      },
      {
        "date": "2023-05-18T06:34:00.000Z",
        "voteCount": 1,
        "content": "D for sure"
      },
      {
        "date": "2023-05-08T23:24:00.000Z",
        "voteCount": 1,
        "content": "Went with D"
      },
      {
        "date": "2023-03-09T12:30:00.000Z",
        "voteCount": 1,
        "content": "Answer D"
      },
      {
        "date": "2023-02-24T15:52:00.000Z",
        "voteCount": 1,
        "content": "CNNs commonly used for image classification and recognition tasks."
      },
      {
        "date": "2023-02-21T15:59:00.000Z",
        "voteCount": 1,
        "content": "CNN scenario"
      },
      {
        "date": "2023-02-08T21:10:00.000Z",
        "voteCount": 1,
        "content": "best way"
      },
      {
        "date": "2022-12-19T17:08:00.000Z",
        "voteCount": 1,
        "content": "D\nCNN is good for images processing\n- https://developers.google.com/machine-learning/practica/image-classification/convolutional-neural-networks"
      },
      {
        "date": "2022-12-15T09:03:00.000Z",
        "voteCount": 2,
        "content": "Obviously D."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 101,
    "url": "https://www.examtopics.com/discussions/google/view/91805-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You are developing an ML model intended to classify whether X-ray images indicate bone fracture risk. You have trained a ResNet architecture on Vertex AI using a TPU as an accelerator, however you are unsatisfied with the training time and memory usage. You want to quickly iterate your training code but make minimal changes to the code. You also want to minimize impact on the model\u2019s accuracy. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReduce the number of layers in the model architecture.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReduce the global batch size from 1024 to 256.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReduce the dimensions of the images used in the model.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure your model to use bfloat16 instead of float32.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-17T03:00:00.000Z",
        "voteCount": 7,
        "content": "i think should be D\nhttps://cloud.google.com/tpu/docs/bfloat16"
      },
      {
        "date": "2024-04-25T23:24:00.000Z",
        "voteCount": 2,
        "content": "Configuring bfloat16 instead of float32 (D): This offers a good balance between speed, memory usage, and minimal code changes. Bfloat16 uses 16 bits per float value compared to 32 bits for float32.\n\npen_spark\nexpand_more This can significantly reduce memory usage while maintaining similar accuracy in many machine learning models, especially for image recognition tasks.expand_more It's a quick change with minimal impact on the code and potentially large gains in training speed."
      },
      {
        "date": "2024-04-14T22:11:00.000Z",
        "voteCount": 1,
        "content": "\"the Google hardware team chose bfloat16 for Cloud TPUs to improve hardware efficiency while maintaining the ability to train deep learning models accurately, all with minimal switching costs from float32\""
      },
      {
        "date": "2023-09-13T04:26:00.000Z",
        "voteCount": 1,
        "content": "while reducing the global batch size (Option B) and configuring your model to use bfloat16 (Option D) are both valid options, reducing the global batch size is typically a safer and more straightforward choice to quickly iterate and make minimal changes to your code while still achieving reasonable model performance."
      },
      {
        "date": "2023-09-13T04:28:00.000Z",
        "voteCount": 1,
        "content": "Why not D:\nNumerical Precision: bfloat16 has a lower numerical precision compared to float32\nCompatibility: Not all machine learning frameworks and libraries support bfloat16 natively.\nHyperparameter Tuning: When switching to bfloat16, you may need to adjust hyperparameters, such as learning rates and gradient clipping thresholds, to accommodate the lower numerical precision\nModel Architecture: Some model architectures and layers may be more sensitive to reduced precision than others."
      },
      {
        "date": "2023-11-10T00:20:00.000Z",
        "voteCount": 2,
        "content": "TPUs are optimized for operations with bfloat16 data types. By switching from float32 to bfloat16, you can benefit from the TPU's hardware acceleration capabilities, leading to faster computation and reduced memory usage without significant changes to your code.\n\nWhile bfloat16 offers a lower precision compared to float32, it maintains a similar dynamic range. This means that the reduction in numerical precision is unlikely to have a substantial impact on the accuracy of your model, especially in the context of image classification tasks like bone fracture risk assessment in X-rays.\n\nWhile reducing the batch size can decrease memory usage, it can also affect the model's convergence and accuracy. Additionally, TPUs are highly efficient with large batch sizes, so reducing the batch size might not fully leverage the TPU's capabilities."
      },
      {
        "date": "2023-06-05T06:02:00.000Z",
        "voteCount": 1,
        "content": "I think it should be D since they are using a TPU.https://cloud.google.com/tpu/docs/bfloat16"
      },
      {
        "date": "2023-05-08T23:25:00.000Z",
        "voteCount": 1,
        "content": "Went with D"
      },
      {
        "date": "2023-03-27T10:29:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/tpu/docs/bfloat16"
      },
      {
        "date": "2023-03-09T12:29:00.000Z",
        "voteCount": 2,
        "content": "Answer D"
      },
      {
        "date": "2023-02-23T10:35:00.000Z",
        "voteCount": 2,
        "content": "\"the Google hardware team chose bfloat16 for Cloud TPUs to improve hardware efficiency while maintaining the ability to train deep learning models accurately, all with minimal switching costs from float32\" so since its already trained on TPU, D maybe has no effect?"
      },
      {
        "date": "2023-01-16T07:21:00.000Z",
        "voteCount": 2,
        "content": "I go with D exactly, primarily. the rest don't make any sense at all"
      },
      {
        "date": "2023-01-03T08:22:00.000Z",
        "voteCount": 1,
        "content": "It should be D."
      },
      {
        "date": "2022-12-19T17:15:00.000Z",
        "voteCount": 2,
        "content": "D\nAgree with  mymy9418"
      },
      {
        "date": "2022-12-18T13:04:00.000Z",
        "voteCount": 1,
        "content": "Agree with D"
      },
      {
        "date": "2022-12-16T03:34:00.000Z",
        "voteCount": 1,
        "content": "It should be B."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 102,
    "url": "https://www.examtopics.com/discussions/google/view/92165-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You have successfully deployed to production a large and complex TensorFlow model trained on tabular data. You want to predict the lifetime value (LTV) field for each subscription stored in the BigQuery table named subscription. subscriptionPurchase in the project named my-fortune500-company-project.<br><br>You have organized all your training code, from preprocessing data from the BigQuery table up to deploying the validated model to the Vertex AI endpoint, into a TensorFlow Extended (TFX) pipeline. You want to prevent prediction drift, i.e., a situation when a feature data distribution in production changes significantly over time. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement continuous retraining of the model daily using Vertex AI Pipelines.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a model monitoring job where 10% of incoming predictions are sampled 24 hours.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a model monitoring job where 90% of incoming predictions are sampled 24 hours.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a model monitoring job where 10% of incoming predictions are sampled every hour."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 16,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-04-02T09:24:00.000Z",
        "voteCount": 2,
        "content": "You need to monitor it first and foremost to see if there is a drift and if there is then a measure can be devised. training every date is an over kill."
      },
      {
        "date": "2023-09-25T01:25:00.000Z",
        "voteCount": 3,
        "content": "Continuous Retraining: Continuously retraining the model allows it to adapt to changes in the data distribution, helping to mitigate prediction drift. Daily retraining provides a good balance between staying up-to-date and avoiding excessive retraining.\nOptions B, C, and D involve model monitoring but do not address the issue of keeping the model updated with the changing data distribution. Monitoring alone can help you detect drift, but it does not actively prevent it. Retraining the model is necessary to address drift effectively."
      },
      {
        "date": "2023-10-23T01:26:00.000Z",
        "voteCount": 1,
        "content": "Option A can prevent drift prediction. All the other options can only detect. \nTherefore the correct answer is A unless it is possible to monitor drifts and then remediate without retrainings."
      },
      {
        "date": "2024-01-04T09:03:00.000Z",
        "voteCount": 2,
        "content": "Follow me on X (twitter): @nbcodes for more useful tips.\n\nI think you're slightly missing the point, the answer should be B, let me explain why..\n\nThe whole point of this question is to come up with a PREVENTATIVE way of handling prediction drift so you need to find a way to DETECT the drift before it occurs, this is exactly what solution B does and ensures it's done in a way that is not too frequent i.e D and not too resource intensive with the large sample i.e C remember if sampling is done well you don't need 90% of the data to detect drift.\n\nSolution A suggests retraining every day which is a CRAZY proposal, why would you retrain every day even if you don't know if your data is drifting?? Huge waste of resources and time."
      },
      {
        "date": "2023-05-08T23:25:00.000Z",
        "voteCount": 1,
        "content": "Went with B"
      },
      {
        "date": "2023-03-27T10:31:00.000Z",
        "voteCount": 4,
        "content": "Continuous retraining (option A) is not necessarily the best solution for preventing prediction drift, as it can be time-consuming and expensive. Instead, monitoring the performance of the model in production is a better approach. Option B is a good choice because it samples a small percentage of incoming predictions and checks for any significant changes in the feature data distribution over a 24-hour period. This allows you to detect any drift and take appropriate action to address it before it affects the model's performance. Options C and D are less effective because they either sample too many or too few predictions and/or at too frequent intervals."
      },
      {
        "date": "2023-09-10T03:37:00.000Z",
        "voteCount": 1,
        "content": "I am just not sure why sampling too few (10%) is important. Is this a costly service?"
      },
      {
        "date": "2023-11-10T00:26:00.000Z",
        "voteCount": 1,
        "content": "Model monitoring, especially at a large scale, can consume significant computational resources. Sampling a smaller percentage of predictions (like 10%) helps manage these resource demands and associated costs. The more predictions you sample, the more storage, computation, and network resources you'll need to analyze the data, potentially increasing the cost.\n\nIn many cases, a 10% sample of the data can provide statistically significant insights into the model's performance and the presence of drift. It's a balancing act between getting enough data to make informed decisions and not overburdening the system.\n\nIn some datasets, especially large ones, a lot of the data might be redundant or not particularly informative. Sampling a smaller fraction can help filter out noise and focus on the most relevant information."
      },
      {
        "date": "2023-11-14T10:13:00.000Z",
        "voteCount": 1,
        "content": "Neither B,C or D have a step to prevent the prediction drift. \n\nThe question says: \"you want to prevent prediction drift\""
      },
      {
        "date": "2023-03-09T12:29:00.000Z",
        "voteCount": 1,
        "content": "Answer B"
      },
      {
        "date": "2023-02-07T23:02:00.000Z",
        "voteCount": 3,
        "content": "B , I got it from Machine Learning in the Enterprise course for google partnet skillboost\nyou can watch cafully on video \"Model management using Vertex AI\"\nI imply that it is default setting on typical case."
      },
      {
        "date": "2023-01-07T06:25:00.000Z",
        "voteCount": 1,
        "content": "Using 10% of hourly requests would yield a better distribution and faster feed back loop"
      },
      {
        "date": "2022-12-27T20:34:00.000Z",
        "voteCount": 2,
        "content": "I think it is B, we can say 10% to be a sample but not 90%"
      },
      {
        "date": "2022-12-21T02:01:00.000Z",
        "voteCount": 3,
        "content": "I guess 10% of 24 hours should be good enough?"
      },
      {
        "date": "2022-12-20T03:36:00.000Z",
        "voteCount": 2,
        "content": "B (not sure)\n- https://cloud.google.com/vertex-ai/docs/model-monitoring/overview\n- https://cloud.google.com/vertex-ai/docs/model-monitoring/using-model-monitoring#drift-detection"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 103,
    "url": "https://www.examtopics.com/discussions/google/view/92019-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You recently developed a deep learning model using Keras, and now you are experimenting with different training strategies. First, you trained the model using a single GPU, but the training process was too slow. Next, you distributed the training across 4 GPUs using tf.distribute.MirroredStrategy (with no other changes), but you did not observe a decrease in training time. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDistribute the dataset with tf.distribute.Strategy.experimental_distribute_dataset",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a custom training loop.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a TPU with tf.distribute.TPUStrategy.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the batch size.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 21,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 10,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-27T00:11:00.000Z",
        "voteCount": 10,
        "content": "Ans D: Check this link https://www.tensorflow.org/guide/gpu_performance_analysis for details on how to Optimize the performance on the multi-GPU single host"
      },
      {
        "date": "2024-04-14T22:18:00.000Z",
        "voteCount": 2,
        "content": "when using tf.distribute.MirroredStrategy, TensorFlow automatically takes care of distributing the dataset across the available devices (GPUs in this case).\n\nTo make sure that the data is efficiently distributed across the GPUs, you should increase the global batch size. This ensures that each GPU receives a larger batch of data to process, effectively utilizing the additional computational power. The global batch size is the sum of the batch sizes for all devices. For example, if you had a batch size of 64 for a single GPU, you would set the global batch size to 256 (64 * 4) when using 4 GPUs."
      },
      {
        "date": "2023-11-14T10:17:00.000Z",
        "voteCount": 3,
        "content": "When you distribute the training across multiple GPUs using tf.distribute.MirroredStrategy, the training time may not decrease if the dataset loading and preprocessing become a bottleneck. In this case, option A, distributing the dataset with tf.distribute.Strategy.experimental_distribute_dataset, can help improve the performance."
      },
      {
        "date": "2023-11-14T10:18:00.000Z",
        "voteCount": 1,
        "content": "option D can be a reasonable step to try, but it's important to carefully monitor the training process, consider memory constraints, and assess the impact on model performance. It might be a good idea to try both option A (distributing the dataset) and option D (increasing the batch size) to see if there is any improvement in training time."
      },
      {
        "date": "2023-07-31T07:33:00.000Z",
        "voteCount": 1,
        "content": "A. Distribute the dataset with tf.distribute.Strategy.experimental_distribute_dataset\nWhen you distribute the training across multiple GPUs using tf.distribute.MirroredStrategy, you need to make sure that the data is also distributed across the GPUs to fully utilize the computational power. By default, the tf.distribute.MirroredStrategy replicates the model and uses synchronous training, but it does not automatically distribute the dataset across the GPUs."
      },
      {
        "date": "2023-11-10T00:38:00.000Z",
        "voteCount": 1,
        "content": "You are right,  However, when using tf.distribute.MirroredStrategy, TensorFlow automatically takes care of distributing the dataset across the available devices (GPUs in this case).\n\nTo make sure that the data is efficiently distributed across the GPUs, you should increase the global batch size. This ensures that each GPU receives a larger batch of data to process, effectively utilizing the additional computational power. The global batch size is the sum of the batch sizes for all devices. For example, if you had a batch size of 64 for a single GPU, you would set the global batch size to 256 (64 * 4) when using 4 GPUs."
      },
      {
        "date": "2023-05-09T01:23:00.000Z",
        "voteCount": 2,
        "content": "When going from training with a single GPU to multiple GPUs on the same host, ideally you should experience the performance scaling with only the additional overhead of gradient communication and increased host thread utilization. Because of this overhead, you will not have an exact 2x speedup if you move from 1 to 2 GPUs.\n\nTry to maximize the batch size, which will lead to higher device utilization and amortize the costs of communication across multiple GPUs. Using the memory profiler helps get a sense of how close your program is to peak memory utilization. Note that while a higher batch size can affect convergence, this is usually outweighed by the performance benefits."
      },
      {
        "date": "2023-05-08T23:25:00.000Z",
        "voteCount": 1,
        "content": "Went with D"
      },
      {
        "date": "2023-03-27T10:39:00.000Z",
        "voteCount": 3,
        "content": "If distributing the training across multiple GPUs did not result in a decrease in training time, the issue may be related to the batch size being too small. When using multiple GPUs, each GPU gets a smaller portion of the batch size, which can lead to slower training times due to increased communication overhead. Therefore, increasing the batch size can help utilize the GPUs more efficiently and speed up training."
      },
      {
        "date": "2023-03-09T12:28:00.000Z",
        "voteCount": 1,
        "content": "Answer D"
      },
      {
        "date": "2023-02-08T00:03:00.000Z",
        "voteCount": 4,
        "content": "D: it is best  https://www.tensorflow.org/guide/distributed_training#use_tfdistributestrategy_with_keras_modelfit\nEach epoch will then train faster as you add more GPUs. Typically, you would want to increase your batch size as you add more accelerators,\nC is rule out because of GPU\nA and B   , as reading on https://www.tensorflow.org/guide/distributed_training#use_tfdistributestrategy_with_custom_training_loops   To use custom loop , we have   call  If you are writing a custom training loop, you will need to call a few more methods, see the guide:\n\nStart by creating a tf.data.Dataset normally.\nUse tf.distribute.Strategy.experimental_distribute_dataset to convert a tf.data.Dataset to something that produces \"per-replica\" values. If you want to\nhttps://www.tensorflow.org/api_docs/python/tf/distribute/Strategy"
      },
      {
        "date": "2023-01-05T06:30:00.000Z",
        "voteCount": 1,
        "content": "To speed up the training of the deep learning model,  increasing the batch size. When using multiple GPUs with tf.distribute.MirroredStrategy, increasing the batch size can help to better utilize the additional GPUs and potentially reduce the training time. This is because larger batch sizes allow each GPU to process more data in parallel, which can help to improve the efficiency of the training process."
      },
      {
        "date": "2023-01-03T08:16:00.000Z",
        "voteCount": 1,
        "content": "TPUs are Google's specialized ASICs designed to dramatically accelerate machine learning workloads. Hence it should be C."
      },
      {
        "date": "2022-12-29T19:12:00.000Z",
        "voteCount": 1,
        "content": "I think it's D"
      },
      {
        "date": "2022-12-20T19:22:00.000Z",
        "voteCount": 4,
        "content": "I think its A"
      },
      {
        "date": "2022-12-20T04:03:00.000Z",
        "voteCount": 1,
        "content": "B (not sure)\n- https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch\n-https://www.tensorflow.org/guide/distributed_training#use_tfdistributestrategy_with_custom_training_loops"
      },
      {
        "date": "2022-12-28T04:08:00.000Z",
        "voteCount": 1,
        "content": "Sorry, ans D (by ediaa link)"
      },
      {
        "date": "2022-12-27T06:03:00.000Z",
        "voteCount": 1,
        "content": "It's should A"
      },
      {
        "date": "2022-12-18T12:46:00.000Z",
        "voteCount": 3,
        "content": "I think it's A, \n\nhttps://www.tensorflow.org/api_docs/python/tf/distribute/Strategy#in_short"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 104,
    "url": "https://www.examtopics.com/discussions/google/view/92657-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You work for a gaming company that has millions of customers around the world. All games offer a chat feature that allows players to communicate with each other in real time. Messages can be typed in more than 20 languages and are translated in real time using the Cloud Translation API. You have been asked to build an ML system to moderate the chat in real time while assuring that the performance is uniform across the various languages and without changing the serving infrastructure.<br><br>You trained your first model using an in-house word2vec model for embedding the chat messages translated by the Cloud Translation API. However, the model has significant differences in performance across the different languages. How should you improve it?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a regularization term such as the Min-Diff algorithm to the loss function.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTrain a classifier using the chat messages in their original language.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReplace the in-house word2vec with GPT-3 or T5.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRemove moderation for languages for which the false positive rate is too high."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 15,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 6,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-03-09T12:28:00.000Z",
        "voteCount": 8,
        "content": "Answer B\nSince the performance of the model varies significantly across different languages, it suggests that the translation process might have introduced some noise in the chat messages, making it difficult for the model to generalize across languages. One way to address this issue is to train a classifier using the chat messages in their original language."
      },
      {
        "date": "2024-01-24T07:54:00.000Z",
        "voteCount": 1,
        "content": "uniform performance"
      },
      {
        "date": "2024-04-14T22:20:00.000Z",
        "voteCount": 1,
        "content": "Adding a regularization term to the loss function can help prevent overfitting of the model, but it may not necessarily address the language-specific differences in performance. The Min-Diff algorithm is a type of regularization technique that aims to minimize the difference between the model predictions and the ground truth while ensuring that the model remains simple. While this can improve the generalization performance of the model, it may not be sufficient to address the language-specific differences in performance. Therefore, training a classifier using the chat messages in their original language can be a better solution to improve the performance of the moderation system across different languages."
      },
      {
        "date": "2023-07-26T22:25:00.000Z",
        "voteCount": 2,
        "content": "Min-diff may reduce model unfairness, but here the concern is about improving performance. Training models avoiding Cloud Natural API should be more suitable."
      },
      {
        "date": "2023-07-31T12:19:00.000Z",
        "voteCount": 1,
        "content": "Adding a regularization term to the loss function can help prevent overfitting of the model, but it may not necessarily address the language-specific differences in performance. The Min-Diff algorithm is a type of regularization technique that aims to minimize the difference between the model predictions and the ground truth while ensuring that the model remains simple. While this can improve the generalization performance of the model, it may not be sufficient to address the language-specific differences in performance. Therefore, training a classifier using the chat messages in their original language can be a better solution to improve the performance of the moderation system across different languages."
      },
      {
        "date": "2023-07-21T04:19:00.000Z",
        "voteCount": 2,
        "content": "A is correct since it encourages the model to have similar performance across languages. \n\nB would entail training 20 word2vec embeddings + maintaining 20 models at the same time. On top of that, there would be no guarantee that those models will have comparable performance across languages. This is certainly not something you would do after training your first model."
      },
      {
        "date": "2023-06-22T10:44:00.000Z",
        "voteCount": 1,
        "content": "A is correct, the key part of the question is \u201e[\u2026] assuring the performance is uniform [\u2026]\u201c which is baked into the Min-Diff regularisation: https://ai.googleblog.com/2020/11/mitigating-unfair-bias-in-ml-models.html"
      },
      {
        "date": "2023-05-08T23:26:00.000Z",
        "voteCount": 1,
        "content": "Went with B"
      },
      {
        "date": "2023-03-27T10:48:00.000Z",
        "voteCount": 4,
        "content": "Since the current model has significant differences in performance across the different languages, it is likely that the translations produced by the Cloud Translation API are not of uniform quality across all languages. Therefore, it would be best to train a classifier using the chat messages in their original language instead of relying on translations.\n\nThis approach has several advantages. First, the model can directly learn the nuances of each language, leading to better performance across all languages. Second, it eliminates the need for translation, reducing the possibility of errors and improving the overall speed of the system. Finally, it is a relatively simple approach that can be implemented without changing the serving infrastructure."
      },
      {
        "date": "2023-03-07T14:28:00.000Z",
        "voteCount": 2,
        "content": "should be A\nhttps://ai.googleblog.com/2020/11/mitigating-unfair-bias-in-ml-models.html"
      },
      {
        "date": "2023-03-03T16:56:00.000Z",
        "voteCount": 3,
        "content": "B i think is the correct answer \nC is an overkill , you have just developed your first model you don\u2019t jump into solution like C , in addition the problem is that there is a significant difference between language note the model is enormously underperforming . \nFinally you are serving millions of users , running chat GPT or T5 for a task like chat moderation (and in real time) is extremely wasteful ."
      },
      {
        "date": "2023-02-08T00:27:00.000Z",
        "voteCount": 3,
        "content": "Given that GPT-3 is rival of google , C is  not possible certainly ."
      },
      {
        "date": "2023-02-08T00:40:00.000Z",
        "voteCount": 1,
        "content": "we are taking into account 20 muti classification, it is relevant about FP or FN."
      },
      {
        "date": "2022-12-27T00:20:00.000Z",
        "voteCount": 3,
        "content": "GPT-3 is best for generating human-like Text"
      },
      {
        "date": "2023-02-24T04:09:00.000Z",
        "voteCount": 2,
        "content": "Does \"moderate\" means we need to generate text?"
      },
      {
        "date": "2022-12-24T09:23:00.000Z",
        "voteCount": 1,
        "content": "Ans : C\nhttps://towardsdatascience.com/poor-mans-gpt-3-few-shot-text-generation-with-t5-transformer-51f1b01f843e"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 105,
    "url": "https://www.examtopics.com/discussions/google/view/92224-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You work for a gaming company that develops massively multiplayer online (MMO) games. You built a TensorFlow model that predicts whether players will make in-app purchases of more than $10 in the next two weeks. The model\u2019s predictions will be used to adapt each user\u2019s game experience. User data is stored in BigQuery. How should you serve your model while optimizing cost, user experience, and ease of management?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImport the model into BigQuery ML. Make predictions using batch reading data from BigQuery, and push the data to Cloud SQL\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the model to Vertex AI Prediction. Make predictions using batch reading data from Cloud Bigtable, and push the data to Cloud SQL.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEmbed the model in the mobile application. Make predictions after every in-app purchase event is published in Pub/Sub, and push the data to Cloud SQL.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEmbed the model in the streaming Dataflow pipeline. Make predictions after every in-app purchase event is published in Pub/Sub, and push the data to Cloud SQL."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 16,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 6,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-20T13:57:00.000Z",
        "voteCount": 12,
        "content": "it seens A (not sure)\n- https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-create-tensorflow"
      },
      {
        "date": "2024-04-14T22:24:00.000Z",
        "voteCount": 2,
        "content": "Make predictions after every in-app purchase it it not necessary -&gt; A"
      },
      {
        "date": "2023-11-14T22:26:00.000Z",
        "voteCount": 1,
        "content": "Embedding the model in a streaming Dataflow pipeline allows low latency predictions on real-time events published to Pub/Sub. This provides a responsive user experience.\nDataflow provides a managed service to scale predictions and integrate with Pub/Sub, without having to manage servers.\nStreaming predictions only when events occur optimizes cost compared to bulk or client-side prediction.\nPushing results to Cloud SQL provides a managed database for persistence.\nIn contrast, options A and B use inefficient batch predictions. Option C increases mobile app size and cost."
      },
      {
        "date": "2023-07-07T23:04:00.000Z",
        "voteCount": 1,
        "content": "D could be correct"
      },
      {
        "date": "2023-07-04T11:18:00.000Z",
        "voteCount": 1,
        "content": "These were my reasonings to choose D as best option:\nB -&gt; Vertex AI would not minimize cost\nC -&gt; Would not optimize user experience (this may lead to slow running of the game (lag)?)\nA- &gt; Would not optimize ease of management / automatization\nD -&gt; Best choice?"
      },
      {
        "date": "2023-11-10T00:46:00.000Z",
        "voteCount": 3,
        "content": "Why do you want to make a prediction after every app purchase bro?"
      },
      {
        "date": "2023-05-08T09:07:00.000Z",
        "voteCount": 3,
        "content": "For \"used to adapt each user's game experience\" points out to non-batch, hence excludes A &amp; B, and embedding the model in the mobile app would not necessarily \"optimize cost\". Plus, the classical streaming solution builds on Dataflow along with Pub/Sub and BigQuery, embedding ML in Dataflow is low-code https://cloud.google.com/blog/products/data-analytics/latest-dataflow-innovations-for-real-time-streaming-and-aiml and apparently a modified version of the question points to the same direction https://mikaelahonen.com/en/data/gcp-mle-exam-questions/"
      },
      {
        "date": "2023-07-26T22:35:00.000Z",
        "voteCount": 3,
        "content": "there's no need to make a prediction after every in-app purchase event. Am i wrong?"
      },
      {
        "date": "2023-04-16T23:52:00.000Z",
        "voteCount": 2,
        "content": "Yeah its A"
      },
      {
        "date": "2023-03-09T12:25:00.000Z",
        "voteCount": 2,
        "content": "Answer C"
      },
      {
        "date": "2023-03-27T10:55:00.000Z",
        "voteCount": 2,
        "content": "Option C, embedding the model in the mobile application, can increase the size of the application and may not be suitable for real-time prediction."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 106,
    "url": "https://www.examtopics.com/discussions/google/view/92226-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You are building a linear regression model on BigQuery ML to predict a customer\u2019s likelihood of purchasing your company\u2019s products. Your model uses a city name variable as a key predictive component. In order to train and serve the model, your data must be organized in columns. You want to prepare your data using the least amount of coding while maintaining the predictable variables. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse TensorFlow to create a categorical variable with a vocabulary list. Create the vocabulary file, and upload it as part of your model to BigQuery ML.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new view with BigQuery that does not include a column with city information",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Data Fusion to assign each city to a region labeled as 1, 2, 3, 4, or 5, and then use that number to represent the city in the model.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Dataprep to transform the state column using a one-hot encoding method, and make each city a column with binary values.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 15,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-04-25T01:11:00.000Z",
        "voteCount": 3,
        "content": "A. Using TensorFlow: This is an overkill for this scenario. BigQuery ML can handle one-hot encoding natively within Dataprep.\nB. Excluding City Information: This removes a potentially important predictive variable, reducing model accuracy.\nC. Assigning Region Labels: This approach loses granularity and might not capture the specific variations between cities."
      },
      {
        "date": "2023-09-10T03:56:00.000Z",
        "voteCount": 1,
        "content": "D by elimination but ...\nDoes not bigquery automatically do one-hot encoding of categorical features for you?\nAlso the wording of the question does not seem right: a linear regression model to predict the likelihodd that the customer ... isnt that a classification model?"
      },
      {
        "date": "2023-05-08T23:27:00.000Z",
        "voteCount": 1,
        "content": "Went with D"
      },
      {
        "date": "2023-03-18T01:35:00.000Z",
        "voteCount": 1,
        "content": "Is it correct to say that A is technically a better way to do things if the ask wast for separate columns?"
      },
      {
        "date": "2023-03-27T10:57:00.000Z",
        "voteCount": 4,
        "content": "\"least amount of coding\""
      },
      {
        "date": "2023-03-14T06:50:00.000Z",
        "voteCount": 3,
        "content": "One-hot is a good way to use categorical variables in regressions problems\nhttps://academic.oup.com/rheumatology/article/54/7/1141/1849688\nhttps://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-auto-preprocessing"
      },
      {
        "date": "2023-03-09T12:23:00.000Z",
        "voteCount": 1,
        "content": "Answer D"
      },
      {
        "date": "2023-02-16T07:34:00.000Z",
        "voteCount": 1,
        "content": "for a fuller answer, D--&gt; transforms \u201cstate\u201d column not city column\nC--&gt; at least works with city column"
      },
      {
        "date": "2023-03-27T10:58:00.000Z",
        "voteCount": 1,
        "content": "Read smarques comment"
      },
      {
        "date": "2023-02-08T07:11:00.000Z",
        "voteCount": 2,
        "content": "https://docs.trifacta.com/display/SS/Prepare+Data+for+Machine+Processing"
      },
      {
        "date": "2023-01-18T06:01:00.000Z",
        "voteCount": 1,
        "content": "This will allow you to maintain the city name variable as a predictor while ensuring that the data is in a format that can be used to train a linear regression model on BigQuery ML."
      },
      {
        "date": "2022-12-29T18:51:00.000Z",
        "voteCount": 1,
        "content": "Answer D"
      },
      {
        "date": "2022-12-29T10:51:00.000Z",
        "voteCount": 1,
        "content": "Answer is D"
      },
      {
        "date": "2022-12-21T02:07:00.000Z",
        "voteCount": 2,
        "content": "one-hot encoding makes sense to me"
      },
      {
        "date": "2022-12-20T14:03:00.000Z",
        "voteCount": 2,
        "content": "I vote for C"
      },
      {
        "date": "2022-12-27T06:36:00.000Z",
        "voteCount": 3,
        "content": "Changing my vote to D"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 107,
    "url": "https://www.examtopics.com/discussions/google/view/91498-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You are an ML engineer at a bank that has a mobile application. Management has asked you to build an ML-based biometric authentication for the app that verifies a customer\u2019s identity based on their fingerprint. Fingerprints are considered highly sensitive personal information and cannot be downloaded and stored into the bank databases. Which learning strategy should you recommend to train and deploy this ML mode?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tData Loss Prevention API",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFederated learning\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMD5 to encrypt data",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDifferential privacy"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 17,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-20T14:08:00.000Z",
        "voteCount": 9,
        "content": "B\nWith federated learning, all the data is collected, and the model is trained with algorithms across multiple decentralized edge devices such as cell phones or websites, without exchanging them.\n(Journey to Become a Google Cloud Machine Learning Engineer: Build the mind and hand of a Google Certified ML professional)"
      },
      {
        "date": "2024-04-25T01:13:00.000Z",
        "voteCount": 1,
        "content": "Federated learning allows training the model on the user's devices themselves.\n\npen_spark\nexpand_more The model updates its parameters based on local training data on the device without ever needing the raw fingerprint information to leave the device. This ensures the highest level of privacy for sensitive biometric data."
      },
      {
        "date": "2024-04-25T01:13:00.000Z",
        "voteCount": 1,
        "content": "Data Loss Prevention API (DLAPI): This focuses on protecting data at rest and in transit, not relevant to training a model without storing data.\nMD5 Encryption: This is a one-way hashing function, not suitable for encryption and decryption needed for training.expand_more\nDifferential privacy: While it adds noise to protect privacy, it's not ideal for training image recognition models like fingerprint identification."
      },
      {
        "date": "2023-06-05T06:16:00.000Z",
        "voteCount": 2,
        "content": "B. Federated learning. \n\"information and cannot be downloaded and stored into the bank databases\" That excludes DLP. ederated Learning enables mobile phones to collaboratively learn a shared prediction model while keeping all the training data on device, decoupling the ability to do machine learning from the need to store the data in the cloud."
      },
      {
        "date": "2023-05-08T23:27:00.000Z",
        "voteCount": 1,
        "content": "Went with B"
      },
      {
        "date": "2023-03-20T23:00:00.000Z",
        "voteCount": 3,
        "content": "I think the giveaway is in the question \"Which learning strategy..\"... Federated Learning seems to be the only one !"
      },
      {
        "date": "2023-03-07T05:58:00.000Z",
        "voteCount": 2,
        "content": "B. Federated learning would be the best learning strategy to train and deploy the ML model for biometric authentication in this scenario. Federated learning allows for training an ML model on distributed data without transferring the raw data to a centralized location."
      },
      {
        "date": "2023-02-22T07:05:00.000Z",
        "voteCount": 1,
        "content": "Ans is A for me"
      },
      {
        "date": "2023-01-07T06:56:00.000Z",
        "voteCount": 1,
        "content": "It seems A, to me."
      },
      {
        "date": "2022-12-13T13:52:00.000Z",
        "voteCount": 1,
        "content": "Federated Learning enables mobile phones to collaboratively learn a shared prediction model while keeping all the training data on device.\nhttps://ai.googleblog.com/2017/04/federated-learning-collaborative.html"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 108,
    "url": "https://www.examtopics.com/discussions/google/view/91900-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You are experimenting with a built-in distributed XGBoost model in Vertex AI Workbench user-managed notebooks. You use BigQuery to split your data into training and validation sets using the following queries:<br><br>CREATE OR REPLACE TABLE \u2018myproject.mydataset.training\u2018 AS<br>(SELECT * FROM \u2018myproject.mydataset.mytable\u2018 WHERE RAND() &lt;= 0.8);<br><br>CREATE OR REPLACE TABLE \u2018myproject.mydataset.validation\u2018 AS<br>(SELECT * FROM \u2018myproject.mydataset.mytable\u2018 WHERE RAND() &lt;= 0.2);<br><br>After training the model, you achieve an area under the receiver operating characteristic curve (AUC ROC) value of 0.8, but after deploying the model to production, you notice that your model performance has dropped to an AUC ROC value of 0.65. What problem is most likely occurring?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThere is training-serving skew in your production environment.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThere is not a sufficient amount of training data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe tables that you created to hold your training and validation records share some records, and you may not be using all the data in your initial table.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe RAND() function generated a number that is less than 0.2 in both instances, so every record in the validation table will also be in the training table."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 17,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-08-25T11:31:00.000Z",
        "voteCount": 1,
        "content": "Answer C"
      },
      {
        "date": "2023-05-08T10:59:00.000Z",
        "voteCount": 4,
        "content": "- Excluding D as RAND() samples 80% for \u201c.training\u201d &amp; 20% for \u201c.validaton\u201d: https://stackoverflow.com/questions/42115968/how-does-rand-works-in-bigquery;\n- Could be that those 2 samplings share some records since pseudo-randomly sampled over the same \u201c.mytable\u201d, &amp; therefore might not be using all of its data, thus C seems valid;\n- Excluding B as there is no indication otherwise of insufficient amount of training data, after training AUC ROC was 0.8, that we know;\n- There could be a training-serving skew occurring in Prod, but \u201cmost likely occurring\u201d is C as a result of the selective information presented: https://developers.google.com/machine-learning/guides/rules-of-ml#training-serving_skew"
      },
      {
        "date": "2023-04-17T06:06:00.000Z",
        "voteCount": 2,
        "content": "Answer C"
      },
      {
        "date": "2023-03-18T01:41:00.000Z",
        "voteCount": 1,
        "content": "C seems closest here"
      },
      {
        "date": "2023-03-09T12:22:00.000Z",
        "voteCount": 1,
        "content": "Answer C"
      },
      {
        "date": "2023-02-23T11:35:00.000Z",
        "voteCount": 2,
        "content": "since we are calling rand twice it might be that data that was in training set ends up in testing set too. If we had called it just once I would say D."
      },
      {
        "date": "2022-12-28T08:49:00.000Z",
        "voteCount": 1,
        "content": "Hesitated between C and D, but D looks more precise"
      },
      {
        "date": "2023-01-28T10:52:00.000Z",
        "voteCount": 2,
        "content": "If there were one RAND() in front of those two queries it would be true. There are two separate RAND() and \"every record in the validation table will also be in the training table\" is not true."
      },
      {
        "date": "2022-12-20T14:17:00.000Z",
        "voteCount": 4,
        "content": "C (not sure)"
      },
      {
        "date": "2022-12-17T03:47:00.000Z",
        "voteCount": 2,
        "content": "the rand is generated twice"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 109,
    "url": "https://www.examtopics.com/discussions/google/view/91954-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "During batch training of a neural network, you notice that there is an oscillation in the loss. How should you adjust your model to ensure that it converges?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDecrease the size of the training batch.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDecrease the learning rate hyperparameter.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the learning rate hyperparameter.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the size of the training batch."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-21T02:09:00.000Z",
        "voteCount": 8,
        "content": "B\nlarger learning rates can reduce training time but may lead to model oscillation and may miss the optimal model parameter values."
      },
      {
        "date": "2024-04-25T01:23:00.000Z",
        "voteCount": 1,
        "content": "A. Decrease Batch Size: While a smaller batch size can sometimes help with convergence, it can also lead to slower training. It might not necessarily address the issue of oscillation.\nC. Increase Learning Rate: A higher learning rate can cause the loss to jump around more erratically, potentially worsening the oscillation problem.\nD. Increase Batch Size: A larger batch size can lead to smoother updates but might also make the model less sensitive to local gradients and hinder convergence, especially with an already oscillating loss."
      },
      {
        "date": "2024-04-20T01:31:00.000Z",
        "voteCount": 2,
        "content": "I don't understand"
      },
      {
        "date": "2023-05-08T23:27:00.000Z",
        "voteCount": 1,
        "content": "Went with B"
      },
      {
        "date": "2023-03-09T12:20:00.000Z",
        "voteCount": 1,
        "content": "Answer B"
      },
      {
        "date": "2023-02-09T08:42:00.000Z",
        "voteCount": 1,
        "content": "having a large learning rate results in Instability or Oscillations. Thus, the first solution is to tune the learning rate by gradually decreasing it.\nhttps://towardsdatascience.com/8-common-pitfalls-in-neural-network-training-workarounds-for-them-7d3de51763ad"
      },
      {
        "date": "2022-12-17T17:58:00.000Z",
        "voteCount": 2,
        "content": "https://ai.stackexchange.com/questions/14079/what-could-an-oscillating-training-loss-curve-represent#:~:text=Try%20lowering%20the%20learning%20rate,step%20and%20overshoot%20it%20again."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 110,
    "url": "https://www.examtopics.com/discussions/google/view/91493-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You work for a toy manufacturer that has been experiencing a large increase in demand. You need to build an ML model to reduce the amount of time spent by quality control inspectors checking for product defects. Faster defect detection is a priority. The factory does not have reliable Wi-Fi. Your company wants to implement the new ML model as soon as possible. Which model should you use?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAutoML Vision Edge mobile-high-accuracy-1 model",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAutoML Vision Edge mobile-low-latency-1 model\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAutoML Vision model",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAutoML Vision Edge mobile-versatile-1 model"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 16,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-12-13T13:01:00.000Z",
        "voteCount": 9,
        "content": "Hence faster defect detection is a priority, AutoML Vision Edge mobile-low-latency-1 model should be the choice. This model is designed to run efficiently on mobile devices and prioritize low latency, which means that it can provide fast defect detection without requiring a connection to the cloud.\nhttps://cloud.google.com/vision/automl/docs/train-edge"
      },
      {
        "date": "2023-10-23T03:49:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/vertex-ai/docs/training/automl-edge-api"
      },
      {
        "date": "2022-12-21T02:12:00.000Z",
        "voteCount": 6,
        "content": "B\n\"reduce the amount of time spent by quality control inspectors checking for product defects.\"-&gt; low latency"
      },
      {
        "date": "2024-08-09T21:03:00.000Z",
        "voteCount": 1,
        "content": "low latency (MOBILE_TF_LOW_LATENCY_1)\ngeneral purpose usage (MOBILE_TF_VERSATILE_1)\nhigher prediction quality (MOBILE_TF_HIGH_ACCURACY_1)"
      },
      {
        "date": "2024-04-25T01:27:00.000Z",
        "voteCount": 2,
        "content": "The AutoML Vision Edge mobile-low-latency-1 model prioritizes speed over accuracy, making it ideal for real-time defect detection on the factory floor without a stable internet connection. This allows for faster inspections and quicker identification of faulty products."
      },
      {
        "date": "2024-04-25T01:27:00.000Z",
        "voteCount": 1,
        "content": "Faster Defect Detection: This is the main priority, and the low-latency model is specifically designed for speed.\nEdge Device Compatibility: The model should run on a device without relying on Wi-Fi. AutoML Vision Edge models are optimized for edge deployments."
      },
      {
        "date": "2024-04-25T01:27:00.000Z",
        "voteCount": 2,
        "content": "A. AutoML Vision mobile-high-accuracy-1 model: While high accuracy is desirable, faster defect detection is the top priority in this case. This model might be slower due to its focus on accuracy.\nC. AutoML Vision model: This model is likely designed for cloud deployment and might not be suitable for running on an edge device without reliable Wi-Fi.\nD. AutoML Vision Edge mobile-versatile-1 model: This model prioritizes a balance between accuracy and latency. While faster than the high-accuracy model, it might be slower than the low-latency model for this specific use case."
      },
      {
        "date": "2024-04-02T10:21:00.000Z",
        "voteCount": 1,
        "content": "Edge device with low latency"
      },
      {
        "date": "2023-05-08T23:28:00.000Z",
        "voteCount": 1,
        "content": "Went with B"
      },
      {
        "date": "2023-03-09T12:19:00.000Z",
        "voteCount": 1,
        "content": "Answer B"
      },
      {
        "date": "2023-01-03T06:53:00.000Z",
        "voteCount": 1,
        "content": "It's B."
      },
      {
        "date": "2022-12-13T13:02:00.000Z",
        "voteCount": 4,
        "content": "vote B"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 111,
    "url": "https://www.examtopics.com/discussions/google/view/91955-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You need to build classification workflows over several structured datasets currently stored in BigQuery. Because you will be performing the classification several times, you want to complete the following steps without writing code: exploratory data analysis, feature selection, model building, training, and hyperparameter tuning and serving. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTrain a TensorFlow model on Vertex AI.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTrain a classification Vertex AutoML model.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun a logistic regression job on BigQuery ML.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse scikit-learn in Notebooks with pandas library."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 15,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-12-21T02:16:00.000Z",
        "voteCount": 7,
        "content": "B (similar to question 7)"
      },
      {
        "date": "2024-04-25T19:19:00.000Z",
        "voteCount": 2,
        "content": "Vertex AutoML is a Google Cloud Platform service designed for building machine learning models without writing code.expand_more It automates various stages of the machine learning pipeline, including those you mentioned:\n\nExploratory data analysis\nFeature selection\nModel building (supports various classification algorithms)\nTraining\nHyperparameter tuning"
      },
      {
        "date": "2023-05-08T23:28:00.000Z",
        "voteCount": 1,
        "content": "Went with B"
      },
      {
        "date": "2023-03-09T12:18:00.000Z",
        "voteCount": 1,
        "content": "Answer B"
      },
      {
        "date": "2023-01-03T06:43:00.000Z",
        "voteCount": 1,
        "content": "A and D need coding. C is regression, not classification. Hence B."
      },
      {
        "date": "2023-01-06T06:41:00.000Z",
        "voteCount": 3,
        "content": "My mistake, it's logistic regression, meaning classification. But it still requires some coding. So still B."
      },
      {
        "date": "2022-12-17T18:08:00.000Z",
        "voteCount": 3,
        "content": "BQML will need coding\nonly AutoML in Vertex AI is codeless from end to end"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 112,
    "url": "https://www.examtopics.com/discussions/google/view/91491-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You are an ML engineer in the contact center of a large enterprise. You need to build a sentiment analysis tool that predicts customer sentiment from recorded phone conversations. You need to identify the best approach to building a model while ensuring that the gender, age, and cultural differences of the customers who called the contact center do not impact any stage of the model development pipeline and results. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConvert the speech to text and extract sentiments based on the sentences.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConvert the speech to text and build a model based on the words.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExtract sentiment directly from the voice recordings.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConvert the speech to text and extract sentiment using syntactical analysis."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 19,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 9,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-17T18:17:00.000Z",
        "voteCount": 10,
        "content": "Syntactic Analysis is not for sentiment analysis"
      },
      {
        "date": "2024-04-25T19:25:00.000Z",
        "voteCount": 4,
        "content": "A. Convert speech to text and extract sentiments based on sentences: This method focuses on the content of the conversation, minimizing the influence of factors like voice tone (which can be culturally or gender-specific). Sentiment analysis techniques can analyze the meaning and context of sentences to identify positive, negative, or neutral sentiment."
      },
      {
        "date": "2024-04-25T19:26:00.000Z",
        "voteCount": 2,
        "content": "B. Convert speech to text and build a model based on the words: While words are important, relying solely on them can miss the context and lead to bias. For example, \"great\" might be positive in most cases, but in some cultures, it might be used sarcastically.\n\nC. Extract sentiment directly from voice recordings: This approach can be biased as voice characteristics like pitch or pace can vary based on gender, age, and cultural background.\n\nD. Convert speech to text and extract sentiment using syntactical analysis: While syntax can provide some clues, it's not the strongest indicator of sentiment. Additionally, cultural differences in sentence structure could impact accuracy."
      },
      {
        "date": "2023-11-28T09:44:00.000Z",
        "voteCount": 2,
        "content": "The correct answer should be A. Word embeddings have static embeddings for the same words, while contextual embeddings vary depending on the context.\n\n\"May\u2019s sentence embedding adaptation of WEAT, known as the Sentence Embedding Association Test (SEAT), shows less clear racial and gender bias in language models and embeddings than the corresponding word embedding formulation\"\n\nFrom: https://medium.com/institute-for-applied-computational-science/bias-in-nlp-embeddings-b1dabb8bbe20"
      },
      {
        "date": "2023-11-16T07:08:00.000Z",
        "voteCount": 2,
        "content": "This approach involves converting the speech to text, which allows you to analyze the content of the conversations without directly dealing with the speakers' gender, age, or cultural differences. By building a model based on the words, you can focus on the language used in the conversations to predict sentiment, making the model more inclusive and less sensitive to demographic factors.\n\nOption A could be influenced by the syntactical nuances and structures used in different cultures, and option C might be impacted by the variations in voice tones across genders and ages. Option B, on the other hand, relies on the text content, which provides a more neutral and content-focused basis for sentiment analysis."
      },
      {
        "date": "2023-10-22T04:43:00.000Z",
        "voteCount": 1,
        "content": "B: People of different cultures will often use difference sentence structures, so words would be safer than sentences"
      },
      {
        "date": "2023-11-10T01:06:00.000Z",
        "voteCount": 1,
        "content": "Yeah, but they(words) may miss the context of the sentiment, leading to inaccuracies!"
      },
      {
        "date": "2023-07-31T12:08:00.000Z",
        "voteCount": 1,
        "content": "building a model based on words, may also be effective but could potentially be influenced by factors such as accents, dialects, or language variations that may differ between speakers.extracting sentiment directly from voice recordings, may be less accurate due to the subjective nature of interpreting emotions from audio alone.using syntactical analysis, may be useful in certain contexts but may not capture the full range of sentiment expressed in a conversation. Therefore, A provides the most comprehensive and unbiased approach to sentiment analysis in this scenario."
      },
      {
        "date": "2023-11-16T07:09:00.000Z",
        "voteCount": 1,
        "content": "Option A could be influenced by the syntactical nuances and structures used in different cultures"
      },
      {
        "date": "2023-11-22T11:15:00.000Z",
        "voteCount": 1,
        "content": "See, both have their own advantages &amp; dissadvantages, but we should choose the option which is more relevant"
      },
      {
        "date": "2023-07-27T00:40:00.000Z",
        "voteCount": 1,
        "content": "Answer A"
      },
      {
        "date": "2023-07-27T00:40:00.000Z",
        "voteCount": 1,
        "content": "Answer B*"
      },
      {
        "date": "2023-07-16T09:33:00.000Z",
        "voteCount": 1,
        "content": "By working directly with the audio data, you can account for important aspects like tone, pitch, and rhythm of speech, which might provide valuable information regarding sentiment."
      },
      {
        "date": "2023-07-21T09:15:00.000Z",
        "voteCount": 1,
        "content": "But the audio will be affected by gender, age, and cultural differences of the customers. When you convert the recording to text, this problem is less pronounced. So the answer cannot be C"
      },
      {
        "date": "2023-07-08T09:11:00.000Z",
        "voteCount": 1,
        "content": "vote for A\nbetween words and sentences:\nAge and gender considerations: Sentences provide a broader view of sentiment that can help mitigate age and gender biases. Analyzing at the sentence level allows you to observe sentiment patterns across various demographic groups, which can help identify any biases that may arise. By considering the overall sentiment expressed in sentences, you can minimize the impact of individual words that might carry specific biases."
      },
      {
        "date": "2023-05-08T12:27:00.000Z",
        "voteCount": 2,
        "content": "There is the possibility for a more sophisticated architecture for an audio processing pipeline, and the \u201cnot impact any stage of the model development pipeline and results\u201d somewhat calls for a more holistic answer: https://cloud.google.com/architecture/categorizing-audio-files-using-ml#converting_speech_to_text. Plus, it adds \u201cvoice emotion information, related to an audio recording, indicating that a vocal utterance of a speaker is spoken with negative or positive emotion\u201d: https://patents.google.com/patent/US20140220526A1/en."
      },
      {
        "date": "2023-05-08T12:28:00.000Z",
        "voteCount": 1,
        "content": "The emphasis here is on #ResponsibleAI https://cloud.google.com/natural-language/automl/docs/beginners-guide"
      },
      {
        "date": "2023-05-08T12:27:00.000Z",
        "voteCount": 1,
        "content": "A reason why one could exclude \u201cConvert the speech to text\u201d altogether [Options A, B &amp; D] could be, for instance, because \u201cspeech transcription may have higher error rates for African Americans than White Americans [3]\u201d: https://developers.googleblog.com/2018/04/text-embedding-models-contain-bias.html."
      },
      {
        "date": "2023-05-08T12:28:00.000Z",
        "voteCount": 1,
        "content": "\u201cCloud NL API can perform syntactic analysis directly on a file located in Cloud Storage.\u201d \u201cSyntactic Analysis [Option D] breaks up the given text into a series of sentences [Option A] and tokens (generally, words [Option B]) and provides linguistic information about those tokens\u201d: https://cloud.google.com/natural-language/docs/analyzing-syntax. \nIt \u201ccan be used to identify the parts of speech, determine the structure of a sentence, and determine the meaning of words in context\u201d: https://ts2.space/en/a-comprehensive-guide-to-google-cloud-natural-language-apis-syntax-analysis/."
      },
      {
        "date": "2023-04-21T09:39:00.000Z",
        "voteCount": 1,
        "content": "Can anyone explain how to choose between words and sentences? I feel like the model could pick up bias from both"
      },
      {
        "date": "2023-04-17T07:12:00.000Z",
        "voteCount": 2,
        "content": "I agree with qaz09. To avoid demographical variables influence model shoud be built on the words."
      },
      {
        "date": "2023-03-07T05:55:00.000Z",
        "voteCount": 1,
        "content": "Answer A"
      },
      {
        "date": "2023-02-10T01:04:00.000Z",
        "voteCount": 3,
        "content": "For \"ensuring that the gender, age, and cultural differences of the customers who called the contact center do not impact any stage of the model development pipeline and results\" I think the model should be built on the words rather than sentences"
      },
      {
        "date": "2023-01-03T06:40:00.000Z",
        "voteCount": 1,
        "content": "A makes sense, to me."
      },
      {
        "date": "2022-12-21T02:20:00.000Z",
        "voteCount": 1,
        "content": "A\nConvert the speech to text and extract sentiments based on the sentences."
      },
      {
        "date": "2022-12-13T12:48:00.000Z",
        "voteCount": 1,
        "content": "vote D"
      },
      {
        "date": "2023-03-18T02:01:00.000Z",
        "voteCount": 2,
        "content": "Based only on words might be misleading; at a minimum need to go w sentences"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 113,
    "url": "https://www.examtopics.com/discussions/google/view/91957-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You need to analyze user activity data from your company\u2019s mobile applications. Your team will use BigQuery for data analysis, transformation, and experimentation with ML algorithms. You need to ensure real-time ingestion of the user activity data into BigQuery. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Pub/Sub to stream the data into BigQuery.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun an Apache Spark streaming job on Dataproc to ingest the data into BigQuery.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun a Dataflow streaming job to ingest the data into BigQuery.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Pub/Sub and a Dataflow streaming job to ingest the data into BigQuery,"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 26,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 13,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-20T12:40:00.000Z",
        "voteCount": 19,
        "content": "Previously Google pattern was Pub/Sub -&gt; Dataflow -&gt; BQ\nbut now it looks as there is new Pub/Sub -&gt; BQ\nhttps://cloud.google.com/blog/products/data-analytics/pub-sub-launches-direct-path-to-bigquery-for-streaming-analytics"
      },
      {
        "date": "2023-03-07T05:51:00.000Z",
        "voteCount": 1,
        "content": "New pub sub??? heheheh"
      },
      {
        "date": "2023-03-07T05:54:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/blog/products/data-analytics/pub-sub-launches-direct-path-to-bigquery-for-streaming-analytics\nYou should have said pub sub has been upgrade to directly stream to bigquery templates...not new pub sub"
      },
      {
        "date": "2024-09-09T09:06:00.000Z",
        "voteCount": 1,
        "content": "The question specifies that transformation occurs in Bigquery. This means the new direct pub/sub to bigquery streaming path is correct."
      },
      {
        "date": "2024-06-29T07:51:00.000Z",
        "voteCount": 1,
        "content": "Need PubSub and Dataflow both for this"
      },
      {
        "date": "2024-03-01T02:46:00.000Z",
        "voteCount": 1,
        "content": "Werner123 i agree"
      },
      {
        "date": "2024-02-29T05:30:00.000Z",
        "voteCount": 2,
        "content": "User data would most likely include PII, for that case it is still recommended to use Dataflow since you need to remove/anonymise sensitive data."
      },
      {
        "date": "2023-11-16T07:17:00.000Z",
        "voteCount": 1,
        "content": "I would have added \"with / without data transformation\" to the question to choose the right answer between A or D"
      },
      {
        "date": "2023-09-10T09:45:00.000Z",
        "voteCount": 3,
        "content": "I had my doubts between A and D.\nBut since the transformation will occur in bigquery I think Pubsub suffices."
      },
      {
        "date": "2023-05-08T22:36:00.000Z",
        "voteCount": 3,
        "content": "Agree with TNT87. From the same link: \u201cFor Pub/Sub messages where advanced preload transformations or data processing before landing data in BigQuery (such as masking PII) is necessary, we still recommend going through Dataflow.\u201d It\u2019s \u201canalyze user activity data\u201d, not merely streaming IoT into BigQuery so that concerns like privacy are per se n/a. One can deal with PII after landing in BigQuery as well, but apparently that\u2019s not what they recommend."
      },
      {
        "date": "2023-03-29T17:36:00.000Z",
        "voteCount": 2,
        "content": "Pub/Sub -&gt; DataFlow -&gt; BigQuery"
      },
      {
        "date": "2023-03-07T05:51:00.000Z",
        "voteCount": 2,
        "content": "D. Configure Pub/Sub and a Dataflow streaming job to ingest the data into BigQuery.\n\nThis solution involves using Google Cloud Pub/Sub as the messaging service to receive the data from the mobile application, and then using Google Cloud Dataflow to transform and load the data into BigQuery in real time. Pub/Sub is a scalable and reliable messaging service that can handle high-volume real-time data streaming, while Dataflow provides a unified programming model to develop and run data processing pipelines. This solution is suitable for handling large volumes of user activity data from mobile applications and ingesting it into BigQuery in real-time for analysis and ML experimentation."
      },
      {
        "date": "2023-03-07T05:55:00.000Z",
        "voteCount": 1,
        "content": "Starting today, you no longer have to write or run your own pipelines for data ingestion from Pub/Sub into BigQuery. We are introducing a new type of Pub/Sub subscription called a \u201cBigQuery subscription\u201d that writes directly from Cloud Pub/Sub to BigQuery. This new extract, load, and transform (ELT) path will be able to simplify your event-driven architecture. For Pub/Sub messages where advanced preload transformations or data processing before landing data in BigQuery (such as masking PII) is necessary, we still recommend going through Dataflow"
      },
      {
        "date": "2022-12-21T02:37:00.000Z",
        "voteCount": 3,
        "content": "A\nagree with pshemol"
      },
      {
        "date": "2022-12-17T18:27:00.000Z",
        "voteCount": 2,
        "content": "need dataflow"
      },
      {
        "date": "2022-12-20T01:10:00.000Z",
        "voteCount": 6,
        "content": "transformation will be handled in BQ hence I think A"
      },
      {
        "date": "2022-12-29T05:57:00.000Z",
        "voteCount": 1,
        "content": "agree."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 114,
    "url": "https://www.examtopics.com/discussions/google/view/92218-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You work for a gaming company that manages a popular online multiplayer game where teams with 6 players play against each other in 5-minute battles. There are many new players every day. You need to build a model that automatically assigns available players to teams in real time. User research indicates that the game is more enjoyable when battles have players with similar skill levels. Which business metrics should you track to measure your model\u2019s performance?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAverage time players wait before being assigned to a team",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPrecision and recall of assigning players to teams based on their predicted versus actual ability",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUser engagement as measured by the number of battles played daily per user\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRate of return as measured by additional revenue generated minus the cost of developing a new model"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 36,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 19,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-20T12:50:00.000Z",
        "voteCount": 9,
        "content": "The game is more enjoyable - the better and \"business metrics\" points me to user engagement as best metric"
      },
      {
        "date": "2024-09-09T09:12:00.000Z",
        "voteCount": 1,
        "content": "This question doesn't specify how \"additional revenue\" is measured. Most businesses I've worked for would love \"D\" for all our models instead of anything else. That being said, C is the only measurable business metric there."
      },
      {
        "date": "2024-04-25T19:39:00.000Z",
        "voteCount": 1,
        "content": "focusing on user engagement through the number of battles played daily provides a clearer indication of whether the model successfully creates balanced and enjoyable matches, which is the core objective. If players find battles more engaging due to fairer competition, they're more likely to keep playing. This can then translate to long-term benefits like increased retention and potential monetization opportunities."
      },
      {
        "date": "2024-04-25T19:39:00.000Z",
        "voteCount": 1,
        "content": "A. Average time players wait before being assigned to a team: While faster matchmaking is desirable, it shouldn't come at the expense of balanced teams. If wait times are very low but battles are imbalanced due to poor matchmaking, user engagement might suffer.\nB. Precision and recall of assigning players to skill level: These metrics are valuable for evaluating the model's ability to predict skill accurately. However, they don't directly measure the impact on user experience and enjoyment.\nD. Rate of return: This metric focuses on financial gain, which might not be the primary objective in this case. Prioritizing balanced teams for a more enjoyable experience can indirectly lead to higher user retention and potentially more revenue in the long run."
      },
      {
        "date": "2024-03-06T02:51:00.000Z",
        "voteCount": 2,
        "content": "Tempted by B but \"user engagement\" is the keyword."
      },
      {
        "date": "2024-03-06T02:51:00.000Z",
        "voteCount": 2,
        "content": "I meant \"business metric\"."
      },
      {
        "date": "2024-01-23T13:10:00.000Z",
        "voteCount": 2,
        "content": "Looking for \"business metrics to track,\" I think C could be the most important metric. Although, option B is also a good choice."
      },
      {
        "date": "2023-10-22T04:44:00.000Z",
        "voteCount": 1,
        "content": "C: Business metric i.e. outcome driven"
      },
      {
        "date": "2023-07-31T11:49:00.000Z",
        "voteCount": 3,
        "content": "\"Business metrics\" does suggest that the question is looking for metrics that are relevant to the business goals of the company, rather than purely technical metrics. In that case, C.could be a good choice. User engagement is an important metric for any online service, as it reflects how much users are enjoying and using the product. In the context of a multiplayer game, the number of battles played daily per user can indicate how well the model is doing in creating balanced teams that are enjoyable to play against. If the model is successful in creating balanced teams, then users are likely to play more games, which would increase user engagement.\n\nTherefore, C could be a suitable choice to track the performance of the model."
      },
      {
        "date": "2023-07-05T13:36:00.000Z",
        "voteCount": 2,
        "content": "The focus is to obtain a model that assigns players to teams with players with similar level of skill (or average team 1 skill == average team 2 skill)\n\nA: A fast queue assignment may not focus on pearing players with the same levels of skills. A random assignment would work.\n\nB: This would be an option but is more difficult to measure than C, we don\u2019t know If we have a measure of skill level. Also, for new players this metric would not be available at the beginning. I think  \u201cThere are many new players every day.\u201d is a key point important to discard answer B.\n\nC: Players play more games daily \u2190 players enjoy the game more frequently and the other way round should also apply. Easy to measure also for new players.\n\nD:This focus on costs and revenue not on players matchmaking.\n\nI would go with C."
      },
      {
        "date": "2023-05-14T09:28:00.000Z",
        "voteCount": 3,
        "content": "C because \"user engagement\" is a business metric https://support.google.com/analytics/answer/11109416?hl=en"
      },
      {
        "date": "2023-05-08T23:29:00.000Z",
        "voteCount": 1,
        "content": "Went with C"
      },
      {
        "date": "2023-04-21T10:31:00.000Z",
        "voteCount": 3,
        "content": "This is B, as it directly relates to our model's ability to predict player ability. There are many factors beyond our model which will impact user engagement (e.g. whether the game is actually enjoyable) so it's not a good measurement of the model performance"
      },
      {
        "date": "2023-04-16T23:32:00.000Z",
        "voteCount": 1,
        "content": "Answer C"
      },
      {
        "date": "2023-03-29T21:21:00.000Z",
        "voteCount": 4,
        "content": "The question is asking about \"available players\".  Therefore, the business metric is the user engagement."
      },
      {
        "date": "2023-03-28T07:02:00.000Z",
        "voteCount": 1,
        "content": "Asks for &gt;business metric&lt;, and problem states \"user research indicates that the game is more enjoyable when battles have players with similar skill levels.\", which means more battles per user if your model is performing well."
      },
      {
        "date": "2023-03-14T09:14:00.000Z",
        "voteCount": 4,
        "content": "It's C. The question specifically asks for a business metric. Precision and recall are not business metrics, but user engagement is"
      },
      {
        "date": "2023-03-14T07:25:00.000Z",
        "voteCount": 3,
        "content": "The template uses the 'ability' to create teams. For this, we can conclude that the system measures the player's skill. So, nothing better than comparing the predict ability with the actual ability to understand the performance of the model."
      },
      {
        "date": "2023-03-07T05:49:00.000Z",
        "voteCount": 4,
        "content": "A. Average time players wait before being assigned to a team\nB. Precision and recall of assigning players to teams based on their predicted versus actual ability\n\nThese two metrics are the most relevant for measuring the performance of the model in assigning players to teams based on skill level. The average wait time can indicate whether the model is making efficient and quick team assignments, while precision and recall can measure the accuracy of the model's predictions. It's important to balance precision and recall since assigning players to a team with a large difference in skill level could have a negative impact on the players' gaming experience.\n\nC and D are also important metrics to track, but they may not be as directly tied to the performance of the team assignment model. User engagement can indicate the success of the overall gaming experience, but it can be influenced by other factors beyond team assignments. The rate of return is also an important metric, but it may not be a direct measure of the success of the team assignment model."
      },
      {
        "date": "2023-04-16T23:31:00.000Z",
        "voteCount": 2,
        "content": "Answer C , user engagement"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 115,
    "url": "https://www.examtopics.com/discussions/google/view/91959-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You are building an ML model to predict trends in the stock market based on a wide range of factors. While exploring the data, you notice that some features have a large range. You want to ensure that the features with the largest magnitude don\u2019t overfit the model. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStandardize the data by transforming it with a logarithmic function.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tApply a principal component analysis (PCA) to minimize the effect of any particular feature.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a binning strategy to replace the magnitude of each feature with the appropriate bin number.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNormalize the data by scaling it to have values between 0 and 1.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 23,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 12,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-04-25T19:42:00.000Z",
        "voteCount": 4,
        "content": "D. Normalize the data by scaling it to have values between 0 and 1 (Min-Max scaling): This technique ensures all features contribute proportionally to the model's learning process.\n\npen_spark\nexpand_more It prevents features with a larger magnitude from dominating the model and reduces the risk of overfitting.expand_more"
      },
      {
        "date": "2024-04-25T19:43:00.000Z",
        "voteCount": 1,
        "content": "A. Standardize the data by transforming it with a logarithmic function: While logarithmic transformation can help compress the range of skewed features, it might not be suitable for all features, and it can introduce non-linear relationships that might not be ideal for all machine learning algorithms.\n\nB. Apply a principal component analysis (PCA) to minimize the effect of any particular feature: PCA is a dimensionality reduction technique that can be useful, but its primary function is to reduce the number of features, not specifically address differences in feature scales.\n\nC. Use a binning strategy to replace the magnitude of each feature with the appropriate bin number: Binning can introduce information loss and might not capture the nuances within each bin, potentially affecting the model's accuracy."
      },
      {
        "date": "2024-04-21T02:45:00.000Z",
        "voteCount": 1,
        "content": "agree with pico"
      },
      {
        "date": "2023-11-16T07:23:00.000Z",
        "voteCount": 4,
        "content": "Not A because a logarithmic transformation may be appropriate for data with a skewed distribution, but it doesn't necessarily address the issue of features having different scales."
      },
      {
        "date": "2023-11-11T08:41:00.000Z",
        "voteCount": 1,
        "content": "Features with a larger magnitude might still dominate after a log transformation if the range of values is significantly different from other features. Scaling is better, will go with Option D"
      },
      {
        "date": "2023-08-08T06:07:00.000Z",
        "voteCount": 1,
        "content": "by abylead: Min-Max scaling is a popular technique for normalizing stock price data. Logs are commonly used in finance to normalize relative data, such as returns.https://itadviser.dev/stock-market-data-normalization-for-time-series/"
      },
      {
        "date": "2023-07-23T09:04:00.000Z",
        "voteCount": 2,
        "content": "The correct answer is D. Min-max scaling will render all variables comparable by bringing them to a common ground.\n\nA is wrong for the following reasons:\n1. It is never mentioned that all variables are positive. If some columns have negative values, log transformation is not applicable.\n2. Log transformation of variables having small positive values (close to 0) will increase their magnitude. For example, ln(0.0001) = -9.2, which will increase this variable's effect considerably."
      },
      {
        "date": "2023-07-13T02:39:00.000Z",
        "voteCount": 1,
        "content": "D is the right answer"
      },
      {
        "date": "2023-07-08T09:22:00.000Z",
        "voteCount": 1,
        "content": "go for D, z-score. This question doesn't mention outlier,  just large range.\nreason why not log transformation:\nlog transformation is more suitable for addressing skewed distributions and reducing the impact of outliers. It compresses the range of values, especially for features with a large dynamic range. While it can help normalize the distribution, it doesn't directly address the issue of feature magnitude overpowering the model."
      },
      {
        "date": "2023-07-07T23:52:00.000Z",
        "voteCount": 1,
        "content": "From my point of view, log transformation is more tolerant to outliers. Thus, went to A."
      },
      {
        "date": "2023-07-11T11:25:00.000Z",
        "voteCount": 1,
        "content": "n cases where the data has significant skewness or a large number of outliers, option A (log transformation) might be more suitable. However, if the primary concern is to equalize the influence of features with different magnitudes and the data is not heavily skewed or has few outliers, option D (normalizing the data) would be more appropriate."
      },
      {
        "date": "2023-06-26T00:13:00.000Z",
        "voteCount": 2,
        "content": "See https://developers.google.com/machine-learning/data-prep/transform/normalization"
      },
      {
        "date": "2023-05-14T09:17:00.000Z",
        "voteCount": 1,
        "content": "A is a better option  because Log transform data used when we want a heavily skewed feature to be transformed into a normal distribution as close as possible, because when you normalize data using Minimum Maximum scaler, It doesn't work well with many outliers and its prone to unexpected behaviours if values go out of the given range in the test set. It is a less popular alternative to scaling."
      },
      {
        "date": "2023-07-11T11:24:00.000Z",
        "voteCount": 1,
        "content": "If your data is heavily skewed and has a significant number of outliers, log transformation (option A) might be a better choice. However, if your primary concern is to ensure that the features with the largest magnitudes don't overfit the model and the data does not have a significant skew or too many outliers, normalizing the data (option D) would be more appropriate."
      },
      {
        "date": "2023-05-09T01:12:00.000Z",
        "voteCount": 1,
        "content": "The challenge is the \u201cscale\u201d (significant variations in magnitude and spread): https://stats.stackexchange.com/questions/462380/does-data-normalization-reduce-over-fitting-when-training-a-model,\napparently largely used anyhow: https://itadviser.dev/stock-market-data-normalization-for-time-series/."
      },
      {
        "date": "2023-05-09T01:13:00.000Z",
        "voteCount": 1,
        "content": "Even if binning \u201cprevents overfitting and increases the robustness of the model\u201d: https://www.analyticsvidhya.com/blog/2020/10/getting-started-with-feature-engineering, \nthe disadvantage is that information is lost, particularly on features sharper than the binning: https://www.kaggle.com/questions-and-answers/171942,\nand then you need to reasonably re-adjust the binning to spot the moving target \u201ctrends\u201d [excluding C]: https://stats.stackexchange.com/questions/230750/when-should-we-discretize-bin-continuous-independent-variables-features-and-when."
      },
      {
        "date": "2023-05-09T01:13:00.000Z",
        "voteCount": 1,
        "content": "\u201c(\u2026) some features have a large range\u201d, possible presence of outliers exclude standardization [excluding A]: https://www.analyticsvidhya.com/blog/2020/04/feature-scaling-machine-learning-normalization-standardization/. \n\u201c(\u2026) a wide range of factors\u201d, PCA transform the data so that it can be described with fewer dimensions / features: https://en.wikipedia.org/wiki/Principal_component_analysis, but [excluding B]: it asks to \u201censure that the features with largest magnitude don\u2019t overfit the model\u201d."
      },
      {
        "date": "2023-03-30T01:04:00.000Z",
        "voteCount": 1,
        "content": "The question doesn't talk about the skewness within each feature. It talks about normalizing the effect of features with large range. So scaling each feature within (0,1) range will solve the problem"
      },
      {
        "date": "2023-03-28T07:05:00.000Z",
        "voteCount": 1,
        "content": "Really need more info to answer this: what does \"large range\" mean? Distribution follows a power law --&gt; use log(). Or are they more evenly/linearly distributed --&gt; use (0,1) scaling."
      },
      {
        "date": "2023-03-14T07:35:00.000Z",
        "voteCount": 3,
        "content": "I think C could be a better choice. Bucketizing the data we can fix the distribution problem by bins. \n\nin letter A, standardization by log could not be effective if the range of the data has negative and positive values. \n\nIn letter D, definitely normalization does not resolve the skew problem. Data normalization assumes that data has some normal distribution. \n\nhttps://medium.com/analytics-vidhya/data-transformation-for-numeric-features-fb16757382c0"
      },
      {
        "date": "2023-03-07T05:47:00.000Z",
        "voteCount": 1,
        "content": "D. Normalize the data by scaling it to have values between 0 and 1.\n\nStandardization and normalization are common techniques to preprocess the data to be more suitable for machine learning models. Normalization scales the data to be within a specific range (commonly between 0 and 1 or -1 and 1), which can help prevent features with large magnitudes from dominating the model. This approach is especially useful when using models that are sensitive to the magnitude of features, such as distance-based models or neural networks."
      },
      {
        "date": "2023-02-24T15:23:00.000Z",
        "voteCount": 1,
        "content": "https://developers.google.com/machine-learning/data-prep/transform/normalization#log-scaling"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 116,
    "url": "https://www.examtopics.com/discussions/google/view/92322-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You work for a biotech startup that is experimenting with deep learning ML models based on properties of biological organisms. Your team frequently works on early-stage experiments with new architectures of ML models, and writes custom TensorFlow ops in C++. You train your models on large datasets and large batch sizes. Your typical batch size has 1024 examples, and each example is about 1 MB in size. The average size of a network with all weights and embeddings is 20 GB. What hardware should you choose for your models?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA cluster with 2 n1-highcpu-64 machines, each with 8 NVIDIA Tesla V100 GPUs (128 GB GPU memory in total), and a n1-highcpu-64 machine with 64 vCPUs and 58 GB RAM",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA cluster with 2 a2-megagpu-16g machines, each with 16 NVIDIA Tesla A100 GPUs (640 GB GPU memory in total), 96 vCPUs, and 1.4 TB RAM",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA cluster with an n1-highcpu-64 machine with a v2-8 TPU and 64 GB RAM",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA cluster with 4 n1-highcpu-96 machines, each with 96 vCPUs and 86 GB RAM\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 16,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 7,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-05-20T18:57:00.000Z",
        "voteCount": 5,
        "content": "D: use CPU when models that contain many custom TensorFlow operations written in C++\nhttps://cloud.google.com/tpu/docs/intro-to-tpu#cpus"
      },
      {
        "date": "2024-03-07T03:35:00.000Z",
        "voteCount": 2,
        "content": "B looks like unleashing a rocket launcher to swat a fly (\"early-stage experiments\"). D is enough (c++)."
      },
      {
        "date": "2023-07-30T11:54:00.000Z",
        "voteCount": 1,
        "content": "While it is true that using CPUs can be more efficient when dealing with custom TensorFlow operations written in C++, it is important to consider the specific requirements of your models. In his case, we mentioned large batch sizes (1024 examples), large example sizes (1 MB each), and large network sizes (20 GB). 4 n1-highcpu-96 machines, each with 96 vCPUs and 86 GB RAM. While this configuration would provide a high number of vCPUs for custom TensorFlow operations, it lacks the GPU memory and overall RAM necessary to handle the large batch sizes and network sizes of your models."
      },
      {
        "date": "2023-07-22T22:18:00.000Z",
        "voteCount": 1,
        "content": "B: https://cloud.google.com/tpu/docs/intro-to-tpu#cpus"
      },
      {
        "date": "2024-04-12T23:38:00.000Z",
        "voteCount": 1,
        "content": "so D, not B..."
      },
      {
        "date": "2023-06-08T08:09:00.000Z",
        "voteCount": 3,
        "content": "D: use CPU when models that contain many custom TensorFlow operations written in C++\nhttps://cloud.google.com/tpu/docs/intro-to-tpu#cpus"
      },
      {
        "date": "2023-05-25T23:39:00.000Z",
        "voteCount": 3,
        "content": "Wouldn't all PC's work here? I could do this model on my own home PC just fine."
      },
      {
        "date": "2023-05-09T01:36:00.000Z",
        "voteCount": 2,
        "content": "\u201cwrites custom TensorFlow ops in C++\u201d -&gt; use CPUs when \u201cModels that contain many custom TensorFlow operations written in C++\u201d: https://cloud.google.com/tpu/docs/intro-to-tpu#when_to_use_tpus"
      },
      {
        "date": "2023-04-15T05:39:00.000Z",
        "voteCount": 4,
        "content": "The best hardware for your models would be a cluster with 2 a2-megagpu-16g machines, each with 16 NVIDIA Tesla A100 GPUs (640 GB GPU memory in total), 96 vCPUs, and 1.4 TB RAM.\n\nThis hardware will give you the following benefits:\n\nHigh GPU memory: Each A100 GPU has 40 GB of memory, which is more than enough to store the weights and embeddings of your models.\nLarge batch sizes: With 16 GPUs per machine, you can train your models with large batch sizes, which will improve training speed.\nFast CPUs: The 96 vCPUs on each machine will provide the processing power you need to run your custom TensorFlow ops in C++.\nAdequate RAM: The 1.4 TB of RAM on each machine will ensure that your models have enough memory to train and run.\nThe other options are not as suitable for your needs. Option A has less GPU memory, which will slow down training. Option B has more GPU memory, but it is also more expensive. Option C has a TPU, which is a good option for some deep learning tasks, but it is not as well-suited for your needs as a GPU cluster. Option D has more vCPUs and RAM, but it does not have enough GPU memory to train your models.\n\nTherefore, the best hardware for your models is a cluster with 2 a2-megagpu-16g machines."
      },
      {
        "date": "2023-03-07T05:45:00.000Z",
        "voteCount": 3,
        "content": "To determine the appropriate hardware for training the models, we need to calculate the required memory and processing power based on the size of the model and the size of the input data.\n\nGiven that the batch size is 1024 and each example is 1 MB, the total size of each batch is 1024 * 1 MB = 1024 MB = 1 GB. Therefore, we need to load 1 GB of data into memory for each batch.\n\nThe total size of the network is 20 GB, which means that it can fit in the memory of most modern GPUs."
      },
      {
        "date": "2023-01-18T00:36:00.000Z",
        "voteCount": 1,
        "content": "It's D"
      },
      {
        "date": "2023-01-18T00:42:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/tpu/docs/tpus"
      },
      {
        "date": "2022-12-21T03:25:00.000Z",
        "voteCount": 3,
        "content": "D\nCPUs are recommended for TensorFlow ops written in C++\n- https://cloud.google.com/tpu/docs/tensorflow-ops (Cloud TPU only supports Python)"
      },
      {
        "date": "2023-01-18T04:44:00.000Z",
        "voteCount": 3,
        "content": "GPU can apply through C++ implement,but C rule out for sure."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 117,
    "url": "https://www.examtopics.com/discussions/google/view/91413-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You are an ML engineer at an ecommerce company and have been tasked with building a model that predicts how much inventory the logistics team should order each month. Which approach should you take?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a clustering algorithm to group popular items together. Give the list to the logistics team so they can increase inventory of the popular items.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a regression model to predict how much additional inventory should be purchased each month. Give the results to the logistics team at the beginning of the month so they can increase inventory by the amount predicted by the model.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a time series forecasting model to predict each item's monthly sales. Give the results to the logistics team so they can base inventory on the amount predicted by the model.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a classification model to classify inventory levels as UNDER_STOCKED, OVER_STOCKED, and CORRECTLY_STOCKEGive the report to the logistics team each month so they can fine-tune inventory levels."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 16,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-12-13T05:58:00.000Z",
        "voteCount": 9,
        "content": "This type of model is well-suited to predicting inventory levels because it can take into account trends and patterns in the data over time, such as seasonal fluctuations in demand or changes in customer behavior."
      },
      {
        "date": "2023-05-09T01:51:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/learn/what-is-time-series\n\"For example, a large retail store may have millions of items to forecast so that inventory is available when demand is high, and not overstocked when demand is low.\""
      },
      {
        "date": "2023-03-07T05:44:00.000Z",
        "voteCount": 1,
        "content": "Answer C"
      },
      {
        "date": "2023-01-18T00:46:00.000Z",
        "voteCount": 1,
        "content": "Yup it's C (Time series forecasting)"
      },
      {
        "date": "2023-01-03T06:08:00.000Z",
        "voteCount": 1,
        "content": "Time-series forecasting model is the key expression, for me."
      },
      {
        "date": "2022-12-21T03:27:00.000Z",
        "voteCount": 3,
        "content": "C (by experience)\n Use a time series forecasting model to predict each item's monthly sales. Give the results to the logistics team so they can base inventory on the amount predicted by the model."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 118,
    "url": "https://www.examtopics.com/discussions/google/view/91960-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You are building a TensorFlow model for a financial institution that predicts the impact of consumer spending on inflation globally. Due to the size and nature of the data, your model is long-running across all types of hardware, and you have built frequent checkpointing into the training process. Your organization has asked you to minimize cost. What hardware should you choose?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA Vertex AI Workbench user-managed notebooks instance running on an n1-standard-16 with 4 NVIDIA P100 GPUs",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA Vertex AI Workbench user-managed notebooks instance running on an n1-standard-16 with an NVIDIA P100 GPU",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA Vertex AI Workbench user-managed notebooks instance running on an n1-standard-16 with a non-preemptible v3-8 TPU",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA Vertex AI Workbench user-managed notebooks instance running on an n1-standard-16 with a preemptible v3-8 TPU\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 16,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-21T03:31:00.000Z",
        "voteCount": 7,
        "content": "D\nyou have built frequent checkpointing into the training process / minimize cost -&gt; preemptible"
      },
      {
        "date": "2024-09-04T11:04:00.000Z",
        "voteCount": 1,
        "content": "For financial institutions, reliability and minimizing interruptions are crucial. While preemptible instances are cost-effective, they do come with the risk of being terminated unexpectedly, which might not be ideal for critical financial applications."
      },
      {
        "date": "2023-05-09T01:58:00.000Z",
        "voteCount": 2,
        "content": "Follows same principle as #70"
      },
      {
        "date": "2023-04-15T05:50:00.000Z",
        "voteCount": 1,
        "content": "Preemptible v3-8 TPUs are the most cost-effective option for training large TensorFlow models. They are up to 80% cheaper than non-preemptible v3-8 TPUs, and they are only preempted if Google Cloud needs the resources for other workloads.\n\nIn this case, the model is long-running and checkpointing is used. This means that the training process can be interrupted and resumed without losing any progress. Therefore, preemptible TPUs are a safe choice, as the training process will not be interrupted if the TPU is preempted.\n\nThe other options are not as cost-effective."
      },
      {
        "date": "2023-03-09T12:04:00.000Z",
        "voteCount": 1,
        "content": "Answer D"
      },
      {
        "date": "2023-01-07T06:13:00.000Z",
        "voteCount": 4,
        "content": "Frequent checkpoints --&gt; Preemptible --&gt; D"
      },
      {
        "date": "2022-12-17T18:47:00.000Z",
        "voteCount": 1,
        "content": "preemptible is the keyword to me"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 119,
    "url": "https://www.examtopics.com/discussions/google/view/91415-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You work for a company that provides an anti-spam service that flags and hides spam posts on social media platforms. Your company currently uses a list of 200,000 keywords to identify suspected spam posts. If a post contains more than a few of these keywords, the post is identified as spam. You want to start using machine learning to flag spam posts for human review. What is the main advantage of implementing machine learning for this business case?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPosts can be compared to the keyword list much more quickly.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNew problematic phrases can be identified in spam posts.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA much longer keyword list can be used to flag spam posts.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSpam posts can be flagged using far fewer keywords."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 14,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-12-13T06:02:00.000Z",
        "voteCount": 7,
        "content": "I vote B. Machine learning algorithms can learn to identify spam posts based on a wider range of factors, such as the content of the post, the user's behavior, and the context in which the post appears."
      },
      {
        "date": "2023-05-09T02:40:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/blog/topics/developers-practitioners/how-spam-detection-taught-us-better-tech-support\n\"Borrowing spam tech\n(...) Those engineers had thought through \u201chow do we detect a new spam campaign quickly?\u201d  Spammers rapidly send bulk messages with slight variations in content (noise, misspellings, etc.) Most classification attempts would become a game of cat and mouse since it takes classifiers some time to learn about new patterns.\nInvoking a trend identification engine using unsupervised density clustering on unstructured text unlocked the ability for Gmail to detect ephemeral spam campaigns more quickly.\""
      },
      {
        "date": "2023-03-09T12:04:00.000Z",
        "voteCount": 1,
        "content": "Answer B"
      },
      {
        "date": "2023-01-03T05:59:00.000Z",
        "voteCount": 3,
        "content": "B screams machine learning with every letter."
      },
      {
        "date": "2022-12-21T03:33:00.000Z",
        "voteCount": 2,
        "content": "B make sense &amp; I agree with mill_sypro"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 120,
    "url": "https://www.examtopics.com/discussions/google/view/91418-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "One of your models is trained using data provided by a third-party data broker. The data broker does not reliably notify you of formatting changes in the data. You want to make your model training pipeline more robust to issues like this. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse TensorFlow Data Validation to detect and flag schema anomalies.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse TensorFlow Transform to create a preprocessing component that will normalize data to the expected distribution, and replace values that don\u2019t match the schema with 0.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse tf.math to analyze the data, compute summary statistics, and flag statistical anomalies.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse custom TensorFlow functions at the start of your model training to detect and flag known formatting errors."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 14,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-09-04T11:13:00.000Z",
        "voteCount": 1,
        "content": "i would choose A and B because For the model to be truly robust, it needs to adapt to new formats, not just detect and flag anomalies. In this case, combining detection with adaptive preprocessing would be the best approach"
      },
      {
        "date": "2023-05-09T03:36:00.000Z",
        "voteCount": 1,
        "content": "Went with A"
      },
      {
        "date": "2023-03-18T10:32:00.000Z",
        "voteCount": 2,
        "content": "You need to know problem b4 fixing w transform, hence A"
      },
      {
        "date": "2023-03-07T05:44:00.000Z",
        "voteCount": 1,
        "content": "Answer A"
      },
      {
        "date": "2023-02-08T22:36:00.000Z",
        "voteCount": 1,
        "content": "https://www.tensorflow.org/tfx/guide/tfdv#schema_based_example_validation"
      },
      {
        "date": "2023-01-03T05:54:00.000Z",
        "voteCount": 1,
        "content": "Tensorflow Data Validation (TFDV) can analyze training and serving data to: compute descriptive statistics, infer a schema, detect data anomalies. A."
      },
      {
        "date": "2022-12-21T03:39:00.000Z",
        "voteCount": 3,
        "content": "A\n- https://www.tensorflow.org/tfx/data_validation/get_started"
      },
      {
        "date": "2022-12-13T06:07:00.000Z",
        "voteCount": 4,
        "content": "TensorFlow Data Validation (TFDV) is a library that can help you detect and flag anomalies in your dataset, such as changes in the schema or data types.\nhttps://www.tensorflow.org/tfx/data_validation/get_started"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 121,
    "url": "https://www.examtopics.com/discussions/google/view/91961-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You work for a company that is developing a new video streaming platform. You have been asked to create a recommendation system that will suggest the next video for a user to watch. After a review by an AI Ethics team, you are approved to start development. Each video asset in your company\u2019s catalog has useful metadata (e.g., content type, release date, country), but you do not have any historical user event data. How should you build the recommendation system for the first version of the product?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLaunch the product without machine learning. Present videos to users alphabetically, and start collecting user event data so you can develop a recommender model in the future.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLaunch the product without machine learning. Use simple heuristics based on content metadata to recommend similar videos to users, and start collecting user event data so you can develop a recommender model in the future.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLaunch the product with machine learning. Use a publicly available dataset such as MovieLens to train a model using the Recommendations AI, and then apply this trained model to your data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLaunch the product with machine learning. Generate embeddings for each video by training an autoencoder on the content metadata using TensorFlow. Cluster content based on the similarity of these embeddings, and then recommend videos from the same cluster."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 18,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 8,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-02-28T11:00:00.000Z",
        "voteCount": 1,
        "content": "D is overkill"
      },
      {
        "date": "2024-02-01T13:22:00.000Z",
        "voteCount": 2,
        "content": "My choice is B. This is because both B and D have the same goal (recommendation based on content), but option B is simpler for this initial context."
      },
      {
        "date": "2023-07-11T11:05:00.000Z",
        "voteCount": 3,
        "content": "This is because you do not have any historical user event data, so you cannot use a collaborative filtering approach to build a recommender system. However, you can still use simple heuristics based on content metadata to recommend similar videos to users. For example, you could recommend videos that are in the same genre, have the same release date, or are from the same country.\nYou should also start collecting user event data as soon as possible. This data will be valuable for training a recommender model in the future.\n\nOption D is a more complex approach that would require you to have more expertise in machine learning.(FOR THE FIRST VERSION OF THE PRODUCT)"
      },
      {
        "date": "2023-06-08T08:11:00.000Z",
        "voteCount": 3,
        "content": "B:\nhttps://developers.google.com/machine-learning/guides/rules-of-ml"
      },
      {
        "date": "2023-05-20T18:59:00.000Z",
        "voteCount": 1,
        "content": "B since we can't use othe encoded data to test on some other system"
      },
      {
        "date": "2023-05-09T03:52:00.000Z",
        "voteCount": 1,
        "content": "Went with D"
      },
      {
        "date": "2023-04-25T16:16:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/blog/topics/developers-practitioners/meet-ais-multitool-vector-embeddings\nOption D is about creating clusters based on the content metadata and using that to provide recos to users"
      },
      {
        "date": "2023-03-18T10:50:00.000Z",
        "voteCount": 1,
        "content": "Key is the mention \u201cfirst version of product\u201d"
      },
      {
        "date": "2023-03-14T08:02:00.000Z",
        "voteCount": 2,
        "content": "It is possible to create a recommendation system just using metadata information, like in:\n\nhttps://developers.google.com/machine-learning/crash-course/embeddings/categorical-input-data\n\nOne of the initial problems of recommender systems is precisely the lack of data for collaborative recommendation. However, this does not prevent other recommendation algorithms, for example, those that use content suggestion."
      },
      {
        "date": "2024-02-01T13:22:00.000Z",
        "voteCount": 1,
        "content": "Change my mind: My choice is B. This is because both B and D have the same goal (recommendation based on content), but option B is simpler for this initial context."
      },
      {
        "date": "2023-03-07T05:43:00.000Z",
        "voteCount": 2,
        "content": "Since you do not have any historical user event data, options C and D are not suitable. In this scenario, it is better to start with a simpler approach, so options A and B are the most suitable. However, option B is preferred because it uses some logic based on content metadata to provide recommendations, which may be more personalized and relevant than presenting videos in alphabetical order. Additionally, collecting user event data from the beginning will help improve the recommendation system in the future."
      },
      {
        "date": "2023-02-09T10:45:00.000Z",
        "voteCount": 1,
        "content": "ans B, you need something easier to implement"
      },
      {
        "date": "2023-01-04T22:25:00.000Z",
        "voteCount": 4,
        "content": "https://developers.google.com/machine-learning/crash-course/embeddings/categorical-input-data"
      },
      {
        "date": "2022-12-21T13:51:00.000Z",
        "voteCount": 3,
        "content": "B\n- https://developers.google.com/machine-learning/guides/rules-of-ml"
      },
      {
        "date": "2022-12-17T19:05:00.000Z",
        "voteCount": 2,
        "content": "because no user event data, the pretrain model won't help"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 122,
    "url": "https://www.examtopics.com/discussions/google/view/91425-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You recently built the first version of an image segmentation model for a self-driving car. After deploying the model, you observe a decrease in the area under the curve (AUC) metric. When analyzing the video recordings, you also discover that the model fails in highly congested traffic but works as expected when there is less traffic. What is the most likely reason for this result?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe model is overfitting in areas with less traffic and underfitting in areas with more traffic.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAUC is not the correct metric to evaluate this classification model.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tToo much data representing congested areas was used for model training.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGradients become small and vanish while backpropagating from the output to input nodes."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-09-22T05:22:00.000Z",
        "voteCount": 1,
        "content": "AOC makes sense in a binary classification problem, but that's not the case here. That is the biggest red flag right there in the question."
      },
      {
        "date": "2024-04-22T18:39:00.000Z",
        "voteCount": 3,
        "content": "Overfitting and Underfitting: Overfitting describes a model that performs well on the training data but struggles with unseen data. Underfitting signifies a model that hasn't learned enough patterns from the training data.\nCongested Traffic as Unseen Data: If the training data primarily consisted of scenarios with less traffic, the model might not have been exposed to enough examples of congested situations. This would make congested traffic act like unseen data, leading to underfitting and poor performance."
      },
      {
        "date": "2023-09-10T10:25:00.000Z",
        "voteCount": 2,
        "content": "very tricky, here is my view:\n\nA. The model is overfitting in areas with less traffic and underfitting in areas with more traffic. Most Voted\n&gt; I dont think so because: \"the model fails in highly congested traffic but works as expected when there is less traffic\" which means it is NOT OVERFITTING with less traffic. Actually the contrary would make more sense.\nB. AUC is not the correct metric to evaluate this classification model.\n&gt; My option. Image Segmentation is about ditinguishing objects, not sure AUC is right for this.\nC. Too much data representing congested areas was used for model training.\n&gt; Cant be. THat would actually make it perform at least as good.\nD. Gradients become small and vanish while backpropagating from the output to input nodes.\n&gt; No clue."
      },
      {
        "date": "2023-11-10T11:48:00.000Z",
        "voteCount": 2,
        "content": "Overfitting occurs when a model learns the details and noise in the training data to the extent that it negatively impacts the performance of the model on new data. If your training data included more examples of less congested areas, the model might have overfitted to these scenarios and, as a result, performs poorly in unrepresented or underrepresented situations, such as heavy traffic.\n\nAUC (Area Under the Curve) is a widely used metric for evaluating the performance of classification models. However, it might not be the sole or most appropriate metric for a complex task like image segmentation in self-driving cars. Other metrics like Intersection over Union (IoU) or pixel accuracy might be more relevant for evaluating segmentation tasks. Still, this doesn't explain the model's performance drop in different traffic conditions."
      },
      {
        "date": "2023-07-19T05:33:00.000Z",
        "voteCount": 2,
        "content": "D. Gradients become small and vanish while backpropagating from the output to input nodes.\n\nThis issue is known as the vanishing gradient problem, which can occur during the training of deep neural networks. In highly congested traffic scenes, there might be complex patterns and details that the image segmentation model needs to capture. However, if the model architecture is too deep and the gradients become very small during backpropagation, the model may struggle to update its weights effectively to learn these complex patterns. As a result, the model may fail to correctly segment objects in congested traffic scenes, leading to a decrease in performance.\n\nVanishing gradients can prevent the model from effectively learning representations and features in the deeper layers of the network. It's possible that the model is working fine in less congested areas because the patterns are simpler and easier to learn, allowing the gradients to propagate more effectively."
      },
      {
        "date": "2023-10-23T07:56:00.000Z",
        "voteCount": 1,
        "content": "There's a paper saying that: https://www.sciencedirect.com/science/article/abs/pii/S0925231218313821"
      },
      {
        "date": "2023-09-10T10:26:00.000Z",
        "voteCount": 1,
        "content": "mmm, possibly"
      },
      {
        "date": "2023-09-10T10:27:00.000Z",
        "voteCount": 1,
        "content": "dont think so because: \"the model fails in highly congested traffic but works as expected when there is less traffic\" which means it is NOT OVERFITTING with less traffic. Actually the contrary would make more sense."
      },
      {
        "date": "2023-05-09T04:19:00.000Z",
        "voteCount": 1,
        "content": "Went with A"
      },
      {
        "date": "2023-04-17T15:04:00.000Z",
        "voteCount": 1,
        "content": "The most likely reason for this result is the model is overfitting in areas with less traffic and underfitting in areas with more traffic.\n\nProbably because the model was trained on a dataset that did not have enough examples of congested traffic. As a result, the model is not able to generalise well. When the model is validated on congested traffic, it makes mistakes because it has not seen this type of data before."
      },
      {
        "date": "2023-09-10T10:27:00.000Z",
        "voteCount": 1,
        "content": "dont think so because: \"the model fails in highly congested traffic but works as expected when there is less traffic\" which means it is NOT OVERFITTING with less traffic. Actually the contrary would make more sense."
      },
      {
        "date": "2023-03-09T12:01:00.000Z",
        "voteCount": 1,
        "content": "Answer A"
      },
      {
        "date": "2023-09-10T10:27:00.000Z",
        "voteCount": 2,
        "content": "dont think so because: \"the model fails in highly congested traffic but works as expected when there is less traffic\" which means it is NOT OVERFITTING with less traffic. Actually the contrary would make more sense."
      },
      {
        "date": "2023-02-09T10:47:00.000Z",
        "voteCount": 1,
        "content": "the model was trained with bias"
      },
      {
        "date": "2022-12-21T13:55:00.000Z",
        "voteCount": 3,
        "content": "A\nIt's an example of overfitting/underfitting problem"
      },
      {
        "date": "2022-12-13T06:21:00.000Z",
        "voteCount": 4,
        "content": "I vote A, it is likely that the model was trained on data that included mostly images of less congested traffic, and therefore did not generalize well to images of more congested traffic."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 123,
    "url": "https://www.examtopics.com/discussions/google/view/92318-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You are developing an ML model to predict house prices. While preparing the data, you discover that an important predictor variable, distance from the closest school, is often missing and does not have high variance. Every instance (row) in your data is important. How should you handle the missing data?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDelete the rows that have missing values.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tApply feature crossing with another column that does not have missing values.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPredict the missing values using linear regression.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReplace the missing values with zeros."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-22T18:43:00.000Z",
        "voteCount": 3,
        "content": "Preserves Information: Deleting rows (Option A) throws away valuable data, especially since every instance is important.\nNot Applicable Technique: Feature crossing (Option B) creates new features by multiplying existing features. It wouldn't address missing values directly.\nZero Imputation Might Bias: Replacing missing values with zeros (Option D) can introduce bias if zeros have a specific meaning in the data (e.g., distance cannot be zero)."
      },
      {
        "date": "2023-05-28T00:06:00.000Z",
        "voteCount": 3,
        "content": "Went with A: Predict the missing values using linear regression as the data does not have high variance."
      },
      {
        "date": "2023-05-09T04:25:00.000Z",
        "voteCount": 1,
        "content": "Went with C"
      },
      {
        "date": "2023-03-09T12:01:00.000Z",
        "voteCount": 1,
        "content": "Answer is C\nPredicting the missing values using linear regression can be a good approach, especially if the variable is important for the prediction. The values can be imputed using regression, where the missing variable can be the dependent variable, and other relevant variables can be used as predictors"
      },
      {
        "date": "2023-01-18T10:04:00.000Z",
        "voteCount": 1,
        "content": "Regression  https://cran.r-project.org/web/packages/miceRanger/vignettes/miceAlgorithm.html\n\u2022 Find linear or non-linear relationships between the missing feature and other\nfeatures\n\u2022 Most advanced technique: MICE (Multiple Imputation by Chained Equations)"
      },
      {
        "date": "2023-01-03T05:28:00.000Z",
        "voteCount": 1,
        "content": "It's C."
      },
      {
        "date": "2022-12-22T00:29:00.000Z",
        "voteCount": 1,
        "content": "My answer was based on the below article \nhttps://towardsdatascience.com/7-ways-to-handle-missing-values-in-machine-learning-1a6326adf79e"
      },
      {
        "date": "2022-12-22T00:29:00.000Z",
        "voteCount": 2,
        "content": "One of the ways to handle missing data is deleting the rows. but question here says that every row is important. so I think another possible option could be to predict the missing value. Option C could be correct !"
      },
      {
        "date": "2022-12-21T14:30:00.000Z",
        "voteCount": 2,
        "content": "C (not sure)"
      },
      {
        "date": "2022-12-21T02:48:00.000Z",
        "voteCount": 2,
        "content": "A no - Every row is important\nB no - product of other feature values with no values makes no sense to me\nD no - zero value would bias the model as zero distance from school has the highest value to model\nC yes - there is an approach using linear regression to predict missing values"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 124,
    "url": "https://www.examtopics.com/discussions/google/view/92398-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You are an ML engineer responsible for designing and implementing training pipelines for ML models. You need to create an end-to-end training pipeline for a TensorFlow model. The TensorFlow model will be trained on several terabytes of structured data. You need the pipeline to include data quality checks before training and model quality checks after training but prior to deployment. You want to minimize development time and the need for infrastructure maintenance. How should you build and orchestrate your training pipeline?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate the pipeline using Kubeflow Pipelines domain-specific language (DSL) and predefined Google Cloud components. Orchestrate the pipeline using Vertex AI Pipelines.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate the pipeline using TensorFlow Extended (TFX) and standard TFX components. Orchestrate the pipeline using Vertex AI Pipelines.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate the pipeline using Kubeflow Pipelines domain-specific language (DSL) and predefined Google Cloud components. Orchestrate the pipeline using Kubeflow Pipelines deployed on Google Kubernetes Engine.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate the pipeline using TensorFlow Extended (TFX) and standard TFX components. Orchestrate the pipeline using Kubeflow Pipelines deployed on Google Kubernetes Engine."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 22,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-22T18:49:00.000Z",
        "voteCount": 3,
        "content": "TFX for TensorFlow Models: TensorFlow Extended (TFX) is an end-to-end machine learning platform built on top of TensorFlow. It provides a set of pre-built components specifically designed for TensorFlow models, simplifying development and ensuring compatibility.\nVertex AI Pipelines for Orchestration: Vertex AI Pipelines, a managed service from Google Cloud, is ideal for orchestrating ML pipelines. It integrates seamlessly with TFX and provides features like monitoring, scheduling, and scaling, reducing infrastructure maintenance needs."
      },
      {
        "date": "2024-04-22T18:49:00.000Z",
        "voteCount": 4,
        "content": "A. Kubeflow Pipelines with Predefined Components: While Kubeflow Pipelines offer a DSL for building pipelines, using standard TFX components within Vertex AI Pipelines offers a more streamlined solution designed for TensorFlow models.\nC &amp; D. Kubeflow Pipelines with Manual Deployment: Both options involve using Kubeflow Pipelines, but deploying it on Google Kubernetes Engine requires additional infrastructure management compared to using the managed service, Vertex AI Pipelines."
      },
      {
        "date": "2023-07-08T09:25:00.000Z",
        "voteCount": 2,
        "content": "B.\nwhy not C?  as the question content mentioned, this model is built by tensorflow"
      },
      {
        "date": "2023-07-08T00:19:00.000Z",
        "voteCount": 2,
        "content": "B should be correct"
      },
      {
        "date": "2023-05-09T04:33:00.000Z",
        "voteCount": 3,
        "content": "Went with B"
      },
      {
        "date": "2023-03-29T02:29:00.000Z",
        "voteCount": 4,
        "content": "B.  \nStraight from the docs: https://cloud.google.com/vertex-ai/docs/pipelines/build-pipeline#sdk"
      },
      {
        "date": "2023-03-07T05:42:00.000Z",
        "voteCount": 2,
        "content": "B. Create the pipeline using TensorFlow Extended (TFX) and standard TFX components. Orchestrate the pipeline using Vertex AI Pipelines.\n\nTFX provides a set of standard components for building end-to-end ML pipelines, including data validation and model analysis. Vertex AI Pipelines is a fully managed service for building and orchestrating machine learning pipelines on Google Cloud."
      },
      {
        "date": "2023-01-03T05:24:00.000Z",
        "voteCount": 2,
        "content": "It's B!"
      },
      {
        "date": "2022-12-29T05:25:00.000Z",
        "voteCount": 3,
        "content": "Reference: https://www.tensorflow.org/tfx/guide/tfdv"
      },
      {
        "date": "2022-12-21T15:40:00.000Z",
        "voteCount": 3,
        "content": "B (not sure)"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 125,
    "url": "https://www.examtopics.com/discussions/google/view/91963-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You manage a team of data scientists who use a cloud-based backend system to submit training jobs. This system has become very difficult to administer, and you want to use a managed service instead. The data scientists you work with use many different frameworks, including Keras, PyTorch, theano, scikit-learn, and custom libraries. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Vertex AI Training to submit training jobs using any framework.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Kubeflow to run on Google Kubernetes Engine and submit training jobs through TFJob.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a library of VM images on Compute Engine, and publish these images on a centralized repository.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up Slurm workload manager to receive jobs that can be scheduled to run on your cloud infrastructure."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 13,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-22T18:51:00.000Z",
        "voteCount": 3,
        "content": "Managed Service: It eliminates the need to administer a complex backend system, reducing your team's workload.\nFramework Agnostic: Vertex AI Training supports various frameworks like Keras, PyTorch, scikit-learn, and custom libraries, aligning with your data scientists' needs."
      },
      {
        "date": "2024-04-22T18:52:00.000Z",
        "voteCount": 2,
        "content": "B. Kubeflow on GKE with TFJob: While Kubeflow offers framework flexibility, setting it up and managing it on Google Kubernetes Engine (GKE) adds complexity compared to a fully managed service like Vertex AI Training.\nC. VM Image Library: Creating and maintaining a library of VM images for every framework is cumbersome and doesn't scale well for various frameworks and custom libraries.\nD. Slurm Workload Manager: Slurm is a workload manager, not a training service.\n It wouldn't directly address the need for framework-agnostic training job submission."
      },
      {
        "date": "2024-02-22T06:57:00.000Z",
        "voteCount": 2,
        "content": "Answer A"
      },
      {
        "date": "2023-07-08T00:20:00.000Z",
        "voteCount": 2,
        "content": "replicated # 5"
      },
      {
        "date": "2023-05-28T00:23:00.000Z",
        "voteCount": 2,
        "content": "Went with A. Use the Vertex AI Training to submit training jobs using any framework. As the request states managed service I will discard Compute Engine and Kubernetes. I discarded D since Google is not in the picture"
      },
      {
        "date": "2023-03-09T11:59:00.000Z",
        "voteCount": 1,
        "content": "Answer A"
      },
      {
        "date": "2022-12-21T15:47:00.000Z",
        "voteCount": 3,
        "content": "A (similar question 5)"
      },
      {
        "date": "2022-12-17T19:17:00.000Z",
        "voteCount": 2,
        "content": "https://www.examtopics.com/discussions/google/view/54653-exam-professional-machine-learning-engineer-topic-1-question/"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 126,
    "url": "https://www.examtopics.com/discussions/google/view/91964-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You are training an object detection model using a Cloud TPU v2. Training time is taking longer than expected. Based on this simplified trace obtained with a Cloud TPU profile, what action should you take to decrease training time in a cost-efficient way?<br><br><img src=\"https://img.examtopics.com/professional-machine-learning-engineer/image1.png\">",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMove from Cloud TPU v2 to Cloud TPU v3 and increase batch size.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMove from Cloud TPU v2 to 8 NVIDIA V100 GPUs and increase batch size.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRewrite your input function to resize and reshape the input images.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRewrite your input function using parallel reads, parallel processing, and prefetch.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 16,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-12-20T13:52:00.000Z",
        "voteCount": 6,
        "content": "parallel reads, parallel processing, and prefetch is needed here"
      },
      {
        "date": "2024-04-24T20:15:00.000Z",
        "voteCount": 3,
        "content": "Optimizing the data pipeline with parallel reads, processing, and prefetching can significantly improve training speed on TPUs by reducing I/O wait times. This approach utilizes the TPU's capabilities more effectively and avoids extra costs associated with hardware upgrades."
      },
      {
        "date": "2024-04-24T20:15:00.000Z",
        "voteCount": 1,
        "content": "A. Moving to a different TPU version (v3) and increasing the batch size might improve training speed, but it's an expensive solution without a guarantee of the most efficient outcome.\nB. Switching to GPUs (V100) also increases costs and may not be optimized for your specific workload."
      },
      {
        "date": "2024-04-24T20:15:00.000Z",
        "voteCount": 1,
        "content": "(C) can be part of the preprocessing step, but it likely won't address the core issue if the bottleneck is related to how data is being fed into the training process."
      },
      {
        "date": "2023-05-09T04:51:00.000Z",
        "voteCount": 1,
        "content": "Went with D"
      },
      {
        "date": "2023-03-07T05:41:00.000Z",
        "voteCount": 1,
        "content": "Based on the profile, it appears that the Compute time is relatively low compared to the HostToDevice and DeviceToHost time. This suggests that the data transfer between the host (CPU) and the TPU device is a bottleneck. Therefore, the best action to decrease training time in a cost-efficient way would be to reduce the amount of data transferred between the host and the device."
      },
      {
        "date": "2022-12-21T15:50:00.000Z",
        "voteCount": 4,
        "content": "D\n- https://www.tensorflow.org/guide/data_performance"
      },
      {
        "date": "2022-12-17T19:20:00.000Z",
        "voteCount": 1,
        "content": "i didn't see v3 has any benefit than v2\nhttps://cloud.google.com/tpu/docs/system-architecture-tpu-vm#performance_benefits_of_tpu_v3_over_v2"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 127,
    "url": "https://www.examtopics.com/discussions/google/view/92402-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "While performing exploratory data analysis on a dataset, you find that an important categorical feature has 5% null values. You want to minimize the bias that could result from the missing values. How should you handle the missing values?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRemove the rows with missing values, and upsample your dataset by 5%.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReplace the missing values with the feature\u2019s mean.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReplace the missing values with a placeholder category indicating a missing value.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMove the rows with missing values to your validation dataset."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 12,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-24T20:18:00.000Z",
        "voteCount": 4,
        "content": "Minimizes Bias: Removing rows (A) with missing data can introduce bias if the missingness is not random.expand_more Upsampling the remaining data (A) might not address the underlying cause of missing values.\nUnsuitable for Categorical Features: Replacing with the mean (B) only works for numerical features.\nTransparency and Model Interpretation: A placeholder category (C) explicitly acknowledges the missing data and avoids introducing assumptions during model training. It also improves model interpretability.\nValidation Set Contamination (D): Moving rows with missing values to the validation set (D) contaminates the validation data and hinders its ability to assess model performance on unseen data.\nUsing a placeholder category creates a separate category for missing values, allowing the model to handle them explicitly. This approach is particularly suitable for categorical features with a relatively small percentage of missing values (like 5% in this case)."
      },
      {
        "date": "2024-04-26T00:09:00.000Z",
        "voteCount": 1,
        "content": "if B nominate mode instead of mean?"
      },
      {
        "date": "2023-05-09T05:19:00.000Z",
        "voteCount": 1,
        "content": "http://webcache.googleusercontent.com/search?q=cache:FzNjYfqNEZ0J:https://towardsdatascience.com/missing-values-dont-drop-them-f01b1d8ff557&amp;hl=de&amp;gl=de&amp;strip=1&amp;vwsrc=0\n\nSee also #62, #123"
      },
      {
        "date": "2023-05-09T05:24:00.000Z",
        "voteCount": 1,
        "content": "Also, tab \"Forecasting\":\n\"For forecasting models, null values are imputed from the surrounding data. (There is no option to leave a null value as null.) If you would prefer to control the way null values are imputed, you can impute them explicitly. The best values to use might depend on your data and your business problem.\nMissing rows (for example, no row for a specific date, with a data granularity of daily) are allowed, but Vertex AI does not impute values for the missing data. Because missing rows can decrease model quality, you should avoid missing rows where possible. For example, if a row is missing because sales quantity for that day was zero, add a row for that day and explicitly set sales data to 0.\"\nhttps://cloud.google.com/vertex-ai/docs/datasets/data-types-tabular#null-values"
      },
      {
        "date": "2023-03-07T05:39:00.000Z",
        "voteCount": 2,
        "content": "C. Replace the missing values with a placeholder category indicating a missing value.\n\nThis approach is often referred to as \"imputing\" missing values, and it is a common technique for dealing with missing data in categorical features. By using a placeholder category, you explicitly indicate that the value is missing, rather than assuming that the missing value is a particular category. This can help to minimize bias in downstream analyses, as it does not introduce any assumptions about the missing data that could bias your results."
      },
      {
        "date": "2023-02-24T03:07:00.000Z",
        "voteCount": 3,
        "content": "When handling missing values in a categorical feature, replacing the missing values with a placeholder category indicating a missing value, as described in option C, is the most appropriate solution in order to minimize bias that could result from the missing values. This approach allows the algorithm to treat missing values as a separate category, avoiding the risk of any assumptions being made about the missing values.\nOption A, removing the rows with missing values and upsampling the dataset by 5%, can lead to a loss of valuable data and can also introduce bias into the data. This approach can lead to overrepresentation of certain classes and underrepresentation of others.\n\nOption B, replacing the missing values with the feature's mean, is not appropriate for categorical features as there is no meaningful average value for categorical features.\n\nOption D, moving the rows with missing values to the validation dataset, is not a good solution. This approach may introduce bias into the validation dataset and can lead to overfitting."
      },
      {
        "date": "2023-02-23T12:48:00.000Z",
        "voteCount": 1,
        "content": "I am not really understanding the concept of C. What information should the model learn from that missing value category?"
      },
      {
        "date": "2023-01-25T01:17:00.000Z",
        "voteCount": 2,
        "content": "If you want to minimize the bias, why do not you use mean?"
      },
      {
        "date": "2023-02-05T00:46:00.000Z",
        "voteCount": 2,
        "content": "It is categorical field, you can replace with median or mode not with mean"
      },
      {
        "date": "2023-01-03T03:35:00.000Z",
        "voteCount": 1,
        "content": "C, for me."
      },
      {
        "date": "2022-12-23T01:43:00.000Z",
        "voteCount": 2,
        "content": "C looks correct. We should replace the values with the a placeholder"
      },
      {
        "date": "2022-12-21T16:20:00.000Z",
        "voteCount": 1,
        "content": "C (not sure)"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 128,
    "url": "https://www.examtopics.com/discussions/google/view/92320-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You are an ML engineer on an agricultural research team working on a crop disease detection tool to detect leaf rust spots in images of crops to determine the presence of a disease. These spots, which can vary in shape and size, are correlated to the severity of the disease. You want to develop a solution that predicts the presence and severity of the disease with high accuracy. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an object detection model that can localize the rust spots.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDevelop an image segmentation ML model to locate the boundaries of the rust spots.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDevelop a template matching algorithm using traditional computer vision libraries.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDevelop an image classification ML model to predict the presence of the disease."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 25,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-28T01:43:00.000Z",
        "voteCount": 8,
        "content": "Not D because Classification can't predict the severity for that we need Segmentation"
      },
      {
        "date": "2024-04-24T20:23:00.000Z",
        "voteCount": 2,
        "content": "Rust Spot Location and Size: Object detection (A) primarily focuses on identifying and bounding the location of objects.expand_more While it can detect the presence of rust spots, it wouldn't capture the variations in size and shape that correlate with disease severity.\nDetailed Boundaries: Image classification (D) would only predict the presence or absence of the disease based on the entire image. It wouldn't provide details about the location or extent of the rust spots.\nTemplate matching (C) with traditional libraries might be computationally expensive and struggle with the variability in spot shapes and sizes."
      },
      {
        "date": "2023-05-18T22:06:00.000Z",
        "voteCount": 2,
        "content": "only B gets the severity here"
      },
      {
        "date": "2023-05-09T05:42:00.000Z",
        "voteCount": 2,
        "content": "Object Detection [Option A] and Image Segmentation [Option B]:\nhttps://www.oreilly.com/library/view/practical-machine-learning/9781098102357/ch04.html\nImage Recognition [Option D]:\nhttps://www.oreilly.com/library/view/practical-machine-learning/9781098102357/ch03.html#image_vision"
      },
      {
        "date": "2023-03-07T05:38:00.000Z",
        "voteCount": 2,
        "content": "B. Develop an image segmentation ML model to locate the boundaries of the rust spots.\n\nAn image segmentation model is well-suited for this task because it can identify the exact location and shape of the rust spots in the image, which is critical for determining the severity of the disease. Once the rust spots have been identified, other algorithms can be used to analyze the data and predict the severity of the disease. Object detection models are another option, but they may not be as accurate as image segmentation models when it comes to identifying the exact boundaries of the rust spots. Template matching algorithms using traditional computer vision libraries are generally not as accurate as ML models when it comes to image analysis."
      },
      {
        "date": "2023-09-10T21:32:00.000Z",
        "voteCount": 2,
        "content": "Your reasoning is correct, but the headline says: \"crop disease detection tool to detect leaf rust spots in images of crops to determine the presence of a disease\". So I understand that from the output after processing the images is Disease/No Disease. Which I guess could be achieved with Classification."
      },
      {
        "date": "2024-02-29T05:04:00.000Z",
        "voteCount": 1,
        "content": "\"You want to develop a solution that predicts the presence and severity of the disease with high accuracy.\"\nTherefore, detection only is not the best solution as it is not reliable as to the size of the detected object (rust spot)"
      },
      {
        "date": "2022-12-30T07:29:00.000Z",
        "voteCount": 3,
        "content": "the shape of the spot is quite important for the severity of the disease, and image segmentation could help us to determine it in a more granular manner. And it is often used in the healthcare industry, for getting the shapes of all the cancerous cells"
      },
      {
        "date": "2022-12-29T19:17:00.000Z",
        "voteCount": 2,
        "content": "Answer B"
      },
      {
        "date": "2022-12-28T09:21:00.000Z",
        "voteCount": 4,
        "content": "To determine severity of the disease, boundary of rust spots should be determined - for size/ shape etc."
      },
      {
        "date": "2022-12-21T16:25:00.000Z",
        "voteCount": 3,
        "content": "D should works"
      },
      {
        "date": "2022-12-21T03:15:00.000Z",
        "voteCount": 1,
        "content": "I think D"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 129,
    "url": "https://www.examtopics.com/discussions/google/view/92404-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You have been asked to productionize a proof-of-concept ML model built using Keras. The model was trained in a Jupyter notebook on a data scientist\u2019s local machine. The notebook contains a cell that performs data validation and a cell that performs model analysis. You need to orchestrate the steps contained in the notebook and automate the execution of these steps for weekly retraining. You expect much more training data in the future. You want your solution to take advantage of managed services while minimizing cost. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMove the Jupyter notebook to a Notebooks instance on the largest N2 machine type, and schedule the execution of the steps in the Notebooks instance using Cloud Scheduler.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite the code as a TensorFlow Extended (TFX) pipeline orchestrated with Vertex AI Pipelines. Use standard TFX components for data validation and model analysis, and use Vertex AI Pipelines for model retraining.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRewrite the steps in the Jupyter notebook as an Apache Spark job, and schedule the execution of the job on ephemeral Dataproc clusters using Cloud Scheduler.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExtract the steps contained in the Jupyter notebook as Python scripts, wrap each script in an Apache Airflow BashOperator, and run the resulting directed acyclic graph (DAG) in Cloud Composer."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 12,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-05-09T06:13:00.000Z",
        "voteCount": 2,
        "content": "Went with B"
      },
      {
        "date": "2023-04-17T15:31:00.000Z",
        "voteCount": 3,
        "content": "I believe it B. Write the code as a TensorFlow Extended (TFX) pipeline orchestrated with Vertex AI Pipelines. Use standard TFX components for data validation and model analysis, and use Vertex AI Pipelines for model retraining. Because :\n \n- Solution A is not scalable and will be expensive to run. It also does not take advantage of managed services.\n\n Solution C is more scalable than option A, but it is still not as scalable as using TFX and Vertex AI Pipelines. It also does not take advantage of managed services.\n\n- Solution D is the most flexible, but it is also the most complex. It requires more knowledge of Apache Airflow and is more difficult to manage.\n\nOverall, the best solution to productionize the proof-of-concept ML model is to use TFX and Vertex AI Pipelines. This solution is scalable, reliable, and easy to manage. It also takes advantage of managed services, which can help to reduce costs."
      },
      {
        "date": "2023-03-07T05:36:00.000Z",
        "voteCount": 3,
        "content": "B. Write the code as a TensorFlow Extended (TFX) pipeline orchestrated with Vertex AI Pipelines. Use standard TFX components for data validation and model analysis, and use Vertex AI Pipelines for model retraining.\n\nThe reason for this choice is that TFX and Vertex AI Pipelines provide a scalable and cost-effective solution for productionizing machine learning models. TFX is an end-to-end ML platform for building scalable and repeatable ML workflows, while Vertex AI Pipelines provides a fully managed service for orchestrating ML workflows at scale. By using TFX and Vertex AI Pipelines, you can automate the execution of the steps contained in the Jupyter notebook, and schedule the pipeline for weekly retraining. This approach also takes advantage of managed services, which helps to minimize cost."
      },
      {
        "date": "2023-01-03T03:33:00.000Z",
        "voteCount": 2,
        "content": "All the others look really wrong, so B."
      },
      {
        "date": "2022-12-21T16:31:00.000Z",
        "voteCount": 2,
        "content": "B (not sure)"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 130,
    "url": "https://www.examtopics.com/discussions/google/view/92333-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You are working on a system log anomaly detection model for a cybersecurity organization. You have developed the model using TensorFlow, and you plan to use it for real-time prediction. You need to create a Dataflow pipeline to ingest data via Pub/Sub and write the results to BigQuery. You want to minimize the serving latency as much as possible. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tContainerize the model prediction logic in Cloud Run, which is invoked by Dataflow.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLoad the model directly into the Dataflow job as a dependency, and use it for prediction.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the model to a Vertex AI endpoint, and invoke this endpoint in the Dataflow job.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the model in a TFServing container on Google Kubernetes Engine, and invoke it in the Dataflow job."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 19,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 9,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-02-01T13:53:00.000Z",
        "voteCount": 5,
        "content": "C. According Google:\n\"Instead of deploying the model to an endpoint, you can use the RunInference API to serve machine learning models in your Apache Beam pipeline. This approach has several advantages, including flexibility and portability. However, deploying the model in Vertex AI offers many additional benefits, such as the platform's built-in tools for model monitoring, TensorBoard, and model registry governance.\nVertex AI also provides the ability to use Optimized TensorFlow runtime in your endpoints. To do this, simply specify the TensorFlow runtime container when you deploy your model.\"\n\nhttps://cloud.google.com/blog/products/ai-machine-learning/streaming-prediction-with-dataflow-and-vertex"
      },
      {
        "date": "2024-05-05T01:01:00.000Z",
        "voteCount": 2,
        "content": "It's a toss up between B and C.\n\nI chose B because using vertex AI as an endpoint introduces network latency which naturally does not meet the criteria of \"minimizing latency\".\n\nHowever, choosing option B also implies that I have more overhead by directly running the model in the dataflow pipeline. Since the question didn't mention any limitations on resources, I assumed that the resources can be scaled accordingly to minimize latency. I might be overthinking on this option though seeing how most of Google questions have a strong preference on their \"recommended platforms\" like vertex AI. Most of the questions and the community answers seem to tend towards anything that mentions \"vertex ai\"."
      },
      {
        "date": "2024-02-01T13:52:00.000Z",
        "voteCount": 2,
        "content": "According Google:\n\"Instead of deploying the model to an endpoint, you can use the RunInference API to serve machine learning models in your Apache Beam pipeline. This approach has several advantages, including flexibility and portability. However, deploying the model in Vertex AI offers many additional benefits, such as the platform's built-in tools for model monitoring, TensorBoard, and model registry governance.\nVertex AI also provides the ability to use Optimized TensorFlow runtime in your endpoints. To do this, simply specify the TensorFlow runtime container when you deploy your model.\"\n\nhttps://cloud.google.com/blog/products/ai-machine-learning/streaming-prediction-with-dataflow-and-vertex"
      },
      {
        "date": "2023-07-08T10:45:00.000Z",
        "voteCount": 3,
        "content": "In this case, the best way to minimize the serving latency of the system log anomaly detection model is to deploy it to a Vertex AI endpoint. This will allow Dataflow to invoke the model directly, without having to load it into the job as a dependency. This will significantly reduce the serving latency, as Dataflow will not have to wait for the model to load before it can make a prediction.\n\nOption B would involve loading the model directly into the Dataflow job as a dependency. This would also add an additional layer of latency, as Dataflow would have to load the model into memory before it could make a prediction."
      },
      {
        "date": "2023-06-05T07:25:00.000Z",
        "voteCount": 1,
        "content": "C. Deploy the model to a Vertex AI endpoint, and invoke this endpoint in the Dataflow job\nhttps://cloud.google.com/architecture/minimizing-predictive-serving-latency-in-machine-learning"
      },
      {
        "date": "2023-05-18T21:38:00.000Z",
        "voteCount": 1,
        "content": "C\nI eliminate B because Dataflow is a batch-prediction solution, not real-time"
      },
      {
        "date": "2024-04-07T01:48:00.000Z",
        "voteCount": 1,
        "content": "Dataflow has a streaming pipeline solution as well."
      },
      {
        "date": "2023-05-09T07:31:00.000Z",
        "voteCount": 1,
        "content": "Went with C"
      },
      {
        "date": "2023-04-18T13:44:00.000Z",
        "voteCount": 1,
        "content": "I believe it is C when deploying the model to a Vertex AI endpoint it provides a dedicated prediction service optimised for real-time inference. Vertex AI endpoints are designed for high performance and low latency, making them ideal for real-time prediction use cases. Dataflow can easily invoke the Vertex AI endpoint to perform predictions, minimising serving latency."
      },
      {
        "date": "2023-03-28T22:08:00.000Z",
        "voteCount": 4,
        "content": "B. Load the model directly into the Dataflow job as a dependency, and use it for prediction.\n\nBy loading the model directly into the Dataflow job as a dependency, you minimize the serving latency since the model is available within the pipeline itself. This way, you avoid additional network latency that would be introduced by invoking external services, such as Cloud Run, Vertex AI endpoints, or TFServing containers."
      },
      {
        "date": "2023-04-18T13:42:00.000Z",
        "voteCount": 1,
        "content": "Actually in retrospect C is the correct answer, not B because loading the model directly into the Dataflow job as a dependency may cause unnecessary overhead, as Dataflow jobs are primarily designed for batch processing and may not be optimized for real-time prediction. Additionally, loading the model as a dependency may increase the size of the Dataflow job and introduce complexity in managing dependencies."
      },
      {
        "date": "2023-03-25T09:09:00.000Z",
        "voteCount": 1,
        "content": "By loading the model directly into the Dataflow job as a dependency, you can perform predictions within the same job. This approach helps minimize serving latency since there is no need to make external calls to another service or endpoint. Instead, the model is directly available within the Dataflow pipeline, allowing for efficient and fast processing of the streaming data."
      },
      {
        "date": "2023-03-07T05:35:00.000Z",
        "voteCount": 2,
        "content": "C. Deploy the model to a Vertex AI endpoint, and invoke this endpoint in the Dataflow job.\n\nThe reason for this choice is that deploying the model to a Vertex AI endpoint and invoking it in the Dataflow job is the most efficient and scalable option for real-time prediction. Vertex AI provides a fully managed, serverless platform for deploying and serving machine learning models. It allows for high availability and low-latency serving of models, and can handle a large volume of requests in parallel. Invoking the model via an endpoint in the Dataflow job minimizes the latency for model prediction, as it avoids any unnecessary data transfers or containerization"
      },
      {
        "date": "2023-04-16T23:09:00.000Z",
        "voteCount": 1,
        "content": "Using private endpoints to serve online predictions with Vertex AI provides a low-latency, secure connection to the Vertex AI online prediction service. This guide shows how to configure private endpoints on Vertex AI by using VPC Network Peering to peer your network with the Vertex AI online prediction service\n\nhttps://cloud.google.com/vertex-ai/docs/predictions/using-private-endpoints \n\nAnswer C"
      },
      {
        "date": "2023-02-24T03:20:00.000Z",
        "voteCount": 1,
        "content": "Option B, loading the model directly into the Dataflow job as a dependency and using it for prediction, may not provide the optimal performance because Dataflow may not be optimized for low-latency predictions."
      },
      {
        "date": "2023-02-12T22:24:00.000Z",
        "voteCount": 2,
        "content": "These are anwser\nhttps://cloud.google.com/dataflow/docs/notebooks/run_inference_tensorflow\n\nhttps://beam.apache.org/documentation/sdks/python-machine-learning/\n\nhttps://beam.apache.org/documentation/transforms/python/elementwise/runinference/"
      },
      {
        "date": "2023-01-23T02:13:00.000Z",
        "voteCount": 1,
        "content": "C  OR B.\nit is straightforward that it should be  C  as the follwing link https://cloud.google.com/architecture/detecting-anomalies-in-financial-transactions\nbut B it seems a newer way. it keeps questionable."
      },
      {
        "date": "2023-03-24T21:51:00.000Z",
        "voteCount": 1,
        "content": "Reading through this link, look like dataflow itself is doing prediction directly .. so B"
      },
      {
        "date": "2023-01-03T03:09:00.000Z",
        "voteCount": 1,
        "content": "C, for me."
      },
      {
        "date": "2022-12-21T17:01:00.000Z",
        "voteCount": 4,
        "content": "C\n- https://cloud.google.com/architecture/detecting-anomalies-in-financial-transactions\n- https://cloud.google.com/blog/topics/financial-services/detect-anomalies-in-real-time-forex-data-with-ml"
      },
      {
        "date": "2022-12-21T05:15:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/blog/products/data-analytics/influsing-ml-models-into-production-pipelines-with-dataflow"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 131,
    "url": "https://www.examtopics.com/discussions/google/view/91489-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You are an ML engineer at a mobile gaming company. A data scientist on your team recently trained a TensorFlow model, and you are responsible for deploying this model into a mobile application. You discover that the inference latency of the current model doesn\u2019t meet production requirements. You need to reduce the inference time by 50%, and you are willing to accept a small decrease in model accuracy in order to reach the latency requirement. Without training a new model, which model optimization technique for reducing latency should you try first?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWeight pruning",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDynamic range quantization\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModel distillation",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDimensionality reduction"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 14,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-03-07T05:35:00.000Z",
        "voteCount": 5,
        "content": "B. Dynamic range quantization\n\nThe reason for this choice is that dynamic range quantization is a model optimization technique that can significantly reduce model size and inference time while maintaining reasonable model accuracy. Dynamic range quantization uses fewer bits to represent the weights of the model, reducing the memory required to store the model and the time required for inference."
      },
      {
        "date": "2023-05-18T21:47:00.000Z",
        "voteCount": 3,
        "content": "B.\nA, C, D --&gt; have to retrain"
      },
      {
        "date": "2023-05-09T08:20:00.000Z",
        "voteCount": 1,
        "content": "Plus: \u201cMagnitude-based weight pruning gradually zeroes out model weights during the training process to achieve model sparsity. Sparse models are easier to compress, and we can skip the zeroes during inference for latency improvements.\u201d https://www.tensorflow.org/model_optimization/guide/pruning, where \u201cduring the training process\u201d disqualifies Option A."
      },
      {
        "date": "2023-05-09T08:20:00.000Z",
        "voteCount": 1,
        "content": "https://en.wikipedia.org/wiki/Knowledge_distillation is the process of transferring knowledge from a large model to a smaller one. As smaller models are less expensive to evaluate, they can be deployed on less powerful hardware (such as a mobile device). https://en.wikipedia.org/wiki/Dimensionality_reduction is the transformation of data from a high-dimensional space into a low-dimensional space so that the low-dimensional representation retains some meaningful properties of the original data. \n\u201cWithout training a new model\u201d disqualifies both Option C and D."
      },
      {
        "date": "2023-01-03T02:56:00.000Z",
        "voteCount": 3,
        "content": "'Without training a new model' --&gt; B"
      },
      {
        "date": "2022-12-21T17:12:00.000Z",
        "voteCount": 4,
        "content": "B\n- https://www.tensorflow.org/lite/performance/post_training_quantization#dynamic_range_quantization"
      },
      {
        "date": "2022-12-21T17:12:00.000Z",
        "voteCount": 1,
        "content": "B\n-https://www.tensorflow.org/lite/performance/post_training_quantization#dynamic_range_quantization"
      },
      {
        "date": "2022-12-13T11:53:00.000Z",
        "voteCount": 3,
        "content": "The requirement is \"Without training a new model\" hence dynamic range quantization.\nhttps://www.tensorflow.org/lite/performance/post_training_quant"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 132,
    "url": "https://www.examtopics.com/discussions/google/view/91488-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You work on a data science team at a bank and are creating an ML model to predict loan default risk. You have collected and cleaned hundreds of millions of records worth of training data in a BigQuery table, and you now want to develop and compare multiple models on this data using TensorFlow and Vertex AI. You want to minimize any bottlenecks during the data ingestion state while considering scalability. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the BigQuery client library to load data into a dataframe, and use tf.data.Dataset.from_tensor_slices() to read it.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExport data to CSV files in Cloud Storage, and use tf.data.TextLineDataset() to read them.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConvert the data into TFRecords, and use tf.data.TFRecordDataset() to read them.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse TensorFlow I/O\u2019s BigQuery Reader to directly read the data.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 19,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-12-22T01:56:00.000Z",
        "voteCount": 6,
        "content": "D\n- https://www.tensorflow.org/io/api_docs/python/tfio/bigquery"
      },
      {
        "date": "2022-12-13T11:43:00.000Z",
        "voteCount": 5,
        "content": "Vote on D. This will allow to directly access the data from BigQuery without having to first load it into a dataframe or export it to files in Cloud Storage."
      },
      {
        "date": "2024-04-24T20:49:00.000Z",
        "voteCount": 2,
        "content": "Direct Data Access: TensorFlow I/O's BigQuery Reader allows you to directly access data from BigQuery tables within your TensorFlow script.expand_more This eliminates the need for intermediate data movement (e.g., to CSV files) and data manipulation steps (e.g., loading into DataFrames).exclamation\nScalability: BigQuery Reader is designed to handle large datasets efficiently. It leverages BigQuery's parallel processing capabilities to stream data into your TensorFlow training pipeline, minimizing processing bottlenecks and enabling scalability as your data volume grows."
      },
      {
        "date": "2024-04-24T20:49:00.000Z",
        "voteCount": 1,
        "content": ". BigQuery Client Library and Dataframe: While the BigQuery client library can access BigQuery data, loading it into a DataFrame and using tf.data.Dataset.from_tensor_slices() is inefficient for massive datasets due to memory limitations and potential processing bottlenecks.\nB. CSV Files and TextLineDataset: Exporting data to CSV and using tf.data.TextLineDataset() introduces unnecessary data movement and processing overhead, hindering both efficiency and scalability.\nC. TFRecords: TFRecords can be efficient for certain use cases, but converting hundreds of millions of records into TFRecords can be time-consuming and resource-intensive.\n\npen_spark\nexclamation Additionally, reading them might require parsing logic within your TensorFlow script."
      },
      {
        "date": "2024-02-01T14:04:00.000Z",
        "voteCount": 1,
        "content": "D\nhttps://cloud.google.com/blog/products/ai-machine-learning/tensorflow-enterprise-makes-accessing-data-on-google-cloud-faster-and-easier"
      },
      {
        "date": "2023-05-18T21:51:00.000Z",
        "voteCount": 2,
        "content": "D\nBigQuery is more compact way to store the data than TFRecords"
      },
      {
        "date": "2023-05-09T08:38:00.000Z",
        "voteCount": 1,
        "content": "Went with D"
      },
      {
        "date": "2023-03-07T05:33:00.000Z",
        "voteCount": 2,
        "content": "D. Use TensorFlow I/O\u2019s BigQuery Reader to directly read the data.\n\nThe reason for this choice is that using TensorFlow I/O\u2019s BigQuery Reader is the most efficient and scalable option for reading data directly from BigQuery into TensorFlow models. It allows for distributed processing and avoids unnecessary data duplication, which can cause bottlenecks and consume large amounts of storage. Additionally, the BigQuery Reader is optimized for reading data in parallel from BigQuery tables and streaming them directly into TensorFlow. This eliminates the need for any intermediate file formats or data copies, reducing latency and increasing performance."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 133,
    "url": "https://www.examtopics.com/discussions/google/view/91467-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You have recently created a proof-of-concept (POC) deep learning model. You are satisfied with the overall architecture, but you need to determine the value for a couple of hyperparameters. You want to perform hyperparameter tuning on Vertex AI to determine both the appropriate embedding dimension for a categorical feature used by your model and the optimal learning rate. You configure the following settings:<br>\u2022\tFor the embedding dimension, you set the type to INTEGER with a minValue of 16 and maxValue of 64.<br>\u2022\tFor the learning rate, you set the type to DOUBLE with a minValue of 10e-05 and maxValue of 10e-02.<br><br>You are using the default Bayesian optimization tuning algorithm, and you want to maximize model accuracy. Training time is not a concern. How should you set the hyperparameter scaling for each hyperparameter and the maxParallelTrials?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse UNIT_LINEAR_SCALE for the embedding dimension, UNIT_LOG_SCALE for the learning rate, and a large number of parallel trials.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse UNIT_LINEAR_SCALE for the embedding dimension, UNIT_LOG_SCALE for the learning rate, and a small number of parallel trials.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse UNIT_LOG_SCALE for the embedding dimension, UNIT_LINEAR_SCALE for the learning rate, and a large number of parallel trials.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse UNIT_LOG_SCALE for the embedding dimension, UNIT_LINEAR_SCALE for the learning rate, and a small number of parallel trials."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 30,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-13T20:28:00.000Z",
        "voteCount": 11,
        "content": "Vote B"
      },
      {
        "date": "2024-02-01T14:22:00.000Z",
        "voteCount": 10,
        "content": "B: Here's why:\n**Embedding Dimension:** UNIT_LINEAR_SCALE is appropriate for integer hyperparameters with a continuous range like the embedding dimension. It linearly scales the search space from minValue to maxValue.\n\n**Learning Rate:**  UNIT_LOG_SCALE is generally recommended for hyperparameters with values spanning multiple orders of magnitude like the learning rate (10e-05 - 10e-02). This ensures equal sampling probability across different log-scaled ranges.\n\n**Parallel Trials:** as the documentation specifies, parallelization speeds up.&nbsp;However, this speedup comes at the cost of potentially sacrificing the quality of the results. Since training time is not a factor in this case, the benefit of speeding things up with many parallel trials is less valuable.\n\nhttps://cloud.google.com/vertex-ai/docs/training/using-hyperparameter-tuning#parallel-trials"
      },
      {
        "date": "2024-02-29T05:31:00.000Z",
        "voteCount": 1,
        "content": "this is the perfect answer and explanation."
      },
      {
        "date": "2024-04-15T10:29:00.000Z",
        "voteCount": 1,
        "content": "Training time is not a concern -&gt; B (the benefit of speeding things up with many parallel trials is less valuable)"
      },
      {
        "date": "2024-04-14T22:38:00.000Z",
        "voteCount": 1,
        "content": "Vote B"
      },
      {
        "date": "2023-11-12T10:37:00.000Z",
        "voteCount": 2,
        "content": "because training time is not a concern and you want to maximize accuracy, using a large number of maxParallelTrials (option A) allows thoroughly searching the hyperparameter space."
      },
      {
        "date": "2023-06-06T05:19:00.000Z",
        "voteCount": 1,
        "content": "B. Use UNIT_LINEAR_SCALE for the embedding dimension, UNIT_LOG_SCALE for the learning rate, and a small number of parallel trials.\nhttps://cloud.google.com/vertex-ai/docs/training/using-hyperparameter-tuning\nFirst we should choos an option with small trials: \n\"Before starting a job with a large number of trials, you may want to start with a small number of trials to gauge the effect your chosen hyperparameters have on your model's accuracy.\"\nNow, the embeddings should be linear https://cloud.google.com/blog/products/gcp/hyperparameter-tuning-on-google-cloud-platform-is-now-faster-and-smarter"
      },
      {
        "date": "2023-05-09T09:06:00.000Z",
        "voteCount": 1,
        "content": "Went with B"
      },
      {
        "date": "2023-03-29T04:29:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/vertex-ai/docs/training/using-hyperparameter-tuning#parallel-trials\n\"Running parallel trials has the benefit of reducing the time the training job takes (real time\u2014the total processing time required is not typically changed). However, running in parallel can reduce the effectiveness of the tuning job overall.\"\nSince opt. for accuracy and ignore training time, use above.  \nLinear for learning rate doesn't really make sense, think that one is obvious imo."
      },
      {
        "date": "2023-03-09T11:57:00.000Z",
        "voteCount": 1,
        "content": "Answer is B , even my explanation is on B not C\nOption B is the best choice: Use UNIT_LOG_SCALE for the embedding dimension, UNIT_LINEAR_SCALE for the learning rate, and a large number of parallel trials.\n\nThe reason for this choice is as follows:\n\nFor the embedding dimension, it is better to use a logarithmic scale because the effect of increasing the dimensionality is likely to diminish as the dimension grows larger. Therefore, the logarithmic scale will allow the tuning algorithm to explore a wider range of values with less bias towards higher values"
      },
      {
        "date": "2023-03-07T05:32:00.000Z",
        "voteCount": 1,
        "content": "Option C is the best choice: Use UNIT_LOG_SCALE for the embedding dimension, UNIT_LINEAR_SCALE for the learning rate, and a large number of parallel trials.\n\nThe reason for this choice is as follows:\n\nFor the embedding dimension, it is better to use a logarithmic scale because the effect of increasing the dimensionality is likely to diminish as the dimension grows larger. Therefore, the logarithmic scale will allow the tuning algorithm to explore a wider range of values with less bias towards higher values"
      },
      {
        "date": "2023-03-09T11:56:00.000Z",
        "voteCount": 1,
        "content": "Meant to choose B ahhhh"
      },
      {
        "date": "2023-01-23T04:11:00.000Z",
        "voteCount": 2,
        "content": "Learning Rage is subtle and take time  so, it use Log Scale"
      },
      {
        "date": "2023-01-07T03:40:00.000Z",
        "voteCount": 1,
        "content": "It's B!"
      },
      {
        "date": "2022-12-22T02:06:00.000Z",
        "voteCount": 1,
        "content": "A\n- https://cloud.google.com/ai-platform/training/docs/reference/rest/v1/projects.jobs#HyperparameterSpec\n- https://cloud.google.com/vertex-ai/docs/reference/rest/v1beta1/StudySpec"
      },
      {
        "date": "2022-12-27T06:25:00.000Z",
        "voteCount": 1,
        "content": "Sorry, B is the answer"
      },
      {
        "date": "2022-12-13T10:18:00.000Z",
        "voteCount": 2,
        "content": "Vote D, this can help the tuning algorithm explore a wider range of values for the learning rate, while also focusing on a smaller range of values for the embedding dimension."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 134,
    "url": "https://www.examtopics.com/discussions/google/view/91468-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You are the Director of Data Science at a large company, and your Data Science team has recently begun using the Kubeflow Pipelines SDK to orchestrate their training pipelines. Your team is struggling to integrate their custom Python code into the Kubeflow Pipelines SDK. How should you instruct them to proceed in order to quickly integrate their code with the Kubeflow Pipelines SDK?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the func_to_container_op function to create custom components from the Python code.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the predefined components available in the Kubeflow Pipelines SDK to access Dataproc, and run the custom code there.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPackage the custom Python code into Docker containers, and use the load_component_from_file function to import the containers into the pipeline.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the custom Python code to Cloud Functions, and use Kubeflow Pipelines to trigger the Cloud Function."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-05-09T09:16:00.000Z",
        "voteCount": 1,
        "content": "Went with A"
      },
      {
        "date": "2023-04-18T13:57:00.000Z",
        "voteCount": 1,
        "content": "The answer is A. because the Kubeflow Pipelines SDK provides a convenient way to create custom components from existing Python code using the func_to_container_op function. This allows data science team to encapsulate the custom code as containerised components that can be easily integrated into the kubeflow pipeline. This approach allows for seamless integration of custom Python code into the Kubeflow Pipelines SDK without requiring additional dependencies or infrastructure setup."
      },
      {
        "date": "2023-03-07T05:30:00.000Z",
        "voteCount": 3,
        "content": "A. Use the func_to_container_op function to create custom components from the Python code.\n\nThe func_to_container_op function in the Kubeflow Pipelines SDK is specifically designed to convert Python functions into containerized components that can be executed in a Kubernetes cluster. By using this function, the Data Science team can easily integrate their custom Python code into the Kubeflow Pipelines SDK without having to learn the details of containerization or Kubernetes."
      },
      {
        "date": "2022-12-22T02:10:00.000Z",
        "voteCount": 4,
        "content": "A\n-https://kubeflow-pipelines.readthedocs.io/en/stable/source/kfp.components.html?highlight=func_to_container_op%20#kfp.components.func_to_container_op"
      },
      {
        "date": "2022-12-13T10:21:00.000Z",
        "voteCount": 2,
        "content": "Use the func_to_container_op function to create custom components from their code. This function allows you to define a Python function that can be used as a pipeline component, and it automatically creates a Docker container with the necessary dependencies"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 135,
    "url": "https://www.examtopics.com/discussions/google/view/91471-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You work for the AI team of an automobile company, and you are developing a visual defect detection model using TensorFlow and Keras. To improve your model performance, you want to incorporate some image augmentation functions such as translation, cropping, and contrast tweaking. You randomly apply these functions to each training batch. You want to optimize your data processing pipeline for run time and compute resources utilization. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEmbed the augmentation functions dynamically in the tf.Data pipeline.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEmbed the augmentation functions dynamically as part of Keras generators.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Dataflow to create all possible augmentations, and store them as TFRecords.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Dataflow to create the augmentations dynamically per training run, and stage them as TFRecords."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 18,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-02-01T15:18:00.000Z",
        "voteCount": 3,
        "content": "Option A: By embedding augmentation in the tf.data pipeline, data augmentation is applied on-the-fly during training, reducing the need to store pre-augmented data. Option B could be a choice, but since Keras generators are built on top of tf.data, they are less flexible and have a lower level of optimization compared to tf.data."
      },
      {
        "date": "2023-11-12T11:31:00.000Z",
        "voteCount": 2,
        "content": "A is best because, \n1. It allows you to apply the augmentations on-the-fly during training, which eliminates the need for pre-processing and storing a large number of augmented images. This saves both storage space and compute resources.\n2. The tf.Data pipeline is highly optimized for efficient data loading and processing, ensuring that your model training process is not bottlenecked by data preprocessing.\n3. By applying augmentations randomly to each training batch, you increase the diversity of your training data, which can help your model generalize better to unseen data.\n\nKeras generators can be used for data augmentation, but tf.Data pipelines are generally more efficient and flexible for creating complex data processing pipelines."
      },
      {
        "date": "2023-11-12T09:33:00.000Z",
        "voteCount": 1,
        "content": "B (but also A)\n\nB is a common and efficient approach for applying data augmentation during training. This allows you to apply data augmentation on-the-fly without the need to pre-generate or store augmented images separately, which saves storage space and reduces the preprocessing time. Keras provides various tools and functions for data augmentation, and you can easily incorporate them into your training data pipeline.\n\nA can also be a good choice, especially if you are using TensorFlow's tf.data API for data loading and preprocessing. It can provide similar benefits by applying augmentations on-the-fly, but it may require more custom code to implement compared to Keras data generators."
      },
      {
        "date": "2024-02-01T15:19:00.000Z",
        "voteCount": 1,
        "content": "Yes, but I think the questions says: \"You want to optimize your data processing pipeline for run time and compute resources utilization\". keras is not optmizated as tf.Data"
      },
      {
        "date": "2023-08-09T06:45:00.000Z",
        "voteCount": 1,
        "content": "by abylead: B) Keras generators embedded augmentation functions offers at least Translation, Crop, and Contrast preprocessing. You can either permanently integrate the functions or you randomly use a dataset with non CPU blocking async training batches &amp; optimized GPU processing overlapping. By applying Keras embedded augmentation functions, the tf.data pipeline can still be performance optimized.\nWith tf.image pipelines you lack pipeline performance optimization &amp; the deprecated translation function. In addition, the complex application hinders random operation flexibilty."
      },
      {
        "date": "2023-07-19T07:52:00.000Z",
        "voteCount": 1,
        "content": "B - TensorFlow's Keras API provides built-in support for data augmentation using various image preprocessing layers, such as RandomTranslation, RandomCrop, and RandomContrast, among others. You can create custom image augmentation functions and include them as part of your Keras generators, tailoring them to your specific use case and needs.\n\nIn summary, Option B, embedding the augmentation functions dynamically as part of Keras generators, offers efficient on-the-fly data augmentation, reduced storage overhead, optimized resource utilization, and greater flexibility, making it the best choice for thisscenario."
      },
      {
        "date": "2023-07-27T11:26:00.000Z",
        "voteCount": 2,
        "content": "lthough Keras generators can be used for data augmentation, using the tf.data pipeline provides better performance and efficiency. The tf.data API is more flexible and better integrated with TensorFlow, allowing for more optimizations.especially if you have a large number of images to process."
      },
      {
        "date": "2023-05-10T09:48:00.000Z",
        "voteCount": 1,
        "content": "Went with A"
      },
      {
        "date": "2023-04-09T04:33:00.000Z",
        "voteCount": 2,
        "content": "https://www.tensorflow.org/tutorials/load_data/images?hl=ja#tfdata_%E3%82%92%E4%BD%BF%E7%94%A8%E3%81%97%E3%81%A6%E3%82%88%E3%82%8A%E7%B2%BE%E5%AF%86%E3%81%AB%E5%88%B6%E5%BE%A1%E3%81%99%E3%82%8B"
      },
      {
        "date": "2023-04-09T04:35:00.000Z",
        "voteCount": 2,
        "content": "https://www.tensorflow.org/tutorials/load_data/images#using_tfdata_for_finer_control"
      },
      {
        "date": "2023-03-24T22:52:00.000Z",
        "voteCount": 1,
        "content": "https://towardsdatascience.com/time-to-choose-tensorflow-data-over-imagedatagenerator-215e594f2435"
      },
      {
        "date": "2023-03-07T05:28:00.000Z",
        "voteCount": 2,
        "content": "A. Embed the augmentation functions dynamically in the tf.Data pipeline is the best approach to optimize the data processing pipeline for runtime and compute resource utilization.\n\nUsing the tf.data pipeline, you can apply data augmentation functions dynamically to each batch during training. This approach avoids the overhead of creating preprocessed TFRecords or Keras generators, which can consume additional disk space, memory, and CPU. Additionally, using the tf.data pipeline, you can parallelize data preprocessing, input pipeline operations, and model training"
      },
      {
        "date": "2023-02-24T03:49:00.000Z",
        "voteCount": 2,
        "content": "Embedding the augmentation functions dynamically in the tf.Data pipeline allows the data pipeline to apply the augmentations on the fly as the data is being loaded into the model during training. This means that the model can utilize the compute resources effectively by loading and processing the data as needed, rather than pre-generating all possible augmentations ahead of time (as in options C and D), which could be computationally expensive and time-consuming.\n\nOption B is also a viable choice, but it may not be as efficient as option A since the data augmentation functions would be applied during training using Keras generators, which could cause some overhead."
      },
      {
        "date": "2023-01-26T07:57:00.000Z",
        "voteCount": 1,
        "content": "will go for B too\nhttps://www.analyticsvidhya.com/blog/2020/08/image-augmentation-on-the-fly-using-keras-imagedatagenerator/"
      },
      {
        "date": "2023-01-23T05:11:00.000Z",
        "voteCount": 1,
        "content": "Either of  A or B : I am not convinced of what the right answer is.  but it is  on https://www.tensorflow.org/tutorials/images/data_augmentation#apply_augmentation_to_a_dataset certainly"
      },
      {
        "date": "2022-12-22T02:49:00.000Z",
        "voteCount": 1,
        "content": "A (not sure)"
      },
      {
        "date": "2022-12-13T20:34:00.000Z",
        "voteCount": 2,
        "content": "will go for B \nhttps://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly"
      },
      {
        "date": "2022-12-13T10:31:00.000Z",
        "voteCount": 4,
        "content": "incorporating the augmentation functions into the pipeline, you can apply them dynamically to each training batch, without the need to generate all possible augmentations in advance or stage them as TFRecords."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 136,
    "url": "https://www.examtopics.com/discussions/google/view/91469-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You work for an online publisher that delivers news articles to over 50 million readers. You have built an AI model that recommends content for the company\u2019s weekly newsletter. A recommendation is considered successful if the article is opened within two days of the newsletter\u2019s published date and the user remains on the page for at least one minute.<br><br>All the information needed to compute the success metric is available in BigQuery and is updated hourly. The model is trained on eight weeks of data, on average its performance degrades below the acceptable baseline after five weeks, and training time is 12 hours. You want to ensure that the model\u2019s performance is above the acceptable baseline while minimizing cost. How should you monitor the model to determine when retraining is necessary?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Vertex AI Model Monitoring to detect skew of the input features with a sample rate of 100% and a monitoring frequency of two days.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSchedule a cron job in Cloud Tasks to retrain the model every week before the newsletter is created.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSchedule a weekly query in BigQuery to compute the success metric.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSchedule a daily Dataflow job in Cloud Composer to compute the success metric."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 21,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-03-07T05:27:00.000Z",
        "voteCount": 8,
        "content": "Option C is the best answer. Since all the information needed to compute the success metric is available in BigQuery and is updated hourly, scheduling a weekly query in BigQuery to compute the success metric is the simplest and most cost-effective way to monitor the model's performance. By comparing the computed success metric against the acceptable baseline, you can determine when the model's performance has degraded below the threshold, and retrain the model accordingly. This approach avoids the cost of additional monitoring infrastructure and leverages existing data processing capabilities."
      },
      {
        "date": "2024-04-21T23:54:00.000Z",
        "voteCount": 2,
        "content": "Weekly checks are frequent enough to catch performance degradation before the next newsletter (5-week threshold).\nThe success metric can be directly calculated within the query, providing a clear indication for retraining."
      },
      {
        "date": "2024-04-21T23:55:00.000Z",
        "voteCount": 1,
        "content": "A. Vertex AI Model Monitoring for feature skew: This monitors data drift, which can be helpful, but it doesn't directly address the success metric of article opens and dwell time.\nB. Cron job for weekly retraining: Retraining every week, regardless of performance, is excessive and costly, considering the 12-hour training time.\nD. Daily Dataflow job: While daily computation provides more data points, it might be overkill compared to a weekly check. Additionally, Cloud Composer adds complexity for a simple task."
      },
      {
        "date": "2023-05-18T21:25:00.000Z",
        "voteCount": 2,
        "content": "As we have all the data in BigQuery"
      },
      {
        "date": "2023-05-10T09:50:00.000Z",
        "voteCount": 3,
        "content": "Went with C"
      },
      {
        "date": "2023-04-18T14:16:00.000Z",
        "voteCount": 1,
        "content": "Option A because when using Vertex AI Model Monitoring, you can set up automated monitoring of the model's performance by detecting skew of the input features, which can help you identify any changes in the data distribution that may impact the model's performance. Setting the sample rate to 100% ensures that all incoming data is monitored, and a monitoring frequency of two days allows for timely detection of any deviations from the expected data distribution"
      },
      {
        "date": "2023-05-12T14:18:00.000Z",
        "voteCount": 1,
        "content": "I have changed my mind. I will choose C"
      },
      {
        "date": "2023-02-14T07:55:00.000Z",
        "voteCount": 2,
        "content": "This question tweak from this article surely.\nhttps://cloud.google.com/blog/topics/developers-practitioners/continuous-model-evaluation-bigquery-ml-stored-procedures-and-cloud-scheduler"
      },
      {
        "date": "2023-01-23T05:27:00.000Z",
        "voteCount": 2,
        "content": "The anwner is on here https://cloud.google.com/blog/topics/developers-practitioners/continuous-model-evaluation-bigquery-ml-stored-procedures-and-cloud-scheduler"
      },
      {
        "date": "2022-12-22T02:16:00.000Z",
        "voteCount": 1,
        "content": "C (not sure)"
      },
      {
        "date": "2022-12-21T06:12:00.000Z",
        "voteCount": 3,
        "content": "\"All the information needed to compute the success metric is available in BigQuery\" and \"on average its performance degrades below the acceptable baseline after five weeks\"\nso once per week is enough to check models performance. And it's the cheapest solution too."
      },
      {
        "date": "2022-12-13T10:29:00.000Z",
        "voteCount": 1,
        "content": "This can help to ensure that the model\u2019s performance is above the baseline, while minimizing cost by avoiding unnecessary retraining."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 137,
    "url": "https://www.examtopics.com/discussions/google/view/91473-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You deployed an ML model into production a year ago. Every month, you collect all raw requests that were sent to your model prediction service during the previous month. You send a subset of these requests to a human labeling service to evaluate your model\u2019s performance. After a year, you notice that your model's performance sometimes degrades significantly after a month, while other times it takes several months to notice any decrease in performance. The labeling service is costly, but you also need to avoid large performance degradations. You want to determine how often you should retrain your model to maintain a high level of performance while minimizing cost. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTrain an anomaly detection model on the training dataset, and run all incoming requests through this model. If an anomaly is detected, send the most recent serving data to the labeling service.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIdentify temporal patterns in your model\u2019s performance over the previous year. Based on these patterns, create a schedule for sending serving data to the labeling service for the next year.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCompare the cost of the labeling service with the lost revenue due to model performance degradation over the past year. If the lost revenue is greater than the cost of the labeling service, increase the frequency of model retraining; otherwise, decrease the model retraining frequency.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun training-serving skew detection batch jobs every few days to compare the aggregate statistics of the features in the training dataset with recent serving data. If skew is detected, send the most recent serving data to the labeling service.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-03-07T05:24:00.000Z",
        "voteCount": 5,
        "content": "Option D is the best approach to determine how often to retrain the model while minimizing cost. Running training-serving skew detection batch jobs every few days to compare the aggregate statistics of the features in the training dataset with recent serving data is an effective way to detect when the model's performance has degraded. If skew is detected, the most recent serving data should be sent to the labeling service to evaluate the model's performance. This approach is more cost-effective than sending a subset of requests to the labeling service every month because it only sends data when there is a high probability that the model's performance has degraded. By doing this, the model can be retrained at the right time, and the cost of the labeling service can be minimized."
      },
      {
        "date": "2023-05-10T09:50:00.000Z",
        "voteCount": 2,
        "content": "Went with D"
      },
      {
        "date": "2023-01-21T08:50:00.000Z",
        "voteCount": 2,
        "content": "D https://cloud.google.com/blog/topics/developers-practitioners/monitor-models-training-serving-skew-vertex-aiew-vertex-ai&amp;ved=2ahUKEwiRg_aoj9n8AhWb7TgGHcGCDREQFnoECAwQAQ&amp;usg=AOvVaw197NneIJM0ra7fLq2zsOin"
      },
      {
        "date": "2023-01-07T03:04:00.000Z",
        "voteCount": 2,
        "content": "B looks the only option, to me."
      },
      {
        "date": "2022-12-22T02:29:00.000Z",
        "voteCount": 4,
        "content": "D\n- https://cloud.google.com/blog/topics/developers-practitioners/monitor-models-training-serving-skew-vertex-ai\n- https://developers.google.com/machine-learning/guides/rules-of-ml"
      },
      {
        "date": "2022-12-21T02:31:00.000Z",
        "voteCount": 1,
        "content": "I think D"
      },
      {
        "date": "2022-12-13T10:39:00.000Z",
        "voteCount": 2,
        "content": "\"After a year, you notice that your model's performance sometimes degrades significantly after a month, while other times it takes several months to notice any decrease in performance.\" Hence I vote B"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 138,
    "url": "https://www.examtopics.com/discussions/google/view/91474-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You work for a company that manages a ticketing platform for a large chain of cinemas. Customers use a mobile app to search for movies they\u2019re interested in and purchase tickets in the app. Ticket purchase requests are sent to Pub/Sub and are processed with a Dataflow streaming pipeline configured to conduct the following steps:<br>1. Check for availability of the movie tickets at the selected cinema.<br>2. Assign the ticket price and accept payment.<br>3. Reserve the tickets at the selected cinema.<br>4. Send successful purchases to your database.<br><br>Each step in this process has low latency requirements (less than 50 milliseconds). You have developed a logistic regression model with BigQuery ML that predicts whether offering a promo code for free popcorn increases the chance of a ticket purchase, and this prediction should be added to the ticket purchase process. You want to identify the simplest way to deploy this model to production while adding minimal latency. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun batch inference with BigQuery ML every five minutes on each new set of tickets issued.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExport your model in TensorFlow format, and add a tfx_bsl.public.beam.RunInference step to the Dataflow pipeline.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExport your model in TensorFlow format, deploy it on Vertex AI, and query the prediction endpoint from your streaming pipeline.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConvert your model with TensorFlow Lite (TFLite), and add it to the mobile app so that the promo code and the incoming request arrive together in Pub/Sub.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 20,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 6,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-01-05T00:59:00.000Z",
        "voteCount": 9,
        "content": "D as you want to do the prediction before the purchase"
      },
      {
        "date": "2022-12-22T02:37:00.000Z",
        "voteCount": 5,
        "content": "D\n- https://www.tensorflow.org/lite/guide"
      },
      {
        "date": "2024-06-06T03:32:00.000Z",
        "voteCount": 2,
        "content": "D makes no senses -&gt; if the prediction is made on the phone, why send it to the server ? \n\nC is the best choice because it splits the responsability, and use best practices and scalable tools"
      },
      {
        "date": "2024-04-13T03:54:00.000Z",
        "voteCount": 1,
        "content": "B. Export your model in TensorFlow format, and add a tfx_bsl.public.beam.RunInference step to the Dataflow pipeline.\n\nHere's why this approach offers minimal latency:\n\nIn-Pipeline Prediction: The model is integrated directly into the Dataflow pipeline, enabling real-time predictions for each ticket purchase request without external calls.\nDataflow Integration: tfx_bsl.public.beam.RunInference is a Beam utility specifically designed for integrating TensorFlow models into Dataflow pipelines, ensuring efficient execution."
      },
      {
        "date": "2024-03-17T01:21:00.000Z",
        "voteCount": 1,
        "content": "B\nFor D - How can we assume the model does be feasible to convert to Mobile app?"
      },
      {
        "date": "2023-11-11T05:57:00.000Z",
        "voteCount": 2,
        "content": "Question looks ambiguous! However considering some keywords like low latency and more importantly ML usage for maximising the ticket purchase requests using the promo code means that model embedded to the device looks more appropriate, however there are a lot of downsides to it like model management and upgrades but that does not seem to be the consideration here anyway. Just looking at low latency and ML to maximise the ticket sales, I will go with D as thats much simpler to implement"
      },
      {
        "date": "2023-09-11T00:22:00.000Z",
        "voteCount": 2,
        "content": "the whole question does not make too much sense to me.\nfirst of all, it seems that the Dataflow streaming job would \"accept payment\" meaning it communicates with payment gateways and back to the user, which does not sound right to do in Dataflow.\nthe model \"predicts whether offering a promo code for free popcorn increases the chance of a ticket purchase\" is necesssarily executed before processing payment, so  D seems the best. \n\nAwkward ...."
      },
      {
        "date": "2023-05-10T09:51:00.000Z",
        "voteCount": 1,
        "content": "Went with D"
      },
      {
        "date": "2023-04-16T22:26:00.000Z",
        "voteCount": 4,
        "content": "Answer D"
      },
      {
        "date": "2023-03-16T23:23:00.000Z",
        "voteCount": 1,
        "content": "is the simplest way to deploy the logistic regression model to production with minimal latency. Exporting the model in TensorFlow format and adding a tfx_bsl.public.beam.RunInference step to the existing Dataflow pipeline enables the model to be integrated directly into the ticket purchase process."
      },
      {
        "date": "2023-07-27T11:13:00.000Z",
        "voteCount": 1,
        "content": "would also not be suitable because adding a tfx_bsl.public.beam.RunInference step to the Dataflow pipeline would still require the model to be executed within the same pipeline, potentially introducing additional latency and computational overhead."
      },
      {
        "date": "2023-03-07T05:22:00.000Z",
        "voteCount": 1,
        "content": "Option C is the best solution. Since the entire process has low latency requirements, running batch inference every five minutes is not a suitable option. Option B requires a TensorFlow model format, which may not be available since the model is created using BigQuery ML. Option D is not recommended because it requires deploying the model to the mobile app, which may not be feasible or desired. Deploying the model on Vertex AI and querying the prediction endpoint from the streaming pipeline adds minimal latency and is the simplest solution."
      },
      {
        "date": "2023-03-16T23:22:00.000Z",
        "voteCount": 1,
        "content": "Answer B"
      },
      {
        "date": "2023-03-09T11:53:00.000Z",
        "voteCount": 1,
        "content": "Aiiii between B and C"
      },
      {
        "date": "2023-02-17T06:09:00.000Z",
        "voteCount": 3,
        "content": "I perfectly agree with behzadsw. \nYou send a Pub/Sub request when you already want to buy, you must add the coupon before this process."
      },
      {
        "date": "2023-02-14T08:24:00.000Z",
        "voteCount": 3,
        "content": "B (is it possible)  along with What I get fromthis question.\n1. this prediction should be added to the ticket purchase process ( it mean that it have to be include in rocessed with a Dataflow streaming pipeline\n2.Each step in this process has low latency requirements (less than 50 milliseconds) , which signifies that whatever you will process in dataflow, there are no  latency requirements issues"
      },
      {
        "date": "2023-02-14T08:24:00.000Z",
        "voteCount": 1,
        "content": "https://www.tensorflow.org/tfx/tfx_bsl/api_docs/python/tfx_bsl/public/beam/RunInference"
      },
      {
        "date": "2022-12-27T02:55:00.000Z",
        "voteCount": 1,
        "content": "Answer D\nhttps://www.tensorflow.\norg/lite/guide"
      },
      {
        "date": "2023-03-09T11:54:00.000Z",
        "voteCount": 1,
        "content": "Nope answer is B"
      },
      {
        "date": "2022-12-13T10:43:00.000Z",
        "voteCount": 1,
        "content": "By deploying your model on Vertex AI, you can quickly and easily add the prediction step to your streaming pipeline, without the need to add additional infrastructure or manage model deployment and management."
      },
      {
        "date": "2022-12-17T20:34:00.000Z",
        "voteCount": 1,
        "content": "but maybe D is faster?"
      },
      {
        "date": "2022-12-18T06:52:00.000Z",
        "voteCount": 4,
        "content": "Hey I think you're right, predictions on the device itself, which avoids the need to send requests over the network. Should be D"
      },
      {
        "date": "2023-01-05T00:59:00.000Z",
        "voteCount": 3,
        "content": "You also want to do the prediction before the purchase right? so D is correct"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 139,
    "url": "https://www.examtopics.com/discussions/google/view/91476-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You work on a team in a data center that is responsible for server maintenance. Your management team wants you to build a predictive maintenance solution that uses monitoring data to detect potential server failures. Incident data has not been labeled yet. What should you do first?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTrain a time-series model to predict the machines\u2019 performance values. Configure an alert if a machine\u2019s actual performance values significantly differ from the predicted performance values.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDevelop a simple heuristic (e.g., based on z-score) to label the machines\u2019 historical performance data. Use this heuristic to monitor server performance in real time.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDevelop a simple heuristic (e.g., based on z-score) to label the machines\u2019 historical performance data. Train a model to predict anomalies based on this labeled dataset.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tHire a team of qualified analysts to review and label the machines\u2019 historical performance data. Train a model based on this manually labeled dataset."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 15,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 8,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-11-13T08:25:00.000Z",
        "voteCount": 5,
        "content": "Should be B"
      },
      {
        "date": "2023-11-13T08:29:00.000Z",
        "voteCount": 1,
        "content": "Compare to 94"
      },
      {
        "date": "2024-04-13T04:36:00.000Z",
        "voteCount": 1,
        "content": "C. Develop a simple heuristic (e.g., based on z-score) to label the machines\u2019 historical performance data. Train a model to predict anomalies based on this labeled dataset.\n\nReal-Time Heuristic Monitoring (Option B): Using a z-score based heuristic for real-time monitoring can be helpful as an initial step, but it might not capture complex anomaly patterns that a trained model could identify."
      },
      {
        "date": "2024-04-26T00:32:00.000Z",
        "voteCount": 2,
        "content": "\"What should you do first?\"...."
      },
      {
        "date": "2024-03-21T05:56:00.000Z",
        "voteCount": 1,
        "content": "I go for C because is more practical and efficient"
      },
      {
        "date": "2023-11-17T04:22:00.000Z",
        "voteCount": 1,
        "content": "D: This approach involves creating a labeled dataset through human analysis, which serves as the ground truth for training a predictive maintenance model. Manual labeling allows you to identify instances of actual failures and non-failure states in the historical performance data. Once the dataset is labeled, you can train a machine learning model to detect patterns associated with potential server failures."
      },
      {
        "date": "2023-11-17T04:23:00.000Z",
        "voteCount": 1,
        "content": "why not B (or C): While heuristics can be quick to implement, they may lack accuracy and may not capture complex patterns associated with server failures. Additionally, using a heuristic alone might not provide the necessary foundation for a robust predictive maintenance model."
      },
      {
        "date": "2024-02-29T21:16:00.000Z",
        "voteCount": 3,
        "content": "Google Rules of ML: Rule #1: Don\u2019t be afraid to launch a product without machine learning.\nhttps://developers.google.com/machine-learning/guides/rules-of-ml#rule_1_don%E2%80%99t_be_afraid_to_launch_a_product_without_machine_learning"
      },
      {
        "date": "2023-11-13T01:25:00.000Z",
        "voteCount": 2,
        "content": "Option B also falls short as it focuses on real-time monitoring based on a heuristic but doesn't utilize historical data to create a predictive model. This approach might raise false alarms and lacks the ability to learn from the data over time."
      },
      {
        "date": "2023-11-11T05:38:00.000Z",
        "voteCount": 2,
        "content": "Clearly the ask is an approach to build an ML application to detect potential server failures. Using labelled data to monitor it in real time does not give a proactive solution rather it becomes a reactive solution. I will go with C"
      },
      {
        "date": "2024-02-29T21:17:00.000Z",
        "voteCount": 2,
        "content": "It does not say use ML. It only says a predictive maintenance solution, that could be using a simple heuristic."
      },
      {
        "date": "2023-05-10T09:51:00.000Z",
        "voteCount": 1,
        "content": "The goal / \u201cyour\u201d task is to predict or \u201cbuild a predictive maintenance solution\u201d, i.e., \u201cTrain a model to predict anomalies\u201d [Option C]; not to perform monitoring or \u201cto monitor server performance in real time\u201d [Option B], there is a whole team \u201cresponsible for server maintenance\u201d.\nThe \u201cdo first\u201d part refers to the use of a simple heuristic for initial labeling, not to what to do with the results of it. \nThe more sophisticated solution: https://cloud.google.com/blog/products/ai-machine-learning/event-monitoring-with-explanations-on-the-google-cloud."
      },
      {
        "date": "2023-05-15T02:15:00.000Z",
        "voteCount": 3,
        "content": "Changed to B, based on the comparison with #94, assuming that by \u201cUse this heuristic to monitor server performance in real time\u201d is meant to \u201cfirst\u201d test this heuristic for labelling in a Prod. environment, as a quick reality-check, before training a whole model on a roughly inaccurate labelled dataset."
      },
      {
        "date": "2023-11-17T04:30:00.000Z",
        "voteCount": 1,
        "content": "why do you assume that this needs to be done \"quick\" instead of \"good\"?"
      },
      {
        "date": "2023-03-23T23:39:00.000Z",
        "voteCount": 2,
        "content": "ANSWER B"
      },
      {
        "date": "2023-03-13T05:35:00.000Z",
        "voteCount": 3,
        "content": "https://developers.google.com/machine-learning/guides/rules-of-ml"
      },
      {
        "date": "2024-08-10T17:30:00.000Z",
        "voteCount": 1,
        "content": "https://developers.google.com/machine-learning/guides/rules-of-ml#rule_1_don%E2%80%99t_be_afraid_to_launch_a_product_without_machine_learning\n&gt; Rule #1: Don\u2019t be afraid to launch a product without machine learning.\n&gt; Machine learning is cool, but it requires data. Theoretically, you can take data from a different problem and then tweak the model for a new product, but this will likely underperform basic heuristics. If you think that machine learning will give you a 100% boost, then a heuristic will get you 50% of the way there.\n\n&gt; For instance, if you are ranking apps in an app marketplace, you could use the install rate or number of installs as heuristics. If you are detecting spam, filter out publishers that have sent spam before. Don\u2019t be afraid to use human editing either. If you need to rank contacts, rank the most recently used highest (or even rank alphabetically). If machine learning is not absolutely required for your product, don't use it until you have data."
      },
      {
        "date": "2023-03-07T05:20:00.000Z",
        "voteCount": 1,
        "content": "D. Hire a team of qualified analysts to review and label the machines' historical performance data. Training a model based on this manually labeled dataset would be the most accurate and effective approach. Developing a simple heuristic to label the machines' historical performance data may not be accurate enough to detect all potential failures, and training a model without labeled data could result in poor performance. Additionally, it's important to ensure that the team of analysts is qualified and experienced in labeling this type of data accurately to ensure the model is trained with high-quality labeled data."
      },
      {
        "date": "2023-03-23T23:39:00.000Z",
        "voteCount": 1,
        "content": "Answer B"
      },
      {
        "date": "2023-02-17T06:03:00.000Z",
        "voteCount": 1,
        "content": "I like this question because it's helpful to remember that ML is used when needed.\nIn this case you have unlabeled target classes so you can use unsupervised learning techniques like clustering to identify patterns or just develop a heuristic method.\nAnswer 'B' in my opinion."
      },
      {
        "date": "2023-02-17T06:04:00.000Z",
        "voteCount": 1,
        "content": "sorry I meant 'B'"
      },
      {
        "date": "2022-12-27T02:53:00.000Z",
        "voteCount": 2,
        "content": "https://developers.\ngoogle.com/machine-\nlearning/\nguides/rules-of-ml   \nAnswer B"
      },
      {
        "date": "2022-12-22T02:56:00.000Z",
        "voteCount": 2,
        "content": "B\n- https://developers.google.com/machine-learning/guides/rules-of-ml"
      },
      {
        "date": "2022-12-17T20:36:00.000Z",
        "voteCount": 1,
        "content": "B should be first to do"
      },
      {
        "date": "2022-12-13T10:49:00.000Z",
        "voteCount": 1,
        "content": "Vote D"
      },
      {
        "date": "2022-12-18T06:48:00.000Z",
        "voteCount": 2,
        "content": "Should be B"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 140,
    "url": "https://www.examtopics.com/discussions/google/view/91478-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You work for a retailer that sells clothes to customers around the world. You have been tasked with ensuring that ML models are built in a secure manner. Specifically, you need to protect sensitive customer data that might be used in the models. You have identified four fields containing sensitive data that are being used by your data science team: AGE, IS_EXISTING_CUSTOMER, LATITUDE_LONGITUDE, and SHIRT_SIZE. What should you do with the data before it is made available to the data science team for training purposes?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTokenize all of the fields using hashed dummy values to replace the real values.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse principal component analysis (PCA) to reduce the four sensitive fields to one PCA vector.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCoarsen the data by putting AGE into quantiles and rounding LATITUDE_LONGTTUDE into single precision. The other two fields are already as coarse as possible.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRemove all sensitive data fields, and ask the data science team to build their models using non-sensitive data."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 19,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 5,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-06-06T03:39:00.000Z",
        "voteCount": 3,
        "content": "The best approach is C. Coarsen the data by putting AGE into quantiles and rounding LATITUDE_LONGITUDE into single precision. The other two fields are already as coarse as possible.\n\nHere's why:\n\nPreserves Utility: Coarsening the data reduces its sensitivity while retaining some of its informational value for modeling. Age quantiles and approximate location can still be useful features for certain types of models.\nMinimizes Risk: By removing the exact age and precise location, you significantly reduce the risk of re-identification or misuse of sensitive information.\nPracticality: Coarsening is a relatively simple technique to implement and doesn't require complex transformations or additional model training.\n\npen_spark"
      },
      {
        "date": "2023-11-17T04:42:00.000Z",
        "voteCount": 1,
        "content": "This approach involves not providing the sensitive fields (AGE, IS_EXISTING_CUSTOMER, LATITUDE_LONGITUDE, and SHIRT_SIZE) to the data science team for model training. Instead, the team can focus on building models using non-sensitive data. This helps to mitigate the risk of exposing sensitive customer information during the development and training process.\n\nWhile options A, B, and C propose different methods of obfuscating or transforming the sensitive data, they may introduce complexities and potential risks. Tokenizing with hashed dummy values (option A) may not be foolproof in terms of security, and PCA (option B) may not effectively retain the necessary information for accurate modeling. Coarsening the data (option C) might still retain some level of identifiable information, and it may not be sufficient for ensuring the privacy of sensitive data."
      },
      {
        "date": "2024-02-29T07:31:00.000Z",
        "voteCount": 2,
        "content": "why would you remove potential important features from the training?"
      },
      {
        "date": "2023-05-10T09:52:00.000Z",
        "voteCount": 3,
        "content": "Went with A"
      },
      {
        "date": "2023-03-07T05:19:00.000Z",
        "voteCount": 1,
        "content": "D. Remove all sensitive data fields, and ask the data science team to build their models using non-sensitive data. This is the best approach to protect sensitive customer data. Removing the sensitive fields is the most secure option because it eliminates the risk of any potential data breaches. Tokenizing or coarsening the data may still reveal sensitive information if the hashed dummy values can be reversed or if the coarsening can be used to identify individual customers. PCA can also be a useful technique to reduce dimensionality and protect privacy, but it may not be appropriate in this case because it is not clear how the sensitive fields can be combined into a single PCA vector without losing information."
      },
      {
        "date": "2023-07-27T11:04:00.000Z",
        "voteCount": 1,
        "content": "Removing all sensitive data fields (Option D) would likely limit the effectiveness of the machine learning model, as important predictive variables would be excluded from the training process. It is important to balance privacy considerations with the need to train accurate models that can provide valuable insights and predictions."
      },
      {
        "date": "2023-11-17T04:41:00.000Z",
        "voteCount": 1,
        "content": "But in option A, Hashing can result in information loss. While the original values are hidden, the hashed values might not retain the same level of information, which can impact the effectiveness of the machine learning models."
      },
      {
        "date": "2023-02-17T05:57:00.000Z",
        "voteCount": 4,
        "content": "B -&gt; possible in general but not suitable in this case  since you don't know  AGE, IS_EXISTING_CUSTOMER, LATITUDE_LONGITUDE, and SHIRT_SIZE are the first components in PCA.\nC -&gt; You are changing data which could be highly correlated with the output\nD -&gt; like C explanation \n\nAnswer 'A' uses hashing so you encript the data without losing relevant information"
      },
      {
        "date": "2023-01-02T07:48:00.000Z",
        "voteCount": 4,
        "content": "Hashing --&gt; A"
      },
      {
        "date": "2022-12-27T02:07:00.000Z",
        "voteCount": 3,
        "content": "Answer A"
      },
      {
        "date": "2022-12-22T02:59:00.000Z",
        "voteCount": 3,
        "content": "A (by experience)"
      },
      {
        "date": "2022-12-25T13:42:00.000Z",
        "voteCount": 4,
        "content": "https://cloud.google.com/blog/products/identity-security/take-charge-of-your-data-how-tokenization-makes-data-usable-without-sacrificing-privacy"
      },
      {
        "date": "2022-12-17T20:38:00.000Z",
        "voteCount": 2,
        "content": "I think hash should be better"
      },
      {
        "date": "2022-12-13T10:54:00.000Z",
        "voteCount": 3,
        "content": "Removing the sensitive data fields is the safest and most effective way to ensure that customer data is not used in the training of your models."
      },
      {
        "date": "2022-12-25T13:42:00.000Z",
        "voteCount": 1,
        "content": "see https://cloud.google.com/blog/products/identity-security/take-charge-of-your-data-how-tokenization-makes-data-usable-without-sacrificing-privacy"
      },
      {
        "date": "2023-07-27T11:04:00.000Z",
        "voteCount": 1,
        "content": "Removing all sensitive data fields (Option D) would likely limit the effectiveness of the machine learning model, as important predictive variables would be excluded from the training process. It is important to balance privacy considerations with the need to train accurate models that can provide valuable insights and predictions."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 141,
    "url": "https://www.examtopics.com/discussions/google/view/92951-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You work for a magazine publisher and have been tasked with predicting whether customers will cancel their annual subscription. In your exploratory data analysis, you find that 90% of individuals renew their subscription every year, and only 10% of individuals cancel their subscription. After training a NN Classifier, your model predicts those who cancel their subscription with 99% accuracy and predicts those who renew their subscription with 82% accuracy. How should you interpret these results?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThis is not a good result because the model should have a higher accuracy for those who renew their subscription than for those who cancel their subscription.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThis is not a good result because the model is performing worse than predicting that people will always renew their subscription.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThis is a good result because predicting those who cancel their subscription is more difficult, since there is less data for this group.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThis is a good result because the accuracy across both groups is greater than 80%."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 19,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 8,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-27T02:06:00.000Z",
        "voteCount": 6,
        "content": "Answer C"
      },
      {
        "date": "2023-11-17T04:49:00.000Z",
        "voteCount": 3,
        "content": "Here's the reasoning:\n\nThe overall renewal rate is 90%, meaning that if the model simply predicted that everyone would renew, it would have an accuracy of 90%. The model's accuracy for predicting renewals (82%) is lower than this baseline accuracy.\nThe model's accuracy for predicting cancellations is high (99%), but this could be misleading. If only 10% of individuals cancel their subscription, a model that predicts no cancellations at all would still have a high accuracy of 90%. Therefore, the high accuracy for cancellations may not be very informative.\nIn summary, the model is not performing well, especially when compared to a simple baseline of always predicting renewals."
      },
      {
        "date": "2024-06-11T05:04:00.000Z",
        "voteCount": 1,
        "content": "if we suppose the case where the model simply predicted that everyone would renew, the renewals rate should be always higher than the cancellations. Therefore, This case means that the model made some asumptions about how a cancellation looks like and misled some of the renewals cases (it could make some wrong asumptions because there are few data)"
      },
      {
        "date": "2023-11-17T04:50:00.000Z",
        "voteCount": 2,
        "content": "C suggests that predicting cancellations is more difficult due to less data for this group. While it's true that imbalanced datasets, where one class is underrepresented, can pose challenges for machine learning models, the key issue here is that the model's accuracy for predicting renewals is lower than the accuracy for predicting cancellations.\n\nIn this scenario, the imbalance alone does not explain the lower accuracy for renewals. The model should ideally perform well on both classes, and the fact that it doesn't, especially when compared to a simple baseline of always predicting renewals (which would have an accuracy of 90%), suggests that there's a problem with the model's performance.\n\nTherefore, option B is a more appropriate interpretation, highlighting that the model is performing worse than a basic strategy of always predicting renewals."
      },
      {
        "date": "2023-07-06T11:10:00.000Z",
        "voteCount": 2,
        "content": "since there is less data for this group. While the accuracy for predicting subscription renewals is lower, it is still above chance and may still be useful. Additionally, the high accuracy for predicting cancellations is promising, as this is the group of interest for the publisher. However, it would still be important to assess the model's precision and recall to fully evaluate its performance."
      },
      {
        "date": "2023-05-28T03:34:00.000Z",
        "voteCount": 4,
        "content": "Went with C: This is a good result because predicting those who cancel their subscription is more difficult, since there is less data for this group\n My Reason: \"You have been tasked with predicting whether customers will cancel their annual subscription.\" And in that task you are getting 99% of accuracy"
      },
      {
        "date": "2023-05-26T12:44:00.000Z",
        "voteCount": 2,
        "content": "Task is to predict whether customer will cancel subscription, so both renew and cancel predictions are important. The overall accuracy is 99% x 10% + 82% x 90% = 83%, while guessing always renew has 90% accuracy."
      },
      {
        "date": "2023-05-10T09:53:00.000Z",
        "voteCount": 2,
        "content": "#ResponsibleAI, predicting the majority class (imbalanced data) topic: \u201cthe model [82% accuracy for renew] is performing worse than predicting that people will always [90% accuracy] renew their subscription\u201d.\nhttps://developers.google.com/machine-learning/crash-course/classification/check-your-understanding-accuracy-precision-recall\n\u201cA deadly, but curable, medical condition afflicts .01% of the population. An ML model (\u2026) predicts (\u2026) with an accuracy of 99.99%. (\u2026) After all, even a \"dumb\" model that always predicts \"not sick\" would still be 99.99% accurate.\u201c"
      },
      {
        "date": "2023-04-24T00:54:00.000Z",
        "voteCount": 1,
        "content": "The 82% accuracy for renewals is lower than a naive model that always predicts renewals (which would have a 90% accuracy)."
      },
      {
        "date": "2023-02-17T05:53:00.000Z",
        "voteCount": 3,
        "content": "I think C is the only way"
      },
      {
        "date": "2023-02-14T20:46:00.000Z",
        "voteCount": 2,
        "content": "We can consider it as follows reasonably.\nA: it doesn't make any sense,  given that cancel=99% but renew =82%, how did you make renew class (82%) beat the Cancel class(99%), it must be 100% accuracy ( bullshit)\nB:  Cancel class have more accuracy than renew (99%&gt;82%)\nD: You can justify, both are good 80% if we have a balance class.\n\nSo it left us with C.   This model predicts well upon imbalanced class circumstances.\ntarget class =10 samles meanwhile  the another =90 samples"
      },
      {
        "date": "2023-01-02T07:33:00.000Z",
        "voteCount": 2,
        "content": "Logically, it should be C."
      },
      {
        "date": "2022-12-28T09:54:00.000Z",
        "voteCount": 1,
        "content": "Since 90% of dataset represent customer who will renew subscription, accuracy should have been greater than 82%"
      },
      {
        "date": "2023-02-15T23:24:00.000Z",
        "voteCount": 1,
        "content": "I think C is the most likely.  we are experiencing an imbalanced dataset and the target class is canceled.   \nActually this case we have to use other metrics like F1 and precision/recall\nIf you want to get ReNew accuracy higher than CAncel, you have to make it greater than 99% as compared to another.  it is hard to archive."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 142,
    "url": "https://www.examtopics.com/discussions/google/view/91483-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You have built a model that is trained on data stored in Parquet files. You access the data through a Hive table hosted on Google Cloud. You preprocessed these data with PySpark and exported it as a CSV file into Cloud Storage. After preprocessing, you execute additional steps to train and evaluate your model. You want to parametrize this model training in Kubeflow Pipelines. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRemove the data transformation step from your pipeline.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tContainerize the PySpark transformation step, and add it to your pipeline.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a ContainerOp to your pipeline that spins a Dataproc cluster, runs a transformation, and then saves the transformed data in Cloud Storage.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy Apache Spark at a separate node pool in a Google Kubernetes Engine cluster. Add a ContainerOp to your pipeline that invokes a corresponding transformation job for this Spark instance."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 18,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 7,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-13T11:02:00.000Z",
        "voteCount": 7,
        "content": "This will allow to reuse the same pipeline for different datasets without the need to manually preprocess and transform the data each time."
      },
      {
        "date": "2023-07-06T11:07:00.000Z",
        "voteCount": 6,
        "content": "Since the data is stored in Parquet format, it's more efficient to use Spark to transform it. Containerizing the PySpark transformation step and adding it to the pipeline may not be the optimal solution since it may require additional resources to run this container. Deploying Apache Spark at a separate node pool in a Google Kubernetes Engine cluster and adding a ContainerOp to invoke a corresponding transformation job for this Spark instance is also a possible solution, but it may require more setup and configuration.\n\nUsing Dataproc can simplify this process since it's a fully managed service that simplifies running Apache Spark and Hadoop clusters. A ContainerOp can be added to the pipeline to spin up a Dataproc cluster, run the transformation using PySpark, and save the transformed data in Cloud Storage. This solution is more efficient since Dataproc can scale the cluster based on the size of the data and the complexity of the transformation."
      },
      {
        "date": "2023-06-28T14:58:00.000Z",
        "voteCount": 1,
        "content": "you can conteinerize the transformation and then save to google storage"
      },
      {
        "date": "2023-07-27T10:21:00.000Z",
        "voteCount": 1,
        "content": "it is not the most efficient and scalable solution when working with big data in the context of Google Cloud."
      },
      {
        "date": "2023-05-10T09:53:00.000Z",
        "voteCount": 1,
        "content": "https://kubeflow-pipelines.readthedocs.io/en/stable/source/kfp.dsl.html#kfp.dsl.ContainerOp\nhttps://medium.com/@vignesh093/running-preprocessing-and-ml-workflow-in-kubeflow-with-google-dataproc-84103a9ef67e"
      },
      {
        "date": "2023-03-07T05:02:00.000Z",
        "voteCount": 2,
        "content": "C. Add a ContainerOp to your pipeline that spins a Dataproc cluster, runs a transformation, and then saves the transformed data in Cloud Storage.\n\nThe recommended approach to parametrize the model training in Kubeflow Pipelines would be to add a ContainerOp to the pipeline that spins up a Dataproc cluster, runs the PySpark transformation step, and saves the transformed data in Cloud Storage. This approach allows for easy integration of PySpark transformations with Kubeflow Pipelines while taking advantage of the scalability and efficiency of Dataproc."
      },
      {
        "date": "2023-02-26T15:15:00.000Z",
        "voteCount": 6,
        "content": "All the wrong answers on this site really baffle me...correct answer is B... you must containerize your component for Kubeflow to run it.\n\nhttps://www.kubeflow.org/docs/components/pipelines/v1/sdk/component-development/#containerize-your-components-code"
      },
      {
        "date": "2023-03-07T05:02:00.000Z",
        "voteCount": 3,
        "content": "C. Add a ContainerOp to your pipeline that spins a Dataproc cluster, runs a transformation, and then saves the transformed data in Cloud Storage.\n\nThe recommended approach to parametrize the model training in Kubeflow Pipelines would be to add a ContainerOp to the pipeline that spins up a Dataproc cluster, runs the PySpark transformation step, and saves the transformed data in Cloud Storage. This approach allows for easy integration of PySpark transformations with Kubeflow Pipelines while taking advantage of the scalability and efficiency of Dataproc."
      },
      {
        "date": "2022-12-27T02:06:00.000Z",
        "voteCount": 2,
        "content": "Answer C"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 143,
    "url": "https://www.examtopics.com/discussions/google/view/100436-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You have developed an ML model to detect the sentiment of users\u2019 posts on your company's social media page to identify outages or bugs. You are using Dataflow to provide real-time predictions on data ingested from Pub/Sub. You plan to have multiple training iterations for your model and keep the latest two versions live after every run. You want to split the traffic between the versions in an 80:20 ratio, with the newest model getting the majority of the traffic. You want to keep the pipeline as simple as possible, with minimal management required. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the models to a Vertex AI endpoint using the traffic-split=0=80, PREVIOUS_MODEL_ID=20 configuration.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrap the models inside an App Engine application using the --splits PREVIOUS_VERSION=0.2, NEW_VERSION=0.8 configuration",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrap the models inside a Cloud Run container using the REVISION1=20, REVISION2=80 revision configuration.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement random splitting in Dataflow using beam.Partition() with a partition function calling a Vertex AI endpoint."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 15,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-03-07T04:58:00.000Z",
        "voteCount": 8,
        "content": "A. Deploy the models to a Vertex AI endpoint using the traffic-split=0=80, PREVIOUS_MODEL_ID=20 configuration.\n\nThe recommended approach to achieve the desired outcome would be to deploy the ML models to a Vertex AI endpoint and configure the traffic splitting using the traffic-split parameter. The traffic-split parameter enables you to split traffic between multiple versions of a model based on a percentage split. In this case, the newest model should receive the majority of the traffic, which can be achieved by setting the traffic-split parameter to 0=80. The previous version of the model should receive the remaining 20% of the traffic, which can be achieved by setting the PREVIOUS_MODEL_ID parameter to 20."
      },
      {
        "date": "2024-04-22T02:12:00.000Z",
        "voteCount": 1,
        "content": "Vertex AI Traffic Splitting: Vertex AI natively supports traffic splitting between deployed models through the traffic-split parameter. This allows you to specify the desired traffic distribution (80% to the newest model, 20% to the previous one) during deployment."
      },
      {
        "date": "2023-05-10T09:54:00.000Z",
        "voteCount": 2,
        "content": "Went with A"
      },
      {
        "date": "2023-02-22T15:41:00.000Z",
        "voteCount": 4,
        "content": "I think is A because traffic can be split across different versions when using Endpoints https://cloud.google.com/vertex-ai/docs/general/deployment#models-endpoint.\nThe --trafic-split flag does exist, but in the question the syntax is incorrect, it should be \"--traffic-split = [MODEL_ID_1=value, MODEL_ID_2=value]\" as explained in https://cloud.google.com/sdk/gcloud/reference/ai/endpoints/deploy-model"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 144,
    "url": "https://www.examtopics.com/discussions/google/view/99050-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You are developing an image recognition model using PyTorch based on ResNet50 architecture. Your code is working fine on your local laptop on a small subsample. Your full dataset has 200k labeled images. You want to quickly scale your training workload while minimizing cost. You plan to use 4 V100 GPUs. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Google Kubernetes Engine cluster with a node pool that has 4 V100 GPUs. Prepare and submit a TFJob operator to this node pool.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Vertex AI Workbench user-managed notebooks instance with 4 V100 GPUs, and use it to train your model.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPackage your code with Setuptools, and use a pre-built container. Train your model with Vertex AI using a custom tier that contains the required GPUs.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a Compute Engine VM with all the dependencies that launches the training. Train your model with Vertex AI using a custom tier that contains the required GPUs."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 15,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-03-07T07:45:00.000Z",
        "voteCount": 9,
        "content": "Custom trainer , don't overthink 1000%, this is google recommendation.\nyou don't need  Vertex AI Workbench user-managed notebooks,Google Kubernetes Engine,  Compute Engine at at all , it is a waste of your effort\nhttps://cloud.google.com/vertex-ai/docs/training/configure-compute#specifying_gpus\nYou can choose as your want"
      },
      {
        "date": "2024-04-25T09:56:00.000Z",
        "voteCount": 1,
        "content": "Why in the world would you setup a Compute engine VM, when your custom training job on vertex runs \"serverless\" atleast for the user side don't have to maintain the vm. You literally just have to select the region , machine type and accelerators that's all."
      },
      {
        "date": "2024-04-22T02:14:00.000Z",
        "voteCount": 3,
        "content": "Pre-built container: Utilizing a pre-built PyTorch container image eliminates the need to manage dependencies within your container, saving time and simplifying the process.\nVertex AI custom tier: Vertex AI custom tiers allow you to configure a machine type with the desired GPUs (4 V100 in this case) and pay only for the resources you use. This is more cost-effective than managing a dedicated VM instance.\nSetuptools packaging: Packaging your code with tools like Setuptools ensures all necessary libraries and scripts are included within the container, creating a self-contained training environment."
      },
      {
        "date": "2023-11-14T09:10:00.000Z",
        "voteCount": 1,
        "content": "Using Vertex AI allows you to easily leverage multiple GPUs without managing infrastructure yourself. The custom tier gives you control to specify 4 V100 GPUs.\nPackaging with Setuptools and using a pre-built container ensures a consistent and portable environment with all dependencies readily available.\nThis approach minimizes overhead and cost by relying on Vertex AI's managed service instead of setting up your own Kubernetes cluster or VMs."
      },
      {
        "date": "2023-07-20T02:59:00.000Z",
        "voteCount": 1,
        "content": "Correct Ans is C. Below mentioned why B is incorrect."
      },
      {
        "date": "2023-07-20T02:58:00.000Z",
        "voteCount": 1,
        "content": "Option B (using a Vertex AI Workbench user-managed notebooks instance with 4 V100 GPUs) is more suitable for interactive data exploration and experimentation rather than large-scale model training. Vertex AI Workbench is designed for collaborative data science, but using it for model training might not be the most efficient approach."
      },
      {
        "date": "2023-05-22T00:21:00.000Z",
        "voteCount": 1,
        "content": "What is Supetools?"
      },
      {
        "date": "2023-05-25T01:10:00.000Z",
        "voteCount": 1,
        "content": "Found it. Python package that provides a mechanism for packaging, distributing, and installing Python libraries or modules."
      },
      {
        "date": "2023-05-10T09:54:00.000Z",
        "voteCount": 1,
        "content": "\u201cVertex AI provides flexible and scalable hardware and secured infrastructure to train PyTorch based deep learning models with pre-built containers and custom containers. (\u2026) use PyTorch ResNet-50 as the example model and train it on ImageNet validation data (50K images) to measure the training performance for different training strategies\u201d: https://cloud.google.com/blog/products/ai-machine-learning/efficient-pytorch-training-with-vertex-ai"
      },
      {
        "date": "2023-05-10T09:55:00.000Z",
        "voteCount": 1,
        "content": "There is no indication otherwise why there would be a need for full control over the environment, provided by \u201cuser-managed workbooks\u201d within the Vertex AI Workbench [Option B], except for the \u201cplan to use 4 V100 GPUs\u201d, but one can do that with \u201cmanaged workbooks\u201d as well:\nhttps://cloud.google.com/vertex-ai/docs/workbench/notebook-solution#control_your_hardware_and_framework_from_jupyterlab"
      },
      {
        "date": "2023-04-23T12:01:00.000Z",
        "voteCount": 2,
        "content": "Can someone explain why is B wrong?"
      },
      {
        "date": "2023-09-11T05:36:00.000Z",
        "voteCount": 1,
        "content": "Very likely because of the consideration: \n\"You want to quickly scale your training workload while minimizing cost\"\nBut I agfree with you ... I chose B (notebook) thinking the question was more oriented to haquickly achieving an MVP."
      },
      {
        "date": "2023-04-18T00:03:00.000Z",
        "voteCount": 1,
        "content": "Answer C\nOption A involves using Google Kubernetes Engine, which is a platform for deploying, managing, and scaling containerized applications. However, it requires more setup time and knowledge of Kubernetes, which might not be ideal for quickly scaling up training workloads. Furthermore, the use of the TensorFlow Job operator seems inappropriate for a PyTorch-based model."
      },
      {
        "date": "2023-03-25T03:01:00.000Z",
        "voteCount": 1,
        "content": "Select C"
      },
      {
        "date": "2023-03-25T03:03:00.000Z",
        "voteCount": 1,
        "content": "The TFJob operator is designed for TensorFlow workloads, not PyTorch. So option A is out.\nVertex AI Workbench is primarily designed for interactive work with Jupyter Notebooks and not optimized for large-scale, long-running model training. Moreover, it may not provide the same level of cost optimization as Vertex AI Training, which automatically provisions and manages resources, and can scale down when not in use. So option B also out."
      },
      {
        "date": "2023-03-07T05:00:00.000Z",
        "voteCount": 1,
        "content": "C. Package your code with Setuptools, and use a pre-built container. Train your model with Vertex AI using a custom tier that contains the required GPUs.\n\nThe recommended approach to scale the training workload while minimizing cost would be to package the code with Setuptools and use a pre-built container, then train the model with Vertex AI using a custom tier that contains the required GPUs. This approach allows for quick and easy scaling of the training workload while minimizing infrastructure management costs."
      },
      {
        "date": "2023-02-23T12:36:00.000Z",
        "voteCount": 1,
        "content": "Vote for A, as you need to scale"
      },
      {
        "date": "2023-03-07T01:20:00.000Z",
        "voteCount": 2,
        "content": "It clearly says a Pytorch model, you cannot use a TFjob"
      },
      {
        "date": "2023-02-16T08:34:00.000Z",
        "voteCount": 2,
        "content": "It's B according to me, since VertexAI Notebook has alla dependencies for PyTorch that is the fastest solution"
      },
      {
        "date": "2023-07-27T10:14:00.000Z",
        "voteCount": 1,
        "content": "It involves using a managed notebooks instance, which might have limitations in terms of customizability and flexibility compared to a containerized approach."
      },
      {
        "date": "2023-02-13T03:14:00.000Z",
        "voteCount": 2,
        "content": "Google Kubernetes Engine (GKE) is a powerful and easy-to-use platform for deploying and managing containerized applications. It allows you to create a cluster of virtual machines that are pre-configured with the necessary dependencies and resources to run your machine learning workloads. By creating a GKE cluster with a node pool that has 4 V100 GPUs, you can take advantage of the powerful processing capabilities of these GPUs to train your model quickly and efficiently.\n\nYou can then use the Kubernetes Framework such as TFJob operator to submit the job of training your model, which will automatically distribute the workload across the available GPUs.\n\nReferences:\n\nGoogle Kubernetes Engine\nTFJob operator\nVertex Al"
      },
      {
        "date": "2023-03-07T01:21:00.000Z",
        "voteCount": 1,
        "content": "It clearly says a Pytorch model, you cannot use a TFjob"
      },
      {
        "date": "2023-03-07T05:01:00.000Z",
        "voteCount": 2,
        "content": "Answer C"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 145,
    "url": "https://www.examtopics.com/discussions/google/view/97893-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You have trained a DNN regressor with TensorFlow to predict housing prices using a set of predictive features. Your default precision is tf.float64, and you use a standard TensorFlow estimator:<br><br><img src=\"https://img.examtopics.com/professional-machine-learning-engineer/image2.png\"><br><br>Your model performs well, but just before deploying it to production, you discover that your current serving latency is 10ms @ 90 percentile and you currently serve on CPUs. Your production requirements expect a model latency of 8ms @ 90 percentile. You're willing to accept a small decrease in performance in order to reach the latency requirement.<br>Therefore your plan is to improve latency while evaluating how much the model's prediction decreases. What should you first try to quickly lower the serving latency?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSwitch from CPU to GPU serving.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tApply quantization to your SavedModel by reducing the floating point precision to tf.float16.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the dropout rate to 0.8 and retrain your model.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the dropout rate to 0.8 in _PREDICT mode by adjusting the TensorFlow Serving parameters."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 21,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 9,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-09-10T07:18:00.000Z",
        "voteCount": 1,
        "content": "I know the answer is B becaue the question is telegraphing it so much: \"You can lower quality a bit\" (waggles eyebrows) - that obviously means quantizing (the other changes are silly). But in reality A would be much more normal thing to do. It's unusual to even attempt serving an NN on CPU these days."
      },
      {
        "date": "2024-04-22T02:18:00.000Z",
        "voteCount": 4,
        "content": "Reduced model size: Quantization reduces the model size by using lower precision data types like tf.float16 instead of the default tf.float64. This smaller size leads to faster loading and processing during inference.\nMinimal performance impact: Quantization often introduces a small decrease in model accuracy, but it's a good initial step to explore due to the potential latency gains with minimal performance trade-offs."
      },
      {
        "date": "2024-04-15T02:31:00.000Z",
        "voteCount": 1,
        "content": "I went with B."
      },
      {
        "date": "2024-02-28T16:02:00.000Z",
        "voteCount": 1,
        "content": "I went with B."
      },
      {
        "date": "2023-12-22T10:23:00.000Z",
        "voteCount": 4,
        "content": "Switching from CPU to GPU serving could also improve latency, but it may not be considered a \"quick\" solution compared to model quantization because it involves additional hardware requirements and potentially more complex deployment changes. Additionally, not all models see a latency improvement on GPUs, especially if the model is not large enough to utilize the GPU effectively or if the infrastructure does not support GPU optimizations.\nTherefore, the first thing to try would be quantization, which can be done relatively quickly and directly within the TensorFlow framework. After applying quantization, you should evaluate the model to ensure that the decrease in precision does not lead to an unacceptable drop in prediction accuracy."
      },
      {
        "date": "2023-11-14T09:22:00.000Z",
        "voteCount": 1,
        "content": "Very confusing A or B but leaning to A"
      },
      {
        "date": "2023-11-16T08:42:00.000Z",
        "voteCount": 2,
        "content": "Changed to B"
      },
      {
        "date": "2023-09-11T21:20:00.000Z",
        "voteCount": 2,
        "content": "B based on the consideration: \"Therefore your plan is to improve latency while evaluating how much the model's prediction decreases\""
      },
      {
        "date": "2023-06-08T08:39:00.000Z",
        "voteCount": 4,
        "content": "To me is \nB. Apply quantization to your SavedModel by reducing the floating point precision to tf.float16.\nObviously that switthing to GPU improve latency BUT.... it says \"Therefore your plan is to improve latency while evaluating how much the model's prediction decreases.\" If you want evaluate how much decrease is because you are going to make changes that affect the prediction"
      },
      {
        "date": "2023-06-12T20:10:00.000Z",
        "voteCount": 2,
        "content": "according to the documentation we have to convert to TensorFlowLite before applying quantization or use an API https://www.tensorflow.org/model_optimization/guide/quantization/training\ndoesn't look to be first option. Second maybe?"
      },
      {
        "date": "2023-05-28T04:28:00.000Z",
        "voteCount": 1,
        "content": "Going with A:\nMy reason to discard B: from https://www.tensorflow.org/lite/performance/post_training_quantization#float16_quantization\nThe advantages of float16 quantization are as follows:\n\nIt reduces model size by up to half (since all weights become half of their original size).\nIt causes minimal loss in accuracy.\nIt supports some delegates (e.g. the GPU delegate) which can operate directly on float16 data, resulting in faster execution than float32 computations.\nThe disadvantages of float16 quantization are as follows:\n\nIt does not reduce latency as much as a quantization to fixed point math.\nBy default, a float16 quantized model will \"dequantize\" the weights values to float32 when run on the CPU. (Note that the GPU delegate will not perform this dequantization, since it can operate on float16 data.)"
      },
      {
        "date": "2023-05-16T05:10:00.000Z",
        "voteCount": 4,
        "content": "Going with B because quantization can reduce the model size and inference latency by using lower-precision arithmetic operations, while maintaining acceptable accuracy. The other options are either not feasible or not effective for lowering the serving latency. Switching from CPU to GPU serving may not be possible or cost-effective, increasing the dropout rate may degrade the model performance significantly, and dropout is not applied in _PREDICT mode by default."
      },
      {
        "date": "2023-05-10T09:56:00.000Z",
        "voteCount": 2,
        "content": "For tf.float16 [Option B], we would have to be on TFLite:\nhttps://discuss.tensorflow.org/t/convert-tensorflow-saved-model-from-float32-to-float16/12130 and resp.\nhttps://www.tensorflow.org/lite/performance/post_training_quantization#float16_quantization (plus \u201cBy default, a float16 quantized model will \"dequantize\" the weights values to float32 when run on the CPU. (Note that the GPU delegate will not perform this dequantization, since it can operate on float16 data.)\u201d"
      },
      {
        "date": "2023-05-10T09:56:00.000Z",
        "voteCount": 1,
        "content": "But even before that, tf.estimator.DNNRegressor is deprecated, \u201cUse tf.keras instead\u201d: https://www.tensorflow.org/api_docs/python/tf/estimator/DNNRegressor. \nWhen used with Keras (a high-level NN library that runs on top of TF), for training though, \u201cIt is not recommended to set this to float16 for training, as this will likely cause numeric stability issues. Instead, mixed precision, which is using a mix of float16 and float32, can be used\u201d: https://www.tensorflow.org/api_docs/python/tf/keras/backend/set_floatx."
      },
      {
        "date": "2023-05-10T09:56:00.000Z",
        "voteCount": 1,
        "content": "But then, \u201cOn CPUs, mixed precision will run significantly slower, however.\u201d: https://www.tensorflow.org/guide/mixed_precision#supported_hardware.\nAnd, \u201cThe policy will run on other GPUs and CPUs but may not improve performance.\u201d: https://www.tensorflow.org/guide/mixed_precision#setting_the_dtype_policy."
      },
      {
        "date": "2023-05-10T09:57:00.000Z",
        "voteCount": 1,
        "content": "\u201cThis can take around 500ms to process a single Tweet (of at most 128 tokens) on a CPU-based machine. The processing time can be greatly reduced to 20ms by running the model on a GPU instance (\u2026). An option to dynamically quantize a TensorFlow model wasn\u2019t available, so we updated the script to convert the TensorFlow models into TFLite and created the options to apply int8 or fp16 quantization.\u201d: https://blog.twitter.com/engineering/en_us/topics/insights/2021/speeding-up-transformer-cpu-inference-in-google-cloud"
      },
      {
        "date": "2023-04-24T06:05:00.000Z",
        "voteCount": 1,
        "content": "A and B both work well here, but I prefer A since B would imply some minor tradeoff between latency and model accuracy, which isn't the case for A. So I would consider quantization after switching to GPU serving. Can anyone explain why B might be better than A?"
      },
      {
        "date": "2023-04-25T01:47:00.000Z",
        "voteCount": 3,
        "content": "Since you're allowing a small decrease in accuracy, you should choose B as it is more cost effective than A."
      },
      {
        "date": "2023-05-15T09:57:00.000Z",
        "voteCount": 2,
        "content": "There's no mention that we're cost sensitive in this scenario. Why should we assume that reducing cost is more important than model accuracy?"
      },
      {
        "date": "2023-07-26T11:26:00.000Z",
        "voteCount": 2,
        "content": "Yes you're right. But, Quantization reduces the floating point precision, which can result in a smaller model size and lower memory footprint. This can lead to faster serving times and improved latency. In comparison, switching to GPU serving doesn't necessarily reduce the model size or memory footprint.Also, it provides a more direct way to balance the trade-off between model performance and latency, as it directly impacts the model's precision. Switching to GPU serving may improve latency but doesn't directly address the trade-off between performance and latency. While Switching to GPU serving may require changes to your existing serving infrastructure, which can be time-consuming and may not be compatible with your current setup. Quantization, on the other hand, is a model optimization technique that can be applied directly to the model without requiring changes to the serving infrastructure."
      },
      {
        "date": "2023-03-07T05:16:00.000Z",
        "voteCount": 4,
        "content": "A makes sense too"
      },
      {
        "date": "2023-03-09T11:48:00.000Z",
        "voteCount": 2,
        "content": "But answer is B , i dnt know how i clicked A"
      },
      {
        "date": "2023-03-07T05:17:00.000Z",
        "voteCount": 2,
        "content": "GPU serving can significantly speed up the serving of models due to the parallel processing power of GPUs. By switching from CPU to GPU serving, you can quickly lower the serving latency without making changes to the model architecture or precision. Once you have switched to GPU serving, you can evaluate the impact on the model's prediction quality and consider further optimization techniques if necessary. Therefore, the correct option is A."
      },
      {
        "date": "2023-02-13T03:09:00.000Z",
        "voteCount": 1,
        "content": "Answer B\nApplying quantization to your SavedModel by reducing the floating point precision can help reduce the serving latency by decreasing the amount of memory and computation required to make a prediction. TensorFlow provides tools such as the tf.quantization module that can be used to quantize models and reduce their precision, which can significantly reduce serving latency without a significant decrease in model performance"
      },
      {
        "date": "2023-02-03T21:02:00.000Z",
        "voteCount": 4,
        "content": "Vote B. https://www.tensorflow.org/lite/performance/post_training_float16_quant"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 146,
    "url": "https://www.examtopics.com/discussions/google/view/97895-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You work on the data science team at a manufacturing company. You are reviewing the company\u2019s historical sales data, which has hundreds of millions of records. For your exploratory data analysis, you need to calculate descriptive statistics such as mean, median, and mode; conduct complex statistical tests for hypothesis testing; and plot variations of the features over time. You want to use as much of the sales data as possible in your analyses while minimizing computational resources. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tVisualize the time plots in Google Data Studio. Import the dataset into Vertex Al Workbench user-managed notebooks. Use this data to calculate the descriptive statistics and run the statistical analyses.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSpin up a Vertex Al Workbench user-managed notebooks instance and import the dataset. Use this data to create statistical and visual analyses.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse BigQuery to calculate the descriptive statistics. Use Vertex Al Workbench user-managed notebooks to visualize the time plots and run the statistical analyses.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse BigQuery to calculate the descriptive statistics, and use Google Data Studio to visualize the time plots. Use Vertex Al Workbench user-managed notebooks to run the statistical analyses."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 11,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 6,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-04-15T02:33:00.000Z",
        "voteCount": 1,
        "content": "went with c"
      },
      {
        "date": "2024-03-21T00:01:00.000Z",
        "voteCount": 2,
        "content": "went with c"
      },
      {
        "date": "2023-11-13T02:25:00.000Z",
        "voteCount": 3,
        "content": "Option D is not as efficient because using Google Data Studio for time plots may not be as well-suited for handling large datasets, and it's more focused on data visualization. Option A involves importing data into Vertex AI Workbench first, which may not be the most efficient way to leverage BigQuery for handling large-scale data computations."
      },
      {
        "date": "2023-09-11T21:36:00.000Z",
        "voteCount": 3,
        "content": "I would go with D, thinking that Bigquery + Datastudio avoid having to load 100s of MILLIONS of records in memory for the most basic tasks, as required by the Notebook."
      },
      {
        "date": "2023-06-22T11:42:00.000Z",
        "voteCount": 3,
        "content": "D minimizes resources the most, since it minimizes the usage of Vertex AI notebooks, which basically require provisioning a VM in the background for the entire duration of development."
      },
      {
        "date": "2023-06-14T13:17:00.000Z",
        "voteCount": 4,
        "content": "Why not D ?"
      },
      {
        "date": "2023-07-06T10:45:00.000Z",
        "voteCount": 1,
        "content": "Using BigQuery to calculate the descriptive statistics is a good choice, but using Google Data Studio for visualizations may not be as flexible as using Vertex AI Workbench user-managed notebooks. Google Data Studio is a great tool for creating dashboards and reports, but it may not allow for the level of customization that is required for detailed exploratory data analysis."
      },
      {
        "date": "2023-05-10T09:57:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/architecture/data-science-with-r-on-gcp-eda#ai_platform_notebooks\nhttps://cloud.google.com/vertex-ai-workbench#section-5"
      },
      {
        "date": "2023-04-24T06:59:00.000Z",
        "voteCount": 1,
        "content": "I think the key here is that it says the dataset would be imported into the notebook for B, therefore no longer utilising BigQuery for calculating the descriptive stats, otherwise I would pick B. Therefore I think C is better. Can anyone find any documentation where Google gives best practice on this? It seems quite subjective"
      },
      {
        "date": "2023-03-07T05:12:00.000Z",
        "voteCount": 3,
        "content": "C. Use BigQuery to calculate the descriptive statistics. Use Vertex AI Workbench user-managed notebooks to visualize the time plots and run the statistical analyses.\n\nBigQuery is a powerful data analysis tool that can handle massive datasets, making it an ideal solution for calculating descriptive statistics for hundreds of millions of records. It can also perform complex statistical tests for hypothesis testing. For time series analysis, using Vertex AI Workbench user-managed notebooks would be the best solution as it provides a flexible environment for data exploration, visualization, and statistical analysis. By using the two tools together, the data science team can efficiently analyze the sales data while minimizing computational resources.\nIts C not B"
      },
      {
        "date": "2023-04-27T11:02:00.000Z",
        "voteCount": 7,
        "content": "What is the point of actually giving chatGPT answers, some of which are incorrect?"
      },
      {
        "date": "2023-03-07T02:38:00.000Z",
        "voteCount": 1,
        "content": "Answer B\nhttps://cloud.google.com/vertex-ai-workbench"
      },
      {
        "date": "2023-03-09T11:47:00.000Z",
        "voteCount": 1,
        "content": "Answer C not B"
      },
      {
        "date": "2023-02-03T21:06:00.000Z",
        "voteCount": 3,
        "content": "Vote B. You can do all of the task in vertex AI workbench while minimizing computational resources."
      },
      {
        "date": "2023-07-06T10:43:00.000Z",
        "voteCount": 1,
        "content": "Option B is also a viable solution, but it has some drawbacks compared to option C. While it is true that you can spin up a Vertex AI Workbench user-managed notebooks instance and import the dataset, this option requires more computational resources and may not be as cost-effective as using BigQuery to calculate the descriptive statistics. Additionally, while you can create statistical and visual analyses within the Vertex AI Workbench user-managed notebooks, it may not be as easy to create custom visualizations as it is with Google Data Studio.\n\nTherefore, while option B is a valid solution, option C is likely to be more efficient and cost-effective, as it takes advantage of the strengths of each tool."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 147,
    "url": "https://www.examtopics.com/discussions/google/view/98088-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "Your data science team needs to rapidly experiment with various features, model architectures, and hyperparameters. They need to track the accuracy metrics for various experiments and use an API to query the metrics over time. What should they use to track and report their experiments while minimizing manual effort?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Vertex Al Pipelines to execute the experiments. Query the results stored in MetadataStore using the Vertex Al API.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Vertex Al Training to execute the experiments. Write the accuracy metrics to BigQuery, and query the results using the BigQuery API.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Vertex Al Training to execute the experiments. Write the accuracy metrics to Cloud Monitoring, and query the results using the Monitoring API.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Vertex Al Workbench user-managed notebooks to execute the experiments. Collect the results in a shared Google Sheets file, and query the results using the Google Sheets API."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-22T17:53:00.000Z",
        "voteCount": 2,
        "content": "Vertex AI Pipelines: Pipelines are designed for automating experiment execution. You can define different steps like data preprocessing, training with various configurations, and evaluation. This allows rapid experimentation with minimal manual intervention.\nVertex ML Metadata: Vertex AI Pipelines integrate seamlessly with Vertex ML Metadata, which automatically tracks experiment runs, metrics, and artifacts. This eliminates the need for manual data collection in spreadsheets.\nVertex AI API: The Vertex AI API allows you to programmatically query the Vertex ML Metadata store. You can retrieve experiment details, including accuracy metrics, for further analysis or visualization."
      },
      {
        "date": "2024-04-22T17:53:00.000Z",
        "voteCount": 1,
        "content": "B. BigQuery and Monitoring are not designed for experiment tracking: BigQuery excels at large-scale data analysis, and Cloud Monitoring is primarily for monitoring system health. While you could store metrics, querying them for experiment comparisons would be cumbersome.\nC. Manual collection in Google Sheets: This approach is highly error-prone and inefficient for rapid experimentation. Version control and querying metrics across multiple experiments would be challenging.\nD. Vertex AI Training (standalone): While Vertex AI Training can run experiments, it lacks built-in experiment tracking and querying functionalities.\n You'd need to develop custom solutions for managing metrics."
      },
      {
        "date": "2023-05-10T09:58:00.000Z",
        "voteCount": 2,
        "content": "Went with A"
      },
      {
        "date": "2023-03-09T11:46:00.000Z",
        "voteCount": 1,
        "content": "Option A is the best approach to track and report experiments while minimizing manual effort. The Vertex AI Pipelines provide a powerful tool for automating machine learning workflows, including data preparation, training, and deployment. MetadataStore can be used to track the performance of different models by logging accuracy metrics and other important information. The Vertex AI API can then be used to query the metadata store and retrieve the results of different experiments."
      },
      {
        "date": "2023-02-26T11:06:00.000Z",
        "voteCount": 1,
        "content": "Vertex AI Pipelines covers everything.\n\n\"Vertex AI Pipelines helps you to automate, monitor, and govern your ML systems by orchestrating your ML workflow in a serverless manner, and storing your workflow's artifacts using Vertex ML Metadata. By storing the artifacts of your ML workflow in Vertex ML Metadata, you can analyze the lineage of your workflow's artifacts \u2014 for example, an ML model's lineage may include the training data, hyperparameters, and code that were used to create the model.\""
      },
      {
        "date": "2023-02-21T23:38:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/vertex-ai/docs/ml-metadata/analyzing"
      },
      {
        "date": "2023-02-16T03:53:00.000Z",
        "voteCount": 1,
        "content": "Your goal is to use API to query results while minimizing manual effort. The answer 'A' achieves the goal and requires less manual effort"
      },
      {
        "date": "2023-02-05T07:39:00.000Z",
        "voteCount": 1,
        "content": "its A - Use Vertex Al Pipelines to execute the experiments. Query the results stored in MetadataStore using the Vertex Al API."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 148,
    "url": "https://www.examtopics.com/discussions/google/view/97897-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You are training an ML model using data stored in BigQuery that contains several values that are considered Personally Identifiable Information (PII). You need to reduce the sensitivity of the dataset before training your model. Every column is critical to your model. How should you proceed?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUsing Dataflow, ingest the columns with sensitive data from BigQuery, and then randomize the values in each sensitive column.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Cloud Data Loss Prevention (DLP) API to scan for sensitive data, and use Dataflow with the DLP API to encrypt sensitive values with Format Preserving Encryption.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Cloud Data Loss Prevention (DLP) API to scan for sensitive data, and use Dataflow to replace all sensitive data by using the encryption algorithm AES-256 with a salt.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBefore training, use BigQuery to select only the columns that do not contain sensitive data. Create an authorized view of the data so that sensitive values cannot be accessed by unauthorized individuals."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 9,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-04-28T05:51:00.000Z",
        "voteCount": 3,
        "content": "Not A: Randomizing values alters the data in a way that it can significantly degrade the utility of the data for machine learning purposes (does not preserve original distributions, ...)\nNot C: Encryption with AES-256 secures the data, but does not preserve the format and would make the data unusable for ML models \nNot D: This ignores columns with sensitive data, which is not viable here as every column is critical to the model. + Creating an authorized view does not alter the data itself but restricts access, which does not address the need to reduce data sensitivity for model training"
      },
      {
        "date": "2023-05-10T09:58:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/dlp/docs/transformations-reference#types_of_de-identification_techniques\nhttps://cloud.google.com/dlp/docs/transformations-reference#crypto"
      },
      {
        "date": "2023-02-22T03:47:00.000Z",
        "voteCount": 1,
        "content": "Answer B"
      },
      {
        "date": "2023-03-09T11:45:00.000Z",
        "voteCount": 1,
        "content": "model. The Cloud Data Loss Prevention (DLP) API can scan for sensitive data in the dataset and can help to encrypt the sensitive data using Format Preserving Encryption. This approach will allow for the preservation of the data distribution and format, enabling the model to maintain its accuracy. Additionally, using Dataflow with the DLP API can help to efficiently process the data at scale."
      },
      {
        "date": "2023-02-16T08:40:00.000Z",
        "voteCount": 3,
        "content": "Format Preserving Encryption uses deidentify configuration in which you can specify the param wrapped_key (the encrypted ('wrapped') AES-256 key to use).\nAnswer is B according to me.\nRef: https://cloud.google.com/dlp/docs/samples/dlp-deidentify-fpe"
      },
      {
        "date": "2023-02-16T01:47:00.000Z",
        "voteCount": 2,
        "content": "This approach would allow you to keep the critical columns of data while reducing the sensitivity of the dataset by removing the personally identifiable information (PII) before training the model. By creating an authorized view of the data, you can ensure that sensitive values cannot be accessed by unauthorized individuals.\n\nhttps://cloud.google.com/bigquery/docs/data-governance#data_loss_prevention"
      },
      {
        "date": "2023-02-22T03:47:00.000Z",
        "voteCount": 1,
        "content": "Actually its B"
      },
      {
        "date": "2023-02-21T02:13:00.000Z",
        "voteCount": 2,
        "content": "It says \"every\" column is critical to your model, why would select specific columns?"
      },
      {
        "date": "2023-02-21T22:50:00.000Z",
        "voteCount": 1,
        "content": "Hence i provided a link, that should answer your flimsy question. it says \"BigQuery that contains several values that are considered Personally Identifiable Information (PII)\"  i dnt know where you are getting it wrong. the \"every\" means you cant leave out sensitive data to train your model because every column is critical. its not difficult its easy bro...."
      },
      {
        "date": "2023-02-05T07:35:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/dlp/docs/samples/dlp-deidentify-fpe"
      },
      {
        "date": "2023-02-03T21:14:00.000Z",
        "voteCount": 1,
        "content": "Vote C. https://cloud.google.com/dlp/docs/samples/dlp-deidentify-fpe"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 149,
    "url": "https://www.examtopics.com/discussions/google/view/99253-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You recently deployed an ML model. Three months after deployment, you notice that your model is underperforming on certain subgroups, thus potentially leading to biased results. You suspect that the inequitable performance is due to class imbalances in the training data, but you cannot collect more data. What should you do? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRemove training examples of high-performing subgroups, and retrain the model.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd an additional objective to penalize the model more for errors made on the minority class, and retrain the model\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRemove the features that have the highest correlations with the majority class.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpsample or reweight your existing training data, and retrain the model\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRedeploy the model, and provide a label explaining the model's behavior to users."
    ],
    "answer": "BD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BD",
        "count": 12,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-22T17:40:00.000Z",
        "voteCount": 2,
        "content": "Penalizing Errors on Minority Class (B): This technique, also known as cost-sensitive learning, modifies the loss function during training. Assigning a higher penalty to misclassifications of the minority class steers the model to prioritize learning from those examples.\n\nUpsampling/Reweighting Training Data (D):\n\nUpsampling increases the representation of the minority class in the training data by duplicating existing data points.\nReweighting assigns higher weights to data points from the minority class during training, making their influence more significant."
      },
      {
        "date": "2024-04-22T17:41:00.000Z",
        "voteCount": 2,
        "content": "A. Removing High-Performing Subgroup Examples: This removes valuable data and can worsen overall model performance.\nC. Removing High-Correlation Features: This might eliminate informative features and could negatively impact model accuracy.\nE. Redeploying with Explanation: While transparency is essential, it doesn't address the underlying performance disparity"
      },
      {
        "date": "2024-02-28T15:33:00.000Z",
        "voteCount": 1,
        "content": "I went B &amp; D."
      },
      {
        "date": "2023-07-20T07:12:00.000Z",
        "voteCount": 1,
        "content": "D. Upsample or reweight your existing training data, and retrain the model.\n\nE. Redeploy the model, and provide a label explaining the model's behavior to users.\n\nOption D: Upsampling or reweighting your existing training data and retraining the model can help address the class imbalance issue and improve the performance on certain subgroups. By duplicating or adjusting the weights of samples from the minority class, the model will receive more exposure to these samples during training, leading to better learning and performance on the underrepresented subgroups.\n\nOption E: Redeploying the model and providing a label explaining the model's behavior to users is essential for transparency and accountability. If the model exhibits biased behavior or inequitable performance on certain subgroups, informing users about this issue can help them interpret the model's predictions more effectively and make informed decisions based on the model's output."
      },
      {
        "date": "2023-05-10T09:59:00.000Z",
        "voteCount": 2,
        "content": "Went with B, D"
      },
      {
        "date": "2023-03-08T05:47:00.000Z",
        "voteCount": 2,
        "content": "should be B,D"
      },
      {
        "date": "2023-03-07T05:09:00.000Z",
        "voteCount": 4,
        "content": "Option B and D could be good approaches to address the issue.\n\nB. Adding an additional objective to penalize the model more for errors made on the minority class can help the model to focus more on correctly classifying the underrepresented class.\n\nD. Upsampling or reweighting the existing training data can help balance the class distribution and increase the model's sensitivity to the underrepresented class."
      },
      {
        "date": "2023-02-16T01:21:00.000Z",
        "voteCount": 2,
        "content": "https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/"
      },
      {
        "date": "2023-02-21T22:46:00.000Z",
        "voteCount": 2,
        "content": "https://www.analyticsvidhya.com/blog/2020/07/10-techniques-to-deal-with-class-imbalance-in-machine-learning/"
      },
      {
        "date": "2023-02-14T23:48:00.000Z",
        "voteCount": 1,
        "content": "https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 150,
    "url": "https://www.examtopics.com/discussions/google/view/99380-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You are working on a binary classification ML algorithm that detects whether an image of a classified scanned document contains a company\u2019s logo. In the dataset, 96% of examples don\u2019t have the logo, so the dataset is very skewed. Which metric would give you the most confidence in your model?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPrecision",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRecall",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRMSE",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tF1 score\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 15,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-04-22T17:43:00.000Z",
        "voteCount": 7,
        "content": "Precision vs. Recall:\nPrecision focuses on the percentage of predicted positive cases (logo present) that are actually correct.\nRecall emphasizes the model's ability to identify all actual positive cases (correctly identifying all logos).\nIn a highly imbalanced dataset, a naive model could simply predict \"no logo\" for every image and achieve very high accuracy (almost 96%!). However, this wouldn't be a useful model since it would miss all the actual logos (low recall).\n\nF1 Score: The F1 score strikes a balance between precision and recall. It takes the harmonic mean of these two metrics, giving a more comprehensive picture of the model's performance in both identifying logos (recall) and avoiding false positives (precision)."
      },
      {
        "date": "2024-05-20T21:16:00.000Z",
        "voteCount": 1,
        "content": "very well explained!"
      },
      {
        "date": "2023-07-20T07:21:00.000Z",
        "voteCount": 7,
        "content": "B. Recall\n\nIn a highly imbalanced dataset like the one described (96% of examples are in the negative class), the metric that would give the most confidence in the model's performance is recall.\n\nRecall (also known as sensitivity or true positive rate) is the proportion of actual positive cases that were correctly identified by the model. In this context, it means the percentage of images containing the company's logo that the model correctly classified as positive out of all the actual positive cases. Since the dataset is heavily skewed, a high recall value would indicate that the model is effectively capturing the positive cases (images with the logo) despite the class imbalance.\nF1 score (D) is a balance between precision and recall and is a useful metric for imbalanced datasets. However, in this specific case, recall is more important because we want to be confident in detecting the logo images, even if it comes at the cost of having some false positives (lower precision)."
      },
      {
        "date": "2023-12-14T09:09:00.000Z",
        "voteCount": 1,
        "content": "I go for B as well"
      },
      {
        "date": "2024-04-15T02:39:00.000Z",
        "voteCount": 1,
        "content": "Went with D"
      },
      {
        "date": "2024-04-01T00:26:00.000Z",
        "voteCount": 3,
        "content": "B\nSee #90, should be F score with Recall weights more than Precision."
      },
      {
        "date": "2024-03-13T08:23:00.000Z",
        "voteCount": 1,
        "content": "I went with B."
      },
      {
        "date": "2024-02-10T15:41:00.000Z",
        "voteCount": 2,
        "content": "F1 score provides a comprehensive evaluation by penalizing models that excel in just one aspect at the expense of the other. By considering both precision and recall, it helps identify models that effectively balance true positive identification with minimal false positives, making it a more suitable metric for imbalanced data like your logo detection problem."
      },
      {
        "date": "2023-05-10T09:59:00.000Z",
        "voteCount": 2,
        "content": "See #90!"
      },
      {
        "date": "2023-02-23T14:42:00.000Z",
        "voteCount": 1,
        "content": "F1 score works well for imbalanced data sets"
      },
      {
        "date": "2023-02-16T01:10:00.000Z",
        "voteCount": 2,
        "content": "https://stephenallwright.com/imbalanced-data-metric/"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 151,
    "url": "https://www.examtopics.com/discussions/google/view/98090-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "While running a model training pipeline on Vertex Al, you discover that the evaluation step is failing because of an out-of-memory error. You are currently using TensorFlow Model Analysis (TFMA) with a standard Evaluator TensorFlow Extended (TFX) pipeline component for the evaluation step. You want to stabilize the pipeline without downgrading the evaluation quality while minimizing infrastructure overhead. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInclude the flag -runner=DataflowRunner in beam_pipeline_args to run the evaluation step on Dataflow.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMove the evaluation step out of your pipeline and run it on custom Compute Engine VMs with sufficient memory.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate your pipeline to Kubeflow hosted on Google Kubernetes Engine, and specify the appropriate node parameters for the evaluation step.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd tfma.MetricsSpec () to limit the number of metrics in the evaluation step."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 23,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 8,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-10-29T15:11:00.000Z",
        "voteCount": 6,
        "content": "\"Evaluator leverages the TensorFlow Model Analysis library to perform the analysis, which in turn use Apache Beam for scalable processing.\" Since Dataflow is Google Cloud's serverless Apache Beam offering, this option can easily be implemented to address the issue while leaving the evaluation logic as such identical\n\nhttps://www.tensorflow.org/tfx/guide/evaluator#evaluator_and_tensorflow_model_analysis"
      },
      {
        "date": "2023-11-17T10:59:00.000Z",
        "voteCount": 1,
        "content": "If we have to add dataflow then this condition is not met: minimizing infrastructure overhead"
      },
      {
        "date": "2024-03-21T11:36:00.000Z",
        "voteCount": 2,
        "content": "No, it is. If we choose another option, there would be:\nB - you need to configure VMs and migrate all workloads\nC - also overhead with migrating \nD - downgrading the evaluation quality\nSo just switch runner seems a very easy option"
      },
      {
        "date": "2023-05-10T10:01:00.000Z",
        "voteCount": 5,
        "content": "Links already provided below:\n\u201cThat works fine for one hundred records, but what if the goal was to process all 187,002,0025 rows in the dataset? For this, the pipeline is switched from the DirectRunner to the production Dataflow runner.\u201d [Option A] https://blog.tensorflow.org/2020/03/tensorflow-extended-tfx-using-apache-beam-large-scale-data-processing.html.\n\"Metrics to configure (only required if additional metrics are being added outside of those saved with the model).\u201d https://www.tensorflow.org/tfx/guide/evaluator#using_the_evaluator_component\nwill thus add, not \u201climit the number of metrics in the evaluation step\u201d. [Option D]"
      },
      {
        "date": "2024-04-21T03:27:00.000Z",
        "voteCount": 2,
        "content": "with D we're downgrading evaluation. Dataflow is serverless so no infrastructure overhead is included"
      },
      {
        "date": "2023-11-17T10:57:00.000Z",
        "voteCount": 2,
        "content": "Limiting Metrics: TensorFlow Model Analysis (TFMA) allows you to define a subset of metrics that you are interested in during the evaluation step. By using tfma.MetricsSpec(), you can specify a subset of metrics to be computed during the evaluation, which can help reduce the memory requirements.\n\nOut-of-Memory Error: Out-of-memory errors during model evaluation often occur when the system is trying to compute and store a large number of metrics, especially if the model or dataset is large. By limiting the number of metrics using tfma.MetricsSpec(), you can potentially reduce the memory footprint and resolve the out-of-memory error."
      },
      {
        "date": "2023-07-20T07:40:00.000Z",
        "voteCount": 1,
        "content": "Based on the question's context, the correct option to stabilize the pipeline without downgrading the evaluation quality while minimizing infrastructure overhead is:\n\nD. Add tfma.MetricsSpec() to limit the number of metrics in the evaluation step.\n\nThe question specifies that the evaluation step is failing due to an out-of-memory error. In such a scenario, limiting the number of metrics to be computed during evaluation using tfma.MetricsSpec() can help reduce memory requirements and potentially resolve the out-of-memory issue."
      },
      {
        "date": "2023-07-06T10:28:00.000Z",
        "voteCount": 1,
        "content": "By adding tfma.MetricsSpec(), you can limit the number of metrics that are computed during the evaluation step, thus reducing the memory requirement. This will help stabilize the pipeline without downgrading the evaluation quality, while minimizing infrastructure overhead. This option is a quick and easy solution that can be implemented without significant changes to the pipeline or infrastructure.\n\nOption A: Including the flag -runner=DataflowRunner in beam_pipeline_args to run the evaluation step on Dataflow may help to increase memory availability, but it may also increase infrastructure overhead."
      },
      {
        "date": "2023-07-26T11:16:00.000Z",
        "voteCount": 1,
        "content": "it seems, Option D might reduce memory usage, it could potentially compromise the evaluation quality by not considering all the necessary metrics. Confused in A/D!"
      },
      {
        "date": "2023-04-27T11:50:00.000Z",
        "voteCount": 1,
        "content": "D does not harm the evaluation quality."
      },
      {
        "date": "2023-04-24T09:16:00.000Z",
        "voteCount": 2,
        "content": "Surely removing evaluation metrics downgrades the quality of the evaluation"
      },
      {
        "date": "2023-04-23T11:26:00.000Z",
        "voteCount": 2,
        "content": "I'm not very sure, but wouldn't be A?.D is degrading evaluation quality (if you're getting less metrics, then the evaluation is worse, at least less complete)"
      },
      {
        "date": "2023-03-19T23:24:00.000Z",
        "voteCount": 2,
        "content": "TFX 0.30 and above adds an interface, with_beam_pipeline_args, for extending the pipeline level beam args per component\n\ntfma.MetricSpec() OOB has recommended metrics; reducing any further might not serve the purpose."
      },
      {
        "date": "2023-03-07T05:06:00.000Z",
        "voteCount": 4,
        "content": "Add tfma.MetricsSpec () to limit the number of metrics in the evaluation step.\n\nLimiting the number of metrics in the evaluation step using tfma.MetricsSpec() can reduce the memory usage during evaluation and address the out-of-memory error. This can help stabilize the pipeline without downgrading the evaluation quality or incurring additional infrastructure overhead. Running the evaluation step on Dataflow or custom Compute Engine VMs can be resource-intensive and expensive, while migrating the pipeline to Kubeflow would require additional setup and configuration.\n\n\n\nANSWER D"
      },
      {
        "date": "2023-03-04T11:13:00.000Z",
        "voteCount": 2,
        "content": "A is wrong , it does not even make sense , the default runner for evaluator component of TFX is data flow so setting runner to dataflow does not change anything  , the answer is D because it does not include the any infrastructure minpulation and reduce the memory useable of the TfX component"
      },
      {
        "date": "2023-03-06T21:06:00.000Z",
        "voteCount": 1,
        "content": "https://www.tensorflow.org/tfx/guide/evaluator"
      },
      {
        "date": "2023-02-16T01:33:00.000Z",
        "voteCount": 2,
        "content": "Answer A"
      },
      {
        "date": "2023-03-07T05:07:00.000Z",
        "voteCount": 1,
        "content": "Answer D"
      },
      {
        "date": "2023-02-05T08:15:00.000Z",
        "voteCount": 4,
        "content": "https://blog.tensorflow.org/2020/03/tensorflow-extended-tfx-using-apache-beam-large-scale-data-processing.html"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 152,
    "url": "https://www.examtopics.com/discussions/google/view/99379-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You are developing an ML model using a dataset with categorical input variables. You have randomly split half of the data into training and test sets. After applying one-hot encoding on the categorical variables in the training set, you discover that one categorical variable is missing from the test set. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse sparse representation in the test set.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRandomly redistribute the data, with 70% for the training set and 30% for the test set",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tApply one-hot encoding on the categorical variables in the test data\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCollect more data representing all categories"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 13,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-09-10T08:11:00.000Z",
        "voteCount": 1,
        "content": "I've very grudgingly ticket C, as the question is missing \"handle the missing category by one hot encoding all zeros for the missing feature column\". It otherwise doesn't make sense as will have the wrong amount of entries."
      },
      {
        "date": "2024-04-21T20:01:00.000Z",
        "voteCount": 2,
        "content": "The correct approach is to handle the missing category during one-hot encoding of the test data. Here's how to address this issue:\n\nIdentify the Missing Category: After applying one-hot encoding to the training set, compare the categories (unique values) present in the training data with the categories in the test data. This will reveal the missing category.\n\nAdd a Column for the Missing Category in the Test Data: Include a new column in the test data specifically for the missing category. Initialize the values in this column with 0.\n\nApply One-Hot Encoding to the Test Data: Now that the test data includes a column for the missing category, proceed with one-hot encoding the categorical variables in the test data. This will ensure the test data has the same structure as the encoded training data."
      },
      {
        "date": "2024-09-10T08:07:00.000Z",
        "voteCount": 1,
        "content": "But your description includes a missing critical step that the question is missing to make it make sense."
      },
      {
        "date": "2024-02-08T10:07:00.000Z",
        "voteCount": 1,
        "content": "Answer C"
      },
      {
        "date": "2023-08-22T23:05:00.000Z",
        "voteCount": 3,
        "content": "Answer options analysis:\n\nC. Since one categorical variable is missing from the test set, (As I understand: \u201ca categorical variable is in the test but not in train\u201d) apply one hot encoding (trained with the train set?) to the test set, for the variables not present in train we just would obtain an array of all 0\u2019s, so that would be OK.\nD. That data collection could be not feasible depending on the real-world-problem.\nB. Randomness would not always fix the problem.\nA. Not recommended to use different representations for train/test. Sparse representation doesn't magically recover missing categories; it's a way to efficiently store data with a large number of zeros.\n\nI would go with C."
      },
      {
        "date": "2023-07-08T01:04:00.000Z",
        "voteCount": 1,
        "content": "C but not really sure"
      },
      {
        "date": "2023-05-25T05:38:00.000Z",
        "voteCount": 2,
        "content": "You must apply one hot enconding alsto for the test dataset. However, i find this answer incomplete."
      },
      {
        "date": "2024-09-10T08:08:00.000Z",
        "voteCount": 1,
        "content": "Yeah 100% - it's missing the \"but make sure it deals with the missing category by adding a \"missing\" or something to it so the one hot representation has the right number of items."
      },
      {
        "date": "2023-05-24T03:27:00.000Z",
        "voteCount": 2,
        "content": "Add data to the test set to get the same OHE"
      },
      {
        "date": "2023-07-06T10:16:00.000Z",
        "voteCount": 2,
        "content": "Option D (collecting more data) may not be feasible or necessary if the missing category is not significant or if one-hot encoding is sufficient to handle it."
      },
      {
        "date": "2023-05-10T10:01:00.000Z",
        "voteCount": 2,
        "content": "\u201cRows are selected for a data split randomly, but deterministically. (\u2026) Training a new model with the same training data results in the same data split.\u201d https://cloud.google.com/vertex-ai/docs/tabular-data/data-splits#classification-random. \u201cRandomly redistribute data\u201d [Option B] with different fractions, will result in a different data split. Having a higher fraction split of 70% for the training set will additionally help the model to better generalize (compared to only 50%), thus perform better when testing, the ultimate goal."
      },
      {
        "date": "2023-10-26T07:43:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/vertex-ai/docs/tabular-data/data-splits#classification-random\nI think it's applicable to VertexAI auto ML only"
      },
      {
        "date": "2023-05-10T10:02:00.000Z",
        "voteCount": 1,
        "content": "Sparse representation is one \u201cin which only nonzero values are stored\u201d, excluding [Option A]: https://developers.google.com/machine-learning/crash-course/representation/feature-engineering#sparse-representation.\nApplying \u201cone-hot encoding\u201d to the columns will not help finding the missing column, thus excluding [Option C]. No indication provided for a need to \u201ccollect more data\u201d, excluding [Option D]."
      },
      {
        "date": "2023-05-25T00:56:00.000Z",
        "voteCount": 2,
        "content": "it is possible that category is very rare and that is the reason we don't have it in the test. So I guess we should just apply the train data transformations and use one-hot"
      },
      {
        "date": "2023-04-27T12:09:00.000Z",
        "voteCount": 2,
        "content": "By using a sparse representation, you will be losing the information contained in the missing categorical variable. This could lead to the model making incorrect predictions on the test set."
      },
      {
        "date": "2023-04-25T23:09:00.000Z",
        "voteCount": 1,
        "content": "I agree with formazioneQl that if a different one hot encoding is used for the test set compared to the train set then the results would be poor. However, there is no problem with not having all combinations in  the test set if all possibilities are present in the training set. So assuming that we are using the same mapping of data in the train and test set, I would vote C. If we don't encode the test set, the variable is meaningless anyways. So I would lean C."
      },
      {
        "date": "2023-04-18T08:05:00.000Z",
        "voteCount": 3,
        "content": "Since one categorical variable is missing from the test set, C would result in a different number of columns in the training and test sets."
      },
      {
        "date": "2023-07-06T10:17:00.000Z",
        "voteCount": 1,
        "content": "Option A (sparse representation) may not work well in this case, as it can lead to sparsity issues and affect the model's performance."
      },
      {
        "date": "2023-03-07T05:05:00.000Z",
        "voteCount": 2,
        "content": "C. Apply one-hot encoding on the categorical variables in the test data.\n\nWhen using one-hot encoding on categorical variables, each unique value of the variable is represented as a separate binary variable. Therefore, it is important to ensure that the same set of binary variables is present in both the training and test datasets. Since one categorical variable is missing in the test set, the recommended approach is to apply one-hot encoding on the categorical variables in the test set to ensure that the same set of binary variables is present in both datasets."
      },
      {
        "date": "2023-02-16T01:00:00.000Z",
        "voteCount": 1,
        "content": "Answer C"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 153,
    "url": "https://www.examtopics.com/discussions/google/view/98540-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You work for a bank and are building a random forest model for fraud detection. You have a dataset that includes transactions, of which 1% are identified as fraudulent. Which data transformation strategy would likely improve the performance of your classifier?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the target variable using the Box-Cox transformation.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tZ-normalize all the numeric features.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOversample the fraudulent transaction 10 times.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLog transform all numeric features."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-21T20:03:00.000Z",
        "voteCount": 3,
        "content": "Oversampling is a common technique to address class imbalance and can significantly improve the performance of the random forest model in fraud detection. It's important to note that oversampling can lead to overfitting, so monitoring the model's performance on unseen data (validation set) is crucial. You might also consider exploring other techniques like undersampling the majority class or using SMOTE (Synthetic Minority Oversampling Technique) for a more balanced approach."
      },
      {
        "date": "2024-04-21T20:03:00.000Z",
        "voteCount": 1,
        "content": "Class Imbalance: The dataset has a significant class imbalance, with only 1% of transactions being fraudulent (minority class). Random forest models can be biased towards the majority class during training.\n\nOversampling: Oversampling replicates instances from the minority class (fraudulent transactions) in this case. By increasing the representation of the fraudulent class (10 times in this scenario), the model is exposed to more examples of fraud, improving its ability to learn and detect fraudulent patterns."
      },
      {
        "date": "2024-04-07T04:36:00.000Z",
        "voteCount": 1,
        "content": "See #60!"
      },
      {
        "date": "2023-05-10T10:03:00.000Z",
        "voteCount": 2,
        "content": "See #60!\nThe End. Good luck everyone!!!"
      },
      {
        "date": "2023-02-16T06:47:00.000Z",
        "voteCount": 4,
        "content": "The answer is C beacause it's the only way to improve model performance.\nBox-Cox transformation: transform feature values according to normal distribution\nZ-normalization: transform feature values according to x_new = (x \u2013 \u03bc) / \u03c3 (so {x_new} have mean 0 and std dev 1)\nLog transform: just log transformation \nAlso, the Random Forest algorithm is not a distance-based model but it is a tree-based model, there's no need of normalization process."
      },
      {
        "date": "2023-02-09T04:48:00.000Z",
        "voteCount": 1,
        "content": "https://towardsdatascience.com/how-to-build-a-machine-learning-model-to-identify-credit-card-fraud-in-5-stepsa-hands-on-modeling-5140b3bd19f1"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 154,
    "url": "https://www.examtopics.com/discussions/google/view/109098-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You are developing a classification model to support predictions for your company\u2019s various products. The dataset you were given for model development has class imbalance You need to minimize false positives and false negatives What evaluation metric should you use to properly train the model?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tF1 score\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRecall",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAccuracy",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPrecision"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-05-12T11:48:00.000Z",
        "voteCount": 7,
        "content": "if there wasn't a class imbalance that C. Accuracy would have been the right answer. There A. F1-score which is harmonic mean of precision and recall, that balances the trade-off between precision and recall. It is useful when both false positives and false negatives are important as per the question at hand, and you want to optimize for both."
      },
      {
        "date": "2024-06-21T09:24:00.000Z",
        "voteCount": 1,
        "content": "In this case, you want to minimize both false positives and false negatives. The F1 score takes into account both the number of true positives and true negatives, making it a suitable choice for evaluating your model."
      },
      {
        "date": "2024-04-21T20:06:00.000Z",
        "voteCount": 3,
        "content": "Class Imbalance: When dealing with imbalanced data, metrics like accuracy can be misleading. A model that simply predicts the majority class all the time can achieve high accuracy, but it wouldn't be very useful for identifying the minority class (which is likely more important in this scenario).\n\nF1 Score: The F1 score is the harmonic mean of precision and recall. Precision measures the proportion of positive predictions that are actually correct, while recall measures the proportion of actual positive cases that are correctly identified. By considering both metrics, F1 score provides a balanced view of the model's performance in identifying both positive and negative cases.\n\nMinimizing False Positives and False Negatives:  Since a high F1 score indicates a good balance between precision and recall, it translates to minimizing both false positives (incorrect positive predictions) and false negatives (missed positive cases)."
      },
      {
        "date": "2023-07-20T07:59:00.000Z",
        "voteCount": 1,
        "content": "Recall (True Positive Rate): It measures the ability of the model to correctly identify all positive instances out of the total actual positive instances. High recall means fewer false negatives, which is desired when minimizing the risk of missing important positive cases.\n\nF1 Score: It is the harmonic mean of precision and recall. F1 score gives equal weight to both precision and recall and is suitable when you want a balanced metric. However, it might not be the best choice when the primary focus is on minimizing false positives and false negatives."
      },
      {
        "date": "2023-07-20T07:57:00.000Z",
        "voteCount": 1,
        "content": "both recall and F1 score are valuable metrics, but based on the question's specific requirement to minimize false positives and false negatives, recall (Option B) is the best answer. It directly focuses on reducing false negatives, which is crucial when dealing with class imbalance and minimizing the risk of missing important positive cases."
      },
      {
        "date": "2023-07-08T01:00:00.000Z",
        "voteCount": 1,
        "content": "F1 should be correct"
      },
      {
        "date": "2023-05-24T03:32:00.000Z",
        "voteCount": 1,
        "content": "class imbalance = F1 score"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 155,
    "url": "https://www.examtopics.com/discussions/google/view/115038-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You are training an object detection machine learning model on a dataset that consists of three million X-ray images, each roughly 2 GB in size. You are using Vertex AI Training to run a custom training application on a Compute Engine instance with 32-cores, 128 GB of RAM, and 1 NVIDIA P100 GPU. You notice that model training is taking a very long time. You want to decrease training time without sacrificing model performance. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the instance memory to 512 GB, and increase the batch size.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReplace the NVIDIA P100 GPU with a K80 GPU in the training job.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable early stopping in your Vertex AI Training job.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the tf.distribute.Strategy API and run a distributed training job.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-04-21T20:08:00.000Z",
        "voteCount": 5,
        "content": "Large Dataset: With millions of images, training on a single machine can be very slow. Distributed training allows you to split the training data and workload across multiple machines, significantly speeding up the process.\n\nVertex AI Training and tf.distribute: Vertex AI Training supports TensorFlow, and the tf.distribute library provides tools for implementing distributed training strategies. By leveraging this functionality, you can efficiently distribute the training tasks across the available cores and GPU on your Compute Engine instance (32 cores and 1 NVIDIA P100 GPU)."
      },
      {
        "date": "2024-09-10T08:16:00.000Z",
        "voteCount": 1,
        "content": "Some strategies, like tf.distribute.MirroredStrategy, can provide performance optimizations even on a single GPU. For example, it can take advantage of better gradient computation or data parallelism during backpropagation, which can slightly optimize performance."
      },
      {
        "date": "2024-07-03T07:39:00.000Z",
        "voteCount": 1,
        "content": "Same Question as 96?"
      },
      {
        "date": "2024-04-21T10:26:00.000Z",
        "voteCount": 1,
        "content": "https://www.tensorflow.org/guide/distributed_training#onedevicestrategy"
      },
      {
        "date": "2024-02-04T14:39:00.000Z",
        "voteCount": 4,
        "content": "D. Use the tf.distribute.Strategy API and run a distributed training job.\n\nHere's why:\n\nA. Increase instance memory and batch size: This might not be helpful. While increasing memory could help with loading more images at once, the main bottleneck here is likely processing these large images. Increasing the batch size can worsen the problem by further straining the GPU's memory.\nB. Replace P100 with K80 GPU: A weaker GPU would likely slow down training instead of speeding it up.\nC. Enable early stopping: This can save time but might stop training before reaching optimal performance.\nD. Use tf.distribute.Strategy: This allows you to distribute the training workload across multiple GPUs or cores within your instance, significantly accelerating training without changing the model itself. This effectively leverages the available hardware efficiently."
      },
      {
        "date": "2023-08-30T04:50:00.000Z",
        "voteCount": 1,
        "content": "perhaps the fact that the second or more GPUs are created is implied and the answer is D\n https://codelabs.developers.google.com/vertex_multiworker_training#2"
      },
      {
        "date": "2023-07-25T10:01:00.000Z",
        "voteCount": 2,
        "content": "The same comment as in Q96. If we look at our training infrastructure, we can see the bottleneck is obviously the GPU, which has 12GB or 16GB memory depending on the model (https://www.leadtek.com/eng/products/ai_hpc(37)/tesla_p100(761)/detail). This means we can afford to have a batch size of only 6-8 images (2GB each) even if we assume the GPU is utilized 100% and model weights take 0 memory. And remember the training size is 3M, which means each epoch will have 375-500K steps even in this unlikely best case.\n\nWith 32-cores and 128GB memory, we are able to afford higher batch sizes (e.g., 32), so moving to a K80 GPU that has 24GB of memory will accelerate the training.\n\nA is wrong because we can't afford a larger batch size with the current GPU. D is wrong because you don't have multiple GPUs and your current GPU is saturated. C is a viable option, but it seems less optimal than B."
      },
      {
        "date": "2023-07-26T10:59:00.000Z",
        "voteCount": 3,
        "content": "but using the tf.distribute.Strategy API is not limited to multiple GPU configurations. Although the current setup has only one GPU, you can still use the API to distribute the training across multiple Compute Engine instances, each with its own GPU. By running a distributed training job in this manner, you can effectively decrease the training time without sacrificing model performance."
      },
      {
        "date": "2023-11-15T10:19:00.000Z",
        "voteCount": 1,
        "content": "also,  Replacing the NVIDIA P100 GPU with a K80 GPU is not recommended, as the K80 is an older, less powerful GPU compared to the P100. This might actually slow down the training process."
      },
      {
        "date": "2023-08-27T06:03:00.000Z",
        "voteCount": 1,
        "content": "What you say makes sens for the most part except that K80 GPU has only 12GB of DDR5 memory not 24 , https://cloud.google.com/compute/docs/gpus#nvidia_k80_gpus\nSo that leaves me with the only viable option which is C."
      },
      {
        "date": "2023-07-24T22:17:00.000Z",
        "voteCount": 1,
        "content": "https://www.tensorflow.org/guide/gpu \n?"
      },
      {
        "date": "2023-07-27T08:39:00.000Z",
        "voteCount": 1,
        "content": "I was wrong. It's A."
      },
      {
        "date": "2023-07-20T08:01:00.000Z",
        "voteCount": 1,
        "content": "to decrease training time without sacrificing model performance, the best approach is to use the tf.distribute.Strategy API and run a distributed training job, leveraging the capabilities of the available GPU(s) for parallelized training."
      },
      {
        "date": "2023-07-13T04:45:00.000Z",
        "voteCount": 1,
        "content": "A\nsince we just have one gpu, we could not use tf.distribute.Strategy in D"
      },
      {
        "date": "2023-07-13T04:46:00.000Z",
        "voteCount": 1,
        "content": "And C early stopping maybe hurt the performance"
      },
      {
        "date": "2023-11-27T07:52:00.000Z",
        "voteCount": 1,
        "content": "The increased batch size also can hurt the performance if it is not followed by further optimizations with regards to learning rate for example. If early stopping is applied according to common convention, by stopping when the validation loss starts increasing, it should not hurt the performance. However it is not specified in the answer sadly."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 156,
    "url": "https://www.examtopics.com/discussions/google/view/115039-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You need to build classification workflows over several structured datasets currently stored in BigQuery. Because you will be performing the classification several times, you want to complete the following steps without writing code: exploratory data analysis, feature selection, model building, training, and hyperparameter tuning and serving. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTrain a TensorFlow model on Vertex AI.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTrain a classification Vertex AutoML model.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun a logistic regression job on BigQuery ML.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse scikit-learn in Vertex AI Workbench user-managed notebooks with pandas library."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-21T22:39:00.000Z",
        "voteCount": 2,
        "content": "Vertex AutoML Tables is a managed service specifically designed for building machine learning models from structured data in BigQuery, all without writing code. It automates various stages of the machine learning pipeline, including:\n\nExploratory data analysis: AutoML Tables performs basic data understanding to identify potential issues.\nFeature selection: It can automatically select relevant features for model training.\nModel building: AutoML Tables trains and evaluates various machine learning models and chooses the best performing one for classification.\nHyperparameter tuning: It automatically tunes hyperparameters to optimize model performance.\nServing: You can deploy the trained model for making predictions on new data."
      },
      {
        "date": "2024-04-08T04:19:00.000Z",
        "voteCount": 1,
        "content": "B, since it's specifying without writing code"
      },
      {
        "date": "2024-02-28T04:50:00.000Z",
        "voteCount": 1,
        "content": "No writing Code.\nOption B."
      },
      {
        "date": "2024-02-17T08:48:00.000Z",
        "voteCount": 1,
        "content": "AutoML -&gt; No writing code"
      },
      {
        "date": "2024-01-07T01:44:00.000Z",
        "voteCount": 1,
        "content": "B\nWith automl we don\u2019t write any line of code"
      },
      {
        "date": "2023-12-07T09:12:00.000Z",
        "voteCount": 1,
        "content": "B is correct.\nA and D imply writing code in TF and Sklearn respectively.\nC is writing BQML code for logistic regression as well. Furthermore, how would you do the EDA and feature selection etc. without writing code?\nAutoML is THE codeless solution automating all the steps mentioned above. https://cloud.google.com/automl?hl=en"
      },
      {
        "date": "2023-08-23T02:47:00.000Z",
        "voteCount": 1,
        "content": "A and D would require writing code.\nC. would also imply some \u201ccode writing\u201d in BigQuery ML\n\nI would go with B."
      },
      {
        "date": "2023-07-13T04:50:00.000Z",
        "voteCount": 1,
        "content": "B\n\"without writing code\""
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 157,
    "url": "https://www.examtopics.com/discussions/google/view/115867-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You recently developed a deep learning model. To test your new model, you trained it for a few epochs on a large dataset. You observe that the training and validation losses barely changed during the training run. You want to quickly debug your model. What should you do first?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tVerify that your model can obtain a low loss on a small subset of the dataset\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd handcrafted features to inject your domain knowledge into the model",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Vertex AI hyperparameter tuning service to identify a better learning rate",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse hardware accelerators and train your model for more epochs"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-21T22:43:00.000Z",
        "voteCount": 4,
        "content": "Isolating the Issue: Training on a small subset helps isolate the problem to the model itself rather than the entire training pipeline or large dataset.\nEfficiency: Debugging with a small dataset is faster, allowing you to iterate through potential solutions quicker.\nIdentifying Fundamental Issues: If the model struggles to learn even on a small dataset, it indicates a more fundamental problem in the model architecture, data preprocessing, or learning algorithm."
      },
      {
        "date": "2023-11-15T10:23:00.000Z",
        "voteCount": 3,
        "content": "Verifying that your model can obtain a low loss on a small subset of the dataset is a good first step for debugging because it helps you determine if your model is capable of fitting the data and learning from it. If your model cannot fit a small subset of the data, it may indicate issues with the model architecture, initialization, or optimization algorithm. By starting with a small subset, you can identify and fix these issues more quickly, before moving on to larger-scale training and more complex debugging tasks."
      },
      {
        "date": "2023-07-31T10:17:00.000Z",
        "voteCount": 1,
        "content": "I choose A"
      },
      {
        "date": "2023-07-20T08:06:00.000Z",
        "voteCount": 1,
        "content": "the first step to quickly debug the deep learning model is to verify that it can obtain a low loss on a small subset of the dataset (Option A). If the model fails to achieve good results on the smaller subset, further investigation is required to identify and address potential issues with the model."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 158,
    "url": "https://www.examtopics.com/discussions/google/view/115868-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You are a data scientist at an industrial equipment manufacturing company. You are developing a regression model to estimate the power consumption in the company\u2019s manufacturing plants based on sensor data collected from all of the plants. The sensors collect tens of millions of records every day. You need to schedule daily training runs for your model that use all the data collected up to the current date. You want your model to scale smoothly and require minimal development work. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDevelop a custom TensorFlow regression model, and optimize it using Vertex AI Training.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDevelop a regression model using BigQuery ML.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDevelop a custom scikit-learn regression model, and optimize it using Vertex AI Training.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDevelop a custom PyTorch regression model, and optimize it using Vertex AI Training."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 12,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-06-30T07:22:00.000Z",
        "voteCount": 1,
        "content": "minimal development work +  regression model = BigQuery ML"
      },
      {
        "date": "2024-06-21T11:57:00.000Z",
        "voteCount": 1,
        "content": "B. Develop a regression model using BigQuery ML.\n\n\nYou're looking for a solution that scales smoothly and requires minimal development work. BigQuery ML is an excellent choice because it allows you to create machine learning models directly in BigQuery, without the need to write code or set up complex infrastructure."
      },
      {
        "date": "2024-04-21T22:45:00.000Z",
        "voteCount": 3,
        "content": "Scalability: BigQuery is a serverless data warehouse designed to handle massive datasets. It can efficiently process tens of millions of records daily for model training.\nMinimal Development Work: BigQuery ML offers built-in regression models like linear regression that you can train directly on your data stored in BigQuery. This eliminates the need for extensive custom code development with TensorFlow, PyTorch, or scikit-learn (options A, C, and D).\nDaily Training Runs:\n BigQuery ML allows scheduling queries for automated model training. You can set up a daily scheduled query to train your model on the latest data."
      },
      {
        "date": "2024-04-07T08:09:00.000Z",
        "voteCount": 3,
        "content": "Minimal development effort can be achieved with BigQuery ML. Also the amount of data is already in BQ."
      },
      {
        "date": "2024-04-07T04:43:00.000Z",
        "voteCount": 1,
        "content": "Minimal dev effort =&gt; BigQueryML"
      },
      {
        "date": "2024-02-28T04:47:00.000Z",
        "voteCount": 1,
        "content": "I went C."
      },
      {
        "date": "2023-07-31T10:23:00.000Z",
        "voteCount": 3,
        "content": "Minimal development effort =&gt; BigQueryML"
      },
      {
        "date": "2023-07-20T08:08:00.000Z",
        "voteCount": 1,
        "content": "for scheduling daily training runs with minimal development work and seamless scaling, the best option is to develop a regression model using BigQuery ML (Option B). It allows you to perform model training and inference directly within BigQuery, taking advantage of its distributed processing capabilities to handle large datasets effortlessly."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 159,
    "url": "https://www.examtopics.com/discussions/google/view/115869-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "Your organization manages an online message board. A few months ago, you discovered an increase in toxic language and bullying on the message board. You deployed an automated text classifier that flags certain comments as toxic or harmful. Now some users are reporting that benign comments referencing their religion are being misclassified as abusive. Upon further inspection, you find that your classifier's false positive rate is higher for comments that reference certain underrepresented religious groups. Your team has a limited budget and is already overextended. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd synthetic training data where those phrases are used in non-toxic ways.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRemove the model and replace it with human moderation.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReplace your model with a different text classifier.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRaise the threshold for comments to be considered toxic or harmful.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 9,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 6,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-09-17T03:48:00.000Z",
        "voteCount": 1,
        "content": "Gonna go with A on this one. Some toxic comments will still make it through if you choose D, whereas A addresses the problem fully and directly. Therefore I think A is a more complete answer than D."
      },
      {
        "date": "2024-09-17T03:54:00.000Z",
        "voteCount": 1,
        "content": "Even though in the question it says \"Your team has a limited budget and is already overextended\" I still think A is the better answer because it doesn't take much effort to create synthetic data and add it to train. The outcome will be more accurate than D."
      },
      {
        "date": "2024-09-10T08:22:00.000Z",
        "voteCount": 1,
        "content": "A is better than D, because D means that more geniunely toxic comments will make it through. A will teach the model to acknowledge the small subset of mislabelled comments, without exposing the customers to additional toxicity."
      },
      {
        "date": "2024-06-21T12:03:00.000Z",
        "voteCount": 2,
        "content": "option A (Add synthetic training data where those phrases are used in non-toxic ways) directly addresses the specific issue of bias and improves the model's accuracy by providing more contextually relevant training examples. This approach is more targeted and has a lower risk of introducing new biases or negatively impacting other aspects of comment moderation.\n\n\nI hope this additional explanation helps clarify why option D might not be the best choice in this scenario!"
      },
      {
        "date": "2024-06-21T12:04:00.000Z",
        "voteCount": 1,
        "content": "Raising the threshold would mean increasing the minimum score required for a comment to be classified as toxic or harmful. This could potentially reduce the number of false positives (benign comments being misclassified as toxic) by making it harder for the model to classify a comment as toxic."
      },
      {
        "date": "2024-05-27T10:10:00.000Z",
        "voteCount": 2,
        "content": "A option directly addresses the bias issue without incurring significant ongoing costs or burdening the moderation team. By augmenting the training dataset with synthetic examples where phrases related to underrepresented religious groups are used in non-toxic ways, the classifier can learn to distinguish between toxic and benign comments more accurately."
      },
      {
        "date": "2024-04-21T03:33:00.000Z",
        "voteCount": 1,
        "content": "agree with daidai75"
      },
      {
        "date": "2024-04-13T04:21:00.000Z",
        "voteCount": 2,
        "content": "Your team has a limited budget and is already overextended"
      },
      {
        "date": "2024-04-07T08:18:00.000Z",
        "voteCount": 2,
        "content": "I went fo A because it directly tackels the issue of misclassification and improving the models unterstanding of religious references. B and C don't make sense.\nD would generally reduce the number of comments flagged as toxic, which could decrease the false positive rate. However, this approach risks allowing genuinely harmful comments to go unflagged. It addresses the symptom (high false positive rate) rather than the underlying cause"
      },
      {
        "date": "2024-03-06T12:19:00.000Z",
        "voteCount": 1,
        "content": "B and C are non sense, I don't want to risk potentially increasing the FNR by reducing the FPR (Raise the threshold). Thus A."
      },
      {
        "date": "2024-02-01T01:15:00.000Z",
        "voteCount": 2,
        "content": "Your team has a limited budget and is already overextended, that means the re-training is hardly possible."
      },
      {
        "date": "2023-07-26T10:46:00.000Z",
        "voteCount": 1,
        "content": "In the long run, usually we go with A, but Option D could be a temporary solution to reduce false positives, while being aware that it may allow some genuinely toxic comments to go unnoticed. However, this may be a necessary trade-off until your team has the resources to improve the classifier or find a better solution."
      },
      {
        "date": "2023-07-25T06:06:00.000Z",
        "voteCount": 2,
        "content": "\"Your team has a limited budget and is already overextended\""
      },
      {
        "date": "2023-07-22T23:45:00.000Z",
        "voteCount": 2,
        "content": "By raising the threshold for comments to be considered toxic or harmful, you will decrease the number of false positives.\n\nB is wrong because we are taking a Google MLE exam :) A and C are wrong because both of them involve a good amount of additional work, either for extending the dataset or training/experimenting with a new model. Considering your team is already over the budget and has too many tasks on their plate (overextended), these two options are not available for you."
      },
      {
        "date": "2023-11-15T10:29:00.000Z",
        "voteCount": 1,
        "content": "But, by raising the threshold, we might be allowing some genuinely toxic comments to pass through without being flagged. This could potentially lead to an increase in the false negative rate, right?"
      },
      {
        "date": "2023-07-20T08:10:00.000Z",
        "voteCount": 1,
        "content": "A. Add synthetic training data where those phrases are used in non-toxic ways.\n\nIn this situation, where your automated text classifier is misclassifying benign comments referencing certain underrepresented religious groups as toxic or harmful, adding synthetic training data where those phrases are used in non-toxic ways can be a cost-effective solution to improve the model's performance."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 160,
    "url": "https://www.examtopics.com/discussions/google/view/115870-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You work for a magazine distributor and need to build a model that predicts which customers will renew their subscriptions for the upcoming year. Using your company\u2019s historical data as your training set, you created a TensorFlow model and deployed it to Vertex AI. You need to determine which customer attribute has the most predictive power for each prediction served by the model. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStream prediction results to BigQuery. Use BigQuery\u2019s CORR(X1, X2) function to calculate the Pearson correlation coefficient between each feature and the target variable.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Vertex Explainable AI. Submit each prediction request with the explain' keyword to retrieve feature attributions using the sampled Shapley method.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Vertex AI Workbench user-managed notebooks to perform a Lasso regression analysis on your model, which will eliminate features that do not provide a strong signal.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the What-If tool in Google Cloud to determine how your model will perform when individual features are excluded. Rank the feature importance in order of those that caused the most significant performance drop when removed from the model."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-21T22:53:00.000Z",
        "voteCount": 3,
        "content": "Feature Importance per Prediction: Vertex Explainable AI with the Shapley method provides feature attributions for each individual prediction. This allows you to understand which attributes were most influential in the model's decision for that specific customer.\nNo Code Required: This approach leverages a built-in Vertex AI service and doesn't require writing additional code for Lasso regression (option C) or using the What-If tool (option D)."
      },
      {
        "date": "2024-04-07T08:20:00.000Z",
        "voteCount": 1,
        "content": "I went for B, but not sure why it is not D. Is it even possible to model time series with the What If tool?"
      },
      {
        "date": "2023-11-12T00:54:00.000Z",
        "voteCount": 3,
        "content": "Option B"
      },
      {
        "date": "2023-07-20T08:12:00.000Z",
        "voteCount": 2,
        "content": "to determine which customer attribute has the most predictive power for each prediction served by the model, you should use Vertex Explainable AI (Option B) with the 'explain' keyword to retrieve feature attributions using the sampled Shapley method. This will give you insights into feature importance at the individual prediction level, allowing you to understand the model's behavior for specific customers."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 161,
    "url": "https://www.examtopics.com/discussions/google/view/128242-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You are an ML engineer at a manufacturing company. You are creating a classification model for a predictive maintenance use case. You need to predict whether a crucial machine will fail in the next three days so that the repair crew has enough time to fix the machine before it breaks. Regular maintenance of the machine is relatively inexpensive, but a failure would be very costly. You have trained several binary classifiers to predict whether the machine will fail, where a prediction of 1 means that the ML model predicts a failure.<br><br>You are now evaluating each model on an evaluation dataset. You want to choose a model that prioritizes detection while ensuring that more than 50% of the maintenance jobs triggered by your model address an imminent machine failure. Which model should you choose?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe model with the highest area under the receiver operating characteristic curve (AUC ROC) and precision greater than 0.5",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe model with the lowest root mean squared error (RMSE) and recall greater than 0.5.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe model with the highest recall where precision is greater than 0.5.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe model with the highest precision where recall is greater than 0.5."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-06-21T12:11:00.000Z",
        "voteCount": 1,
        "content": "C. The model with the highest recall where precision is greater than 0.5.\n\n\nIn this predictive maintenance use case, you want to prioritize detection (i.e., detecting imminent failures) while ensuring that most of the maintenance jobs triggered by your model address actual machine failures (i.e., true positives). Recall measures the proportion of actual failures detected by the model, which aligns with your goal of prioritizing detection."
      },
      {
        "date": "2024-04-21T23:06:00.000Z",
        "voteCount": 4,
        "content": "Prioritizing Detection: Recall measures how well the model identifies true positives (correctly predicts failures). A high recall ensures most imminent failures are caught.\nBalancing with Precision: Precision measures how many of the predicted failures are true positives (avoiding unnecessary maintenance). The requirement of a precision greater than 0.5 ensures a reasonable number of triggered maintenances actually address failures."
      },
      {
        "date": "2024-04-07T05:05:00.000Z",
        "voteCount": 1,
        "content": "went with C"
      },
      {
        "date": "2024-02-06T09:21:00.000Z",
        "voteCount": 3,
        "content": "Early detection of potential failures is crucial, even if it leads to some unnecessary maintenance (\"false positives\"). Therefore, we prioritize recall, which measures the ability to correctly identify true failures.\nWhile detection is important, we don't want to trigger too many unnecessary repairs (\"false positives\"). So, we set a minimum threshold of precision greater than 0.5, meaning at least 50% of triggered maintenance should address real failures."
      },
      {
        "date": "2023-12-28T02:44:00.000Z",
        "voteCount": 2,
        "content": "Priority is to detect(Pointing to Recall) and correctly detect (more that 50% - pointing to Precision)"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 162,
    "url": "https://www.examtopics.com/discussions/google/view/128243-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You built a custom ML model using scikit-learn. Training time is taking longer than expected. You decide to migrate your model to Vertex AI Training, and you want to improve the model\u2019s training time. What should you try out first?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTrain your model in a distributed mode using multiple Compute Engine VMs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTrain your model using Vertex AI Training with CPUs.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate your model to TensorFlow, and train it using Vertex AI Training.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTrain your model using Vertex AI Training with GPUs."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-07-02T01:43:00.000Z",
        "voteCount": 1,
        "content": "Scikit-learn is not intended to be used as a deep-learning framework and it does not provide any GPU support. (Ref: https://stackoverflow.com/questions/41567895/will-scikit-learn-utilize-gpu).\nSo I go with B"
      },
      {
        "date": "2024-06-21T12:20:00.000Z",
        "voteCount": 1,
        "content": "You decided to migrate to Vertex AI, If you have a model that requires significant computational resources and doesn't rely heavily on specialized GPU operations (like those in option D), then option B might still be a good choice. However, if your model is computationally intensive or involves complex neural network architectures I would go with D instead of B."
      },
      {
        "date": "2024-04-27T06:45:00.000Z",
        "voteCount": 3,
        "content": "B is correct, because scikit only has CPU support for the following services: \n- prebuilt containers for custom training (this is the case here)\n- prebuilt containers for predictions and explanations \n- Vertex AI Pipelines \n- Vertex AI Workbench user-managed notebooks\nhttps://cloud.google.com/vertex-ai/docs/supported-frameworks-list#scikit-learn_2"
      },
      {
        "date": "2024-02-28T04:34:00.000Z",
        "voteCount": 1,
        "content": "scikit-learn no GPU support."
      },
      {
        "date": "2024-02-06T09:38:00.000Z",
        "voteCount": 1,
        "content": "Scikit-learn doesn't natively support GPUs for training. However, many scikit-learn algorithms rely on libraries like NumPy and SciPy. These libraries can leverage GPUs if they're available on the system, potentially benefiting scikit-learn models indirectly."
      },
      {
        "date": "2024-01-06T01:05:00.000Z",
        "voteCount": 3,
        "content": "SK-Learn offers no GPU support. Answer is B!"
      },
      {
        "date": "2024-01-01T00:47:00.000Z",
        "voteCount": 1,
        "content": "GPU helps speeding up training process"
      },
      {
        "date": "2023-12-22T09:38:00.000Z",
        "voteCount": 1,
        "content": "Why no A?"
      },
      {
        "date": "2023-12-10T13:33:00.000Z",
        "voteCount": 2,
        "content": "B. Train your model using Vertex AI Training with CPUs.\nNo GPUs for ScikitLearn, but parrallelize/distribute training is a good way to increase model building"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 163,
    "url": "https://www.examtopics.com/discussions/google/view/130482-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You are an ML engineer at a retail company. You have built a model that predicts a coupon to offer an ecommerce customer at checkout based on the items in their cart. When a customer goes to checkout, your serving pipeline, which is hosted on Google Cloud, joins the customer's existing cart with a row in a BigQuery table that contains the customers' historic purchase behavior and uses that as the model's input. The web team is reporting that your model is returning predictions too slowly to load the coupon offer with the rest of the web page. How should you speed up your model's predictions?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAttach an NVIDIA P100 GPU to your deployed model\u2019s instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a low latency database for the customers\u2019 historic purchase behavior.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy your model to more instances behind a load balancer to distribute traffic.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a materialized view in BigQuery with the necessary data for predictions."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 9,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-08-07T14:02:00.000Z",
        "voteCount": 1,
        "content": "It says that you have to join the cart data, so you can't use the materialized view because it means that you should materialize the view every time a new cart shows up. So use a low latency DB it's the only way"
      },
      {
        "date": "2024-06-18T13:44:00.000Z",
        "voteCount": 1,
        "content": "In my opinion the materialized view could be the best way but it says that the cart data have to join with historic behaviour so it's impossibile to have all the needed data for the prediction in the materialized view because cart data are not in the database."
      },
      {
        "date": "2024-05-16T11:12:00.000Z",
        "voteCount": 1,
        "content": "Both B and D in theory does reduce latency but B implies that we might need to migrate the database to another low latency database. This migration and setup might incur additional costs and effort.\n\nIn contrast, creating a materialized view seems much more straight forward since there is already a preexisting big query table mentioned in the question."
      },
      {
        "date": "2024-05-15T03:13:00.000Z",
        "voteCount": 1,
        "content": "Coupon to offer an ecommerce customer at checkout based on the items in their cart not the customer historic behaviour. That's creating confusion while choosing B."
      },
      {
        "date": "2024-04-21T23:15:00.000Z",
        "voteCount": 2,
        "content": "Reduced Join Cost: Joining the customer's cart with their purchase history in BigQuery during each prediction can be slow. A materialized view pre-computes and stores the join results, eliminating the need for repetitive joins and significantly reducing latency.\nTargeted Data Access: Materialized views allow you to specify the exact columns needed for prediction, minimizing data transferred between BigQuery and your serving pipeline."
      },
      {
        "date": "2024-04-23T00:46:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/architecture/minimizing-predictive-serving-latency-in-machine-learning#online_real-time_prediction\n\ni'm not sure that bq is the best option, what do you think?"
      },
      {
        "date": "2024-04-15T21:41:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/architecture/minimizing-predictive-serving-latency-in-machine-learning#online_real-time_prediction\n\n\"Analytical data stores such as BigQuery are not engineered for low-latency singleton read operations, where the result is a single row with many columns.\""
      },
      {
        "date": "2024-02-18T11:08:00.000Z",
        "voteCount": 3,
        "content": "I changed my mind. \n\nB: Im read a lot this page\n\nhttps://cloud.google.com/architecture/minimizing-predictive-serving-latency-in-machine-learning#online_real-time_prediction\n\nIf the web team is reporting that the model is returning predictions too slowly to load the coupon offer with the rest of the web page, it suggests that the bottleneck might indeed be in the inference process rather than in data retrieval or processing. \nGiven that the model is deployed on Google Cloud, choosing a low-latency database makes it suitable for scenarios where quick access to data is crucial, such as real-time predictions for web applications.\n\nOption D: While pre-aggregating data in BigQuery can improve query speed,&nbsp;it might not be as efficient as a low-latency database for frequently accessed data like customer purchase history."
      },
      {
        "date": "2024-02-06T10:11:00.000Z",
        "voteCount": 1,
        "content": "Firstly, I believe the correct choice should be B. This is supported by a comprehensive Google page discussing methods to minimize real-time prediction latency. In this resource, they don't mention using a BigQuery view but instead suggest precomputing and lookup approaches to minimize prediction time.\n\nhttps://cloud.google.com/architecture/minimizing-predictive-serving-latency-in-machine-learning#online_real-time_prediction\n\nHowever, I will stick with option D because it's not clear whether option B suggests changing the entire database or just utilizing it as a preliminary step for online prediction."
      },
      {
        "date": "2024-02-18T11:08:00.000Z",
        "voteCount": 1,
        "content": "I change for B"
      },
      {
        "date": "2024-01-30T03:45:00.000Z",
        "voteCount": 2,
        "content": "Queries that use materialized views are generally faster and consume fewer resources than queries that retrieve the same data only from the base tables. Materialized views can significantly improve the performance of workloads that have the characteristic of common and repeated queries."
      },
      {
        "date": "2024-01-29T06:43:00.000Z",
        "voteCount": 3,
        "content": "D. Create a materialized view in BigQuery with the necessary data for predictions.\n\nHere's why:\n\nCurrent bottleneck: Joining the cart data with the BigQuery table containing historic purchases likely creates the latency bottleneck. Fetching data from BigQuery on every prediction request can be slow.\nMaterialized view: A materialized view pre-computes and stores the join between the cart data and the relevant historic purchase information in BigQuery. This eliminates the need for real-time joins during prediction, significantly reducing latency.\nFaster access: The pre-computed data in the materialized view is readily available within BigQuery, ensuring faster access for your serving pipeline when predicting the coupon offer.\nLower cost: Compared to additional instances or GPU resources, a materialized view can be a more cost-effective solution, especially if prediction requests are frequent."
      },
      {
        "date": "2024-01-06T19:50:00.000Z",
        "voteCount": 1,
        "content": "Option B seems most sensible."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 164,
    "url": "https://www.examtopics.com/discussions/google/view/130483-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You work for a small company that has deployed an ML model with autoscaling on Vertex AI to serve online predictions in a production environment. The current model receives about 20 prediction requests per hour with an average response time of one second. You have retrained the same model on a new batch of data, and now you are canary testing it, sending ~10% of production traffic to the new model. During this canary test, you notice that prediction requests for your new model are taking between 30 and 180 seconds to complete. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSubmit a request to raise your project quota to ensure that multiple prediction services can run concurrently.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTurn off auto-scaling for the online prediction service of your new model. Use manual scaling with one node always available.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRemove your new model from the production environment. Compare the new model and existing model codes to identify the cause of the performance bottleneck.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRemove your new model from the production environment. For a short trial period, send all incoming prediction requests to BigQuery. Request batch predictions from your new model, and then use the Data Labeling Service to validate your model\u2019s performance before promoting it to production."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 8,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-08-10T21:54:00.000Z",
        "voteCount": 1,
        "content": "I don't see B as the right answer.\nThe Vertex AI Endpoint cannot scale to 0 for newer version of the model.\n\n&gt; When you configure a DeployedModel, you must set dedicatedResources.minReplicaCount to at least 1. In other words, you cannot configure the DeployedModel to scale to 0 prediction nodes when it is unused.\n\nhttps://cloud.google.com/vertex-ai/docs/general/deployment#scaling"
      },
      {
        "date": "2024-08-10T22:04:00.000Z",
        "voteCount": 1,
        "content": "I was convinced that the machines that are autoscaled by the Vertex AI Endpoint seem to be tied to the endpoint, not the model in which they are deployed."
      },
      {
        "date": "2024-04-27T07:20:00.000Z",
        "voteCount": 2,
        "content": "B can be effective in controlling resources available to the new model, ensuring that it is not delayed by the autoscaling trying to scale up from 0. \n\nNot A: there is no indication in the description that quota limits cause the slowdown and does not address issue where new model is performing poorly on canary testing. \nNot C : when you pull the new model from prod environment, you could affect end-user experience\nNot D: Same as C plus you rely on batch predictions which does not align with the need for online, real-time predictions in the prod environemnt. Data Labeling Service is more about assessing accuracy and less about resolving latency issues."
      },
      {
        "date": "2024-04-07T05:29:00.000Z",
        "voteCount": 1,
        "content": "You have retrained the same model on a new batch of data"
      },
      {
        "date": "2024-04-15T11:10:00.000Z",
        "voteCount": 3,
        "content": "the new model has too few requests per hour and therefore scales downs to 0. Which means it has to create the an instance every time it serves a request, and this takes time.\nBy manually setting the number of nodes, the nodes will always be running, whether or not they are serving predictions"
      },
      {
        "date": "2024-03-23T00:30:00.000Z",
        "voteCount": 1,
        "content": "bottleneck seems to be start of node as there are very low number of requests so having one node always available will help in this case."
      },
      {
        "date": "2024-03-21T02:41:00.000Z",
        "voteCount": 1,
        "content": "went with c"
      },
      {
        "date": "2024-02-28T04:23:00.000Z",
        "voteCount": 1,
        "content": "I went C. \nDiagnosing the root cause."
      },
      {
        "date": "2024-02-06T10:37:00.000Z",
        "voteCount": 1,
        "content": "Choose C.\n\nThe significant increase in response time from 1 second to between 30 and 180 seconds indicates a performance issue with the new model. Before making any further changes or decisions, it's crucial to identify the root cause of this performance bottleneck. By comparing the code of the new model with the existing model, you can pinpoint any differences that might be causing the slowdown. \nIn A, This may not be the root cause and could incur unnecessary costs without addressing the performance issue. In B, &nbsp;it doesn't address the underlying issue causing the significant increase in response time observed during canary testing. in D, This would significantly increase latency and hinder real-time predictions, negatively impacting user experience."
      },
      {
        "date": "2024-02-11T15:47:00.000Z",
        "voteCount": 2,
        "content": "But in the question it says \"You have retrained the same model on a new batch of data\" it's just the data that changed so no need to check for the code check."
      },
      {
        "date": "2024-01-30T03:55:00.000Z",
        "voteCount": 4,
        "content": "sounds to me that the new model has too few requests per hour and therefore scales downs to 0. Which means it has to create the an instance every time it serves a request, and this takes time. \nBy manually setting the number of nodes, the nodes will always be running, whether or not they are serving predictions"
      },
      {
        "date": "2024-01-08T02:11:00.000Z",
        "voteCount": 1,
        "content": "Unsure on this one, but I would go with A.\nB. Turning off auto-scaling is a good measure when dealing with datasets with steep spikes of requests traffic (here we are dealing with avg. 20 request per hour) \"The service may not be able to bring nodes online fast enough to keep up with large spikes of request traffic.\" https://cloud.google.com/blog/products/ai-machine-learning/scaling-machine-learning-predictions\nC. You retrain the SAME model on a different batch of data. It is implied that the code is the same too?\nD. Actual quality of the model is not in question here, but rather the long prediction time per request.\n\nEven if the requests traffic is very low, I can only consider option A: the selected quota cannot deal with the amount of concurrent prediction requests."
      },
      {
        "date": "2024-01-06T19:56:00.000Z",
        "voteCount": 1,
        "content": "Option B or D is completely wrong. Option A to raise the quota might be necessary in some situations but doesn't necessarily deal with the performance issue at the test. Option C seems like the most suitable option."
      },
      {
        "date": "2024-03-07T03:29:00.000Z",
        "voteCount": 1,
        "content": "You only retrained the same model, your code hasn't changed, you won't find anything with C.\nIt's B."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 165,
    "url": "https://www.examtopics.com/discussions/google/view/130484-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You want to train an AutoML model to predict house prices by using a small public dataset stored in BigQuery. You need to prepare the data and want to use the simplest, most efficient approach. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite a query that preprocesses the data by using BigQuery and creates a new table. Create a Vertex AI managed dataset with the new table as the data source.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Dataflow to preprocess the data. Write the output in TFRecord format to a Cloud Storage bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite a query that preprocesses the data by using BigQuery. Export the query results as CSV files, and use those files to create a Vertex AI managed dataset.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a Vertex AI Workbench notebook instance to preprocess the data by using the pandas library. Export the data as CSV files, and use those files to create a Vertex AI managed dataset."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 6,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-06-09T08:01:00.000Z",
        "voteCount": 2,
        "content": "A) Keep the data in BigQuery and create a new table to avoid latency moving data out of BigQuery"
      },
      {
        "date": "2024-05-30T01:51:00.000Z",
        "voteCount": 1,
        "content": "A seems the correct one"
      },
      {
        "date": "2024-04-15T21:46:00.000Z",
        "voteCount": 1,
        "content": "I go for A:"
      },
      {
        "date": "2024-01-14T12:57:00.000Z",
        "voteCount": 1,
        "content": "can export directly from big query as vertex ai managed dataset to use train an autoML model"
      },
      {
        "date": "2024-01-11T01:48:00.000Z",
        "voteCount": 2,
        "content": "A \nBy writing a query that preprocesses the data using BigQuery and creating a new table, you can directly create a Vertex AI managed dataset with the new table as the data source. This approach is efficient because it leverages BigQuery\u2019s powerful data processing capabilities and avoids the need to export data to another format or service. It also simplifies the process by keeping everything within the Google Cloud ecosystem. This makes it easier to manage and monitor your data and model training process."
      },
      {
        "date": "2024-01-08T11:07:00.000Z",
        "voteCount": 2,
        "content": "I go for A:"
      },
      {
        "date": "2024-01-08T07:39:00.000Z",
        "voteCount": 1,
        "content": "Forgot to vote"
      },
      {
        "date": "2024-01-08T03:38:00.000Z",
        "voteCount": 2,
        "content": "A seems the easiest to me: preprocess the data on BigQuery (where the input table is stored) and export directly as Vertex AI managed dataset."
      },
      {
        "date": "2024-01-06T20:01:00.000Z",
        "voteCount": 1,
        "content": "Dataflow seems like the easiest and most scalable way to deal with this issue. Option B."
      },
      {
        "date": "2024-04-26T04:38:00.000Z",
        "voteCount": 1,
        "content": "small dataset -&gt; no dataflow"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 166,
    "url": "https://www.examtopics.com/discussions/google/view/130571-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You developed a Vertex AI ML pipeline that consists of preprocessing and training steps and each set of steps runs on a separate custom Docker image. Your organization uses GitHub and GitHub Actions as CI/CD to run unit and integration tests. You need to automate the model retraining workflow so that it can be initiated both manually and when a new version of the code is merged in the main branch. You want to minimize the steps required to build the workflow while also allowing for maximum flexibility. How should you configure the CI/CD workflow?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTrigger a Cloud Build workflow to run tests, build custom Docker images, push the images to Artifact Registry, and launch the pipeline in Vertex AI Pipelines.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTrigger GitHub Actions to run the tests, launch a job on Cloud Run to build custom Docker images, push the images to Artifact Registry, and launch the pipeline in Vertex AI Pipelines.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTrigger GitHub Actions to run the tests, build custom Docker images, push the images to Artifact Registry, and launch the pipeline in Vertex AI Pipelines.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTrigger GitHub Actions to run the tests, launch a Cloud Build workflow to build custom Docker images, push the images to Artifact Registry, and launch the pipeline in Vertex AI Pipelines.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 13,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 9,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-10T01:09:00.000Z",
        "voteCount": 5,
        "content": "Considering the goal of minimizing steps while allowing for flexibility, option C - \"Trigger GitHub Actions to run the tests, build custom Docker images, push the images to Artifact Registry, and launch the pipeline in Vertex AI Pipelines\" appears to be the most straightforward approach. It leverages GitHub Actions for testing and image building, then directly triggers the Vertex AI Pipelines, simplifying the workflow and reducing unnecessary services involved in the process."
      },
      {
        "date": "2024-08-04T04:35:00.000Z",
        "voteCount": 2,
        "content": "The correct answer is a. I think is tricky because D is posible, but add one step. and we want to minimize the steps."
      },
      {
        "date": "2024-06-21T13:15:00.000Z",
        "voteCount": 1,
        "content": "option D might seem appealing at first, but it adds unnecessary complexity and makes it more challenging to manage the state of your pipeline. Option C, on the other hand, provides a simpler and more straightforward approach to automating your model retraining workflow using GitHub Actions."
      },
      {
        "date": "2024-04-27T07:35:00.000Z",
        "voteCount": 3,
        "content": "Not A: does not leverage the integration capabilities of GitHub Actions with GitHub for initial testing, which is more efficient when managing repo triggers and workflows directly from Github. \n\nNot B: Cloud Run for running stateless containers, not for CI/CD tasks like building and pushing images \n\nNot C: building docker images directly in github Actions can encounter limits in terms of build performance and resource availability, esp. for complex images"
      },
      {
        "date": "2024-04-21T04:24:00.000Z",
        "voteCount": 1,
        "content": "agree with guilhermebutzke"
      },
      {
        "date": "2024-04-21T03:53:00.000Z",
        "voteCount": 1,
        "content": "Security: GitHub Actions are ideal for running unit and integration tests within the controlled environment of your GitHub repository. This keeps your test code separate from the production pipeline code running in Cloud Build.\nScalability and Resource Management: Cloud Build is a managed service specifically designed for building container images in Google Cloud. It offers better resource management and scalability for building Docker images compared to Cloud Run, which is primarily designed for running stateless containers.\nFlexibility: This configuration allows for independent scaling of test execution (in GitHub Actions) and image building (in Cloud Build). You can modify the workflow files in each platform independently without affecting the other."
      },
      {
        "date": "2024-04-21T03:54:00.000Z",
        "voteCount": 1,
        "content": "A &amp; B. Cloud Run for Image Building: While Cloud Run can build Docker images, it's not its primary function. Cloud Build is a more robust and scalable solution for container image building in Google Cloud.\nC. Building Images in GitHub Actions: GitHub Actions might have limitations on resource allocation and might not be suitable for building complex Docker images, especially if they have large dependencies."
      },
      {
        "date": "2024-04-07T05:50:00.000Z",
        "voteCount": 1,
        "content": "i agree with guilhermebutzke"
      },
      {
        "date": "2024-02-06T10:59:00.000Z",
        "voteCount": 2,
        "content": "Choose D:\n\nGitHub Actions should be used to run tests and initiate the workflow upon code merges. Then, Cloud Build is a suitable service for building Docker images and handling the subsequent steps of pushing the images to Artifact Registry. So, Vertex AI Pipelines can be launched as part of the Cloud Build workflow for model retraining.\n\nIn A Using Cloud Build directly from GitHub Actions would bypass GitHub Actions' capabilities for triggering and testing. In B, Cloud Run for building Docker images can introduce potential compatibility issues with Vertex AI Pipelines. In C, &nbsp;Skipping Cloud Build for image building limits the workflow's portability and integration with Vertex AI.\n\nhttps://cloud.google.com/vertex-ai/docs/pipelines/introduction\nhttps://medium.com/@cait.ray13/serving-ml-model-using-google-pub-sub-python-f569c46e7eb0"
      },
      {
        "date": "2024-02-04T14:15:00.000Z",
        "voteCount": 2,
        "content": "It has to be C. Therese no need to use both GH Actions and Cloud Build when GH Actions can do it all by itself"
      },
      {
        "date": "2024-01-18T04:25:00.000Z",
        "voteCount": 3,
        "content": "D\nhttps://cloud.google.com/build/docs/building/build-containers\nhttps://cloud.google.com/build/docs/build-push-docker-image"
      },
      {
        "date": "2024-01-11T01:55:00.000Z",
        "voteCount": 2,
        "content": "The best approach would be Option C.\n\nBy triggering GitHub Actions to run the tests, build custom Docker images, push the images to Artifact Registry, and launch the pipeline in Vertex AI Pipelines, you can automate the model retraining workflow. This approach allows for maximum flexibility and minimizes the steps required to build the workflow."
      },
      {
        "date": "2024-01-08T07:15:00.000Z",
        "voteCount": 2,
        "content": "I am torn between C and D. GitHub actions to run the tests is definitely the simplest. Cloud Build allows to access fully managed CI/CD workflow (you could setup the Docker build job), but I figure it would be easier to do it from GitHub actions directly (https://docs.github.com/en/actions/creating-actions/creating-a-docker-container-action) which allows you to use 1 tool less and achieve the same result."
      },
      {
        "date": "2024-01-08T06:55:00.000Z",
        "voteCount": 2,
        "content": "i think it's D"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 167,
    "url": "https://www.examtopics.com/discussions/google/view/130575-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You are working with a dataset that contains customer transactions. You need to build an ML model to predict customer purchase behavior. You plan to develop the model in BigQuery ML, and export it to Cloud Storage for online prediction. You notice that the input data contains a few categorical features, including product category and payment method. You want to deploy the model as quickly as possible. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the TRANSFORM clause with the ML.ONE_HOT_ENCODER function on the categorical features at model creation and select the categorical and non-categorical features.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the ML.ONE_HOT_ENCODER function on the categorical features and select the encoded categorical features and non-categorical features as inputs to create your model.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the CREATE MODEL statement and select the categorical and non-categorical features.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the ML.MULTI_HOT_ENCODER function on the categorical features, and select the encoded categorical features and non-categorical features as inputs to create your model."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 6,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-08-10T22:32:00.000Z",
        "voteCount": 1,
        "content": "TRANSFORM is used to transform the input for both learning and inference.\nONE_HOT_ENCODER can also be used within TRANSFORM.\nThe other options require conversion on the input in prediction.\nA is correct."
      },
      {
        "date": "2024-08-10T22:38:00.000Z",
        "voteCount": 1,
        "content": "Sorry, BlehMaks is correct.\nIn this case, we don't use TRANSFORM, we need to do the conversion in the forecast as well."
      },
      {
        "date": "2024-06-06T11:08:00.000Z",
        "voteCount": 2,
        "content": "CREATE OR REPLACE MODEL `project.dataset.model_name`\nOPTIONS(model_type='logistic_reg') AS\nSELECT\n  *,\n  TRANSFORM(\n    product_category,\n    payment_method\n  USING\n    ML.ONE_HOT_ENCODER(product_category) AS encoded_product_category,\n    ML.ONE_HOT_ENCODER(payment_method) AS encoded_payment_method\n  )\nFROM\n  `project.dataset.table_name`;"
      },
      {
        "date": "2024-01-12T07:30:00.000Z",
        "voteCount": 4,
        "content": "When the TRANSFORM clause is present, only output columns from the TRANSFORM clause are used in training. Any results from query_statement that don't appear in the TRANSFORM clause are ignored. https://cloud.google.com/bigquery/docs/reference/standard-sql/bigqueryml-syntax-create#transform\nso if you want TRANSFORM then use TRANSFORM for both categorical  and non-categorical features"
      },
      {
        "date": "2024-01-10T01:07:00.000Z",
        "voteCount": 1,
        "content": "Given the goal of quickly deploying the model for predicting customer purchase behavior while handling categorical features, option B - \"Use the ML.ONE_HOT_ENCODER function on the categorical features and select the encoded categorical features and non-categorical features as inputs to create your model\" seems to be the most appropriate. This approach directly handles the encoding of categorical features using one-hot encoding and selects the necessary features for model creation, ensuring efficient utilization of categorical data in the BigQuery ML model."
      },
      {
        "date": "2024-01-08T07:31:00.000Z",
        "voteCount": 1,
        "content": "Only B and D make sense. Between the two, after reading the use case of multi-hot encoding (https://cloud.google.com/bigquery/docs/auto-preprocessing#feature-transform), I would tend towards B, since one-hot encoding is preferred over in case of using non-numerical, non-array features (product category and payment methods are often respresented as such); multi-hot encoding is preferred in case of non-numerical, array features, which is not the case here."
      },
      {
        "date": "2024-01-08T07:33:00.000Z",
        "voteCount": 1,
        "content": "Also I understand it cannot be A because it says \"take the categorical features\" as opposed to the more specific \"take the encoded categorical features\" in B"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 168,
    "url": "https://www.examtopics.com/discussions/google/view/130580-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You need to develop an image classification model by using a large dataset that contains labeled images in a Cloud Storage bucket. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Vertex AI Pipelines with the Kubeflow Pipelines SDK to create a pipeline that reads the images from Cloud Storage and trains the model.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Vertex AI Pipelines with TensorFlow Extended (TFX) to create a pipeline that reads the images from Cloud Storage and trains the model.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImport the labeled images as a managed dataset in Vertex AI and use AutoML to train the model.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConvert the image dataset to a tabular format using Dataflow Load the data into BigQuery and use BigQuery ML to train the model."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 6,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 4,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-06-21T13:43:00.000Z",
        "voteCount": 1,
        "content": "B is right in my opinion, while both options C and B involve importing labeled images into Vertex AI, using AutoML for image classification might not be the most suitable choice. TFX is a more specialized tool that provides a robust pipeline framework specifically designed for image classification tasks, making it a better fit for this particular use case."
      },
      {
        "date": "2024-04-07T05:57:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/vertex-ai/docs/tutorials/image-classification-automl/dataset"
      },
      {
        "date": "2024-04-26T04:42:00.000Z",
        "voteCount": 1,
        "content": "no need to use a pipeline, automl is ok"
      },
      {
        "date": "2024-02-06T14:34:00.000Z",
        "voteCount": 4,
        "content": "My answer: B \n\nTensorFlow Extended (TFX) and Kubeflow provide capabilities for building machine learning pipelines that can handle data stored in Google Cloud Storage (GCS). However, when it comes to ease of use specifically for working with data in GCS, TFX may have a slight edge over Kubeflow for \n1- Integration with GCS- TensorFlow: TFX is tightly integrated with TensorFlow that has built-in support for GCS and provides convenient APIs for reading data directly from GCS buckets\n2 - Abstraction of Data Handling TFX provides higher-level abstractions and components specifically designed for common machine learning tasks, including data preprocessing, model training, and model evaluation"
      },
      {
        "date": "2024-04-26T10:19:00.000Z",
        "voteCount": 2,
        "content": "Which SDK use?\n\u2022\tIf you use TensorFlow in an ML workflow that processes terabytes of structured data or text data -&gt; TFX\n\u2022\tFor other use-cases -&gt; KFP"
      },
      {
        "date": "2024-01-13T00:38:00.000Z",
        "voteCount": 3,
        "content": "It's C"
      },
      {
        "date": "2024-01-12T08:08:00.000Z",
        "voteCount": 1,
        "content": "95th is the similar question. https://cloud.google.com/vertex-ai/docs/pipelines/build-pipeline#sdk"
      },
      {
        "date": "2024-01-17T08:13:00.000Z",
        "voteCount": 1,
        "content": "95 is a similar question but it does not offer Vertex AI AutoML as an option. which I think it's the right answer here consider the little amount of info provided in the question"
      },
      {
        "date": "2024-01-08T07:59:00.000Z",
        "voteCount": 2,
        "content": "Very vaguely put. I choose C over B just because it sounds like a simpler approach, but both should theoretically work."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 169,
    "url": "https://www.examtopics.com/discussions/google/view/130583-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You are developing a model to detect fraudulent credit card transactions. You need to prioritize detection, because missing even one fraudulent transaction could severely impact the credit card holder. You used AutoML to tram a model on users' profile information and credit card transaction data After training the initial model, you notice that the model is failing to detect many fraudulent transactions. How should you adjust the training parameters in AutoML to improve model performance? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the score threshold",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDecrease the score threshold.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd more positive examples to the training set",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd more negative examples to the training set",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReduce the maximum number of node hours for training"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 7,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 5,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-08-13T04:17:00.000Z",
        "voteCount": 1,
        "content": "B&amp;C\n\nIf we want to increase the detection rate of fraudulent transactions, we can lower the classification threshold. By doing so, the model becomes less strict and classifies more transactions as potentially fraudulent. This implies including a higher number of false positives in our results.\n\nTo improve the performance, we can also add more fradulent transactions examples to the dataset (fraudulent transactions are the positivies, in this case)"
      },
      {
        "date": "2024-04-21T04:16:00.000Z",
        "voteCount": 2,
        "content": "B &amp; D\nD. Add more negative examples to the training set: Fraudulent transactions are typically a minority compared to legitimate transactions. By increasing the number of negative examples (fraudulent transactions) in your training data, you provide AutoML with more information about the patterns of fraudulent activity. This can help the model better distinguish between legitimate and fraudulent transactions.\nB. Decrease the score threshold: The score threshold determines the level of suspicion assigned to a transaction by the model. A lower threshold means the model flags more transactions as suspicious, potentially catching more fraudulent activities. However, this might also lead to an increase in false positives (flagging legitimate transactions). You'll need to find a balance between fraud detection and acceptable false positive rates based on your business needs."
      },
      {
        "date": "2024-08-13T04:12:00.000Z",
        "voteCount": 1,
        "content": "Positive is fraudulent in this case, so B &amp; C"
      },
      {
        "date": "2024-04-23T06:42:00.000Z",
        "voteCount": 3,
        "content": "positive is fraudulent.. aka minority class"
      },
      {
        "date": "2024-04-21T04:18:00.000Z",
        "voteCount": 1,
        "content": "A. Increase the score threshold: This would make the model more conservative and less likely to flag fraudulent transactions, potentially missing actual fraud.\nC. Add more positive examples (legitimate transactions): While having a balanced dataset is important, in this case, prioritizing fraud detection suggests focusing on improving the model's ability to identify fraudulent transactions (negative examples) rather than adding more legitimate ones.\nE. Reduce the maximum number of node hours for training: Reducing training time might limit the model's ability to learn complex patterns, potentially hindering its performance."
      },
      {
        "date": "2024-01-14T13:48:00.000Z",
        "voteCount": 2,
        "content": "B&amp;C - Fraudulent transactions are often rare events, so the model might not have enough exposure to learn their patterns effectively."
      },
      {
        "date": "2024-01-13T08:48:00.000Z",
        "voteCount": 2,
        "content": "B &amp; C\nThey are the options"
      },
      {
        "date": "2024-01-12T08:23:00.000Z",
        "voteCount": 3,
        "content": "BC\nB. More suspicious transactions are marked as fraudulent\nC. Usually real fraudulent transactions are rare in datasets so we need to add more examples to make our model focus more on them"
      },
      {
        "date": "2024-01-10T01:17:00.000Z",
        "voteCount": 2,
        "content": "B &amp; D\n\nB. Decrease the score threshold: This adjustment could make the model more sensitive, potentially reducing the chance of missing fraudulent transactions, but might increase false positives.\n\nD. Add more negative examples to the training set: Providing more examples of non-fraudulent transactions could help the model better distinguish between legitimate and fraudulent transactions, improving its overall performance."
      },
      {
        "date": "2024-04-28T01:46:00.000Z",
        "voteCount": 1,
        "content": "Option D's approach could be beneficial in a scenario where the model is overfitting to the fraudulent (positive) cases due to an imbalance in the training data favoring fraudulent examples. But, as per the question \"model is failing to detect many fraudulent transactions\""
      },
      {
        "date": "2024-01-08T08:23:00.000Z",
        "voteCount": 2,
        "content": "Regarding the 2nd choice (did not notice), I would choose C: adding more positive examples to the training set. It did not sound like a change of parameter to me, but apparently AutoML allows parametrization of data split: https://cloud.google.com/vertex-ai/docs/general/ml-use. I am not entirely convinced but it seems more likely than any other option (reducing max number of hours per node for training can only affect performance negatively I reckon?)"
      },
      {
        "date": "2024-01-08T08:13:00.000Z",
        "voteCount": 2,
        "content": "B. Decreasing the score threshold will cause the model to make more positive predictions and potentially decrease the number of false negatives (non detected fraudulent transactions)"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 170,
    "url": "https://www.examtopics.com/discussions/google/view/130587-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You need to deploy a scikit-leam classification model to production. The model must be able to serve requests 24/7, and you expect millions of requests per second to the production application from 8 am to 7 pm. You need to minimize the cost of deployment. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy an online Vertex AI prediction endpoint. Set the max replica count to 1",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy an online Vertex AI prediction endpoint. Set the max replica count to 100\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy an online Vertex AI prediction endpoint with one GPU per replica. Set the max replica count to 1",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy an online Vertex AI prediction endpoint with one GPU per replica. Set the max replica count to 100"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-06-21T13:58:00.000Z",
        "voteCount": 1,
        "content": "Option A (Deploying an online Vertex AI prediction endpoint. Set the max replica count to 1) is still a good choice for minimizing costs. By setting the max replica count to 1, you are allowing Vertex AI to scale up or down based on load, which means that during off-peak hours, you won't be paying for unnecessary instances."
      },
      {
        "date": "2024-04-13T04:41:00.000Z",
        "voteCount": 1,
        "content": "see pikachu007"
      },
      {
        "date": "2024-01-13T08:49:00.000Z",
        "voteCount": 2,
        "content": "B\nwe don't need GPU for scikit-learn"
      },
      {
        "date": "2024-01-12T08:36:00.000Z",
        "voteCount": 2,
        "content": "scikit-learn doesn't support GPU\nhttps://scikit-learn.org/stable/faq.html#will-you-add-gpu-support"
      },
      {
        "date": "2024-01-10T01:18:00.000Z",
        "voteCount": 4,
        "content": "B. Deploy an online Vertex AI prediction endpoint. Set the max replica count to 100:\nThis option provides a higher number of replicas (100) to handle the expected high volume of requests during peak hours. While it might result in increased costs, it provides the necessary scalability to manage the incoming traffic efficiently. During non-peak hours, you can consider scaling down the replicas to reduce costs, as Vertex AI allows dynamic scaling based on demand."
      },
      {
        "date": "2024-01-08T08:33:00.000Z",
        "voteCount": 1,
        "content": "B.\nscikit-learn -&gt; no need for GPU\nmax number of replicas -&gt; 1 is too little if we are serving online predictions at such a massive scale (millions per second)"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 171,
    "url": "https://www.examtopics.com/discussions/google/view/130588-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You work with a team of researchers to develop state-of-the-art algorithms for financial analysis. Your team develops and debugs complex models in TensorFlow. You want to maintain the ease of debugging while also reducing the model training time. How should you set up your training environment?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a v3-8 TPU VM. SSH into the VM to train and debug the model.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a v3-8 TPU node. Use Cloud Shell to SSH into the Host VM to train and debug the model.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a n1 -standard-4 VM with 4 NVIDIA P100 GPUs. SSH into the VM and use ParameterServerStraregv to train the model.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a n1-standard-4 VM with 4 NVIDIA P100 GPUs. SSH into the VM and use MultiWorkerMirroredStrategy to train the model.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 11,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-09-11T00:58:00.000Z",
        "voteCount": 1,
        "content": "MultiWorkerMirroredStrategy is for multiple workers, each with one or more GPUs. For a single worker/vm with multiple GPUs it would be MirroredStrategy, so D is definitely wrong.\nC is wrong as that is a totally unrelated concept, B is probably wrong as it's much less convenient than using a terminal (B vs A is tough call, but A replicates their existing setup most closely)"
      },
      {
        "date": "2024-06-21T14:04:00.000Z",
        "voteCount": 1,
        "content": "Option D Configure a n1-standard-4 VM with 4 NVIDIA P100 GPUs. SSH into the VM and use MultiWorkerMirroredStrategy to train the model. is indeed a correct answer.\nMultiWorkerMirroredStrategy: This strategy allows you to distribute your training process across multiple machines (in this case, the 4 NVIDIA P100 GPUs) while maintaining synchronization between them.\nNVIDIA P100 GPUs: These high-performance GPUs are well-suited for computationally intensive tasks like deep learning model training."
      },
      {
        "date": "2024-06-18T14:04:00.000Z",
        "voteCount": 2,
        "content": "It says \"state-of-art\" and TPU is more recent than GPU. No need to log using Cloud Shell into VM and there's no mention about cost. So TPU + SSH directly into VM could be the choice."
      },
      {
        "date": "2024-04-21T04:52:00.000Z",
        "voteCount": 2,
        "content": "Debugging Ease: SSHing into a VM provides a familiar environment for researchers to use familiar debugging tools within the VM for their complex TensorFlow models. This maintains ease of debugging compared to TPUs which require special considerations.\nFaster Training: Utilizing 4 NVIDIA P100 GPUs within the VM leverages parallel processing capabilities to significantly accelerate training compared to a CPU-only VM."
      },
      {
        "date": "2024-04-07T06:04:00.000Z",
        "voteCount": 2,
        "content": "the need to balance ease of debugging and reduce training time"
      },
      {
        "date": "2024-02-06T14:57:00.000Z",
        "voteCount": 2,
        "content": "My choice is D. \n\nWhile TPUs offer faster training, they can be less convenient for debugging due to limitations in tooling and visualization, such as the lack of support for some debuggers and limited visualization options. \n\nComparing options C and D, MultiWorkerMirroredStrategy uses synchronous distributed training across multiple workers, making it easier to inspect intermediate states and variables during debugging. In contrast, ParameterServerStraregv utilizes asynchronous multi-machine training, which can be less intuitive to debug. However, it's important to note that ParameterServerStraregv might be more efficient for training extremely large models. Therefore, considering the specific need for ease of debugging in this scenario, MultiWorkerMirroredStrategy appears to be the more suitable choice."
      },
      {
        "date": "2024-01-10T01:21:00.000Z",
        "voteCount": 4,
        "content": "Given the need to balance ease of debugging and reduce training time for complex models in TensorFlow, option D - \"Configure an n1-standard-4 VM with 4 NVIDIA P100 GPUs. SSH into the VM and use MultiWorkerMirroredStrategy to train the model\" appears to be more suitable. This setup utilizes NVIDIA P100 GPUs for computational power and employs MultiWorkerMirroredStrategy, which can distribute the workload across GPUs efficiently, potentially reducing training time while maintaining a relatively straightforward environment for debugging."
      },
      {
        "date": "2024-01-08T08:51:00.000Z",
        "voteCount": 1,
        "content": "D.\n\nCannot be B, because node architecture make it difficult to debug: https://cloud.google.com/tpu/docs/system-architecture-tpu-vm#tpu-node-arch\n\nWhile TPUs are faster than GPUs for certain scenarios, and never slower, they are less easy to debug. Parallelizing the training across different workers (GPUs) using MultiWorkerMirroredStrategy makes most sense to me."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 172,
    "url": "https://www.examtopics.com/discussions/google/view/130590-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You created an ML pipeline with multiple input parameters. You want to investigate the tradeoffs between different parameter combinations. The parameter options are<br>\u2022\tInput dataset<br>\u2022\tMax tree depth of the boosted tree regressor<br>\u2022\tOptimizer learning rate<br><br>You need to compare the pipeline performance of the different parameter combinations measured in F1 score, time to train, and model complexity. You want your approach to be reproducible, and track all pipeline runs on the same platform. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Use BigQueryML to create a boosted tree regressor, and use the hyperparameter tuning capability.<br>2. Configure the hyperparameter syntax to select different input datasets: max tree depths, and optimizer learning rates. Choose the grid search option.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a Vertex AI pipeline with a custom model training job as part of the pipeline. Configure the pipeline\u2019s parameters to include those you are investigating.<br>2. In the custom training step, use the Bayesian optimization method with F1 score as the target to maximize.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a Vertex AI Workbench notebook for each of the different input datasets.<br>2. In each notebook, run different local training jobs with different combinations of the max tree depth and optimizer learning rate parameters.<br>3. After each notebook finishes, append the results to a BigQuery table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create an experiment in Vertex AI Experiments.<br>2. Create a Vertex AI pipeline with a custom model training job as part of the pipeline. Configure the pipeline\u2019s parameters to include those you are investigating.<br>3. Submit multiple runs to the same experiment, using different values for the parameters.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-21T17:52:00.000Z",
        "voteCount": 2,
        "content": "Vertex AI Experiments: This service allows you to group and track different pipeline runs associated with the same experiment. This facilitates comparing runs with various parameter combinations.\nVertex AI Pipelines: Pipelines enable you to define a workflow for training your model. You can include a custom training step within the pipeline and configure its parameters as needed. This ensures reproducibility as all runs follow the same defined workflow.\nSubmitting multiple runs: By submitting multiple pipeline runs to the same experiment with different parameter values, you can efficiently explore various configurations and track their performance metrics like F1 score, training time, and model complexity within Vertex AI Experiments."
      },
      {
        "date": "2024-04-21T17:52:00.000Z",
        "voteCount": 1,
        "content": "A. BigQuery ML: BigQuery ML doesn't offer functionalities like Vertex AI Pipelines for building and managing workflows. It also lacks experiment tracking capabilities.\nC. Vertex AI Workbench notebooks: While Vertex AI Workbench provides notebooks for running training jobs, this approach wouldn't be reproducible. Each notebook would be a separate entity, making it difficult to track runs and manage different parameter combinations."
      },
      {
        "date": "2024-04-07T06:07:00.000Z",
        "voteCount": 1,
        "content": "Vertex AI Experiment was created to compare runs."
      },
      {
        "date": "2024-01-13T09:06:00.000Z",
        "voteCount": 2,
        "content": "D\nThe best option for investigating the tradeoffs between different parameter combinations is to create an experiment in Vertex AI Experiments,"
      },
      {
        "date": "2024-01-12T10:28:00.000Z",
        "voteCount": 1,
        "content": "Vertex AI Experiment was created to compare runs.\nA is incorrect because you can't create a boosted tree  using BigQueryML \nhttps://cloud.google.com/bigquery/docs/bqml-introduction#supported_models"
      },
      {
        "date": "2024-01-10T01:26:00.000Z",
        "voteCount": 1,
        "content": "Given the objective of investigating parameter tradeoffs while ensuring reproducibility and tracking, option D - \"Create an experiment in Vertex AI Experiments and submit multiple runs to the same experiment, using different values for the parameters\" seems to be the most suitable. This approach provides a structured and trackable environment within Vertex AI Experiments, allowing multiple runs with varied parameters to be monitored for F1 score, training times, and potentially model complexity, enabling a comprehensive analysis of parameter combinations' tradeoffs."
      },
      {
        "date": "2024-01-08T11:38:00.000Z",
        "voteCount": 1,
        "content": "I go with D : https://cloud.google.com/vertex-ai/docs/evaluation/introduction#tabular"
      },
      {
        "date": "2024-01-08T08:58:00.000Z",
        "voteCount": 1,
        "content": "You want to investigate tradeoffs between different parameter combinations and track all runs on the same platform -&gt; clearly D. Vertex AI experiments etcetera."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 173,
    "url": "https://www.examtopics.com/discussions/google/view/130592-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You received a training-serving skew alert from a Vertex AI Model Monitoring job running in production. You retrained the model with more recent training data, and deployed it back to the Vertex AI endpoint, but you are still receiving the same alert. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the model monitoring job to use a lower sampling rate.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the model monitoring job to use the more recent training data that was used to retrain the model.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTemporarily disable the alert. Enable the alert again after a sufficient amount of new production traffic has passed through the Vertex AI endpoint.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTemporarily disable the alert until the model can be retrained again on newer training data. Retrain the model again after a sufficient amount of new production traffic has passed through the Vertex AI endpoint."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 6,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-06-21T14:24:00.000Z",
        "voteCount": 1,
        "content": "C. Temporarily disable the alert. Enable the alert again after a sufficient amount of new production traffic has passed through the Vertex AI endpoint.\n\n\nHere's why:\n\n\n\nYou've already retrained the model with more recent training data and deployed it back to the Vertex AI endpoint, but the alert persists.\nThis suggests that the model is still adapting to the changing data distribution in production.\nTemporarily disabling the alert will give the model a chance to adjust to the new data distribution before the monitoring job starts firing alerts again.\nOnce enough new traffic has passed through, you can re-enable the alert and continue monitoring the model's performance."
      },
      {
        "date": "2024-06-17T07:34:00.000Z",
        "voteCount": 1,
        "content": "The baseline is calculated when you create a Vertex AI Model Monitoring job, and is only recalculated if you update the training dataset for the job."
      },
      {
        "date": "2024-05-06T20:19:00.000Z",
        "voteCount": 2,
        "content": "Is B actually the correct answer? According to the documentation, training-serving skew detection can only be enabled if the original training data is available. Furthermore, the baseline is automatically recalculated when the training data is updated. \n\nSo does this question imply that the model is trained on data without updating the original training-dataset? If so then B is clearly correct. If they updated the training dataset with new data and then retrained the model then the model monitoring job's baseline should automatically have been recalculated. I see no other valid answers in that case?"
      },
      {
        "date": "2024-04-13T04:45:00.000Z",
        "voteCount": 1,
        "content": "This option can help align the baseline distribution of the model monitoring job with the current distribution of the production data, and eliminate the false positive alerts."
      },
      {
        "date": "2024-01-13T09:11:00.000Z",
        "voteCount": 3,
        "content": "B\nThis option can help align the baseline distribution of the model monitoring job with the current distribution of the production data, and eliminate the false positive alerts."
      },
      {
        "date": "2024-01-12T10:48:00.000Z",
        "voteCount": 3,
        "content": "the cause of the issue could be that the developer forgot to switch their monitoring job to the latest training dataset and the monitoring job still compares prod data with old training dataset and they of course have a skew"
      },
      {
        "date": "2024-01-11T03:08:00.000Z",
        "voteCount": 1,
        "content": "B. Update the model monitoring job to use the more recent training data that was used to retrain the model:\n\nThis option directly aligns the model monitoring with the recently retrained model and ensures that the monitoring job reflects the characteristics of the latest training data."
      },
      {
        "date": "2024-01-08T09:17:00.000Z",
        "voteCount": 1,
        "content": "A. Changing the sampling rate affects not training skew but cost efficiency: https://cloud.google.com/vertex-ai/docs/model-monitoring/overview#considerations\nB. The model monitoring job is already using the most recent data to detect skew.\nC&amp;D are the same, except for D being more specific, so I would tend towards D."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 174,
    "url": "https://www.examtopics.com/discussions/google/view/130611-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You developed a custom model by using Vertex AI to forecast the sales of your company\u2019s products based on historical transactional data. You anticipate changes in the feature distributions and the correlations between the features in the near future. You also expect to receive a large volume of prediction requests. You plan to use Vertex AI Model Monitoring for drift detection and you want to minimize the cost. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the features for monitoring. Set a monitoring-frequency value that is higher than the default.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the features for monitoring. Set a prediction-sampling-rate value that is closer to 1 than 0.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the features and the feature attributions for monitoring. Set a monitoring-frequency value that is lower than the default.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the features and the feature attributions for monitoring. Set a prediction-sampling-rate value that is closer to 0 than 1.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-21T18:09:00.000Z",
        "voteCount": 3,
        "content": "Feature and Feature Attribution Monitoring: Since you anticipate changes in feature distributions and correlations, monitoring both features and their attributions provides a more comprehensive view of potential drift. Feature attributions explain how each feature contributes to the model's predictions. Monitoring them helps identify if these contributions are changing as expected.\nLower Prediction Sampling Rate: This reduces the cost associated with Vertex AI Model Monitoring. The sampling rate determines the percentage of prediction requests used for monitoring calculations. A lower rate reduces the number of predictions analyzed, lowering monitoring costs. However, it's important to strike a balance between cost and having enough data for drift detection."
      },
      {
        "date": "2024-01-12T14:01:00.000Z",
        "voteCount": 3,
        "content": "if we expect a large volume of prediction requests then pick D. if we expect the changes to be  infrequent then C\nhttps://cloud.google.com/vertex-ai/docs/model-monitoring/overview#considerations"
      },
      {
        "date": "2024-01-11T03:10:00.000Z",
        "voteCount": 2,
        "content": "Given the need to minimize costs while addressing changes in feature distributions and correlations, option D - \"Use the features and the feature attributions for monitoring. Set a prediction-sampling-rate value that is closer to 0 than 1\" seems to be a reasonable choice. This option allows monitoring both features and feature attributions, offering insights into changes in feature importance, while the lower prediction-sampling-rate helps manage costs by monitoring a subset of predictions. It's a trade-off between cost efficiency and the need for effective drift detection"
      },
      {
        "date": "2024-01-08T10:23:00.000Z",
        "voteCount": 2,
        "content": "Not A. because higher monitoring frequency, higher cost.\nNot B. because higher prediction request sample rate, higher cost.\nBetween the remaining 2, better to lower the prediction request sample rate so only a small fraction of the latest data is evaluated for drift, also because lots of data are expected so a small perecentage should suffice to detect drift."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 175,
    "url": "https://www.examtopics.com/discussions/google/view/130797-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You have recently trained a scikit-learn model that you plan to deploy on Vertex AI. This model will support both online and batch prediction. You need to preprocess input data for model inference. You want to package the model for deployment while minimizing additional code. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Upload your model to the Vertex AI Model Registry by using a prebuilt scikit-ieam prediction container.<br>2. Deploy your model to Vertex AI Endpoints, and create a Vertex AI batch prediction job that uses the instanceConfig.instanceType setting to transform your input data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Wrap your model in a custom prediction routine (CPR). and build a container image from the CPR local model.<br>2. Upload your scikit learn model container to Vertex AI Model Registry.<br>3. Deploy your model to Vertex AI Endpoints, and create a Vertex AI batch prediction job\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a custom container for your scikit learn model.<br>2. Define a custom serving function for your model.<br>3. Upload your model and custom container to Vertex AI Model Registry.<br>4. Deploy your model to Vertex AI Endpoints, and create a Vertex AI batch prediction job.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a custom container for your scikit learn model.<br>2. Upload your model and custom container to Vertex AI Model Registry.<br>3. Deploy your model to Vertex AI Endpoints, and create a Vertex AI batch prediction job that uses the instanceConfig.instanceType setting to transform your input data."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 11,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-14T14:04:00.000Z",
        "voteCount": 5,
        "content": "B - Creating a custom container without CPR adds additional complexity. i.e. write model server write dockerfile and also build and upload image. Where as using a CPR only requires writing a predictor and using vertex SDK to build image.\nhttps://cloud.google.com/vertex-ai/docs/predictions/custom-prediction-routines"
      },
      {
        "date": "2024-06-06T12:36:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/vertex-ai/docs/predictions/custom-prediction-routines"
      },
      {
        "date": "2024-04-21T04:36:00.000Z",
        "voteCount": 1,
        "content": "agree with shadz10"
      },
      {
        "date": "2024-02-06T17:01:00.000Z",
        "voteCount": 1,
        "content": "My choose: C\n\nOption C ensures that the scikit-learn model is properly packaged, deployed, and integrated with Vertex AI services while minimizing the need for additional code beyond what is necessary for customizing the serving function.\n\nOption B is not considered correct because it suggests wrapping the scikit-learn model in a custom prediction routine (CPR), which might not be the most suitable approach for deploying scikit-learn models on Vertex AI.\n\nOptions A and D using InstanceConfig, that is limited for preprocessing. Uploading the container without a serving function won't work."
      },
      {
        "date": "2024-01-11T03:15:00.000Z",
        "voteCount": 1,
        "content": "Considering the goal of minimizing additional code and complexity, option D - \"Create a custom container for your scikit-learn model, upload your model and custom container to Vertex AI Model Registry, deploy your model to Vertex AI Endpoints, and create a Vertex AI batch prediction job that uses the instanceConfig.instanceType setting to transform your input data\" seems to be a more straightforward and efficient approach. It involves customizing the container for the scikit-learn model, leveraging the Vertex AI Model Registry, and utilizing the specified instance type for batch prediction without introducing unnecessary complexity like custom prediction routines."
      },
      {
        "date": "2024-01-10T08:24:00.000Z",
        "voteCount": 4,
        "content": "I go with B:\n\n\u201cCustom prediction routines (CPR) lets you build&nbsp;custom containers&nbsp;with pre/post processing code easily, without dealing with the details of setting up an HTTP server or building a container from scratch.\u201d (https://cloud.google.com/vertex-ai/docs/predictions/custom-prediction-routines). This alone makes B preferable to C and D, provided lack of complex model architecture.\n\nRegarding A, pre-built containers only allow serving predictions, but not preprocessing of data (https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers#use_a_prebuilt_container). B thus remains the most likely option."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 176,
    "url": "https://www.examtopics.com/discussions/google/view/130800-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You work for a food product company. Your company\u2019s historical sales data is stored in BigQuery.You need to use Vertex AI\u2019s custom training service to train multiple TensorFlow models that read the data from BigQuery and predict future sales. You plan to implement a data preprocessing algorithm that performs mm-max scaling and bucketing on a large number of features before you start experimenting with the models. You want to minimize preprocessing time, cost, and development effort. How should you configure this workflow?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite the transformations into Spark that uses the spark-bigquery-connector, and use Dataproc to preprocess the data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite SQL queries to transform the data in-place in BigQuery.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd the transformations as a preprocessing layer in the TensorFlow models.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Dataflow pipeline that uses the BigQuerylO connector to ingest the data, process it, and write it back to BigQuery."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-21T18:24:00.000Z",
        "voteCount": 2,
        "content": "In-place Transformation: BigQuery allows you to perform data transformations directly within the data warehouse using SQL queries. This eliminates the need for data movement and reduces processing time compared to other options that involve data transfer.\nMinimized Development Effort: Since you're already familiar with SQL, writing queries for mm-max scaling and bucketing requires minimal additional development effort compared to learning and implementing new frameworks like Spark or Dataflow.\nCost-Effective: BigQuery's serverless architecture scales processing power based on your workload. This can be more cost-effective than managing separate processing clusters like Dataproc."
      },
      {
        "date": "2024-01-14T14:13:00.000Z",
        "voteCount": 2,
        "content": "B - Keeps the preprocessing algorithm seperate from the model"
      },
      {
        "date": "2024-01-13T09:33:00.000Z",
        "voteCount": 1,
        "content": "C \nThis option allows you to leverage the power and simplicity of TensorFlow to preprocess and transform the data with simple Python code"
      },
      {
        "date": "2024-01-12T14:45:00.000Z",
        "voteCount": 1,
        "content": "BigQuery can do both transformations https://cloud.google.com/bigquery/docs/manual-preprocessing#numerical_functions"
      },
      {
        "date": "2024-01-10T08:43:00.000Z",
        "voteCount": 1,
        "content": "BigQuery (SQL) is the easiest, cheapest approach"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 177,
    "url": "https://www.examtopics.com/discussions/google/view/130839-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You have created a Vertex AI pipeline that includes two steps. The first step preprocesses 10 TB data completes in about 1 hour, and saves the result in a Cloud Storage bucket. The second step uses the processed data to train a model. You need to update the model\u2019s code to allow you to test different algorithms. You want to reduce pipeline execution time and cost while also minimizing pipeline changes. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a pipeline parameter and an additional pipeline step. Depending on the parameter value, the pipeline step conducts or skips data preprocessing, and starts model training.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate another pipeline without the preprocessing step, and hardcode the preprocessed Cloud Storage file location for model training.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a machine with more CPU and RAM from the compute-optimized machine family for the data preprocessing step.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable caching for the pipeline job, and disable caching for the model training step.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 9,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-04-21T18:30:00.000Z",
        "voteCount": 1,
        "content": "Caching Preprocessed Data: Since the preprocessed data (10 TB) is the same for different model training runs, enabling caching allows Vertex AI to reuse it for subsequent pipeline executions. This significantly reduces execution time and cost, especially for large datasets.\nDisabling Model Training Cache: Model training is typically non-deterministic due to factors like random initialization. Caching the model training step could lead to stale models and inaccurate results. Disabling caching ensures the model is re-trained each time with potentially updated code for different algorithms."
      },
      {
        "date": "2024-04-21T04:38:00.000Z",
        "voteCount": 1,
        "content": "agree with guilhermebutzke"
      },
      {
        "date": "2024-02-09T11:18:00.000Z",
        "voteCount": 3,
        "content": "According to this documentation cited: https://cloud.google.com/vertex-ai/docs/pipelines/configure-caching\n\nit is possible to write a pipeline setting True or False for each task component, like this:\n\n# Model training step with caching disabled\ntrain_model_task = train_model_op()\ntrain_model_task.set_caching_options(False)  # Disable caching for this step\n\n# Model training step depends on the preprocessing step\ntrain_model_task.after(preprocess_task)\n\nSo, with this, letter D is the best option. Furthermore, letter A and, Adding a pipeline parameter and an additional pipeline step introduces unnecessary complexity when caching can handle conditional execution efficiently and in letter C, configuring a machine with more CPU and RAM for preprocessing does not address the goal of minimizing pipeline changes and reducing execution time/cost effectively."
      },
      {
        "date": "2024-01-11T04:44:00.000Z",
        "voteCount": 4,
        "content": "Not A. Adding a pipeline parameter and new pipeline steps does not minimise pipeline changes.\n\nNot C. The idea is not to re-run the preprocessing step at all.\n\nNot B. Creating a whole new pipeline implies a significant investment of effort.\n\nI opt for D: Enabling caching only for preprocessing job (although it says \u201cpipeline job\u201d in the option, I think that is a typo). Quoting Vertex AI docs: \u201cIf there is a matching execution in Vertex ML Metadata, the outputs of that execution are used and the step is skipped. This helps to reduce costs by skipping computations that were completed in a previous pipeline run.\u201d https://cloud.google.com/vertex-ai/docs/pipelines/configure-caching"
      },
      {
        "date": "2024-01-11T03:24:00.000Z",
        "voteCount": 1,
        "content": "The pipeline already generates the preprocessed dataset and stores, there's no need to preprocess again for another model"
      },
      {
        "date": "2024-01-12T19:11:00.000Z",
        "voteCount": 1,
        "content": "rereading the question, I agree with  b1a8fae that its D"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 178,
    "url": "https://www.examtopics.com/discussions/google/view/130840-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You work for a bank. You have created a custom model to predict whether a loan application should be flagged for human review. The input features are stored in a BigQuery table. The model is performing well, and you plan to deploy it to production. Due to compliance requirements the model must provide explanations for each prediction. You want to add this functionality to your model code with minimal effort and provide explanations that are as accurate as possible. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AutoML tabular model by using the BigQuery data with integrated Vertex Explainable AI.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a BigQuery ML deep neural network model and use the ML.EXPLAIN_PREDICT method with the num_integral_steps parameter.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpload the custom model to Vertex AI Model Registry and configure feature-based attribution by using sampled Shapley with input baselines.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the custom serving container to include sampled Shapley-based explanations in the prediction outputs."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 9,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-04-21T18:42:00.000Z",
        "voteCount": 1,
        "content": "Existing Custom Model: This approach leverages your already-developed, well-performing model. There's no need to rebuild it using AutoML or BigQuery ML, which might require significant code changes.\nVertex Explainable AI (XAI): Vertex AI offers XAI integration with custom models through feature-based attribution methods like sampled Shapley. This provides explanations for each prediction without requiring major modifications to your model code.\nSampled Shapley with Baselines: Sampled Shapley is a robust attribution method for explaining model predictions. Using input baselines (like zero values) helps improve the interpretability of explanations, especially for features with large ranges."
      },
      {
        "date": "2024-02-11T11:30:00.000Z",
        "voteCount": 3,
        "content": "According to the documentation at https://cloud.google.com/vertex-ai/docs/explainable-ai/overview, we can utilize both feature-based attribution and sampled Shapley-based explanations. Therefore, for providing explanations for each prediction in a loan classification problem, I believe that feature-based attribution is the optimal approach. Furthermore, updating the custom serving container to include sampled Shapley-based explanations, as suggested in option D, might require more effort, considering that the custom model deployed on Vertex AI already provides this option for explanations."
      },
      {
        "date": "2024-02-04T06:06:00.000Z",
        "voteCount": 2,
        "content": "\"minimal effort and provide explanations that are as accurate as possible\"\nthis makes the answer C, based on this:\nhttps://cloud.google.com/vertex-ai/docs/explainable-ai/improving-explanations"
      },
      {
        "date": "2024-01-31T02:53:00.000Z",
        "voteCount": 3,
        "content": "Feature attribution is supported for all types of models (both AutoML and custom-trained), frameworks (TensorFlow, scikit, XGBoost), BigQuery ML models, and modalities (images, text, tabular, video).\nhttps://cloud.google.com/vertex-ai/docs/explainable-ai/overview"
      },
      {
        "date": "2024-01-13T09:42:00.000Z",
        "voteCount": 2,
        "content": "C\nyou find the answer here https://cloud.google.com/vertex-ai/docs/explainable-ai/overview"
      },
      {
        "date": "2024-01-12T07:46:00.000Z",
        "voteCount": 1,
        "content": "pikachu007 answer made me reconsider"
      },
      {
        "date": "2024-01-31T02:52:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/vertex-ai/docs/explainable-ai/overview. According to this web link, Feature attribution is supported for all types of models (both AutoML and custom-trained), frameworks (TensorFlow, scikit, XGBoost), BigQuery ML models, and modalities (images, text, tabular, video)."
      },
      {
        "date": "2024-01-11T04:52:00.000Z",
        "voteCount": 1,
        "content": "Not a deep neural network for sure (B). Out of the remaining 3, A is the simplest approach."
      },
      {
        "date": "2024-01-11T03:30:00.000Z",
        "voteCount": 1,
        "content": "A and B is out because you already have a model, C does not provide an explanation for each prediction. Therefore D meets all the criteria."
      },
      {
        "date": "2024-01-21T15:48:00.000Z",
        "voteCount": 1,
        "content": "Why does not C  provide an explanation for each prediction? As for me both C and D options provide an explanation for each prediction, the difference is only in the amount of effort required to configure explanations"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 179,
    "url": "https://www.examtopics.com/discussions/google/view/130625-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You recently used XGBoost to train a model in Python that will be used for online serving. Your model prediction service will be called by a backend service implemented in Golang running on a Google Kubernetes Engine (GKE) cluster. Your model requires pre and postprocessing steps. You need to implement the processing steps so that they run at serving time. You want to minimize code changes and infrastructure maintenance, and deploy your model into production as quickly as possible. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse FastAPI to implement an HTTP server. Create a Docker image that runs your HTTP server, and deploy it on your organization\u2019s GKE cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse FastAPI to implement an HTTP server. Create a Docker image that runs your HTTP server, Upload the image to Vertex AI Model Registry and deploy it to a Vertex AI endpoint.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Predictor interface to implement a custom prediction routine. Build the custom container, upload the container to Vertex AI Model Registry and deploy it to a Vertex AI endpoint.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the XGBoost prebuilt serving container when importing the trained model into Vertex AI. Deploy the model to a Vertex AI endpoint. Work with the backend engineers to implement the pre- and postprocessing steps in the Golang backend service."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 8,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-31T03:26:00.000Z",
        "voteCount": 6,
        "content": "Use the Predictor interface to implement a custom prediction routine. This allows you to include the preprocessing and postprocessing steps in the same deployment package as your model.\n\nBuild the custom container, which packages your model and the associated preprocessing and postprocessing code together, simplifying deployment.\n\nUpload the container to Vertex AI Model Registry. This makes your model available for deployment on Vertex AI.\n\nDeploy it to a Vertex AI endpoint. This allows your model to be used for online serving.\n\nhttps://blog.thecloudside.com/custom-predict-routines-in-vertex-ai-46a7473c95db"
      },
      {
        "date": "2024-07-02T08:00:00.000Z",
        "voteCount": 1,
        "content": "This approach minimizes code changes and infrastructure maintenance by leveraging Vertex AI's managed services for deployment. Implementing the preprocessing and postprocessing steps in a FastAPI server within a Docker container allows you to handle these steps at serving time efficiently. Deploying this Docker image to a Vertex AI endpoint simplifies the deployment process and reduces the burden of managing the infrastructure."
      },
      {
        "date": "2024-06-21T15:25:00.000Z",
        "voteCount": 1,
        "content": "Option C is a good choice if\nYou have specific requirements for preprocessing or postprocessing that can't be met by the prebuilt XGBoost serving container.\nYou need more control over the deployment process or want to integrate with other services.\nYou're comfortable building and managing custom containers.\nHowever, if you just want a simple, straightforward way to deploy your model as a RESTful API, Option D (using the XGBoost prebuilt serving container) might be a better fit!"
      },
      {
        "date": "2024-05-27T01:47:00.000Z",
        "voteCount": 1,
        "content": "FastAPI allows to create a lightweight HTTP server with minimal code."
      },
      {
        "date": "2024-03-13T05:47:00.000Z",
        "voteCount": 1,
        "content": "\u3057\n\nPre-built XGBoost container already includes pre- and postprocessing steps."
      },
      {
        "date": "2024-02-11T13:02:00.000Z",
        "voteCount": 2,
        "content": "My answer C:\n\nConsidering pre- and postprocessing implementation, The option C directly deals with implementing the processing steps in a custom container,&nbsp;offering full control over their placement and execution. \n\nThis documentation says: \u201cCustom prediction routines (CPR) lets you build&nbsp;[custom containers](https://cloud.google.com/vertex-ai/docs/predictions/use-custom-container)&nbsp;with pre/post processing code easily, without dealing with the details of setting up an HTTP server or building a container from scratch.\u201d\nhttps://cloud.google.com/vertex-ai/docs/predictions/custom-prediction-routines\n\nSo, it is better to use C instead of A or B. D is better because it offers the option of pre and post-processing, which is not available in D due to its use of prebuilt serving."
      },
      {
        "date": "2024-01-13T09:53:00.000Z",
        "voteCount": 4,
        "content": "C\n. Build the custom\ncontainer, upload the container to Vertex AI Model Registry, and deploy it to a Vertex AI endpoint.\nThis option allows you to leverage the power and simplicity of Vertex AI to serve your XGBoost model\nwith minimal effort and customization. Vertex AI is a unified platform for building and deploying\nmachine learning solutions on Google Cloud. Vertex AI can deploy a trained XGBoost model to an online prediction endpoint, which can provide low-latency predictions for individual instances. A custom prediction routine (CPR) is a Python script that defines the logic for preprocessing the input data, running the prediction, and postprocessing the output data."
      },
      {
        "date": "2024-01-11T03:34:00.000Z",
        "voteCount": 1,
        "content": "Considering the goal of minimizing code changes, infrastructure maintenance, and quickly deploying the model into production, option D seems to be a pragmatic approach. It leverages the prebuilt XGBoost serving container in Vertex AI, providing a managed environment for serving. The pre- and postprocessing steps can be implemented in the Golang backend service, maintaining consistency with the existing Golang implementation and reducing the need for significant code changes."
      },
      {
        "date": "2024-01-08T12:06:00.000Z",
        "voteCount": 1,
        "content": "I would say D"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 180,
    "url": "https://www.examtopics.com/discussions/google/view/130841-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You recently deployed a pipeline in Vertex AI Pipelines that trains and pushes a model to a Vertex AI endpoint to serve real-time traffic. You need to continue experimenting and iterating on your pipeline to improve model performance. You plan to use Cloud Build for CI/CD You want to quickly and easily deploy new pipelines into production, and you want to minimize the chance that the new pipeline implementations will break in production. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up a CI/CD pipeline that builds and tests your source code. If the tests are successful, use the Google. Cloud console to upload the built container to Artifact Registry and upload the compiled pipeline to Vertex AI Pipelines.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up a CI/CD pipeline that builds your source code and then deploys built artifacts into a pre-production environment. Run unit tests in the pre-production environment. If the tests are successful deploy the pipeline to production.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up a CI/CD pipeline that builds and tests your source code and then deploys built artifacts into a pre-production environment. After a successful pipeline run in the pre-production environment, deploy the pipeline to production.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up a CI/CD pipeline that builds and tests your source code and then deploys built artifacts into a pre-production environment. After a successful pipeline run in the pre-production environment, rebuild the source code and deploy the artifacts to production."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-21T18:50:00.000Z",
        "voteCount": 1,
        "content": "CI/CD Pipeline: This automates the build, test, and deployment process, enabling faster iterations and reducing manual errors.\nPre-production Environment: Deploying to a pre-production environment (staging) allows you to test the new pipeline functionality with simulated real-world data. This helps identify and fix potential issues before impacting production.\nSuccessful Pipeline Run: Verifying a successful run in the pre-production environment provides confidence that the new pipeline functions as expected."
      },
      {
        "date": "2024-04-07T06:54:00.000Z",
        "voteCount": 1,
        "content": "Unit test is insufficient, there should be a pipeline run."
      },
      {
        "date": "2024-01-31T03:44:00.000Z",
        "voteCount": 2,
        "content": "C. \nPre-production environment: Deploying to a pre-production environment before production allows you to thoroughly test the new pipeline's functionality and performance without affecting real-time traffic.\nSuccessful pipeline run: This ensures the entire pipeline executes correctly in the pre-production environment, including training, model pushing, and endpoint deployment.\nNo rebuild in production: Rebuilding the source code after a successful pre-production run is unnecessary and adds an extra step that could potentially introduce new errors."
      },
      {
        "date": "2024-01-13T09:57:00.000Z",
        "voteCount": 1,
        "content": "C\nThe best option for continuing experimenting and iterating on your pipeline to improve model\nperformance, using Cloud Build for CI/CD, and deploying new pipelines into production quickly and easily, is to set up a CI/CD pipeline that builds and tests your source code and then deploys built artifacts into a pre-production environment. After a successful pipeline run in the pre-production environment, deploy the pipeline to production. This option allows you to leverage the power and simplicity of Cloud Build to automate, monitor, and manage your pipeline development and deployment workflow."
      },
      {
        "date": "2024-01-11T03:45:00.000Z",
        "voteCount": 2,
        "content": "C. Set up a CI/CD pipeline that builds and tests your source code and then deploys built artifacts into a pre-production environment. After a successful pipeline run in the pre-production environment, deploy the pipeline to production.\n\nA - Does not have pre-production environment. \nB - Unit test is insufficient, there should be a pipeline run.\nD - (Uncertain) but there's shouldn't be a rebuilding as you have already built and tested successfully, feels redundant to rebuild."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 181,
    "url": "https://www.examtopics.com/discussions/google/view/130842-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You work for a bank with strict data governance requirements. You recently implemented a custom model to detect fraudulent transactions. You want your training code to download internal data by using an API endpoint hosted in your project\u2019s network. You need the data to be accessed in the most secure way, while mitigating the risk of data exfiltration. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable VPC Service Controls for peerings, and add Vertex AI to a service perimeter.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Cloud Run endpoint as a proxy to the data. Use Identity and Access Management (IAM) authentication to secure access to the endpoint from the training job.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure VPC Peering with Vertex AI, and specify the network of the training job.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDownload the data to a Cloud Storage bucket before calling the training job."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 8,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 6,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-08-13T04:47:00.000Z",
        "voteCount": 1,
        "content": "VPC Service Controls: This feature allows you to define network boundaries (service perimeters) and control the flow of data between services. By adding Vertex AI to a service perimeter, you can restrict its access to only the necessary resources, including the API endpoint. &nbsp; \n\nWith peerings you can enable secure communication between your VPC and the VPC where Vertex AI is running, ensuring data stays within your network boundary."
      },
      {
        "date": "2024-06-25T11:27:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2024-04-19T22:42:00.000Z",
        "voteCount": 3,
        "content": "It's literally written in the description of this service: avoid data exfiltration."
      },
      {
        "date": "2024-04-19T12:35:00.000Z",
        "voteCount": 2,
        "content": "Security: Cloud Run offers a secure environment to run your proxy code. IAM authentication ensures only authorized training jobs have access to the data endpoint.\nData Minimization: The proxy can potentially filter or transform data before sending it to the training code, reducing the amount of sensitive information exposed.\nNetwork Isolation: The proxy acts as an additional layer of isolation between the training code and the internal data source."
      },
      {
        "date": "2024-04-19T12:36:00.000Z",
        "voteCount": 1,
        "content": "A. VPC Service Controls: While VPC Service Controls offer network segmentation, they wouldn't directly address data exfiltration risk from the training code itself.\nC. VPC Peering: VPC Peering allows communication between networks but doesn't provide access control mechanisms like IAM.\nD. Downloading to Cloud Storage: This approach creates an unnecessary data transfer step and doesn't address the risk of the training code potentially leaking data after download."
      },
      {
        "date": "2024-04-20T05:08:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/vpc-service-controls/docs/overview#how-vpc-service-controls-works"
      },
      {
        "date": "2024-04-13T05:04:00.000Z",
        "voteCount": 1,
        "content": "To mitigate data exfiltration risks, your organization might also want to ensure secure data exchange across organizational boundaries with fine-grained controls. As an administrator, you might want to ensure the following:\n\nClients with privileged access don't also have access to partner resources.\nClients with access to sensitive data can only read public data sets but not write to them"
      },
      {
        "date": "2024-02-20T21:42:00.000Z",
        "voteCount": 2,
        "content": "It should be A, VPC service controls can reduce data exfiltration risks. \nhttps://cloud.google.com/vpc-service-controls/docs/overview"
      },
      {
        "date": "2024-02-18T08:33:00.000Z",
        "voteCount": 3,
        "content": "My Answer B:\nCreating a Cloud Run endpoint as a proxy to the data allows you to control access to the internal data through an API endpoint. By using IAM authentication, you can enforce strict access controls, ensuring that only authorized entities (such as your training job) can access the data. This approach helps mitigate the risk of data exfiltration by providing a secure and controlled access point to the internal data.\n- Option A: may help control access within Google Cloud Platform services, but it does not directly address securing access to the internal data through an API endpoint.\n- Option C:  is more about network configurations and does not provide a solution for securely accessing the internal data through an API endpoint.\n- Option D: transferring the data to a Cloud Storage bucket, which might introduce additional security risks during the data transfer process."
      },
      {
        "date": "2024-02-18T08:31:00.000Z",
        "voteCount": 3,
        "content": "My Answer B:\nCreating a Cloud Run endpoint as a proxy to the data allows you to control access to the internal data through an API endpoint. By using Identity and Access Management (IAM) authentication, you can enforce strict access controls, ensuring that only authorized entities (such as your training job) can access the data. This approach helps mitigate the risk of data exfiltration by providing a secure and controlled access point to the internal data.\n\n- Option A: may help control access within Google Cloud Platform services, but it does not directly address securing access to the internal data through an API endpoint.\n- Option C:  is more about network configurations and does not provide a solution for securely accessing the internal data through an API endpoint.\n- Option D: involves transferring the data to a Cloud Storage bucket, which might introduce additional security risks during the data transfer process."
      },
      {
        "date": "2024-01-31T03:59:00.000Z",
        "voteCount": 1,
        "content": "A. https://cloud.google.com/security/vpc-service-controls?hl=en\nThe first benefit on the official google cloud site is \"Mitigate data exfiltration risks\"\nHere's why:\n\nVPC Service Controls: This powerful tool allows you to restrict the network connectivity of resources within your VPC network. By enabling it for peerings, you can control which services within your project can access specific internal resources.\n\nService perimeter: Adding Vertex AI to a service perimeter further restricts its access to only approved internal resources, including the API endpoint for your bank's data. This creates a secure zone where your model training can happen without jeopardizing sensitive data."
      },
      {
        "date": "2024-01-31T01:38:00.000Z",
        "voteCount": 1,
        "content": "I will go with A."
      },
      {
        "date": "2024-01-11T03:50:00.000Z",
        "voteCount": 1,
        "content": "It provides a controlled and secure way to allow the training job to access the necessary data while adhering to strict data governance requirements."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 182,
    "url": "https://www.examtopics.com/discussions/google/view/130843-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You are deploying a new version of a model to a production Vertex Al endpoint that is serving traffic. You plan to direct all user traffic to the new model. You need to deploy the model with minimal disruption to your application. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a new endpoint<br>2. Create a new model. Set it as the default version. Upload the model to Vertex AI Model Registry<br>3. Deploy the new model to the new endpoint<br>4. Update Cloud DNS to point to the new endpoint",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a new endpoint<br>2. Create a new model. Set the parentModel parameter to the model ID of the currently deployed model and set it as the default version. Upload the model to Vertex AI Model Registry<br>3. Deploy the new model to the new endpoint, and set the new model to 100% of the traffic.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a new model. Set the parentModel parameter to the model ID of the currently deployed model. Upload the model to Vertex AI Model Registry.<br>2. Deploy the new model to the existing endpoint, and set the new model to 100% of the traffic\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a new model. Set it as the default version. Upload the model to Vertex AI Model Registry<br>2. Deploy the new model to the existing endpoint"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 8,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-04-19T12:38:00.000Z",
        "voteCount": 2,
        "content": "Minimal Downtime: By deploying the new model to the existing endpoint, you avoid any service interruptions caused by creating and switching to a completely new endpoint.\nVersioning: Setting the parentModel parameter allows you to track the lineage of your models and easily revert to the previous version if needed.\nTraffic Control: Vertex AI lets you control traffic allocation between different versions of a model deployed on the same endpoint. Setting the new model to 100% traffic directs all user requests to the new version."
      },
      {
        "date": "2024-04-19T12:39:00.000Z",
        "voteCount": 1,
        "content": "A. Creating a New Endpoint: This approach introduces downtime as you need to switch DNS records to point to the new endpoint.\nB. Creating a New Endpoint with Default Version: While using a parentModel helps with versioning, creating a new endpoint still leads to service disruption.\nD. Deploying to Existing Endpoint Without Traffic Control: This might cause unexpected behavior if the new model isn't ready for production traffic."
      },
      {
        "date": "2024-02-18T08:41:00.000Z",
        "voteCount": 2,
        "content": "My Answer: C\n\n In the context of deploying machine learning models, setting the **`parentModel`** parameter to the model ID of the currently deployed model means that the new model being deployed is created as a child model or an iteration of the existing model. This allows the new model to inherit certain properties or characteristics from the existing model, such as the architecture, hyperparameters, or feature transformations.\n\nCreate a new Endpoint is Unnecessary."
      },
      {
        "date": "2024-02-04T06:25:00.000Z",
        "voteCount": 1,
        "content": "Optionally set this model as the default version. The default version is preselected whenever the model is used for prediction (although you can still select other versions).\nhttps://cloud.google.com/vertex-ai/docs/model-registry/versioning"
      },
      {
        "date": "2024-01-13T08:37:00.000Z",
        "voteCount": 1,
        "content": "a,c -creating new endpoint is an unnecessary disruption to the application\nd - doesn't work, two models are on the same endpoint and traffic is still going through the old model"
      },
      {
        "date": "2024-01-11T03:57:00.000Z",
        "voteCount": 3,
        "content": "Leverages existing endpoint: Using the same endpoint maintains the same endpoint URL, avoiding DNS updates and potential service interruptions.\nGradual traffic transition: Vertex AI allows you to gradually shift traffic between model versions, ensuring a smooth transition without impacting users.\nClear versioning: Setting parentModel establishes a relationship between the new model and the existing one, aiding in organization and tracking model lineage."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 183,
    "url": "https://www.examtopics.com/discussions/google/view/130844-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You are training an ML model on a large dataset. You are using a TPU to accelerate the training process. You notice that the training process is taking longer than expected. You discover that the TPU is not reaching its full capacity. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the learning rate",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the number of epochs",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDecrease the learning rate",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the batch size\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-19T12:42:00.000Z",
        "voteCount": 2,
        "content": "A common reason for underutilized TPUs is a small batch size. TPUs are designed for high throughput, and feeding them small batches doesn't leverage their full potential.\nTry increasing the batch size while monitoring model performance. A larger batch size can lead to faster training but might also affect accuracy. Experiment to find the optimal balance."
      },
      {
        "date": "2024-01-13T10:14:00.000Z",
        "voteCount": 1,
        "content": "D\ntaking big batch size allows to use more memory and decrease the train time"
      },
      {
        "date": "2024-01-13T08:46:00.000Z",
        "voteCount": 1,
        "content": "Batch size is too small because of sharding\nhttps://cloud.google.com/tpu/docs/performance-guide"
      },
      {
        "date": "2024-01-11T03:58:00.000Z",
        "voteCount": 2,
        "content": "D, the bigger the batch size, the more resource is taken up"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 184,
    "url": "https://www.examtopics.com/discussions/google/view/130628-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You work for a retail company. You have a managed tabular dataset in Vertex AI that contains sales data from three different stores. The dataset includes several features, such as store name and sale timestamp. You want to use the data to train a model that makes sales predictions for a new store that will open soon. You need to split the data between the training, validation, and test sets. What approach should you use to split the data?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Vertex AI manual split, using the store name feature to assign one store for each set",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Vertex AI default data split",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Vertex AI chronological split, and specify the sales timestamp feature as the time variable\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Vertex AI random split, assigning 70% of the rows to the training set, 10% to the validation set, and 20% to the test set"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 8,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-04-19T12:46:00.000Z",
        "voteCount": 3,
        "content": "Time-Series Data: Your sales data has timestamps, indicating it's time-series data. A chronological split considers the order of the timestamps, ensuring the model is trained on historical trends.\nPredicting for New Store: Since you want to predict sales for a new store, a chronological split is better than a random split (option D) which wouldn't prioritize recent trends.\nVertex AI Functionality: Vertex AI's chronological split functionality is specifically designed for time-series data and leverages the timestamp feature you provide to separate data for training, validation, and testing."
      },
      {
        "date": "2024-04-19T12:47:00.000Z",
        "voteCount": 1,
        "content": "A. Manual Split by Store: While this might work, it doesn't consider the time element crucial for sales predictions. The new store's performance might not be well-represented by data from a single existing store.\nB. Default Split (Random): The default random split in Vertex AI might not prioritize recent data which could be more relevant for predicting sales in the new store.\nD. Random Split with Specific Ratios: Similar to the default split, a random approach might not capture the time-series aspect and recent trends that are important for your new store predictions."
      },
      {
        "date": "2024-02-18T11:15:00.000Z",
        "voteCount": 2,
        "content": "My answer C: \n\nA: Not Correct: &nbsp;Splitting based on store name wouldn't guarantee temporal separation of data. Furthermore, for this problem is note to assign one store for each set, because the target is for a new store. \n\nB: Not Correct: &nbsp;Randomly choosing data points across different time periods could lead to the model not capturing seasonal trends or temporal patterns effectively.\n\nC: CORRECT:  it leverages the chronological nature of the data. Since the dataset contains sales data over time from different stores, using a chronological split ensures that the model is trained on data from earlier time periods and validated/tested on more recent data.\n\nD: Not Correct: Similar to B,&nbsp;a custom random split wouldn't ensure temporal separation and could lead to issues with capturing temporal trends."
      },
      {
        "date": "2024-01-14T19:46:00.000Z",
        "voteCount": 1,
        "content": "I agree with b1a8fae"
      },
      {
        "date": "2024-01-13T09:26:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/automl-tables/docs/data-best-practices#time"
      },
      {
        "date": "2024-01-12T07:52:00.000Z",
        "voteCount": 1,
        "content": "Anything different than option C could potentially lead to data leakage imo."
      },
      {
        "date": "2024-01-11T04:03:00.000Z",
        "voteCount": 1,
        "content": "By using a manual split based on store names, you can train a model that is more sensitive to the unique characteristics of each store, ultimately leading to better predictions for the new store."
      },
      {
        "date": "2024-01-08T12:19:00.000Z",
        "voteCount": 1,
        "content": "I say C , time-based splitting is always suggest"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 185,
    "url": "https://www.examtopics.com/discussions/google/view/130634-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You have developed a BigQuery ML model that predicts customer chum, and deployed the model to Vertex AI Endpoints. You want to automate the retraining of your model by using minimal additional code when model feature values change. You also want to minimize the number of times that your model is retrained to reduce training costs. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1 Enable request-response logging on Vertex AI Endpoints<br>2. Schedule a TensorFlow Data Validation job to monitor prediction drift<br>3. Execute model retraining if there is significant distance between the distributions",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Enable request-response logging on Vertex AI Endpoints<br>2. Schedule a TensorFlow Data Validation job to monitor training/serving skew<br>3. Execute model retraining if there is significant distance between the distributions",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a Vertex AI Model Monitoring job configured to monitor prediction drift<br>2. Configure alert monitoring to publish a message to a Pub/Sub queue when a monitoring alert is detected<br>3. Use a Cloud Function to monitor the Pub/Sub queue, and trigger retraining in BigQuery",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a Vertex AI Model Monitoring job configured to monitor training/serving skew<br>2. Configure alert monitoring to publish a message to a Pub/Sub queue when a monitoring alert is detected<br>3. Use a Cloud Function to monitor the Pub/Sub queue, and trigger retraining in BigQuery\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 17,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 10,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-02-18T11:40:00.000Z",
        "voteCount": 6,
        "content": "My answer: D\n\nGiven the emphasis on \"model feature values change\" in the question, the most suitable option would be D.\n\nAlthough option C involves monitoring prediction drift, which may indirectly capture changes in feature values, option D directly addresses the need to monitor training/serving skew. By detecting discrepancies between the training and serving data distributions, option D is more aligned with the requirement to automate retraining when model feature values change. Therefore, option D is the most suitable choice in this context."
      },
      {
        "date": "2024-06-06T13:20:00.000Z",
        "voteCount": 3,
        "content": "Skew should be detected at the beginning of the productionalisation of the model -&gt; skew test the training data Vs the real data -&gt; a skew indicates you trained in a dataset that is not alined with your data that you have in input\n\nDrift is used when the model works well at the beginning, but the world change and the data input changes -&gt; drift is more long term\n\nhere it is a drift issue"
      },
      {
        "date": "2024-06-30T05:56:00.000Z",
        "voteCount": 1,
        "content": "Agreed"
      },
      {
        "date": "2024-04-30T04:42:00.000Z",
        "voteCount": 1,
        "content": "if the model training is done through bigquery ML, we don't have access to the training data after export, so I don't understand how training/serving skew can be applied. Can someone who is voting in favour of D clarify?"
      },
      {
        "date": "2024-04-16T22:48:00.000Z",
        "voteCount": 1,
        "content": "I go with D"
      },
      {
        "date": "2024-04-07T07:07:00.000Z",
        "voteCount": 1,
        "content": "It's D"
      },
      {
        "date": "2024-04-23T08:11:00.000Z",
        "voteCount": 1,
        "content": "see  guilhermebutzke"
      },
      {
        "date": "2024-02-15T16:37:00.000Z",
        "voteCount": 3,
        "content": "changed my mind it's D"
      },
      {
        "date": "2024-02-09T12:12:00.000Z",
        "voteCount": 2,
        "content": "I go with C but D is pretty similar.\n\nC -&gt; Prediction drift (When the overall distribution of predictions changes significantly between training and serving data).\n\nD -&gt; Training/serving skew (When the distribution of specific features between training and serving data differs significantly)."
      },
      {
        "date": "2024-02-15T16:33:00.000Z",
        "voteCount": 1,
        "content": "It's D"
      },
      {
        "date": "2024-01-31T07:18:00.000Z",
        "voteCount": 2,
        "content": "Option C: \n\nThis option directly addresses your requirements:\nVertex AI Model Monitoring: It allows efficient monitoring of prediction drift through metrics like Mean Squared Error or AUC-ROC.\nPub/Sub alerts: Alert triggers notification upon significant drift, minimizing unnecessary retraining.\nCloud Function: It reacts to Pub/Sub messages and triggers retraining in BigQuery using minimal additional code."
      },
      {
        "date": "2024-01-15T04:36:00.000Z",
        "voteCount": 2,
        "content": "After reconsidering, I think it is C:\n- No need to use TF to enable model monitoring as stated here: https://cloud.google.com/vertex-ai/docs/model-monitoring/using-model-monitoring\n(even if it uses it under the hood: https://cloud.google.com/vertex-ai/docs/model-monitoring/overview#calculating-skew-and-drift)\n\n- The problem speaks about alerting of model feature changes, which happens over time, and uses a baseline of the historical production data -&gt; prediction skew. (if the problem specified that it changes compared to training data, then it would be training-skew) (https://cloud.google.com/vertex-ai/docs/model-monitoring/monitor-explainable-ai#feature_attribution_training-serving_skew_and_prediction_drift)"
      },
      {
        "date": "2024-01-15T02:01:00.000Z",
        "voteCount": 4,
        "content": "I would avoid using TensorFlow validation to minimize code written. That leaves us with options C and D. Now, since it is the values of the features that we want to flag and not the value of the predictions, this sounds more like training-serving skew situation than prediction drift. Hence, I would go for D."
      },
      {
        "date": "2024-01-14T04:41:00.000Z",
        "voteCount": 1,
        "content": "i've changed my mind) it's D\nhttps://www.evidentlyai.com/blog/machine-learning-monitoring-data-and-concept-drift"
      },
      {
        "date": "2024-01-14T03:50:00.000Z",
        "voteCount": 1,
        "content": "we might need to retrain if the feature data distribution in the production and training are significantly different(training/serving skew). Prediction drift occurs when feature data distribution in production changes significantly over time. Should we retrain our model every time when we meet prediction drift? I dont think so, better to analyze why this drift happens.\nhttps://cloud.google.com/vertex-ai/docs/model-monitoring/overview#considerations"
      },
      {
        "date": "2024-01-13T10:27:00.000Z",
        "voteCount": 2,
        "content": "C\nThe best option for automating the retraining of your model by using minimal additional code when model feature values change, and minimizing the number of times that your model is retrained to reduce training costs, is to create a Vertex AI Model Monitoring job configured to monitor prediction drift, configure alert monitoring to publish a message to a Pub/Sub queue when a monitoring alert is detected, and use a Cloud Function to monitor the Pub/Sub queue, and trigger retraining in\nBigQuery. This option allows you to leverage the power and simplicity of Vertex AI, Pub/Sub, and Cloud Functions to monitor your model performance and retrain your model when needed. Vertex AI is a unified platform for building and deploying machine learning solutions on Google Cloud."
      },
      {
        "date": "2024-01-11T04:05:00.000Z",
        "voteCount": 1,
        "content": "A and B: TensorFlow Data Validation jobs require more setup and maintenance, and they might not integrate as seamlessly with Vertex AI Endpoints for automated retraining.\nD: Monitoring training/serving skew focuses on differences between training and deployment environments, which might not directly address feature value changes."
      },
      {
        "date": "2024-01-08T12:30:00.000Z",
        "voteCount": 2,
        "content": "I go with : C. \n1. Create a Vertex AI Model Monitoring job configured to monitor prediction drift - &gt; if the modle is already in production we have to considet Prediction drift\n2. Configure alert monitoring to publish a message to a Pub/Sub queue when a monitoring alert is detected -&gt; set Pub/Sub notification channels.\n3. Use a Cloud Function to monitor the Pub/Sub queue, and trigger retraining in BigQuery -&gt; to eimport new data in BQ"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 186,
    "url": "https://www.examtopics.com/discussions/google/view/130635-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You have been tasked with deploying prototype code to production. The feature engineering code is in PySpark and runs on Dataproc Serverless. The model training is executed by using a Vertex AI custom training job. The two steps are not connected, and the model training must currently be run manually after the feature engineering step finishes. You need to create a scalable and maintainable production process that runs end-to-end and tracks the connections between steps. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Vertex AI Workbench notebook. Use the notebook to submit the Dataproc Serverless feature engineering job. Use the same notebook to submit the custom model training job. Run the notebook cells sequentially to tie the steps together end-to-end.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Vertex AI Workbench notebook. Initiate an Apache Spark context in the notebook and run the PySpark feature engineering code. Use the same notebook to run the custom model training job in TensorFlow. Run the notebook cells sequentially to tie the steps together end-to-end.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Kubeflow pipelines SDK to write code that specifies two components:<br>- The first is a Dataproc Serverless component that launches the feature engineering job<br>- The second is a custom component wrapped in the create_custom_training_job_from_component utility that launches the custom model training job<br>Create a Vertex AI Pipelines job to link and run both components\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Kubeflow pipelines SDK to write code that specifies two components<br>- The first component initiates an Apache Spark context that runs the PySpark feature engineering code<br>- The second component runs the TensorFlow custom model training code<br>Create a Vertex AI Pipelines job to link and run both components."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-05-26T06:21:00.000Z",
        "voteCount": 1,
        "content": "The first is a Dataproc Serverless component that launches the feature engineering job\nThe second is a custom component wrapped in the create_custom_training_job_from_component utility that launches the custom model training job\nCreate a Vertex AI Pipelines job to link and run both components"
      },
      {
        "date": "2024-04-21T01:09:00.000Z",
        "voteCount": 1,
        "content": "The first is a Dataproc Serverless component that launches the feature engineering job\nThe second is a custom component wrapped in the create_custom_training_job_from_component utility that launches the custom model training job\nCreate a Vertex AI Pipelines job to link and run both components"
      },
      {
        "date": "2024-04-21T01:09:00.000Z",
        "voteCount": 2,
        "content": "A. Vertex AI Workbench notebook: While notebooks are a good way to prototype workflows, they are not ideal for production due to limitations in scalability and version control. Running everything sequentially also doesn't allow for potential parallelization of tasks.\nB. Apache Spark context in notebook: Similar to A, notebooks are not ideal for production. Additionally, running the model training with TensorFlow within the notebook ties the process to a specific framework, making it less flexible.\nD. Kubeflow pipelines with Spark context: This option gets close, but it's unnecessary to initiate a Spark context within the first component. Dataproc Serverless already handles the Spark environment for running PySpark code."
      },
      {
        "date": "2024-02-09T07:31:00.000Z",
        "voteCount": 1,
        "content": "I went with C"
      },
      {
        "date": "2024-01-13T16:09:00.000Z",
        "voteCount": 1,
        "content": "Vote C"
      },
      {
        "date": "2024-01-13T10:31:00.000Z",
        "voteCount": 1,
        "content": "C\nThe best option for creating a scalable and maintainable production process that runs end-to-end and tracks the connections between steps, using prototype code to production, feature engineering code in PySpark that runs on Dataproc Serverless, and model training that is executed by using a Vertex AI custom training job, is to use the Kubeflow pipelines SDK to write code that specifies two components. The first is a Dataproc Serverless component that launches the feature engineering job. The second is a custom component wrapped in the create_custom_training_job_from_component utility that launches the custom model training job. This option allows you to leverage the power and simplicity of Kubeflow pipelines to orchestrate and automate your machine learning workflows on Vertex AI. Kubeflow pipelines is a platform that can build, deploy, and manage machine learning pipelines on Kubernetes."
      },
      {
        "date": "2024-01-12T19:05:00.000Z",
        "voteCount": 3,
        "content": "By using Kubeflow Pipelines, you establish a structured, scalable, and maintainable production process for end-to-end model development and deployment, ensuring proper orchestration, tracking, and integration with the chosen services."
      },
      {
        "date": "2024-01-08T12:32:00.000Z",
        "voteCount": 1,
        "content": "I go for C"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 187,
    "url": "https://www.examtopics.com/discussions/google/view/131000-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You recently deployed a scikit-learn model to a Vertex AI endpoint. You are now testing the model on live production traffic. While monitoring the endpoint, you discover twice as many requests per hour than expected throughout the day. You want the endpoint to efficiently scale when the demand increases in the future to prevent users from experiencing high latency. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy two models to the same endpoint, and distribute requests among them evenly",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an appropriate minReplicaCount value based on expected baseline traffic\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the target utilization percentage in the autoscailngMetricSpecs configuration to a higher value",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the model\u2019s machine type to one that utilizes GPUs"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 9,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-04-21T01:13:00.000Z",
        "voteCount": 3,
        "content": "Autoscaling based on baseline: Vertex AI endpoints have built-in autoscaling capabilities. Setting a minReplicaCount ensures there are always at least that many replicas running, handling the baseline traffic efficiently. When demand increases above the baseline, autoscaling will automatically provision additional replicas to maintain performance.\nEfficient scaling: This approach allows the endpoint to scale up smoothly as traffic increases, preventing sudden spikes in latency for users.\nTargeted resource allocation: Unlike option A (deploying multiple models), this method avoids redundant resources when traffic is low. Additionally, option D (switching to GPUs) might be unnecessary if the bottleneck isn't processing power."
      },
      {
        "date": "2024-04-21T01:14:00.000Z",
        "voteCount": 1,
        "content": "A. Deploying multiple models: This creates additional overhead and resource usage without directly addressing autoscaling. Traffic distribution may also not be perfectly even.\nC. Increasing target utilization: Raising the target utilization could lead to under-provisioning during peak hours, causing latency issues. It's better to set a baseline with minReplicaCount and let autoscaling handle peak loads.\nD. Switching to GPUs: While GPUs can be beneficial for computationally intensive models, it might be an unnecessary expense if the current model doesn't heavily utilize the CPU. Analyze the CPU usage before switching to a GPU-based machine type."
      },
      {
        "date": "2024-02-11T13:21:00.000Z",
        "voteCount": 2,
        "content": "My Answer B\n\nThe letter C would be the correct answer if the target were set lower to anticipate traffic spikes, not set higher as the answer says. However, considering that the minReplicaCount is now twice the known value, letter B is the most appropriate answer as it suggests considering setting a new minReplicaCount, which could be the best choice."
      },
      {
        "date": "2024-01-28T06:30:00.000Z",
        "voteCount": 4,
        "content": "Not C, if set to a higher value, it is less easier to autoscale to another instance, as it will wait the utilisation to a even higher value."
      },
      {
        "date": "2024-01-15T02:17:00.000Z",
        "voteCount": 1,
        "content": "I go with C. It calculates the number of replicas based on CPU utilization. https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform_v1.types.AutoscalingMetricSpec"
      },
      {
        "date": "2024-01-13T10:33:00.000Z",
        "voteCount": 2,
        "content": "B\nThis\noption allows you to leverage the power and simplicity of Vertex AI to automatically scale your endpoint resources according to the traffic patterns."
      },
      {
        "date": "2024-01-12T19:10:00.000Z",
        "voteCount": 1,
        "content": "c as it is dynamic"
      },
      {
        "date": "2024-01-30T05:45:00.000Z",
        "voteCount": 2,
        "content": "yes it's dynamic, but the target should be set lower, not higher, if you want to anticipate traffic spikes."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 188,
    "url": "https://www.examtopics.com/discussions/google/view/131001-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You work at a bank. You have a custom tabular ML model that was provided by the bank\u2019s vendor. The training data is not available due to its sensitivity. The model is packaged as a Vertex AI Model serving container, which accepts a string as input for each prediction instance. In each string, the feature values are separated by commas. You want to deploy this model to production for online predictions and monitor the feature distribution over time with minimal effort. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Upload the model to Vertex AI Model Registry, and deploy the model to a Vertex AI endpoint<br>2. Create a Vertex AI Model Monitoring job with feature drift detection as the monitoring objective, and provide an instance schema\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Upload the model to Vertex AI Model Registry, and deploy the model to a Vertex AI endpoint<br>2. Create a Vertex AI Model Monitoring job with feature skew detection as the monitoring objective, and provide an instance schema",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Refactor the serving container to accept key-value pairs as input format<br>2. Upload the model to Vertex AI Model Registry, and deploy the model to a Vertex AI endpoint<br>3. Create a Vertex AI Model Monitoring job with feature drift detection as the monitoring objective.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Refactor the serving container to accept key-value pairs as input format<br>2. Upload the model to Vertex AI Model Registry, and deploy the model to a Vertex AI endpoint<br>3. Create a Vertex AI Model Monitoring job with feature skew detection as the monitoring objective"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-07T07:26:00.000Z",
        "voteCount": 3,
        "content": "Training data not available -&gt; can't be skew, so it must be drift"
      },
      {
        "date": "2024-02-08T08:05:00.000Z",
        "voteCount": 1,
        "content": "I have a doubt, could someone please help with this?\nWhile \"drift\" (Option A) might imply gradual changes, \"skew\" (Option B) is more suitable for sudden shifts in feature distributions, potentially relevant for sensitive data.\nIs option B better than A?"
      },
      {
        "date": "2024-04-30T01:15:00.000Z",
        "voteCount": 4,
        "content": "Feature skew is typically used to compare the feature distribution between training data and serving data, which is not as relevant here because you do not have access to the training data. Therefore, Option B is less suitable."
      },
      {
        "date": "2024-01-15T04:41:00.000Z",
        "voteCount": 4,
        "content": "A.\nMinimum effort -&gt; ditch refactoring (hopefully not needed)\nTraining data not available -&gt; can't be skew, so it must be drift"
      },
      {
        "date": "2024-01-12T19:15:00.000Z",
        "voteCount": 1,
        "content": "Handles string input format: Vertex AI Model Monitoring can parse comma-separated feature values, avoiding the need to refactor the serving container.\n\n It directly monitors feature distribution over time, aligning with the goal of detecting potential drifts."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 189,
    "url": "https://www.examtopics.com/discussions/google/view/131005-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You are implementing a batch inference ML pipeline in Google Cloud. The model was developed using TensorFlow and is stored in SavedModel format in Cloud Storage. You need to apply the model to a historical dataset containing 10 TB of data that is stored in a BigQuery table. How should you perform the inference?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExport the historical data to Cloud Storage in Avro format. Configure a Vertex AI batch prediction job to generate predictions for the exported data",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImport the TensorFlow model by using the CREATE MODEL statement in BigQuery ML. Apply the historical data to the TensorFlow model\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExport the historical data to Cloud Storage in CSV format. Configure a Vertex AI batch prediction job to generate predictions for the exported data",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a Vertex AI batch prediction job to apply the model to the historical data in BigQuery"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 8,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 4,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-10-11T12:15:00.000Z",
        "voteCount": 1,
        "content": "My answer is D."
      },
      {
        "date": "2024-04-07T07:31:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/vertex-ai/docs/tabular-data/classification-regression/get-batch-predictions#input_data_requirements"
      },
      {
        "date": "2024-03-08T04:39:00.000Z",
        "voteCount": 2,
        "content": "The choice is between B and D, both good BUT:\nImporting and making batch predictions is quite straightforward in BQ ML\nhttps://cloud.google.com/bigquery/docs/making-predictions-with-imported-tensorflow-models\nif not pre-processing needed on the data. If we need a more complete pipeline I'd chose D, but the tables need partitioning (100GB is the limit in Vertex AI):\nhttps://cloud.google.com/vertex-ai/docs/tabular-data/classification-regression/get-batch-predictions#input_data_requirements"
      },
      {
        "date": "2024-02-18T10:32:00.000Z",
        "voteCount": 1,
        "content": "My Answer: D \n\nThe historical dataset is stored in BigQuery, which can be directly accessed by Vertex AI. Vertex AI offers batch prediction capabilities, allowing you to apply the model to the data stored in BigQuery without the need to export it. So, This approach leverages the scalability of Google Cloud infrastructure and avoids unnecessary data movement, being not necessary to export data to Cloud Store (options A and C), nor Import the TensorFlow model to BQ (option B)."
      },
      {
        "date": "2024-02-01T02:08:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/bigquery/docs/making-predictions-with-imported-tensorflow-models#:~:text=Import%20TensorFlow%20models,-To%20import%20TensorFlow&amp;text=In%20the%20Google%20Cloud%20console%2C%20go%20to%20the%20BigQuery%20page.&amp;text=In%20the%20query%20editor%2C%20enter,MODEL%20statement%20like%20the%20following.&amp;text=The%20preceding%20query%20imports%20a,BigQuery%20ML%20model%20named%20imported_tf_model%20."
      },
      {
        "date": "2024-01-30T05:55:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/bigquery/docs/reference/standard-sql/bigqueryml-syntax-create-tensorflow#limitations"
      },
      {
        "date": "2024-01-28T06:59:00.000Z",
        "voteCount": 2,
        "content": "Has to be B, because D has limitations:\nBigQuery data source tables must be no larger than 100 GB.\nhttps://cloud.google.com/vertex-ai/docs/tabular-data/classification-regression/get-batch-predictions#input_data_requirements"
      },
      {
        "date": "2024-01-22T08:47:00.000Z",
        "voteCount": 1,
        "content": "Same platform as data == less computation required to load and pass it to model"
      },
      {
        "date": "2024-01-22T08:48:00.000Z",
        "voteCount": 1,
        "content": "i mean B"
      },
      {
        "date": "2024-01-15T04:52:00.000Z",
        "voteCount": 2,
        "content": "It could either be B or D. It seems like most of the limitations of B are mentioned in the problem (https://cloud.google.com/bigquery/docs/reference/standard-sql/bigqueryml-syntax-create-tensorflow#limitations) but some of them are not and we are left questioning if the model will match the remaining requirements.\n\nTherefore, I would go for D, which can import data from BigQuery. https://cloud.google.com/vertex-ai/docs/predictions/get-batch-predictions#bigquery"
      },
      {
        "date": "2024-01-12T19:37:00.000Z",
        "voteCount": 1,
        "content": "Limitations of other options:\n\nA and C. Exporting data: Exporting 10 TB of data to Cloud Storage incurs additional storage costs, transfer time, and potential data management complexities.\nB. BigQuery ML: While BigQuery ML supports some TensorFlow models, it might have limitations with certain model architectures or features. Additionally, it might not be as optimized for large-scale batch inference as Vertex AI."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 190,
    "url": "https://www.examtopics.com/discussions/google/view/131006-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You recently deployed a model to a Vertex AI endpoint. Your data drifts frequently, so you have enabled request-response logging and created a Vertex AI Model Monitoring job. You have observed that your model is receiving higher traffic than expected. You need to reduce the model monitoring cost while continuing to quickly detect drift. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReplace the monitoring job with a DataFlow pipeline that uses TensorFlow Data Validation (TFDV)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReplace the monitoring job with a custom SQL script to calculate statistics on the features and predictions in BigQuery",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDecrease the sample_rate parameter in the RandomSampleConfig of the monitoring job\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the monitor_interval parameter in the ScheduleConfig of the monitoring job"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-06-30T01:50:00.000Z",
        "voteCount": 1,
        "content": "fitri001 2 months, 1 week ago\nA. DataFlow pipeline with TFDV: While DataFlow pipelines with TFDV can be used for data validation, they require additional development and management overhead compared to simply adjusting the Vertex AI Model Monitoring job configuration.\nB. Custom SQL script: Custom SQL scripts might not be as efficient or maintainable as the built-in Vertex AI Model Monitoring features. Additionally, it would require manually calculating drift metrics, which can be error-prone.\nD. Increase monitor_interval: Increasing the monitoring interval reduces the frequency of monitoring checks, potentially delaying drift detection. This is not ideal if data drifts frequently."
      },
      {
        "date": "2024-04-21T01:31:00.000Z",
        "voteCount": 1,
        "content": "Reduced Monitoring Overhead: By decreasing the sample_rate, you instruct Vertex AI Model Monitoring to analyze a smaller percentage of incoming requests. This directly reduces the billing cost associated with monitoring.\nFast Drift Detection: A well-chosen sampling rate can still provide enough data to capture significant data drift. Monitoring a smaller sample shouldn't significantly impact your ability to detect drift if it's happening rapidly."
      },
      {
        "date": "2024-04-21T01:32:00.000Z",
        "voteCount": 2,
        "content": "A. DataFlow pipeline with TFDV: While DataFlow pipelines with TFDV can be used for data validation, they require additional development and management overhead compared to simply adjusting the Vertex AI Model Monitoring job configuration.\nB. Custom SQL script: Custom SQL scripts might not be as efficient or maintainable as the built-in Vertex AI Model Monitoring features. Additionally, it would require manually calculating drift metrics, which can be error-prone.\nD. Increase monitor_interval: Increasing the monitoring interval reduces the frequency of monitoring checks, potentially delaying drift detection. This is not ideal if data drifts frequently."
      },
      {
        "date": "2024-02-27T04:14:00.000Z",
        "voteCount": 1,
        "content": "I went with C."
      },
      {
        "date": "2024-02-01T02:22:00.000Z",
        "voteCount": 1,
        "content": "C as the sample size will be relative to the traffic and also reduce costs."
      },
      {
        "date": "2024-01-15T04:58:00.000Z",
        "voteCount": 1,
        "content": "C. https://cloud.google.com/vertex-ai/docs/model-monitoring/overview#considerations"
      },
      {
        "date": "2024-01-12T19:44:00.000Z",
        "voteCount": 2,
        "content": "The answer is C, simplest and does not affect the time it takes to detect the drift"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 191,
    "url": "https://www.examtopics.com/discussions/google/view/131007-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You work for a retail company. You have created a Vertex AI forecast model that produces monthly item sales predictions. You want to quickly create a report that will help to explain how the model calculates the predictions. You have one month of recent actual sales data that was not included in the training dataset. How should you generate data for your report?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a batch prediction job by using the actual sales data. Compare the predictions to the actuals in the report.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a batch prediction job by using the actual sales data, and configure the job settings to generate feature attributions. Compare the results in the report.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGenerate counterfactual examples by using the actual sales data. Create a batch prediction job using the actual sales data and the counterfactual examples. Compare the results in the report.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTrain another model by using the same training dataset as the original, and exclude some columns. Using the actual sales data create one batch prediction job by using the new model and another one with the original model. Compare the two sets of predictions in the report."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-21T01:43:00.000Z",
        "voteCount": 1,
        "content": "Feature Attribution: By enabling feature attributions in the batch prediction job, you gain insights into how each feature in the actual sales data contributes to the model's predictions. This information is crucial for explaining the model's reasoning to non-technical audiences.\nDirect Model Insights: Analyzing the feature attributions allows you to demonstrate how the model uses historical trends, seasonality, and other factors (represented by features) to predict future sales."
      },
      {
        "date": "2024-04-21T01:44:00.000Z",
        "voteCount": 1,
        "content": "A. Prediction vs. Actuals: While comparing predictions to actuals can be informative, it doesn't directly explain how the model arrives at those predictions.\nC. Counterfactual Examples: Counterfactuals can be useful for understanding model behavior, but creating them requires additional effort and might not be necessary for explaining the basic prediction process.\nD. Training a New Model: Training another model is time-consuming and unnecessary. Feature attributions provide valuable insights without needing a separate model."
      },
      {
        "date": "2024-04-04T08:39:00.000Z",
        "voteCount": 1,
        "content": "B is the best answer but am unsure why the report has to be compared with actual sales"
      },
      {
        "date": "2024-02-01T02:28:00.000Z",
        "voteCount": 3,
        "content": "B) Will actually give you the information needed with Feature Attributions. E.g. The importance of each feature influencing the predictions sales items."
      },
      {
        "date": "2024-01-12T19:47:00.000Z",
        "voteCount": 2,
        "content": "Feature attributions explicitly measure how much each input feature contributed to each prediction, providing the most relevant insights for understanding model behavior."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 192,
    "url": "https://www.examtopics.com/discussions/google/view/130850-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "Your team has a model deployed to a Vertex AI endpoint. You have created a Vertex AI pipeline that automates the model training process and is triggered by a Cloud Function. You need to prioritize keeping the model up-to-date, but also minimize retraining costs. How should you configure retraining?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Pub/Sub to call the Cloud Function when a sufficient amount of new data becomes available",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a Cloud Scheduler job that calls the Cloud Function at a predetermined frequency that fits your team\u2019s budget",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable model monitoring on the Vertex AI endpoint. Configure Pub/Sub to call the Cloud Function when anomalies are detected",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable model monitoring on the Vertex AI endpoint. Configure Pub/Sub to call the Cloud Function when feature drift is detected\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-21T01:47:00.000Z",
        "voteCount": 2,
        "content": "Data-driven Retraining: Monitoring for feature drift identifies significant changes in the underlying data distribution used to train the model. Retraining based on drift detection ensures the model stays relevant to evolving data patterns, prioritizing model accuracy.\nReduced Cost: Triggering retraining only when drift is detected avoids unnecessary training runs, minimizing costs associated with Vertex AI training jobs."
      },
      {
        "date": "2024-04-21T01:48:00.000Z",
        "voteCount": 1,
        "content": "A. New Data Availability: While new data is important, it might not always necessitate retraining, especially if the new data aligns with existing patterns.\nB. Predetermined Frequency: Fixed scheduling can lead to either under-training (data evolves faster than the schedule) or over-training (drift happens slower than the schedule), potentially wasting resources.\nC. Anomaly Detection: Anomalies might not directly indicate feature drift, and retraining based solely on anomalies could introduce noise into the model."
      },
      {
        "date": "2024-02-01T02:33:00.000Z",
        "voteCount": 2,
        "content": "D) Makes the most sense and scales"
      },
      {
        "date": "2024-01-15T05:15:00.000Z",
        "voteCount": 1,
        "content": "Keep the model up to date -&gt; monitoring drift (distribution of production data doesnt change wildly). Only rerun training when necessary."
      },
      {
        "date": "2024-01-12T19:49:00.000Z",
        "voteCount": 1,
        "content": "It proactively triggers retraining when feature drift is detected, ensuring the model adapts to changing data patterns and maintains accuracy."
      },
      {
        "date": "2024-01-11T05:36:00.000Z",
        "voteCount": 1,
        "content": "feature drifting detecting to trigger retraining"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 193,
    "url": "https://www.examtopics.com/discussions/google/view/131009-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "Your company stores a large number of audio files of phone calls made to your customer call center in an on-premises database. Each audio file is in wav format and is approximately 5 minutes long. You need to analyze these audio files for customer sentiment. You plan to use the Speech-to-Text API You want to use the most efficient approach. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Upload the audio files to Cloud Storage<br>2. Call the speech:longrunningrecognize API endpoint to generate transcriptions<br>3. Call the predict method of an AutoML sentiment analysis model to analyze the transcriptions.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Upload the audio files to Cloud Storage.<br>2. Call the speech:longrunningrecognize API endpoint to generate transcriptions<br>3. Create a Cloud Function that calls the Natural Language API by using the analyzeSentiment method\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Iterate over your local files in Python<br>2. Use the Speech-to-Text Python library to create a speech.RecognitionAudio object, and set the content to the audio file data<br>3. Call the speech:recognize API endpoint to generate transcriptions<br>4. Call the predict method of an AutoML sentiment analysis model to analyze the transcriptions.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Iterate over your local files in Python<br>2. Use the Speech-to-Text Python Library to create a speech.RecognitionAudio object and set the content to the audio file data<br>3. Call the speech:longrunningrecognize API endpoint to generate transcriptions.<br>4. Call the Natural Language API by using the analyzeSentiment method"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 17,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 6,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-02-11T13:41:00.000Z",
        "voteCount": 8,
        "content": "My answer: B\n\nAccording to https://cloud.google.com/vertex-ai/docs/text-data/sentiment-analysis/prepare-data, AutoML sentiment analysis requires a minimum of 10 labeled training documents per sentiment category, with a maximum of 100,000 total training documents. This means you need to ensure you have an adequate amount of labeled data to train a reliable model. Therefore, option B is more suitable since the API will return the sentiment and there is no mention of a customized problem that justifies the use of AutoML."
      },
      {
        "date": "2024-04-26T05:39:00.000Z",
        "voteCount": 1,
        "content": "Agree with guilhermebutzke"
      },
      {
        "date": "2024-04-21T01:51:00.000Z",
        "voteCount": 2,
        "content": "Scalability: Uploading audio files to Cloud Storage provides a scalable and reliable storage solution for your large dataset.\nAsynchronous Processing: The speech:longrunningrecognize API enables asynchronous transcription, allowing your code to proceed without waiting for each file to finish processing. This improves overall throughput.\nManaged Service: Cloud Functions are serverless functions that automatically scale to handle the workload. You don't need to manage servers or infrastructure.\nNatural Language API Integration: The Cloud Function can directly call the Natural Language API's analyzeSentiment method for sentiment analysis, streamlining the workflow."
      },
      {
        "date": "2024-04-21T01:51:00.000Z",
        "voteCount": 1,
        "content": "C &amp; D: Local Processing: Iterating over local files and using the Speech-to-Text Python library might be suitable for small datasets. However, for a large number of audio files, local processing becomes slow and inefficient, especially for long audio files (5 minutes).\nC: Speech-to-Text API Limitation: The speech:recognize API is designed for short audio snippets (less than a minute) and might not be suitable for 5-minute audio files."
      },
      {
        "date": "2024-04-16T23:07:00.000Z",
        "voteCount": 1,
        "content": "Agree with guilhermebutzke"
      },
      {
        "date": "2024-02-01T02:42:00.000Z",
        "voteCount": 2,
        "content": "A)\nEfficiency: Option A leverages the optimized and scalable infrastructure of Google Cloud Platform (GCP). Using the speech:longrunningrecognize API allows you to transcribe large audio files efficiently without overwhelming your local machine or network.\n\nCost-effectiveness: Paying for processing in Cloud Storage can be more cost-effective than performing it locally, especially for large datasets.\n\nEase of use: The Cloud Storage and Speech-to-Text APIs are well-documented and provide readily available libraries for easy integration.\n\nScalability: This approach scales easily as your dataset grows, as GCP can handle large workloads efficiently."
      },
      {
        "date": "2024-01-16T17:24:00.000Z",
        "voteCount": 1,
        "content": "Re-considering as question states large dataset going with option A"
      },
      {
        "date": "2024-01-16T03:49:00.000Z",
        "voteCount": 1,
        "content": "I\u2019m going with b and agree with BlehMaks - For your convenience, the Natural Language API can perform sentiment analysis directly on a file located in Cloud Storage, without the need to send the contents of the file in the body of your request. \nGoogles best practices try api first then auto ml then custom training. \nhttps://cloud.google.com/natural-language/docs/analyzing-sentiment"
      },
      {
        "date": "2024-01-15T05:34:00.000Z",
        "voteCount": 1,
        "content": "A.\nIt must be longrunningrecognize -&gt; no C.\nNo point speaking about Python files -&gt; no D.\nFinal question being: NL analyzeSentiment or AutoML sentiment? I feel due to large dataset VertexAI AutoML is the way to go (can scale to large volumes of data)"
      },
      {
        "date": "2024-01-15T05:37:00.000Z",
        "voteCount": 1,
        "content": "More info: what natural language is right for you? https://cloud.google.com/natural-language?hl=en"
      },
      {
        "date": "2024-01-14T13:53:00.000Z",
        "voteCount": 4,
        "content": "Vertex AI AutoML is overkill as the build-in NL API provides sentiment analysis."
      },
      {
        "date": "2024-01-13T10:58:00.000Z",
        "voteCount": 2,
        "content": "B\nBecause don't need to train model just use google api transcride and sentiment analysis"
      },
      {
        "date": "2024-01-12T19:54:00.000Z",
        "voteCount": 2,
        "content": "Efficient audio processing: speech:longrunningrecognize is specifically designed for handling large audio files, offering asynchronous processing and optimized performance.\nScalability: Cloud Storage and Vertex AI AutoML scale seamlessly to handle large volumes of data and model inferences.\nCost-effectiveness: Separating transcription and sentiment analysis allows for potential cost optimization by using different pricing models for each service."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 194,
    "url": "https://www.examtopics.com/discussions/google/view/131010-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You work for a social media company. You want to create a no-code image classification model for an iOS mobile application to identify fashion accessories. You have a labeled dataset in Cloud Storage. You need to configure a training workflow that minimizes cost and serves predictions with the lowest possible latency. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTrain the model by using AutoML, and register the model in Vertex AI Model Registry. Configure your mobile application to send batch requests during prediction.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTrain the model by using AutoML Edge, and export it as a Core ML model. Configure your mobile application to use the .mlmodel file directly.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTrain the model by using AutoML Edge, and export the model as a TFLite model. Configure your mobile application to use the .tflite file directly.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTrain the model by using AutoML, and expose the model as a Vertex AI endpoint. Configure your mobile application to invoke the endpoint during prediction."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-24T23:26:00.000Z",
        "voteCount": 1,
        "content": "'for ios mobile' = edge\n'.mlmodel directly' = minimizes the cost"
      },
      {
        "date": "2024-04-21T01:58:00.000Z",
        "voteCount": 2,
        "content": "No-code Training: AutoML Edge simplifies model training without needing extensive coding knowledge.\nOn-device Processing: Core ML models run directly on the iOS device, minimizing latency by eliminating the need for network calls to a cloud endpoint.\nCost-effective: Training on AutoML Edge and deploying the model on the device avoids ongoing costs associated with Vertex AI endpoints."
      },
      {
        "date": "2024-04-21T01:58:00.000Z",
        "voteCount": 1,
        "content": "A. AutoML with Batch Requests: While AutoML offers powerful model training, batch requests for prediction still incur network latency and might not be ideal for real-time mobile applications.\nC &amp; D. TFLite and Vertex AI Endpoint: Both TFLite and Vertex AI endpoints are viable options, but they require additional steps for mobile integration compared to Core ML, which is native to iOS. Additionally, a Vertex AI endpoint introduces cloud communication and potential costs."
      },
      {
        "date": "2024-04-07T07:45:00.000Z",
        "voteCount": 1,
        "content": "Core ML is specifically designed for iOS devices, ensuring efficient inference and low latency."
      },
      {
        "date": "2024-02-18T10:51:00.000Z",
        "voteCount": 1,
        "content": "My Answer: B\n\nAutoML Edge or Vertex AI endpoint?: This option is specifically designed for training models that run on edge devices like mobile phones. It optimizes models for size and efficiency, minimizing cost and latency. While AutoML can train the model, using a Vertex AI endpoint adds unnecessary overhead and potential latency for mobile predictions. Batch requests wouldn't significantly improve latency here.\n\nCore ML or TFLite: While TFLite is compatible with some mobile platforms, Core ML is specifically designed for iOS and offers better performance and integration."
      },
      {
        "date": "2024-01-15T05:50:00.000Z",
        "voteCount": 1,
        "content": "B.\n\nConfused as AutoML Vision Edge seems like the right tool for this problematic but is deprecated according to docs: https://firebase.google.com/docs/ml/automl-image-labeling\n\nI will assume that the question needs updating but we should go with that + core ML is specifically designed for iOS apps. https://www.netguru.com/blog/coreml-vs-tensorflow-lite-mobile"
      },
      {
        "date": "2024-01-14T14:07:00.000Z",
        "voteCount": 2,
        "content": "it's possible to use either Core ML or TF Lite, but since it's necessary to ensure the lowest possible latency, choose Core ML\nhttps://cloud.google.com/vertex-ai/docs/export/export-edge-model#classification"
      },
      {
        "date": "2024-01-13T11:01:00.000Z",
        "voteCount": 1,
        "content": "B\nFor no code , automl is the best , for minimizing the cost we export as Core ML model"
      },
      {
        "date": "2024-01-12T19:55:00.000Z",
        "voteCount": 2,
        "content": "No-code model development: AutoML Edge provides a no-code interface for model training, aligning with the requirement.\nOptimized for mobile devices: Core ML is specifically designed for iOS devices, ensuring efficient inference and low latency.\nOffline capability: The app can run predictions locally without requiring network calls, reducing costs and ensuring availability even without internet connectivity.\nNo ongoing endpoint costs: Unlike using a Vertex AI endpoint, there are no extra costs associated with hosting and serving the model."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 195,
    "url": "https://www.examtopics.com/discussions/google/view/131011-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You work for a retail company. You have been asked to develop a model to predict whether a customer will purchase a product on a given day. Your team has processed the company\u2019s sales data, and created a table with the following rows:<br>\u2022\tCustomer_id<br>\u2022\tProduct_id<br>\u2022\tDate<br>\u2022\tDays_since_last_purchase (measured in days)<br>\u2022\tAverage_purchase_frequency (measured in 1/days)<br>\u2022\tPurchase (binary class, if customer purchased product on the Date)<br><br>You need to interpret your model\u2019s results for each individual prediction. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a BigQuery table. Use BigQuery ML to build a boosted tree classifier. Inspect the partition rules of the trees to understand how each prediction flows through the trees.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Vertex AI tabular dataset. Train an AutoML model to predict customer purchases. Deploy the model to a Vertex AI endpoint and enable feature attributions. Use the \u201cexplain\u201d method to get feature attribution values for each individual prediction.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a BigQuery table. Use BigQuery ML to build a logistic regression classification model. Use the values of the coefficients of the model to interpret the feature importance, with higher values corresponding to more importance",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Vertex AI tabular dataset. Train an AutoML model to predict customer purchases. Deploy the model to a Vertex AI endpoint. At each prediction, enable L1 regularization to detect non-informative features."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-06-30T01:41:00.000Z",
        "voteCount": 1,
        "content": "\" simplest approach\", the option B is the best choice."
      },
      {
        "date": "2024-04-21T02:06:00.000Z",
        "voteCount": 2,
        "content": "Individual Prediction Explanation: Vertex AI feature attributions provide insights into how each feature (e.g., days_since_last_purchase, average_purchase_frequency) contributes to a specific prediction for a customer-product combination. This allows you to understand the rationale behind the model's prediction for each instance.\nAutoML Convenience: AutoML simplifies model training without extensive configuration."
      },
      {
        "date": "2024-04-21T02:06:00.000Z",
        "voteCount": 2,
        "content": "A. BigQuery ML with Boosted Trees: While BigQuery ML can build boosted tree models, interpreting individual predictions by inspecting partition rules can be cumbersome and less intuitive compared to feature attributions.\nC. BigQuery ML Logistic Regression: Logistic regression coefficients indicate feature importance, but they don't directly explain how a specific feature value influences a single prediction.\nD. L1 Regularization: L1 regularization can help identify potentially unimportant features during training, but it doesn't directly explain individual predictions."
      },
      {
        "date": "2024-02-01T03:15:00.000Z",
        "voteCount": 1,
        "content": "Vertex AI feature attributions: This is the most direct approach. By enabling feature attributions, you get explanations for each prediction, highlighting how individual features contribute to the model's output. This is crucial for understanding specific customer purchase predictions."
      },
      {
        "date": "2024-01-14T14:31:00.000Z",
        "voteCount": 2,
        "content": "B is correct"
      },
      {
        "date": "2024-01-13T11:03:00.000Z",
        "voteCount": 1,
        "content": "B\nloca interpretability we Use the \"explain\" method to get feature attribution values for each individual prediction."
      },
      {
        "date": "2024-01-12T19:59:00.000Z",
        "voteCount": 1,
        "content": "Individual prediction interpretability: Feature attributions specifically address the need to understand how features contribute to individual predictions, providing fine-grained insights.\nVertex AI integration: Vertex AI offers seamless integration of feature attributions with AutoML models, simplifying the process.\nModel flexibility: AutoML can explore various model architectures, potentially finding the most suitable one for this task, while still providing interpretability."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 196,
    "url": "https://www.examtopics.com/discussions/google/view/130805-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You work for a company that captures live video footage of checkout areas in their retail stores. You need to use the live video footage to build a model to detect the number of customers waiting for service in near real time. You want to implement a solution quickly and with minimal effort. How should you build the model?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Vertex AI Vision Occupancy Analytics model.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Vertex AI Vision Person/vehicle detector model.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTrain an AutoML object detection model on an annotated dataset by using Vertex AutoML.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTrain a Seq2Seq+ object detection model on an annotated dataset by using Vertex AutoML."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-08-13T06:16:00.000Z",
        "voteCount": 1,
        "content": "It makes sense to use Vertex AI Vision Occupancy to reduce the effort of obtaining a model that identifies the number of people in a video, although I am hesitant about the fact that it says 'BUILD a model' and strictly speaking, no model is actually built with that option."
      },
      {
        "date": "2024-06-30T06:33:00.000Z",
        "voteCount": 1,
        "content": "https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/vehicle-detector\nOccupancy analytics has other features too like zone detection, dwell time, and more, which is not needed in this scenario."
      },
      {
        "date": "2024-04-18T03:56:00.000Z",
        "voteCount": 4,
        "content": "A. Use the Vertex AI Vision Occupancy Analytics model: This is a pre-built model specifically designed for analyzing occupancy in videos. It's ideal for this scenario as it requires minimal configuration and can likely be deployed quickly."
      },
      {
        "date": "2024-04-18T03:56:00.000Z",
        "voteCount": 1,
        "content": "C. Train an AutoML object detection model: While this could be a good solution in the long run, training a custom model requires creating an annotated dataset and takes time.\nD. Seq2Seq+ object detection model: This is an overly complex approach for this task. Seq2Seq models are used for sequence-to-sequence prediction tasks and are not necessary here."
      },
      {
        "date": "2024-02-11T13:48:00.000Z",
        "voteCount": 4,
        "content": "My Answer: A:\n\nVertex AI Vision Occupancy Analytics is a pre-trained model specifically designed to count people in live video streams.&nbsp;This removes the need for expensive and time-consuming data labeling and training,&nbsp;making it ideal for quick implementation. ****Vertex AI Vision Person/Vehicle Detector model detects individual people and vehicles,&nbsp;not specifically focusing on occupancy counting.&nbsp;It would require further processing to estimate the number of waiting customers. Option C and D requires labeling data and training,&nbsp;which adds effort and time.\nhttps://cloud.google.com/vision-ai/docs/overview"
      },
      {
        "date": "2024-02-01T03:21:00.000Z",
        "voteCount": 2,
        "content": "A. Use the Vertex AI Vision Occupancy Analytics model.\n\nHere's why:\n\nPre-trained and optimized: Occupancy Analytics is a pre-trained and optimized model specifically designed for counting people in video footage, aligning perfectly with your task. This eliminates the need for extensive data collection, annotation, and training, saving time and effort.\n\nNear real-time performance: The model is designed for low latency and near real-time inference, providing results quickly with minimal delay, important for live video analysis.\n\nMinimal configuration: Compared to training your own model, this option requires minimal configuration within the Vertex AI console, allowing for a quicker setup and deployment."
      },
      {
        "date": "2024-01-15T06:04:00.000Z",
        "voteCount": 1,
        "content": "All you need is counting the number of customers in the video stream. I would say no need to have the extra functionalities of occupancy analytics, person/vehicle is enough for this use case. https://cloud.google.com/vision-ai/docs/person-vehicle-model"
      },
      {
        "date": "2024-01-10T09:23:00.000Z",
        "voteCount": 3,
        "content": "https://codelabs.developers.google.com/vertex-ai-vision-queue-detection#0"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 197,
    "url": "https://www.examtopics.com/discussions/google/view/131012-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You work as an analyst at a large banking firm. You are developing a robust scalable ML pipeline to tram several regression and classification models. Your primary focus for the pipeline is model interpretability. You want to productionize the pipeline as quickly as possible. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Tabular Workflow for Wide &amp; Deep through Vertex AI Pipelines to jointly train wide linear models and deep neural networks",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Google Kubernetes Engine to build a custom training pipeline for XGBoost-based models",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Tabular Workflow for TabNet through Vertex AI Pipelines to train attention-based models\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Composer to build the training pipelines for custom deep learning-based models"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-11T17:34:00.000Z",
        "voteCount": 2,
        "content": "My Answer: C\n\nLink: https://cloud.google.com/vertex-ai/docs/tabular-data/tabular-workflows/overview"
      },
      {
        "date": "2024-02-01T03:38:00.000Z",
        "voteCount": 3,
        "content": "https://www.sciencedirect.com/science/article/pii/S0957417423000441\n\u2022\nWhen compared to XGBoost &amp; GLM, TabNet provides better or comparable performance.\n\n\u2022\nUnlike other Deep Learning models, TabNet is highly interpretable."
      },
      {
        "date": "2024-01-30T06:40:00.000Z",
        "voteCount": 1,
        "content": "agree, C, as this is specifically one of Tabnet's strengths"
      },
      {
        "date": "2024-01-13T02:05:00.000Z",
        "voteCount": 1,
        "content": "according to the documentation: \"TabNet uses sequential attention to choose which features to reason from at each decision step. This promotes interpretability and more efficient learning because the learning capacity is used for the most salient features.\""
      },
      {
        "date": "2024-01-12T20:07:00.000Z",
        "voteCount": 1,
        "content": "TabNet models are inherently more interpretable than deep neural networks or XGBoost models due to their attention mechanism. This aligns with the primary focus on interpretability."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 198,
    "url": "https://www.examtopics.com/discussions/google/view/131013-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You developed a Transformer model in TensorFlow to translate text. Your training data includes millions of documents in a Cloud Storage bucket. You plan to use distributed training to reduce training time. You need to configure the training job while minimizing the effort required to modify code and to manage the cluster\u2019s configuration. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Vertex AI custom training job with GPU accelerators for the second worker pool. Use tf.distribute.MultiWorkerMirroredStrategy for distribution.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Vertex AI custom distributed training job with Reduction Server. Use N1 high-memory machine type instances for the first and second pools, and use N1 high-CPU machine type instances for the third worker pool.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a training job that uses Cloud TPU VMs. Use tf.distribute.TPUStrategy for distribution.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Vertex AI custom training job with a single worker pool of A2 GPU machine type instances. Use tf.distribute.MirroredStrategv for distribution."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-18T04:02:00.000Z",
        "voteCount": 2,
        "content": "Vertex AI custom training job: This leverages a managed service within GCP, reducing cluster configuration and management overhead.\nGPU accelerators for the second worker pool: This allows for distributed training across multiple GPUs, significantly speeding up training compared to a single worker pool.\ntf.distribute.MultiWorkerMirroredStrategy: This is a TensorFlow strategy specifically designed for distributed training on multiple machines. It minimizes code changes as it handles data parallelization and model replication across devices."
      },
      {
        "date": "2024-04-18T04:02:00.000Z",
        "voteCount": 1,
        "content": "B. Reduction Server: While Vertex AI supports Reduction Servers, it's generally not required for text translation with Transformers. It's more commonly used for distributed training with specific model architectures.\nC. Cloud TPU VMs: While Cloud TPUs offer excellent performance, they require significant code modifications to work with Transformer models in TensorFlow. Additionally, managing Cloud TPU VMs involves more complexity compared to Vertex AI custom training jobs.\nD. Single worker pool: This limits training to a single machine, negating the benefits of distributed training."
      },
      {
        "date": "2024-02-27T03:44:00.000Z",
        "voteCount": 2,
        "content": "Why not C?"
      },
      {
        "date": "2024-04-30T02:31:00.000Z",
        "voteCount": 2,
        "content": "Yeah, but as the question mentions \"minimizing the effort required to modify code and to manage the cluster\u2019s configuration\", and TPus may require specific adaptations in the model code to fully exploit TPU capabilities."
      },
      {
        "date": "2024-04-07T07:51:00.000Z",
        "voteCount": 1,
        "content": "for me is C"
      },
      {
        "date": "2024-02-13T16:54:00.000Z",
        "voteCount": 2,
        "content": "My Answer: A\n\n- Distributed training:&nbsp;Utilizes GPUs in 2nd worker pool for speedup.\n- Minimal code changes:&nbsp;Vertex AI custom job for ease of use.\n- Managed cluster: No manual configuration needed.\n\nOther options:\n- B:&nbsp;Complex setup with different machine types and Reduction Server.\n- C:&nbsp;TPUs may not be optimal for Transformers and require code changes.\n- D:&nbsp;Lacks distributed training,&nbsp;limiting speed improvement."
      },
      {
        "date": "2024-01-12T20:11:00.000Z",
        "voteCount": 3,
        "content": "Minimizes code modification: MultiWorkerMirroredStrategy often requires minimal code changes to distribute training across multiple workers, aligning with the goal of minimizing effort.\nSimplifies cluster management: Vertex AI handles cluster configuration and scaling for custom training jobs, reducing the need for manual management.\nEffective distributed training: MultiWorkerMirroredStrategy is well-suited for large models and datasets, efficiently distributing training across GPUs."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 199,
    "url": "https://www.examtopics.com/discussions/google/view/131119-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You are developing a process for training and running your custom model in production. You need to be able to show lineage for your model and predictions. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a Vertex AI managed dataset.<br>2. Use a Vertex AI training pipeline to train your model.<br>3. Generate batch predictions in Vertex AI.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Use a Vertex AI Pipelines custom training job component to tram your model.<br>2. Generate predictions by using a Vertex AI Pipelines model batch predict component.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Upload your dataset to BigQuery.<br>2. Use a Vertex AI custom training job to train your model.<br>3. Generate predictions by using Vertex Al SDK custom prediction routines.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Use Vertex AI Experiments to train your model.<br>2. Register your model in Vertex AI Model Registry.<br>3. Generate batch predictions in Vertex AI.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 10,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 9,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-02-13T18:28:00.000Z",
        "voteCount": 5,
        "content": "My Answer: D\n\nAccording with: https://cloud.google.com/vertex-ai/docs/experiments/intro-vertex-ai-experiments\n\n\u201cVertex AI Experiments is a tool that helps you track and analyze different model architectures, hyperparameters, and training environments, letting you track the steps, inputs, and outputs of an experiment run. Vertex AI Experiments can also evaluate how your model performed in aggregate, against test datasets, and during the training run. You can then use this information to select the best model for your particular use case.\u201d.\n\nConsidering that both options A and B could demonstrate some form of lineage, I believe option D is the most suitable. The text explicitly states \"show lineage for your model and predictions,\" which aligns perfectly with the functionality provided by Vertex AI Experiments."
      },
      {
        "date": "2024-10-11T13:05:00.000Z",
        "voteCount": 1,
        "content": "My answer is B."
      },
      {
        "date": "2024-09-11T23:00:00.000Z",
        "voteCount": 1,
        "content": "It's a bit ambiguously worded this question. Model lineage involves knowledge of the data it was trained on, so that should be A. That being said, I think the question is implying D from it's wording, experiment tracking. I went for A, but suspect it's wrong."
      },
      {
        "date": "2024-05-07T09:27:00.000Z",
        "voteCount": 2,
        "content": "Option A/B doesn't mention anything about lineage. C is definitely wrong as there is no need to upload the dataset to Bigquery. \n\nOnly correct answer is D"
      },
      {
        "date": "2024-04-26T05:48:00.000Z",
        "voteCount": 1,
        "content": "running your custom model in production -&gt; need pipeline\n-&gt; B"
      },
      {
        "date": "2024-04-24T09:40:00.000Z",
        "voteCount": 2,
        "content": "Agree with  guilhermebutzke"
      },
      {
        "date": "2024-04-03T06:34:00.000Z",
        "voteCount": 1,
        "content": "A because to track lineage you need a managed dataset and vertex ai pipelines"
      },
      {
        "date": "2024-04-07T07:55:00.000Z",
        "voteCount": 1,
        "content": "lineage of the model i think, not for data, so it's B"
      },
      {
        "date": "2024-03-11T07:55:00.000Z",
        "voteCount": 1,
        "content": "A\nD cannot provide lineage for the source of your data. \nHas to be A to go with Vertex AI managed dataset."
      },
      {
        "date": "2024-03-08T05:39:00.000Z",
        "voteCount": 3,
        "content": "Vertex AI Pipelines are suited to do artifact lineage\nhttps://cloud.google.com/vertex-ai/docs/pipelines/lineage\nExperiments can do it also, but their main goal is to \"track and analyze different model architectures, hyperparameters, and training environments\""
      },
      {
        "date": "2024-02-05T06:51:00.000Z",
        "voteCount": 1,
        "content": "Managed data set to help track lineage \n https://cloud.google.com/vertex-ai/docs/training/using-managed-datasets"
      },
      {
        "date": "2024-02-01T06:28:00.000Z",
        "voteCount": 4,
        "content": "B) REF https://cloud.google.com/vertex-ai/docs/pipelines/lineage\n\nTrack the lineage of pipeline artifacts \n\nWhen you run a pipeline using Vertex AI Pipelines, the artifacts and parameters of your pipeline run are stored using Vertex ML Metadata. Vertex ML Metadata makes it easier to analyze the lineage of your pipeline's artifacts, by saving you the difficulty of keeping track of your pipeline's metadata.\n\nAn artifact's lineage includes all the factors that contributed to its creation, as well as artifacts and metadata that are derived from this artifact. For example, a model's lineage could include the following:\n\nThe training, test, and evaluation data used to create the model.\nThe hyperparameters used during model training.\nMetadata recorded from the training and evaluation process, such as the model's accuracy.\nArtifacts that descend from this model, such as the results of batch predictions."
      },
      {
        "date": "2024-01-15T06:58:00.000Z",
        "voteCount": 1,
        "content": "D. Sample on how to keep track of experiments lineage -&gt; https://cloud.google.com/vertex-ai/docs/experiments/user-journey/uj-model-training"
      },
      {
        "date": "2024-01-14T15:58:00.000Z",
        "voteCount": 1,
        "content": "Vertex AI Pipelines provides ability to track the lineage for your model and predictions"
      },
      {
        "date": "2024-01-13T11:26:00.000Z",
        "voteCount": 1,
        "content": "D\n\u201ctrack the lineage of pipeline artifacts\u201d. Vertex AI Experiments2 is a service that allows you to track and compare the results of your model training runs. Vertex AI Experiments automatically logs metadata such as hyperparameters, metrics, and artifacts for each training run."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 200,
    "url": "https://www.examtopics.com/discussions/google/view/131016-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You work for a hotel and have a dataset that contains customers\u2019 written comments scanned from paper-based customer feedback forms, which are stored as PDF files. Every form has the same layout. You need to quickly predict an overall satisfaction score from the customer comments on each form. How should you accomplish this task?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Vision API to parse the text from each PDF file. Use the Natural Language API analyzeSentiment feature to infer overall satisfaction scores.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Vision API to parse the text from each PDF file. Use the Natural Language API analyzeEntitySentiment feature to infer overall satisfaction scores.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUptrain a Document AI custom extractor to parse the text in the comments section of each PDF file. Use the Natural Language API analyzeSentiment feature to infer overall satisfaction scores.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUptrain a Document AI custom extractor to parse the text in the comments section of each PDF file. Use the Natural Language API analyzeEntitySentiment feature to infer overall satisfaction scores."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 7,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-10-11T13:09:00.000Z",
        "voteCount": 1,
        "content": "My vote is a. It is simple and do the job."
      },
      {
        "date": "2024-06-21T18:16:00.000Z",
        "voteCount": 1,
        "content": "C is right\nDocument AI custom extractor: Allows you to train a custom model to extract relevant information (in this case, customer comments) from the PDF files.\nNatural Language API analyzeSentiment feature: Analyzes the sentiment of the extracted text to predict an overall satisfaction score."
      },
      {
        "date": "2024-06-06T14:49:00.000Z",
        "voteCount": 1,
        "content": "C &amp; D are overkill\n\nWe don't care about entities sentiment -&gt; B is out \n\nLeft with A and https://cloud.google.com/natural-language/docs/reference/rest/v1/documents/analyzeSentiment"
      },
      {
        "date": "2024-04-18T04:24:00.000Z",
        "voteCount": 4,
        "content": "Document AI custom extractor: Since the layout of the feedback forms is consistent, training a custom extractor in Document AI allows for efficient and accurate extraction of the specific comments section. This ensures the Natural Language API receives the relevant text for sentiment analysis.\nNatural Language API - analyzeSentiment: This functionality within the Natural Language API is specifically designed to analyze sentiment in a piece of text. It provides an overall sentiment score that can be mapped to a satisfaction score (e.g., high positive sentiment translates to high satisfaction)."
      },
      {
        "date": "2024-04-18T04:25:00.000Z",
        "voteCount": 1,
        "content": "A. Vision API - parseText: While the Vision API can extract text from PDFs, it wouldn't necessarily target the specific comments section without a custom parser.\nB. Natural Language API - analyzeEntitySentiment: This feature focuses on sentiment analysis for named entities within the text. It might not be ideal for overall satisfaction extraction from general customer comments."
      },
      {
        "date": "2024-04-13T05:30:00.000Z",
        "voteCount": 1,
        "content": "quickly predict an overall satisfaction -&gt; a"
      },
      {
        "date": "2024-04-26T05:46:00.000Z",
        "voteCount": 1,
        "content": "no sorrt, it's C, you need doc AI"
      },
      {
        "date": "2024-03-08T05:48:00.000Z",
        "voteCount": 2,
        "content": "I go with A, because \"you need quickly predict\", no time for fine-tunning."
      },
      {
        "date": "2024-02-13T19:20:00.000Z",
        "voteCount": 1,
        "content": "My answer: Letter C\n\nDocument AI is a suitable tool for cases where there are patterns of forms or documentation. Additionally, it is possible to directly read PDF files. In the Natural Language API, the analyzeSentiment function can determine the overall sentiment, as the text asks, \"You need to quickly predict an overall satisfaction.\" The analyzeEntitySentiment function provides a score for each entity or word found.\nhttps://cloud.google.com/natural-language/docs/basics"
      },
      {
        "date": "2024-02-01T06:35:00.000Z",
        "voteCount": 1,
        "content": "Document AI custom extractor: This allows you to tailor the text extraction specifically to the layout and format of your customer feedback forms, ensuring accurate capture of the comments section.\n\nNatural Language API analyzeSentiment: This feature analyzes the extracted text and provides an overall sentiment score, which can be used to gauge customer satisfaction."
      },
      {
        "date": "2024-01-12T20:18:00.000Z",
        "voteCount": 1,
        "content": "Precision in text extraction: Document AI is specifically designed for extracting text from structured documents like forms, ensuring accurate extraction of comments, even with varying handwriting styles.\nCustom model for form layout: Training a custom extractor tailored to the hotel's feedback form layout further enhances accuracy and targets the relevant comments section effectively.\nSentiment analysis: Natural Language API's analyzeSentiment feature analyzes overall sentiment in a text block, aligning with the goal of deriving overall satisfaction scores."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 201,
    "url": "https://www.examtopics.com/discussions/google/view/131017-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You developed a Vertex AI pipeline that trains a classification model on data stored in a large BigQuery table. The pipeline has four steps, where each step is created by a Python function that uses the KubeFlow v2 API. The components have the following names:<br><br><img src=\"https://img.examtopics.com/professional-machine-learning-engineer/image3.png\"><br><br>You launch your Vertex AI pipeline as the following:<br><br><img src=\"https://img.examtopics.com/professional-machine-learning-engineer/image4.png\"><br><br>You perform many model iterations by adjusting the code and parameters of the training step. You observe high costs associated with the development, particularly the data export and preprocessing steps. You need to reduce model development costs. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the components\u2019 YAML filenames to export.yaml, preprocess,yaml, f \"train-<br>{dt}.yaml\", f\"calibrate-{dt).vaml\".\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd the {\"kubeflow.v1.caching\": True} parameter to the set of params provided to your PipelineJob.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMove the first step of your pipeline to a separate step, and provide a cached path to Cloud Storage as an input to the main pipeline.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the name of the pipeline to f\"my-awesome-pipeline-{dt}\"."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 7,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 4,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-10-11T13:22:00.000Z",
        "voteCount": 1,
        "content": "I select C:\nBy leveraging a Dataproc cluster, you can maintain compatibility with your existing PySpark jobs, minimize management overhead, and create a scalable proof of concept quickly and efficiently."
      },
      {
        "date": "2024-10-11T13:18:00.000Z",
        "voteCount": 1,
        "content": "I select B.\nA:\n\nChanging the YAML filenames does not affect caching behavior or cost reduction. The pipeline's efficiency and cost effectiveness are primarily governed by how it handles inputs and outputs rather than the filenames of the components.\nC:\n\nMoving the first step to a separate pipeline may help with organization but doesn\u2019t directly address the cost incurred by repeated data exports and preprocessing. Also, simply providing a cached path does not guarantee that the preprocessing step itself won\u2019t be executed multiple times.\nD:\n\nChanging the name of the pipeline to include a timestamp or other identifier does not influence caching or resource usage. It merely alters the identification of the pipeline runs without any impact on the efficiency of the operations being performed."
      },
      {
        "date": "2024-04-17T10:36:00.000Z",
        "voteCount": 1,
        "content": "see guilhermebutzke"
      },
      {
        "date": "2024-04-16T04:21:00.000Z",
        "voteCount": 1,
        "content": "see guilhermebutzke"
      },
      {
        "date": "2024-03-11T06:59:00.000Z",
        "voteCount": 1,
        "content": "C\nCaching should be enabled for all steps, e.g., export, preprocessing and training."
      },
      {
        "date": "2024-02-13T20:12:00.000Z",
        "voteCount": 4,
        "content": "My Answer: A\n\nFrom what I understood, it's about optimizing the process of adjusting code while utilizing previously processed results from the pipeline. Kubeflow inherently caches these steps, eliminating the need to explicitly store results in a designated path.\n\nHowever, the original filenames include a timestamp (**`-dt`**), suggesting that by removing this timestamp, the pipeline steps might not rerun as expected.\n\nOption C could be an approach, but it would require more effort to implement (since Kubeflow handles it automatically). Additionally, the beginning of the option only mentions moving the first step, which is the export, and doesn't say anything about preprocessing (which could be one of the more expensive steps).\n\nSo, considering all of these factors, I think A is the best choice.\""
      },
      {
        "date": "2024-01-16T17:37:00.000Z",
        "voteCount": 2,
        "content": "Not A - Changing file names does not help with reducing costs\nNot B - you cannot directly use kubeflow.v1.caching on a pipeline that uses the KubeFlow v2 API. Version Incompatibility: The kubeflow.v1.caching module is specifically designed for KubeFlow Pipelines v1, and its structure and functionality are not directly compatible with KubeFlow Pipelines v2.\nso best option here is C"
      },
      {
        "date": "2024-01-15T07:34:00.000Z",
        "voteCount": 1,
        "content": "I considered B but a search of \"kubeflow.v1.caching\" on Google only produces 1 result, which is this very question on this very website. Thus, I rule it out as non-existent (please share a resource if there is any that proves it exists) and opt for C."
      },
      {
        "date": "2024-01-15T06:18:00.000Z",
        "voteCount": 1,
        "content": "i think it's A. \n1)if we want to use the same results several times we shouldn't rename them. so we need to delete {dt} from the first two components names.\n2)we already have this option enable_caching = True, why do we need kubeflow.v1.caching then?\n3)i'm not sure but may be it does metter"
      },
      {
        "date": "2024-01-15T06:21:00.000Z",
        "voteCount": 1,
        "content": "3)i'm not sure but may be it does matter that KubeFlow v2 API and  kubeflow.v1.caching have different versions (v1 and v2)"
      },
      {
        "date": "2024-01-12T20:21:00.000Z",
        "voteCount": 2,
        "content": "Enables caching: Setting this parameter instructs Vertex AI Pipelines to cache the outputs of pipeline steps that have successfully completed. This means that if a step's inputs haven't changed, its execution can be skipped, reusing the cached output instead.\nTargets costly steps: The prompt highlights that data export and preprocessing steps are particularly expensive. Caching these steps can significantly reduce costs during model iterations."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 202,
    "url": "https://www.examtopics.com/discussions/google/view/131018-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You work for a startup that has multiple data science workloads. Your compute infrastructure is currently on-premises, and the data science workloads are native to PySpark. Your team plans to migrate their data science workloads to Google Cloud. You need to build a proof of concept to migrate one data science job to Google Cloud. You want to propose a migration process that requires minimal cost and effort. What should you do first?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a n2-standard-4 VM instance and install Java, Scala, and Apache Spark dependencies on it.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Google Kubernetes Engine cluster with a basic node pool configuration, install Java, Scala, and Apache Spark dependencies on it.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Standard (1 master, 3 workers) Dataproc cluster, and run a Vertex AI Workbench notebook instance on it.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Vertex AI Workbench notebook with instance type n2-standard-4."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 12,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 8,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-09-11T23:25:00.000Z",
        "voteCount": 1,
        "content": "C and D are both valid, as people point out you can technically have Spark preinstalled on D. But this is for a proof of concept for the real design. The concept is not proved by using a notebook, as it's not best practice. Therefore C makes more sense, and is still low effort as it's managed."
      },
      {
        "date": "2024-08-02T15:42:00.000Z",
        "voteCount": 1,
        "content": "C is the answer"
      },
      {
        "date": "2024-07-07T18:18:00.000Z",
        "voteCount": 2,
        "content": "I'm following option C. Please take a look the concept of 'Dataproc documentation' (ref: https://cloud.google.com/dataproc/docs)\n\nWith option D: doesn't provide a solution for managing and scaling the Spark environment, which is necessary for running PySpark workloads."
      },
      {
        "date": "2024-04-18T19:57:00.000Z",
        "voteCount": 1,
        "content": "Vertex AI Workbench notebook: This option provides a pre-configured environment with popular data science libraries like PySpark already installed. It allows you to focus on migrating your PySpark code with minimal changes.\nn2-standard-4 instance type: This is a general-purpose machine type suitable for various data science tasks. It offers a good balance between cost and performance for initial exploration."
      },
      {
        "date": "2024-08-29T18:52:00.000Z",
        "voteCount": 1,
        "content": "Option D doesnt provide Pyspark out of the box, you have to manually install it wherelse in C dataproc is managed spark and hadoop services which supports running pyspark services right away."
      },
      {
        "date": "2024-04-20T06:34:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/architecture/hadoop/migrating-apache-spark-jobs-to-cloud-dataproc#overview \nwhy not c?"
      },
      {
        "date": "2024-04-18T19:57:00.000Z",
        "voteCount": 1,
        "content": "A. Create a n2-standard-4 VM instance: This option requires manually installing Java, Scala, and Spark dependencies, which is time-consuming and prone to errors. It also involves managing the VM instance lifecycle, increasing complexity.\nB. Create a Google Kubernetes Engine cluster: Setting up and managing a Kubernetes cluster for a single job is overkill for a proof of concept. It adds unnecessary complexity and cost.\nC. Create a Standard Dataproc cluster: While Dataproc is a managed Spark environment on GCP, setting up a full cluster (master and workers) might be more resource-intensive than needed for a single job, especially for a proof of concept."
      },
      {
        "date": "2024-04-17T10:41:00.000Z",
        "voteCount": 2,
        "content": "went with D: https://cloud.google.com/vertex-ai/docs/workbench/instances/create-dataproc-enabled"
      },
      {
        "date": "2024-04-20T06:35:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/architecture/hadoop/migrating-apache-spark-jobs-to-cloud-dataproc#overview"
      },
      {
        "date": "2024-04-07T23:25:00.000Z",
        "voteCount": 1,
        "content": "When you want to move your Apache Spark workloads from an on-premises environment to Google Cloud, we recommend using Dataproc to run Apache Spark/Apache Hadoop clusters.\n\nhttps://cloud.google.com/architecture/hadoop/migrating-apache-spark-jobs-to-cloud-dataproc#overview"
      },
      {
        "date": "2024-03-11T07:26:00.000Z",
        "voteCount": 2,
        "content": "D\nCan use Notebook pre-installed libraries and tools, including PySpark."
      },
      {
        "date": "2024-03-02T07:55:00.000Z",
        "voteCount": 1,
        "content": "My bad, I mean is Option D."
      },
      {
        "date": "2024-03-02T07:54:00.000Z",
        "voteCount": 2,
        "content": "I went with C.\nFor Proof Of Concept and requires minimal cost and effort. Furthermore, Vertex AI Workbench notebooks come pre-configured with PySpark."
      },
      {
        "date": "2024-02-13T20:26:00.000Z",
        "voteCount": 3,
        "content": "My answer: C\nC: This option leverages Google Cloud's Dataproc service, which is designed for running Apache Spark and other big data processing frameworks. By creating a Standard Dataproc cluster, you can easily scale resources as needed for your workload. \n\nA. n2-standard-4 VM:&nbsp;This requires manual setup and ongoing maintenance,&nbsp;increasing cost and effort.\n\nB. GKE cluster:&nbsp;While offering containerization benefits,&nbsp;it necessitates managing containers and Spark configurations,&nbsp;adding complexity.\n\nD. With Vertex AI Workbench, your team can develop, train, and deploy machine learning models using popular frameworks like TensorFlow, PyTorch, and scikit-learn. However, while Vertex AI Workbench supports PySpark, it may not be the optimal choice for migrating existing PySpark workloads, as it's primarily focused on machine learning tasks."
      },
      {
        "date": "2024-02-26T16:52:00.000Z",
        "voteCount": 2,
        "content": "You're right but I have a doubt about in a part of Option D\n\"You need to build a proof of concept to migrate one data science job to Google Cloud\""
      },
      {
        "date": "2024-02-02T00:53:00.000Z",
        "voteCount": 1,
        "content": "Agree with BlehMaks https://cloud.google.com/architecture/hadoop/migrating-apache-spark-jobs-to-cloud-dataproc#overview Dataproc cluster seems more suitable"
      },
      {
        "date": "2024-01-17T13:07:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/vertex-ai-notebooks?hl=en\nData Data Lake and Spark in one place\n\nWhether you use TensorFlow, PyTorch, or Spark, you can run any engine from Vertex AI Workbench.&nbsp;\n\nD is correct"
      },
      {
        "date": "2024-01-15T06:53:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/architecture/hadoop/migrating-apache-spark-jobs-to-cloud-dataproc#overview"
      },
      {
        "date": "2024-01-12T20:23:00.000Z",
        "voteCount": 1,
        "content": "Minimal setup: Vertex AI Workbench notebooks come pre-configured with PySpark and other data science tools, eliminating the need for manual installation and setup.\nCost-effectiveness: Vertex AI Workbench offers managed notebooks with pay-as-you-go pricing, making it a cost-efficient option for proof-of-concept testing.\nEase of use: Data scientists can directly run PySpark code in the notebook without managing infrastructure, streamlining the migration process.\nScalability: Vertex AI Workbench can easily scale to handle larger workloads or multiple users if the proof-of-concept is successful."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 203,
    "url": "https://www.examtopics.com/discussions/google/view/131019-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You work for a bank. You have been asked to develop an ML model that will support loan application decisions. You need to determine which Vertex AI services to include in the workflow. You want to track the model\u2019s training parameters and the metrics per training epoch. You plan to compare the performance of each version of the model to determine the best model based on your chosen metrics. Which Vertex AI services should you use?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tVertex ML Metadata, Vertex AI Feature Store, and Vertex AI Vizier",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tVertex AI Pipelines, Vertex AI Experiments, and Vertex AI Vizier",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tVertex ML Metadata, Vertex AI Experiments, and Vertex AI TensorBoard\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tVertex AI Pipelines, Vertex AI Feature Store, and Vertex AI TensorBoard"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-06-27T11:56:00.000Z",
        "voteCount": 1,
        "content": "Agree with C"
      },
      {
        "date": "2024-04-07T23:27:00.000Z",
        "voteCount": 1,
        "content": "agree with pikachu007"
      },
      {
        "date": "2024-03-15T03:51:00.000Z",
        "voteCount": 2,
        "content": "Why not B ?"
      },
      {
        "date": "2024-08-11T05:57:00.000Z",
        "voteCount": 1,
        "content": "I guess because Vizier is a tool that helps to tune hyperparameters, and in a contrary Tensorboard is a tool to explore experiments."
      },
      {
        "date": "2024-02-26T16:48:00.000Z",
        "voteCount": 1,
        "content": "I went C"
      },
      {
        "date": "2024-01-13T02:18:00.000Z",
        "voteCount": 2,
        "content": "use Tensorboard to track the model\u2019s training parameters and the metrics per training epoch."
      },
      {
        "date": "2024-01-12T20:31:00.000Z",
        "voteCount": 3,
        "content": "Vertex ML Metadata:\nTracks model training parameters, hyperparameters, metrics, and lineage information.\nStores metadata in a central repository for easy access and comparison.\nIntegrates seamlessly with Vertex AI Experiments and TensorBoard.\nVertex AI Experiments:\nOrganizes and manages model training runs as experiments.\nVisualizes experiment results, including metrics and parameter comparisons.\nFacilitates tracking of the best performing model versions.\nVertex AI TensorBoard:\nProvides detailed visualizations of training metrics and model performance.\nEnables analysis of model behavior at each training epoch.\nIntegrates with Vertex AI Experiments for seamless access to experiment data."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 204,
    "url": "https://www.examtopics.com/discussions/google/view/131020-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You work for an auto insurance company. You are preparing a proof-of-concept ML application that uses images of damaged vehicles to infer damaged parts. Your team has assembled a set of annotated images from damage claim documents in the company\u2019s database. The annotations associated with each image consist of a bounding box for each identified damaged part and the part name. You have been given a sufficient budget to train models on Google Cloud. You need to quickly create an initial model. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDownload a pre-trained object detection model from TensorFlow Hub. Fine-tune the model in Vertex AI Workbench by using the annotated image data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTrain an object detection model in AutoML by using the annotated image data.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a pipeline in Vertex AI Pipelines and configure the AutoMLTrainingJobRunOp component to train a custom object detection model by using the annotated image data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTrain an object detection model in Vertex AI custom training by using the annotated image data."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-10-11T13:33:00.000Z",
        "voteCount": 1,
        "content": "My vote is B"
      },
      {
        "date": "2024-06-29T08:23:00.000Z",
        "voteCount": 1,
        "content": "quickly create an initial model = automl"
      },
      {
        "date": "2024-04-07T23:28:00.000Z",
        "voteCount": 2,
        "content": "went with B"
      },
      {
        "date": "2024-03-08T10:48:00.000Z",
        "voteCount": 2,
        "content": "By doing B we are doing D. I suppose B in more specific about the model and thus \"more\" correct?\nThoughts?"
      },
      {
        "date": "2024-02-02T02:08:00.000Z",
        "voteCount": 1,
        "content": "B makes the most sense, data is already labelled and a pretrained model may not fit for this specific case"
      },
      {
        "date": "2024-01-12T20:34:00.000Z",
        "voteCount": 4,
        "content": "Speed: AutoML excels in creating high-quality models with minimal code and setup, significantly accelerating model development.\nEase of use: It provides a user-friendly interface and automates many aspects of model training, making it accessible even for those without extensive ML expertise.\nAutomatic optimization: AutoML automatically handles hyperparameter tuning, feature engineering, and architecture selection, reducing manual effort and expertise required.\nCustom object detection: It supports custom object detection tasks, directly addressing the need to identify damaged parts in images."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 205,
    "url": "https://www.examtopics.com/discussions/google/view/131024-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You are analyzing customer data for a healthcare organization that is stored in Cloud Storage. The data contains personally identifiable information (PII). You need to perform data exploration and preprocessing while ensuring the security and privacy of sensitive fields. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Cloud Data Loss Prevention (DLP) API to de-identify the PII before performing data exploration and preprocessing.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse customer-managed encryption keys (CMEK) to encrypt the PII data at rest, and decrypt the PII data during data exploration and preprocessing.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a VM inside a VPC Service Controls security perimeter to perform data exploration and preprocessing.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Google-managed encryption keys to encrypt the PII data at rest, and decrypt the PII data during data exploration and preprocessing."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-18T20:16:00.000Z",
        "voteCount": 2,
        "content": "Cloud DLP API: This service redacts or replaces sensitive information in your data before processing. It allows data exploration and analysis without exposing PII directly.\nPrivacy Preservation: De-identification ensures sensitive information is not revealed during analysis, protecting patient privacy."
      },
      {
        "date": "2024-04-18T20:17:00.000Z",
        "voteCount": 1,
        "content": "B. CMEK and decryption: While CMEKs provide strong encryption, decrypting PII data during exploration exposes sensitive information. This increases the risk of accidental leaks or unauthorized access.\nC. VM with VPC Service Controls: This approach can add complexity and doesn't directly address PII privacy concerns during analysis.\nD. Google-managed encryption and decryption: Similar to option B, decrypting PII data for exploration weakens privacy."
      },
      {
        "date": "2024-04-07T23:32:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/dlp/docs/inspect-sensitive-text-de-identify"
      },
      {
        "date": "2024-03-08T10:49:00.000Z",
        "voteCount": 1,
        "content": "A is obvious."
      },
      {
        "date": "2024-01-15T07:42:00.000Z",
        "voteCount": 1,
        "content": "A. https://cloud.google.com/dlp/docs/inspect-sensitive-text-de-identify"
      },
      {
        "date": "2024-01-12T20:35:00.000Z",
        "voteCount": 2,
        "content": "Minimizes exposure of sensitive data: De-identification replaces or removes sensitive information, reducing the risk of accidental exposure or unauthorized access during analysis.\nPreserves data utility: DLP can de-identify data while maintaining its usefulness for exploration and preprocessing, ensuring meaningful analysis without compromising privacy.\nFlexibility in de-identification: You can choose appropriate de-identification techniques (e.g., masking, pseudonymization, generalization) based on specific privacy requirements and analysis needs."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 206,
    "url": "https://www.examtopics.com/discussions/google/view/131026-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You are building a predictive maintenance model to preemptively detect part defects in bridges. You plan to use high definition images of the bridges as model inputs. You need to explain the output of the model to the relevant stakeholders so they can take appropriate action. How should you build the model?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse scikit-learn to build a tree-based model, and use SHAP values to explain the model output.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse scikit-learn to build a tree-based model, and use partial dependence plots (PDP) to explain the model output.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse TensorFlow to create a deep learning-based model, and use Integrated Gradients to explain the model output.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse TensorFlow to create a deep learning-based model, and use the sampled Shapley method to explain the model output."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-06-28T00:18:00.000Z",
        "voteCount": 2,
        "content": "Use Integrated Gradients to explain the model output"
      },
      {
        "date": "2024-04-07T23:48:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/vertex-ai/docs/explainable-ai/overview"
      },
      {
        "date": "2024-04-05T09:53:00.000Z",
        "voteCount": 2,
        "content": "Given the scenario of using high definition images as inputs for predictive maintenance on bridges, and the need to explain the model output to stakeholders, the most appropriate choice would be:\n\nC. Use TensorFlow to create a deep learning-based model, and use Integrated Gradients to explain the model output.\n\nIntegrated Gradients is a method used to explain the predictions of deep learning models by attributing the contribution of each pixel in the input image to the final prediction. This would provide insights into which parts of the bridge images are most influential in the model's decision-making process, helping stakeholders understand why a particular prediction was made and allowing them to take appropriate action."
      },
      {
        "date": "2024-01-15T08:17:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/ai-platform/prediction/docs/ai-explanations/overview#compare-methods"
      },
      {
        "date": "2024-04-07T23:47:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/vertex-ai/docs/explainable-ai/overview\n\nthis is right, your is deprecated!"
      },
      {
        "date": "2024-01-12T20:43:00.000Z",
        "voteCount": 1,
        "content": "Handling image input: Deep learning models excel in processing complex visual data like high-definition images, making them ideal for extracting relevant features from bridge images for defect detection.\nExplainability with Integrated Gradients: Integrated Gradients is a powerful technique specifically designed to explain the predictions of deep learning models. It attributes model output to specific input features, providing insights into how the model makes decisions.\nVisualization: Integrated Gradients can generate visual explanations, such as heatmaps, that highlight image regions most influential to predictions, aiding in understanding and trust for stakeholders."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 207,
    "url": "https://www.examtopics.com/discussions/google/view/130543-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You work for a hospital that wants to optimize how it schedules operations. You need to create a model that uses the relationship between the number of surgeries scheduled and beds used. You want to predict how many beds will be needed for patients each day in advance based on the scheduled surgeries. You have one year of data for the hospital organized in 365 rows.<br><br>The data includes the following variables for each day:<br>\u2022\tNumber of scheduled surgeries<br>\u2022\tNumber of beds occupied<br>\u2022\tDate<br><br>You want to maximize the speed of model development and testing. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a BigQuery table. Use BigQuery ML to build a regression model, with number of beds as the target variable, and number of scheduled surgeries and date features (such as day of week) as the predictors.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a BigQuery table. Use BigQuery ML to build an ARIMA model, with number of beds as the target variable, and date as the time variable.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Vertex AI tabular dataset. Train an AutoML regression model, with number of beds as the target variable, and number of scheduled minor surgeries and date features (such as day of the week) as the predictors.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Vertex AI tabular dataset. Train a Vertex AI AutoML Forecasting model, with number of beds as the target variable, number of scheduled surgeries as a covariate and date as the time variable.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 12,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 4,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-07-27T21:39:00.000Z",
        "voteCount": 2,
        "content": "'Vertex AI AutoML Forecasting' == for forecasting time series data"
      },
      {
        "date": "2024-06-29T08:29:00.000Z",
        "voteCount": 1,
        "content": "\"You want to predict how many beds will be needed for patients each day\" = Forecasting"
      },
      {
        "date": "2024-06-28T00:18:00.000Z",
        "voteCount": 1,
        "content": "Train a Vertex AI AutoML Forecasting model"
      },
      {
        "date": "2024-06-17T09:27:00.000Z",
        "voteCount": 1,
        "content": "IDK, i going with A, because its maximize the speed of development and testing. Also in question it says: You need to create a model that uses the \"\"\"relationship\"\" between the number of surgeries scheduled and beds used.  = linear regression problem."
      },
      {
        "date": "2024-04-26T07:19:00.000Z",
        "voteCount": 1,
        "content": "I don't think this is a time series forecasting problem. The question clearly states that we should predict the number of beds based on the number of scheduled surgeries. this is a simple linear regression problem."
      },
      {
        "date": "2024-04-28T05:22:00.000Z",
        "voteCount": 1,
        "content": "\"You want to predict how many beds will be needed for patients each day in advance based on the scheduled surgeries.\""
      },
      {
        "date": "2024-04-18T20:22:00.000Z",
        "voteCount": 2,
        "content": "Vertex AI AutoML Forecasting: This option leverages Vertex AI's AutoML capabilities for time series forecasting. It automatically explores different model types and hyperparameters to find the best fit for your data. This can significantly speed up model development compared to building a model from scratch.\nDate as time variable, surgeries as covariate: This approach acknowledges the time-series nature of bed occupancy with \"date\" as the time series variable. It also incorporates the \"number of scheduled surgeries\" as a covariate, allowing the model to learn the relationship between surgeries and bed usage."
      },
      {
        "date": "2024-04-18T20:22:00.000Z",
        "voteCount": 2,
        "content": "A. BigQuery ML regression: While BigQuery ML offers quick model building, a regression model might not capture the time-series aspect of daily bed occupancy. Daily bed occupancy might have trends or seasonality which a plain regression model wouldn't capture.\nB. BigQuery ML ARIMA: ARIMA models are specifically for stationary time series data, and hospital bed occupancy might not always be stationary (e.g., holiday season might lead to higher occupancy). Additionally, ARIMA models typically don't incorporate additional features like the number of scheduled surgeries.\nC. Vertex AI AutoML Regression: Similar to option A, a regression model might not capture the time series aspect. While Vertex AI offers AutoML regression, using a solution designed for time series forecasting is more suitable here."
      },
      {
        "date": "2024-04-07T23:56:00.000Z",
        "voteCount": 1,
        "content": "best suited"
      },
      {
        "date": "2024-04-16T04:36:00.000Z",
        "voteCount": 1,
        "content": "not b: ARIMA does not use number of scheduled surgeries, and it is stated that the prediction must be based on that variable"
      },
      {
        "date": "2024-03-12T06:09:00.000Z",
        "voteCount": 1,
        "content": "I went with B."
      },
      {
        "date": "2024-01-31T00:13:00.000Z",
        "voteCount": 1,
        "content": "best suited, and treats the input as a time series, unlike A"
      },
      {
        "date": "2024-01-28T08:00:00.000Z",
        "voteCount": 2,
        "content": "D, as B doesn't mention the 'number of scheduled surgeries'."
      },
      {
        "date": "2024-01-16T04:11:00.000Z",
        "voteCount": 2,
        "content": "D is correct I believe"
      },
      {
        "date": "2024-01-17T13:29:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/vertex-ai/docs/tabular-data/forecasting/overview"
      },
      {
        "date": "2024-01-15T07:59:00.000Z",
        "voteCount": 3,
        "content": "A.\nUsing BigQuery to comply requirement of speed of development.\nARIMA does not use number of scheduled surgeries, and it is stated that the prediction must be based on that variable. So it must be A. LR model on BQ using scheduled surgeries, day of the week, etc, as predictors."
      },
      {
        "date": "2024-01-09T15:24:00.000Z",
        "voteCount": 3,
        "content": "365 days of data may be insufficient for big query I\u2019m going with C"
      },
      {
        "date": "2024-01-10T22:24:00.000Z",
        "voteCount": 2,
        "content": "D* not C Forecasting models are well-suited for predicting future values based on past trends, making them ideal for the goal of predicting bed occupancy for upcoming days. Dataset is too small for bigquery."
      },
      {
        "date": "2024-01-07T20:08:00.000Z",
        "voteCount": 1,
        "content": "Using B instead of D as it requires speed of development."
      },
      {
        "date": "2024-01-16T04:10:00.000Z",
        "voteCount": 2,
        "content": "While ARIMA models are commonly used for time series forecasting, they are more suitable for univariate time series data and might require additional manual intervention for feature engineering.\n\nIn this case, we have multiple variables such as the number of scheduled surgeries, the number of beds occupied, and the date. AutoML Forecasting in option D is designed to handle multivariate time series data, and it automates much of the modeling process, including feature selection and hyperparameter tuning. This can potentially result in a faster and more efficient development and testing process compared to manually implementing and tuning an ARIMA model."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 208,
    "url": "https://www.examtopics.com/discussions/google/view/131028-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You recently developed a wide and deep model in TensorFlow. You generated training datasets using a SQL script that preprocessed raw data in BigQuery by performing instance-level transformations of the data. You need to create a training pipeline to retrain the model on a weekly basis. The trained model will be used to generate daily recommendations. You want to minimize model development and training time. How should you develop the training pipeline?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Kubeflow Pipelines SDK to implement the pipeline. Use the BigQueryJobOp component to run the preprocessing script and the CustomTrainingJobOp component to launch a Vertex AI training job.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Kubeflow Pipelines SDK to implement the pipeline. Use the DataflowPythonJobOp component to preprocess the data and the CustomTrainingJobOp component to launch a Vertex AI training job.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the TensorFlow Extended SDK to implement the pipeline Use the ExampleGen component with the BigQuery executor to ingest the data the Transform component to preprocess the data, and the Trainer component to launch a Vertex AI training job.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the TensorFlow Extended SDK to implement the pipeline Implement the preprocessing steps as part of the input_fn of the model. Use the ExampleGen component with the BigQuery executor to ingest the data and the Trainer component to launch a Vertex AI training job."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 5,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 4,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-08-28T07:28:00.000Z",
        "voteCount": 1,
        "content": "\"If you use TensorFlow in an ML workflow that processes terabytes of structured data or text data, we recommend that you build your pipeline using TFX.\"\nhttps://cloud.google.com/vertex-ai/docs/pipelines/build-pipeline\n\nGoogle recommends TFX for large amount of structured data. Use input_fn for the Tensorflow model as it will output a tf.data.Dataset object.\n\nNote: As it is not mentionned that we are working with terabytes of data, Kubeflow is a viable option and i would choose answer A but i'll stick to google's recommendations"
      },
      {
        "date": "2024-08-09T23:33:00.000Z",
        "voteCount": 1,
        "content": "Option C is the most suitable because TFX provides a comprehensive MLOps framework, seamlessly integrating data ingestion, preprocessing, and model training, while also offering strong support for Vertex AI, making it the most efficient solution for the given use case."
      },
      {
        "date": "2024-08-02T21:03:00.000Z",
        "voteCount": 1,
        "content": "C. Use the TensorFlow Extended SDK to implement the pipeline. Use the ExampleGen component with the BigQuery executor to ingest the data, the Transform component to preprocess the data, and the Trainer component to launch a Vertex AI training job."
      },
      {
        "date": "2024-07-08T06:05:00.000Z",
        "voteCount": 1,
        "content": "I go with A\nKubeflow Pipelines SDK: supports machine learning and includes components specifically for tasks like data preprocessing, model training, and validation.\n\nBigQueryJobOp: enabling you to preprocess data using SQL scripts efficiently within BigQuery."
      },
      {
        "date": "2024-05-19T04:17:00.000Z",
        "voteCount": 1,
        "content": "Example Gen directly ingest data from BigQuery and the transform component makes it more efficient than using an input fn.\n\nI chose C over A and B because kubeflow pipelines is more sophisticated and requires more setup and effort because of it's customizability."
      },
      {
        "date": "2024-04-20T08:37:00.000Z",
        "voteCount": 1,
        "content": "agree with guilhermebutzke"
      },
      {
        "date": "2024-04-13T06:08:00.000Z",
        "voteCount": 1,
        "content": "agree with guilhermebutzke"
      },
      {
        "date": "2024-04-05T09:58:00.000Z",
        "voteCount": 1,
        "content": "Given the requirement to minimize model development and training time while creating a training pipeline for a wide and deep model trained on datasets preprocessed using a SQL script in BigQuery, the most suitable option is:\n\nC. Use the TensorFlow Extended SDK to implement the pipeline. Use the ExampleGen component with the BigQuery executor to ingest the data, the Transform component to preprocess the data, and the Trainer component to launch a Vertex AI training job.\n\nThis option leverages TensorFlow Extended (TFX), which is designed for scalable and production-ready machine learning pipelines. The ExampleGen component with the BigQuery executor efficiently ingests data from BigQuery. The Transform component applies preprocessing steps to the data, and the Trainer component launches a Vertex AI training job, minimizing the time and effort required for model development and training."
      },
      {
        "date": "2024-03-02T08:20:00.000Z",
        "voteCount": 1,
        "content": "Why not C?"
      },
      {
        "date": "2024-02-18T11:56:00.000Z",
        "voteCount": 3,
        "content": "My Answer: A\nAccording with this documentation:\nhttps://cloud.google.com/vertex-ai/docs/tabular-data/tabular-workflows/overview\n\nA: CORRECT: BigQueryJobOp for running the existing preprocessing script that already resides there, CustomTrainingJobOp for launching custom training jobs on Vertex AI,&nbsp;which aligns with the requirement of using the pre-trained TensorFlow model.\n\nB:  Not Correct: While DataflowPythonJobOp can be used for preprocessingthis&nbsp;increasing development time compared to the simpler BigQueryJobOp approach.\n\nC and D: Not Correct: While possible,&nbsp;using the TensorFlow Extended SDK with its components introduces unnecessary complexity for this specific scenario.&nbsp;For example, why use ExampleGen? Implementing preprocessing within the model's input_fn is generally not recommended due to potential efficiency drawbacks and training-serving skew issues."
      },
      {
        "date": "2024-01-24T08:39:00.000Z",
        "voteCount": 2,
        "content": "D is wrong. Google doesn't recommend to use input_fn for preprocessing\nhttps://www.tensorflow.org/tfx/guide/tft_bestpractices#preprocessing_options_summary"
      },
      {
        "date": "2024-01-12T21:29:00.000Z",
        "voteCount": 1,
        "content": "Addressing Limitations of Other Options:\n\nKubeflow Pipelines (A and B): While Kubeflow offers flexibility, it might require more setup and configuration, potentially increasing development time compared to TFX's integrated approach.\nSeparate Preprocessing (C): Using a separate Transform component for preprocessing can add complexity and potential overheads, especially for instance-level transformations that can often be directly integrated within the model's input pipeline."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 209,
    "url": "https://www.examtopics.com/discussions/google/view/131029-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You are training a custom language model for your company using a large dataset. You plan to use the Reduction Server strategy on Vertex AI. You need to configure the worker pools of the distributed training job. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the machines of the first two worker pools to have GPUs, and to use a container image where your training code runs. Configure the third worker pool to have GPUs, and use the reductionserver container image.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the machines of the first two worker pools to have GPUs and to use a container image where your training code runs. Configure the third worker pool to use the reductionserver container image without accelerators, and choose a machine type that prioritizes bandwidth.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the machines of the first two worker pools to have TPUs and to use a container image where your training code runs. Configure the third worker pool without accelerators, and use the reductionserver container image without accelerators, and choose a machine type that prioritizes bandwidth.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the machines of the first two pools to have TPUs, and to use a container image where your training code runs. Configure the third pool to have TPUs, and use the reductionserver container image."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-09-21T14:52:00.000Z",
        "voteCount": 1,
        "content": "The real reason for answer B is the custom model, which means it was not suited well for TPU"
      },
      {
        "date": "2024-04-18T20:28:00.000Z",
        "voteCount": 2,
        "content": "GPUs for Training: Configure the first two worker pools with GPUs to leverage the hardware acceleration capabilities for your custom language model training code.\nReduction Server without GPUs: The third worker pool should use the reductionserver container image. This image is pre-configured for Reduction Server functionality and doesn't require GPUs.\nHigh-Bandwidth CPU: Choose a machine type with high bandwidth for the third pool since Reduction Server focuses on communication and gradient reduction."
      },
      {
        "date": "2024-04-18T20:29:00.000Z",
        "voteCount": 1,
        "content": "A. GPUs for Reduction Server: Reduction Server itself doesn't require or benefit from GPUs. It focuses on communication and reduction of gradients. It's better to use a CPU-based machine type for the third pool.\nC. TPUs instead of GPUs: While TPUs can be used for training some language models, Reduction Server specifically works with GPUs using the NCCL library. Configure your first two pools with GPUs for your training code.\nD. TPUs in Reduction Server pool: Similar to option A, Reduction Server doesn't benefit from TPUs. It's best to use a CPU with high bandwidth for the third pool."
      },
      {
        "date": "2024-04-08T00:25:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/blog/topics/developers-practitioners/optimize-training-performance-reduction-server-vertex-ai\n\nIn this article, we introduce Reduction Server, a new Vertex AI feature that optimizes bandwidth and latency of multi-node distributed training on NVIDIA GPUs for synchronous data parallel algorithms."
      },
      {
        "date": "2024-01-14T21:05:00.000Z",
        "voteCount": 3,
        "content": "TPUs are not supported for reductionserver so B"
      },
      {
        "date": "2024-01-13T02:26:00.000Z",
        "voteCount": 2,
        "content": "bandwidth is important for the reduction server"
      },
      {
        "date": "2024-01-12T21:32:00.000Z",
        "voteCount": 2,
        "content": "Worker Pools 1 and 2:\nThese pools are responsible for the actual model training tasks.\nThey require GPUs (or TPUs, if applicable to your model) to accelerate model computations.\nThey run the container image containing your training code.\nWorker Pool 3:\nThis pool is dedicated to the reduction server.\nIt doesn't require accelerators (GPUs or TPUs) for gradient aggregation.\nPrioritize machines with high network bandwidth to optimize gradient exchange.\nUse the specific reductionserver"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 210,
    "url": "https://www.examtopics.com/discussions/google/view/131030-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You have trained a model by using data that was preprocessed in a batch Dataflow pipeline. Your use case requires real-time inference. You want to ensure that the data preprocessing logic is applied consistently between training and serving. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPerform data validation to ensure that the input data to the pipeline is the same format as the input data to the endpoint.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRefactor the transformation code in the batch data pipeline so that it can be used outside of the pipeline. Use the same code in the endpoint.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRefactor the transformation code in the batch data pipeline so that it can be used outside of the pipeline. Share this code with the end users of the endpoint.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBatch the real-time requests by using a time window and then use the Dataflow pipeline to preprocess the batched requests. Send the preprocessed requests to the endpoint."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-18T20:30:00.000Z",
        "voteCount": 1,
        "content": "Refactored Transformation Code: By refactoring the transformation code from the batch pipeline, you can create a reusable module that performs the same preprocessing steps.\nSame Code in Endpoint: Utilize the refactored code within your real-time inference endpoint. This ensures the data is preprocessed identically to how it was preprocessed during training."
      },
      {
        "date": "2024-04-18T20:30:00.000Z",
        "voteCount": 2,
        "content": "A. Data Validation: While data validation is important, it doesn't guarantee consistent preprocessing logic. You need to ensure the same transformations are applied.\nC. Share Code with End Users: Sharing code with end-users might not be ideal, especially if it requires specific libraries or configurations for execution outside of the pipeline.\nD. Batching and Dataflow: Batching real-time requests for Dataflow processing might introduce latency and defeat the purpose of real-time inference."
      },
      {
        "date": "2024-04-13T06:11:00.000Z",
        "voteCount": 1,
        "content": "agree with  guilhermebutzke"
      },
      {
        "date": "2024-02-18T14:24:00.000Z",
        "voteCount": 3,
        "content": "My Answer B:\n\nB. This option ensures that the preprocessing logic used during training, which has already been validated and tested, is applied consistently during real-time inference. By making the transformation code reusable outside of the batch pipeline and utilizing it in the endpoint, you ensure that the same preprocessing steps are applied to incoming data during inference, thus maintaining consistency between training and serving.\n\nA: &nbsp;While data validation is essential,&nbsp;it only ensures the format.&nbsp;It doesn't guarantee consistent preprocessing logic between training and serving.\n\nC:&nbsp;Sharing code with end-users might not be desirable for security or maintainability reasons.\n\nD: Batching introduces latency and might not be suitable for real-time needs.&nbsp;Additionally,&nbsp;using the entire Dataflow pipeline might be inefficient for individual requests."
      },
      {
        "date": "2024-01-17T14:13:00.000Z",
        "voteCount": 3,
        "content": "The transformation logic code in the&nbsp;serving_fn&nbsp;function defines the serving interface of your SavedModel for online prediction. If you implement the same transformations that were used for preparing training data in the transformation logic code of the&nbsp;serving_fn&nbsp;function, it ensures that the same transformations are applied to new prediction data points when they're served.\nhttps://www.tensorflow.org/tfx/guide/tft_bestpractices"
      },
      {
        "date": "2024-01-12T21:36:00.000Z",
        "voteCount": 1,
        "content": "A. Data validation: While essential, it doesn't guarantee consistency if the preprocessing logic itself differs between pipeline and endpoint.\nC. Sharing code with end users: This shifts the preprocessing burden to end users, potentially leading to inconsistencies and errors, and isn't feasible for real-time inference.\nD. Batching real-time requests: This introduces latency and might not align with real-time requirements, as users expect immediate responses."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 211,
    "url": "https://www.examtopics.com/discussions/google/view/131031-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You need to develop a custom TensorFlow model that will be used for online predictions. The training data is stored in BigQuery You need to apply instance-level data transformations to the data for model training and serving. You want to use the same preprocessing routine during model training and serving. How should you configure the preprocessing routine?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a BigQuery script to preprocess the data, and write the result to another BigQuery table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a pipeline in Vertex AI Pipelines to read the data from BigQuery and preprocess it using a custom preprocessing component.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a preprocessing function that reads and transforms the data from BigQuery. Create a Vertex AI custom prediction routine that calls the preprocessing function at serving time.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Apache Beam pipeline to read the data from BigQuery and preprocess it by using TensorFlow Transform and Dataflow.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 11,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-02-14T10:24:00.000Z",
        "voteCount": 7,
        "content": "My answer: D\n\nAccording to this documentation, it is very clear that using BigQuery is not a good approach for online prediction at the instance level. That's because we won't use the same code for both training and prediction serving. In the same documentation, the final table on the page recommends using Dataflow with TensorFlow Transform for instance-level data transformation.\n\nhttps://www.tensorflow.org/tfx/guide/tft_bestpractices"
      },
      {
        "date": "2024-04-16T04:48:00.000Z",
        "voteCount": 1,
        "content": "https://www.tensorflow.org/tfx/guide/tft_bestpractices#preprocessing_options_summary"
      },
      {
        "date": "2024-02-09T23:42:00.000Z",
        "voteCount": 2,
        "content": "D - Apache Beam + tf.transform or Dataflow.\nhttps://notebook.community/GoogleCloudPlatform/training-data-analyst/courses/machine_learning/deepdive/04_advanced_preprocessing/a_dataflow"
      },
      {
        "date": "2024-01-16T07:42:00.000Z",
        "voteCount": 1,
        "content": "the simplest way"
      },
      {
        "date": "2024-01-14T21:18:00.000Z",
        "voteCount": 1,
        "content": "D- Vertex AI isn't designed for instance-level data transformations"
      },
      {
        "date": "2024-01-17T14:18:00.000Z",
        "voteCount": 1,
        "content": "This document also provides an overview of&nbsp;TensorFlow Transform&nbsp;(tf.Transform), a library for TensorFlow that lets you define both instance-level and full-pass data transformation through data preprocessing pipelines. These pipelines are executed with&nbsp;Apache Beam, and they create artifacts that let you apply the same transformations during prediction as when the model is served.\nhttps://www.tensorflow.org/tfx/guide/tft_bestpractices"
      },
      {
        "date": "2024-01-14T21:18:00.000Z",
        "voteCount": 3,
        "content": "D- Vertex AI isn't designed for instance-level data transformations"
      },
      {
        "date": "2024-01-12T21:52:00.000Z",
        "voteCount": 1,
        "content": "Addressing limitations of other options:\n\nA. Data validation: While essential, it doesn't guarantee consistency if the preprocessing logic itself differs between pipeline and endpoint.\nC. Sharing code with end users: This shifts the preprocessing burden to end users, potentially leading to inconsistencies and errors, and isn't feasible for real-time inference.\nD. Batching real-time requests: This introduces latency and might not align with real-time requirements, as users expect immediate responses."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 212,
    "url": "https://www.examtopics.com/discussions/google/view/131032-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You are pre-training a large language model on Google Cloud. This model includes custom TensorFlow operations in the training loop. Model training will use a large batch size, and you expect training to take several weeks. You need to configure a training architecture that minimizes both training time and compute costs. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement 8 workers of a2-megagpu-16g machines by using tf.distribute.MultiWorkerMirroredStrategy.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement a TPU Pod slice with -accelerator-type=v4-l28 by using tf.distribute.TPUStrategy.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement 16 workers of c2d-highcpu-32 machines by using tf.distribute.MirroredStrategy.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement 16 workers of a2-highgpu-8g machines by using tf.distribute.MultiWorkerMirroredStrategy."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 10,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 7,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-09-12T04:07:00.000Z",
        "voteCount": 1,
        "content": "This could be A or D, because they both will perform will with custom Tensorflow operations. A is likely to be better with large batch sizes, which require bigger GPUs, so I went A."
      },
      {
        "date": "2024-08-04T07:49:00.000Z",
        "voteCount": 3,
        "content": "B is not correct as  TPUs not suitable for TensorFlow custom operations and C doesn't make any sense. A or D?. I would go with A"
      },
      {
        "date": "2024-06-17T05:24:00.000Z",
        "voteCount": 2,
        "content": "Should be A or D.  TPU is ok, but TPUs not suitable for TensorFlow custom operations."
      },
      {
        "date": "2024-06-13T23:25:00.000Z",
        "voteCount": 3,
        "content": "B. TPU Acceleration: the question says that uses Tensorflow custom operations in the main loop and Google documentation literatelly says about TPU use: \"Models with no custom TensorFlow/PyTorch/JAX operations inside the main training loop\" \n\nC.  High-CPU Machines: Make no sense because tell you to use a cpu (which does not help us in this case)\n\nSo the correct answer is between A and D. However the question says that they are planning to use a large batch size so we need RAM. Therefore we should take the one with more.\n\nCorrect answer: Option A"
      },
      {
        "date": "2024-04-17T22:38:00.000Z",
        "voteCount": 1,
        "content": "TPU Acceleration: TPUs are specifically designed for machine learning workloads and offer significant speedups compared to GPUs or CPUs, especially for large models like yours. Utilizing a TPU Pod slice provides access to a collection of interconnected TPUs for efficient parallel training.\ntf.distribute.TPUStrategy: This strategy is specifically designed to work with TPUs in TensorFlow. It handles data distribution, model replication, and gradient aggregation across the TPU cores, enabling efficient training with custom TensorFlow operations."
      },
      {
        "date": "2024-04-17T22:38:00.000Z",
        "voteCount": 2,
        "content": "why not the others?\nA. MultiWorkerMirroredStrategy with GPUs: While GPUs offer some acceleration, TPUs are generally better suited for large language model pre-training due to their architectural optimizations. Additionally, managing 8 workers across separate machines can introduce communication overhead compared to a tightly coupled TPU Pod.\nC. MirroredStrategy with High-CPU Machines: CPU-based training would be significantly slower than TPUs or even GPUs for a large language model. While the high CPU count might seem beneficial for custom operations, the overall training speed would still be limited.\nD. MultiWorkerMirroredStrategy with Multiple High-GPU Machines: Similar to option A, using multiple high-GPU machines with this strategy would incur communication overhead and potentially be less cost-effective compared to a single TPU Pod slice."
      },
      {
        "date": "2024-01-16T08:16:00.000Z",
        "voteCount": 2,
        "content": "It should be TPU but i'm a bit concerned about this point from Google documentation:\nModels with no custom TensorFlow/PyTorch/JAX operations inside the main training loop\nhttps://cloud.google.com/tpu/docs/intro-to-tpu#TPU"
      },
      {
        "date": "2024-01-15T08:30:00.000Z",
        "voteCount": 3,
        "content": "B.\nNGL quite lost on this one but if the training set is big enough to span over several weeks I would go with the most powerful resource (TPUs) but I might be completely wrong."
      },
      {
        "date": "2024-01-12T21:54:00.000Z",
        "voteCount": 4,
        "content": "TPU Advantages:\n\nHighly Specialized: TPUs (Tensor Processing Units) are custom-designed hardware accelerators specifically optimized for machine learning workloads, particularly those involving large batch sizes and matrix-heavy computations, common in large language models.\nExceptional Performance: TPUs can significantly outperform CPUs and GPUs in terms of speed and efficiency for these types of tasks.\nCost-Effective: While TPUs might have a higher hourly cost, their exceptional performance often leads to lower overall costs due to faster training times and reduced resource usage.\nTPU Pod Slice:\n\nScalability: TPU Pod slices allow you to distribute training across multiple TPUv4 chips for even greater performance and scalability.\nCustom Operations: The tf.distribute.TPUStrategy ensures compatibility with custom TensorFlow operations,"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 213,
    "url": "https://www.examtopics.com/discussions/google/view/131033-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You are building a TensorFlow text-to-image generative model by using a dataset that contains billions of images with their respective captions. You want to create a low maintenance, automated workflow that reads the data from a Cloud Storage bucket collects statistics, splits the dataset into training/validation/test datasets performs data transformations trains the model using the training/validation datasets, and validates the model by using the test dataset. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Apache Airflow SDK to create multiple operators that use Dataflow and Vertex AI services. Deploy the workflow on Cloud Composer.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the MLFlow SDK and deploy it on a Google Kubernetes Engine cluster. Create multiple components that use Dataflow and Vertex AI services.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Kubeflow Pipelines (KFP) SDK to create multiple components that use Dataflow and Vertex AI services. Deploy the workflow on Vertex AI Pipelines.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the TensorFlow Extended (TFX) SDK to create multiple components that use Dataflow and Vertex AI services. Deploy the workflow on Vertex AI Pipelines.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 9,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 8,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-09-21T19:00:00.000Z",
        "voteCount": 1,
        "content": "in this one will go with D, TFX is more specialized than kfp"
      },
      {
        "date": "2024-09-12T04:10:00.000Z",
        "voteCount": 1,
        "content": "TFX is going to be easier than kubeflow with custom code, as it basically does exactly what is listed there, by default."
      },
      {
        "date": "2024-06-28T10:00:00.000Z",
        "voteCount": 1,
        "content": "Agree with TFX"
      },
      {
        "date": "2024-06-10T11:11:00.000Z",
        "voteCount": 2,
        "content": "D) TFX is the way forward as it has services to support every step of the use case presented."
      },
      {
        "date": "2024-04-17T22:44:00.000Z",
        "voteCount": 2,
        "content": "KFP Pipelines: Kubeflow Pipelines (KFP) is a popular open-source framework for building and deploying machine learning workflows. It provides a user-friendly SDK for defining pipelines as components and simplifies workflow orchestration.\nVertex AI Pipelines Integration: Vertex AI Pipelines is a managed service from Google Cloud that integrates seamlessly with KFP. You can deploy your KFP-defined workflow on Vertex AI Pipelines, leveraging its features like scheduling, monitoring, and versioning.\nDataflow and Vertex AI Services: Both Dataflow and Vertex AI are Google Cloud services well-suited for this workflow"
      },
      {
        "date": "2024-04-17T22:44:00.000Z",
        "voteCount": 2,
        "content": "why not others?\nA. Airflow with Dataflow and Vertex AI: While Airflow is a powerful workflow management tool, deploying it on Cloud Composer adds additional complexity compared to the managed environment of Vertex AI Pipelines.\nB. MLflow with Dataflow and Vertex AI: MLflow focuses primarily on model lifecycle management. While it can be used for building pipelines, KFP offers a more specialized and user-friendly approach for this specific use case.\nD. TFX with Dataflow and Vertex AI: TFX is a comprehensive end-to-end ML platform. While it offers several functionalities, it might be an overkill for this scenario focusing on data processing, training, and validation. KFP provides a simpler solution for this specific workflow."
      },
      {
        "date": "2024-04-08T02:32:00.000Z",
        "voteCount": 4,
        "content": "If you use TensorFlow in an ML workflow that processes terabytes of structured data or text data, we recommend that you build your pipeline using TFX.\nFor other use cases, we recommend that you build your pipeline using the Kubeflow Pipelines SDK\n\nhttps://cloud.google.com/vertex-ai/docs/pipelines/build-pipeline#sdk"
      },
      {
        "date": "2024-01-19T02:47:00.000Z",
        "voteCount": 2,
        "content": "C and D are valid options. if the model is created in TF, use TFX, in any other case, use KFP; therefore, here is D"
      },
      {
        "date": "2024-04-08T02:32:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/vertex-ai/docs/pipelines/build-pipeline#sdk"
      },
      {
        "date": "2024-01-16T09:19:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/vertex-ai/docs/pipelines/build-pipeline#sdk"
      },
      {
        "date": "2024-01-12T21:59:00.000Z",
        "voteCount": 2,
        "content": "Airflow (A): While versatile, Airflow often requires more manual configuration and integration with ML services, potentially increasing maintenance effort.\nMLFlow (B): MLFlow focuses on experiment tracking and model management, lacking built-in pipeline components for data processing and model training.\nKubeflow Pipelines (C): KFP is flexible but requires more setup and infrastructure management compared to TFX's managed services."
      },
      {
        "date": "2024-04-08T02:32:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/vertex-ai/docs/pipelines/build-pipeline#sdk"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 214,
    "url": "https://www.examtopics.com/discussions/google/view/131034-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You are developing an ML pipeline using Vertex AI Pipelines. You want your pipeline to upload a new version of the XGBoost model to Vertex AI Model Registry and deploy it to Vertex AI Endpoints for online inference. You want to use the simplest approach. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Vertex AI REST API within a custom component based on a vertex-ai/prediction/xgboost-cpu image",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Vertex AI ModelEvaluationOp component to evaluate the model",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Vertex AI SDK for Python within a custom component based on a python:3.10 image",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChain the Vertex AI ModelUploadOp and ModelDeployOp components together\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-17T22:50:00.000Z",
        "voteCount": 1,
        "content": "Built-in Functionality: Both ModelUploadOp and ModelDeployOp are pre-built components within Vertex AI Pipelines specifically designed for uploading models and deploying them to endpoints.\nEase of Use: These components offer a user-friendly interface within the pipeline definition. You only need to specify essential details like the model path, container image URI (pre-built for XGBoost is available), endpoint configuration, etc.\nReduced Code Complexity: Using these components eliminates the need for writing custom code within your pipeline for model upload and deployment, simplifying your pipeline logic."
      },
      {
        "date": "2024-04-17T22:50:00.000Z",
        "voteCount": 1,
        "content": "why not the others?\nA. Custom Component with Vertex AI REST API: While this approach provides flexibility, it requires writing custom code to interact with the Vertex AI REST API within a container image. This adds complexity compared to using pre-built components.\nB. ModelEvaluationOp: This component is designed for model evaluation within the pipeline, not for uploading or deploying models.\nC. Custom Component with Python SDK: Similar to option A, using the Python SDK within a custom component offers flexibility but requires writing more code compared to using the pre-built ModelUploadOp and ModelDeployOp components."
      },
      {
        "date": "2024-04-08T02:35:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/vertex-ai/docs/pipelines/model-endpoint-component"
      },
      {
        "date": "2024-01-17T14:39:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/vertex-ai/docs/pipelines/model-endpoint-component"
      },
      {
        "date": "2024-01-12T22:01:00.000Z",
        "voteCount": 1,
        "content": "A. Custom Component with REST API: This involves more manual coding and understanding of REST API endpoints, potentially increasing complexity and maintenance.\nB. ModelEvaluationOp: This component is primarily for model evaluation, not model upload and deployment.\nC. Custom Component with SDK: While feasible, it involves more setup and dependency management compared to using built-in components."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 215,
    "url": "https://www.examtopics.com/discussions/google/view/131035-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You work for an online retailer. Your company has a few thousand short lifecycle products. Your company has five years of sales data stored in BigQuery. You have been asked to build a model that will make monthly sales predictions for each product. You want to use a solution that can be implemented quickly with minimal effort. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Prophet on Vertex AI Training to build a custom model.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Vertex AI Forecast to build a NN-based model.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse BigQuery ML to build a statistical ARIMA_PLUS model.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse TensorFlow on Vertex AI Training to build a custom model."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-17T22:53:00.000Z",
        "voteCount": 1,
        "content": "Quick Implementation: BigQuery ML simplifies the process. You can train and deploy the model directly within BigQuery, eliminating the need for complex model deployment or data movement.\nMinimal Effort: ARIMA_PLUS is a pre-built statistical model available in BigQuery ML. You don't need to write custom code for a complex neural network (NN) model like in option B or D.\nTime Series Data: ARIMA models are well-suited for time series forecasting, which is ideal for your monthly sales prediction task."
      },
      {
        "date": "2024-04-17T22:54:00.000Z",
        "voteCount": 1,
        "content": "why not others?\nA. Prophet on Vertex AI Training: While Prophet is a good choice for time series forecasting with holidays and seasonality, using Vertex AI Training requires additional setup and potentially custom code compared to the readily available ARIMA_PLUS model within BigQuery ML.\nB. Vertex AI Forecast with NN-based Model: Building a custom NN-based model using Vertex AI Forecast offers flexibility but requires more effort and expertise in model development and potentially hyperparameter tuning. This might not be ideal for a quick implementation.\nD. TensorFlow on Vertex AI Training: Similar to option B, using TensorFlow for a custom model offers flexibility but requires significant coding and expertise, making it less suitable for a quick and low-effort approach."
      },
      {
        "date": "2024-04-08T02:36:00.000Z",
        "voteCount": 1,
        "content": "data on bigquery + minimal effort -&gt; C"
      },
      {
        "date": "2024-01-16T01:38:00.000Z",
        "voteCount": 1,
        "content": "Given amount of data (few thousand short-cycled products) and frequency of predictions (monthly) C is the way to go."
      },
      {
        "date": "2024-01-12T22:02:00.000Z",
        "voteCount": 3,
        "content": "Ease of Use: BigQuery ML integrates seamlessly with BigQuery, allowing you to create and train models directly within SQL queries, eliminating the need for separate environments or coding.\nStatistical ARIMA_PLUS Strengths: This model is well-suited for time series forecasting, automatically handling seasonality, trends, and holidays, making it appropriate for monthly sales predictions.\nMinimal Effort: BigQuery ML handles model training and tuning, reducing the need for manual configuration or hyperparameter tuning.\nFast Implementation: Model creation and training can be done in a few lines of SQL, enabling rapid deployment."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 216,
    "url": "https://www.examtopics.com/discussions/google/view/131036-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You are creating a model training pipeline to predict sentiment scores from text-based product reviews. You want to have control over how the model parameters are tuned, and you will deploy the model to an endpoint after it has been trained. You will use Vertex AI Pipelines to run the pipeline. You need to decide which Google Cloud pipeline components to use. What components should you choose?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTabularDatasetCreateOp, CustomTrainingJobOp, and EndpointCreateOp",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTextDatasetCreateOp, AutoMLTextTrainingOp, and EndpointCreateOp",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTabularDatasetCreateOp. AutoMLTextTrainingOp, and ModelDeployOp",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTextDatasetCreateOp, CustomTrainingJobOp, and ModelDeployOp\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-06-29T08:49:00.000Z",
        "voteCount": 1,
        "content": "\"Text dataset -&gt; TextDatasetCreateOp\nControl over parameters -&gt; CustomTrainingJobOp\""
      },
      {
        "date": "2024-04-17T23:20:00.000Z",
        "voteCount": 1,
        "content": "TextDatasetCreateOp: This component is specifically designed to handle text-based data like product reviews. It reads and prepares the text data for training the model.\nCustomTrainingJobOp: Since you want control over hyperparameter tuning, a custom training job is the most suitable option. This component allows you to define your training script using a framework like TensorFlow and configure hyperparameters for optimization.\nModelDeployOp: After training, this component uploads the trained model to the Vertex AI Model Registry and deploys it to a Vertex AI Endpoint for serving predictions."
      },
      {
        "date": "2024-04-17T23:20:00.000Z",
        "voteCount": 2,
        "content": "why not others?\nA. TabularDatasetCreateOp and EndpointCreateOp:\nTabularDatasetCreateOp is designed for tabular data, not raw text.\nEndpointCreateOp creates an endpoint, but you need a model upload step before deployment (handled by ModelDeployOp).\nB. AutoMLTextTrainingOp: While AutoML offers convenience, it removes control over hyperparameter tuning, which you require.\nC. TabularDatasetCreateOp and AutoMLTextTrainingOp: Similar to option A, TabularDatasetCreateOp is not ideal for text data, and AutoML removes hyperparameter control."
      },
      {
        "date": "2024-04-08T02:39:00.000Z",
        "voteCount": 1,
        "content": "D fits perfect"
      },
      {
        "date": "2024-02-08T01:28:00.000Z",
        "voteCount": 1,
        "content": "D  AutoML uses a predefined set of hyperparameter values for each algorithm used in model training. We can not have a control over hyperparameter"
      },
      {
        "date": "2024-01-16T01:44:00.000Z",
        "voteCount": 2,
        "content": "Text dataset -&gt; TextDatasetCreateOp\nControl over parameters -&gt; CustomTrainingJobOp"
      },
      {
        "date": "2024-01-12T22:04:00.000Z",
        "voteCount": 1,
        "content": "TextDatasetCreateOp: This component is specifically designed to create datasets from text-based data, essential for handling product reviews.\nCustomTrainingJobOp: This component provides full control over the training process, allowing you to specify model architecture, hyperparameter tuning strategies, and other training parameters, aligning with the requirement for control over model tuning.\nModelDeployOp: This component streamlines model deployment to a Vertex AI endpoint for real-time or batch inference, enabling the trained model to serve predictions."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 217,
    "url": "https://www.examtopics.com/discussions/google/view/131037-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "Your team frequently creates new ML models and runs experiments. Your team pushes code to a single repository hosted on Cloud Source Repositories. You want to create a continuous integration pipeline that automatically retrains the models whenever there is any modification of the code. What should be your first step to set up the CI pipeline?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a Cloud Build trigger with the event set as \"Pull Request\"",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a Cloud Build trigger with the event set as \"Push to a branch\"\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a Cloud Function that builds the repository each time there is a code change",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a Cloud Function that builds the repository each time a new branch is created"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-27T21:57:00.000Z",
        "voteCount": 1,
        "content": "B. According to Gemini-Advanced."
      },
      {
        "date": "2024-04-17T23:24:00.000Z",
        "voteCount": 1,
        "content": "Continuous Integration: CI pipelines aim for frequent integration of code changes. Triggering the build pipeline upon every push to a branch (including the main branch) ensures your models retrain whenever the code relevant to them is modified.\nFocus on Relevant Changes: Compared to option A (\"Pull Request\"), triggering on pushes allows retraining even for direct pushes to the main branch, not just pull request merges. This can be crucial for catching critical code changes that might bypass pull requests."
      },
      {
        "date": "2024-04-17T23:24:00.000Z",
        "voteCount": 1,
        "content": "C. Cloud Function for Code Changes: While Cloud Functions can be used for CI pipelines, manually configuring a function for every code change might become cumbersome and less scalable compared to a dedicated CI/CD service like Cloud Build with built-in triggering functionalities.\nD. Cloud Function for New Branches: Triggering on new branch creation alone wouldn't retrain models on existing branches where your team actively works. You'd need an additional trigger for existing branches (e.g., push to branch) to achieve automatic retraining."
      },
      {
        "date": "2024-04-08T02:41:00.000Z",
        "voteCount": 1,
        "content": "For ANY modifications, \u201cPush to a branch\u201d is the best choice in Cloud Build trigger."
      },
      {
        "date": "2024-02-14T12:09:00.000Z",
        "voteCount": 1,
        "content": "My Answer B:\n\nFor ANY modifications, \u201cPush to a branch\u201d is the best choice in Cloud Build trigger. However, when it comes to ML model training, retraining models on every push might be resource-intensive, especially if the training process is computationally expensive. So, I think triggering the CI pipeline on a pull request allows for changes to be tested before merging into the main branch. would be a better choice \u2026"
      },
      {
        "date": "2024-01-16T01:49:00.000Z",
        "voteCount": 1,
        "content": "B. Any code change on the Cloud repo is done by pushing to a branch."
      },
      {
        "date": "2024-01-12T22:06:00.000Z",
        "voteCount": 4,
        "content": "Cloud Build Integration: Cloud Build is Google Cloud's fully managed CI/CD platform, designed to automate builds and deployments, making it ideal for this task.\nTrigger on Code Pushes: Setting the trigger event to \"Push to a branch\" ensures that the pipeline automatically activates whenever new code is pushed to any branch of the repository, aligning with the goal of retraining models on code modifications."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 218,
    "url": "https://www.examtopics.com/discussions/google/view/131038-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You have built a custom model that performs several memory-intensive preprocessing tasks before it makes a prediction. You deployed the model to a Vertex AI endpoint, and validated that results were received in a reasonable amount of time. After routing user traffic to the endpoint, you discover that the endpoint does not autoscale as expected when receiving multiple requests. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a machine type with more memory",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDecrease the number of workers per machine",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the CPU utilization target in the autoscaling configurations.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDecrease the CPU utilization target in the autoscaling configurations\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 15,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 5,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-16T02:00:00.000Z",
        "voteCount": 9,
        "content": "D. \nThe idea behind this question is getting autoscaling to handle well the fluctuating input of requests. Changing the machine (A) is not related to autoscaling, and you might not be using the full potential of the machine during the whole time, bur rather only during instances of peak traffic. You need to lower the autoscaling threshold (the target utilization metric mentioned in the options is CPU, so we will go with this) so you make use of more resources whenever too many memory-intensive requests are happening. \n\nhttps://cloud.google.com/compute/docs/autoscaler/scaling-cpu#scaling_based_on_cpu_utilization\nhttps://cloud.google.com/compute/docs/autoscaler#autoscaling_policy"
      },
      {
        "date": "2024-01-16T02:10:00.000Z",
        "voteCount": 2,
        "content": "Addition: although memory-intensive is not directly related to CPU, for me the key is \"the model does not autoscale as expected\". To me this is addressing directly the settings of autoscaling, which won't change by changing the machine."
      },
      {
        "date": "2024-01-12T22:08:00.000Z",
        "voteCount": 5,
        "content": "B. Decreasing Workers: This might reduce memory usage per machine but could also decrease overall throughput, potentially impacting performance.\nC. Increasing CPU Utilization Target: This wouldn't directly address the memory bottleneck and could trigger unnecessary scaling based on CPU usage, not memory requirements.\nD. Decreasing CPU Utilization Target: This could lead to premature scaling, potentially increasing costs without addressing the root cause."
      },
      {
        "date": "2024-06-29T08:50:00.000Z",
        "voteCount": 1,
        "content": "\"use autoscale\" = deacrease cpu utilization target"
      },
      {
        "date": "2024-04-17T23:28:00.000Z",
        "voteCount": 2,
        "content": "D. Decrease the CPU utilization target: This is the most suitable approach. By lowering the CPU utilization target, the endpoint will scale up at a lower CPU usage level. This increases the likelihood of scaling up when the memory-intensive preprocessing tasks cause a rise in CPU utilization, even though memory is the root cause."
      },
      {
        "date": "2024-04-17T23:29:00.000Z",
        "voteCount": 1,
        "content": "A. Use a machine type with more memory: While this might seem logical, autoscaling in Vertex AI endpoints relies on CPU utilization as the metric, not directly on memory usage. Even with more memory, the endpoint might not scale up if CPU utilization remains below the threshold.\nB. Decrease the number of workers per machine (Not applicable to Vertex AI Endpoints): This option might be relevant for some serving frameworks, but Vertex AI Endpoints don't typically use a worker concept. Scaling down workers wouldn't directly address the memory bottleneck.\nC. Increase the CPU utilization target: This would instruct the endpoint to scale up only when CPU usage reaches a higher threshold. Since the issue is memory usage, increasing the CPU target wouldn't trigger scaling when memory is the limiting factor."
      },
      {
        "date": "2024-02-14T12:27:00.000Z",
        "voteCount": 3,
        "content": "Option D, \"Decrease the CPU utilization target in the autoscaling configurations,\" could be a valid approach to address the issue of autoscaling and anticipate spikes in traffic. By lowering the threshold, the autoscaling system would initiate scaling actions at a lower CPU utilization level, allowing for a more proactive response to increasing demands."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 219,
    "url": "https://www.examtopics.com/discussions/google/view/131039-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "Your company manages an ecommerce website. You developed an ML model that recommends additional products to users in near real time based on items currently in the user\u2019s cart. The workflow will include the following processes:<br><br>1. The website will send a Pub/Sub message with the relevant data and then receive a message with the prediction from Pub/Sub<br>2. Predictions will be stored in BigQuery<br>3. The model will be stored in a Cloud Storage bucket and will be updated frequently<br><br>You want to minimize prediction latency and the effort required to update the model. How should you reconfigure the architecture?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite a Cloud Function that loads the model into memory for prediction. Configure the function to be triggered when messages are sent to Pub/Sub.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a pipeline in Vertex AI Pipelines that performs preprocessing, prediction, and postprocessing. Configure the pipeline to be triggered by a Cloud Function when messages are sent to Pub/Sub.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExpose the model as a Vertex AI endpoint. Write a custom DoFn in a Dataflow job that calls the endpoint for prediction.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the RunInference API with WatchFilePattern in a Dataflow job that wraps around the model and serves predictions.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 9,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-02-14T13:42:00.000Z",
        "voteCount": 5,
        "content": "My answer: D \n\nThis Google Documentation explains \u201cInstead of deploying the model to an endpoint, you can use the&nbsp;RunInference API&nbsp;to serve machine learning models in your Apache Beam pipeline. This approach has several advantages, including flexibility and portability.\u201d\nhttps://cloud.google.com/blog/products/ai-machine-learning/streaming-prediction-with-dataflow-and-vertex\n\nThis documentation uses RunInference and WatchFilePattern to \u201cto automatically update the ML model without stopping the Apache Beam\u201d.\nhttps://cloud.google.com/dataflow/docs/notebooks/automatic_model_refresh\n\nSo, thinking in \u201cminimize prediction latency\u201d, its suggested use RunInfenrece, while \u201ceffort required to update the model\u201d the **WatchFilePattern is the best approach.**  I think D is the best option"
      },
      {
        "date": "2024-06-10T11:40:00.000Z",
        "voteCount": 1,
        "content": "C) Expose the model as Vertex AI End Point"
      },
      {
        "date": "2024-04-16T10:09:00.000Z",
        "voteCount": 1,
        "content": "agree with guilhermebutzke"
      },
      {
        "date": "2024-03-10T02:20:00.000Z",
        "voteCount": 1,
        "content": "A for me."
      },
      {
        "date": "2024-02-05T05:33:00.000Z",
        "voteCount": 3,
        "content": "Automatic Model Updates: WatchFilePattern automatically detects model changes in Cloud Storage, leading to seamless updates without managing endpoint deployments."
      },
      {
        "date": "2024-01-12T22:16:00.000Z",
        "voteCount": 2,
        "content": "Low Latency:\n\nServerless Execution: Cloud Functions start up almost instantly, reducing prediction latency compared to alternatives that require longer setup or deployment times.\nIn-Memory Model: Loading the model into memory eliminates disk I/O overhead, further contributing to rapid predictions."
      },
      {
        "date": "2024-02-06T06:22:00.000Z",
        "voteCount": 2,
        "content": "Cloud Functions offer low latency but it might not scale well."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 220,
    "url": "https://www.examtopics.com/discussions/google/view/131040-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You are collaborating on a model prototype with your team. You need to create a Vertex AI Workbench environment for the members of your team and also limit access to other employees in your project. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a new service account and grant it the Notebook Viewer role<br>2. Grant the Service Account User role to each team member on the service account<br>3. Grant the Vertex AI User role to each team member<br>4. Provision a Vertex AI Workbench user-managed notebook instance that uses the new service account",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Grant the Vertex AI User role to the default Compute Engine service account<br>2. Grant the Service Account User role to each team member on the default Compute Engine service account<br>3. Provision a Vertex AI Workbench user-managed notebook instance that uses the default Compute Engine service account.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a new service account and grant it the Vertex AI User role<br>2. Grant the Service Account User role to each team member on the service account<br>3. Grant the Notebook Viewer role to each team member.<br>4. Provision a Vertex AI Workbench user-managed notebook instance that uses the new service account\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Grant the Vertex AI User role to the primary team member<br>2. Grant the Notebook Viewer role to the other team members<br>3. Provision a Vertex AI Workbench user-managed notebook instance that uses the primary user\u2019s account"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 7,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-04-17T23:37:00.000Z",
        "voteCount": 1,
        "content": "1. Create a new service account and grant it the Vertex AI User role: This dedicated service account will control access to the Vertex AI Workbench environment.\n\n2. Grant the Service Account User role to each team member on the service account: This grants your team members the ability to use the service account to access the Workbench environment.\n\n3. Grant the Notebook Viewer role to each team member:  While they can't modify notebooks, this role allows team members to view and run existing notebooks within the Workbench environment.\n\n4. Provision a Vertex AI Workbench user-managed notebook instance that uses the new service account:  By associating the instance with the service account, you ensure only authorized team members (through the service account) can access the environment."
      },
      {
        "date": "2024-04-17T23:38:00.000Z",
        "voteCount": 1,
        "content": "A. Notebook Viewer with Service Account User: Granting the Notebook User role on the service account would allow team members to modify notebooks, potentially exceeding your intended access limitations.\nB. Default Service Account: Granting access on the default Compute Engine service account is not recommended for security reasons. It's a shared resource and could grant unintended access.\nD. Primary User Access: Granting access through a single user account creates a security risk and is not scalable for managing team member permissions."
      },
      {
        "date": "2024-02-14T15:20:00.000Z",
        "voteCount": 2,
        "content": "My Answer: C\n\nThis approach ensures that each team member has access to the necessary resources while limiting access to other employees not involved in the project. In A,  the Notebook Viewer role is just to see, which is not sufficient for accessing Vertex AI resources. In B, This option grants permissions to the default Compute Engine service account, which may not be ideal for managing access to Vertex AI resources specifically. In D, This approach does not provide uniform access control for all team members and may lead to inconsistencies in resource management."
      },
      {
        "date": "2024-02-05T04:46:00.000Z",
        "voteCount": 1,
        "content": "Why not A?\n\nMainly because of the fact that we're only giving the role \"Notebook Viewer\" to the SA, which is not sufficient."
      },
      {
        "date": "2024-02-14T15:20:00.000Z",
        "voteCount": 1,
        "content": "in A, the Notebook Viewer role is just to see, which is not sufficient for accessing Vertex AI resources."
      },
      {
        "date": "2024-01-16T02:22:00.000Z",
        "voteCount": 1,
        "content": "A and C really sound like the same. Only going for A because I understand it gives the lowest level of permission role when creating the project (that is, all members in the Compute Engine Project); and subsequently, grants User role ONLY to the team members. https://cloud.google.com/iam/docs/overview#resource"
      },
      {
        "date": "2024-02-06T11:03:00.000Z",
        "voteCount": 1,
        "content": "Creating a new service account with the Notebook Viewer role would not provide sufficient permissions for managing the Vertex AI Workbench environment, right?"
      },
      {
        "date": "2024-01-12T22:18:00.000Z",
        "voteCount": 3,
        "content": "Dedicated Service Account: Creating a separate service account ensures isolation and control over access to Vertex AI resources.\nVertex AI User Role: Granting this role to the service account provides it with necessary permissions to interact with Vertex AI services.\nService Account User Role: Assigning this role to team members allows them to impersonate the service account, enabling them to use its permissions.\nNotebook Viewer Role: This role grants team members access to the notebook instance, but not direct Vertex AI resource management.\nUser-Managed Notebook Instance: This type of instance uses a specific service account, ensuring access control is aligned with the designated service account's permissions."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 221,
    "url": "https://www.examtopics.com/discussions/google/view/131041-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You work at a leading healthcare firm developing state-of-the-art algorithms for various use cases. You have unstructured textual data with custom labels. You need to extract and classify various medical phrases with these labels. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Healthcare Natural Language API to extract medical entities",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a BERT-based model to fine-tune a medical entity extraction model",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AutoML Entity Extraction to train a medical entity extraction model\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse TensorFlow to build a custom medical entity extraction model"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 11,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 4,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-16T02:29:00.000Z",
        "voteCount": 8,
        "content": "C. \"AutoML Entity Extraction for Healthcare allows you to create a custom entity extraction model trained using your own annotated medical text and using your own categories.\" https://cloud.google.com/healthcare-api/docs/concepts/nlp#choosing_between_the_and"
      },
      {
        "date": "2024-01-31T01:19:00.000Z",
        "voteCount": 1,
        "content": "textbook use case as described in the link provided"
      },
      {
        "date": "2024-01-27T23:14:00.000Z",
        "voteCount": 1,
        "content": "Full Agreed"
      },
      {
        "date": "2024-06-29T08:58:00.000Z",
        "voteCount": 1,
        "content": "\"unstructured textual data with custom labels \" = AutoML Entity Extraction"
      },
      {
        "date": "2024-04-18T00:35:00.000Z",
        "voteCount": 2,
        "content": "Pre-built Functionality: It's a pre-built and managed service within Vertex AI that streamlines the process of building custom entity extraction models. This can save you time and resources compared to building a model from scratch using TensorFlow (option D).\nCustomizable Labels: AutoML Entity Extraction allows you to define your custom labels for medical phrases, which aligns well with your specific needs.\nUnstructured Text Support: It's designed to handle unstructured text data like your medical records.\nFaster Experimentation: Compared to a custom BERT-based model (option B), AutoML Entity Extraction often allows for faster experimentation as it automates many hyperparameter tuning aspects."
      },
      {
        "date": "2024-04-18T00:36:00.000Z",
        "voteCount": 1,
        "content": "A. Healthcare Natural Language API: While this API can extract medical entities like diseases or medications, it might not support the level of customization you need for your specific medical phrases with custom labels.\nB. BERT-based Model with Fine-tuning: Fine-tuning a BERT model can be effective, but it requires significant expertise in machine learning and natural language processing. AutoML Entity Extraction provides a more accessible and potentially faster approach for your use case.\nD. TensorFlow for Custom Model: Building a custom model with TensorFlow offers maximum control, but it requires a high level of expertise and can be time-consuming, especially for a team that might not specialize in NLP."
      },
      {
        "date": "2024-02-14T15:45:00.000Z",
        "voteCount": 2,
        "content": "My answer: B \n\nLooking for \u201cdeveloping state-of-the-art algorithms for various use cases\u201d in the question, I think the best approach is BERT-based model. AutoML Entity Extraction could be a approach for a quickstart, and Healthcare Natural Language API might not have your custom labels built-in, limiting its effectiveness. Tensorflow model can be time-consuming and require significant expertise\n\nhttps://cloud.google.com/healthcare-api/docs/concepts/nlp#choosing_between_the_and"
      },
      {
        "date": "2024-01-21T05:43:00.000Z",
        "voteCount": 3,
        "content": "A.- \"The Healthcare Natural Language API parses unstructured medical text such as medical records or insurance claims. It then generates a structured data representation of the medical knowledge entities stored in these data sources for downstream analysis and automatio\""
      },
      {
        "date": "2024-01-12T22:21:00.000Z",
        "voteCount": 2,
        "content": "A. Healthcare Natural Language API: While convenient, it lacks the customization capabilities for fine-tuning with custom labels, potentially limiting accuracy for your specific needs.\nC. AutoML Entity Extraction: It's generally well-suited for common entity types, but its pre-defined label set might not accommodate the full range of medical entities and relationships you need to extract.\nD. TensorFlow Custom Model: Building a model from scratch requires significant expertise, time, and resources, often less efficient than leveraging the power of pre-trained BERT models."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 222,
    "url": "https://www.examtopics.com/discussions/google/view/131046-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You developed a custom model by using Vertex AI to predict your application's user churn rate. You are using Vertex AI Model Monitoring for skew detection. The training data stored in BigQuery contains two sets of features - demographic and behavioral. You later discover that two separate models trained on each set perform better than the original model. You need to configure a new model monitoring pipeline that splits traffic among the two models. You want to use the same prediction-sampling-rate and monitoring-frequency for each model. You also want to minimize management effort. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tKeep the training dataset as is. Deploy the models to two separate endpoints, and submit two Vertex AI Model Monitoring jobs with appropriately selected feature-thresholds parameters.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tKeep the training dataset as is. Deploy both models to the same endpoint and submit a Vertex AI Model Monitoring job with a monitoring-config-from-file parameter that accounts for the model IDs and feature selections.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSeparate the training dataset into two tables based on demographic and behavioral features. Deploy the models to two separate endpoints, and submit two Vertex AI Model Monitoring jobs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSeparate the training dataset into two tables based on demographic and behavioral features. Deploy both models to the same endpoint, and submit a Vertex AI Model Monitoring job with a monitoring-config-from-file parameter that accounts for the model IDs and training datasets."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 7,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-02-14T16:03:00.000Z",
        "voteCount": 6,
        "content": "My answer: B\n\nIf you're using Vertex AI Model Monitoring for skew detection and your data is stored in BigQuery, it's not strictly necessary to separate the data into two tables. Vertex AI Model Monitoring can indeed analyze each feature individually to detect skew. So, isn't necessary to separate data. \n\nThen, the `monitoring-config-from-file`&nbsp;parameter lets you specify unique configurations for each model, including ID and training data information. This ensures targeted monitoring and analysis and a unique monitoring job."
      },
      {
        "date": "2024-09-26T22:12:00.000Z",
        "voteCount": 1,
        "content": "My vote is D, have to separate the training dataset"
      },
      {
        "date": "2024-08-04T17:08:00.000Z",
        "voteCount": 1,
        "content": "The question mentions skew, yo need to configure the model monitoring with this in mind, so the better option is to separate in two diferent tables to user skew detection"
      },
      {
        "date": "2024-04-18T00:46:00.000Z",
        "voteCount": 3,
        "content": "Reduced Management Effort: You only need to deploy and monitor a single endpoint, minimizing complexity compared to managing two separate endpoints and monitoring jobs (Option A and C).\nEfficient Data Usage: Maintaining the original training dataset simplifies data management and avoids the need to split it into separate tables (Option C and D).\nGranular Monitoring: The monitoring-config-from-file parameter allows you to specify configurations for each model within the same monitoring job. You can define the model ID and the features to monitor for potential skew or drift for each model independently."
      },
      {
        "date": "2024-04-18T00:46:00.000Z",
        "voteCount": 2,
        "content": "A. Separate Endpoints and Monitoring Jobs: This approach requires managing two endpoints and monitoring jobs, increasing complexity.\nC. Separate Training Data and Separate Endpoints: While it separates training data, it requires managing separate endpoints and monitoring jobs, similar to option A. Additionally, splitting the data might be unnecessary for monitoring purposes in this scenario.\nD. Separate Training Data (Optional) and Single Endpoint: Splitting the data (optional) adds complexity, and while you can use a single endpoint, defining configurations for each model within the monitoring job is more efficient using the monitoring-config-from-file parameter (option B)."
      },
      {
        "date": "2024-04-08T03:59:00.000Z",
        "voteCount": 1,
        "content": "I don't understand why it is necessary to separate dataset when there is Vertex AI Monitoring"
      },
      {
        "date": "2024-06-06T03:31:00.000Z",
        "voteCount": 1,
        "content": "For training-skew detection, you require the training dataset. Hence, by splitting the original dataset into the two features, it would make management easier later on.\n\nCorrect me if I'm wrong, but you would have to update the monitoring job when you retrain the model to keep the monitoring job updated as well. Hence splitting it makes sense. Agreed that same endpoint would be easier to manage as opposed to two. \n\nAs a result, my answer is D."
      },
      {
        "date": "2024-03-29T08:33:00.000Z",
        "voteCount": 1,
        "content": "Not B, as training on separate datasets is recommended."
      },
      {
        "date": "2024-04-08T04:00:00.000Z",
        "voteCount": 1,
        "content": "why? i don't understand sorry"
      },
      {
        "date": "2024-03-10T00:54:00.000Z",
        "voteCount": 1,
        "content": "D\n\nSeparate data to 2 tables to make sure both models are trained with most relevant data."
      },
      {
        "date": "2024-01-16T02:39:00.000Z",
        "voteCount": 1,
        "content": "D.\nYou need to split the training dataset for each respective model. Furthermore, you only need to control for 2 differences between models in monitoring-config-from-file: model ID, and training set. Feature selection should be the same in both models."
      },
      {
        "date": "2024-02-09T17:42:00.000Z",
        "voteCount": 2,
        "content": "Why not B?"
      },
      {
        "date": "2024-01-14T21:46:00.000Z",
        "voteCount": 2,
        "content": "D - makes more sense two models to be trained seperately and more accuarately also submits a Vertex Al Model Monitoring job with a monitoring-config-from parameter which would enable the skew detecttion to work for each model"
      },
      {
        "date": "2024-01-12T23:15:00.000Z",
        "voteCount": 4,
        "content": "A. Separate Endpoints: This approach involves more management overhead and potentially complicates monitoring configurations.\nC. Separate Datasets: Splitting the dataset into two tables is unnecessary for model monitoring and could introduce data management complexities.\nD. Separate Datasets, Same Endpoint: While feasible, this option lacks the flexibility of granular feature control provided by monitoring-config-from-file."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 223,
    "url": "https://www.examtopics.com/discussions/google/view/131047-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You work for a pharmaceutical company based in Canada. Your team developed a BigQuery ML model to predict the number of flu infections for the next month in Canada. Weather data is published weekly, and flu infection statistics are published monthly. You need to configure a model retraining policy that minimizes cost. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDownload the weather and flu data each week. Configure Cloud Scheduler to execute a Vertex AI pipeline to retrain the model weekly.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDownload the weather and flu data each month. Configure Cloud Scheduler to execute a Vertex AI pipeline to retrain the model monthly.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDownload the weather and flu data each week. Configure Cloud Scheduler to execute a Vertex AI pipeline to retrain the model every month.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDownload the weather data each week, and download the flu data each month. Deploy the model to a Vertex AI endpoint with feature drift monitoring, and retrain the model if a monitoring alert is detected.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 12,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-18T00:50:00.000Z",
        "voteCount": 4,
        "content": "Weather Data Update: Downloading weather data weekly captures the latest trends potentially influencing flu infections.\nFlu Data Update: Downloading flu statistics monthly aligns with the data publication schedule and avoids unnecessary processing for data that might not have changed.\nFeature Drift Monitoring: Vertex AI endpoint monitoring helps identify significant changes in the weather data distribution (feature drift) over time.\nRetrain Based on Alerts: Retraining the model is triggered only when feature drift is detected, ensuring the model stays relevant without unnecessary retraining cycles."
      },
      {
        "date": "2024-04-18T00:50:00.000Z",
        "voteCount": 1,
        "content": "A. Weekly Retraining: Retraining the model every week incurs processing costs even if the flu data (target variable) hasn't changed, potentially leading to wasted resources.\nB. Monthly Retraining: While cheaper than option A, it might miss capturing the impact of recent weather changes on flu infections.\nC. Weekly Data Download, Monthly Retraining: This approach downloads weather data more frequently than necessary and still incurs retraining costs even if feature drift hasn't occurred."
      },
      {
        "date": "2024-04-08T04:05:00.000Z",
        "voteCount": 1,
        "content": "minimize cost"
      },
      {
        "date": "2024-02-18T15:46:00.000Z",
        "voteCount": 1,
        "content": "My Answer: D\n\nEven though the model predicts values for the next month, it is necessary to consume weekly data because the model's output could change based on new weekly data. Therefore, it is necessary to download data weekly and monthly. Furthermore, it is not necessary to retrain the model if the feature distribution remains unchanged."
      },
      {
        "date": "2024-01-16T04:43:00.000Z",
        "voteCount": 4,
        "content": "D. This way, cost is minimized by only retraining when feature drift takes place."
      },
      {
        "date": "2024-01-12T23:19:00.000Z",
        "voteCount": 2,
        "content": "Selective Retraining: Retraining occurs only when necessary, triggered by feature drift alerts, reducing cloud resource usage and associated costs.\nEfficient Data Utilization: Weather data is downloaded weekly to capture potential changes, but model retraining waits for monthly flu data, ensuring model relevance without excessive updates.\nEarly Drift Detection: Vertex AI's feature drift monitoring proactively identifies model performance degradation, prompting timely retraining to maintain accuracy."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 224,
    "url": "https://www.examtopics.com/discussions/google/view/131048-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You are building a MLOps platform to automate your company\u2019s ML experiments and model retraining. You need to organize the artifacts for dozens of pipelines. How should you store the pipelines\u2019 artifacts?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore parameters in Cloud SQL, and store the models\u2019 source code and binaries in GitHub.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore parameters in Cloud SQL, store the models\u2019 source code in GitHub, and store the models\u2019 binaries in Cloud Storage.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore parameters in Vertex ML Metadata, store the models\u2019 source code in GitHub, and store the models\u2019 binaries in Cloud Storage.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore parameters in Vertex ML Metadata and store the models\u2019 source code and binaries in GitHub."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-18T00:53:00.000Z",
        "voteCount": 3,
        "content": "Vertex ML Metadata: This service is specifically designed to store and track metadata for ML pipelines, including parameters. It provides a centralized location to manage and query pipeline execution details, making it ideal for dozens of pipelines.\nCloud Storage: This is a scalable and cost-effective storage solution for model binaries. It integrates well with Vertex AI and other cloud services.\nGitHub: While not a Google Cloud service, it's a popular version control system well-suited for storing and managing your models' source code, particularly for collaboration among team members."
      },
      {
        "date": "2024-04-18T00:54:00.000Z",
        "voteCount": 1,
        "content": "A. Cloud SQL for Parameters: While Cloud SQL is a relational database service, Vertex ML Metadata offers a dedicated solution for ML metadata management, including parameters, providing better integration and functionality within the MLOps context.\nD. Vertex ML Metadata for Source Code and Binaries: Vertex ML Metadata is primarily focused on ML pipeline metadata and experiment tracking. Cloud Storage is a more appropriate service for storing large binary files like model artifacts."
      },
      {
        "date": "2024-04-08T04:08:00.000Z",
        "voteCount": 1,
        "content": "shadz10"
      },
      {
        "date": "2024-01-17T15:00:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/architecture/architecture-for-mlops-using-tfx-kubeflow-pipelines-and-cloud-build"
      },
      {
        "date": "2024-01-12T23:24:00.000Z",
        "voteCount": 2,
        "content": "A. Cloud SQL and GitHub: Cloud SQL isn't designed for ML metadata management, potentially leading to challenges in tracking experiment details and lineage.\nB. Cloud SQL, GitHub, and Cloud Storage: While viable, this approach misses the benefits of Vertex ML Metadata for organized ML artifact management.\nD. Vertex ML Metadata and GitHub: Storing model binaries in GitHub can be inefficient for large files and might incur higher storage costs."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 225,
    "url": "https://www.examtopics.com/discussions/google/view/131049-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You work for a telecommunications company. You\u2019re building a model to predict which customers may fail to pay their next phone bill. The purpose of this model is to proactively offer at-risk customers assistance such as service discounts and bill deadline extensions. The data is stored in BigQuery and the predictive features that are available for model training include:<br><br>- Customer_id<br>- Age<br>- Salary (measured in local currency)<br>- Sex<br>- Average bill value (measured in local currency)<br>- Number of phone calls in the last month (integer)<br>- Average duration of phone calls (measured in minutes)<br><br>You need to investigate and mitigate potential bias against disadvantaged groups, while preserving model accuracy.<br><br>What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDetermine whether there is a meaningful correlation between the sensitive features and the other features. Train a BigQuery ML boosted trees classification model and exclude the sensitive features and any meaningfully correlated features.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTrain a BigQuery ML boosted trees classification model with all features. Use the ML.GLOBAL_EXPLAIN method to calculate the global attribution values for each feature of the model. If the feature importance value for any of the sensitive features exceeds a threshold, discard the model and tram without this feature.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTrain a BigQuery ML boosted trees classification model with all features. Use the ML.EXPLAIN_PREDICT method to calculate the attribution values for each feature for each customer in a test set. If for any individual customer, the importance value for any feature exceeds a predefined threshold, discard the model and train the model again without this feature.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDefine a fairness metric that is represented by accuracy across the sensitive features. Train a BigQuery ML boosted trees classification model with all features. Use the trained model to make predictions on a test set. Join the data back with the sensitive features, and calculate a fairness metric to investigate whether it meets your requirements.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-18T00:57:00.000Z",
        "voteCount": 2,
        "content": "Fairness Metric: Defining a metric like parity (equal accuracy) or calibration (similar predicted probabilities) across sensitive features like age, sex, or salary allows you to quantify potential bias.\nModel Training with All Features (Initially): Training the model with all features provides a baseline performance and allows you to identify potentially biased features later.\nTest Set Predictions: Making predictions on a held-out test set ensures the evaluation is based on unseen data and avoids overfitting.\nJoining Back Sensitive Features: Reintroducing sensitive features after prediction allows you to calculate fairness metrics for different customer groups.\nIterative Refinement: Based on the fairness metric results, you can determine if further mitigation strategies are needed."
      },
      {
        "date": "2024-04-18T00:58:00.000Z",
        "voteCount": 1,
        "content": "A. Excluding Features Based on Correlation: While correlated features might indicate bias, simply excluding them can discard valuable information and potentially reduce model accuracy.\nB. Global Attribution for Feature Removal: Using global feature importance might not reveal bias impacting specific customer groups. Additionally, discarding a feature solely based on importance could affect model performance.\nC. Individual Attribution for Model Discarding: While individual attribution can identify per-customer bias, discarding the model entirely based on a single instance might be overly cautious and lead to starting from scratch frequently."
      },
      {
        "date": "2024-04-08T04:14:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/vertex-ai/docs/evaluation/intro-evaluation-fairness"
      },
      {
        "date": "2024-03-10T22:23:00.000Z",
        "voteCount": 1,
        "content": "Answer is A"
      },
      {
        "date": "2024-01-17T15:03:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/vertex-ai/docs/evaluation/intro-evaluation-fairness"
      },
      {
        "date": "2024-01-12T23:27:00.000Z",
        "voteCount": 2,
        "content": "Direct Bias Assessment: It directly measures model fairness using a relevant metric, providing clear insights into potential issues.\nPreserving Information: It avoids prematurely removing features, potentially capturing valuable predictive signals while mitigating bias.\nAligning with Goals: It allows tailoring the fairness metric to specific ethical and business objectives."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 226,
    "url": "https://www.examtopics.com/discussions/google/view/131050-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You recently trained a XGBoost model that you plan to deploy to production for online inference. Before sending a predict request to your model\u2019s binary, you need to perform a simple data preprocessing step. This step exposes a REST API that accepts requests in your internal VPC Service Controls and returns predictions. You want to configure this preprocessing step while minimizing cost and effort. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore a pickled model in Cloud Storage. Build a Flask-based app, package the app in a custom container image, and deploy the model to Vertex AI Endpoints.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBuild a Flask-based app, package the app and a pickled model in a custom container image, and deploy the model to Vertex AI Endpoints.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBuild a custom predictor class based on XGBoost Predictor from the Vertex AI SDK, package it and a pickled model in a custom container image based on a Vertex built-in image, and deploy the model to Vertex AI Endpoints.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBuild a custom predictor class based on XGBoost Predictor from the Vertex AI SDK, and package the handler in a custom container image based on a Vertex built-in container image. Store a pickled model in Cloud Storage, and deploy the model to Vertex AI Endpoints.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-17T17:45:00.000Z",
        "voteCount": 3,
        "content": "why not c?\nWhile it utilizes the XGBoost Predictor, packaging the pickled model in the container increases image size and requires redeploying the container for model updates."
      },
      {
        "date": "2024-04-17T17:45:00.000Z",
        "voteCount": 2,
        "content": "why D?\nReduced Code Footprint: You only need to write the custom predictor logic, not a full Flask application. This minimizes development effort and container size.\nLeverages Vertex AI Features: By using the XGBoost Predictor from the Vertex AI SDK, you benefit from pre-built functionality for handling XGBoost models.\nCost-Effective Deployment: Utilizing Vertex built-in container images reduces the need for custom image maintenance and potentially lowers container runtime costs.\nSeparate Model Storage: Storing the pickled model in Cloud Storage keeps the model separate from the prediction logic, allowing for easier model updates without redeploying the entire container."
      },
      {
        "date": "2024-02-18T16:35:00.000Z",
        "voteCount": 1,
        "content": "My Answer: D\n\nThis option involves using the Vertex AI SDK to build a custom predictor class, which allows for easy integration with the XGBoost model. Packaging the handler in a custom container image based on a Vertex built-in container image ensures compatibility and smooth deployment. Storing the pickled model in Cloud Storage provides a scalable and reliable way to access the model.  Deploying the model to Vertex AI Endpoints allows for easy management and scaling of inference requests, while minimizing cost and effort.\n\nThe main difference between C and D is where the model is saved. So, is a good practice to save models in GCS because Separation of Concerns, Flexibility, and Reduced Image Size"
      },
      {
        "date": "2024-01-12T23:35:00.000Z",
        "voteCount": 1,
        "content": "Minimal Custom Code: Leverages the pre-built XGBoost Predictor class for core model prediction, reducing development effort and potential errors.\nOptimized Container Image: Utilizes a Vertex built-in container image, pre-configured for efficient model serving and compatibility with Vertex AI Endpoints.\nSeparated Model Storage: Stores the model in Cloud Storage, reducing container image size and simplifying model updates independently of the container.\nVPC Service Controls: Vertex AI Endpoints support VPC Service Controls, ensuring adherence to internal traffic restrictions."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 227,
    "url": "https://www.examtopics.com/discussions/google/view/130773-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You work at a bank. You need to develop a credit risk model to support loan application decisions. You decide to implement the model by using a neural network in TensorFlow. Due to regulatory requirements, you need to be able to explain the model\u2019s predictions based on its features. When the model is deployed, you also want to monitor the model\u2019s performance over time. You decided to use Vertex AI for both model development and deployment. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Vertex Explainable AI with the sampled Shapley method, and enable Vertex AI Model Monitoring to check for feature distribution drift.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Vertex Explainable AI with the sampled Shapley method, and enable Vertex AI Model Monitoring to check for feature distribution skew.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Vertex Explainable AI with the XRAI method, and enable Vertex AI Model Monitoring to check for feature distribution drift.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Vertex Explainable AI with the XRAI method, and enable Vertex AI Model Monitoring to check for feature distribution skew."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 12,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-16T07:39:00.000Z",
        "voteCount": 8,
        "content": "Not image -&gt; not XRAI\nPerformance over time -&gt; drift, not skew"
      },
      {
        "date": "2024-04-17T17:52:00.000Z",
        "voteCount": 2,
        "content": "why not the others?\nB. Feature Distribution Skew: While skew can be relevant, drift is generally a more significant concern for credit risk models. Drift indicates a change in the underlying data distribution, potentially impacting model performance.\nC &amp; D. XRAI Method: XRAI (Explainable AI for Images) is specifically designed for interpreting image classification models. It wouldn't be the most effective choice for a neural network-based credit risk model working with tabular data."
      },
      {
        "date": "2024-04-17T17:54:00.000Z",
        "voteCount": 1,
        "content": "Vertex Explainable AI: This is a built-in Vertex AI feature that helps understand how features contribute to model predictions.\nSampled Shapley Method: This is a well-suited method for explaining complex models like neural networks. It provides insights into feature importance without requiring retraining the entire model."
      },
      {
        "date": "2024-01-10T01:41:00.000Z",
        "voteCount": 2,
        "content": "Explainable AI with the XRAI method is for unstructured, image region analysis, in this case we use structured data for loan approval analysis."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 228,
    "url": "https://www.examtopics.com/discussions/google/view/131051-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You are investigating the root cause of a misclassification error made by one of your models. You used Vertex AI Pipelines to train and deploy the model. The pipeline reads data from BigQuery. creates a copy of the data in Cloud Storage in TFRecord format, trains the model in Vertex AI Training on that copy, and deploys the model to a Vertex AI endpoint. You have identified the specific version of that model that misclassified, and you need to recover the data this model was trained on. How should you find that copy of the data?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Vertex AI Feature Store. Modify the pipeline to use the feature store, and ensure that all training data is stored in it. Search the feature store for the data used for the training.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the lineage feature of Vertex AI Metadata to find the model artifact. Determine the version of the model and identify the step that creates the data copy and search in the metadata for its location.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the logging features in the Vertex AI endpoint to determine the timestamp of the model\u2019s deployment. Find the pipeline run at that timestamp. Identify the step that creates the data copy, and search in the logs for its location.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFind the job ID in Vertex AI Training corresponding to the training for the model. Search in the logs of that job for the data used for the training."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-17T17:57:00.000Z",
        "voteCount": 3,
        "content": "Vertex AI Metadata Lineage: This feature tracks the relationships between pipeline components and the artifacts they produce. By identifying the model version's lineage, you can pinpoint the specific pipeline run that generated it.\nData Copy Step: Within the pipeline run, locate the step responsible for creating the data copy in TFRecord format for training.\nMetadata Search: Vertex AI Metadata likely stores information about the data copy's location in Cloud Storage, allowing you to access it."
      },
      {
        "date": "2024-04-17T17:58:00.000Z",
        "voteCount": 1,
        "content": "A. Feature Store: Feature Store is designed for managing feature engineering and serving preprocessed features, not necessarily raw training data. While it could be a good practice for future pipelines, it wouldn't help recover historical data.\nC. Endpoint Logs: Endpoint logs primarily focus on model deployment details and might not provide information about the specific training data used for a particular version.\nD. Training Job Logs: Training job logs might contain references to the data used, but they might not be as detailed or structured as Vertex AI Metadata lineage, making it harder to pinpoint the exact data copy location."
      },
      {
        "date": "2024-04-08T04:20:00.000Z",
        "voteCount": 1,
        "content": "agree with shadz10 and pikachu007"
      },
      {
        "date": "2024-01-17T15:29:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/vertex-ai/docs/ml-metadata/introduction"
      },
      {
        "date": "2024-01-12T23:42:00.000Z",
        "voteCount": 3,
        "content": "A. Feature Store: While useful for managing features, it might not store complete training datasets, and modifying the pipeline would not help recover historical data.\nC. Endpoint Logs and Pipeline Run: This approach involves more manual searching and might be less precise for identifying the exact data copy.\nD. Training Job Logs: Training job logs might not reliably contain complete data paths or might be purged after a certain period."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 229,
    "url": "https://www.examtopics.com/discussions/google/view/131297-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You work for a manufacturing company. You need to train a custom image classification model to detect product defects at the end of an assembly line. Although your model is performing well, some images in your holdout set are consistently mislabeled with high confidence. You want to use Vertex AI to understand your model\u2019s results. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure feature-based explanations by using Integrated Gradients. Set visualization type to PIXELS, and set clip_percent_upperbound to 95.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an index by using Vertex AI Matching Engine. Query the index with your mislabeled images.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure feature-based explanations by using XRAI. Set visualization type to OUTLINES, and set polarity to positive.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure example-based explanations. Specify the embedding output layer to be used for the latent space representation.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 5,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-10-12T23:44:00.000Z",
        "voteCount": 1,
        "content": "It is to understand why model is making specific mistakes, so example-based explanation makes sense to me."
      },
      {
        "date": "2024-09-01T10:00:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/vertex-ai/docs/explainable-ai/overview#example-based\n\n\"Improve your data or model: One of the core use cases for example-based explanations is helping you understand why your model made certain mistakes in its predictions, and using those insights to improve your data or model. [...]\n\nFor example, suppose we have a model that classifies images as either a bird or a plane, and that it is misclassifying the following bird as a plane with high confidence. You can use Example-based explanations to retrieve similar images from the training set to figure out what is happening.\"\n\nNot A: Integrated Gradients is recommended for low-contrast images, such as X-rays\nhttps://cloud.google.com/vertex-ai/docs/explainable-ai/overview#compare-methods\n\nNot C: Cannot set Outlines for XRAI\nhttps://cloud.google.com/ai-platform/prediction/docs/ai-explanations/visualizing-explanations"
      },
      {
        "date": "2024-03-15T00:24:00.000Z",
        "voteCount": 3,
        "content": "Improve your data or model: One of the core use cases for example-based explanations is helping you understand why your model made certain mistakes in its predictions, and using those insights to improve your data or model.\n\nhttps://cloud.google.com/vertex-ai/docs/explainable-ai/overview"
      },
      {
        "date": "2024-02-15T11:46:00.000Z",
        "voteCount": 3,
        "content": "My Answer: A\n\nAccording to this documentation:\n\nhttps://cloud.google.com/vertex-ai/docs/explainable-ai/visualization-settings\n\nThis option A aligns with using Integrated Gradients, which is suitable for feature-based explanations. Setting the visualization type to PIXELS allows for per-pixel attribution, which can help in understanding the specific regions of the image influencing the model's decision. Additionally, setting the clip_percent_upperbound parameter to 95 helps in filtering out noise and focusing on areas of strong attribution, which is crucial for understanding mislabeled images with high confidence.\n\nOption C suggests using XRAI for feature-based explanations and setting the visualization type to OUTLINES, along with setting the polarity to positive. However, based on the provided documentation, XRAI is recommended to have its visualization type set to PIXELS, not OUTLINES."
      },
      {
        "date": "2024-01-31T01:56:00.000Z",
        "voteCount": 2,
        "content": "Although Xrai could be an option, it doesn't not allow you to set those options, so only other answer is A\nhttps://cloud.google.com/vertex-ai/docs/explainable-ai/visualization-settings#visualization_options"
      },
      {
        "date": "2024-02-03T18:20:00.000Z",
        "voteCount": 1,
        "content": "Why not it's D? \nhttps://cloud.google.com/vertex-ai/docs/explainable-ai/overview"
      },
      {
        "date": "2024-02-03T18:22:00.000Z",
        "voteCount": 1,
        "content": "For example, suppose we have a model that classifies images as either a bird or a plane, and that it is misclassifying the following bird as a plane with high confidence. You can use Example-based explanations to retrieve similar images from the training set to figure out what is happening."
      },
      {
        "date": "2024-02-05T05:06:00.000Z",
        "voteCount": 1,
        "content": "yes you are correct, but having to specify the output layer to be used is definitely no guarantee that you'll get examples that are easily interpretable  (imo)"
      },
      {
        "date": "2024-01-16T04:31:00.000Z",
        "voteCount": 2,
        "content": "Going with A\nNot c - For XRAI, Pixels is the default setting and shows areas of attribution. Outlines is not recommended for XRAI.\nhttps://cloud.google.com/ai-platform/prediction/docs/ai-explanations/visualizing-explanations"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 230,
    "url": "https://www.examtopics.com/discussions/google/view/131392-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You are training models in Vertex AI by using data that spans across multiple Google Cloud projects. You need to find, track, and compare the performance of the different versions of your models. Which Google Cloud services should you include in your ML workflow?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDataplex, Vertex AI Feature Store, and Vertex AI TensorBoard",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tVertex AI Pipelines, Vertex AI Feature Store, and Vertex AI Experiments",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDataplex, Vertex AI Experiments, and Vertex AI ML Metadata",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tVertex AI Pipelines, Vertex AI Experiments, and Vertex AI Metadata\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 7,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-04-17T18:22:00.000Z",
        "voteCount": 3,
        "content": "Why not the others?\nA. Dataplex &amp; Vertex AI Feature Store: While Dataplex can manage data across projects, it's not directly tied to model versioning and comparison. Feature Store focuses on feature engineering, not model version management.\nB. Vertex AI Feature Store &amp; Vertex AI TensorBoard: Similar to option A, Feature Store isn't directly involved in model version tracking, and TensorBoard is primarily for visualizing training data and metrics, not model version comparison across projects.\nC. Dataplex &amp; Vertex AI ML Metadata: Dataplex, as mentioned earlier, doesn't directly address model version comparison. While ML Metadata tracks lineage, it might not have the experiment management features of Vertex AI Experiments."
      },
      {
        "date": "2024-04-17T18:23:00.000Z",
        "voteCount": 2,
        "content": "Vertex AI Pipelines (Optional): While optional, pipelines can automate your training workflow, including data access from BigQuery tables in different projects. It helps orchestrate the training process across projects.\nVertex AI Experiments: This service is crucial for tracking and comparing the performance of different model versions. It allows you to:\nRun multiple training experiments with different configurations.\nTrack experiment metrics like accuracy, precision, recall, etc.\nCompare the performance of different model versions trained in various projects.\nVertex AI Metadata: This service provides a centralized view of your ML workflow, including model lineage and versioning. It's particularly helpful in your scenario because:\nIt tracks the origin and relationships between models, including the specific data used for training, regardless of the project.\nYou can see how different model versions (potentially trained across projects) relate to each other and the data they were trained on."
      },
      {
        "date": "2024-03-25T08:55:00.000Z",
        "voteCount": 2,
        "content": "went with D"
      },
      {
        "date": "2024-03-10T00:37:00.000Z",
        "voteCount": 2,
        "content": "I would go with option D.\n\nNo Vertex AI pipeline no orchestration. So rule out A and C.\nVertex AI Metadata is for 'spans across multiple Google Cloud projects' data used by the model."
      },
      {
        "date": "2024-02-26T05:10:00.000Z",
        "voteCount": 1,
        "content": "Why not Option D?"
      },
      {
        "date": "2024-02-18T16:49:00.000Z",
        "voteCount": 2,
        "content": "My Answer: B\nVertex AI Pipelines: to create, deploy, and manage ML pipelines, which are essential for orchestrating your ML workflow, especially when dealing with data spanning multiple projects.\nVertex AI Feature Store: It's crucial for managing feature data across different projects.\nVertex AI Experiments: track and compare the performance of different versions of your models, enabling you to experiment \n\nWhy not the other:\nDataplex: not specifically tailored for managing ML workflows or model training.\nVertex AI ML metadata:  not sufficient on its own to cover all aspects of managing the ML workflow across multiple projects.\nVertex AI TensorBoard:  not specifically designed for managing the end-to-end ML workflow or tracking model versions across multiple projects."
      },
      {
        "date": "2024-05-07T03:53:00.000Z",
        "voteCount": 1,
        "content": "I feel, Vertex AI Feature Store is valuable for managing and serving features for ML models, but it doesn't address the need for tracking experiments and managing metadata, right?"
      },
      {
        "date": "2024-01-29T13:48:00.000Z",
        "voteCount": 1,
        "content": "Dataplex works well with the data across projects and even on-prem, but doesn't work well with the ML related data like tracking and performance. So options A and C are considered wrong. \n\nMetadata is to store metadata. So it is not required while we consider to compare the model performance. So option D is wrong. \n\nOn the other hand Feature store brings meaningful data for comparing the Models performance based on feature data. So Option B is correct"
      },
      {
        "date": "2024-01-17T06:57:00.000Z",
        "voteCount": 3,
        "content": "I go with C.\nDataplex to centralize different Google projects.\nVertex AI experiments + ML Metadata to track experiment lineage, parameter usage etc and compare models."
      },
      {
        "date": "2024-01-26T03:11:00.000Z",
        "voteCount": 5,
        "content": "How about Option D? the Pipeline can also do the cross project data processing."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 231,
    "url": "https://www.examtopics.com/discussions/google/view/131052-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You are using Keras and TensorFlow to develop a fraud detection model. Records of customer transactions are stored in a large table in BigQuery. You need to preprocess these records in a cost-effective and efficient way before you use them to train the model. The trained model will be used to perform batch inference in BigQuery. How should you implement the preprocessing workflow?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement a preprocessing pipeline by using Apache Spark, and run the pipeline on Dataproc. Save the preprocessed data as CSV files in a Cloud Storage bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLoad the data into a pandas DataFrame. Implement the preprocessing steps using pandas transformations, and train the model directly on the DataFrame.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPerform preprocessing in BigQuery by using SQL. Use the BigQueryClient in TensorFlow to read the data directly from BigQuery.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement a preprocessing pipeline by using Apache Beam, and run the pipeline on Dataflow. Save the preprocessed data as CSV files in a Cloud Storage bucket."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-17T07:10:00.000Z",
        "voteCount": 5,
        "content": "Easiest to preprocess the data on BigQuery."
      },
      {
        "date": "2024-04-08T10:50:00.000Z",
        "voteCount": 2,
        "content": "went with C"
      },
      {
        "date": "2024-04-26T07:29:00.000Z",
        "voteCount": 2,
        "content": "Easiest to preprocess the data on BigQuery."
      },
      {
        "date": "2024-01-12T23:57:00.000Z",
        "voteCount": 2,
        "content": "A. Spark on Dataproc: While powerful, it incurs additional cluster setup and management costs, potentially less cost-effective for this specific use case.\nB. pandas DataFrame: Loading large datasets into memory might lead to resource constraints and performance issues, especially for large-scale preprocessing.\nD. Apache Beam on Dataflow: While scalable, it introduces extra complexity for managing a separate pipeline and storage for preprocessed data."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 232,
    "url": "https://www.examtopics.com/discussions/google/view/131053-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You need to use TensorFlow to train an image classification model. Your dataset is located in a Cloud Storage directory and contains millions of labeled images. Before training the model, you need to prepare the data. You want the data preprocessing and model training workflow to be as efficient, scalable, and low maintenance as possible. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a Dataflow job that creates sharded TFRecord files in a Cloud Storage directory.<br>2. Reference tf.data.TFRecordDataset in the training script.<br>3. Train the model by using Vertex AI Training with a V100 GPU.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a Dataflow job that moves the images into multiple Cloud Storage directories, where each directory is named according to the corresponding label<br>2. Reference tfds.folder_dataset:ImageFolder in the training script.<br>3. Train the model by using Vertex AI Training with a V100 GPU.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a Jupyter notebook that uses an nt-standard-64 V100 GPU Vertex AI Workbench instance.<br>2. Write a Python script that creates sharded TFRecord files in a directory inside the instance.<br>3. Reference tf.data.TFRecordDataset in the training script.<br>4. Train the model by using the Workbench instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a Jupyter notebook that uses an n1-standard-64, V100 GPU Vertex AI Workbench instance.<br>2. Write a Python script that copies the images into multiple Cloud Storage directories, where each. directory is named according to the corresponding label.<br>3. Reference tfds.foladr_dataset.ImageFolder in the training script.<br>4. Train the model by using the Workbench instance."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-08T10:53:00.000Z",
        "voteCount": 5,
        "content": "millions of labeled images -&gt; dataflow\ntfrecord faster than folder-based"
      },
      {
        "date": "2024-07-04T16:31:00.000Z",
        "voteCount": 1,
        "content": "A is correct Here's why \nYou need to prepare the data before training an image classification model.\nUsing TFRecord files allows you to store your data in a format that can be efficiently read and processed by TensorFlow.\nSharding the data into multiple files allows for parallel processing and scalability.\nDataflow is a Google Cloud service that provides a scalable and reliable way to process large datasets.\nBy using Vertex AI Training with a V100 GPU, you can train your model in an efficient and cost-effective manner."
      },
      {
        "date": "2024-01-17T07:18:00.000Z",
        "voteCount": 3,
        "content": "Ideally you want to export your data in TFRecords (most efficient image format) in Cloud Storage, and not in the instance (to improve scalability)"
      },
      {
        "date": "2024-01-13T00:01:00.000Z",
        "voteCount": 2,
        "content": "B. Folder-Based Structure: While viable, it's less efficient for large datasets compared to TFRecord files, potentially leading to slower I/O during training.\nC. Workbench Processing: Local preprocessing on a single instance can be less scalable and efficient for millions of images, potentially introducing bottlenecks.\nD. Workbench Training: While Workbench offers a Jupyter environment, Vertex AI Training is specifically designed for scalable model training, providing optimized hardware and infrastructure."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 233,
    "url": "https://www.examtopics.com/discussions/google/view/131054-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You are building a custom image classification model and plan to use Vertex AI Pipelines to implement the end-to-end training. Your dataset consists of images that need to be preprocessed before they can be used to train the model. The preprocessing steps include resizing the images, converting them to grayscale, and extracting features. You have already implemented some Python functions for the preprocessing tasks. Which components should you use in your pipeline?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDataprocSparkBatchOp and CustomTrainingJobOp",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDataflowPythonJobOp, WaitGcpResourcesOp, and CustomTrainingJobOp\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tdsl.ParallelFor, dsl.component, and CustomTrainingJobOp",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImageDatasetImportDataOp, dsl.component, and AutoMLImageTrainingJobRunOp"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-09-17T05:38:00.000Z",
        "voteCount": 1,
        "content": "B is definitely right, no doubt"
      },
      {
        "date": "2024-04-08T10:57:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/vertex-ai/docs/pipelines/dataflow-component#dataflowpythonjobop"
      },
      {
        "date": "2024-02-15T13:15:00.000Z",
        "voteCount": 3,
        "content": "My Answer: B\n\nLooking for the options, DataflowPythonJobOp can be used for parallelizing the preprocessing tasks, which is suitable for image resizing, converting to grayscale, and extracting features. dsl.ParallelFor could be useful for parallelizing tasks but might not be the most straightforward option for image preprocessing. \n\nGenerally DataflowPythonJobOp is followed by WaitGcpResourcesOp. \n\nhttps://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/fe7d3e4b8edc137d90ec061789b879b7cc8d3854/notebooks/community/ml_ops/stage3/get_started_with_dataflow_flex_template_component.ipynb"
      },
      {
        "date": "2024-01-17T07:29:00.000Z",
        "voteCount": 1,
        "content": "I go with B. Custom training is surely required. Discarding A because Spark is not mentioned anywhere in the problem description. C involves Kubeflow which seems a bit overkill imo. DataflowPythonJobOp operator lets you create a Vertex AI Pipelines component that prepares data -&gt; seems like the appropriate course of action to me. https://cloud.google.com/vertex-ai/docs/pipelines/dataflow-component#dataflowpythonjobop"
      },
      {
        "date": "2024-01-13T00:04:00.000Z",
        "voteCount": 1,
        "content": "A. DataprocSparkBatchOp: While capable of data processing, it's less well-suited for image-specific tasks like resizing and grayscale conversion compared to DataflowPythonJobOp.\nC. dsl.ParallelFor, dsl.component: While offering flexibility, they require more manual orchestration and potentially less efficient for image preprocessing compared to DataflowPythonJobOp.\nD. ImageDatasetImportDataOp, AutoMLImageTrainingJobRunOp: These components are designed for AutoML Image training, not directly compatible with custom preprocessing and training tasks."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 234,
    "url": "https://www.examtopics.com/discussions/google/view/131055-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You work for a retail company that is using a regression model built with BigQuery ML to predict product sales. This model is being used to serve online predictions. Recently you developed a new version of the model that uses a different architecture (custom model). Initial analysis revealed that both models are performing as expected. You want to deploy the new version of the model to production and monitor the performance over the next two months. You need to minimize the impact to the existing and future model users. How should you deploy the model?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImport the new model to the same Vertex AI Model Registry as a different version of the existing model. Deploy the new model to the same Vertex AI endpoint as the existing model, and use traffic splitting to route 95% of production traffic to the BigQuery ML model and 5% of production traffic to the new model.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImport the new model to the same Vertex AI Model Registry as the existing model. Deploy the models to one Vertex AI endpoint. Route 95% of production traffic to the BigQuery ML model and 5% of production traffic to the new model.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImport the new model to the same Vertex AI Model Registry as the existing model. Deploy each model to a separate Vertex AI endpoint.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the new model to a separate Vertex AI endpoint. Create a Cloud Run service that routes the prediction requests to the corresponding endpoints based on the input feature values."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 11,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-07-28T15:07:00.000Z",
        "voteCount": 1,
        "content": "I\u2019m considering two options, A and B. Both deploy to the same endpoint and divide traffic in a similar way. However, option B is more appropriate because it generates a new model rather than just creating a new version of the existing model."
      },
      {
        "date": "2024-04-17T19:21:00.000Z",
        "voteCount": 3,
        "content": "Minimal Disruption: Deploying the new model to the same endpoint avoids changes for existing users. Traffic splitting ensures a gradual rollout, minimizing any potential impact on production.\nPerformance Monitoring: By routing a small percentage of traffic (5%) to the new model, you can monitor its performance in a controlled environment for the next two months. Metrics like prediction accuracy and latency can be compared with the BigQuery ML model.\nVersioning in Model Registry: Storing both models in the same Vertex AI Model Registry with clear versioning allows easy tracking and management."
      },
      {
        "date": "2024-04-17T19:21:00.000Z",
        "voteCount": 1,
        "content": "why not others option?\nB. Deploying Models to One Endpoint without Traffic Splitting: This approach doesn't allow for controlled rollout and could abruptly switch all traffic to the new model, potentially causing disruptions.\nC. Deploying Models to Separate Endpoints: This requires users to update their prediction pipelines to interact with the new endpoint, introducing unnecessary complexity and potential delays.\nD. Cloud Run Service with Feature-Based Routing: While Cloud Run can route traffic, feature-based routing might be more complex to implement for sales prediction and might not be necessary with traffic splitting."
      },
      {
        "date": "2024-04-08T11:04:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/vertex-ai/docs/general/deployment#models-endpoint"
      },
      {
        "date": "2024-01-28T19:46:00.000Z",
        "voteCount": 4,
        "content": "A, no need to separate endpoint."
      },
      {
        "date": "2024-01-18T14:55:00.000Z",
        "voteCount": 2,
        "content": "as i understand we need to minimize the impact to the model users, so if we take a part of the traffic from the old model users, we will effect them. As for me we should deploy models to separated endpoints and duplicate the traffic"
      },
      {
        "date": "2024-01-13T00:07:00.000Z",
        "voteCount": 2,
        "content": "B. Doesn't Specify Traffic Splitting: Deploying models to a single endpoint without explicit traffic splitting might lead to unpredictable model selection behavior, hindering controlled evaluation.\nC. Separate Endpoints: While isolating models, it introduces complexity in managing multiple endpoints and routing logic, increasing operational overhead.\nD. Cloud Run Routing: Adds complexity by requiring a separate service to manage routing, potentially increasing latency and maintenance overhead compared to Vertex AI's built-in traffic splitting."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 235,
    "url": "https://www.examtopics.com/discussions/google/view/131056-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You are using Vertex AI and TensorFlow to develop a custom image classification model. You need the model\u2019s decisions and the rationale to be understandable to your company\u2019s stakeholders. You also want to explore the results to identify any issues or potential biases. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Use TensorFlow to generate and visualize features and statistics.<br>2. Analyze the results together with the standard model evaluation metrics.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Use TensorFlow Profiler to visualize the model execution.<br>2. Analyze the relationship between incorrect predictions and execution bottlenecks.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Use Vertex Explainable AI to generate example-based explanations.<br>2. Visualize the results of sample inputs from the entire dataset together with the standard model evaluation metrics.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Use Vertex Explainable AI to generate feature attributions. Aggregate feature attributions over the entire dataset.<br>2. Analyze the aggregation result together with the standard model evaluation metrics.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-02-18T16:55:00.000Z",
        "voteCount": 6,
        "content": "My Answer: D\n\nThis approach leverages Vertex Explainable AI to provide feature attributions, which helps in understanding the rationale behind the model's decisions. By aggregating these feature attributions over the entire dataset, you can gain insights into potential biases or areas of concern. Analyzing these results alongside standard model evaluation metrics allows for a comprehensive understanding of the model's performance and its interpretability.\n\nOption C is better to understand specific cases, but does not show overall contributions."
      },
      {
        "date": "2024-04-20T09:24:00.000Z",
        "voteCount": 1,
        "content": "agree with guilhermebutzke"
      },
      {
        "date": "2024-04-16T10:37:00.000Z",
        "voteCount": 1,
        "content": "Debugging models: Feature attributions can help detect issues in the data that standard model evaluation techniques would usually miss."
      },
      {
        "date": "2024-01-17T16:59:00.000Z",
        "voteCount": 3,
        "content": "If you inspect specific instances, and also aggregate feature attributions across your training dataset, you can get deeper insight into how your model works. Consider the following advantages:\n\nDebugging models: Feature attributions can help detect issues in the data that standard model evaluation techniques would usually miss.\nhttps://cloud.google.com/vertex-ai/docs/explainable-ai/overview"
      },
      {
        "date": "2024-01-17T07:52:00.000Z",
        "voteCount": 3,
        "content": "C. Example-based explanations make more sense in this case than feature based attributions (we want to understand with examples what kind of decisions the model takes; also explore the amount of bias in a visual, understandable way) https://cloud.google.com/vertex-ai/docs/explainable-ai/overview#example-based"
      },
      {
        "date": "2024-01-13T00:17:00.000Z",
        "voteCount": 3,
        "content": "Feature-Level Insights: Feature attributions pinpoint which image regions contribute most to predictions, offering granular understanding of model reasoning.\nBias Detection: Aggregating feature attributions over the entire dataset can reveal systematic biases or patterns of model behavior, helping identify potential fairness issues.\nComplementary to Evaluation Metrics: Combining attributions with standard metrics (e.g., accuracy, precision, recall) provides a comprehensive view of model performance and fairness."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 236,
    "url": "https://www.examtopics.com/discussions/google/view/131057-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You work for a large retailer, and you need to build a model to predict customer chum. The company has a dataset of historical customer data, including customer demographics purchase history, and website activity. You need to create the model in BigQuery ML and thoroughly evaluate its performance. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a linear regression model in BigQuery ML, and register the model in Vertex AI Model Registry. Evaluate the model performance in Vertex AI .",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a logistic regression model in BigQuery ML and register the model in Vertex AI Model Registry. Evaluate the model performance in Vertex AI .\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a linear regression model in BigQuery ML. Use the ML.EVALUATE function to evaluate the model performance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a logistic regression model in BigQuery ML. Use the ML.CONFUSION_MATRIX function to evaluate the model performance."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 11,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-09-17T05:43:00.000Z",
        "voteCount": 1,
        "content": "B is the definitive answer. By breaking down the question we know it is a classification problem, so A and C are wrong since they're linear regression.\n\nUsing confusion matrix to evaluate the model is not wrong (actually it's even the textbook answer to do it), but it is not enough if you want to thoroughly evaluate its performance. Hence the best way to do it is with Vertex AI."
      },
      {
        "date": "2024-04-20T09:25:00.000Z",
        "voteCount": 3,
        "content": "logistic since it's classification, and Vertex AI because we need to \"thoroughly evaluate its performance\""
      },
      {
        "date": "2024-04-17T19:41:00.000Z",
        "voteCount": 1,
        "content": "Logistic Regression: While linear regression (option C) can be used for continuous prediction tasks, customer churn is a binary classification problem (churned/not churned). Logistic regression is a better fit for this scenario.\nVertex AI Model Registry: Registering the model in Vertex AI Model Registry provides a centralized location for model management, versioning, and potentially future deployment to other Vertex AI services.\nVertex AI Evaluation: Vertex AI offers more comprehensive evaluation tools than BigQuery ML's ML.EVALUATE function (option C) or ML.CONFUSION_MATRIX function (option D). Vertex AI can provide metrics like accuracy, ROC-AUC, precision, and recall, which are crucial for churn prediction evaluation."
      },
      {
        "date": "2024-02-15T14:24:00.000Z",
        "voteCount": 1,
        "content": "My Answer: B\n\npredict customer churn, which is a binary classification problem (whether a customer will churn or not). And, the phrase \"thoroughly evaluate its performance\" does suggest a more comprehensive approach, and in that sense, option B could be seen as a better answer than D."
      },
      {
        "date": "2024-02-15T14:24:00.000Z",
        "voteCount": 1,
        "content": "My Answer: B\n\npredict customer churn, which is a binary classification problem (whether a customer will churn or not). And, the phrase \"thoroughly evaluate its performance\" does suggest a more comprehensive approach, and in that sense, option B could be seen as a better answer than D."
      },
      {
        "date": "2024-01-18T15:24:00.000Z",
        "voteCount": 1,
        "content": "B because Vertex AI provides us with more functions to evaluate model performance then just CONFUSION_MATRIX \n https://cloud.google.com/vertex-ai/docs/evaluation/introduction#classification_1"
      },
      {
        "date": "2024-01-17T07:59:00.000Z",
        "voteCount": 4,
        "content": "B.\nLinear regression because customer churn is a number of customers (not just 1/0). The key here imo is \"thoroughly evaluate performance\", which Vertex AI seems to be better suited for than BQ (including the possibility of tracking experiment lineage, inspecting parameter selection of each run, etc)"
      },
      {
        "date": "2024-01-13T00:20:00.000Z",
        "voteCount": 2,
        "content": "Customer churn prediction involves a binary classification task (whether a customer will churn or not). Logistic regression is specifically designed for this type of problem, making it the appropriate model.\n\nBigQuery ML allows building and training logistic regression models directly within BigQuery, leveraging its scalability and SQL-like syntax for model development."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 237,
    "url": "https://www.examtopics.com/discussions/google/view/131058-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You are developing a model to identify traffic signs in images extracted from videos taken from the dashboard of a vehicle. You have a dataset of 100,000 images that were cropped to show one out of ten different traffic signs. The images have been labeled accordingly for model training, and are stored in a Cloud Storage bucket. You need to be able to tune the model during each training run. How should you train the model?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTrain a model for object detection by using Vertex AI AutoML.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTrain a model for image classification by using Vertex AI AutoML.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDevelop the model training code for object detection, and train a model by using Vertex AI custom training.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDevelop the model training code for image classification, and train a model by using Vertex AI custom training.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 17,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-13T00:23:00.000Z",
        "voteCount": 14,
        "content": "Not A or B since automl doesnt provide you without flexibility to tune.\n\nNot C because object detection is not required since the images are cropped to a single traffic light"
      },
      {
        "date": "2024-09-17T05:53:00.000Z",
        "voteCount": 1,
        "content": "Answer is D, not C in my opinion. Object detection might be used in a real-world project since there are a lot of variables which may affect the picture like visibility, colour fading, having other things in the picture like streetlights, birds, etc. Way too overkill for our question here. I'm assuming the pictures are already nicely cropped out with none of this extra stuff in the pictures."
      },
      {
        "date": "2024-09-13T07:10:00.000Z",
        "voteCount": 1,
        "content": "There is literally no way to know if this is C or D, as \"labelled\" and \"identify street signs\" are too ambiguous to know if its detection or classification. The \"cropped to a single traffic light\" seems like maybe D, but that's hardly ML knowledge, it's a pub quiz guess."
      },
      {
        "date": "2024-04-20T09:28:00.000Z",
        "voteCount": 1,
        "content": "agree with pikachu007"
      },
      {
        "date": "2024-04-08T11:16:00.000Z",
        "voteCount": 1,
        "content": "Not C because object detection is not required since the images are cropped to a single traffic light"
      },
      {
        "date": "2024-02-15T14:39:00.000Z",
        "voteCount": 2,
        "content": "Correct: C\n\nThe phrases \"identify traffic signs in images extracted from videos\" and \"images that were cropped to show one out of ten different traffic signs\" suggest that this is an image detection problem. The first phrase appears to have the same meaning as \"images with,\" and the second phrase suggests that only one type of traffic sign was used in the problem, indicating that it cannot be used in a multi-class problem. For all these reasons, I believe the best option is C."
      },
      {
        "date": "2024-02-15T14:39:00.000Z",
        "voteCount": 2,
        "content": "My answer: C\n\nThe phrases \"identify traffic signs in images extracted from videos\" and \"images that were cropped to show one out of ten different traffic signs\" suggest that this is an image detection problem. The first phrase appears to have the same meaning as \"images with,\" and the second phrase suggests that only one type of traffic sign was used in the problem, indicating that it cannot be used in a multi-class problem. For all these reasons, I believe the best option is C."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 238,
    "url": "https://www.examtopics.com/discussions/google/view/131059-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You have deployed a scikit-team model to a Vertex AI endpoint using a custom model server. You enabled autoscaling: however, the deployed model fails to scale beyond one replica, which led to dropped requests. You notice that CPU utilization remains low even during periods of high load. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAttach a GPU to the prediction nodes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the number of workers in your model server\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSchedule scaling of the nodes to match expected demand",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the minReplicaCount in your DeployedModel configuration"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 7,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 6,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-31T02:48:00.000Z",
        "voteCount": 6,
        "content": "\"We generally recommend starting with one worker or thread per core. If you notice that CPU utilization is low, especially under high load, or your model is not scaling up because CPU utilization is low, then increase the number of workers.\"\nhttps://cloud.google.com/vertex-ai/docs/general/deployment"
      },
      {
        "date": "2024-01-31T02:49:00.000Z",
        "voteCount": 2,
        "content": "sorry clicked wrong, answer is B"
      },
      {
        "date": "2024-04-17T19:49:00.000Z",
        "voteCount": 1,
        "content": "agree with sonicclasps -&gt; B"
      },
      {
        "date": "2024-04-13T06:58:00.000Z",
        "voteCount": 1,
        "content": "agree with sonicclasps -&gt; B"
      },
      {
        "date": "2024-04-21T10:54:00.000Z",
        "voteCount": 1,
        "content": "NOT D: This might help ensure at least one replica is always available, but it won't address the issue of not scaling up during high load."
      },
      {
        "date": "2024-02-26T04:23:00.000Z",
        "voteCount": 2,
        "content": "I went B"
      },
      {
        "date": "2024-02-15T16:18:00.000Z",
        "voteCount": 1,
        "content": "My answer: C\n\nThe problem is in scale. The provided resources areok. So,\n\nA: Not correct, because CPU is enough.\n\nB: Not correct, because increasing the number of workers will accelerate the process in a single replica, and make the time of prediction faster for example, but not will happen in scale problem.\n\nC:Correct: This option involves adjusting the scaling of resources to match the expected demand, ensuring that the system can handle increased loads effectively\n\nD: This might help ensure at least one replica is always available, but it won't address the issue of not scaling up during high load."
      },
      {
        "date": "2024-01-13T00:25:00.000Z",
        "voteCount": 3,
        "content": "Low CPU Utilization: Despite high load, low CPU utilization indicates underutilization of available resources, suggesting a bottleneck within the model server itself, not overall compute capacity.\nWorker Concurrency: Increasing the number of workers within the model server allows it to handle more concurrent requests, effectively utilizing available CPU resources and addressing the bottleneck."
      },
      {
        "date": "2024-01-19T06:39:00.000Z",
        "voteCount": 1,
        "content": "i don't get it. The autoscaling system should increase/decrease the number of workers itself. if we do it instead of the autoscaling system, why do we need it?"
      },
      {
        "date": "2024-02-15T16:18:00.000Z",
        "voteCount": 1,
        "content": "Increase the number of workers within the model server will distribute the load within the single replica, but it wouldn't address the problem of not scaling beyond one replica. Increasin worker will be a good option for delay in prediction."
      },
      {
        "date": "2024-07-14T13:56:00.000Z",
        "voteCount": 1,
        "content": "Not scaling beyond one replica is symptom and not the source of the problem. The problem is low CPU utilization."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 239,
    "url": "https://www.examtopics.com/discussions/google/view/131300-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You work for a pet food company that manages an online forum. Customers upload photos of their pets on the forum to share with others. About 20 photos are uploaded daily. You want to automatically and in near real time detect whether each uploaded photo has an animal. You want to prioritize time and minimize cost of your application development and deployment. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSend user-submitted images to the Cloud Vision API. Use object localization to identify all objects in the image and compare the results against a list of animals.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDownload an object detection model from TensorFlow Hub. Deploy the model to a Vertex AI endpoint. Send new user-submitted images to the model endpoint to classify whether each photo has an animal.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tManually label previously submitted images with bounding boxes around any animals. Build an AutoML object detection model by using Vertex AI. Deploy the model to a Vertex AI endpoint Send new user-submitted images to your model endpoint to detect whether each photo has an animal.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tManually label previously submitted images as having animals or not. Create an image dataset on Vertex AI. Train a classification model by using Vertex AutoML to distinguish the two classes. Deploy the model to a Vertex AI endpoint. Send new user-submitted images to your model endpoint to classify whether each photo has an animal."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 13,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-18T02:43:00.000Z",
        "voteCount": 8,
        "content": "A. B would also work and I wonder if cost would be lower, but I think going with the google hosted service is most times the most likely choice to be correct."
      },
      {
        "date": "2024-01-21T07:38:00.000Z",
        "voteCount": 3,
        "content": "I think the same, if the question mentions other services and gives you an alternative that Google has, obviously, the \"best option\" is Google, although I think the same, I think that a model downloaded from a HUB would possibly save us a few how many euros.."
      },
      {
        "date": "2024-08-22T05:46:00.000Z",
        "voteCount": 1,
        "content": "The labeling process is simpler than object detection, as it's just a binary classification. AutoML simplifies the model creation process, reducing development time. For the relatively low volume of images (20 per day), this solution is likely to be cost-effective in the long run.\n\nWhy not A? Cloud Vision is overkill for a binary classification and it is very expensive."
      },
      {
        "date": "2024-04-20T09:35:00.000Z",
        "voteCount": 1,
        "content": "agree with b1a8fae"
      },
      {
        "date": "2024-02-07T10:58:00.000Z",
        "voteCount": 2,
        "content": "I went Option B"
      },
      {
        "date": "2024-01-16T05:14:00.000Z",
        "voteCount": 4,
        "content": "As minimising time and cost are of priority and considering the small subset of images I believe A is the best option"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 240,
    "url": "https://www.examtopics.com/discussions/google/view/131060-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You work at a mobile gaming startup that creates online multiplayer games. Recently, your company observed an increase in players cheating in the games, leading to a loss of revenue and a poor user experience You built a binary classification model to determine whether a player cheated after a completed game session, and then send a message to other downstream systems to ban the player that cheated. Your model has performed well during testing, and you now need to deploy the model to production. You want your serving solution to provide immediate classifications after a completed game session to avoid further loss of revenue. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImport the model into Vertex AI Model Registry. Use the Vertex Batch Prediction service to run batch inference jobs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSave the model files in a Cloud Storage bucket. Create a Cloud Function to read the model files and make online inference requests on the Cloud Function.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSave the model files in a VM. Load the model files each time there is a prediction request, and run an inference job on the VM",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImport the model into Vertex AI Model Registry. Create a Vertex AI endpoint that hosts the model, and make online inference requests.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-15T16:48:00.000Z",
        "voteCount": 6,
        "content": "My answer: D\n\nA: Not correct: Batch Prediction is designed for offline processing of large datasets,&nbsp;not for immediate real-time predictions needed in this scenario.\nB: Not correct: While Cloud Functions offer real-time processing,&nbsp;loading the model files each time might introduce latency,&nbsp;especially for larger models\nC: Not correct: Using a VM is less scalable and more complex to manage compared to other options.\nD: CORRECT: Vertex AI Model Registry ensures proper model management,&nbsp;versioning,&nbsp;and access control while Vertex AI endpoint provides a highly scalable and managed solution for real-time online inference,&nbsp;ensuring immediate predictions after game sessions."
      },
      {
        "date": "2024-04-17T20:00:00.000Z",
        "voteCount": 4,
        "content": "Low Latency: Vertex AI Endpoints are specifically designed for low-latency online inference. They offer automatic scaling and efficient resource allocation, ensuring quick responses to game session completion signals.\nReal-time Decisions: This deployment method allows your game backend to send data from finished game sessions to the Vertex AI endpoint in near real-time. The endpoint can then make classifications (cheater or not cheater) promptly.\nManaged Service: Vertex AI handles the infrastructure management and scaling of your model, freeing you from managing servers or virtual machines (VMs)."
      },
      {
        "date": "2024-04-17T20:00:00.000Z",
        "voteCount": 1,
        "content": "A. Vertex Batch Prediction: Batch prediction is designed for offline processing of large datasets, not real-time inference on individual game sessions.\nB. Cloud Function with Model Files: While Cloud Functions can be triggered by events, reading the model files each time and running inference can introduce latency. This might not be ideal for immediate classifications.\nC. Model Files in a VM: Loading the model on a VM for each inference request incurs significant overhead and latency. This approach is not suitable for real-time processing."
      },
      {
        "date": "2024-01-13T00:30:00.000Z",
        "voteCount": 3,
        "content": "Option A: Batch prediction is too slow for your needs.\nOption B: Cloud Functions are ideal for short-lived tasks, not for continuously serving models. Loading the model on every request would be inefficient.\nOption C: VMs offer less scalability and management overhead compared to Vertex AI."
      },
      {
        "date": "2024-01-31T02:52:00.000Z",
        "voteCount": 1,
        "content": "although the game is multiplayer, and you could submit requests for all the players in the game that just ended, as a batch. So I think A is also an option"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 241,
    "url": "https://www.examtopics.com/discussions/google/view/130600-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You have created a Vertex AI pipeline that automates custom model training. You want to add a pipeline component that enables your team to most easily collaborate when running different executions and comparing metrics both visually and programmatically. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a component to the Vertex AI pipeline that logs metrics to a BigQuery table. Query the table to compare different executions of the pipeline. Connect BigQuery to Looker Studio to visualize metrics.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a component to the Vertex AI pipeline that logs metrics to a BigQuery table. Load the table into a pandas DataFrame to compare different executions of the pipeline. Use Matplotlib to visualize metrics.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a component to the Vertex AI pipeline that logs metrics to Vertex ML Metadata. Use Vertex AI Experiments to compare different executions of the pipeline. Use Vertex AI TensorBoard to visualize metrics.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a component to the Vertex AI pipeline that logs metrics to Vertex ML Metadata. Load the Vertex ML Metadata into a pandas DataFrame to compare different executions of the pipeline. Use Matplotlib to visualize metrics."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 9,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-09-13T07:37:00.000Z",
        "voteCount": 1,
        "content": "I can see why C is tempting, but Vertex Experiment's isn't actually required here, just a nice to have, whereas Workbench is actually required as they say \"visually AND programatically\". It's literally the only answer that allows programmatic comparison of the data in the metadata store."
      },
      {
        "date": "2024-04-20T09:39:00.000Z",
        "voteCount": 4,
        "content": "went with C. Experiments can be used to compare executions and metrics"
      },
      {
        "date": "2024-04-17T14:23:00.000Z",
        "voteCount": 4,
        "content": "Why A?\nBigQuery: Stores pipeline metrics from different executions in a central location, allowing easy access for team members.\nBigQuery Queries: Enables programmatic comparison of metrics across runs using SQL queries.\nLooker Studio: Provides a collaborative visualization platform for team members to explore and compare metrics visually.\nwhy not C?\nVertex AI Experiments and TensorBoard: While Vertex AI Experiments can leverage ML Metadata for lineage tracking, it's not ideal for general metric comparison. TensorBoard is primarily for visualizing training data during the pipeline execution, not comparing results across runs."
      },
      {
        "date": "2024-07-14T14:07:00.000Z",
        "voteCount": 1,
        "content": "Isn't BQ too much for a dozen of metrics?"
      },
      {
        "date": "2024-04-21T11:37:00.000Z",
        "voteCount": 1,
        "content": "why log on BQ and not to MetadataAI?"
      },
      {
        "date": "2024-01-18T02:53:00.000Z",
        "voteCount": 3,
        "content": "Clearly C."
      },
      {
        "date": "2024-01-08T09:46:00.000Z",
        "voteCount": 2,
        "content": "C is the correct one here"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 242,
    "url": "https://www.examtopics.com/discussions/google/view/131063-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "Your team is training a large number of ML models that use different algorithms, parameters, and datasets. Some models are trained in Vertex AI Pipelines, and some are trained on Vertex AI Workbench notebook instances. Your team wants to compare the performance of the models across both services. You want to minimize the effort required to store the parameters and metrics. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement an additional step for all the models running in pipelines and notebooks to export parameters and metrics to BigQuery.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Vertex AI experiment. Submit all the pipelines as experiment runs. For models trained on notebooks log parameters and metrics by using the Vertex AI SDK.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement all models in Vertex AI Pipelines Create a Vertex AI experiment, and associate all pipeline runs with that experiment.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore all model parameters and metrics as model metadata by using the Vertex AI Metadata API."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 11,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-04-17T14:32:00.000Z",
        "voteCount": 2,
        "content": "Why B?\nCentralized Tracking: Vertex AI Experiments provides a central location to track and compare models trained in both pipelines and notebooks.\nReduced Overhead: Submitting pipelines as experiment runs leverages the existing pipeline infrastructure for logging and avoids creating additional pipeline steps for all models.\nNotebook Integration: Vertex AI SDK allows notebooks to log parameters and metrics directly to the experiment, simplifying data collection from notebooks.\nwhy not C?\nC. All Models in Pipelines: Moving all models to pipelines might not be feasible or desirable. Pipelines are best suited for automated, repeatable training, while notebooks offer flexibility for exploration."
      },
      {
        "date": "2024-04-15T04:09:00.000Z",
        "voteCount": 2,
        "content": "B. Create a Vertex AI experiment. Submit all the pipelines as experiment runs. For models trained on notebooks log parameters and metrics by using the Vertex AI SDK."
      },
      {
        "date": "2024-02-15T16:59:00.000Z",
        "voteCount": 3,
        "content": "My Answer: B\n\nA: Not Correct: Not the best approach compared with Vertex AI experiment that does the same\nB: CORRECT: By submitting all pipelines as experiment runs, you can centralize the storage of parameters and metrics for models trained in Vertex AI Pipelines. This approach minimizes effort by providing a unified platform for storing and comparing model performance across different services.\nC: Not Correct: not feasible or ideal for models trained on Vertex AI Workbench notebook instances.\nD: Not Correct: If only basic parameter and metric storage is needed,&nbsp;and your team prioritizes simplicity over in-depth comparison,&nbsp;option D could be an alternative. For more complex scenarios requiring comprehensive analysis and comparison across diverse models,&nbsp;option B with Vertex AI Experiments"
      },
      {
        "date": "2024-01-20T09:24:00.000Z",
        "voteCount": 3,
        "content": "Divided between B and C. But logging parameters of models sounds easier than re-implementing a large amount of models as Vertex AI pipelines."
      },
      {
        "date": "2024-01-15T02:12:00.000Z",
        "voteCount": 1,
        "content": "B is The correct answer here I believe - \nVertex AI experiments - provides a unified way to store and compare model runs.\npipeline runs - It provides a unified way to store and compare model runs.\nnotebook instances - models trained on Vertex AI Workbench notebook instances, logging parameters and metrics using the Vertex AI SDK provides a consistent way to record the necessary information."
      },
      {
        "date": "2024-01-13T01:06:00.000Z",
        "voteCount": 1,
        "content": "Options A and B: Logging metrics to BigQuery involves additional setup and integration efforts.\nOption D: Loading Vertex ML Metadata into a pandas DataFrame for visualization requires manual work and doesn't leverage built-in visualization tools."
      },
      {
        "date": "2024-02-23T05:45:00.000Z",
        "voteCount": 2,
        "content": "On option B there are no Logging metrics to BigQuery suggested.\nHence why B is correct."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 243,
    "url": "https://www.examtopics.com/discussions/google/view/131064-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You work on a team that builds state-of-the-art deep learning models by using the TensorFlow framework. Your team runs multiple ML experiments each week, which makes it difficult to track the experiment runs. You want a simple approach to effectively track, visualize, and debug ML experiment runs on Google Cloud while minimizing any overhead code. How should you proceed?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up Vertex AI Experiments to track metrics and parameters. Configure Vertex AI TensorBoard for visualization.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up a Cloud Function to write and save metrics files to a Cloud Storage bucket. Configure a Google Cloud VM to host TensorBoard locally for visualization.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up a Vertex AI Workbench notebook instance. Use the instance to save metrics data in a Cloud Storage bucket and to host TensorBoard locally for visualization.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up a Cloud Function to write and save metrics files to a BigQuery table. Configure a Google Cloud VM to host TensorBoard locally for visualization."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-20T09:25:00.000Z",
        "voteCount": 5,
        "content": "You want to run, track, visualize ML experiments -&gt; look no further, Vertex AI experiments."
      },
      {
        "date": "2024-04-17T14:38:00.000Z",
        "voteCount": 2,
        "content": "Built-in Tracking: Vertex AI Experiments is specifically designed for tracking ML experiments on Google Cloud. It simplifies logging metrics and parameters, eliminating the need for custom code.\nTensorBoard Integration: Vertex AI integrates with TensorBoard, allowing visualization of training logs and metrics directly within the Experiments interface. This provides a centralized location for both tracking and visualization.\nMinimized Overhead: This approach leverages existing services, minimizing the need for additional code or infrastructure setup compared to options with Cloud Functions or VMs."
      },
      {
        "date": "2024-01-13T01:07:00.000Z",
        "voteCount": 3,
        "content": "Options B and D: These options involve more setup and maintenance overhead, as they require managing Cloud Functions, VMs, and storage resources.\nOption C: Vertex AI Workbench is excellent for interactive experimentation, but it's not optimized for long-term experiment tracking and visualization."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 244,
    "url": "https://www.examtopics.com/discussions/google/view/131693-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "Your work for a textile manufacturing company. Your company has hundreds of machines, and each machine has many sensors. Your team used the sensory data to build hundreds of ML models that detect machine anomalies. Models are retrained daily, and you need to deploy these models in a cost-effective way. The models must operate 24/7 without downtime and make sub millisecond predictions. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy a Dataflow batch pipeline and a Vertex AI Prediction endpoint.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy a Dataflow batch pipeline with the Runlnference API, and use model refresh.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy a Dataflow streaming pipeline and a Vertex AI Prediction endpoint with autoscaling.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy a Dataflow streaming pipeline with the Runlnference API, and use automatic model refresh.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 13,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 5,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-04-17T14:44:00.000Z",
        "voteCount": 7,
        "content": "why D?\nReal-time Predictions: Dataflow streaming pipelines continuously process sensor data, enabling real-time anomaly detection with sub-millisecond predictions. This is crucial for immediate response to potential machine issues.\nRunInference API: This API allows invoking TensorFlow models directly within the Dataflow pipeline for on-the-fly inference. This eliminates the need for separate prediction endpoints and reduces latency.\nAutomatic Model Refresh: Since models are retrained daily, automatic refresh ensures the pipeline utilizes the latest version without downtime. This is essential for maintaining model accuracy and anomaly detection effectiveness.\nWhy not C?\nDataflow Streaming Pipeline with Vertex AI Prediction Endpoint with Autoscaling: While autoscaling can handle varying workloads, Vertex AI Prediction endpoints might incur higher costs for real-time, high-volume predictions compared to invoking models directly within the pipeline using RunInference."
      },
      {
        "date": "2024-04-20T09:44:00.000Z",
        "voteCount": 1,
        "content": "agree with fitri001"
      },
      {
        "date": "2024-04-08T11:59:00.000Z",
        "voteCount": 1,
        "content": "With the automatic model refresh feature, when the underlying model changes, your pipeline updates to use the new model. Because the RunInference transform automatically updates the model handler, you don't need to redeploy the pipeline. With this feature, you can update your model in real time, even while the Apache Beam pipeline is running."
      },
      {
        "date": "2024-04-13T07:08:00.000Z",
        "voteCount": 1,
        "content": "and also ai endpoint not good for online inference"
      },
      {
        "date": "2024-02-18T17:25:00.000Z",
        "voteCount": 3,
        "content": "My Answer: C\n\nThe phrase: \u201cThe models must operate 24/7 without downtime and make sub millisecond predictions\u201d configures a case of online prediction (option B or C) \n\nThe phrase: \u201cModels are retrained daily, and you need to deploy these models in a cost-effective way\u201d, choose between \u201c Vertex AI Prediction endpoint with autoscaling\u201d instead \u201cRunlnference API, and use automatic model refresh\u201d looks better because always update with retrained models, and the scalability. \n\nhttps://cloud.google.com/blog/products/ai-machine-learning/streaming-prediction-with-dataflow-and-vertex"
      },
      {
        "date": "2024-01-31T03:40:00.000Z",
        "voteCount": 2,
        "content": "low latency - &gt; streaming\nC &amp; D could both work, but C is the GCP solution. So I chose C"
      },
      {
        "date": "2024-07-14T14:21:00.000Z",
        "voteCount": 1,
        "content": "I don't think autoscaling is relevant to this task, since we have the same amount of sensors at any time."
      },
      {
        "date": "2024-02-10T18:06:00.000Z",
        "voteCount": 2,
        "content": "i think autoscaling will lead to downtime atleast when the replicas are updating ."
      },
      {
        "date": "2024-04-28T07:30:00.000Z",
        "voteCount": 1,
        "content": "i agree,  D is better"
      },
      {
        "date": "2024-01-20T09:35:00.000Z",
        "voteCount": 4,
        "content": "Needs to be active 24/7 -&gt; streaming.\nRunInference API seems like the way to go here, using automatic model refresh on a daily basis. https://beam.apache.org/documentation/ml/about-ml/"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 245,
    "url": "https://www.examtopics.com/discussions/google/view/131065-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You are developing an ML model that predicts the cost of used automobiles based on data such as location, condition, model type, color, and engine/battery efficiency. The data is updated every night. Car dealerships will use the model to determine appropriate car prices. You created a Vertex AI pipeline that reads the data splits the data into training/evaluation/test sets performs feature engineering trains the model by using the training dataset and validates the model by using the evaluation dataset. You need to configure a retraining workflow that minimizes cost. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCompare the training and evaluation losses of the current run. If the losses are similar, deploy the model to a Vertex AI endpoint. Configure a cron job to redeploy the pipeline every night.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCompare the training and evaluation losses of the current run. If the losses are similar, deploy the model to a Vertex AI endpoint with training/serving skew threshold model monitoring. When the model monitoring threshold is triggered redeploy the pipeline.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCompare the results to the evaluation results from a previous run. If the performance improved deploy the model to a Vertex AI endpoint. Configure a cron job to redeploy the pipeline every night.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t Compare the results to the evaluation results from a previous run. If the performance improved deploy the model to a Vertex AI endpoint with training/serving skew threshold model monitoring. When the model monitoring threshold is triggered redeploy the pipeline.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 11,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-13T01:13:00.000Z",
        "voteCount": 5,
        "content": "Option A: Redeploying the pipeline every night without checking for degradation wastes resources if model performance is stable.\nOption C: Comparing results to a previous run doesn't guarantee model degradation detection in the current run.\nOption D: Comparing to a previous run and using model monitoring is redundant; model monitoring alone is sufficient."
      },
      {
        "date": "2024-04-17T14:52:00.000Z",
        "voteCount": 4,
        "content": "Since the goal is to minimize cost while maintaining accuracy, Option D provides a more targeted approach for retraining based on the likelihood of the model being outdated due to data changes. Option B might trigger retraining more frequently even if the performance difference doesn't necessarily stem from a significant shift in the data distribution."
      },
      {
        "date": "2024-04-17T14:53:00.000Z",
        "voteCount": 2,
        "content": "Option D: Utilizes training/serving skew monitoring. This specifically focuses on identifying discrepancies between the training data and the real-world data the deployed model encounters. This is a strong indicator of when the model might be outdated due to changes in the data distribution.\nOption B: Utilizes training/serving loss monitoring. Training loss tells you how well the model performs on the training data, while serving loss tells you how well it performs on real-world data. While high serving loss can indicate a problem, it might not necessarily be due to training/serving skew. Other factors like data quality issues or concept drift (gradual changes in the underlying data patterns) could also lead to high serving loss."
      },
      {
        "date": "2024-04-15T04:11:00.000Z",
        "voteCount": 3,
        "content": "D. Compare the results to the evaluation results from a previous run. If the performance improved, deploy the model to a Vertex AI endpoint with training/serving skew threshold model monitoring. When the model monitoring threshold is triggered, redeploy the pipeline."
      },
      {
        "date": "2024-04-16T10:48:00.000Z",
        "voteCount": 1,
        "content": "i agree, see guilhermebutzke"
      },
      {
        "date": "2024-02-16T11:26:00.000Z",
        "voteCount": 4,
        "content": "My answer D:\nA and C: Not Correct: Schedule a retrain every night is not necessary since the model is performing well.\nB. Not Correct: This approach focuses on&nbsp;internal consistency&nbsp;within the current training run, train versus loss evaluation. Comparing similar training and validation losses doesn't guarantee&nbsp;better performance&nbsp;than previous models. This is an approach to identity overfitting, for example, or model quality. \nD. Correct: This approach focuses on&nbsp;identifying performance changes&nbsp;over time. Comparing to previous runs helps assess if the&nbsp;new model performs better&nbsp;than the old one on the&nbsp;evaluation set. we will check if this new version is better or not than the old one\n\nhttps://www.youtube.com/watch?v=1ykDWsnL2LE&amp;ab_channel=GoogleCloudTech"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 246,
    "url": "https://www.examtopics.com/discussions/google/view/131066-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You recently used BigQuery ML to train an AutoML regression model. You shared results with your team and received positive feedback. You need to deploy your model for online prediction as quickly as possible. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRetrain the model by using BigQuery ML, and specify Vertex AI as the model registry. Deploy the model from Vertex AI Model Registry to a Vertex AI endpoint,",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRetrain the model by using Vertex Al Deploy the model from Vertex AI Model. Registry to a Vertex AI endpoint.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAlter the model by using BigQuery ML, and specify Vertex AI as the model registry. Deploy the model from Vertex AI Model Registry to a Vertex AI endpoint.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExport the model from BigQuery ML to Cloud Storage. Import the model into Vertex AI Model Registry. Deploy the model to a Vertex AI endpoint."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 11,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-13T01:16:00.000Z",
        "voteCount": 7,
        "content": "I think it's D, as model retraining should not be required unless it's specified there's new data."
      },
      {
        "date": "2024-01-15T02:27:00.000Z",
        "voteCount": 2,
        "content": "I agree with pikachu007"
      },
      {
        "date": "2024-02-03T19:52:00.000Z",
        "voteCount": 3,
        "content": "I think it's C\nExported models for model types AUTOML_REGRESSOR and AUTOML_CLASSIFIER do not support AI Platform deployment for online prediction."
      },
      {
        "date": "2024-01-23T01:31:00.000Z",
        "voteCount": 1,
        "content": "Agree with Pikachu007,  the option D is good."
      },
      {
        "date": "2024-01-21T07:56:00.000Z",
        "voteCount": 6,
        "content": "Friend is the C, and with Alter MODEL you can register the model in Vertex AI, I work in a company and I myself have registered models like this."
      },
      {
        "date": "2024-09-27T01:39:00.000Z",
        "voteCount": 1,
        "content": "It's C"
      },
      {
        "date": "2024-08-23T01:12:00.000Z",
        "voteCount": 1,
        "content": "The model has already been trained and received positive feedback, so there's no need to retrain the model."
      },
      {
        "date": "2024-07-04T17:23:00.000Z",
        "voteCount": 1,
        "content": "C is correct Here's why:\n1) You trained an AutoML regression model using BigQuery ML.\n2)To deploy the model for online prediction, you need to export the model in a format that is compatible with Vertex AI.\n3)Altering the model by using BigQuery ML and specifying Vertex AI as the model registry allows you to export the model in the correct format.\nOnce exported, you can deploy the model from Vertex AI Model Registry to a Vertex AI endpoint, which enables online prediction"
      },
      {
        "date": "2024-07-04T17:22:00.000Z",
        "voteCount": 1,
        "content": "C is correct Here's why:\n1) You trained an AutoML regression model using BigQuery ML.\n2)To deploy the model for online prediction, you need to export the model in a format that is compatible with Vertex AI.\n3)Altering the model by using BigQuery ML and specifying Vertex AI as the model registry allows you to export the model in the correct format.\nOnce exported, you can deploy the model from Vertex AI Model Registry to a Vertex AI endpoint, which enables online prediction"
      },
      {
        "date": "2024-04-24T10:54:00.000Z",
        "voteCount": 4,
        "content": "https://cloud.google.com/bigquery/docs/reference/standard-sql/bigqueryml-syntax-alter-model#alter_model_statement"
      },
      {
        "date": "2024-04-18T22:56:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/vertex-ai/docs/model-registry/model-registry-bqml\nhttps://cloud.google.com/bigquery/docs/update_vertex"
      },
      {
        "date": "2024-04-17T16:09:00.000Z",
        "voteCount": 2,
        "content": "You recently used BigQuery ML to train an AutoML regression model. You shared results with your team and received positive feedback. You need to deploy your model for online prediction as quickly as possible. What should you do?\n\nA. Retrain the model by using BigQuery ML, and specify Vertex AI as the model registry. Deploy the model from Vertex AI Model Registry to a Vertex AI endpoint,\nB. Retrain the model by using Vertex Al Deploy the model from Vertex AI Model. Registry to a Vertex AI endpoint.\nC. Alter the model by using BigQuery ML, and specify Vertex AI as the model registry. Deploy the model from Vertex AI Model Registry to a Vertex AI endpoint.\nD. Export the model from BigQuery ML to Cloud Storage. Import the model into Vertex AI Model Registry. Deploy the model to a Vertex AI endpoint."
      },
      {
        "date": "2024-04-17T16:10:00.000Z",
        "voteCount": 1,
        "content": "No Retraining: You've already trained a successful model in BigQuery ML. Retraining (Options A, B, and C) is unnecessary and adds time.\nDirect Deployment: Option D leverages existing tools for streamlined deployment. You export the model directly from BigQuery ML and import it into Vertex AI Model Registry for centralized management. Finally, you deploy the model to a Vertex AI endpoint for online predictions.\nCloud Storage: Cloud Storage provides a readily accessible location to store your exported model before deployment."
      },
      {
        "date": "2024-04-21T01:50:00.000Z",
        "voteCount": 3,
        "content": "alter the model doesn't mean retrain..."
      },
      {
        "date": "2024-04-15T04:12:00.000Z",
        "voteCount": 1,
        "content": "D. Export the model from BigQuery ML to Cloud Storage. Import the model into Vertex AI Model Registry. Deploy the model to a Vertex AI endpoint."
      },
      {
        "date": "2024-04-16T10:50:00.000Z",
        "voteCount": 1,
        "content": "why not C? it is not necessary to export in GCS"
      },
      {
        "date": "2024-04-20T04:12:00.000Z",
        "voteCount": 2,
        "content": "I changed my answer to C. GCS is not necessary"
      },
      {
        "date": "2024-04-14T14:31:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/vertex-ai/docs/model-registry/model-registry-bqml"
      },
      {
        "date": "2024-03-22T21:29:00.000Z",
        "voteCount": 1,
        "content": "Alter the model is correct,no need to export the model : \"You can register BigQuery ML models with the Model Registry, in order to manage them alongside your other ML models without needing to export them\"\nhttps://cloud.google.com/bigquery/docs/managing-models-vertex\na simple update is sufficient :\nhttps://cloud.google.com/bigquery/docs/update_vertex"
      },
      {
        "date": "2024-02-10T17:03:00.000Z",
        "voteCount": 1,
        "content": "I think the answer here is B , because even if we alter or export automl regressor model trained in BQML is not supported in vertex ai for online prediction so we need to retrain using vertex ai"
      },
      {
        "date": "2024-02-03T00:07:00.000Z",
        "voteCount": 1,
        "content": "C) \nhttps://cloud.google.com/bigquery/docs/create_vertex"
      },
      {
        "date": "2024-01-31T03:49:00.000Z",
        "voteCount": 4,
        "content": "the answer is C, no need to export the model : \"You can register BigQuery ML models with the Model Registry, in order to manage them alongside your other ML models without needing to export them\"\nhttps://cloud.google.com/bigquery/docs/managing-models-vertex\na simple update is sufficient :\nhttps://cloud.google.com/bigquery/docs/update_vertex"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 247,
    "url": "https://www.examtopics.com/discussions/google/view/131067-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You built a deep learning-based image classification model by using on-premises data. You want to use Vertex AI to deploy the model to production. Due to security concerns, you cannot move your data to the cloud. You are aware that the input data distribution might change over time. You need to detect model performance changes in production. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Vertex Explainable AI for model explainability. Configure feature-based explanations.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Vertex Explainable AI for model explainability. Configure example-based explanations.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Vertex AI Model Monitoring job. Enable training-serving skew detection for your model.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Vertex AI Model Monitoring job. Enable feature attribution skew and drift detection for your model.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 15,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-20T09:46:00.000Z",
        "voteCount": 7,
        "content": "D. You want to control how much the distribution of the data changes over time -&gt; that's drift."
      },
      {
        "date": "2024-04-20T09:53:00.000Z",
        "voteCount": 2,
        "content": "D, as the training data is not available"
      },
      {
        "date": "2024-04-17T16:16:00.000Z",
        "voteCount": 1,
        "content": "Security: Vertex AI Model Monitoring doesn't require uploading your training data to the cloud. It analyzes model predictions and input features on your on-premises server.\nData Distribution Shifts: Feature attribution techniques like LIME or SHAP within Vertex AI Model Monitoring can identify how different features contribute to model predictions. Detecting drifts in these feature attributions can indicate changes in the underlying data distribution compared to the training data."
      },
      {
        "date": "2024-04-15T03:59:00.000Z",
        "voteCount": 1,
        "content": "Feature Attribution Skew and Drift Detection, this type of monitoring is useful in some cases, it requires access to the training and serving data for analysis. Since data cannot move to the cloud, Option D wouldn't be feasible.\n\nI vote for C. Create a Vertex AI Model Monitoring job. Enable training-serving skew detection for your model."
      },
      {
        "date": "2024-04-20T04:14:00.000Z",
        "voteCount": 2,
        "content": "I changed my answer to D"
      },
      {
        "date": "2024-04-08T12:16:00.000Z",
        "voteCount": 1,
        "content": "the answer cannot be C, cause your training data is not available in production."
      },
      {
        "date": "2024-01-31T03:54:00.000Z",
        "voteCount": 4,
        "content": "the answer cannot be C, cause your training data is not available in production. \nSo D is the only viable answer"
      },
      {
        "date": "2024-01-13T01:19:00.000Z",
        "voteCount": 1,
        "content": "Option A and B: Vertex Explainable AI provides insights into model behavior but doesn't directly detect performance changes or concept drift. It's more suitable for understanding model decisions, not monitoring production performance.\nOption D: Feature attribution skew and drift detection requires feature attributions calculated during training, which might not be feasible without cloud access to the data."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 248,
    "url": "https://www.examtopics.com/discussions/google/view/131068-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You trained a model packaged it with a custom Docker container for serving, and deployed it to Vertex AI Model Registry. When you submit a batch prediction job, it fails with this error: \"Error model server never became ready. Please validate that your model file or container configuration are valid. \" There are no additional errors in the logs. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a logging configuration to your application to emit logs to Cloud Logging",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the HTTP port in your model\u2019s configuration to the default value of 8080",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the healthRoute value in your model\u2019s configuration to /healthcheck",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPull the Docker image locally, and use the docker run command to launch it locally. Use the docker logs command to explore the error logs\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 8,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-09-18T18:39:00.000Z",
        "voteCount": 1,
        "content": "From StackOverflow:\"Validate the container configuration port; it should use port 8080. This configuration is important because Vertex AI sends liveness checks, health checks, and prediction requests to this port on the container. \" Pulling the container to the local machine is like stepping back and saying, \"It works on my computer,\" then solving the problem as it arises."
      },
      {
        "date": "2024-04-17T16:19:00.000Z",
        "voteCount": 1,
        "content": "Isolating the Issue: Running the container locally helps determine if the problem originates from the container configuration or the Vertex AI deployment environment. If the container runs successfully locally, the issue likely lies with Vertex AI.\nDetailed Error Messages: Examining the container logs using docker logs provides detailed error messages specific to the container startup process. These messages can pinpoint the root cause of the model server failure, such as missing dependencies, incorrect model format, or resource limitations."
      },
      {
        "date": "2024-04-15T04:02:00.000Z",
        "voteCount": 1,
        "content": "I vote for D. Pull the Docker image locally, and use the docker run command to launch it locally. Use the docker logs command to explore the error logs. Here's why:\n\n1. Local Testing by running the Docker image locally to replicate the environment the model server encounters within Vertex AI.\n2. Using docker logs allows to inspect the detailed error messages generated by the model server during startup. These logs might provide specific clues about the cause of the \"model server never became ready\" error."
      },
      {
        "date": "2024-03-09T17:02:00.000Z",
        "voteCount": 1,
        "content": "When deploying a custom container to Vertex AI Model Registry, need to follow some requirements for the container configuration. One of these requirements is to use the HTTP port 8080 forserving predictions. If using a different port, the model server might not be able to communicate with Vertex AI and cause the error \u201cError model server never became ready\u201d. To fix this error, change the HTTP port in your model\u2019s configuration to the default value of 8080 and redeploy the container."
      },
      {
        "date": "2024-02-16T11:46:00.000Z",
        "voteCount": 2,
        "content": "My Answer: D\n\nA: Not correct: While logging can be helpful for monitoring and debugging, it won't directly address the issue of the model server not becoming ready. \n\nB: Not correct: The error message doesn't indicate a port issue, changing it preemptively might not resolve the underlying problem.\n\nC: Not correct: changing the health route, which could be helpful if the issue is related to health checks, but without further information, it's not the most conclusive option.\n\nD: CORRECT: This option allows you to simulate the deployment environment locally and inspect the logs directly, which can help diagnose the issue with the model server not becoming ready."
      },
      {
        "date": "2024-02-07T20:15:00.000Z",
        "voteCount": 1,
        "content": "Due to Model size or other reasons so that it cannot pass health check before timeout.\n\nhttps://cloud.google.com/knowledge/kb/unable-to-deploy-a-large-model-into-a-vertex-endpoint-000010439"
      },
      {
        "date": "2024-03-09T02:18:00.000Z",
        "voteCount": 2,
        "content": "I would revise my answer to D, as healthRoute should be defaulted to /healthcheck."
      },
      {
        "date": "2024-02-03T20:28:00.000Z",
        "voteCount": 1,
        "content": "Validate the container configuration port, it should use port 8080. This configuration is important because Vertex AI sends liveness checks, health checks, and prediction requests to this port on the container. \nhttps://www.appsloveworld.com/coding/flask/15/vertex-ai-deployment-failed"
      },
      {
        "date": "2024-01-31T04:04:00.000Z",
        "voteCount": 2,
        "content": "when not specifying the health check, the endpoint uses a default health check which only indicates if the http server is ready, not if the model is ready. \nhttps://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#health"
      },
      {
        "date": "2024-01-13T01:21:00.000Z",
        "voteCount": 4,
        "content": "Option A: Adding logging to Cloud Logging is useful for long-term monitoring but might not provide immediate insights for this specific error.\nOptions B and C: Changing port and health check configuration might be necessary if incorrect, but local debugging often reveals the root cause more effectively."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 249,
    "url": "https://www.examtopics.com/discussions/google/view/131069-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You are developing an ML model to identify your company\u2019s products in images. You have access to over one million images in a Cloud Storage bucket. You plan to experiment with different TensorFlow models by using Vertex AI Training. You need to read images at scale during training while minimizing data I/O bottlenecks. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLoad the images directly into the Vertex AI compute nodes by using Cloud Storage FUSE. Read the images by using the tf.data.Dataset.from_tensor_slices function",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Vertex AI managed dataset from your image data. Access the AIP_TRAINING_DATA_URI environment variable to read the images by using the tf.data.Dataset.list_files function.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConvert the images to TFRecords and store them in a Cloud Storage bucket. Read the TFRecords by using the tf.data.TFRecordDataset function.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the URLs of the images in a CSV file. Read the file by using the tf.data.experimental.CsvDataset function."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 9,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-05-12T11:20:00.000Z",
        "voteCount": 2,
        "content": "TFRecords is a binary storage format optimized for TensorFlow. By storing images as TFRecords, you can improve the I/O efficiency as the data is serialized and can be efficiently loaded off-disk in a batched manner. TFRecordDataset is specifically designed for reading these files efficiently, which helps in minimizing I/O bottlenecks. This approach is typically recommended for large-scale image datasets as it ensures data is read efficiently in a manner suitable for distributed training."
      },
      {
        "date": "2024-04-20T09:56:00.000Z",
        "voteCount": 1,
        "content": "agree with pikachu007"
      },
      {
        "date": "2024-04-17T16:23:00.000Z",
        "voteCount": 1,
        "content": "Read the images by using the tf.data.Dataset.from_tensor_slices function.\n\nHere's why this option is most efficient:\n\nCloud Storage FUSE: This mounts your Cloud Storage bucket directly to the training VM, allowing on-demand access to image data as local files. It minimizes network overhead and data transfer compared to downloading the entire dataset beforehand.\ntf.data.Dataset.from_tensor_slices: This function is suitable for reading data directly from memory. Since Cloud Storage FUSE presents the images as local files, you can leverage this function for efficient data access within your training script."
      },
      {
        "date": "2024-04-17T16:25:00.000Z",
        "voteCount": 1,
        "content": "B. Vertex AI Managed Dataset: While managed datasets offer convenience, accessing them might involve additional network overhead compared to Cloud Storage FUSE.\nC. TFRecords: Converting images to TFRecords can be an additional processing step, potentially introducing I/O overhead. While TFRecord format might be efficient for some models, it's not strictly necessary for minimizing I/O during data access.\nD. CSV with Image URLs: Reading image URLs from a CSV and fetching each image individually creates significant network traffic, leading to I/O bottlenecks. It's less efficient than directly accessing the images through Cloud Storage FUSE."
      },
      {
        "date": "2024-04-17T16:26:00.000Z",
        "voteCount": 1,
        "content": "TensorFlow Datasets (TFDs): Consider implementing TFDs within your training script. They offer functionalities like parallelized data loading and on-the-fly data augmentation to further optimize training efficiency.\nPreprocessing and Caching: Preprocess data (resizing, normalization) within your TFD pipeline or training script. Cache preprocessed data locally on the VM to avoid redundant processing during training iterations."
      },
      {
        "date": "2024-02-23T03:41:00.000Z",
        "voteCount": 2,
        "content": "The TFRecord format is a simple format for storing a sequence of binary records.\n\nProtocol buffers are a cross-platform, cross-language library for efficient serialization of structured data."
      },
      {
        "date": "2024-01-13T01:23:00.000Z",
        "voteCount": 4,
        "content": "Option A: Cloud Storage FUSE can be slower for large datasets and adds complexity.\nOption B: Vertex AI managed datasets offer convenience but might not match TFRecord performance for large-scale image training.\nOption D: CSV files require manual loading and parsing, increasing overhead."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 250,
    "url": "https://www.examtopics.com/discussions/google/view/131070-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You work at an ecommerce startup. You need to create a customer churn prediction model. Your company\u2019s recent sales records are stored in a BigQuery table. You want to understand how your initial model is making predictions. You also want to iterate on the model as quickly as possible while minimizing cost. How should you build your first model?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExport the data to a Cloud Storage bucket. Load the data into a pandas DataFrame on Vertex AI Workbench and train a logistic regression model with scikit-learn.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a tf.data.Dataset by using the TensorFlow BigQueryClient. Implement a deep neural network in TensorFlow.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPrepare the data in BigQuery and associate the data with a Vertex AI dataset. Create an AutoMLTabularTrainingJob to tram a classification model.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExport the data to a Cloud Storage bucket. Create a tf.data.Dataset to read the data from Cloud Storage. Implement a deep neural network in TensorFlow."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-06-11T02:20:00.000Z",
        "voteCount": 4,
        "content": "C) Data preparation in BigQuery. Ease of implementation with AutoML"
      },
      {
        "date": "2024-04-17T16:31:00.000Z",
        "voteCount": 1,
        "content": "Cost-Effectiveness:\nLeverages BigQuery for data storage and preprocessing, minimizing data movement costs.\nUtilizes Vertex AI's AutoML Tabular training, which is a pay-per-use service, reducing upfront costs compared to custom training environments.\nRapid Iteration:\nAutoML Tabular automates feature engineering and model selection, allowing you to experiment with various configurations quickly.\nYou can focus on refining feature engineering and interpreting model behavior based on AutoML's generated explanations."
      },
      {
        "date": "2024-04-17T16:32:00.000Z",
        "voteCount": 1,
        "content": "why not B?\nImplementing a deep neural network from scratch requires significant development effort and might be overkill for an initial model. Interpretability of deep neural networks can also be challenging.\nWhile TensorFlow BigQueryClient allows data access, it requires writing custom training scripts, increasing development time."
      },
      {
        "date": "2024-04-15T04:04:00.000Z",
        "voteCount": 1,
        "content": "You work at an ecommerce startup. You need to create a customer churn prediction model. Your company\u2019s recent sales records are stored in a BigQuery table. You want to understand how your initial model is making predictions. You also want to iterate on the model as quickly as possible while minimizing cost. How should you build your first model?\n\nA. Export the data to a Cloud Storage bucket. Load the data into a pandas DataFrame on Vertex AI Workbench and train a logistic regression model with scikit-learn.\nB. Create a tf.data.Dataset by using the TensorFlow BigQueryClient. Implement a deep neural network in TensorFlow.\nC. Prepare the data in BigQuery and associate the data with a Vertex AI dataset. Create an AutoMLTabularTrainingJob to tram a classification model.\nD. Export the data to a Cloud Storage bucket. Create a tf.data.Dataset to read the data from Cloud Storage. Implement a deep neural network in TensorFlow."
      },
      {
        "date": "2024-02-24T09:40:00.000Z",
        "voteCount": 1,
        "content": "I went Option C"
      },
      {
        "date": "2024-01-13T01:25:00.000Z",
        "voteCount": 4,
        "content": "Option A: While logistic regression is interpretable, manual training in Vertex AI Workbench adds time and complexity.\nOptions B and D: Deep neural networks can be powerful but often lack interpretability, making it challenging to understand model decisions. They also require more hands-on model development and infrastructure management."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 251,
    "url": "https://www.examtopics.com/discussions/google/view/131087-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You are developing a training pipeline for a new XGBoost classification model based on tabular data. The data is stored in a BigQuery table. You need to complete the following steps:<br><br>1. Randomly split the data into training and evaluation datasets in a 65/35 ratio<br>2. Conduct feature engineering<br>3. Obtain metrics for the evaluation dataset<br>4. Compare models trained in different pipeline executions<br><br>How should you execute these steps?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Using Vertex AI Pipelines, add a component to divide the data into training and evaluation sets, and add another component for feature engineering.<br>2. Enable autologging of metrics in the training component.<br>3. Compare pipeline runs in Vertex AI Experiments.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Using Vertex AI Pipelines, add a component to divide the data into training and evaluation sets, and add another component for feature engineering.<br>2. Enable autologging of metrics in the training component.<br>3. Compare models using the artifacts\u2019 lineage in Vertex ML Metadata.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. In BigQuery ML, use the CREATE MODEL statement with BOOSTED_TREE_CLASSIFIER as the model type and use BigQuery to handle the data splits.<br>2. Use a SQL view to apply feature engineering and train the model using the data in that view.<br>3. Compare the evaluation metrics of the models by using a SQL query with the ML.TRAINING_INFO statement.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. In BigQuery ML, use the CREATE MODEL statement with BOOSTED_TREE_CLASSIFIER as the model type and use BigQuery to handle the data splits.<br>2. Use ML TRANSFORM to specify the feature engineering transformations and tram the model using the data in the table.<br>3. Compare the evaluation metrics of the models by using a SQL query with the ML.TRAINING_INFO statement."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 16,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-13T07:00:00.000Z",
        "voteCount": 6,
        "content": "Option B: While Vertex ML Metadata provides artifact lineage, it's less comprehensive for model comparison than Experiments.\nOptions C and D: BigQuery ML is powerful for in-database model training, but it has limitations in pipeline orchestration, complex feature engineering, and detailed model comparison features, making it less suitable for this scenario."
      },
      {
        "date": "2024-09-18T16:11:00.000Z",
        "voteCount": 1,
        "content": "Can anyone give a good reason for the answers without using ChatGPT or Gemini?"
      },
      {
        "date": "2024-08-13T23:04:00.000Z",
        "voteCount": 1,
        "content": "BQ ML falls a bit short when it comes to building pipelines that include feature engineering and experiment comparison (it's better to use Vertex Pipelines and do the comparisons using Vertex Experiments)."
      },
      {
        "date": "2024-04-17T16:39:00.000Z",
        "voteCount": 1,
        "content": "Flexibility and Control: Vertex AI Pipelines allow you to define a custom pipeline with separate components for data splitting, feature engineering, and XGBoost training using your preferred libraries (like BigQueryClient and xgboost). This provides more control and customization compared to BigQuery ML's limited model types and functionality.\nFeature Engineering and Data Splitting: Separate components enable clear separation of concerns and potentially parallel execution for efficiency.\nAutologging and Model Comparison: Vertex AI autologging simplifies capturing evaluation metrics during training. Vertex AI Experiments offer a centralized interface to compare metrics across different pipeline runs (potentially with varying hyperparameter configurations)."
      },
      {
        "date": "2024-04-17T16:39:00.000Z",
        "voteCount": 1,
        "content": "why not C &amp; D?\nC &amp; D. BigQuery ML: While BigQuery ML offers some XGBoost functionality, it has limitations:\nLimited Model Types: BigQuery ML doesn't provide the full flexibility of using custom XGBoost libraries with advanced configurations.\nLess Control over Feature Engineering: Feature engineering using SQL views might be restrictive compared to a dedicated component in Vertex AI Pipelines.\nLimited Model Comparison: While ML.TRAINING_INFO provides some insights, Vertex AI Experiments offer a more comprehensive view for comparing models across pipeline runs."
      },
      {
        "date": "2024-04-13T23:18:00.000Z",
        "voteCount": 1,
        "content": "see b1a8fae"
      },
      {
        "date": "2024-04-13T01:59:00.000Z",
        "voteCount": 1,
        "content": "A: Leverage Vertex AI Pipelines and Experiments"
      },
      {
        "date": "2024-02-18T17:35:00.000Z",
        "voteCount": 2,
        "content": "My Answer: A\n\nA: CORRECT: It involves proper data splitting into training and evaluation sets and conducting feature engineering within the pipeline, fulfilling steps 1 and 2. Enabling autologging of metrics ensures that you can track and compare the performance of different model executions, fulfilling step 3.\n\nB: Not Correct: Better use Vertex AI Experiments\n\nC and D: Not Correct: BigQuery ML lacks functionalities for comparing models across pipeline runs. You would need to rely on external tools or custom scripts to extract and compare evaluation metrics, making the process less streamlined."
      },
      {
        "date": "2024-01-22T01:11:00.000Z",
        "voteCount": 3,
        "content": "Compare models in different pipeline executions -&gt; go for Vertex AI experiments"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 252,
    "url": "https://www.examtopics.com/discussions/google/view/131088-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You work for a company that sells corporate electronic products to thousands of businesses worldwide. Your company stores historical customer data in BigQuery. You need to build a model that predicts customer lifetime value over the next three years. You want to use the simplest approach to build the model and you want to have access to visualization tools. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Vertex AI Workbench notebook to perform exploratory data analysis. Use IPython magics to create a new BigQuery table with input features. Use the BigQuery console to run the CREATE MODEL statement. Validate the results by using the ML.EVALUATE and ML.PREDICT statements.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun the CREATE MODEL statement from the BigQuery console to create an AutoML model. Validate the results by using the ML.EVALUATE and ML.PREDICT statements.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Vertex AI Workbench notebook to perform exploratory data analysis and create input features. Save the features as a CSV file in Cloud Storage. Import the CSV file as a new BigQuery table. Use the BigQuery console to run the CREATE MODEL statement. Validate the results by using the ML.EVALUATE and ML.PREDICT statements.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Vertex AI Workbench notebook to perform exploratory data analysis. Use IPython magics to create a new BigQuery table with input features, create the model, and validate the results by using the CREATE MODEL, ML.EVALUATE, and ML.PREDICT statements."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 6,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-02-24T03:13:00.000Z",
        "voteCount": 5,
        "content": "Option B because there's no mention of \"flexibility\". Easy access to viz tools with Looker"
      },
      {
        "date": "2024-09-18T03:08:00.000Z",
        "voteCount": 1,
        "content": "Going for D"
      },
      {
        "date": "2024-09-05T21:43:00.000Z",
        "voteCount": 1,
        "content": "Option D is the answer given by an official Google trainer."
      },
      {
        "date": "2024-08-13T23:06:00.000Z",
        "voteCount": 1,
        "content": "Simple training and integration with visualization tools = BQ"
      },
      {
        "date": "2024-06-29T23:43:00.000Z",
        "voteCount": 2,
        "content": "As requested :\" simplest approach\", the option B is the best choice."
      },
      {
        "date": "2024-06-21T01:03:00.000Z",
        "voteCount": 3,
        "content": "D \nVertex AI Workbench notebook: Provides an environment for data analysis, model building, and visualization tools all in one place.\nIPython magics: Allows seamless interaction with BigQuery for data exploration and feature creation directly within the notebook.\nCREATE MODEL statement: Enables model creation within the notebook environment, simplifying the workflow.\nML.EVALUATE and ML.PREDICT statements: Facilitate model validation directly within the notebook for assessing performance."
      },
      {
        "date": "2024-04-13T02:00:00.000Z",
        "voteCount": 3,
        "content": "B. Use Bigquery ML Features to create, evaluate and predict"
      },
      {
        "date": "2024-01-23T00:48:00.000Z",
        "voteCount": 2,
        "content": "As requested :\"  simplest approach\", the option B is the best choice."
      },
      {
        "date": "2024-01-22T01:22:00.000Z",
        "voteCount": 1,
        "content": "Forgot to vote."
      },
      {
        "date": "2024-01-22T01:21:00.000Z",
        "voteCount": 2,
        "content": "Simplest approach that allows visualization is option B."
      },
      {
        "date": "2024-01-20T02:09:00.000Z",
        "voteCount": 1,
        "content": "all the other options create a new BQ table, I don't think it's needed."
      },
      {
        "date": "2024-01-13T07:09:00.000Z",
        "voteCount": 2,
        "content": "Option B: While AutoML simplifies model selection and training, it lacks the flexibility and visualization capabilities of Vertex AI Workbench.\nOption C: Manually saving features as CSV files and importing them back into BigQuery involves unnecessary data movement and complexity.\nOption D: Completing all steps within the notebook is possible but requires more coding and might not be as intuitive for those less familiar with BigQuery ML syntax."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 253,
    "url": "https://www.examtopics.com/discussions/google/view/130595-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You work for a delivery company. You need to design a system that stores and manages features such as parcels delivered and truck locations over time. The system must retrieve the features with low latency and feed those features into a model for online prediction. The data science team will retrieve historical data at a specific point in time for model training. You want to store the features with minimal effort. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore features in Bigtable as key/value data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore features in Vertex AI Feature Store.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore features as a Vertex AI dataset, and use those features to train the models hosted in Vertex AI endpoints.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore features in BigQuery timestamp partitioned tables, and use the BigQuery Storage Read API to serve the features."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-18T17:39:00.000Z",
        "voteCount": 4,
        "content": "My Answer: B\n\n Vertex AI Feature Store because of these: \u201cmust retrieve the features with low latency\u201d ,\u201cretrieve historical data at a specific point in time\u201d, and \u201c store the features with minimal effort\u201d"
      },
      {
        "date": "2024-02-18T17:39:00.000Z",
        "voteCount": 1,
        "content": "My Answer: B\n\n Vertex AI Feature Store because of these: \u201cmust retrieve the features with low latency\u201d ,\u201cretrieve historical data at a specific point in time\u201d, and \u201c store the features with minimal effort\u201d"
      },
      {
        "date": "2024-02-06T13:57:00.000Z",
        "voteCount": 1,
        "content": "I agree with dadai75"
      },
      {
        "date": "2024-01-23T00:43:00.000Z",
        "voteCount": 1,
        "content": "As required: \"minimal effort\" and \"load latency\", the Option B is the best choice."
      },
      {
        "date": "2024-01-22T01:25:00.000Z",
        "voteCount": 1,
        "content": "Vertex AI Feature Store is optimized for ultra-low latency serving"
      },
      {
        "date": "2024-01-08T09:33:00.000Z",
        "voteCount": 1,
        "content": "Feature store allows point in time retrieval"
      },
      {
        "date": "2024-01-08T09:32:00.000Z",
        "voteCount": 1,
        "content": "This is B"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 254,
    "url": "https://www.examtopics.com/discussions/google/view/131091-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You are working on a prototype of a text classification model in a managed Vertex AI Workbench notebook. You want to quickly experiment with tokenizing text by using a Natural Language Toolkit (NLTK) library. How should you add the library to your Jupyter kernel?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall the NLTK library from a terminal by using the pip install nltk command.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite a custom Dataflow job that uses NLTK to tokenize your text and saves the output to Cloud Storage.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new Vertex AI Workbench notebook with a custom image that includes the NLTK library.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall the NLTK library from a Jupyter cell by using the !pip install nltk --user command.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-13T07:22:00.000Z",
        "voteCount": 6,
        "content": "Direct Installation: It installs the library directly within the notebook environment, making it immediately available for use.\nSimplicity: It requires a single command in a Jupyter cell, eliminating the need for external tools or configuration.\nUser-Specific Installation: The --user flag ensures the library is installed in your user space, avoiding conflicts with system-wide packages."
      },
      {
        "date": "2024-08-04T21:31:00.000Z",
        "voteCount": 1,
        "content": "Right command : !pip install nltk --user"
      },
      {
        "date": "2024-04-17T16:56:00.000Z",
        "voteCount": 1,
        "content": "Efficiency: It allows installation directly within your notebook cell, minimizing setup time compared to creating a custom image or using an external terminal.\nUser-Level Installation: Using --user ensures the library is installed within your user environment, avoiding conflicts with system-wide installations or impacting other users."
      },
      {
        "date": "2024-04-17T16:57:00.000Z",
        "voteCount": 1,
        "content": "A. Terminal Installation: While possible if allowed, it requires switching contexts outside the notebook and might not be permitted in managed environments.\nB. Dataflow Job: A Dataflow job is an overkill for simple library usage within a notebook. It's designed for large-scale data processing pipelines.\nC. Custom Image: Creating a custom image with NLTK requires additional development effort and can be time-consuming for quick experimentation."
      },
      {
        "date": "2024-02-10T04:54:00.000Z",
        "voteCount": 3,
        "content": "This command installs the NLTK library directly from within your Jupyter notebook, allowing you to quickly proceed with your text tokenization experiments without needing to manage Docker images or set up external data processing jobs. The `--user` flag ensures that the library is installed in the user's space, avoiding potential conflicts with system-wide packages."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 255,
    "url": "https://www.examtopics.com/discussions/google/view/131092-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You have recently used TensorFlow to train a classification model on tabular data. You have created a Dataflow pipeline that can transform several terabytes of data into training or prediction datasets consisting of TFRecords. You now need to productionize the model, and you want the predictions to be automatically uploaded to a BigQuery table on a weekly schedule. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImport the model into Vertex AI and deploy it to a Vertex AI endpoint. On Vertex AI Pipelines, create a pipeline that uses the DataflowPythonJobOp and the ModelBacthPredictOp components.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImport the model into Vertex AI and deploy it to a Vertex AI endpoint. Create a Dataflow pipeline that reuses the data processing logic sends requests to the endpoint, and then uploads predictions to a BigQuery table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImport the model into Vertex AI. On Vertex AI Pipelines, create a pipeline that uses the<br>DataflowPvthonJobOp and the ModelBatchPredictOp components.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImport the model into BigQuery. Implement the data processing logic in a SQL query. On Vertex AI Pipelines create a pipeline that uses the BigquervQueryJobOp and the BigqueryPredictModelJobOp components."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 15,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 10,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-27T14:08:00.000Z",
        "voteCount": 8,
        "content": "The DataflowPythonJobOp operator lets you create a Vertex AI Pipelines component that prepares data by submitting a Python-based Apache Beam job to Dataflow for execution.\nhttps://cloud.google.com/vertex-ai/docs/pipelines/dataflow-component#dataflowpythonjobop\nUsing we can specify an output location for Vertex AI to store predictions results\nhttps://cloud.google.com/vertex-ai/docs/pipelines/batchprediction-component\nA - is incorrect since we dont need an endpoint for batch predictions\nB - creating a new Dataflow pipeline is redundant"
      },
      {
        "date": "2024-08-03T09:40:00.000Z",
        "voteCount": 1,
        "content": "Uploading predictions directly to BigQuery from the Dataflow pipeline integrates seamlessly with your data storage."
      },
      {
        "date": "2024-07-05T06:57:00.000Z",
        "voteCount": 1,
        "content": "B is right because\n1)You've already trained a classification model using TensorFlow, so you need to productionize it by deploying it to a Vertex AI endpoint.\n2)To automate the prediction process on a weekly schedule, you can create a Dataflow pipeline that reuses your existing data processing logic. This pipeline will send requests to the deployed model for inference and then upload the predicted results to BigQuery."
      },
      {
        "date": "2024-07-03T22:57:00.000Z",
        "voteCount": 1,
        "content": "Only option B talks about loading the data to BigQuery"
      },
      {
        "date": "2024-06-21T01:10:00.000Z",
        "voteCount": 2,
        "content": "B Vertex AI Deployment: Vertex AI provides a managed environment for deploying machine learning models. It simplifies the process and ensures scalability.\nDataflow Pipeline Reuse: Reusing the existing Dataflow pipeline for data processing leverages your existing code and avoids redundant logic.\nModel Endpoint Predictions: Sending requests to the deployed model endpoint allows for efficient prediction generation.\nBigQuery Upload: Uploading predictions directly to BigQuery from the Dataflow pipeline integrates seamlessly with your data storage."
      },
      {
        "date": "2024-04-18T23:57:00.000Z",
        "voteCount": 4,
        "content": "No need to deploy to endpoint as we need batch predictions. ModelBatchPredictOp can upload data to BQ. Dataflow pipeline logic can be implemented in DataflowPythonJobOp"
      },
      {
        "date": "2024-04-16T18:00:00.000Z",
        "voteCount": 1,
        "content": "TFRecords is a specific file format designed by TensorFlow for storing data in a way that's efficient for the machine learning framework. Here are some key points about TFRecords:"
      },
      {
        "date": "2024-04-16T17:57:00.000Z",
        "voteCount": 2,
        "content": "Option A: Vertex AI Pipelines' ModelBatchPredictOp is designed for batch prediction within pipelines, not for serving models through an endpoint.\nOption C: Importing the model directly into BigQuery is not feasible for TensorFlow models.\nOption D: Vertex AI Pipelines' BigqueryPredictModelJobOp assumes the model is already trained and hosted in BigQuery ML, which isn't the case here."
      },
      {
        "date": "2024-04-17T12:04:00.000Z",
        "voteCount": 3,
        "content": "Importing the model directly into BigQuery is not feasible for TensorFlow models. -&gt; not true"
      },
      {
        "date": "2024-04-13T23:25:00.000Z",
        "voteCount": 2,
        "content": "ModelBatchPredictOp -&gt; upload automatically on BQ\nNo need for endpoint \n\n--&gt; C"
      },
      {
        "date": "2024-04-08T12:54:00.000Z",
        "voteCount": 1,
        "content": "agree with BlehMaks"
      },
      {
        "date": "2024-02-29T11:17:00.000Z",
        "voteCount": 3,
        "content": "Answer is C. No need for an endpoint here : Simply specify the BigQuery table URI in the ModelBatchPredictOp parameter and you're done automatically uploading to BigQuery"
      },
      {
        "date": "2024-02-16T12:12:00.000Z",
        "voteCount": 2,
        "content": "My Answer: B\n\nThe most complete answer, and reuse a created pipeline. Don\u2019t make sense to use DataflowPythonJobOp when you have already created a dataflow pipeline that does the same."
      },
      {
        "date": "2024-02-10T05:02:00.000Z",
        "voteCount": 1,
        "content": "Not A, C as they does not explicitly mention how the predictions will be uploaded to BigQuery."
      },
      {
        "date": "2024-01-23T00:33:00.000Z",
        "voteCount": 1,
        "content": "The answer is B, optional A and B doesn't mention how to import prediction result to BigQuery."
      },
      {
        "date": "2024-01-13T07:26:00.000Z",
        "voteCount": 1,
        "content": "Option A: Vertex AI Pipelines are excellent for orchestrating ML workflows but might not be as efficient as Dataflow for large-scale data processing, especially with existing Dataflow logic.\nOption C: While Vertex AI Pipelines can handle model loading and prediction, Dataflow is better suited for large-scale data processing and BigQuery integration.\nOption D: BigQuery ML is primarily for in-database model training and prediction, not ideal for external models or large-scale data processing."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 256,
    "url": "https://www.examtopics.com/discussions/google/view/130733-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You work for an online grocery store. You recently developed a custom ML model that recommends a recipe when a user arrives at the website. You chose the machine type on the Vertex AI endpoint to optimize costs by using the queries per second (QPS) that the model can serve, and you deployed it on a single machine with 8 vCPUs and no accelerators.<br><br>A holiday season is approaching and you anticipate four times more traffic during this time than the typical daily traffic. You need to ensure that the model can scale efficiently to the increased demand. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Maintain the same machine type on the endpoint.<br>2. Set up a monitoring job and an alert for CPU usage.<br>3. If you receive an alert, add a compute node to the endpoint.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Change the machine type on the endpoint to have 32 vCPUs.<br>2. Set up a monitoring job and an alert for CPU usage.<br>3. If you receive an alert, scale the vCPUs further as needed.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Maintain the same machine type on the endpoint Configure the endpoint to enable autoscaling based on vCPU usage.<br>2. Set up a monitoring job and an alert for CPU usage.<br>3. If you receive an alert, investigate the cause.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Change the machine type on the endpoint to have a GPU. Configure the endpoint to enable autoscaling based on the GPU usage.<br>2. Set up a monitoring job and an alert for GPU usage.<br>3. If you receive an alert, investigate the cause."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 10,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-04-16T18:07:00.000Z",
        "voteCount": 5,
        "content": "Option A: Manually adding compute nodes after an alert might lead to delays and potential outages during peak traffic.\nOption B: Upgrading to 32 vCPUs upfront might be an overkill if the current machine type with 8 vCPUs can handle the typical daily traffic. Vertical scaling (more vCPUs) might be suitable only if the model can benefit from additional CPU power.\nOption D: Using a GPU is unlikely to benefit a recipe recommendation model, which likely doesn't involve intensive graphical processing. Additionally, monitoring GPU usage wouldn't be relevant."
      },
      {
        "date": "2024-07-05T07:03:00.000Z",
        "voteCount": 1,
        "content": "C is right because\n1)Since you've already optimized your model's deployment on a single machine with 8 vCPUs, it makes sense to maintain the same machine type to avoid any potential performance issues.\n2)Enabling autoscaling based on vCPU usage will allow your endpoint to automatically add more machines as needed to handle the increased traffic during the holiday season. This approach is more efficient and cost-effective than scaling up individual machines or adding new machines manually.\n3)Monitoring CPU usage with a job and alerting when thresholds are exceeded allows you to detect potential issues before they impact performance."
      },
      {
        "date": "2024-04-13T02:02:00.000Z",
        "voteCount": 1,
        "content": "C: Use Autoscaling Based on vCPU Usage"
      },
      {
        "date": "2024-04-09T19:13:00.000Z",
        "voteCount": 1,
        "content": "Autoscaling based on vCPU usage aligns well with the workload."
      },
      {
        "date": "2024-04-09T18:49:00.000Z",
        "voteCount": 2,
        "content": "Option A is manual intervention \nOption B is overprovisioning preemptively, which is an overkill ( autoscaling should be preferred) \nOption D - Unless the recipe recommendation model uses GPU-accelerated computations (e.g., some deep learning models), adding a GPU won't be beneficial and will increase costs.\nI would go with C - Autoscaling based on vCPU usage which aligns well with the workload."
      },
      {
        "date": "2024-01-22T23:39:00.000Z",
        "voteCount": 1,
        "content": "Option B can only support exact 4x times traffic, but the requirement is  four times \"more\", so B is not the best at least for me."
      },
      {
        "date": "2024-01-22T02:00:00.000Z",
        "voteCount": 1,
        "content": "I would go for C as it enables autoscaling when exceeding a determined CPU usage threshold."
      },
      {
        "date": "2024-01-13T07:28:00.000Z",
        "voteCount": 1,
        "content": "Cost Optimization: It starts with the current machine type, avoiding unnecessary upfront costs, and scales only when needed.\nAutoscaling: It automatically adjusts compute resources based on vCPU usage, ensuring the endpoint can handle traffic spikes without manual intervention.\nMonitoring and Alerting: It provides visibility into resource usage and triggers alerts for potential issues, enabling proactive actions.\nInvestigation: It encourages investigation of alerts to identify any underlying problems beyond expected traffic growth, ensuring overall system health."
      },
      {
        "date": "2024-01-09T15:07:00.000Z",
        "voteCount": 1,
        "content": "Voting for B as it's the only option to autoscale even though the cost will go up.  All other options include manual intervention."
      },
      {
        "date": "2024-01-22T01:59:00.000Z",
        "voteCount": 1,
        "content": "Wouldn't scaling up the vCPUs after receiving the alert also be manual? It comes across as such to me at least."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 257,
    "url": "https://www.examtopics.com/discussions/google/view/131093-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You recently trained an XGBoost model on tabular data. You plan to expose the model for internal use as an HTTP microservice. After deployment, you expect a small number of incoming requests. You want to productionize the model with the least amount of effort and latency. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the model to BigQuery ML by using CREATE MODEL with the BOOSTED_TREE_REGRESSOR statement, and invoke the BigQuery API from the microservice.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBuild a Flask-based app. Package the app in a custom container on Vertex AI, and deploy it to Vertex AI Endpoints.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBuild a Flask-based app. Package the app in a Docker image, and deploy it to Google Kubernetes Engine in Autopilot mode.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a prebuilt XGBoost Vertex container to create a model, and deploy it to Vertex AI Endpoints.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 12,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-13T07:30:00.000Z",
        "voteCount": 5,
        "content": "Prebuilt Container: It eliminates the need to build and manage a custom container, reducing development time and complexity.\nVertex AI Endpoints: It provides a managed serving infrastructure with low latency and high availability, optimizing performance for predictions.\nMinimal Effort: It involves simple steps of creating a Vertex model and deploying it to an endpoint, streamlining the process."
      },
      {
        "date": "2024-01-22T05:15:00.000Z",
        "voteCount": 5,
        "content": "Bit lost here. I would discard buiding a Flask app since that is the opposite of \"minimum effort\". Between A and D, I guess a prebuilt container (D) involves less effort, but I am not 100% confident."
      },
      {
        "date": "2024-07-05T07:06:00.000Z",
        "voteCount": 1,
        "content": "Option D is correct : \nUsing a prebuilt XGBoost Vertex container (Option D) is the most straightforward approach. This container is specifically designed for running XGBoost models in production environments and can be easily deployed to Vertex AI Endpoints. This will allow you to expose your model as an HTTP microservice with minimal additional work."
      },
      {
        "date": "2024-04-16T18:12:00.000Z",
        "voteCount": 2,
        "content": "Package the Model: Use a library like xgboost-server to create a minimal server for your XGBoost model. This package helps convert your model into a format suitable for serving predictions through an HTTP endpoint.\nDeploy to Cloud Functions: Deploy the packaged model server as a Cloud Function on Google Cloud Platform (GCP). Cloud Functions are serverless, lightweight execution environments ideal for event-driven applications like microservices.\nConfigure Trigger: Set up an HTTP trigger for your Cloud Function, allowing it to be invoked through HTTP requests."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 258,
    "url": "https://www.examtopics.com/discussions/google/view/131094-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You work for an international manufacturing organization that ships scientific products all over the world. Instruction manuals for these products need to be translated to 15 different languages. Your organization\u2019s leadership team wants to start using machine learning to reduce the cost of manual human translations and increase translation speed. You need to implement a scalable solution that maximizes accuracy and minimizes operational overhead. You also want to include a process to evaluate and fix incorrect translations. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a workflow using Cloud Function triggers. Configure a Cloud Function that is triggered when documents are uploaded to an input Cloud Storage bucket. Configure another Cloud Function that translates the documents using the Cloud Translation API, and saves the translations to an output Cloud Storage bucket. Use human reviewers to evaluate the incorrect translations.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Vertex AI pipeline that processes the documents launches, an AutoML Translation training job, evaluates the translations and deploys the model to a Vertex AI endpoint with autoscaling and model monitoring. When there is a predetermined skew between training and live data, re-trigger the pipeline with the latest data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AutoML Translation to train a model. Configure a Translation Hub project, and use the trained model to translate the documents. Use human reviewers to evaluate the incorrect translations.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Vertex AI custom training jobs to fine-tune a state-of-the-art open source pretrained model with your data. Deploy the model to a Vertex AI endpoint with autoscaling and model monitoring. When there is a predetermined skew between the training and live data, configure a trigger to run another training job with the latest data."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 15,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-23T00:11:00.000Z",
        "voteCount": 5,
        "content": "The answer is C, to use Translation Hub\n1.Accuracy maximization: AutoML Translation uses machine learning to train a translation model on your specific data, which can lead to higher accuracy compared to generic translation models.\n2.Minimal operational overhead: AutoML Translation handles the training and deployment of the translation model, reducing the need for manual intervention.\n3.Evaluation and correction: The solution includes human reviewers to evaluate and correct any incorrect translations, ensuring high quality."
      },
      {
        "date": "2024-07-05T07:15:00.000Z",
        "voteCount": 1,
        "content": "Using AutoML Translation (Option C) allows you to train a model on your data, which can be used for translation. You can then configure a Translation Hub project to manage the translation process and use human reviewers to evaluate any incorrect translations.\nIt is scalable solution that maximizes accuracy and minimizes operational overhead."
      },
      {
        "date": "2024-04-20T10:21:00.000Z",
        "voteCount": 1,
        "content": "if we assume there is training data available (source-target language pairs) then I would go with C."
      },
      {
        "date": "2024-04-16T18:20:00.000Z",
        "voteCount": 3,
        "content": "Option A: Cloud Functions are suitable for simple tasks. This approach wouldn't leverage machine learning for improved translations and lacks features like model evaluation and retraining.\nOption B: Vertex AI pipelines with AutoML Translation training can be powerful, but it might be overkill for this scenario. Additionally, retraining based on a predetermined data skew might not be necessary if human review is effective at catching and correcting errors.\nOption D: While fine-tuning a pre-trained model with Vertex AI custom training offers flexibility, it requires more expertise and ongoing maintenance compared to the simpler approach of using AutoML Translation."
      },
      {
        "date": "2024-04-13T05:59:00.000Z",
        "voteCount": 2,
        "content": "Answer A\n\nIt is the only option that makes sense all over. I would go for C if the first sentence was \nnot there \"Use AutoML Translation\". you can't use autoML because there is no training data."
      },
      {
        "date": "2024-04-13T02:14:00.000Z",
        "voteCount": 2,
        "content": "C: Use AutoML Translation with Translation Hub. Here's why:\n1. Scalability: \n - AutoML Translation: This simplifies model training without extensive manual configuration. \n- Translation Hub:  Centrally stores and manages your translation models, facilitating deployment and reuse across various applications, promoting scalability for your 15 target languages.\n2. Accuracy and Evaluation:\n- AutoML Translation: while pre-trained models might not be perfect, AutoML Translation lets you fine-tune the model with your specific scientific domain data (instruction manuals) to improve accuracy.\n- Human Review and Iteration: This allows for evaluation and correction of any inaccurate translations, improving overall quality. This is crucial for technical documents like instruction manuals."
      },
      {
        "date": "2024-04-13T02:14:00.000Z",
        "voteCount": 1,
        "content": "Why not B: Retraining the model upon data skew detection can become cumbersome and impact translation speed. Translation Hub offers a more streamlined approach for managing model updates."
      },
      {
        "date": "2024-04-09T19:10:00.000Z",
        "voteCount": 1,
        "content": "Translation Hub can manage translation workloads at scale and also integrate human feedback where required."
      },
      {
        "date": "2024-03-11T12:07:00.000Z",
        "voteCount": 2,
        "content": "So what is the deal? pikachu007 authors the question, adds C as suggested answer and then vote for B?"
      },
      {
        "date": "2024-02-22T19:19:00.000Z",
        "voteCount": 1,
        "content": "Agree with pikachu007, I think there is no point in using ML once the manual(human) mode is added."
      },
      {
        "date": "2024-01-22T05:29:00.000Z",
        "voteCount": 3,
        "content": "Translation Hub is a service that allows you to manage and automate your translation workflows on Google Cloud. You can use Translation Hub to upload the documents to a Cloud Storage bucket, select the source and target languages, and apply the trained model to translate the documents. You can use human reviewers to improve the quality and accuracy of the translations, and provide feedback to the ML model."
      },
      {
        "date": "2024-01-13T07:34:00.000Z",
        "voteCount": 2,
        "content": "Option A: While Cloud Functions provide automation, the Cloud Translation API uses generic models that might not be as accurate for domain-specific content, potentially leading to more human corrections.\nOption C: Translation Hub offers collaboration features but lacks automated model training and pipeline orchestration, requiring more manual effort.\nOption D: Vertex AI custom training jobs provide flexibility but require more expertise and effort compared to AutoML Translation, and the pre-trained model might not be as well-suited for the specific domain."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 259,
    "url": "https://www.examtopics.com/discussions/google/view/131096-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You have developed an application that uses a chain of multiple scikit-learn models to predict the optimal price for your company\u2019s products. The workflow logic is shown in the diagram. Members of your team use the individual models in other solution workflows. You want to deploy this workflow while ensuring version control for each individual model and the overall workflow. Your application needs to be able to scale down to zero. You want to minimize the compute resource utilization and the manual effort required to manage this solution. What should you do?<br><br><img src=\"https://img.examtopics.com/professional-machine-learning-engineer/image5.png\">",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExpose each individual model as an endpoint in Vertex AI Endpoints. Create a custom container endpoint to orchestrate the workflow.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a custom container endpoint for the workflow that loads each model\u2019s individual files Track the versions of each individual model in BigQuery.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExpose each individual model as an endpoint in Vertex AI Endpoints. Use Cloud Run to orchestrate the workflow.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLoad each model\u2019s individual files into Cloud Run. Use Cloud Run to orchestrate the workflow. Track the versions of each individual model in BigQuery."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-05T07:26:00.000Z",
        "voteCount": 2,
        "content": "Option C is right because: \n1)Exposing individual models as Vertex AI Endpoints (Option C) allows for version tracking, which is essential for maintaining consistency across different workflows.\n2)Using Cloud Run to orchestrate the workflow (Option C) enables you to scale down to zero and minimize compute resource utilization.\n3)You want to deploy your application while ensuring version control for each individual model and the overall workflow."
      },
      {
        "date": "2024-04-20T10:23:00.000Z",
        "voteCount": 1,
        "content": "B,D not correct since BQ is not the best approach.\nA would require more manual work"
      },
      {
        "date": "2024-02-16T15:07:00.000Z",
        "voteCount": 4,
        "content": "My Answer: C\n\nB and D: Not Correct: Big query is not the best approach to trach versions of model. \n\nA and C: Looking for \u201censuring version control for each individual mode\u201d (endpoints), and \u201cbe able to scale down to zero\u201d,  \u201cminimize the compute resource utilization and the manual effort required to manage this solution\u201d, I think to use Cloud Run could be the best option for those cases.\n\nhttps://www.youtube.com/watch?v=nhwYc4StHIc&amp;ab_channel=GoogleCloudTech"
      },
      {
        "date": "2024-01-13T07:36:00.000Z",
        "voteCount": 4,
        "content": "Option A: A custom container endpoint for orchestration adds complexity and management overhead.\nOption B: Loading model files directly into a custom container endpoint can lead to versioning challenges and potential conflicts if models are shared across workflows.\nOption D: Using BigQuery for model versioning is not its primary function and might introduce complexities in model loading and management."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 260,
    "url": "https://www.examtopics.com/discussions/google/view/131304-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You are developing a model to predict whether a failure will occur in a critical machine part. You have a dataset consisting of a multivariate time series and labels indicating whether the machine part failed. You recently started experimenting with a few different preprocessing and modeling approaches in a Vertex AI Workbench notebook. You want to log data and track artifacts from each run. How should you set up your experiments?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Use the Vertex AI SDK to create an experiment and set up Vertex ML Metadata.<br>2. Use the log_time_series_metrics function to track the preprocessed data, and use the log_merrics function to log loss values.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Use the Vertex AI SDK to create an experiment and set up Vertex ML Metadata.<br>2. Use the log_time_series_metrics function to track the preprocessed data, and use the log_metrics function to log loss values.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a Vertex AI TensorBoard instance and use the Vertex AI SDK to create an experiment and associate the TensorBoard instance.<br>2. Use the assign_input_artifact method to track the preprocessed data and use the log_time_series_metrics function to log loss values.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a Vertex AI TensorBoard instance, and use the Vertex AI SDK to create an experiment and associate the TensorBoard instance.<br>2. Use the log_time_series_metrics function to track the preprocessed data, and use the log_metrics function to log loss values."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 9,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 8,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-09-18T03:21:00.000Z",
        "voteCount": 1,
        "content": "C sounds more correct"
      },
      {
        "date": "2024-07-22T22:34:00.000Z",
        "voteCount": 1,
        "content": "A and B are the same"
      },
      {
        "date": "2024-04-27T02:55:00.000Z",
        "voteCount": 4,
        "content": "Vertex AI Experiment and ML Metadata: This is the foundation for tracking experiments and artifacts within Vertex AI.expand_more Creating an experiment allows you to group related runs and log data associated with those runs. ML Metadata helps manage the lineage of data and models used in your experiments.expand_more\n\nLogging Data:\n\nlog_time_series_metrics: This function is specifically designed for tracking time-series data, making it suitable for logging the preprocessed multivariate time series data in your experiment.\nlog_metrics: This function is appropriate for logging loss values during model training. It can handle numerical values like loss efficiently.\nBy combining these techniques, you can effectively track both the preprocessed data (time series) and the training performance metrics (loss values) within your Vertex AI Experiment."
      },
      {
        "date": "2024-04-27T02:55:00.000Z",
        "voteCount": 2,
        "content": "Option A: It lacks the functionality to log preprocessed data (no log_time_series_metrics).\nOption C and D: While TensorBoard can be used for visualization, it's not directly related to logging data within Vertex AI Experiments.\n\npen_spark\nexclamation Additionally, assign_input_artifact isn't the correct method for logging time series data"
      },
      {
        "date": "2024-04-19T00:27:00.000Z",
        "voteCount": 4,
        "content": "log_time_series_metrics requires setting Tensorboard: https://cloud.google.com/vertex-ai/docs/experiments/log-data\n\nassign_input_artifacts can be used to track input data: https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/experiments/get_started_with_vertex_experiments.ipynb"
      },
      {
        "date": "2024-04-13T02:21:00.000Z",
        "voteCount": 2,
        "content": "Why B?\n1. Experiment Creation: Vertex AI SDK establishes a context for grouping your training runs and facilitates experiment management.\n2. By setting up Vertex ML Metadata (only can be done when creating an experiment with the Vertex AI SDK), you enable tracking of artifacts and metrics associated with each experiment run.\n3. log_time_series_metrics function is well-suited for tracking the preprocessed multivariate time series data associated with each experiment run. This allows you to analyze how preprocessing impacts model performance."
      },
      {
        "date": "2024-03-29T03:14:00.000Z",
        "voteCount": 2,
        "content": "B\nThe assign_input_artifacts method is used to associate input artifacts with an experiment, that is not used for log time series and labels.\nA and B is just with a minor typo (metric vs merric), so select B."
      },
      {
        "date": "2024-02-16T15:32:00.000Z",
        "voteCount": 1,
        "content": "My Answer: C \n\nassign_input_artifact method is a method to Vertex Ai Experiment to track the preprocessed data while log_time_series_metrics  is a function of Vertex AI TensorBoard to log metrics along time. \n\nlook:\n\nhttps://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/experiments/build_model_experimentation_lineage_with_prebuild_code.ipynb\n\nhttps://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/experiments/comparing_local_trained_models.ipynb"
      },
      {
        "date": "2024-01-22T06:25:00.000Z",
        "voteCount": 2,
        "content": "C.\nTensorboard for experimentation and comparison of different model runs.\nassign_input_artifacts to track preprocessed data, since it links artifacts as inputs to the execution. https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.Execution#google_cloud_aiplatform_Execution_assign_input_artifacts\nUsing log_time_series_metrics would make sense if what we were doing is logging a metric, which we aren't when we track the preprocessed data not yet ran by the model."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 261,
    "url": "https://www.examtopics.com/discussions/google/view/131098-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You are developing a recommendation engine for an online clothing store. The historical customer transaction data is stored in BigQuery and Cloud Storage. You need to perform exploratory data analysis (EDA), preprocessing and model training. You plan to rerun these EDA, preprocessing, and training steps as you experiment with different types of algorithms. You want to minimize the cost and development effort of running these steps as you experiment. How should you configure the environment?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Vertex AI Workbench user-managed notebook using the default VM instance, and use the %%bigquerv magic commands in Jupyter to query the tables.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Vertex AI Workbench managed notebook to browse and query the tables directly from the JupyterLab interface.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Vertex AI Workbench user-managed notebook on a Dataproc Hub, and use the %%bigquery magic commands in Jupyter to query the tables.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Vertex AI Workbench managed notebook on a Dataproc cluster, and use the spark-bigquery-connector to access the tables."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 9,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-22T06:38:00.000Z",
        "voteCount": 5,
        "content": "\"Managed notebooks are usually a good choice if you want to use a notebook for data exploration, analysis, modeling, or as part of an end-to-end data science workflow.\n\nManaged notebooks instances let you perform workflow-oriented tasks without leaving the JupyterLab interface. They also have many integrations and features for implementing your data science workflow.\"\n\nvs. \n\n\"User-managed notebooks can be a good choice for users who require extensive customization or who need a lot of control over their environment.\"\n\nSeems more like the former -&gt; B"
      },
      {
        "date": "2024-07-05T08:05:00.000Z",
        "voteCount": 2,
        "content": "B is right because this option allows you to minimize cost and development effort by using a managed notebook in Vertex AI Workbench, which integrates well with BigQuery and Cloud Storage. You can browse and query your data directly within the JupyterLab interface without having to create a separate BigQuery client or use the bq command-line tool."
      },
      {
        "date": "2024-04-21T03:40:00.000Z",
        "voteCount": 1,
        "content": "see b1a8fae"
      },
      {
        "date": "2024-04-20T10:39:00.000Z",
        "voteCount": 1,
        "content": "agree with guilhermebutzke. Also, this option is easier to reuse in multiple experiments"
      },
      {
        "date": "2024-02-16T15:55:00.000Z",
        "voteCount": 1,
        "content": "My Answer: A\n\nA: Default VM instance is the best to minimize the cost, and the command %%bigquery magic is the most easy way to get data from BQ. \n\nB: Not necessary JupyerLab interface to run code. The %%bigquerv magic commands is sufficient to get data and run easily queries. \n\nC: Dataproc Hub seems overkill and it is more expensive than a default VM instance. \n\nC:  spark-bigquery-connector unnecessary to get tables in the notebook. better use  %%bigquery."
      },
      {
        "date": "2024-01-22T22:00:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/bigquery/docs/visualize-jupyter"
      },
      {
        "date": "2024-01-17T20:03:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/vertex-ai/docs/workbench/notebook-solution#:~:text=For%20users%20who%20have%20specific,user%2Dmanaged%20notebooks%20instance's%20VM."
      },
      {
        "date": "2024-01-13T07:45:00.000Z",
        "voteCount": 1,
        "content": "Option A: User-managed notebooks require VM instance management, adding cost and complexity. %%bigquery magic commands are still needed.\nOption C: Dataproc Hub adds unnecessary cost and complexity for simple BigQuery interactions.\nOption D: Spark-bigquery-connector adds complexity and overhead compared to the native BigQuery integration in managed notebooks."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 262,
    "url": "https://www.examtopics.com/discussions/google/view/131099-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You recently deployed a model to a Vertex AI endpoint and set up online serving in Vertex AI Feature Store. You have configured a daily batch ingestion job to update your featurestore. During the batch ingestion jobs, you discover that CPU utilization is high in your featurestore\u2019s online serving nodes and that feature retrieval latency is high. You need to improve online serving performance during the daily batch ingestion. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSchedule an increase in the number of online serving nodes in your featurestore prior to the batch ingestion jobs",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable autoscaling of the online serving nodes in your featurestore\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable autoscaling for the prediction nodes of your DeployedModel in the Vertex AI endpoint",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the worker_count in the ImportFeatureValues request of your batch ingestion job"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 4,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-08-13T23:37:00.000Z",
        "voteCount": 1,
        "content": "Agree with bobjr"
      },
      {
        "date": "2024-07-02T21:05:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/vertex-ai/docs/featurestore/managing-featurestores\nSpecifically mentioned here that --&gt; If CPU utilization is consistently high, consider increasing the number of online serving nodes for your featurestore."
      },
      {
        "date": "2024-06-04T13:36:00.000Z",
        "voteCount": 2,
        "content": "Gemini + Perplexity ai + ChatGPT votes A\n\nBecause : B. Enable Autoscaling: While autoscaling can be useful, it might not react quickly enough to sudden spikes in traffic during batch ingestion. Scheduling the increase ensures that the resources are available when needed."
      },
      {
        "date": "2024-04-24T11:17:00.000Z",
        "voteCount": 1,
        "content": "This question is valid for the Legacy feature store.\nhttps://cloud.google.com/vertex-ai/docs/featurestore/ingesting-batch#import_job_performance"
      },
      {
        "date": "2024-04-26T08:02:00.000Z",
        "voteCount": 1,
        "content": "\"CPU utilization is high in your featurestore\u2019s online serving nodes\""
      },
      {
        "date": "2024-01-22T23:12:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/vertex-ai/docs/featurestore/managing-featurestores?&amp;_gl=1*sswg5e*_ga*NDE2OTc3OTAzLjE3MDU4OTQ5OTE.*_ga_WH2QY8WWF5*MTcwNTkzNDM0NS40LjAuMTcwNTkzNDM0NS4wLjAuMA..&amp;_ga=2.242492743.-416977903.1705894991#online_serving_nodes"
      },
      {
        "date": "2024-01-22T06:48:00.000Z",
        "voteCount": 2,
        "content": "Vertex AI Feature Store provides two options for online serving: Bigtable and optimized online serving. Both options support autoscaling, which means that the number of online serving nodes can automatically adjust to the traffic demand. By enabling autoscaling, you can improve the online serving performance and reduce the feature retrieval latency during the daily batch ingestion. Autoscaling also helps you optimize the cost and resource utilization of your featurestore."
      },
      {
        "date": "2024-01-13T07:47:00.000Z",
        "voteCount": 1,
        "content": "Option A: Manually scheduling node increases requires prior knowledge of batch ingestion times and might not be as responsive to unexpected workload spikes.\nOption C: Autoscaling prediction nodes in the Vertex AI endpoint might help with model prediction latency but doesn't directly address feature retrieval latency from the featurestore.\nOption D: Increasing worker_count in the batch ingestion job could speed up ingestion but might further strain online serving nodes, potentially worsening latency."
      },
      {
        "date": "2024-01-16T04:51:00.000Z",
        "voteCount": 1,
        "content": "Hey Pikachu, \ndid you pass the exam or are you preparing? I am as well preparing and I have noticed that in many question you chose the same answer I would chose, but which is not the indicated answer of my Udemy Course Exam Preparation. \nThanks and Best"
      },
      {
        "date": "2024-01-18T02:28:00.000Z",
        "voteCount": 5,
        "content": "Yes passed"
      },
      {
        "date": "2024-01-25T22:23:00.000Z",
        "voteCount": 1,
        "content": "Hi @pikachu007 May I ask when did you pass the exam? was it after they updated the new questions? I need to take the exam ASAP, just want to make sure the new questions are valid?"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 263,
    "url": "https://www.examtopics.com/discussions/google/view/131100-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You are developing a custom TensorFlow classification model based on tabular data. Your raw data is stored in BigQuery. contains hundreds of millions of rows, and includes both categorical and numerical features. You need to use a MaxMin scaler on some numerical features, and apply a one-hot encoding to some categorical features such as SKU names. Your model will be trained over multiple epochs. You want to minimize the effort and cost of your solution. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Write a SQL query to create a separate lookup table to scale the numerical features.<br>2. Deploy a TensorFlow-based model from Hugging Face to BigQuery to encode the text features.<br>3. Feed the resulting BigQuery view into Vertex AI Training.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Use BigQuery to scale the numerical features.<br>2. Feed the features into Vertex AI Training.<br>3. Allow TensorFlow to perform the one-hot text encoding.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Use TFX components with Dataflow to encode the text features and scale the numerical features.<br>2. Export results to Cloud Storage as TFRecords.<br>3. Feed the data into Vertex AI Training.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Write a SQL query to create a separate lookup table to scale the numerical features.<br>2. Perform the one-hot text encoding in BigQuery.<br>3. Feed the resulting BigQuery view into Vertex AI Training."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 15,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 7,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-04-27T08:34:00.000Z",
        "voteCount": 6,
        "content": "\"Full-pass stateful transformations aren't suitable for implementation in BigQuery. If you use BigQuery for full-pass transformations, you need auxiliary tables to store quantities needed by stateful transformations, such as means and variances to scale numerical features. Further, implementation of full-pass transformations using SQL on BigQuery creates increased complexity in the SQL scripts, and creates intricate dependency between training and the scoring SQL scripts.\"\nhttps://www.tensorflow.org/tfx/guide/tft_bestpractices#where_to_do_preprocessing"
      },
      {
        "date": "2024-07-02T21:19:00.000Z",
        "voteCount": 1,
        "content": "Isn't Dataflow includes a lot of effort as the question asking to minimize the effort here?"
      },
      {
        "date": "2024-10-12T16:15:00.000Z",
        "voteCount": 1,
        "content": "multiple epochs --&gt; need to persist data after preprocessing"
      },
      {
        "date": "2024-09-15T13:22:00.000Z",
        "voteCount": 1,
        "content": "Option D since it says minimize effort and cost following that adding something rather than BQ will increase complexity."
      },
      {
        "date": "2024-07-05T08:16:00.000Z",
        "voteCount": 1,
        "content": "Option C uses TFX (TensorFlow Extended) components with Dataflow, which is a great way to perform complex data preprocessing tasks like one-hot encoding and scaling.\nThis approach allows you to process your data in a scalable and efficient manner, using Cloud Storage as the output location.\nBy exporting the results as TFRecords, you can easily feed this preprocessed data into Vertex AI Training for model development."
      },
      {
        "date": "2024-07-02T04:08:00.000Z",
        "voteCount": 1,
        "content": "agree with TFX components with Dataflow"
      },
      {
        "date": "2024-06-04T13:43:00.000Z",
        "voteCount": 2,
        "content": "GPT says D, Gemini says B, Perplexity says C....\n\nI say D : stay in one tool, BQ, which is cheap and natively scalable.\n\nB has a risk of out of memory error."
      },
      {
        "date": "2024-04-27T03:12:00.000Z",
        "voteCount": 1,
        "content": "BigQuery for Preprocessing:\nBigQuery is a serverless data warehouse optimized for large datasets.expand_more It can handle scaling numerical features using built-in functions like SCALE or QUANTILE_SCALE, reducing the need for complex custom logic or separate lookup tables.\nTensorFlow for One-Hot Encoding:\nTensorFlow excels at in-memory processing. One-hot encoding of categorical features, especially text features like SKU names, can be efficiently performed within your TensorFlow model during training. This avoids unnecessary data movement or transformations in BigQuery.\nVertex AI Training:\nBy feeding the preprocessed data (scaled numerical features) directly into Vertex AI Training, you leverage its managed infrastructure for training your custom TensorFlow model."
      },
      {
        "date": "2024-04-27T03:12:00.000Z",
        "voteCount": 1,
        "content": "Option A: Creates unnecessary complexity and data movement. BigQuery is better suited for scaling numerical features, and TensorFlow is efficient for one-hot encoding.\nOption C: TFX is a powerful framework for complex pipelines, but for a simpler scenario like this, it might be an overkill. Additionally, exporting data as TFRecords adds an extra step, potentially increasing cost and complexity.\nOption D: One-hot encoding in BigQuery might be cumbersome for textual features like SKU names.\n\npen_spark\nexclamation It can be computationally expensive and result in data explosion. TensorFlow handles this efficiently within the model."
      },
      {
        "date": "2024-04-24T11:20:00.000Z",
        "voteCount": 1,
        "content": "Agree with b1a8fae"
      },
      {
        "date": "2024-04-20T10:43:00.000Z",
        "voteCount": 2,
        "content": "agree with daidai75"
      },
      {
        "date": "2024-04-26T08:04:00.000Z",
        "voteCount": 1,
        "content": "Option B is not suitable for the big volume of data processing?????\nBQ is not suitable for big volume??..\n\nfor me is B"
      },
      {
        "date": "2024-02-18T17:56:00.000Z",
        "voteCount": 3,
        "content": "My Answer: B\n\n1. Use BigQuery to scale the numerical features.: Simpler and cheaper then use TFX components with Dataflow to scale the numerical features\n2. Feed the features into Vertex AI Training.\n3. Allow TensorFlow to perform the one-hot text encoding: TensorFlow handles the one-hot text encoding better than BQ."
      },
      {
        "date": "2024-01-22T23:20:00.000Z",
        "voteCount": 2,
        "content": "key messages: \"contains hundreds of millions of rows, and includes both categorical and numerical features. You need to use a MaxMin scaler on some numerical features, and apply a one-hot encoding to some categorical features such as SKU names\". \nOption B is not suitable for the big volume of data processing. Option C is better."
      },
      {
        "date": "2024-01-22T06:57:00.000Z",
        "voteCount": 2,
        "content": "Inclined to choose C over B. By using TFX components with Dataflow, you can perform feature engineering on large-scale tabular data in a distributed and efficient way. You can use the Transform component to apply the MaxMin scaler and the one-hot encoding to the numerical and categorical features, respectively. You can also use the ExampleGen component to read data from BigQuery and the Trainer component to train your TensorFlow model."
      },
      {
        "date": "2024-01-13T08:02:00.000Z",
        "voteCount": 3,
        "content": "Option A: Involves creating a separate lookup table and deploying a Hugging Face model in BigQuery, increasing complexity and cost.\nOption C: While TFX offers robust preprocessing capabilities, it adds overhead for this use case and requires knowledge of Dataflow.\nOption D: Performing one-hot encoding in BigQuery can be less efficient than TensorFlow's optimized implementation."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 264,
    "url": "https://www.examtopics.com/discussions/google/view/131101-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You work for a retail company. You have been tasked with building a model to determine the probability of churn for each customer. You need the predictions to be interpretable so the results can be used to develop marketing campaigns that target at-risk customers. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBuild a random forest regression model in a Vertex AI Workbench notebook instance. Configure the model to generate feature importances after the model is trained.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBuild an AutoML tabular regression model. Configure the model to generate explanations when it makes predictions.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBuild a custom TensorFlow neural network by using Vertex AI custom training. Configure the model to generate explanations when it makes predictions.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBuild a random forest classification model in a Vertex AI Workbench notebook instance. Configure the model to generate feature importances after the model is trained.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 9,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 8,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-10-12T16:15:00.000Z",
        "voteCount": 1,
        "content": "Probability --&gt; regression model"
      },
      {
        "date": "2024-09-15T08:37:00.000Z",
        "voteCount": 1,
        "content": "Churn probability is required; linear regression will give the label, and classification will provide the likelihood as requested."
      },
      {
        "date": "2024-08-11T07:16:00.000Z",
        "voteCount": 2,
        "content": "We can't use AutoML due to the lack of explicability. AutoML is a black box, and we can't know which model is GCP using under the hood:\n\nWhether is true that you can use the feature importance tool when using AutoML, GCP doesn't publicly disclose the specific models used internally for each type of problem (classification, regression, etc.). AutoML employs a wide range of algorithms, from linear models and decision trees to more complex neural networks. Consequently, the lack of explicability lead us to discard any AutoML option.\n\nRegarding the classification/regression discussion, as Roulle says \"Churn problems are cases of classification. We don't predict the label, but the probability of belonging to a given class (churn or not). We then set a threshold to indicate the probability at which we can affirm that the person will or will not unsubscribe.\""
      },
      {
        "date": "2024-07-03T04:27:00.000Z",
        "voteCount": 1,
        "content": "Churn problems are cases of classification. We don't predict the label, but the probability of belonging to a given class (churn or not). We then set a threshold to indicate the probability at which we can affirm that the person will or will not unsubscribe.\nWe can eliminate all responses that mention regression (A &amp; B).\n\nA random forest is therefore less complex to interpret than a neural network.\n\nSo I'm pretty sure it's D"
      },
      {
        "date": "2024-04-19T02:32:00.000Z",
        "voteCount": 1,
        "content": "agree with Yan_X. This is a classification problem, so regression should not be used (rule out A&amp;B). Neural networks don't have explainable features by default, and Random Forest provides global explanations..."
      },
      {
        "date": "2024-04-21T03:44:00.000Z",
        "voteCount": 1,
        "content": "probability of churn for each customer......"
      },
      {
        "date": "2024-04-15T19:59:00.000Z",
        "voteCount": 2,
        "content": "Since interpretability is key for your churn prediction model to inform marketing campaigns, \n--&gt; Choose an interpretable model:\nLogistic Regression: This is a classic choice for interpretability. It provides coefficients for each feature, indicating how a unit increase in that feature impacts the probability of churn. Easy to understand and implement, it's a good starting point.\nDecision Trees with Rule Extraction: Decision trees are inherently interpretable, with each branch representing a decision rule. By extracting these rules, you can understand the specific factors leading to churn (e.g., \"Customers with low tenure and high number of support tickets are more likely to churn\")."
      },
      {
        "date": "2024-04-08T23:27:00.000Z",
        "voteCount": 1,
        "content": "the probability of churn for each customer -&gt; regression -&gt; B"
      },
      {
        "date": "2024-04-04T05:54:00.000Z",
        "voteCount": 2,
        "content": "I don't know which one is correct...\nAs D is 'after the model is trained', so not for each prediction.\nAnd B 'AutoML tabular regression model' is regression, but for not classification problem..."
      },
      {
        "date": "2024-02-16T16:06:00.000Z",
        "voteCount": 3,
        "content": "My Answer: B\n\n\u201cthe probability of churn for each customer\u201d: the probability is a number. So regression problem. (A,B, C)\n\n\u201cpredictions to be interpretable\u201d: explainable in predict not in the model (B,C)\n\nChoosing between \u201cBuild an AutoML tabular regression model\u201d and \u201cBuild a custom TensorFlow neural network by using Vertex AI custom training\u201d, I think B could be the most relevant for the problem.  However I also think that others no enough information in the text to choose between the two."
      },
      {
        "date": "2024-01-31T04:53:00.000Z",
        "voteCount": 1,
        "content": "the question asks for explainability for predictions, answer D does not provide that. \nAlthough not the ideal solution, B is the only answer that suits the requirements, because churn can also be expressed as a probability."
      },
      {
        "date": "2024-02-12T19:43:00.000Z",
        "voteCount": 1,
        "content": "But, in Option B is says \"AutoML Regression\" if the problem statement is about classification!"
      },
      {
        "date": "2024-01-22T23:27:00.000Z",
        "voteCount": 2,
        "content": "The answer is D.\n1.Churn prediction is a classification problem: We want to categorize customers as either churning or not churning, not predict a continuous value like revenue. Therefore, a classification model is needed.\n2.Random forest models are interpretable: Feature importances provide insights into which features contribute most to the model's predictions, making them a good choice for understanding why customers churn. This interpretability is crucial for developing targeted marketing campaigns.\n3.Vertex AI Workbench is a suitable platform: It provides notebook instances for building and training models, making it a good choice for this task."
      },
      {
        "date": "2024-01-17T20:16:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/bigquery/docs/xai-overview"
      },
      {
        "date": "2024-01-13T08:06:00.000Z",
        "voteCount": 2,
        "content": "Option A: Regression, not classification, is used for random forest model, which is not appropriate for predicting probabilities.\nOption B: While AutoML tabular can generate model explanations, random forests inherently provide more granular insights into feature importance.\nOption C: Neural networks can be less interpretable than tree-based models, and generating explanations for them often requires additional techniques and libraries."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 265,
    "url": "https://www.examtopics.com/discussions/google/view/131102-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You work for a company that is developing an application to help users with meal planning. You want to use machine learning to scan a corpus of recipes and extract each ingredient (e.g., carrot, rice, pasta) and each kitchen cookware (e.g., bowl, pot, spoon) mentioned. Each recipe is saved in an unstructured text file. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a text dataset on Vertex AI for entity extraction Create two entities called \u201cingredient\u201d and \u201ccookware\u201d, and label at least 200 examples of each entity. Train an AutoML entity extraction model to extract occurrences of these entity types. Evaluate performance on a holdout dataset.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a multi-label text classification dataset on Vertex AI. Create a test dataset, and label each recipe that corresponds to its ingredients and cookware. Train a multi-class classification model. Evaluate the model\u2019s performance on a holdout dataset.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Entity Analysis method of the Natural Language API to extract the ingredients and cookware from each recipe. Evaluate the model's performance on a prelabeled dataset.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a text dataset on Vertex AI for entity extraction. Create as many entities as there are different ingredients and cookware. Train an AutoML entity extraction model to extract those entities. Evaluate the model\u2019s performance on a holdout dataset."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 7,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-07-05T08:29:00.000Z",
        "voteCount": 1,
        "content": "By choosing option A, you can leverage the power of machine learning to efficiently extract ingredients and cookware from recipes in a scalable manner.\noption C uses the Entity Analysis method of the Natural Language API, which might be a viable option if you had access to the API's pre-trained models. However, since you're working with Vertex AI, creating a dataset for entity extraction is a better choice."
      },
      {
        "date": "2024-04-15T20:14:00.000Z",
        "voteCount": 1,
        "content": "For extracting ingredients and cookware from recipe text files, creating a text dataset on Vertex AI for entity extraction with a custom NER model is the better approach. While it requires more upfront effort for data labeling and training, it offers superior accuracy and control over the types of entities extracted.\n\nHowever, if you need a quick and easy solution to get started, the Natural Language API's Entity Analysis can be a temporary option.  Be aware that the accuracy might be lower, and you might need to post-process the results to filter out irrelevant entities."
      },
      {
        "date": "2024-04-13T00:27:00.000Z",
        "voteCount": 1,
        "content": "Natural Language API offers a pre-built solution for entity analysis which eliminates the need for custom model training and labeling large datasets, saving time and resources.\n\nVertex AI AutoML can aslo be used for entity extraction but it requires data labeling and training, which can be time-consuming for a vast number of potential ingredients and cookware."
      },
      {
        "date": "2024-02-16T16:49:00.000Z",
        "voteCount": 2,
        "content": "My Answer: A\n\n&nbsp;A: is the most suitable approach for this task because we need to identify and extract specific named entities (\"ingredient\" and \"cookware\") from the text,&nbsp;not classify the entire recipe into predefined categories.\n\nB: This approach would require classifying each recipe based on all possible ingredients and cookware,&nbsp;leading to a vast number of classes and potential performance issues.\n\nC: This pre-built solution might not be as customizable or scalable as training a specific model for this task.\n\nD: This is impractical and unnecessary as the number of potential ingredients and cookware is vast."
      },
      {
        "date": "2024-01-22T23:36:00.000Z",
        "voteCount": 1,
        "content": "I prefer to A.\nOption C is not the best, because The NLP API is designed to identify general entities within text. While it's effective for broad categories, it may not be as precise for specialized domains like cooking ingredients and cookware, which require a more tailored approach."
      },
      {
        "date": "2024-01-22T07:17:00.000Z",
        "voteCount": 3,
        "content": "A.\n\"... you might create an entity extraction model to identify specialized terminology in legal documents or patents.\"\n\nI prefer this over C, which might classify carrot as vegetable, chicken as meat... custom entity extraction allows you to specify what entities you wish to extract from the text."
      },
      {
        "date": "2024-01-22T07:17:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/vertex-ai/docs/text-data/entity-extraction/prepare-data"
      },
      {
        "date": "2024-01-17T20:23:00.000Z",
        "voteCount": 1,
        "content": "Reconsidering my answer and going with C \nOption A involves using AutoML entity extraction, which could be a valid approach. However, for extracting entities like ingredients and cookware, Google Cloud's pre-trained Natural Language API might be a more straightforward solution."
      },
      {
        "date": "2024-02-12T19:48:00.000Z",
        "voteCount": 1,
        "content": "No, A is right as it may not be as effective for this specific task unless the ingredients and cookware are already well-represented within the types of entities the API is trained to recognize. This approach might require less initial setup but could be less accurate for specialized domains like recipes."
      },
      {
        "date": "2024-01-15T04:09:00.000Z",
        "voteCount": 1,
        "content": "A is the correct option here"
      },
      {
        "date": "2024-01-13T08:08:00.000Z",
        "voteCount": 1,
        "content": "Option B: Multi-label text classification is less suitable for identifying specific entities within text and would require labeling entire recipes with multiple classes, increasing complexity and reducing model specificity.\nOption C: Natural Language API's Entity Analysis might not be as accurate for this specialized domain as a model trained on custom recipe data.\nOption D: Creating separate entities for each ingredient and cookware type would significantly increase labeling effort and potentially hinder model generalization."
      },
      {
        "date": "2024-01-13T17:41:00.000Z",
        "voteCount": 2,
        "content": "do you mean Option A?"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 266,
    "url": "https://www.examtopics.com/discussions/google/view/131103-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You work for an organization that operates a streaming music service. You have a custom production model that is serving a \u201cnext song\u201d recommendation based on a user's recent listening history. Your model is deployed on a Vertex AI endpoint. You recently retrained the same model by using fresh data. The model received positive test results offline. You now want to test the new model in production while minimizing complexity. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new Vertex AI endpoint for the new model and deploy the new model to that new endpoint. Build a service to randomly send 5% of production traffic to the new endpoint. Monitor end-user metrics such as listening time. If end-user metrics improve between models over time, gradually increase the percentage of production traffic sent to the new endpoint.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCapture incoming prediction requests in BigQuery. Create an experiment in Vertex AI Experiments. Run batch predictions for both models using the captured data. Use the user\u2019s selected song to compare the models performance side by side. If the new model\u2019s performance metrics are better than the previous model, deploy the new model to production.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the new model to the existing Vertex AI endpoint. Use traffic splitting to send 5% of production traffic to the new model. Monitor end-user metrics, such as listening time. If end-user metrics improve between models over time, gradually increase the percentage of production traffic sent to the new model.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a model monitoring job for the existing Vertex AI endpoint. Configure the monitoring job to detect prediction drift and set a threshold for alerts. Update the model on the endpoint from the previous model to the new model. If you receive an alert of prediction drift, revert to the previous model."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-15T20:28:00.000Z",
        "voteCount": 2,
        "content": "For Simplicity: If speed and simplicity are your top priorities, deploying to the existing endpoint with caution (close monitoring during deployment) can work.--&gt; choose C\nFor Safety and Control: If minimizing risk and having better control over the testing process are more important, creating a new endpoint is the better option. This is generally the recommended approach for most production deployments. --&gt; choose A"
      },
      {
        "date": "2024-01-22T21:30:00.000Z",
        "voteCount": 3,
        "content": "Here's why the option C is preferable:\nMinimized complexity:\nLeverages existing endpoint: No need to create and manage a new endpoint, reducing setup and maintenance overhead.\nTraffic splitting readily available: Vertex AI provides built-in traffic splitting functionality, simplifying traffic distribution.\nEfficient testing and monitoring:\nDirect comparison: Sending a percentage of traffic to the new model allows for direct comparison with the current model's performance on real user data.\nGradual rollout: Starting with a small percentage mitigates potential risks and allows for gradual transition based on observed improvements.\nEnd-user metric monitoring: Focusing on metrics like listening time directly reflects user engagement and preference for the new recommendations."
      },
      {
        "date": "2024-01-22T07:20:00.000Z",
        "voteCount": 2,
        "content": "Traffic splitting is a feature of Vertex AI that allows you to distribute the prediction requests among multiple models or model versions within the same endpoint. You can specify the percentage of traffic that each model or model version receives, and change it at any time. Traffic splitting can help you test the new model in production without creating a new endpoint or a separate service. You can deploy the new model to the existing Vertex AI endpoint, and use traffic splitting to send 5% of production traffic to the new model. You can monitor the end-user metrics, such as listening time, to compare the performance of the new model and the previous model. If the end-user metrics improve between models over time, you can gradually increase the percentage of production traffic sent to the new model. This solution can help you test the new model in production while minimizing complexity and cost."
      },
      {
        "date": "2024-01-13T08:12:00.000Z",
        "voteCount": 2,
        "content": "Option A: Building a separate service adds unnecessary complexity and requires managing two endpoints.\nOption B: Batch predictions in Vertex AI Experiments might not reflect real-time user behavior and don't directly affect the production environment.\nOption D: Model monitoring alerts for prediction drift might be triggered by natural variations in user behavior instead of genuine performance issues and could lead to unnecessary model rollbacks."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 267,
    "url": "https://www.examtopics.com/discussions/google/view/131104-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You created a model that uses BigQuery ML to perform linear regression. You need to retrain the model on the cumulative data collected every week. You want to minimize the development effort and the scheduling cost. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse BigQuery\u2019s scheduling service to run the model retraining query periodically.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a pipeline in Vertex AI Pipelines that executes the retraining query, and use the Cloud Scheduler API to run the query weekly.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Scheduler to trigger a Cloud Function every week that runs the query for retraining the model.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the BigQuery API Connector and Cloud Scheduler to trigger Workflows every week that retrains the model."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-05T08:38:00.000Z",
        "voteCount": 1,
        "content": "A is right\nUsing BigQuery's scheduling service allows you to automate the retraining process without needing to write custom code or manage additional dependencies."
      },
      {
        "date": "2024-01-22T21:32:00.000Z",
        "voteCount": 4,
        "content": "No additional setup: BigQuery's scheduling feature is built-in, eliminating the need to create pipelines, functions, or workflows.\nStraightforward configuration: Setting up a schedule for a query is a simple process within the BigQuery interface."
      },
      {
        "date": "2024-01-22T07:24:00.000Z",
        "voteCount": 2,
        "content": "No-brainer A."
      },
      {
        "date": "2024-01-13T08:15:00.000Z",
        "voteCount": 2,
        "content": "Option B: Vertex AI Pipelines offer flexibility for complex workflows, but it involves more development effort and potential costs for pipeline execution.\nOption C: Cloud Functions provide a serverless way to execute code, but they incur execution costs and require additional configuration for triggering and permissions.\nOption D: Workflows can manage complex orchestration, but configuring the BigQuery API Connector and Cloud Scheduler adds complexity and potential costs."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 268,
    "url": "https://www.examtopics.com/discussions/google/view/131105-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You want to migrate a scikit-learn classifier model to TensorFlow. You plan to train the TensorFlow classifier model using the same training set that was used to train the scikit-learn model, and then compare the performances using a common test set. You want to use the Vertex AI Python SDK to manually log the evaluation metrics of each model and compare them based on their F1 scores and confusion matrices. How should you log the metrics?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the aiplatform.log_classification_metrics function to log the F1 score, and use the aiplatform.log_metrics function to log the confusion matrix.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the aiplatform.log_classification_metrics function to log the F1 score and the confusion matrix.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the aiplatform.log_metrics function to log the F1 score and the confusion matrix.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the aiplatform.log_metrics function to log the F1 score: and use the aiplatform.log_classification_metrics function to log the confusion matrix.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 10,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 8,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-10-12T16:22:00.000Z",
        "voteCount": 1,
        "content": "d"
      },
      {
        "date": "2024-06-04T13:07:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/vertex-ai/docs/experiments/log-data#classification-metrics\n\nlog_classification_metrics -&gt; only the confusion matrix, not the F1scores\nlog_metrics -&gt; any number you want -&gt; you can use it to store a F1 scores"
      },
      {
        "date": "2024-04-27T02:10:00.000Z",
        "voteCount": 3,
        "content": "aiplatform.log_classification_metrics is specifically designed for logging classification metrics, which includes F1 score and confusion matrix.\naiplatform.log_metrics is a more generic function for logging any kind of metric, but it wouldn't capture the rich structure of a confusion matrix.\nTherefore, using aiplatform.log_classification_metrics allows you to log both F1 score and confusion matrix in a single call, simplifying your code and ensuring proper handling of these classification-specific metrics."
      },
      {
        "date": "2024-04-28T08:22:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform#google_cloud_aiplatform_log_classification_metrics"
      },
      {
        "date": "2024-04-27T02:11:00.000Z",
        "voteCount": 1,
        "content": "While aiplatform.log_metrics can handle numeric values like F1 score, it wouldn't capture the complexity of a confusion matrix. Confusion matrix is a two-dimensional table and requires specific handling for proper logging.expand_more\naiplatform.log_classification_metrics is designed for classification tasks and understands the structure of both F1 score and confusion matrix, allowing them to be logged efficiently in a single function call."
      },
      {
        "date": "2024-04-27T02:11:00.000Z",
        "voteCount": 1,
        "content": "Therefore, using separate functions like log_metrics for F1 score and log_classification_metrics for confusion matrix would be inefficient and might not capture the matrix structure accurately."
      },
      {
        "date": "2024-08-11T07:59:00.000Z",
        "voteCount": 1,
        "content": "Hi fitri001. You are usually right but, I this particular case, I think D is the right answer.\n\nAs you can see here in the link I provide you below, it \"Currently support confusion matrix and ROC curve.\"\n\nLink: \n\nhttps://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform#google_cloud_aiplatform_log_classification_metrics"
      },
      {
        "date": "2024-04-19T02:51:00.000Z",
        "voteCount": 2,
        "content": "According to docs, log_classification_metrics supports confusion matrix and ROC curve. Not sure if it means that it only supports those... Assuming those are the only ones supported, I would got with D"
      },
      {
        "date": "2024-04-19T02:52:00.000Z",
        "voteCount": 2,
        "content": "forgot to add the link: https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform#google_cloud_aiplatform_log_classification_metrics"
      },
      {
        "date": "2024-04-13T00:38:00.000Z",
        "voteCount": 1,
        "content": "aiplatform.log_classification_metrics to log metrics relevant to classification tasks, including F1 score and confusion matrix."
      },
      {
        "date": "2024-04-28T08:23:00.000Z",
        "voteCount": 1,
        "content": "link??\n\ni find only: https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform#google_cloud_aiplatform_log_classification_metrics\n\nso D NOT B"
      },
      {
        "date": "2024-03-08T21:59:00.000Z",
        "voteCount": 1,
        "content": "The aiplatform.log_classification_metrics function is designed to log classification metrics, including the F1 score and the confusion matrix. It takes the following arguments:\n\npredictions: The predicted labels.\nlabels: The true labels.\nweight: The weight of each sample.\nlogger: The logger to use.\n----------------------------\nThe aiplatform.log_metrics function is designed to log general metrics, such as accuracy, loss, and precision. It takes the following arguments:\n\nmetric: The metric to log.\nvalue: The value of the metric.\nstep: The step at which the metric was logged.\nlogger: The logger to use."
      },
      {
        "date": "2024-01-22T21:37:00.000Z",
        "voteCount": 2,
        "content": "Actually, the F1 score is calculated by the Precision and recall metrics. The the log_classification_metrics is OK for both confusion matrix and F1 score"
      },
      {
        "date": "2024-01-22T07:34:00.000Z",
        "voteCount": 4,
        "content": "I go with D.\nlog_classification_metrics currently support confusion matrix and ROC curve. https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform#google_cloud_aiplatform_log_classification_metrics\nBecause it is not explicitly mentioned in the docs of log_classification_metrics, I assume F1 Score must be logged with log_metrics. https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform#google_cloud_aiplatform_log_metrics (if accuracy and recall are logged in the example, probably F1 is done the same way)"
      },
      {
        "date": "2024-01-13T08:17:00.000Z",
        "voteCount": 1,
        "content": "Option A: It's incorrect because aiplatform.log_metrics is a more general function that doesn't provide the same specialized structure for classification metrics.\nOption C: While technically possible to log both metrics using aiplatform.log_metrics, it's less optimal as it requires manual formatting and might not be as easily interpreted by Vertex AI's visualization tools.\nOption D: This is incorrect as it suggests using aiplatform.log_classification_metrics for the confusion matrix, but that function doesn't support logging confusion matrices directly."
      },
      {
        "date": "2024-01-22T07:32:00.000Z",
        "voteCount": 4,
        "content": "Option B also suggests sing aiplatform.log_classification_metrics for the confusion matrix. Which is supported, btw. https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform#google_cloud_aiplatform_log_classification_metrics"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 269,
    "url": "https://www.examtopics.com/discussions/google/view/131106-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You are developing a model to help your company create more targeted online advertising campaigns. You need to create a dataset that you will use to train the model. You want to avoid creating or reinforcing unfair bias in the model. What should you do? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInclude a comprehensive set of demographic features",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInclude only the demographic groups that most frequently interact with advertisements",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCollect a random sample of production traffic to build the training dataset",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCollect a stratified sample of production traffic to build the training dataset\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConduct fairness tests across sensitive categories and demographics on the trained model"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 15,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-09-14T18:09:00.000Z",
        "voteCount": 1,
        "content": "From my statistical point of view, D and E will mitigate the effect of bias."
      },
      {
        "date": "2024-07-05T11:23:00.000Z",
        "voteCount": 1,
        "content": "D and E is right answer, question asks us to select 2 right answers\n\u2022\tTo avoid creating or reinforcing unfair bias in the model, you should collect a representative and diverse dataset (option D) that includes a stratified sample of production traffic. This ensures that your training data is inclusive and accurately represents the diversity of your target audience.\n\u2022\tOnce you have collected your training dataset, you should conduct fairness tests across sensitive categories and demographics on the trained model (option E). This involves evaluating whether the model treats different demographic groups fairly and without bias. If biases are detected, you can take steps to mitigate them and ensure that your model is fair and accurate."
      },
      {
        "date": "2024-07-05T11:22:00.000Z",
        "voteCount": 1,
        "content": "D and E is right answer, question asks us to select 2 right answers\n\u2022\tTo avoid creating or reinforcing unfair bias in the model, you should collect a representative and diverse dataset (option D) that includes a stratified sample of production traffic. This ensures that your training data is inclusive and accurately represents the diversity of your target audience.\n\u2022\tOnce you have collected your training dataset, you should conduct fairness tests across sensitive categories and demographics on the trained model (option E). This involves evaluating whether the model treats different demographic groups fairly and without bias. If biases are detected, you can take steps to mitigate them and ensure that your model is fair and accurate."
      },
      {
        "date": "2024-07-02T12:10:00.000Z",
        "voteCount": 1,
        "content": "Agree with D and E"
      },
      {
        "date": "2024-04-13T00:41:00.000Z",
        "voteCount": 3,
        "content": "D. Stratified sampling to ensure the different demographic groups or categories are proportionally represented in the training data. This helps mitigate bias that might arise if certain groups are under-represented.\nE. Fairness tests can reveal disparities in how the model treats different populations, allowing you to identify and address potential biases."
      },
      {
        "date": "2024-04-06T09:25:00.000Z",
        "voteCount": 2,
        "content": "D and E is the two answers. Two selections are required"
      },
      {
        "date": "2024-04-08T23:51:00.000Z",
        "voteCount": 2,
        "content": "why not D and A?"
      },
      {
        "date": "2024-02-18T15:39:00.000Z",
        "voteCount": 2,
        "content": "I went D, E"
      },
      {
        "date": "2024-02-16T16:53:00.000Z",
        "voteCount": 1,
        "content": "DE\n\nD. Collect a stratified sample of production traffic to build the training dataset: This ensures that the training data represents the diverse demographics that will be targeted by the advertising campaigns. Random sampling might unintentionally underrepresent certain groups, leading to biased model outputs.\n\nE. Conduct fairness tests across sensitive categories and demographics on the trained model: This allows you to identify and address any potential biases that may have emerged during the training process. Evaluating the model's performance on different groups helps ensure fair and responsible deployment."
      },
      {
        "date": "2024-01-22T21:39:00.000Z",
        "voteCount": 1,
        "content": "I go for D &amp; E:\nA stratified sample ensures that the training data represents the distribution of the target population across relevant demographics or other sensitive categories. This helps mitigate bias arising from underrepresented groups in the data.\nRegularly testing the model for fairness across sensitive categories helps identify and address potential bias issues before deploying the model in production. This can involve metrics like precision, recall, and F1 score for different demographic groups."
      },
      {
        "date": "2024-01-22T07:37:00.000Z",
        "voteCount": 3,
        "content": "D E. ChatGPT explanation below (but I think makes quite a lot of sense)\n\nCollect a Stratified Sample (Option D): Stratified sampling involves dividing the population into subgroups (strata) and then randomly sampling from each subgroup. This ensures that the training dataset represents the diversity of the population, helping to avoid biases. By collecting a stratified sample of production traffic, you are more likely to have a balanced representation of different demographic groups, reducing the risk of biased model outcomes.\n\nConduct Fairness Tests (Option E): After training the model, it's crucial to conduct fairness tests to evaluate its performance across different sensitive categories and demographics. This involves measuring the model's predictions and outcomes for various groups to identify any disparities. Fairness tests help you assess and address biases that may have been inadvertently introduced during the training process."
      },
      {
        "date": "2024-01-18T04:17:00.000Z",
        "voteCount": 1,
        "content": "C, D - Conducting fairness tests across sensitive categories and demographics on the trained model is indeed important. However, this option focuses on post-training analysis rather than dataset creation. While it's a crucial step for ensuring fairness, it doesn't directly address how to create a training dataset to avoid bias.\nHence C,D"
      },
      {
        "date": "2024-02-12T20:11:00.000Z",
        "voteCount": 1,
        "content": "Check b1a8fae comment on why D is better than C!"
      },
      {
        "date": "2024-01-13T08:20:00.000Z",
        "voteCount": 1,
        "content": "D. Stratified Sampling: Randomly sampling your data might not accurately represent the diversity of your target audience, potentially introducing bias by over- or under-representing certain demographics. Stratified sampling ensures your training dataset reflects the distribution of sensitive features (e.g., age, gender, income) observed in your production traffic, helping mitigate bias during model training.\n\nE. Fairness Testing: Simply collecting unbiased data isn't enough. Regularly testing your trained model for fairness across sensitive categories is crucial. This involves measuring and analyzing metrics like accuracy, precision, recall, and F1 score for different demographic groups. Identifying disparities in performance can trigger further investigation and potential re-training to address bias."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 270,
    "url": "https://www.examtopics.com/discussions/google/view/131108-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You are developing an ML model in a Vertex AI Workbench notebook. You want to track artifacts and compare models during experimentation using different approaches. You need to rapidly and easily transition successful experiments to production as you iterate on your model implementation. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Initialize the Vertex SDK with the name of your experiment. Log parameters and metrics for each experiment, and attach dataset and model artifacts as inputs and outputs to each execution.<br>2. After a successful experiment create a Vertex AI pipeline.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Initialize the Vertex SDK with the name of your experiment. Log parameters and metrics for each experiment, save your dataset to a Cloud Storage bucket, and upload the models to Vertex AI Model Registry.<br>2. After a successful experiment, create a Vertex AI pipeline.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a Vertex AI pipeline with parameters you want to track as arguments to your PipelineJob. Use the Metrics, Model, and Dataset artifact types from the Kubeflow Pipelines DSL as the inputs and outputs of the components in your pipeline.<br>2. Associate the pipeline with your experiment when you submit the job.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a Vertex AI pipeline. Use the Dataset and Model artifact types from the Kubeflow Pipelines DSL as the inputs and outputs of the components in your pipeline.<br>2. In your training component, use the Vertex AI SDK to create an experiment run. Configure the log_params and log_metrics functions to track parameters and metrics of your experiment."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 13,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-07-05T11:28:00.000Z",
        "voteCount": 1,
        "content": "Option A correctly describes how to rapidly and easily transition successful experiments to production by initializing the Vertex SDK with the experiment name, logging parameters and metrics, and attaching dataset and model artifacts. The second step of creating a Vertex AI pipeline after a successful experiment allows for easy iteration on the model implementation while maintaining track of the experiment's performance."
      },
      {
        "date": "2024-06-04T13:25:00.000Z",
        "voteCount": 2,
        "content": "Answer B leverages more tools for responsability splitting : they are still tools for early experiments, but would help in the pipeline creation.\n\nC &amp; D are overkill"
      },
      {
        "date": "2024-02-16T17:01:00.000Z",
        "voteCount": 4,
        "content": "I agree with these comments\n\n&gt;&gt; I will go for A, because the requirement is \"rapidly and easily\" \n\n&gt;&gt; B: Manually saving datasets and models to Cloud Storage and Model Registry introduces extra steps and potential for inconsistencies.\n\n&gt;&gt; Options C and D: Prioritizing pipeline creation limits flexibility and visibility during the experimentation phase, making it harder to track artifacts and compare models effectively."
      },
      {
        "date": "2024-01-22T21:53:00.000Z",
        "voteCount": 3,
        "content": "I will go for A, because the requirement is \"rapidly and easily\" transition successful experiments to production. Option B,C,D are too complex to conduct."
      },
      {
        "date": "2024-01-22T07:41:00.000Z",
        "voteCount": 2,
        "content": "I believe is A for the same reasons that pikachu."
      },
      {
        "date": "2024-01-13T08:28:00.000Z",
        "voteCount": 4,
        "content": "Option B: Manually saving datasets and models to Cloud Storage and Model Registry introduces extra steps and potential for inconsistencies.\nOptions C and D: Prioritizing pipeline creation limits flexibility and visibility during the experimentation phase, making it harder to track artifacts and compare models effectively."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 271,
    "url": "https://www.examtopics.com/discussions/google/view/131110-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You recently created a new Google Cloud project. After testing that you can submit a Vertex AI Pipeline job from the Cloud Shell, you want to use a Vertex AI Workbench user-managed notebook instance to run your code from that instance. You created the instance and ran the code but this time the job fails with an insufficient permissions error. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure that the Workbench instance that you created is in the same region of the Vertex AI Pipelines resources you will use.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure that the Vertex AI Workbench instance is on the same subnetwork of the Vertex AI Pipeline resources that you will use.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure that the Vertex AI Workbench instance is assigned the Identity and Access Management (IAM) Vertex AI User role.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure that the Vertex AI Workbench instance is assigned the Identity and Access Management (IAM) Notebooks Runner role."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 12,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-06-04T12:32:00.000Z",
        "voteCount": 1,
        "content": "The job fails, not the access to notebook"
      },
      {
        "date": "2024-04-16T00:02:00.000Z",
        "voteCount": 3,
        "content": "Vertex AI has its own set of specific roles that control access to resources within the Vertex AI platform itself, such as datasets, models, and endpoints. The Vertex AI Notebook Runner falls under this category"
      },
      {
        "date": "2024-04-13T00:56:00.000Z",
        "voteCount": 3,
        "content": "The insufficient permissions error suggests your instance lacks the required authorization to access Vertex AI Pipelines resources."
      },
      {
        "date": "2024-03-08T07:02:00.000Z",
        "voteCount": 2,
        "content": "The question is asking 'submit a Vertex AI Pipeline job', so not just simply run notebooks on Vertex AI Workbench. The role required should be 'IAM Vertex AI User role'.\nSo it is C."
      },
      {
        "date": "2024-02-02T03:05:00.000Z",
        "voteCount": 1,
        "content": "I have done the test, it is D"
      },
      {
        "date": "2024-01-22T07:59:00.000Z",
        "voteCount": 2,
        "content": "I decided to change my mind to C after realizing we need the permissions aiplatform.pipelineJobs, present in vertex AI user. Not sure if the notebook runner role allows to run notebook from pipeline jobs + its specified that it only is allowed to run scheduled notebooks (no mention of scheduling here anywhere)"
      },
      {
        "date": "2024-01-22T07:49:00.000Z",
        "voteCount": 1,
        "content": "I say D.\nYou want to run the code, that's your purpose, and you have insufficient permissions, so all the permissions you need to solve this problem is: being able to run the notebook. Plus, what is a \"AI user role\"? It is not a predefined role according to the docs: https://cloud.google.com/vertex-ai/docs/workbench/user-managed/iam#iam_roles"
      },
      {
        "date": "2024-01-22T07:53:00.000Z",
        "voteCount": 1,
        "content": "Apparently \"Vertex AI user role\" is indeed a thing. I just did not see this link: https://cloud.google.com/vertex-ai/docs/general/access-control#predefined-roles. My point remains: not being able to run the code seems to be the inconvenient here."
      },
      {
        "date": "2024-01-13T08:33:00.000Z",
        "voteCount": 1,
        "content": "A. Region Compatibility: While regional compatibility is important, it's not the primary cause of this permission error.\nB. Subnet Matching: Subnet alignment is usually not a requirement for Vertex AI pipeline job submission.\nD. Notebooks Runner Role: This role is primarily for executing notebook code, not managing Vertex AI resources."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 272,
    "url": "https://www.examtopics.com/discussions/google/view/130559-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You work for a semiconductor manufacturing company. You need to create a real-time application that automates the quality control process. High-definition images of each semiconductor are taken at the end of the assembly line in real time. The photos are uploaded to a Cloud Storage bucket along with tabular data that includes each semiconductor\u2019s batch number, serial number, dimensions, and weight. You need to configure model training and serving while maximizing model accuracy. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Vertex AI Data Labeling Service to label the images, and tram an AutoML image classification model. Deploy the model, and configure Pub/Sub to publish a message when an image is categorized into the failing class.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Vertex AI Data Labeling Service to label the images, and train an AutoML image classification model. Schedule a daily batch prediction job that publishes a Pub/Sub message when the job completes.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConvert the images into an embedding representation. Import this data into BigQuery, and train a BigQuery ML K-means clustering model with two clusters. Deploy the model and configure Pub/Sub to publish a message when a semiconductor\u2019s data is categorized into the failing cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImport the tabular data into BigQuery, use Vertex AI Data Labeling Service to label the data and train an AutoML tabular classification model. Deploy the model, and configure Pub/Sub to publish a message when a semiconductor\u2019s data is categorized into the failing class."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 7,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-07-05T11:35:00.000Z",
        "voteCount": 1,
        "content": "Option A is correct\nThe high-definition images of each semiconductor are taken in real-time at the end of the assembly line.\nThe images are uploaded to Cloud Storage along with tabular data that includes batch number, serial number, dimensions, and weight.\nYou need to configure model training and serving while maximizing model accuracy."
      },
      {
        "date": "2024-04-13T00:59:00.000Z",
        "voteCount": 3,
        "content": "Real-time Processing, uploading images to Cloud Storage triggers the AutoML image classification model for immediate processing, enabling real-time quality control decisions.\nImage Classification, the scenario focuses on classifying images as \"passing\" or \"failing\" quality, making image classification the appropriate approach.\nPub/Sub Notifications, Pub/Sub messaging efficiently alerts downstream systems about failing classifications, allowing for prompt quality control actions."
      },
      {
        "date": "2024-01-22T08:02:00.000Z",
        "voteCount": 3,
        "content": "I go with A."
      },
      {
        "date": "2024-01-13T08:36:00.000Z",
        "voteCount": 1,
        "content": "Option B: Batch prediction jobs introduce latency, making them unsuitable for real-time quality control.\nOption C: K-means clustering is an unsupervised learning technique that doesn't leverage labeled data to distinguish between passing and failing semiconductors, potentially compromising accuracy.\nOption D: Tabular classification focuses on structured data, not images, and might overlook visual defects captured in the photos."
      },
      {
        "date": "2024-01-22T21:27:00.000Z",
        "voteCount": 1,
        "content": "I am afraid the option D is not correct, since this is a image classification task."
      },
      {
        "date": "2024-01-13T08:37:00.000Z",
        "voteCount": 3,
        "content": "The answer should be A*"
      },
      {
        "date": "2024-01-08T04:21:00.000Z",
        "voteCount": 1,
        "content": "The right answer should be A"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 273,
    "url": "https://www.examtopics.com/discussions/google/view/130558-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You work for a rapidly growing social media company. Your team builds TensorFlow recommender models in an on-premises CPU cluster. The data contains billions of historical user events and 100,000 categorical features. You notice that as the data increases, the model training time increases. You plan to move the models to Google Cloud. You want to use the most scalable approach that also minimizes training time. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the training jobs by using TPU VMs with TPUv3 Pod slices, and use the TPUEmbeading API\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the training jobs in an autoscaling Google Kubernetes Engine cluster with CPUs",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy a matrix factorization model training job by using BigQuery ML",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the training jobs by using Compute Engine instances with A100 GPUs, and use the tf.nn.embedding_lookup API"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-08T04:12:00.000Z",
        "voteCount": 6,
        "content": "TPU (Tensor Processing Units) VMs are specialized hardware accelerators designed by Google specifically for machine learning tasks.\nTPUv3 Pod slices offer high scalability and are excellent for distributed training tasks.\nThe TPUEmbedding API is optimized for handling large volumes of categorical features, which fits your scenario with 100,000 categorical features.\nThis option is likely to offer the fastest training times due to specialized hardware and optimized APIs for large-scale machine learning tasks."
      },
      {
        "date": "2024-04-13T01:01:00.000Z",
        "voteCount": 3,
        "content": "Addressing Bottleneck: As data size increases, CPU-based training becomes increasingly slow. TPUs are specifically designed to address this challenge, significantly accelerating training.\nLarge Categorical Features: TPUEmbedding API efficiently handles embedding lookups for a vast number of categorical features, a common characteristic of recommender system data."
      },
      {
        "date": "2024-03-05T20:46:00.000Z",
        "voteCount": 1,
        "content": "Option C"
      },
      {
        "date": "2024-02-19T06:53:00.000Z",
        "voteCount": 1,
        "content": "My Answer: \n\nA: most scalable approach that also minimizes training time: TPU using TPUEmbeading API\n\nhttps://www.tensorflow.org/api_docs/python/tf/tpu/experimental/embedding/TPUEmbedding"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 274,
    "url": "https://www.examtopics.com/discussions/google/view/131111-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You are training and deploying updated versions of a regression model with tabular data by using Vertex AI Pipelines, Vertex AI Training, Vertex AI Experiments, and Vertex AI Endpoints. The model is deployed in a Vertex AI endpoint, and your users call the model by using the Vertex AI endpoint. You want to receive an email when the feature data distribution changes significantly, so you can retrigger the training pipeline and deploy an updated version of your model. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Vertex Al Model Monitoring. Enable prediction drift monitoring on the endpoint, and specify a notification email.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn Cloud Logging, create a logs-based alert using the logs in the Vertex Al endpoint. Configure Cloud Logging to send an email when the alert is triggered.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn Cloud Monitoring create a logs-based metric and a threshold alert for the metric. Configure Cloud Monitoring to send an email when the alert is triggered.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExport the container logs of the endpoint to BigQuery. Create a Cloud Function to run a SQL query over the exported logs and send an email. Use Cloud Scheduler to trigger the Cloud Function."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-07T11:58:00.000Z",
        "voteCount": 2,
        "content": "I went with A"
      },
      {
        "date": "2024-01-22T21:24:00.000Z",
        "voteCount": 1,
        "content": "Vertex AI Model Monitoring is specifically designed for this purpose and provides out-of-the-box functionality for monitoring the data distribution of your model's predictions. It can automatically detect drift and trigger alerts based on predefined thresholds, making it the most efficient and straightforward solution.\n\nOption B,C and D are either over complex or too many manual operations."
      },
      {
        "date": "2024-01-22T08:09:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/blog/topics/developers-practitioners/monitor-models-training-serving-skew-vertex-ai"
      },
      {
        "date": "2024-01-14T13:40:00.000Z",
        "voteCount": 1,
        "content": "A\nPrediction drift is the change in the distribution of feature values or labels over time."
      },
      {
        "date": "2024-01-13T08:42:00.000Z",
        "voteCount": 1,
        "content": "Options B and C: While Cloud Logging and Cloud Monitoring can be used for general monitoring, they don't have the same specialized focus on prediction drift, potentially requiring more complex setup and analysis.\nOption D: Exporting logs to BigQuery and creating a Cloud Function for analysis can be time-consuming and less efficient compared to Vertex AI Model Monitoring's out-of-the-box capabilities."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 275,
    "url": "https://www.examtopics.com/discussions/google/view/131112-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You have trained an XGBoost model that you plan to deploy on Vertex AI for online prediction. You are now uploading your model to Vertex AI Model Registry, and you need to configure the explanation method that will serve online prediction requests to be returned with minimal latency. You also want to be alerted when feature attributions of the model meaningfully change over time. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Specify sampled Shapley as the explanation method with a path count of 5.<br>2. Deploy the model to Vertex AI Endpoints.<br>3. Create a Model Monitoring job that uses prediction drift as the monitoring objective.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Specify Integrated Gradients as the explanation method with a path count of 5.<br>2. Deploy the model to Vertex AI Endpoints.<br>3. Create a Model Monitoring job that uses prediction drift as the monitoring objective.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Specify sampled Shapley as the explanation method with a path count of 50.<br>2. Deploy the model to Vertex AI Endpoints.<br>3. Create a Model Monitoring job that uses training-serving skew as the monitoring objective.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Specify Integrated Gradients as the explanation method with a path count of 50.<br>2. Deploy the model to Vertex AI Endpoints.<br>3. Create a Model Monitoring job that uses training-serving skew as the monitoring objective."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-22T21:25:00.000Z",
        "voteCount": 2,
        "content": "Sampled Shapley is a method suitable for XGBoost models. A lower path count (like 5) would indeed ensure lower latency in explanations, but might compromise on the precision of the explanations.Model Monitoring - Prediction Drift: This monitors the change in model predictions over time, which can indirectly indicate a change in feature attributions, but it's not directly monitoring the attributions themselves."
      },
      {
        "date": "2024-01-15T04:29:00.000Z",
        "voteCount": 3,
        "content": "not B as integrated gradients is only for Custom-trained TensorFlow models that use a TensorFlow prebuilt container to serve predictions and AutoML image models"
      },
      {
        "date": "2024-01-15T04:30:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/vertex-ai/docs/explainable-ai/overview"
      },
      {
        "date": "2024-01-14T13:42:00.000Z",
        "voteCount": 4,
        "content": "A\nSampled Shapley is a fast and scalable approximation of the Shapley value, which is a game-theoretic concept that measures the contribution of each feature to the model prediction. Sampled Shapley is suitable for online prediction requests, as it can return feature attributions with minimal latency. The path count parameter controls the number of samples used to estimate the Shapley value, and a lower value means faster computation. Integrated Gradients is another explanation method that computes the average gradient along the path from a baseline input to the actual input. Integrated Gradients is more accurate than Sampled Shapley, but also more computationally intensive"
      },
      {
        "date": "2024-01-13T08:43:00.000Z",
        "voteCount": 3,
        "content": "Explanation Method:\n\nSampled Shapley: This method provides high-fidelity feature attributions while being computationally efficient, making it ideal for low-latency online predictions.\nIntegrated Gradients: While also accurate, it's generally more computationally intensive than sampled Shapley, potentially introducing latency.\nPath Count:\n\nLower Path Count (5): Reducing path count further decreases computation time, optimizing for faster prediction responses.\nMonitoring Objective:\n\nPrediction Drift: This type of monitoring detects changes in feature importance over time, aligning with the goal of tracking feature attribution shifts.\nTraining-Serving Skew: This monitors discrepancies between training and serving data distributions, which isn't directly related to feature attributions."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 276,
    "url": "https://www.examtopics.com/discussions/google/view/133595-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You work at a gaming startup that has several terabytes of structured data in Cloud Storage. This data includes gameplay time data, user metadata, and game metadata. You want to build a model that recommends new games to users that requires the least amount of coding. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLoad the data in BigQuery. Use BigQuery ML to train an Autoencoder model.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLoad the data in BigQuery. Use BigQuery ML to train a matrix factorization model.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRead data to a Vertex AI Workbench notebook. Use TensorFlow to train a two-tower model.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRead data to a Vertex AI Workbench notebook. Use TensorFlow to train a matrix factorization model."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 6,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-04-13T01:08:00.000Z",
        "voteCount": 1,
        "content": "Minimal Coding: BigQuery ML provides a user-friendly interface for training models, minimizing the need for extensive coding in tools like TensorFlow (C &amp; D)\nEfficient Data Processing: Training directly in BigQuery eliminates data movement and leverages BigQuery's scalable infrastructure."
      },
      {
        "date": "2024-04-13T01:09:00.000Z",
        "voteCount": 3,
        "content": "Matrix Factorization: This collaborative filtering technique is commonly used for recommender systems. BigQuery ML offers built-in support for matrix factorization, making it a good choice for your scenario."
      },
      {
        "date": "2024-04-17T00:56:00.000Z",
        "voteCount": 1,
        "content": "it means you choose B?"
      },
      {
        "date": "2024-04-20T04:59:00.000Z",
        "voteCount": 3,
        "content": "Yes, voted for A by mistake.\nThe answer is B"
      },
      {
        "date": "2024-02-20T05:21:00.000Z",
        "voteCount": 3,
        "content": "least amount of coding--&gt; BQML\nrecommendations--&gt; matrix factorization"
      },
      {
        "date": "2024-02-19T07:01:00.000Z",
        "voteCount": 1,
        "content": "Using BigQuery ML for training a matrix factorization model would require less coding compared to building a custom model with TensorFlow in a Vertex AI Workbench notebook. BigQuery ML provides high-level APIs for machine learning tasks directly within the BigQuery environment, thus reducing the amount of coding needed for data preprocessing and model training. Matrix factorization is a commonly used technique for recommendation systems, making it a suitable choice for recommending new games to users based on their gameplay time data, user metadata, and game metadata."
      },
      {
        "date": "2024-02-12T01:02:00.000Z",
        "voteCount": 2,
        "content": "B\n\nhttps://developers.google.com/machine-learning/recommendation/collaborative/matrix"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 277,
    "url": "https://www.examtopics.com/discussions/google/view/134187-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You work for a large bank that serves customers through an application hosted in Google Cloud that is running in the US and Singapore. You have developed a PyTorch model to classify transactions as potentially fraudulent or not. The model is a three-layer perceptron that uses both numerical and categorical features as input, and hashing happens within the model.<br><br>You deployed the model to the us-central1 region on nl-highcpu-16 machines, and predictions are served in real time. The model's current median response latency is 40 ms. You want to reduce latency, especially in Singapore, where some customers are experiencing the longest delays. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAttach an NVIDIA T4 GPU to the machines being used for online inference.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the machines being used for online inference to nl-highcpu-32.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the model to Vertex AI private endpoints in the us-central1 and asia-southeast1 regions, and allow the application to choose the appropriate endpoint.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate another Vertex AI endpoint in the asia-southeast1 region, and allow the application to choose the appropriate endpoint.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 8,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 7,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-02-19T07:05:00.000Z",
        "voteCount": 6,
        "content": "My Answer: C \n The bottleneck is network latency. So, \nA: Not Correct: might improve performance,&nbsp;but it's an expensive solution and may not be necessary if the bottleneck is network latency.\nB: Not Correct: might offer slight improvement,&nbsp;but the primary issue is geographical distance between users and the model.\nC: CORRECT: This approach leverages the geographical proximity of the endpoints to the users, reducing latency for customers in Singapore without neglecting customers in the US. Additionally, using Vertex AI private endpoints ensures secure and efficient communication between the application and the model.\nD: Not Correct: it's not the most efficient approach because it does not utilize the existing infrastructure in the us-central1 region, and managing multiple endpoints might introduce additional complexity."
      },
      {
        "date": "2024-03-30T11:48:00.000Z",
        "voteCount": 3,
        "content": "Deploying in additional regions (D) does not necessarily negate or underutilize existing deployments but rather complements them to provide a better global service."
      },
      {
        "date": "2024-09-14T16:00:00.000Z",
        "voteCount": 1,
        "content": "I don't have any link to support this other than a simple analysis; if you want the data or process to be low latency, you need to deploy closes where it is required, in this case, to Singapore customers, which reduces latetency addressing the requirement."
      },
      {
        "date": "2024-06-08T01:51:00.000Z",
        "voteCount": 2,
        "content": "I think it's D because C and D should work in the same way, but ensuring the connection through a private endpoint it's not necessary because in the question there's nothing about security or sensitive informations. So the scope for a generic endpoint is \"Accessible from anywhere\", the scope for a private endpoint is \"Accessible only within VPC or private connections\". Don't see why to do that, it's only a matter of latency, not a matter of safety."
      },
      {
        "date": "2024-05-18T11:06:00.000Z",
        "voteCount": 2,
        "content": "Not sure why I'd choose C over D, my choice is D.\nModel is already deployed to us-central1 so now it's only a matter of deploying it to asia-southeast1 and letting the app choose the closer endpoint.\nWhy the need for private endpoints and what will happen with the current already deployed model in us-central1?"
      },
      {
        "date": "2024-04-13T01:13:00.000Z",
        "voteCount": 1,
        "content": "Deploying the model to a Vertex AI private endpoint in the Singapore region brings the model closer to users in that region. This significantly reduces network latency for those users compared to accessing the model hosted in us-central1.\nAllowing the application to choose the appropriate endpoint based on user location (through private endpoints) ensures users access the geographically closest model replica, optimizing latency.\nWhy not D: creating a separate endpoint in Singapore would allow regional deployment, it wouldn't automatically route users to the closest endpoint. You still need additional logic within the application for regional routing, increasing complexity."
      },
      {
        "date": "2024-03-30T11:48:00.000Z",
        "voteCount": 2,
        "content": "By having an endpoint in the asia-southeast1 region (Singapore), the data doesn't have to travel as far, significantly reducing the round-trip time. Allowing the application to choose the appropriate endpoint based on the user's location ensures that requests are handled by the nearest available server, optimizing response times for users in different regions."
      },
      {
        "date": "2024-03-29T03:54:00.000Z",
        "voteCount": 1,
        "content": "I think it is D. C is questionable as why do you need a private endpoint?"
      },
      {
        "date": "2024-07-05T11:53:00.000Z",
        "voteCount": 1,
        "content": "Yes, using private endpoints does introduce some overhead.\nAdditional latency: Establishing a connection to a private endpoint may add some latency compared to using the public endpoint.\nIncreased complexity: Managing private endpoints requires additional configuration and management, which can increase the overall complexity of your deployment.\nHowever, in this scenario, the benefits of using private endpoints (security, control, and isolation) outweigh the potential overhead. The goal is to reduce latency for users in Singapore, and by deploying a private endpoint closer to them, you can achieve this while maintaining security and control over access to your model."
      },
      {
        "date": "2024-07-05T11:57:00.000Z",
        "voteCount": 1,
        "content": "I will go with C.\nIn this scenario, deploying the model to Vertex AI private endpoints in both us-central1 and asia-southeast1 regions is necessary because:\n\nThe application is hosted in Google Cloud and serves customers through APIs.\nBy using private endpoints, you can create a secure connection between your application and the Vertex AI endpoint without exposing the model or data to the public internet. This ensures that sensitive information remains within the cloud.\nPrivate endpoints provide an IP address that is unique to your project, making it easier to manage access control and network policies.\nWithout private endpoints, you would need to expose your model or data to the public internet, which increases the risk of unauthorized access and security breaches. Private endpoints provide a secure and controlled environment for hosting your model, ensuring that only authorized users can access it."
      },
      {
        "date": "2024-04-14T00:21:00.000Z",
        "voteCount": 1,
        "content": "see  guilhermebutzke"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 278,
    "url": "https://www.examtopics.com/discussions/google/view/134188-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You need to train an XGBoost model on a small dataset. Your training code requires custom dependencies. You want to minimize the startup time of your training job. How should you set up your Vertex AI custom training job?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the data in a Cloud Storage bucket, and create a custom container with your training application. In your training application, read the data from Cloud Storage and train the model.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the XGBoost prebuilt custom container. Create a Python source distribution that includes the data and installs the dependencies at runtime. In your training application, load the data into a pandas DataFrame and train the model.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a custom container that includes the data. In your training application, load the data into a pandas DataFrame and train the model.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the data in a Cloud Storage bucket, and use the XGBoost prebuilt custom container to run your training application. Create a Python source distribution that installs the dependencies at runtime. In your training application, read the data from Cloud Storage and train the model."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 11,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-02-19T07:10:00.000Z",
        "voteCount": 5,
        "content": "My Answer: A\n\nFocus on \u201ctraining code requires custom dependencies\u201d and \u201c minimize the startup time of your training job\u201d, the best choice is A because use custom container and read the data from GCS is he faster way"
      },
      {
        "date": "2024-10-14T10:16:00.000Z",
        "voteCount": 1,
        "content": "I select D: While A could work, D is the optimal solution because it balances efficiency, ease of setup, and performance. It minimizes startup time by leveraging Google\u2019s prebuilt XGBoost container and offers flexibility by installing custom dependencies at runtime. This approach avoids the overhead of building and maintaining a custom container from scratch, which is unnecessary for a small dataset with only specific custom dependency needs."
      },
      {
        "date": "2024-09-28T16:19:00.000Z",
        "voteCount": 1,
        "content": "The fastest way is to have most of the things already installed, so that is why option A fits the best"
      },
      {
        "date": "2024-06-20T04:49:00.000Z",
        "voteCount": 2,
        "content": "The focus is on startup time, and the dataset is small, so the container should still be of reasonable size.\nDownloading data from Cloud Storage introduces a delay."
      },
      {
        "date": "2024-06-04T12:17:00.000Z",
        "voteCount": 1,
        "content": "The dataset is small, xgboost is implemented in python... (correcting my error A answer)"
      },
      {
        "date": "2024-06-04T12:14:00.000Z",
        "voteCount": 1,
        "content": "The dataset is small, xgboost is implemented in python..."
      },
      {
        "date": "2024-04-13T01:32:00.000Z",
        "voteCount": 4,
        "content": "Given the focus on minimizing startup time, and based on the information about XGBoost prebuilt container dependencies available here https://cloud.google.com/vertex-ai/docs/training/pre-built-containers#xgboost\n\nA: Separate Data and Custom Container is the best approach for minimizing startup time, especially for small datasets. Separating data in Cloud Storage keeps the container image lean, leading to faster download and startup compared to bundling data within the container.\nB. The prebuilt Container could have unnecessary components, potentially increasing the image size and impacting startup time."
      },
      {
        "date": "2024-03-21T05:13:00.000Z",
        "voteCount": 1,
        "content": "Why not C?"
      },
      {
        "date": "2024-03-30T11:20:00.000Z",
        "voteCount": 3,
        "content": "Because, Including the data in the container image is not recommended as it increases the image size and makes it less reusable."
      },
      {
        "date": "2024-05-02T12:56:00.000Z",
        "voteCount": 1,
        "content": "But the description mentioned it is a small dataset and requires minimizing latency which makes C the best per requirement, there is no mentioning to make the container reusable whatsoever"
      },
      {
        "date": "2024-03-08T06:20:00.000Z",
        "voteCount": 2,
        "content": "B\n\nXGBoost prebuilt customer container already includes XGBoost library and all of its dependencies.\nPython source distribution to avoid overhead of reading the data from Cloud storage the 2nd time.\nLoad data to a Pandas DataFrame is convenient to work with Python. Pandas is for data analysis and manipulation."
      },
      {
        "date": "2024-03-30T11:24:00.000Z",
        "voteCount": 2,
        "content": "However, the question specifically says that the training code requires custom dependencies beyond those included in the prebuilt container. Therefore, using the prebuilt container alone would not be sufficient in this case.\n&amp;\nregarding the use of a Python source distribution to avoid reading data from Cloud Storage multiple times, it's important to consider the trade-off between startup time and potential performance gains. While including the data in the source distribution might save some time during training, it also increases the size of the container and can lead to longer startup times. For small datasets, the overhead of reading data from Cloud Storage is typically negligible compared to the benefits of a smaller container and faster startup."
      },
      {
        "date": "2024-03-30T11:25:00.000Z",
        "voteCount": 1,
        "content": "Also, creating a Python source distribution that includes the data and installs the dependencies at runtime can increase startup time since dependencies have to be installed every time the job runs"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 279,
    "url": "https://www.examtopics.com/discussions/google/view/134189-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You are creating an ML pipeline for data processing, model training, and model deployment that uses different Google Cloud services. You have developed code for each individual task, and you expect a high frequency of new files. You now need to create an orchestration layer on top of these tasks. You only want this orchestration pipeline to run if new files are present in your dataset in a Cloud Storage bucket. You also want to minimize the compute node costs. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a pipeline in Vertex AI Pipelines. Configure the first step to compare the contents of the bucket to the last time the pipeline was run. Use the scheduler API to run the pipeline periodically.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Cloud Function that uses a Cloud Storage trigger and deploys a Cloud Composer directed acyclic graph (DAG).",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a pipeline in Vertex AI Pipelines. Create a Cloud Function that uses a Cloud Storage trigger and deploys the pipeline.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy a Cloud Composer directed acyclic graph (DAG) with a GCSObjectUpdateSensor class that detects when a new file is added to the Cloud Storage bucket."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 7,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-10-14T10:25:00.000Z",
        "voteCount": 1,
        "content": "My answer is D: While C (Cloud Function + Vertex AI Pipelines) is a viable approach for triggering ML pipelines, D (Cloud Composer DAG with GCSObjectUpdateSensor) is the more appropriate and scalable solution when your orchestration spans multiple Google Cloud services and you want to minimize costs by only triggering the pipeline when new files appear."
      },
      {
        "date": "2024-08-11T10:33:00.000Z",
        "voteCount": 1,
        "content": "The key here is \"that uses different Google Cloud services\". Taking this into account, Cloud Composer is the correct answer (for instance, Vertex AI pipelines is not integrated with classic Dataproc or Cloud Composer DAGs). Moreover, GCSObjectUpdateSensor  is more efficient than a Cloud Function."
      },
      {
        "date": "2024-05-26T03:26:00.000Z",
        "voteCount": 4,
        "content": "Option C appears to be the best choice for balancing the requirements of efficient orchestration, cost minimization, and ensuring the pipeline only runs when new files are present. By using a Cloud Function triggered by Cloud Storage events to deploy a Vertex AI Pipeline, you can leverage the event-driven model of Cloud Functions to minimize unnecessary runs and associated costs, while still using the powerful orchestration capabilities of Vertex AI Pipelines."
      },
      {
        "date": "2024-05-26T03:28:00.000Z",
        "voteCount": 1,
        "content": "why not D?\nPros:\nCloud Composer provides a powerful orchestration framework that can handle complex dependencies and workflows.GCSObjectUpdateSensor can efficiently detect new files in the bucket and trigger the pipeline. \nCons:\nCloud Composer can be relatively costly due to the continuous operation of its environment. Overhead of maintaining Cloud Composer for potentially simple file-triggered tasks."
      },
      {
        "date": "2024-08-11T10:35:00.000Z",
        "voteCount": 1,
        "content": "I think we should use Cloud Composer here because of \"that uses different Google Cloud services\". Vertex AI is less integrated with the rest of services than Cloud Composer, which was designed exactly for that."
      },
      {
        "date": "2024-05-22T11:26:00.000Z",
        "voteCount": 1,
        "content": "\"Different Google Cloud services\" and GCSObjectUpdateSensor: This sensor class specifically checks for updates to Cloud Storage objects. This ensures the DAG only triggers when there's a new file in the bucket, minimizing unnecessary executions."
      },
      {
        "date": "2024-03-23T08:34:00.000Z",
        "voteCount": 1,
        "content": "Why not D?"
      },
      {
        "date": "2024-03-08T06:29:00.000Z",
        "voteCount": 3,
        "content": "C\n\nCloud Function to be triggered by Cloud storage trigger, and then deploy the Vertex AI pipeline."
      },
      {
        "date": "2024-03-05T21:25:00.000Z",
        "voteCount": 1,
        "content": "Its C. Vertex pipelines are recommened to run ML pipeline!"
      },
      {
        "date": "2024-02-19T07:15:00.000Z",
        "voteCount": 1,
        "content": "My Answer: B \n\nCloud Function that uses a Cloud Storage trigger (\u201drun if new files are present in your dataset in a Cloud Storage bucket\u201d) and Cloud Composer directed acyclic graph (DAG) (\u201dmodel deployment that uses different Google Cloud services\u201d, \u201dorchestration layer on top of these tasks\u201d,)"
      },
      {
        "date": "2024-02-22T10:48:00.000Z",
        "voteCount": 5,
        "content": "Cloud Composer already provides a way to orchestrate tasks, and creating a Cloud Function to deploy a DAG is not a common practice. The Cloud Function with a Cloud Storage trigger would be redundant since the GCSObjectUpdateSensor within the DAG itself can handle the file detection."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 280,
    "url": "https://www.examtopics.com/discussions/google/view/134190-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You are using Kubeflow Pipelines to develop an end-to-end PyTorch-based MLOps pipeline. The pipeline reads data from BigQuery, processes the data, conducts feature engineering, model training, model evaluation, and deploys the model as a binary file to Cloud Storage. You are writing code for several different versions of the feature engineering and model training steps, and running each new version in Vertex AI Pipelines. Each pipeline run is taking over an hour to complete. You want to speed up the pipeline execution to reduce your development time, and you want to avoid additional costs. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tComment out the part of the pipeline that you are not currently updating.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable caching in all the steps of the Kubeflow pipeline.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDelegate feature engineering to BigQuery and remove it from the pipeline.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a GPU to the model training step."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-03-08T06:46:00.000Z",
        "voteCount": 6,
        "content": "B\n'Different version of feature engineering and model training', so enable cache can help to reuse results of previous run.\nGuess not be C, as it mentioned 'end-to-end' MLOps, if delegate to BigQuery, it is not 'end-to-end' now."
      },
      {
        "date": "2024-04-13T01:49:00.000Z",
        "voteCount": 1,
        "content": "B, and here's why:\n1. Caching directly addresses the issue of redundant computations, especially for frequently used feature engineering versions\n2. End-to-End\" MLOps, Kubeflow Pipelines handle all stages, including feature engineering, maintaining your desired \"end-to-end\" workflow."
      },
      {
        "date": "2024-03-05T21:27:00.000Z",
        "voteCount": 1,
        "content": "Answer is  C"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 281,
    "url": "https://www.examtopics.com/discussions/google/view/134191-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You work at a large organization that recently decided to move their ML and data workloads to Google Cloud. The data engineering team has exported the structured data to a Cloud Storage bucket in Avro format. You need to propose a workflow that performs analytics, creates features, and hosts the features that your ML models use for online prediction. How should you configure the pipeline?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIngest the Avro files into Cloud Spanner to perform analytics. Use a Dataflow pipeline to create the features, and store them in Vertex AI Feature Store for online prediction.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIngest the Avro files into BigQuery to perform analytics. Use a Dataflow pipeline to create the features, and store them in Vertex AI Feature Store for online prediction.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIngest the Avro files into Cloud Spanner to perform analytics. Use a Dataflow pipeline to create the features, and store them in BigQuery for online prediction.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIngest the Avro files into BigQuery to perform analytics. Use BigQuery SQL to create features and store them in a separate BigQuery table for online prediction."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 9,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-02-19T07:25:00.000Z",
        "voteCount": 6,
        "content": "My Answer: B\n\n\u201cYou need to propose a workflow that performs analytics, creates features, and hosts \u201d: &nbsp;Ingest the Avro files into BigQuery to perform analytics\n\n\u201cworkflow that performs analytics, creates features\u201d: Dataflow pipeline to create the features\n\n\u201cand hosts the features that your ML models use for online prediction\u201d:store them in Vertex AI Feature Store for online prediction"
      },
      {
        "date": "2024-07-05T12:56:00.000Z",
        "voteCount": 1,
        "content": "B is right \nThe original audio recordings have an 8 kHz sample rate, which is sufficient for speech recognition.\n\nUsing the Speech-to-Text API with synchronous recognition would require your application to wait for the transcription process to complete before proceeding. This could lead to performance issues and delays in processing large volumes of audio data.\n\nAsynchronous recognition, on the other hand, allows your application to continue processing without waiting for the transcription process to complete. The transcribed text can be retrieved later when needed."
      },
      {
        "date": "2024-06-29T07:58:00.000Z",
        "voteCount": 1,
        "content": "\"performs analytics\" = Bigquery\n\"hosts the features\" = Vertex AI Feature Store\""
      },
      {
        "date": "2024-04-09T22:33:00.000Z",
        "voteCount": 2,
        "content": "Vertex AI Feature Store is designed for managing and serving features for online prediction with low latency."
      },
      {
        "date": "2024-04-06T10:39:00.000Z",
        "voteCount": 1,
        "content": "I think the answer is A because BigQuery does not support Avro format but CloudSpanner does."
      },
      {
        "date": "2024-04-14T06:18:00.000Z",
        "voteCount": 4,
        "content": "FYI BigQuery supports the Avro format. Please check your facts"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 282,
    "url": "https://www.examtopics.com/discussions/google/view/133596-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You work at an organization that maintains a cloud-based communication platform that integrates conventional chat, voice, and video conferencing into one platform. The audio recordings are stored in Cloud Storage. All recordings have an 8 kHz sample rate and are more than one minute long. You need to implement a new feature in the platform that will automatically transcribe voice call recordings into a text for future applications, such as call summarization and sentiment analysis. How should you implement the voice call transcription feature following Google-recommended best practices?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the original audio sampling rate, and transcribe the audio by using the Speech-to-Text API with synchronous recognition.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the original audio sampling rate, and transcribe the audio by using the Speech-to-Text API with asynchronous recognition.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpsample the audio recordings to 16 kHz, and transcribe the audio by using the Speech-to-Text API with synchronous recognition.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpsample the audio recordings to 16 kHz, and transcribe the audio by using the Speech-to-Text API with asynchronous recognition."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 21,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 12,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-03-21T04:16:00.000Z",
        "voteCount": 7,
        "content": "I went with D.\n\"following Google-recommended best practices\"\nhttps://cloud.google.com/speech-to-text/docs/optimizing-audio-files-for-speech-to-text#:~:text=We%20recommend%20a%20sample%20rate%20of%20at%20least%2016%20kHz%20in%20the%20audio%20files%20that%20you%20use%20for%20transcription%20with%20Speech%2Dto%2DText"
      },
      {
        "date": "2024-09-14T15:01:00.000Z",
        "voteCount": 2,
        "content": "Agree on B. If you read carefuly the documentation pointed will come to the conclusion that there is no need to upsample voice"
      },
      {
        "date": "2024-07-17T11:39:00.000Z",
        "voteCount": 4,
        "content": "We have longer than minute, 8KHz recordings.\n\nhttps://cloud.google.com/speech-to-text/docs/best-practices-provide-speech-data\n\"avoid re-sampling. For example, in telephony the native rate is commonly 8000 Hz, which is the rate that should be sent to the service.\"\n-&gt; 8KHz\nhttps://cloud.google.com/speech-to-text/docs/sync-recognize\n\"Synchronous speech recognition returns the recognized text for short audio (less than 60 seconds). To process a speech recognition request for audio longer than 60 seconds, use Asynchronous Speech Recognition.\"\n-&gt; asynchronous\n\nSo, the correct answer is B."
      },
      {
        "date": "2024-06-11T23:54:00.000Z",
        "voteCount": 3,
        "content": "B) Use original sampling rate and use asynchronous recognition...\n\"If possible, set the sampling rate of the audio source to 16000 Hz. Otherwise, set the sample_rate_hertz to match the native sample rate of the audio source (instead of re-sampling).\"\nhttps://cloud.google.com/speech-to-text/docs/best-practices-provide-speech-data#sampling_rate"
      },
      {
        "date": "2024-05-21T21:49:00.000Z",
        "voteCount": 2,
        "content": "According to google recommandation on Sampling rate: \"If possible, set the sampling rate of the audio source to 16000 Hz. Otherwise, set the&nbsp;sample_rate_hertz&nbsp;to match the native sample rate of the audio source (instead of re-sampling).\"\nSo we should match the native sample (8kHz) in the question."
      },
      {
        "date": "2024-04-21T04:35:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/speech-to-text/docs/best-practices-provide-speech-data: Capture audio with a sampling rate of 16,000 Hz or higher.\tLower sampling rates may reduce accuracy. However, avoid re-sampling. For example, in telephony the native rate is commonly 8000 Hz, which is the rate that should be sent to the service.\n\n\nhttps://cloud.google.com/speech-to-text/docs/optimizing-audio-files-for-speech-to-text#sample_rate_frequency_range: It's possible to convert from one sample rate to another. However, there's no benefit to up-sampling the audio, because the frequency range information is limited by the lower sample rate and can't be recovered by converting to a higher sample rate. \n\n\n-----&gt; B, not D"
      },
      {
        "date": "2024-04-20T12:03:00.000Z",
        "voteCount": 2,
        "content": "According to the documentation, it's best to have 16 KHz sample rate, however one should avoid up-sampling and rather use the native sample rate"
      },
      {
        "date": "2024-04-15T09:58:00.000Z",
        "voteCount": 2,
        "content": "Following best practices, the easiest choice is B"
      },
      {
        "date": "2024-04-13T01:54:00.000Z",
        "voteCount": 1,
        "content": "Upsample to 16 kHz and Use Asynchronous Speech-to-Text Recognition"
      },
      {
        "date": "2024-03-30T10:42:00.000Z",
        "voteCount": 4,
        "content": "Upsampling to 16 kHz:\nThe Speech-to-Text API recommends an audio sample rate of 16 kHz for optimal transcription accuracy. Upsampling the 8 kHz recordings to 16 kHz will improve the quality of the transcription.\n\nAsynchronous Recognition:\nAsynchronous recognition is suitable for longer audio recordings (more than one minute). It allows you to submit the audio file and receive the transcription results later, which is more efficient for batch processing.\n\nhttps://cloud.google.com/speech-to-text/docs/best-practices-provide-speech-data"
      },
      {
        "date": "2024-02-19T07:30:00.000Z",
        "voteCount": 2,
        "content": "My Answer: B\n\n- Not necessary upsampling (exclude C and D)\n- Asynchronous means executing different tasks with no sequential order. Therefore, is preferred over synchronous recognition for longer audio recordings as it allows for more efficient processing, especially when dealing with larger volumes of data."
      },
      {
        "date": "2024-02-19T07:30:00.000Z",
        "voteCount": 1,
        "content": "My Answer: B\n\n- Not necessary upsampling (exclude C and D)\n- Asynchronous means executing different tasks with no sequential order. Therefore, is preferred over synchronous recognition for longer audio recordings as it allows for more efficient processing, especially when dealing with larger volumes of data."
      },
      {
        "date": "2024-02-12T01:24:00.000Z",
        "voteCount": 3,
        "content": "B\n\nhttps://cloud.google.com/speech-to-text/docs/speech-to-text-requests#:~:text=Synchronous%20recognition%20requests%20are%20limited,periodically%20poll%20for%20recognition%20results."
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 283,
    "url": "https://www.examtopics.com/discussions/google/view/134192-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You work for a multinational organization that has recently begun operations in Spain. Teams within your organization will need to work with various Spanish documents, such as business, legal, and financial documents. You want to use machine learning to help your organization get accurate translations quickly and with the least effort. Your organization does not require domain-specific terms or jargon. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Vertex AI Workbench notebook instance. In the notebook, extract sentences from the documents, and train a custom AutoML text model.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Google Translate to translate 1,000 phrases from Spanish to English. Using these translated pairs, train a custom AutoML Translation model.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Document Translation feature of the Cloud Translation API to translate the documents.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Vertex AI Workbench notebook instance. In the notebook, convert the Spanish documents into plain text, and create a custom TensorFlow seq2seq translation model."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-06-29T08:05:00.000Z",
        "voteCount": 1,
        "content": "\"translations quickly and with the least effort\" =  Cloud Translation API"
      },
      {
        "date": "2024-04-16T01:22:00.000Z",
        "voteCount": 1,
        "content": "Cloud Translation API - Document Translation: This pre-built service is specifically designed for translating large volumes of documents while preserving the document structure and formatting. It supports various languages, including Spanish, and offers high accuracy for general-purpose translations without domain-specific requirements.\nLeast Effort: Cloud Translation API requires minimal setup. You can directly submit your Spanish documents to the API and receive translated versions in English. There's no need for custom model training or data preparation."
      },
      {
        "date": "2024-04-13T01:56:00.000Z",
        "voteCount": 1,
        "content": "Leverage Document Translation in Cloud Translation API"
      },
      {
        "date": "2024-02-19T07:34:00.000Z",
        "voteCount": 4,
        "content": "My Answer: C\n\nThis option provides a straightforward solution for translating various types of documents (business, legal, financial) quickly and with minimal effort. It leverages Google's Cloud Translation API, which is designed specifically for tasks like this and eliminates the need for manual training or customization.\n\nhttps://cloud.google.com/translate/docs"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 284,
    "url": "https://www.examtopics.com/discussions/google/view/134193-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You have a custom job that runs on Vertex AI on a weekly basis. The job is implemented using a proprietary ML workflow that produces the datasets, models, and custom artifacts, and sends them to a Cloud Storage bucket. Many different versions of the datasets and models were created. Due to compliance requirements, your company needs to track which model was used for making a particular prediction, and needs access to the artifacts for each model. How should you configure your workflows to meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Vertex AI Metadata API inside the custom job to create context, execution, and artifacts for each model, and use events to link them together.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Vertex AI experiment, and enable autologging inside the custom job.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a TensorFlow Extended (TFX) ML Metadata database, and use the ML Metadata API.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRegister each model in Vertex AI Model Registry, and use model labels to store the related dataset and model information."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-13T01:57:00.000Z",
        "voteCount": 3,
        "content": "Track Lineage with Vertex AI Metadata API"
      },
      {
        "date": "2024-04-09T22:47:00.000Z",
        "voteCount": 2,
        "content": "A -  Vertex AI Metadata API provides low-level primitives for creating custom metadata entities and relationships (contexts, executions, artifacts, and events).\n\nB - Autologging  might not capture all the custom artifacts your job produces."
      },
      {
        "date": "2024-02-19T07:41:00.000Z",
        "voteCount": 4,
        "content": "My Answer: A\n\nFocus on \u201cDue to compliance requirements, your company needs to track which model was used for making a particular prediction\u201d and \u201cworkflow that produces the datasets, models, and custom artifacts, and sends them to a Cloud Storage bucket\u201d, use Vertex AI Metadata API is the best approach."
      },
      {
        "date": "2024-04-17T23:16:00.000Z",
        "voteCount": 1,
        "content": "where you find the question? do you pass the exam?"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 285,
    "url": "https://www.examtopics.com/discussions/google/view/134194-exam-professional-machine-learning-engineer-topic-1-question/",
    "body": "You have recently developed a custom model for image classification by using a neural network. You need to automatically identify the values for learning rate, number of layers, and kernel size. To do this, you plan to run multiple jobs in parallel to identify the parameters that optimize performance. You want to minimize custom code development and infrastructure management. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTrain an AutoML image classification model.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a custom training job that uses the Vertex AI Vizier SDK for parameter optimization.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Vertex AI hyperparameter tuning job.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Vertex AI pipeline that runs different model training jobs in parallel."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 13,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-19T07:43:00.000Z",
        "voteCount": 8,
        "content": "My Answer: C\n\nVertex AI provides a service for hyperparameter tuning which allows you to specify the hyperparameters you want to optimize, such as learning rate, number of layers, and kernel size, and then it automatically runs multiple training jobs with different combinations of these hyperparameters to find the configuration that maximizes performance."
      },
      {
        "date": "2024-06-29T08:11:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/vertex-ai/docs/training/using-hyperparameter-tuning"
      },
      {
        "date": "2024-04-13T01:58:00.000Z",
        "voteCount": 2,
        "content": "Leverage Vertex AI Hyperparameter Tuning"
      },
      {
        "date": "2024-04-28T08:36:00.000Z",
        "voteCount": 1,
        "content": "why not b?"
      },
      {
        "date": "2024-08-12T01:46:00.000Z",
        "voteCount": 1,
        "content": "Additional code is required to use the Vertex AI Vizier SDK."
      },
      {
        "date": "2024-03-22T08:18:00.000Z",
        "voteCount": 1,
        "content": "True that"
      }
    ],
    "examNameCode": "professional-machine-learning-engineer",
    "topicNumber": "1"
  }
]