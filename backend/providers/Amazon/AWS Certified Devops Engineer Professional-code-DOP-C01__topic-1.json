[
  {
    "topic": 1,
    "index": 1,
    "url": "https://www.examtopics.com/discussions/amazon/view/48835-exam-aws-devops-engineer-professional-topic-1-question-1/",
    "body": "A company wants to migrate its content sharing web application hosted on Amazon EC2 to a serverless architecture. The company currently deploys changes to its application by creating a new Auto Scaling group of EC2 instances and a new Elastic Load Balancer, and then shifting the traffic away using an Amazon Route<br>53 weighted routing policy.<br>For its new serverless application, the company is planning to use Amazon API Gateway and AWS Lambda. The company will need to update its deployment processes to work with the new application. It will also need to retain the ability to test new features on a small number of users before rolling the features out to the entire user base.<br>Which deployment strategy will meet these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS CDK to deploy API Gateway and Lambda functions. When code needs to be changed, update the AWS CloudFormation stack and deploy the new version of the APIs and Lambda functions. Use a Route 53 failover routing policy for the canary release strategy.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS CloudFormation to deploy API Gateway and Lambda functions using Lambda function versions. When code needs to be changed, update the CloudFormation stack with the new Lambda code and update the API versions using a canary release strategy. Promote the new version when testing is complete.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Elastic Beanstalk to deploy API Gateway and Lambda functions. When code needs to be changed, deploy a new version of the API and Lambda functions. Shift traffic gradually using an Elastic Beanstalk blue/green deployment.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS OpsWorks to deploy API Gateway in the service layer and Lambda functions in a custom layer. When code needs to be changed, use OpsWorks to perform a blue/green deployment and shift traffic gradually."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 3,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-10-18T23:07:00.000Z",
        "voteCount": 12,
        "content": "B\nnoting SAM is built over CloudFormation."
      },
      {
        "date": "2024-09-06T00:46:00.000Z",
        "voteCount": 9,
        "content": "B - Canary Deployment"
      },
      {
        "date": "2024-10-05T04:11:00.000Z",
        "voteCount": 1,
        "content": "B IS HIGHLY CORRECT"
      },
      {
        "date": "2023-07-15T03:14:00.000Z",
        "voteCount": 5,
        "content": "B is the correct answer\n\nThis is a Legit dumps I passed my Exam on June 15th, 2023 with a 870 score. I studied both DOP-C01 and DOP-C02, 95% of the questions came from them. Most questions came from DOP-C01 with 80% and DOP-C02 with 15%. Get contributor access to read all comments and maybe access other exams if you plan to take more exams. Go through the questions at least twice or more to get familiar with it!"
      },
      {
        "date": "2023-05-10T20:30:00.000Z",
        "voteCount": 2,
        "content": "anyone took the exam lately? is this still valid or should I go ahead with C02?"
      },
      {
        "date": "2023-01-30T13:24:00.000Z",
        "voteCount": 1,
        "content": "B - Canary Deployment"
      },
      {
        "date": "2022-12-24T22:32:00.000Z",
        "voteCount": 1,
        "content": "B- Canary deployment of Lambda is done using AWS SAM that comes built in with CodeDeploy. https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/automating-updates-to-serverless-apps.html"
      },
      {
        "date": "2022-10-16T23:46:00.000Z",
        "voteCount": 2,
        "content": "Canary Deployment is the use case for this scenario while the other two are Blue green deployment. Option A is using Route53 failover which is not necessary for the scenario."
      },
      {
        "date": "2022-09-06T21:42:00.000Z",
        "voteCount": 2,
        "content": "New as of September 6th, 2022\nNO.265 A company updated the AWS CloudFormation template tor a critical business application. The stack update process Tailed due to an error in me updated template, and CloudFormation automatically began the stack rollback process Later, a DevOps engineer found the application was still unavailable, and that the stack was in the UPDATE_ROLLBACK_FALED state Which combination of actions will allow the stack rollback to complete successful/? (Select 2)\nA. Attach the AWSCloudFormationFulAccess IAM policy to the CloudFormation role \nB. Automatically heal the stack resources using CloudFormation drift detection.\nC. Issue a ContinueUpdateRolback command from the CloudFormation console or AWS CLI\nD. Manually the resources to match the expectations of the stack.\nE. Update the existing CloudFormation stack using the original template"
      },
      {
        "date": "2024-04-30T01:32:00.000Z",
        "voteCount": 1,
        "content": "ANS: C, D check topic https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-continueupdaterollback.html"
      },
      {
        "date": "2022-09-24T16:28:00.000Z",
        "voteCount": 2,
        "content": "D AND E"
      },
      {
        "date": "2022-09-27T10:14:00.000Z",
        "voteCount": 1,
        "content": "what about the other ones new? thanks"
      },
      {
        "date": "2022-11-07T18:08:00.000Z",
        "voteCount": 1,
        "content": "Passed with a new question?"
      },
      {
        "date": "2022-10-07T22:59:00.000Z",
        "voteCount": 1,
        "content": "it came up in my exam"
      },
      {
        "date": "2022-10-10T03:31:00.000Z",
        "voteCount": 2,
        "content": "Ans: CD\nhttps://aws.amazon.com/tw/premiumsupport/knowledge-center/cloudformation-update-rollback-failed/"
      },
      {
        "date": "2022-12-07T19:32:00.000Z",
        "voteCount": 1,
        "content": "Agree CD\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/troubleshooting.html#troubleshooting-errors-update-rollback-failed"
      },
      {
        "date": "2022-09-06T21:41:00.000Z",
        "voteCount": 1,
        "content": "New as of September 6th, 2022\nA company's application is running on Amazon EC2 instances in an Auto Scaling group. A DevOps engineer needs to ensure there are at least four application servers running at all times. Whenever an update has to be made to the application, the engineer creates a new AMI with the updated configuration and updates the AWS CloudFormation template with the new AMI ID. After the stack update finishes, the engineer manually terminates the old instances one by one. verifying that the new instance is operational before proceeding. The engineer needs to automate this process.\nWhich action will allow for the LEAST number of manual steps moving forward?\nA. Update the CloudFormation template to include the UpdatePolicy attribute with the AutoScalingRollingUpdate policy.\nB. Update the CloudFormation template to include the UpdatePolicy attribute with the AutoScalingReplacingUpdate policy."
      },
      {
        "date": "2022-09-06T21:41:00.000Z",
        "voteCount": 1,
        "content": "C. Use an Auto Scaling lifecycle hook to verify that the previous instance is operational before allowing the DevOps engineer's selected instance to terminate.\nD. Use an Auto Scaling lifecycle hook to confirm there are at least four running instances before allowing the DevOps engineer's selected instance to terminate."
      },
      {
        "date": "2022-10-10T03:36:00.000Z",
        "voteCount": 4,
        "content": "Ans: A"
      },
      {
        "date": "2022-09-06T21:40:00.000Z",
        "voteCount": 1,
        "content": "New as of September 6th, 2022\nA company needs to implement a robust CI/CD pipeline to automate the deployment of an application in AWS. The pipeline must support continuous integration, continuous delivery, and automatic rollback upon deployment failure. The entire CI/CD pipeline must be capable of being re- provisioned in alternate AWS accounts or Regions within minutes. A DevOps engineer has already created an AWS CodeCommit repository to store the source code.\nWhich combination of actions should be taken when building this pipeline to meet these requirements? (Select THREE.)\nA. Configure an AWS CodePipehne pipeline with a build stage using AWS CodeBuild. \nB. Copy the build artifact from CodeCommit to Amazon S3.\nC. Create an Auto Scaling group of Amazon EC2 instances behind an Application Load Balancer (ALB) and set the ALB as the deployment target in AWS CodePipeline.\nD. Create an AWS Elastic Beanstalk environment as the deployment target in AWS CodePipeline.\nE. Implement an Amazon SQS queue to decouple the pipeline components. \nF. Provision all resources using AWS CloudFormation."
      },
      {
        "date": "2022-10-25T08:31:00.000Z",
        "voteCount": 1,
        "content": "Ans: ADE"
      },
      {
        "date": "2022-10-25T08:32:00.000Z",
        "voteCount": 6,
        "content": "sr, I think ADF"
      },
      {
        "date": "2022-12-07T20:25:00.000Z",
        "voteCount": 3,
        "content": "The pipeline must support continuous integration, continuous delivery, and automatic rollback upon deployment failure. \n=&gt; A\nThe entire CI/CD pipeline must be capable of being re- provisioned in alternate AWS accounts or Regions within minutes. \n=&gt; DF"
      },
      {
        "date": "2022-09-06T21:38:00.000Z",
        "voteCount": 2,
        "content": "New as of September 6th, 2022\nA company runs several applications across multiple AWS accounts in an organization in AWS Organizations.\nSome of the resources are not tagged properly, and the company's finance team cannot determine which costs are associated with which applications. A DevOps engineer must remediate this issue and prevent this issue from happening in the future.\nWhich combination of actions should the DevOps engineer take to meet these requirements? (Select TWO.)\n\nA. Activate the user-defined cost allocation tags in each AWS account. \nB. Create and attach an SCP that requires a specific tag.\nC. Define each line of business (LOB) in AWS Budgets. Assign the required tag to each resource. \nD. Scan all accounts with Tag Editor. Assign the required tag to each resource.\nE. Use the budget report to find untagged resources. Assign the required tag to each resource."
      },
      {
        "date": "2022-10-10T03:38:00.000Z",
        "voteCount": 4,
        "content": "Ans: BD"
      },
      {
        "date": "2022-12-07T20:26:00.000Z",
        "voteCount": 1,
        "content": "remediate this issue\n=&gt; D\nprevent this issue from happening in the future.\n=&gt; B"
      },
      {
        "date": "2022-09-06T21:37:00.000Z",
        "voteCount": 1,
        "content": "New as of September 6th, 2022\nA company is using AWS Organizations and wants to implement a governance strategy with the following requirements:\n\n- AWS resource access is restricted to the same two Regions for all accounts.\n- AWS services are limited to a specific group of authorized services for all accounts.\n- Authentication is provided by Active Directory.\n- Access permissions are organized by job function and are identical in each account. Which solution will meet these requirements?\n\nA. Establish an organizational unit (OU) with group policies in the master account to restrict Regions and authorized services. Use AWS Cloud Formation StackSets to provision roles with permissions for each job function, including an IAM trust policy for IAM identity provider authentication in each account."
      },
      {
        "date": "2022-09-06T21:37:00.000Z",
        "voteCount": 1,
        "content": "B. Establish a permission boundary in the master account to restrict Regions and authorized services. Use AWS CloudFormation StackSet to provision roles with permissions for each job function, including an IAM trust policy for IAM identity provider authentication in each account.\nC. Establish a service control policy in the master account to restrict Regions and authorized services. Use AWS Resource Access Manager to share master account roles with permissions for each job function, including AWS SSO for authentication in each account.\nC. Establish a service control policy in the master account to restrict Regions and authorized services. Use CloudFormation StackSet to provision roles with permissions for each job function, including an IAM trust policy for IAM identity provider authentication in each account."
      },
      {
        "date": "2022-10-10T03:40:00.000Z",
        "voteCount": 3,
        "content": "Ans: D"
      },
      {
        "date": "2022-12-07T20:33:00.000Z",
        "voteCount": 1,
        "content": "Agree D"
      },
      {
        "date": "2023-12-16T22:32:00.000Z",
        "voteCount": 1,
        "content": "C has to be the answer , since D does not support use of Active Directory , but C has AWS SSO for authencation which leverage Active Directory for authentication . While both RAM and StackSet can be used to share role and permessions"
      },
      {
        "date": "2024-04-30T02:00:00.000Z",
        "voteCount": 1,
        "content": "RAM to share resource not policy i think"
      },
      {
        "date": "2022-09-06T21:34:00.000Z",
        "voteCount": 1,
        "content": "NEw Q as of September 6th, 2022\nA DevOps engineer is currently running a container-based workload on-premises The engineer wants to move the application to AWS, but needs to keep the on-premises solution active because not all APIs will move at the same time. The traffic between AWS and the on-premises network should be secure and encrypted at all times. Low management overload is also a requirement.\n\nWhich combination of actions will meet these criteria? (Select THREE.)\n\nCreate a Network Load Balancer and. for each service, create a listener that points to the correct set of containers either in AWS or on-premises.\n\nCreate an Application Load Balancer and, for each service, create a listener that points to the correct set of containers either in AWS or on-premises."
      },
      {
        "date": "2022-09-06T21:35:00.000Z",
        "voteCount": 1,
        "content": "A. Create a Network Load Balancer and. for each service, create a listener that points to the correct set of containers either in AWS or on-premises.\nB . Create an Application Load Balancer and, for each service, create a listener that points to the correct set of containers either in AWS or on-premises.\nC. Host the AWS containers in Amazon ECS with an EC2 launch type. (D). Host the AWS containers in Amazon ECS with a Fargate launch type\nD. Use Amazon API Gateway to front the workload, and create a VPC link so API Gateway can forward API calls to the on-premises network through a VPN connection.\nE. Use Amazon API Gateway to front the workload, and set up public endpoints for the on-premises APIs so API Gateway can access them."
      },
      {
        "date": "2022-10-25T08:48:00.000Z",
        "voteCount": 5,
        "content": "B:  Create an Application Load Balancer and, for each service, create a listener that points to the correct set of containers either in AWS or on-premises.\nD:  Host the AWS containers in Amazon ECS with a Fargate launch type\nE. Use Amazon API Gateway to front the workload, and create a VPC link so API Gateway can forward API calls to the on-premises network through a VPN connection."
      },
      {
        "date": "2022-12-07T20:39:00.000Z",
        "voteCount": 1,
        "content": "Agree BDE"
      },
      {
        "date": "2022-11-18T21:47:00.000Z",
        "voteCount": 1,
        "content": "It says three, but what is the correct answer, ABC?"
      },
      {
        "date": "2022-08-30T19:55:00.000Z",
        "voteCount": 3,
        "content": "B make sense"
      },
      {
        "date": "2021-10-05T05:12:00.000Z",
        "voteCount": 1,
        "content": "B ............."
      },
      {
        "date": "2021-09-22T06:36:00.000Z",
        "voteCount": 2,
        "content": "I'll go with B"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 2,
    "url": "https://www.examtopics.com/discussions/amazon/view/28037-exam-aws-devops-engineer-professional-topic-1-question-2/",
    "body": "A company's application is currently deployed to a single AWS Region. Recently, the company opened a new office on a different continent. The users in the new office are experiencing high latency. The company's application runs on Amazon EC2 instances behind an Application Load Balancer (ALB) and uses Amazon<br>DynamoDB as the database layer. The instances run in an EC2 Auto Scaling group across multiple Availability Zones. A DevOps Engineer is tasked with minimizing application response times and improving availability for users in both Regions.<br>Which combination of actions should be taken to address the latency issues? (Choose three.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new DynamoDB table in the new Region with cross-Region replication enabled.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate new ALB and Auto Scaling group global resources and configure the new ALB to direct traffic to the new Auto Scaling group.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate new ALB and Auto Scaling group resources in the new Region and configure the new ALB to direct traffic to the new Auto Scaling group.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate Amazon Route 53 records, health checks, and latency-based routing policies to route to the ALB.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate Amazon Route 53 aliases, health checks, and failover routing policies to route to the ALB.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConvert the DynamoDB table to a global table.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "CDF",
    "answerDescription": "",
    "votes": [
      {
        "answer": "CDF",
        "count": 6,
        "isMostVoted": true
      },
      {
        "answer": "CEF",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-09-27T08:06:00.000Z",
        "voteCount": 17,
        "content": "Answer: C,D,F\nCreating a new table does not solve the problem, but generates a new one to consolidate the data."
      },
      {
        "date": "2024-09-06T00:46:00.000Z",
        "voteCount": 9,
        "content": "CDF is the correct answer."
      },
      {
        "date": "2024-10-05T04:29:00.000Z",
        "voteCount": 1,
        "content": "CDF FOR SURE"
      },
      {
        "date": "2023-12-16T22:42:00.000Z",
        "voteCount": 1,
        "content": "CDF for sure \nDynamodb gloabal tabales\nLatecy based routing \nNew ALB and auto scalign gp"
      },
      {
        "date": "2023-07-27T12:03:00.000Z",
        "voteCount": 1,
        "content": "CDF for me."
      },
      {
        "date": "2023-05-09T11:48:00.000Z",
        "voteCount": 1,
        "content": "CDF is the correct answer."
      },
      {
        "date": "2023-02-26T04:39:00.000Z",
        "voteCount": 1,
        "content": "Agree with CDF"
      },
      {
        "date": "2022-12-24T22:35:00.000Z",
        "voteCount": 1,
        "content": "CDF is the correct answer."
      },
      {
        "date": "2022-11-15T23:14:00.000Z",
        "voteCount": 1,
        "content": "answer C D F"
      },
      {
        "date": "2022-11-01T16:53:00.000Z",
        "voteCount": 1,
        "content": "Perfect"
      },
      {
        "date": "2022-10-25T04:51:00.000Z",
        "voteCount": 1,
        "content": "Answer is \u201dC\u3001D\u3001F\u201d"
      },
      {
        "date": "2022-06-02T15:37:00.000Z",
        "voteCount": 1,
        "content": "Ans: C, D, F\nE is not correct. You need latency-based routing in this scenario and not fail-over routing"
      },
      {
        "date": "2022-04-06T20:09:00.000Z",
        "voteCount": 1,
        "content": "AGRRED CDF"
      },
      {
        "date": "2022-03-05T01:15:00.000Z",
        "voteCount": 1,
        "content": "for ELB, route53 must use alias record"
      },
      {
        "date": "2024-03-30T12:54:00.000Z",
        "voteCount": 1,
        "content": "True, but the trick is in the routing policy. Also, using the cname record in Route53 is not a deal breaker in this question, while latency is!"
      },
      {
        "date": "2022-02-11T01:27:00.000Z",
        "voteCount": 2,
        "content": "CDF, cross region replication is for S3"
      },
      {
        "date": "2021-10-26T04:05:00.000Z",
        "voteCount": 2,
        "content": "C, D, F"
      },
      {
        "date": "2021-10-20T03:33:00.000Z",
        "voteCount": 3,
        "content": "I'll go with C,D,F"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 3,
    "url": "https://www.examtopics.com/discussions/amazon/view/48069-exam-aws-devops-engineer-professional-topic-1-question-3/",
    "body": "A DevOps engineer used an AWS CloudFormation custom resource to set up AD Connector. The AWS Lambda function executed and created AD Connector, but<br>CloudFormation is not transitioning from CREATE_IN_PROGRESS to CREATE_COMPLETE.<br>Which action should the engineer take to resolve this issue?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure the Lambda function code has exited successfully.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure the Lambda function code returns a response to the pre-signed URL.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure the Lambda function IAM role has cloudformation:UpdateStack permissions for the stack ARN.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure the Lambda function IAM role has ds:ConnectDirectory permissions for the AWS account."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-30T11:05:00.000Z",
        "voteCount": 14,
        "content": "I'll go with B because it is right\n\nReference: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/crpg-ref-responses.html"
      },
      {
        "date": "2024-09-06T00:46:00.000Z",
        "voteCount": 9,
        "content": "Yes B is Correct"
      },
      {
        "date": "2024-01-10T20:04:00.000Z",
        "voteCount": 1,
        "content": "There is no mention of S3 so why are using pre signed url? Seems that D is a better option"
      },
      {
        "date": "2023-05-09T11:54:00.000Z",
        "voteCount": 1,
        "content": "Yes B is Correct"
      },
      {
        "date": "2023-03-09T15:08:00.000Z",
        "voteCount": 7,
        "content": "When AWS CloudFormation invokes a Lambda function using a custom resource, it provides a pre-signed URL as part of the request. The Lambda function must return a response to the pre-signed URL to signal the success or failure of the resource creation. If the Lambda function does not return a response to the pre-signed URL, the CloudFormation stack will remain in the CREATE_IN_PROGRESS state, waiting for a response."
      },
      {
        "date": "2023-03-04T05:41:00.000Z",
        "voteCount": 1,
        "content": "Yes B is Correct Custom Resources needs a response to provided through Lambda"
      },
      {
        "date": "2022-12-24T22:38:00.000Z",
        "voteCount": 1,
        "content": "Answer is B- Cloudformation expect the Lambda function invoked using a custom resource to call back at the signed URL"
      },
      {
        "date": "2022-09-05T01:29:00.000Z",
        "voteCount": 3,
        "content": "these passed  to the lambda as event and invoked : \n\n{\n    \"RequestType\" : \"Create\",\n    \"ResponseURL\" : \"http://pre-signed-S3-url-for-response\",\n    \"StackId\" : \"arn:aws:cloudformation:us-west-2:123456789012:stack/stack-name/guid\",\n    \"RequestId\" : \"unique id for this create request\",\n    \"ResourceType\" : \"Custom::TestResource\",\n    \"LogicalResourceId\" : \"MyTestResource\",\n    \"ResourceProperties\" : {\n       \"Name\" : \"Value\",\n       \"List\" : [ \"1\", \"2\", \"3\" ]\n    }\n }\n\nand the response has to have: event['ResponseURL']"
      },
      {
        "date": "2022-06-19T19:48:00.000Z",
        "voteCount": 1,
        "content": "B\nI thinks its B as Cloudformation stack needs to be updated of the status using helper scripts"
      },
      {
        "date": "2021-11-08T12:44:00.000Z",
        "voteCount": 1,
        "content": "I think its A it's not enough with code returns a response to the pre-signed URL it would need to be successfully"
      },
      {
        "date": "2021-11-02T14:57:00.000Z",
        "voteCount": 1,
        "content": "Option A is broad but could be the right answer. For option B, I'm skeptical as it says \"pre-signed URL\" (which is for S3) instead of callback URL or ResponseURL."
      },
      {
        "date": "2021-11-02T02:15:00.000Z",
        "voteCount": 3,
        "content": "The answer is B\nAs noted in the CloudFormation documentation, CloudFormation expects your Lambda function to callback to it once it has completed its operation; CloudFormation will pause execution until this callback is received. The event sent to your Lambda function by CloudFormation contains the callback URL (ResponseURL)"
      },
      {
        "date": "2021-10-25T06:39:00.000Z",
        "voteCount": 2,
        "content": "I'd say B, but the question is a bit light on details.\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-lambda-function-code-cfnresponsemodule.html"
      },
      {
        "date": "2021-09-19T16:30:00.000Z",
        "voteCount": 3,
        "content": "I will go with D"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 4,
    "url": "https://www.examtopics.com/discussions/amazon/view/28020-exam-aws-devops-engineer-professional-topic-1-question-4/",
    "body": "A company plans to stop using Amazon EC2 key pairs for SSH access, and instead plans to use AWS Systems Manager Session Manager. To further enhance security, access to Session Manager must take place over a private network only.<br>Which combinations of actions will accomplish this? (Choose two.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAllow inbound access to TCP port 22 in all associated EC2 security groups from the VPC CIDR range.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAttach an IAM policy with the necessary Systems Manager permissions to the existing IAM instance profile.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a VPC endpoint for Systems Manager in the desired Region.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy a new EC2 instance that will act as a bastion host to the rest of the EC2 instance fleet.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRemove any default routes in the associated route tables."
    ],
    "answer": "BC",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BC",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-19T12:12:00.000Z",
        "voteCount": 27,
        "content": "I choose B&amp;C.\nA - wrong. There is no need to open doors.\nB - Correct\nC - Correct\nD - System Manager does not need a bation host.\nIt's wrong.\n\nRef: https://aws.amazon.com/en/blogs/aws/new-session-manager/\nhttps://cloudonaut.io/goodbye-ssh-use-aws-session-manager-instead/"
      },
      {
        "date": "2024-09-06T00:46:00.000Z",
        "voteCount": 10,
        "content": "B,C is the right answer."
      },
      {
        "date": "2023-03-16T21:47:00.000Z",
        "voteCount": 2,
        "content": "To use AWS Systems Manager Session Manager, the first step is to attach an IAM policy with the necessary Systems Manager permissions to the IAM instance profile associated with the EC2 instances. This will grant the instances permission to use Systems Manager to start and stop sessions using Session Manager.\n\nNext, to ensure that access to Session Manager takes place over a private network only, a VPC endpoint for Systems Manager needs to be created in the desired VPC. This endpoint will allow the instances in the VPC to communicate with the Systems Manager service without needing to traverse the public internet.\n\nTherefore, options A, D, and E are incorrect because they are not relevant to the scenario."
      },
      {
        "date": "2023-02-08T16:05:00.000Z",
        "voteCount": 1,
        "content": "looks right"
      },
      {
        "date": "2022-12-24T22:43:00.000Z",
        "voteCount": 1,
        "content": "B,C is the right answer."
      },
      {
        "date": "2022-12-14T02:03:00.000Z",
        "voteCount": 1,
        "content": "B and C"
      },
      {
        "date": "2021-10-28T13:24:00.000Z",
        "voteCount": 2,
        "content": "b and c"
      },
      {
        "date": "2021-10-27T15:53:00.000Z",
        "voteCount": 5,
        "content": "Remaining Question # 58 (Topic 2)\nA. Update the attached IAM policies to allow access to the appropriate KMS key from the CodeDeploy role where the application will be deployed. \nB. Update the attached IAM policies to allow access to the appropriate KMS key from the EC2 instance roles where the application will be deployed. \nC. Update the CMK key policy to allow access to the appropriate KMS key from the CodeDeploy role where the application will be deployed. \nD. Update the CMK key policy to allow access to the appropriate KMS key from the EC2 instance roles where the application will be deployed.\n\nAnswer-A"
      },
      {
        "date": "2021-10-28T16:18:00.000Z",
        "voteCount": 2,
        "content": "Missing question.."
      },
      {
        "date": "2021-10-31T22:11:00.000Z",
        "voteCount": 1,
        "content": "I'll go with A"
      },
      {
        "date": "2021-10-24T06:30:00.000Z",
        "voteCount": 3,
        "content": "Question # 57 (Topic 2)\nA company uses AWS KMS with CMKs and manual key rotation to meet regulatory compliance requirements. The security team wants to be notified when any keys have not been rotated after 90 days. Which solution will accomplish this? \nA. Configure AWS KMS to publish to an Amazon SNS topic when keys are more than 90 days old.\nB. Configure an Amazon CloudWatch Events event to launch an AWS Lambda function to call the AWS Trusted Advisor API and publish to an Amazon SNS topic.\nC. Develop an AWS Config custom rule that publishes to an Amazon SNS topic when keys are more than 90 days old.\nD. Configure AWS Security Hub lo publish to an Amazon SNS topic when keys are more than 90 days old."
      },
      {
        "date": "2021-10-27T23:45:00.000Z",
        "voteCount": 5,
        "content": "C is the answer"
      },
      {
        "date": "2021-10-18T17:20:00.000Z",
        "voteCount": 2,
        "content": "Remaining Question # 56 (Topic 2)\nB. Establish a permission boundary in the master account to restrict Regions and authorized services. Use AWS CloudFormation StackSet to provision roles with permissions for each job function, including an IAM trust policy for IAM identity provider authentication in each account.\nC. Establish a service control policy in the master account to restrict Regions and authorized services. Use AWS Resource Access Manager to share master account roles with permissions for each job function, including AWS SSO for authentication in each account.\nD. Establish a service control policy in the master account to restrict Regions and authorized services. Use CloudFormation StackSet to provision roles with permissions for each job function, including an IAM trust policy for IAM identity provider authentication in each account."
      },
      {
        "date": "2021-10-18T23:57:00.000Z",
        "voteCount": 6,
        "content": "Well I would go with service control policy so C or D should be correct.\nC wants to use RAM to share roles between master and child accounts.\nFrom this link: https://docs.aws.amazon.com/ram/latest/userguide/shareable.html \nI would say this is not possible.\nSo I would go with D.\nAny thoughts?"
      },
      {
        "date": "2021-11-03T08:37:00.000Z",
        "voteCount": 2,
        "content": "I go with D too."
      },
      {
        "date": "2021-11-05T00:28:00.000Z",
        "voteCount": 4,
        "content": "D  is corrct"
      },
      {
        "date": "2021-10-18T07:05:00.000Z",
        "voteCount": 1,
        "content": "Question # 56 (Topic 2)\nA company is using AWS Organizations and wants to implement a governance strategy with the following requirements: AWS resource access is restricted to the same two Regions for all accounts. AWS services are limited to a specific group of authorized services for all accounts. Authentication is provided by Active Directory. Access permissions are organized by job function and are identical in each account. Which solution will meet these requirements?\nA. Establish an organizational unit (OU) with group policies in the master account to restrict Regions and authorized services. Use AWS CloudFormation StackSets to provision roles with permissions for each job function, including an IAM trust policy for IAM identity provider authentication in each account."
      },
      {
        "date": "2021-10-13T17:21:00.000Z",
        "voteCount": 3,
        "content": "Remaining Question # 55 (Topic 2)\nA. Create an AWS Organizations SCP that denies access to all non-global services in non-US Regions Attach the policy to the root of the organization.\nB. Configure AWS CloudTrail to send logs to Amazon CloudWatch Logs and enable it for all Regions Use a CloudWatch Logs metric filter to send an alert on any service activity in non-US Regions.\nC. Use an AWS Lambda function that checks for AWS service activity and deploy it to all Regions Write an Amazon CloudWatch Events rule that runs the Lambda function every hour, sending an alert if activity is found in a non-US Region.\nD. Use an AWS Lambda function to query Amazon Inspector to look for service activity in non-US Regions and send alerts if any activity is found.\nE. Write a SCP using the awsRequestedRegion condition key limiting access to US Regions Apply the policy to all users, groups, and roles."
      },
      {
        "date": "2021-10-31T07:17:00.000Z",
        "voteCount": 1,
        "content": "Answer please.."
      },
      {
        "date": "2021-11-02T03:06:00.000Z",
        "voteCount": 1,
        "content": "One option is C for sure. Anyone have idea about the other option?"
      },
      {
        "date": "2021-11-03T18:20:00.000Z",
        "voteCount": 4,
        "content": "I think: A B"
      },
      {
        "date": "2021-11-04T02:14:00.000Z",
        "voteCount": 1,
        "content": "B and C"
      },
      {
        "date": "2022-11-29T21:55:00.000Z",
        "voteCount": 1,
        "content": "this is super super WRONG"
      },
      {
        "date": "2022-10-25T08:57:00.000Z",
        "voteCount": 3,
        "content": "Ans: AB"
      },
      {
        "date": "2022-11-29T21:55:00.000Z",
        "voteCount": 1,
        "content": "this seems like the most likely answers."
      },
      {
        "date": "2021-10-13T05:48:00.000Z",
        "voteCount": 1,
        "content": "Question # 55 (Topic 2)\nA DevOps engineer is implementing governance controls for a company that requires its infrastructure to be housed within the United States. The engineer must restrict which Regions can be used. and ensure an alert is sent as soon as possible if any activity outside the governance policy takes place The controls should be automatically enabled on any new Region outside the United States. Which combination of actions will meet these requirements? (Select TWO)"
      },
      {
        "date": "2021-10-12T11:54:00.000Z",
        "voteCount": 1,
        "content": "B and C for me"
      },
      {
        "date": "2021-10-12T04:21:00.000Z",
        "voteCount": 1,
        "content": "BC. D is wrong, SM replaces Bastion"
      },
      {
        "date": "2021-10-09T05:57:00.000Z",
        "voteCount": 3,
        "content": "I'll go with B,C"
      },
      {
        "date": "2021-10-07T23:47:00.000Z",
        "voteCount": 2,
        "content": "B,C \nhttps://aws.amazon.com/premiumsupport/knowledge-center/ec2-systems-manager-vpc-endpoints/"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 5,
    "url": "https://www.examtopics.com/discussions/amazon/view/28128-exam-aws-devops-engineer-professional-topic-1-question-5/",
    "body": "A company runs an application with an Amazon EC2 and on-premises configuration. A DevOps Engineer needs to standardize patching across both environments. Company policy dictates that patching only happens during non-business hours.<br>Which combination of actions will meet these requirements? (Choose three.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd the physical machines into AWS Systems Manager using Systems Manager Hybrid Activations.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAttach an IAM role to the EC2 instances, allowing them to be managed by AWS Systems Manager.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate IAM access keys for the on-premises machines to interact with AWS Systems Manager.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExecute an AWS Systems Manager Automation document to patch the systems every hour.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon CloudWatch Events scheduled events to schedule a patch window.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Systems Manager Maintenance Windows to schedule a patch window.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "ABF",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ABF",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-11-06T22:00:00.000Z",
        "voteCount": 15,
        "content": "I'll go with A,B,F"
      },
      {
        "date": "2024-09-06T00:47:00.000Z",
        "voteCount": 9,
        "content": "A B F are my answers"
      },
      {
        "date": "2023-04-28T11:59:00.000Z",
        "voteCount": 1,
        "content": "ABF for this one"
      },
      {
        "date": "2022-12-24T22:48:00.000Z",
        "voteCount": 1,
        "content": "A,B and F is the right answer"
      },
      {
        "date": "2022-11-25T20:07:00.000Z",
        "voteCount": 1,
        "content": "A,B,F  https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-managed-instance-activation.html"
      },
      {
        "date": "2022-02-10T14:04:00.000Z",
        "voteCount": 1,
        "content": "While ABF is correct, where is Patch Manager?"
      },
      {
        "date": "2021-10-26T04:37:00.000Z",
        "voteCount": 2,
        "content": "A B F are my answers"
      },
      {
        "date": "2021-10-22T17:02:00.000Z",
        "voteCount": 2,
        "content": "Answer: ABF\nC - no, the best practice is to use IAM roles\nD - no, you need a maintenance window\nE- no, you don't need CW in this scenario"
      },
      {
        "date": "2021-10-19T14:39:00.000Z",
        "voteCount": 4,
        "content": "ABF\nB is required by SSM agent to perform patch actions"
      },
      {
        "date": "2023-02-13T12:19:00.000Z",
        "voteCount": 1,
        "content": "unless it is already installed, amazon Linux 2 has it already."
      },
      {
        "date": "2021-09-30T20:55:00.000Z",
        "voteCount": 2,
        "content": "my answer is FAB"
      },
      {
        "date": "2021-10-07T12:57:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-managedinstances.html"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 6,
    "url": "https://www.examtopics.com/discussions/amazon/view/78510-exam-aws-devops-engineer-professional-topic-1-question-6/",
    "body": "A company has many applications. Different teams in the company developed the applications by using multiple languages and frameworks. The applications run on premises and on different servers with different operating systems. Each team has its own release protocol and process. The company wants to reduce the complexity of the release and maintenance of these applications.<br>The company is migrating its technology stacks, including these applications, to AWS. The company wants centralized control of source code, a consistent and automatic delivery pipeline, and as few maintenance tasks as possible on the underlying infrastructure.<br>What should a DevOps engineer do to meet these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate one AWS CodeCommit repository for all applications. Put each application's code in different branch. Merge the branches, and use AWS CodeBuild to build the applications. Use AWS CodeDeploy to deploy the applications to one centralized application server.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate one AWS CodeCommit repository for each of the applications Use AWS CodeBuild to build the applications one at a time. Use AWS CodeDeploy to deploy the applications to one centralized application server.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate one AWS CodeCommit repository for each of the applications. Use AWS CodeBuild to build the applications one at a time to create one AMI for each server. Use AWS CloudFormation StackSets to automatically provision and decommission Amazon EC2 fleets by using these AMIs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate one AWS CodeCommit repository for each of the applications. Use AWS CodeBuild to build one Docker image for each application in Amazon Elastic Container Registry (Amazon ECR). Use AWS CodeDeploy to deploy the applications to Amazon Elastic Container Service (Amazon ECS) on infrastructure that AWS Fargate manages.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 25,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-14T18:47:00.000Z",
        "voteCount": 1,
        "content": "B is correct, D is over-engineered"
      },
      {
        "date": "2023-07-24T02:50:00.000Z",
        "voteCount": 1,
        "content": "B is wrong, D is correct imo"
      },
      {
        "date": "2023-07-24T02:19:00.000Z",
        "voteCount": 3,
        "content": "B says to deploy multiple application to a single Application server which is insane, if one server goes down all the applications go down."
      },
      {
        "date": "2023-06-19T08:44:00.000Z",
        "voteCount": 1,
        "content": "D is more logical"
      },
      {
        "date": "2023-05-22T22:25:00.000Z",
        "voteCount": 1,
        "content": "D, because of \"as few maintenance tasks as possible on the underlying infrastructure\". Fargate does that better than \"one centralized application server\""
      },
      {
        "date": "2023-05-12T23:24:00.000Z",
        "voteCount": 1,
        "content": "D makes more sense"
      },
      {
        "date": "2023-03-19T15:47:00.000Z",
        "voteCount": 2,
        "content": "D is the answer"
      },
      {
        "date": "2023-03-16T21:54:00.000Z",
        "voteCount": 1,
        "content": "Option D is the best solution for meeting the company's requirements. By creating a separate AWS CodeCommit repository for each application, the company can have centralized control of source code for each application. AWS CodeBuild can be used to build a Docker image for each application, which can then be stored in Amazon ECR. AWS CodeDeploy can be used to deploy the applications to Amazon ECS, which will be managed by AWS Fargate. This provides a consistent and automatic delivery pipeline for each application, and reduces maintenance tasks on the underlying infrastructure. Option A and B do not address the requirement of having a consistent and automatic delivery pipeline, while Option C requires manual management of Amazon EC2 fleets using StackSets."
      },
      {
        "date": "2023-03-12T20:22:00.000Z",
        "voteCount": 1,
        "content": "Answer is D:\nB is wrong because of this: Use AWS CodeBuild to build the applications one at a time.\nbecause we can do it parallel way for saving time."
      },
      {
        "date": "2023-03-10T13:36:00.000Z",
        "voteCount": 1,
        "content": "multiple applications in a single application server will be an over head to maintain and deploy changes. I will go with D"
      },
      {
        "date": "2023-03-01T07:27:00.000Z",
        "voteCount": 1,
        "content": "Why is B? If you want to deploy on different, should docker be a better choice?"
      },
      {
        "date": "2023-02-14T07:10:00.000Z",
        "voteCount": 1,
        "content": "Why is B suggested as \"correct answer\" ?"
      },
      {
        "date": "2023-01-30T14:12:00.000Z",
        "voteCount": 2,
        "content": "D will be the answer as we need Docker in this scenario."
      },
      {
        "date": "2023-01-03T08:30:00.000Z",
        "voteCount": 3,
        "content": "There is a requirement for \"as few maintenance tasks as possible on the underlying infrastructure.\". Fargate is serverless. This is the best answer."
      },
      {
        "date": "2023-02-03T04:38:00.000Z",
        "voteCount": 1,
        "content": "I thought D is the solution. However, the revealed solution is B"
      },
      {
        "date": "2022-12-24T22:53:00.000Z",
        "voteCount": 1,
        "content": "Answer is D."
      },
      {
        "date": "2022-12-16T04:39:00.000Z",
        "voteCount": 1,
        "content": "iS B ,  you need docker for all different OS and Fargate toreduce the management of ifrastructure"
      },
      {
        "date": "2022-12-03T04:27:00.000Z",
        "voteCount": 2,
        "content": "B does not look correct. B states CodeDeploy to one application server. Requirements are to move multiple application types running on multiple instance types. D is best answer."
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 7,
    "url": "https://www.examtopics.com/discussions/amazon/view/78521-exam-aws-devops-engineer-professional-topic-1-question-7/",
    "body": "A DevOps engineer is developing an application for a company. The application needs to persist files to Amazon S3. The application needs to upload files with different security classifications that the company defines. These classifications include confidential, private, and public. Files that have a confidential classification must not be viewable by anyone other than the user who uploaded them. The application uses the IAM role of the user to call the S3 API operations.<br>The DevOps engineer has modified the application to add a DataClassification tag with the value of confidential and an Owner tag with the uploading user's ID to each confidential object that is uploaded to Amazon S3.<br>Which set of additional steps must the DevOps engineer take to meet the company's requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the S3 bucket's ACL to grant bucket-owner-read access to the uploading user's IAM role. Create an IAM policy that grants s3:GetObject operations on the S3 bucket when aws:ResourceTag/DataClassification equals confidential, and s3:ExistingObjectTag/Owner equals ${aws:userid}. Attach the policy to the IAM roles for users who require access to the S3 bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the S3 bucket policy to allow the s3:GetObject action when aws:ResourceTag/DataClassification equals confidential, and s3:ExistingObjectTag/Owner equals ${aws:userid}. Create an IAM policy that grants s3:GetObject operations on the S3 bucket. Attach the policy to the IAM roles for users who require access to the S3 bucket.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the S3 bucket policy to allow the s3:GetObject action when aws:ResourceTag/DataClassification equals confidential, and aws:RequesttTag/Owner equals ${aws:userid}. Create an IAM policy that grants s3:GetObject operations on the S3 bucket. Attach the policy to the IAM roles for users who require access to the S3 bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the S3 bucket's ACL to grant authenticated-read access when aws:ResourceTag/DataClassification equals confidential, and s3:ExistingObjectTag/Owner equals ${aws:userid}. Create an IAM policy that grants s3:GetObject operations on the S3 bucket. Attach the policy to the IAM roles for users who require access to the S3 bucket."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-08-31T04:54:00.000Z",
        "voteCount": 8,
        "content": "B - https://docs.aws.amazon.com/AmazonS3/latest/userguide/tagging-and-policies.html"
      },
      {
        "date": "2024-01-27T04:46:00.000Z",
        "voteCount": 1,
        "content": "None\n\nYou can rule out B and C momentarily because GetObject on IAM permission will give it permission to all IAM roles with attached policy regardless of bucket policy, having no deny statements.\n\nYou can rule out A and D because ACLs can be conditionally applied or be applied to specific principals"
      },
      {
        "date": "2023-11-19T06:09:00.000Z",
        "voteCount": 1,
        "content": "B is the answer\n\nTo meet the company\u2019s requirements, the DevOps engineer must modify the S3 bucket policy to allow the s3:GetObject action when aws:ResourceTag/DataClassification equals confidential, and s3:ExistingObjectTag/Owner equals ${aws:userid}. The engineer must also create an IAM policy that grants s3:GetObject operations on the S3 bucket. The policy should be attached to the IAM roles for users who require access to the S3 bucket"
      },
      {
        "date": "2023-03-19T15:53:00.000Z",
        "voteCount": 1,
        "content": "B is the answer"
      },
      {
        "date": "2023-03-01T07:32:00.000Z",
        "voteCount": 1,
        "content": "B sounds great."
      },
      {
        "date": "2022-12-25T22:49:00.000Z",
        "voteCount": 1,
        "content": "B. Based on the link given by ohcn"
      },
      {
        "date": "2022-10-09T23:37:00.000Z",
        "voteCount": 1,
        "content": "good enough"
      },
      {
        "date": "2022-09-05T21:21:00.000Z",
        "voteCount": 1,
        "content": "B sound correct"
      },
      {
        "date": "2022-08-30T16:22:00.000Z",
        "voteCount": 1,
        "content": "B\nSounds most doable"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 8,
    "url": "https://www.examtopics.com/discussions/amazon/view/18123-exam-aws-devops-engineer-professional-topic-1-question-8/",
    "body": "A company has developed an AWS Lambda function that handles orders received through an API. The company is using AWS CodeDeploy to deploy the Lambda function as the final stage of a CI/CD pipeline.<br>A DevOps Engineer has noticed there are intermittent failures of the ordering API for a few seconds after deployment. After some investigation, the DevOps<br>Engineer believes the failures are due to database changes not having fully propagated before the Lambda function begins executing.<br>How should the DevOps Engineer overcome this?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a BeforeAllowTraffic hook to the AppSpec file that tests and waits for any necessary database changes before traffic can flow to the new version of the Lambda function\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd an AfterAllowTraffic hook to the AppSpec file that forces traffic to wait for any pending database changes before allowing the new version of the Lambda function to respond",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a BeforeInstall hook to the AppSpec file that tests and waits for any necessary database changes before deploying the new version of the Lambda function",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a ValidateService hook to the AppSpec file that inspects incoming traffic and rejects the payload if dependent services, such as the database, are not yet ready"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-19T17:03:00.000Z",
        "voteCount": 14,
        "content": "should be A\nhttps://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html#appspec-hooks-lambda"
      },
      {
        "date": "2021-10-19T06:09:00.000Z",
        "voteCount": 9,
        "content": "A - Correct\nB - Does not fit into the frame of the question. We want the database to be up and running BEFORE we start serving production traffic.\nC - Not applicable in the appspec.yml file for Lambda\nD - Same explanation as C"
      },
      {
        "date": "2023-11-19T06:14:00.000Z",
        "voteCount": 2,
        "content": "To overcome the issue of intermittent failures of the ordering API for a few seconds after deployment, the DevOps Engineer should add a BeforeAllowTraffic hook to the AppSpec file that tests and waits for any necessary database changes before traffic can flow to the new version of the Lambda function 1. This will ensure that the database changes have fully propagated before the Lambda function begins executing.\n\nSo the answer is A"
      },
      {
        "date": "2023-03-19T15:56:00.000Z",
        "voteCount": 1,
        "content": "A is the answer"
      },
      {
        "date": "2022-12-31T22:03:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2022-09-05T21:23:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2021-11-04T01:37:00.000Z",
        "voteCount": 6,
        "content": "AWS Lambda deployment allows only 2 hooks i.e. BeforeAllowTraffic  &amp; AfterAllowTraffic . So options C and D are ruled out."
      },
      {
        "date": "2021-10-22T16:17:00.000Z",
        "voteCount": 4,
        "content": "I'll go with A"
      },
      {
        "date": "2021-11-06T23:42:00.000Z",
        "voteCount": 1,
        "content": "I see what you\u2019re doing there kid"
      },
      {
        "date": "2021-10-11T08:24:00.000Z",
        "voteCount": 1,
        "content": "A is correct. Other options is not for Lambda"
      },
      {
        "date": "2021-10-05T04:12:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2021-10-02T01:28:00.000Z",
        "voteCount": 1,
        "content": "A is correct."
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 9,
    "url": "https://www.examtopics.com/discussions/amazon/view/48819-exam-aws-devops-engineer-professional-topic-1-question-9/",
    "body": "A software company wants to automate the build process for a project where the code is stored in GitHub. When the repository is updated, source code should be compiled, tested, and pushed to Amazon S3.<br>Which combination of steps would address these requirements? (Choose three.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a buildspec.yml file to the source code with build instructions.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a GitHub webhook to trigger a build every time a code change is pushed to the repository.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS CodeBuild project with GitHub as the source repository.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS CodeDeploy application with the Amazon EC2/On-Premises compute platform.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS OpsWorks deployment with the install dependencies command.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tProvision an Amazon EC2 instance to perform the build."
    ],
    "answer": "ABC",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ABC",
        "count": 5,
        "isMostVoted": true
      },
      {
        "answer": "AB",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-10-30T18:48:00.000Z",
        "voteCount": 15,
        "content": "I'll go with A, B, C\n\nReference: \nhttps://docs.aws.amazon.com/codebuild/latest/userguide/github-webhook.html"
      },
      {
        "date": "2021-11-01T23:06:00.000Z",
        "voteCount": 6,
        "content": "ABC is the correct answer.\nhttps://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html\nhttps://docs.aws.amazon.com/codebuild/latest/userguide/sample-github-pull-request.html"
      },
      {
        "date": "2023-11-19T06:18:00.000Z",
        "voteCount": 1,
        "content": "To automate the build process for a project where the code is stored in GitHub, the following steps can be taken:\n\nA Add a buildspec.yml file to the source code with build instructions \nB Configure a GitHub webhook to trigger a build every time a code change is pushed to the repository \nC Create an AWS CodeBuild project with GitHub as the source repository \n\nThese steps will ensure that the source code is compiled, tested, and pushed to Amazon S3 when the repository is updated.\n\nhttps://docs.github.com/en/actions/automating-builds-and-tests"
      },
      {
        "date": "2023-04-26T07:47:00.000Z",
        "voteCount": 1,
        "content": "Https credentials can be used to connect through 3rd party code repositories like github, bitbucket and gitlab https://docs.aws.amazon.com/codecommit/latest/userguide/setting-up-git-remote-codecommit.html"
      },
      {
        "date": "2023-03-19T15:57:00.000Z",
        "voteCount": 1,
        "content": "ABC works"
      },
      {
        "date": "2022-12-31T22:06:00.000Z",
        "voteCount": 2,
        "content": "A,B,C is the right answer"
      },
      {
        "date": "2022-12-14T01:34:00.000Z",
        "voteCount": 1,
        "content": "i'll go with this"
      },
      {
        "date": "2022-11-25T00:15:00.000Z",
        "voteCount": 2,
        "content": "I'll go with A, B, C"
      },
      {
        "date": "2022-11-12T03:47:00.000Z",
        "voteCount": 1,
        "content": "I will go for ABC"
      },
      {
        "date": "2022-11-01T23:41:00.000Z",
        "voteCount": 1,
        "content": "ABC for me"
      },
      {
        "date": "2022-09-11T01:07:00.000Z",
        "voteCount": 3,
        "content": "With a webhook in place, each time a Git user pushes a commit, your repository is automatically retrieved, zipped, and uploaded to an Amazon Simple Storage System (Amazon S3) bucket. You can then configure AWS services such as AWS CodePipeline, AWS CodeBuild, and AWS CodeDeploy to use the S3 bucket as a source.\nhttps://aws.amazon.com/quickstart/architecture/git-to-s3-using-webhooks/"
      },
      {
        "date": "2022-09-08T18:35:00.000Z",
        "voteCount": 3,
        "content": "ABC - although the 3 ans address only the CI part. The other options while looking like they address deployments, actually dont fit in to this scenario."
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 10,
    "url": "https://www.examtopics.com/discussions/amazon/view/7681-exam-aws-devops-engineer-professional-topic-1-question-10/",
    "body": "An online retail company based in the United States plans to expand its operations to Europe and Asia in the next six months. Its product currently runs on<br>Amazon EC2 instances behind an Application Load Balancer. The instances run in an Amazon EC2 Auto Scaling group across multiple Availability Zones. All data is stored in an Amazon Aurora database instance.<br>When the product is deployed in multiple regions, the company wants a single product catalog across all regions, but for compliance purposes, its customer information and purchases must be kept in each region.<br>How should the company meet these requirements with the LEAST amount of application changes?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Redshift for the product catalog and Amazon DynamoDB tables for the customer information and purchases.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon DynamoDB global tables for the product catalog and regional tables for the customer information and purchases.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Aurora with read replicas for the product catalog and additional local Aurora instances in each region for the customer information and purchases.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Aurora for the product catalog and Amazon DynamoDB global tables for the customer information and purchases."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-21T01:15:00.000Z",
        "voteCount": 19,
        "content": "since its minimal changes, stick with Aurora"
      },
      {
        "date": "2021-09-25T07:27:00.000Z",
        "voteCount": 13,
        "content": "minimal changes, so change to dynamodb may need schema change, C makes more sense."
      },
      {
        "date": "2024-09-27T13:47:00.000Z",
        "voteCount": 1,
        "content": "C, Single Product Catalog: Using Aurora with read replicas allows you to maintain a single source of truth for your product catalog that can be accessed across multiple regions. Read replicas can help distribute the read load and provide faster access to the catalog.\n\nRegional Customer Information: Setting up additional local Aurora instances in each region for customer information and purchases ensures that sensitive data remains compliant with local regulations while allowing the application to scale regionally.\n\nThis approach minimizes application changes because you can leverage the existing Aurora database structure and simply configure the necessary replicas and local instances."
      },
      {
        "date": "2023-11-19T06:22:00.000Z",
        "voteCount": 1,
        "content": "I will go with C\n\nTo meet the company\u2019s requirements with the least amount of application changes, the company should use Aurora with read replicas for the product catalog and additional local Aurora instances in each region for the customer information and purchases 1. This approach will ensure that the company has a single product catalog across all regions while keeping customer information and purchases in each region for compliance purposes.\n\nOption A is incorrect because Amazon Redshift is not designed for storing product catalogs.\n\nOption B is incorrect because Amazon DynamoDB global tables are not designed for storing product catalogs.\n\nOption D is incorrect because Amazon DynamoDB global tables are not designed for storing customer information and purchases.\n\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Concepts.DBInstanceClass.html"
      },
      {
        "date": "2023-03-19T16:01:00.000Z",
        "voteCount": 1,
        "content": "C because LEAST amount of changes.  DB is already aurora."
      },
      {
        "date": "2022-12-31T22:08:00.000Z",
        "voteCount": 2,
        "content": "Due to least amount of changes needed, the answer is C."
      },
      {
        "date": "2022-09-08T18:36:00.000Z",
        "voteCount": 2,
        "content": "c for me"
      },
      {
        "date": "2022-04-05T06:45:00.000Z",
        "voteCount": 1,
        "content": "C. Use Aurora with read replicas for the product catalog and additional local Aurora instances in each region for the customer information and purchases."
      },
      {
        "date": "2021-11-04T17:24:00.000Z",
        "voteCount": 1,
        "content": "Go C -1"
      },
      {
        "date": "2021-11-03T22:09:00.000Z",
        "voteCount": 2,
        "content": "if you have developed with both SQL-like databases, i.e.: RDS, Aurora, and with DynamoDB, you should know the database schema design and CRUD operations are VASTLY different between SQL and DynamoDB."
      },
      {
        "date": "2021-11-03T21:42:00.000Z",
        "voteCount": 2,
        "content": "I'll go with C)\nThe question wants \"LEAST amount of application changes\", so ANY option with includes DynamoDB (even for half portion) will require a LOT of changes."
      },
      {
        "date": "2021-11-03T15:10:00.000Z",
        "voteCount": 2,
        "content": "C. Use Aurora with read replicas for the product catalog and additional local Aurora instances in each region for the customer information and purchases."
      },
      {
        "date": "2021-10-26T13:59:00.000Z",
        "voteCount": 1,
        "content": "Key info:  \"LEAST amount of application changes\", so not changing DB type or engine, no code refactoring! \nRight: C\nWrong: \n-A: require code changes, Redshift makes no sense here as it is for data warehousing\n-B: require again code changes\n-D: would imply code changes as well."
      },
      {
        "date": "2021-10-23T07:13:00.000Z",
        "voteCount": 4,
        "content": "I'll go with C"
      },
      {
        "date": "2021-10-21T03:18:00.000Z",
        "voteCount": 1,
        "content": "Its  Option C cause, we only need a LEAST amount of changes."
      },
      {
        "date": "2021-10-19T19:14:00.000Z",
        "voteCount": 7,
        "content": "It's C - all the others require application changes to accommodate a different DB, which is undesirable since the question is asking for minimal application changes. It's worth adding that in the real world things are often much more complicated than this, so you'd want to revisit the business requirements, validate them, forecast future requirements and make a decision. Having regions of US, Asia and Europe is a gross simplification, because what you really need to consider is the data protection regulations in specific jurisdictions, not continents. e.g. Indonesia is in Asia and up until Oct 2019 required customer data to be held in-country, but there is no AWS data centre there. What do you do?"
      },
      {
        "date": "2023-02-03T05:47:00.000Z",
        "voteCount": 1,
        "content": "Amazing comment involving real life scenarios. Thanks."
      },
      {
        "date": "2021-10-16T23:03:00.000Z",
        "voteCount": 1,
        "content": "Can't be B as it requires application changes.\nC is correct"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 11,
    "url": "https://www.examtopics.com/discussions/amazon/view/47015-exam-aws-devops-engineer-professional-topic-1-question-11/",
    "body": "A company wants to ensure that their EC2 instances are secure. They want to be notified if any new vulnerabilities are discovered on their instances, and they also want an audit trail of all login activities on the instances.<br>Which solution will meet these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Systems Manager to detect vulnerabilities on the EC2 instances. Install the Amazon Kinesis Agent to capture system logs and deliver them to Amazon S3.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Systems Manager to detect vulnerabilities on the EC2 instances. Install the Systems Manager Agent to capture system logs and view login activity in the CloudTrail console.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Amazon CloudWatch to detect vulnerabilities on the EC2 instances. Install the AWS Config daemon to capture system logs and view them in the AWS Config console.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Amazon Inspector to detect vulnerabilities on the EC2 instances. Install the Amazon CloudWatch Agent to capture system logs and record them via Amazon CloudWatch Logs.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 15,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 6,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-11-01T18:47:00.000Z",
        "voteCount": 17,
        "content": "Has to be D with Inspector &amp; CW Logging there."
      },
      {
        "date": "2024-05-28T20:24:00.000Z",
        "voteCount": 1,
        "content": "Amazon Inspector is for apps / software in the EC2. The question only mentions the vulnerability of the instances itself"
      },
      {
        "date": "2023-07-09T17:41:00.000Z",
        "voteCount": 2,
        "content": "Answer is D. Inspector = Vulnerabilities, Instance level audit logs (not account level login logs) CW agent"
      },
      {
        "date": "2023-06-24T07:26:00.000Z",
        "voteCount": 1,
        "content": "B. It depends on which resources you\u2019re scanning. AWS Systems Manager Agent (SSM Agent) is required for vulnerability scanning of Amazon EC2 instances. No agents are required for network reachability of Amazon EC2 instances and vulnerability scanning of container images, or for vulnerability scanning of Lambda functions."
      },
      {
        "date": "2023-05-01T01:03:00.000Z",
        "voteCount": 1,
        "content": "D is correct answer as we have to detect vulnerabilities and send it to CloudTrail. AWS Inspector works here."
      },
      {
        "date": "2023-04-27T08:16:00.000Z",
        "voteCount": 1,
        "content": "B isn't good because we need a SIEM to send the logging information from cloudtrail.\nWe could use Inspector to detect vulnerabilities on EC2 instances."
      },
      {
        "date": "2023-03-16T22:08:00.000Z",
        "voteCount": 1,
        "content": "Amazon Inspector is a service that automatically assesses applications for vulnerabilities and compliance issues. It is integrated with Amazon EC2 instances and can automatically scan instances for known vulnerabilities. Installing the Amazon CloudWatch agent on the instances allows for capturing system logs and recording them via Amazon CloudWatch Logs. With this setup, the company can receive notifications for new vulnerabilities and also have an audit trail of login activities on the instances."
      },
      {
        "date": "2023-03-01T08:11:00.000Z",
        "voteCount": 2,
        "content": "Only Amazon Inspector can detect vulnerabilities."
      },
      {
        "date": "2023-02-26T22:59:00.000Z",
        "voteCount": 2,
        "content": "D is correct. Only Amazon Inspector can detect vulnerabilities."
      },
      {
        "date": "2023-02-22T20:33:00.000Z",
        "voteCount": 2,
        "content": "Definitely D\nU use CloudWatch agent to get the login info in the ec2, not SSM"
      },
      {
        "date": "2023-02-22T18:34:00.000Z",
        "voteCount": 2,
        "content": "D is the right ans"
      },
      {
        "date": "2023-01-09T07:23:00.000Z",
        "voteCount": 3,
        "content": "It actually is B even though you would think D. Have a look at this article. https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-compliance.html\nCompliance offers the following additional benefits and features:\n\nView compliance history and change tracking for Patch Manager patching data and State Manager associations by using AWS Config.\n\nCustomize Compliance to create your own compliance types based on your IT or business requirements.\n\nRemediate issues by using Run Command, another capability of AWS Systems Manager, State Manager, or Amazon EventBridge.\n\nPort data to Amazon Athena and Amazon QuickSight to generate fleet-wide reports."
      },
      {
        "date": "2023-01-19T12:16:00.000Z",
        "voteCount": 6,
        "content": "Wrong.\n\nCompliance != Security. SSM Compliance is used to ensure the software satisfies your company's standard, but it does not check for vulnerabilities in said patches."
      },
      {
        "date": "2022-12-31T22:09:00.000Z",
        "voteCount": 1,
        "content": "D is the correct answer"
      },
      {
        "date": "2022-11-16T00:06:00.000Z",
        "voteCount": 1,
        "content": "D: AWS Inspector for vulnerability scans"
      },
      {
        "date": "2022-11-05T07:26:00.000Z",
        "voteCount": 4,
        "content": "D.\nSSM can be used to check if configuration is compliant, but Inspector is used to check EC2 vulnerabilities."
      },
      {
        "date": "2022-10-13T07:07:00.000Z",
        "voteCount": 2,
        "content": "B seems right as It requires \nLogin trail as well &gt;&gt; Cloud Trail \nSystem vulnerability &gt;&gt; can be detected by SSM via SSM agent installation"
      },
      {
        "date": "2022-09-17T08:42:00.000Z",
        "voteCount": 2,
        "content": "Obvious Choice, Inspector ! For any vulnerability at instance level.\nAnd Cloud watch to log all actions"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 12,
    "url": "https://www.examtopics.com/discussions/amazon/view/51619-exam-aws-devops-engineer-professional-topic-1-question-12/",
    "body": "A DevOps Engineer needs to back up sensitive Amazon S3 objects that are stored within an S3 bucket with a private bucket policy using the S3 cross-region replication functionality. The objects need to be copied to a target bucket in a different AWS Region and account.<br>Which actions should be performed to enable this replication? (Choose three.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a replication IAM role in the source account.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a replication IAM role in the target account.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd statements to the source bucket policy allowing the replication IAM role to replicate objects.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd statements to the target bucket policy allowing the replication IAM role to replicate objects.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a replication rule in the source bucket to enable the replication.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a replication rule in the target bucket to enable the replication."
    ],
    "answer": "ADE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ADE",
        "count": 7,
        "isMostVoted": true
      },
      {
        "answer": "BCE",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "ACE",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-09-22T18:04:00.000Z",
        "voteCount": 12,
        "content": "ADE -  The replication rule is created in the source bucket"
      },
      {
        "date": "2023-10-24T01:18:00.000Z",
        "voteCount": 1,
        "content": "ACE. The alternative D - \"Add statements to the target bucket policy allowing the replication IAM role to replicate objects\" might be chosen frequently due to a common misconception. \n When setting up cross-region replication in AWS S3, some people might assume that the target bucket (where the objects are being replicated to) also needs to explicitly grant permissions to the replication IAM ___role___. However, this is not the case in AWS S3 cross-region replication setup."
      },
      {
        "date": "2023-05-10T21:26:00.000Z",
        "voteCount": 1,
        "content": "S3 cross-Region replication (CRR) automatically replicates data between buckets across different AWS Regions. To enable CRR, you need to add a replication configuration to your source bucket that specifies the destination bucket, the IAM role, and the encryption type (optional). You also need to grant permissions to the IAM role to perform replication actions on both the source and destination buckets. Additionally, you can choose the destination storage class and enable additional replication options such as S3 Replication Time Control (S3 RTC) or S3 Batch Replication.\nhttps://medium.com/cloud-techies/s3-same-region-replication-srr-and-cross-region-replication-crr-34d446806bab\nhttps://aws.amazon.com/getting-started/hands-on/replicate-data-using-amazon-s3-replication/\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/replication.html"
      },
      {
        "date": "2023-05-09T17:10:00.000Z",
        "voteCount": 1,
        "content": "ACE\nCreate and attach the S3 bucket policy in the source account\nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/copy-data-from-an-s3-bucket-to-another-account-and-region-by-using-the-aws-cli.html"
      },
      {
        "date": "2023-05-01T01:05:00.000Z",
        "voteCount": 1,
        "content": "ADE seems to be correct answer"
      },
      {
        "date": "2023-04-26T10:33:00.000Z",
        "voteCount": 1,
        "content": "D, B, E: Refference: https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/copy-data-from-an-s3-bucket-to-another-account-and-region-by-using-the-aws-cli.html"
      },
      {
        "date": "2023-03-16T22:14:00.000Z",
        "voteCount": 1,
        "content": "ABCE are correct asnwers"
      },
      {
        "date": "2023-02-28T08:53:00.000Z",
        "voteCount": 2,
        "content": "ADE - Correct Answer , Tested and working"
      },
      {
        "date": "2023-02-26T14:49:00.000Z",
        "voteCount": 1,
        "content": "ADE -  The replication rule is created in the source bucket"
      },
      {
        "date": "2023-02-06T13:54:00.000Z",
        "voteCount": 1,
        "content": "The answer is ADE"
      },
      {
        "date": "2023-02-03T06:29:00.000Z",
        "voteCount": 2,
        "content": "1. B from (A, B) : the new IAM role should belong to the target account\n2. C from (C, D):  the policy is used to allow the role to do something to the object what the policy is attached to.\n3. E from (E, F): the rule should be created to the bucket whose objects will be replicated"
      },
      {
        "date": "2023-02-03T07:20:00.000Z",
        "voteCount": 3,
        "content": "My voted answers are wrong. Sorry guys. \nThe permission to replicate should belong to the role on source account --&gt; A from (A, B)\nThe target bucket policy should have statement to grant permission for replicating to the role in the source account.  AWS official article: https://docs.aws.amazon.com/AmazonS3/latest/userguide/replication-walkthrough-2.html\nThe Replication Rule should be created on the target bucket --&gt; F from(E, F).  Please refer to this article which involves screenshots of hands-on experiment. https://aws.plainenglish.io/set-up-an-s3-bucket-with-cross-region-replication-97d43084ff36"
      },
      {
        "date": "2022-12-31T22:23:00.000Z",
        "voteCount": 4,
        "content": "ADE is the correct answer. To enable cross account replication we  need to \n1.  create a replication rule in the source bucket\n2. Create an IAM role in the source account to perform replication\n3. Create a resource policy on the destination bucket that grant permission to the IAM role in the source account to replicate objects into the destination bucket."
      },
      {
        "date": "2022-09-02T11:49:00.000Z",
        "voteCount": 1,
        "content": "ADE - https://docs.aws.amazon.com/AmazonS3/latest/userguide/replication-walkthrough-2.html"
      },
      {
        "date": "2022-08-31T19:14:00.000Z",
        "voteCount": 3,
        "content": "BDE - B becos the source acct should assume the target acct's IAM role to copy things into it."
      },
      {
        "date": "2022-06-16T16:56:00.000Z",
        "voteCount": 3,
        "content": "ADE\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/replication-walkthrough-2.html"
      },
      {
        "date": "2022-02-13T06:26:00.000Z",
        "voteCount": 2,
        "content": "ADE should be the answer. F is incorrect."
      },
      {
        "date": "2022-01-15T17:56:00.000Z",
        "voteCount": 2,
        "content": "I would vote for ADE"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 13,
    "url": "https://www.examtopics.com/discussions/amazon/view/46933-exam-aws-devops-engineer-professional-topic-1-question-13/",
    "body": "A company is using Amazon EC2 for various workloads. Company policy requires that instances be managed centrally to standardize configurations. These configurations include standard logging, metrics, security assessments, and weekly patching.<br>How can the company meet these requirements? (Choose three.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Config to ensure all EC2 instances are managed by Amazon Inspector.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Config to ensure all EC2 instances are managed by AWS Systems Manager.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Systems Manager to install and manage Amazon Inspector, Systems Manager Patch Manager, and the Amazon CloudWatch agent on all instances.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Inspector to install and manage AWS Systems Manager, Systems Manager Patch Manager, and the Amazon CloudWatch agent on all instances.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Systems Manager maintenance windows with Systems Manager Run Command to schedule Systems Manager Patch Manager tasks. Use the Amazon CloudWatch agent to schedule Amazon Inspector assessment runs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Systems Manager maintenance windows with Systems Manager Run Command to schedule Systems Manager Patch Manager tasks. Use Amazon CloudWatch Events to schedule Amazon Inspector assessment runs.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "BCF",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BCF",
        "count": 20,
        "isMostVoted": true
      },
      {
        "answer": "BCE",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-10-04T22:00:00.000Z",
        "voteCount": 27,
        "content": "Ans: BCF"
      },
      {
        "date": "2023-09-15T18:59:00.000Z",
        "voteCount": 1,
        "content": "Please note that option A is incorrect because Amazon Inspector is a service that helps you improve the security and compliance of your applications deployed on AWS by providing insights and recommendations for security vulnerabilities. Option D is incorrect because it suggests using Amazon Inspector to install and manage AWS Systems Manager, which is not possible . Option F is incorrect because it suggests using Amazon CloudWatch Events to schedule Amazon Inspector assessment runs, which is not possible"
      },
      {
        "date": "2024-08-12T01:47:00.000Z",
        "voteCount": 1,
        "content": "you can schedule Inspector with CW event https://aws.amazon.com/about-aws/whats-new/2017/07/amazon-inspector-adds-event-triggers-to-automatically-run-assessments/\nAmazon Inspector assessments can be triggered by any CloudWatch Event. You can set up a recurring Schedule event with either a simple fixed recurring rate or a more detailed Cron expression, or create an event pattern which monitors other AWS services for actions to trigger an assessment"
      },
      {
        "date": "2023-05-12T05:03:00.000Z",
        "voteCount": 1,
        "content": "answer is BCF"
      },
      {
        "date": "2023-05-01T01:15:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer is BCF"
      },
      {
        "date": "2023-03-16T22:19:00.000Z",
        "voteCount": 2,
        "content": "The company can meet these requirements by:\n\nB. Use AWS Config to ensure all EC2 instances are managed by AWS Systems Manager. This will allow for centralized management of instances through a single console.\nC. Use AWS Systems Manager to install and manage Amazon Inspector, Systems Manager Patch Manager, and the Amazon CloudWatch agent on all instances. This will enable standard logging, metrics, security assessments, and weekly patching.\nE. Use AWS Systems Manager maintenance windows with Systems Manager Run Command to schedule Systems Manager Patch Manager tasks. Use the Amazon CloudWatch agent to schedule Amazon Inspector assessment runs. This will provide a standardized approach to patching and vulnerability assessment.\n\nTherefore, the correct options are B, C, and E."
      },
      {
        "date": "2023-02-28T18:15:00.000Z",
        "voteCount": 1,
        "content": "Right answer."
      },
      {
        "date": "2023-02-18T20:36:00.000Z",
        "voteCount": 2,
        "content": "Answer is BCF\nTwo document supporting the answer\n1. Use Systems Manager Run Command to install amazon inspector\nhttps://docs.aws.amazon.com/inspector/v1/userguide/inspector_installing-uninstalling-agents.html\n2. Amazon Inspector schdule can be changed in cloudwatch event\nhttps://aws.amazon.com/about-aws/whats-new/2018/04/amazon-inspector-adds-the-ability-to-schedule-recurring-assessment-runs-and-install-the-agent-from-the-console/?nc1=h_ls"
      },
      {
        "date": "2023-02-06T13:59:00.000Z",
        "voteCount": 1,
        "content": "the answer is BCF"
      },
      {
        "date": "2023-02-03T08:57:00.000Z",
        "voteCount": 1,
        "content": "AWS Config --&gt; AWS Systems Manager --&gt; AWS CloudWatch --&gt; schedule Amazon Inspector\nThe workflow looks sense."
      },
      {
        "date": "2023-02-03T09:14:00.000Z",
        "voteCount": 1,
        "content": "I changed my vote from BCE to BCF.\nCloudWatch agent does not help to schedule running. CloudWatch collects metrics and logs from Amazon EC2 instances and on-premises servers. \n\nThe correct answer shows BDE.  The D must be wrong.  Amazon Inspector does not help to install."
      },
      {
        "date": "2023-01-21T09:59:00.000Z",
        "voteCount": 1,
        "content": "BCF is the correct answer"
      },
      {
        "date": "2022-12-31T22:28:00.000Z",
        "voteCount": 2,
        "content": "B,C,F are the right answers"
      },
      {
        "date": "2022-12-04T01:11:00.000Z",
        "voteCount": 2,
        "content": "Don't know where BDE as correct answer is coming from, but clearly you don't use Inspector to install anything (as stated in E)"
      },
      {
        "date": "2022-11-07T00:20:00.000Z",
        "voteCount": 4,
        "content": "B - AWS System Manager to manage Centrally\nC - Use AWS System Manager agent to install Amazon Inspector, not the vice versa. \nF - Use Cloudwatch Event to schedule, agent is for collecting metrics."
      },
      {
        "date": "2022-11-05T02:28:00.000Z",
        "voteCount": 1,
        "content": "Ans: BCF"
      },
      {
        "date": "2022-10-07T02:38:00.000Z",
        "voteCount": 1,
        "content": "BCF is correct\nhttps://aws.amazon.com/blogs/opensource/getting-started-with-open-source-amazon-cloudwatch-agent/"
      },
      {
        "date": "2022-09-20T02:02:00.000Z",
        "voteCount": 2,
        "content": "BCF \nthe level of this Question is Associate Exam, not Proff!"
      },
      {
        "date": "2022-09-17T16:53:00.000Z",
        "voteCount": 1,
        "content": "thats the way"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 14,
    "url": "https://www.examtopics.com/discussions/amazon/view/5043-exam-aws-devops-engineer-professional-topic-1-question-14/",
    "body": "A business has an application that consists of five independent AWS Lambda functions.<br>The DevOps Engineer has built a CI/CD pipeline using AWS CodePipeline and AWS CodeBuild that builds, tests, packages, and deploys each Lambda function in sequence. The pipeline uses an Amazon CloudWatch Events rule to ensure the pipeline execution starts as quickly as possible after a change is made to the application source code.<br>After working with the pipeline for a few months, the DevOps Engineer has noticed the pipeline takes too long to complete.<br>What should the DevOps Engineer implement to BEST improve the speed of the pipeline?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the CodeBuild projects within the pipeline to use a compute type with more available network throughput.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a custom CodeBuild execution environment that includes a symmetric multiprocessing configuration to run the builds in parallel.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the CodePipeline configuration to execute actions for each Lambda function in parallel by specifying the same runOrder.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify each CodeBuild project to run within a VPC and use dedicated instances to increase throughput."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 3,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-20T21:49:00.000Z",
        "voteCount": 25,
        "content": "i choose C"
      },
      {
        "date": "2022-09-17T08:49:00.000Z",
        "voteCount": 2,
        "content": "Absolutely"
      },
      {
        "date": "2021-09-24T12:50:00.000Z",
        "voteCount": 22,
        "content": "C\nhttps://docs.aws.amazon.com/codepipeline/latest/userguide/reference-pipeline-structure.html\nAWS doc: \"To specify parallel actions, use the same integer for each action you want to run in parallel.\n\nFor example, if you want three actions to run in sequence in a stage, you would give the first action the runOrder value of 1, the second action the runOrder value of 2, and the third the runOrder value of 3. However, if you want the second and third actions to run in parallel, you would give the first action the runOrder value of 1 and both the second and third actions the runOrder value of 2.\""
      },
      {
        "date": "2023-03-16T22:24:00.000Z",
        "voteCount": 1,
        "content": "best is C"
      },
      {
        "date": "2023-02-14T20:21:00.000Z",
        "voteCount": 1,
        "content": "i will go with c"
      },
      {
        "date": "2023-02-03T09:22:00.000Z",
        "voteCount": 1,
        "content": "D is the first one to be excluded.\nA is not correct.  CodeBuild is a managed product. We do not need to modify.\nB is not correct. Because the latency may caused by other steps, like tests. \n\"In sequence\" is the point need to be changed.  The whole pipeline need to be put in parallel, not only the building step."
      },
      {
        "date": "2023-01-31T04:17:00.000Z",
        "voteCount": 1,
        "content": "C will be the answer"
      },
      {
        "date": "2022-12-31T22:31:00.000Z",
        "voteCount": 1,
        "content": "C is the right answer"
      },
      {
        "date": "2022-12-31T22:30:00.000Z",
        "voteCount": 1,
        "content": "C is the right answer."
      },
      {
        "date": "2022-03-23T05:05:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2021-11-01T19:14:00.000Z",
        "voteCount": 3,
        "content": "I'll go with C"
      },
      {
        "date": "2021-10-28T14:21:00.000Z",
        "voteCount": 1,
        "content": "C for me"
      },
      {
        "date": "2021-10-24T07:48:00.000Z",
        "voteCount": 1,
        "content": "C is my answer,.  parallel"
      },
      {
        "date": "2021-10-24T03:55:00.000Z",
        "voteCount": 2,
        "content": "I'll go with C"
      },
      {
        "date": "2021-10-23T03:13:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2021-10-22T22:16:00.000Z",
        "voteCount": 2,
        "content": "C is correct. Notice that the five lambda functions are independent and therefore running them in parallel saves time"
      },
      {
        "date": "2021-10-22T21:11:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2021-10-22T16:35:00.000Z",
        "voteCount": 5,
        "content": "A - this will improve the timing but real problem here is sequential processing still exist\nB - don't think there are multi processing options - https://aws.amazon.com/blogs/opensource/using-aws-codepipeline-and-open-source-tools-for-at-scale-infrastructure-deployment/\nC - correct\nD - running in vpc will further reduce timing and vpc execution should only be used when funciton has to access vpc resources"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 15,
    "url": "https://www.examtopics.com/discussions/amazon/view/8044-exam-aws-devops-engineer-professional-topic-1-question-15/",
    "body": "A company is creating a software solution that executes a specific parallel-processing mechanism. The software can scale to tens of servers in some special scenarios. This solution uses a proprietary library that is license-based, requiring that each individual server have a single, dedicated license installed. The company has 200 licenses and is planning to run 200 server nodes concurrently at most.<br>The company has requested the following features:<br>\u2711 A mechanism to automate the use of the licenses at scale.<br>\u2711 Creation of a dashboard to use in the future to verify which licenses are available at any moment.<br>What is the MOST effective way to accomplish these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpload the licenses to a private Amazon S3 bucket. Create an AWS CloudFormation template with a Mappings section for the licenses. In the template, create an Auto Scaling group to launch the servers. In the user data script, acquire an available license from the Mappings section. Create an Auto Scaling lifecycle hook, then use it to update the mapping after the instance is terminated.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpload the licenses to an Amazon DynamoDB table. Create an AWS CloudFormation template that uses an Auto Scaling group to launch the servers. In the user data script, acquire an available license from the DynamoDB table. Create an Auto Scaling lifecycle hook, then use it to update the mapping after the instance is terminated.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpload the licenses to a private Amazon S3 bucket. Populate an Amazon SQS queue with the list of licenses stored in S3. Create an AWS CloudFormation template that uses an Auto Scaling group to launch the servers. In the user data script acquire an available license from SQS. Create an Auto Scaling lifecycle hook, then use it to put the license back in SQS after the instance is terminated.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpload the licenses to an Amazon DynamoDB table. Create an AWS CLI script to launch the servers by using the parameter --count, with min:max instances to launch. In the user data script, acquire an available license from the DynamoDB table. Monitor each instance and, in case of failure, replace the instance, then manually update the DynamoDB table."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 15,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-21T11:07:00.000Z",
        "voteCount": 24,
        "content": "I believe B is the right solution"
      },
      {
        "date": "2021-10-19T13:56:00.000Z",
        "voteCount": 17,
        "content": "A - license list is dynamic (based on scaling) and mapping are good for values that are static in nature\nB - looks good\nC - sqs -  unnecessary overhead\nD - too many manual tasks"
      },
      {
        "date": "2023-03-16T22:27:00.000Z",
        "voteCount": 1,
        "content": "This approach allows the licenses to be stored and managed in a central location in an Amazon DynamoDB table. The AWS CloudFormation template can then use an Auto Scaling group to launch the server nodes, which will dynamically acquire a license from the DynamoDB table when the instance starts up using the user data script. An Auto Scaling lifecycle hook can be created to update the license mapping after the instance is terminated. This mechanism allows the software solution to scale and manage licenses automatically. Additionally, the DynamoDB table can be used to create a dashboard that displays which licenses are available at any moment, giving the company greater visibility into license usage."
      },
      {
        "date": "2023-03-04T06:13:00.000Z",
        "voteCount": 2,
        "content": "B is correct because as one condition states that we need to create a dashboard that can be done using dynamodb, Cloudformation instead of CLI scripts to launch instance nodes using ASG. We can use acquire license using TransactGetItems which would in User Data script. ASG lifecycle hook consists terminating:wait where we can run scripts to complete dynamodb transaction on the instance using SSM run command with SSM agent installed on the instance from user data script"
      },
      {
        "date": "2023-02-25T15:33:00.000Z",
        "voteCount": 1,
        "content": "B- Correct"
      },
      {
        "date": "2023-02-15T10:07:00.000Z",
        "voteCount": 1,
        "content": "How is B the correct answer?\nWhat does it even mean update the mapping? What mapping?"
      },
      {
        "date": "2023-02-03T09:31:00.000Z",
        "voteCount": 1,
        "content": "DynamoDB is a must.  S3 bucket is not efficient as DynamoDB\nCloudFormation is more effective than self-prepared CLI script"
      },
      {
        "date": "2023-01-08T11:16:00.000Z",
        "voteCount": 2,
        "content": "Why D is set as the correct answer?"
      },
      {
        "date": "2023-02-03T09:32:00.000Z",
        "voteCount": 1,
        "content": "Same confusion."
      },
      {
        "date": "2022-12-31T22:34:00.000Z",
        "voteCount": 1,
        "content": "B is the right answer."
      },
      {
        "date": "2022-12-28T15:10:00.000Z",
        "voteCount": 1,
        "content": "B is the right answer"
      },
      {
        "date": "2022-12-27T06:28:00.000Z",
        "voteCount": 2,
        "content": "B is the good for me"
      },
      {
        "date": "2022-12-03T04:46:00.000Z",
        "voteCount": 4,
        "content": "The Dev Ops Pro is a measure of how well you know automation. D requires manual intervention. B looks to be the right solution."
      },
      {
        "date": "2022-11-07T19:56:00.000Z",
        "voteCount": 4,
        "content": "A - S3 contains static data and there is no information on the available licenses to create dashboard. \nB- DynamoDb contains mapping on the availability of licenses and can create dashboard on the data. \nC - Overhead to maintain S3 and SQS. Hard to create Dashboard out of SQS. \nD - Manual update , no no"
      },
      {
        "date": "2022-10-14T22:45:00.000Z",
        "voteCount": 2,
        "content": "B is right answer"
      },
      {
        "date": "2022-10-07T04:08:00.000Z",
        "voteCount": 1,
        "content": "B is correct."
      },
      {
        "date": "2022-08-31T19:24:00.000Z",
        "voteCount": 2,
        "content": "B - becos:\n1. dynamodb's transactgetitem api- https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/transaction-apis.html#transaction-apis-txgetitems\n2. CF to generate ASG to manage the EC2s."
      },
      {
        "date": "2022-02-09T13:12:00.000Z",
        "voteCount": 2,
        "content": "Should be B"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 16,
    "url": "https://www.examtopics.com/discussions/amazon/view/2335-exam-aws-devops-engineer-professional-topic-1-question-16/",
    "body": "A DevOps Engineer administers an application that manages video files for a video production company. The application runs on Amazon EC2 instances behind an ELB Application Load Balancer. The instances run in an Auto Scaling group across multiple Availability Zones. Data is stored in an Amazon RDS PostgreSQL<br>Multi-AZ DB instance, and the video files are stored in an Amazon S3 bucket. On a typical day, 50 GB of new video are added to the S3 bucket. The Engineer must implement a multi-region disaster recovery plan with the least data loss and the lowest recovery times. The current application infrastructure is already described using AWS CloudFormation.<br>Which deployment option should the Engineer choose to meet the uptime and recovery objectives for the system?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLaunch the application from the CloudFormation template in the second region, which sets the capacity of the Auto Scaling group to 1. Create an Amazon RDS read replica in the second region. In the second region, enable cross-region replication between the original S3 bucket and a new S3 bucket. To fail over, promote the read replica as master. Update the CloudFormation stack and increase the capacity of the Auto Scaling group.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLaunch the application from the CloudFormation template in the second region, which sets the capacity of the Auto Scaling group to 1. Create a scheduled task to take daily Amazon RDS cross-region snapshots to the second region. In the second region, enable cross-region replication between the original S3 bucket and Amazon Glacier. In a disaster, launch a new application stack in the second region and restore the database from the most recent snapshot.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLaunch the application from the CloudFormation template in the second region, which sets the capacity of the Auto Scaling group to 1. Use Amazon CloudWatch Events to schedule a nightly task to take a snapshot of the database, copy the snapshot to the second region, and replace the DB instance in the second region from the snapshot. In the second region, enable cross-region replication between the original S3 bucket and a new S3 bucket. To fail over, increase the capacity of the Auto Scaling group.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon CloudWatch Events to schedule a nightly task to take a snapshot of the database and copy the snapshot to the second region. Create an AWS Lambda function that copies each object to a new S3 bucket in the second region in response to S3 event notifications. In the second region, launch the application from the CloudFormation template and restore the database from the most recent snapshot."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 3,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-21T08:40:00.000Z",
        "voteCount": 48,
        "content": "answer is A"
      },
      {
        "date": "2021-10-11T00:05:00.000Z",
        "voteCount": 6,
        "content": "A is the right choice"
      },
      {
        "date": "2021-10-23T06:13:00.000Z",
        "voteCount": 1,
        "content": "I think D is correct based on comments form @inf"
      },
      {
        "date": "2021-12-15T23:32:00.000Z",
        "voteCount": 5,
        "content": "Its not. Restoring any DB is a time consuming procedure, and this answer implies a restore would have after disaster.\n\nA is the only answer that implies replication, and active replication to bring a failover DB live is the FASTEST way to restore a downed DB."
      },
      {
        "date": "2023-12-25T22:25:00.000Z",
        "voteCount": 1,
        "content": "not to mention we will lose data of the current day , since we rely on the snapshot taken at night previous day"
      },
      {
        "date": "2023-07-10T00:52:00.000Z",
        "voteCount": 1,
        "content": "Answer is A: to meet the uptime and recovery objectives for the system, and this has the lowest RPO"
      },
      {
        "date": "2023-02-03T11:25:00.000Z",
        "voteCount": 1,
        "content": "B is not correct -&gt; Glacier tier does not match the requirement of lowest recovery times.\nC is not correct -&gt; the snapshot copy -&gt; replace operation can fail sometimes\nD is not correct -&gt; the Lambda function on S3 objects is not efficient and stable enough."
      },
      {
        "date": "2022-12-31T22:39:00.000Z",
        "voteCount": 1,
        "content": "A is the right answer."
      },
      {
        "date": "2022-12-27T06:40:00.000Z",
        "voteCount": 1,
        "content": "A:  cross-Region read replica\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html#USER_ReadRepl.XRgn"
      },
      {
        "date": "2022-11-20T12:33:00.000Z",
        "voteCount": 1,
        "content": "answer is A , \nI think, solution do not need new s3 bucket."
      },
      {
        "date": "2022-09-05T21:17:00.000Z",
        "voteCount": 1,
        "content": "Ans: A"
      },
      {
        "date": "2021-11-04T20:02:00.000Z",
        "voteCount": 3,
        "content": "Go A -1"
      },
      {
        "date": "2021-11-04T16:30:00.000Z",
        "voteCount": 2,
        "content": "I'll go with A)\nFor those choosing D) recall enable cross-region replication for S3  is always the best practice and is strongly recommended.\nCopying such volume of data using lambda will result in high costs for Lambda execution time, due the time it takes to copy and process ."
      },
      {
        "date": "2021-11-03T09:06:00.000Z",
        "voteCount": 1,
        "content": "D is totally: 1 \"copies each object\" , 2 kind of situations RDS-PG is the best option for the cross region"
      },
      {
        "date": "2021-10-31T21:12:00.000Z",
        "voteCount": 1,
        "content": "I would keep it simple and efficient: A"
      },
      {
        "date": "2021-10-31T08:58:00.000Z",
        "voteCount": 3,
        "content": "I'll go with A"
      },
      {
        "date": "2021-10-31T06:54:00.000Z",
        "voteCount": 2,
        "content": "Only A is quick for DB recovery in RPO. Other options which go with daily or night schedule will take ~ 24 hours for backup interval, so it does meet RPO condition."
      },
      {
        "date": "2021-10-29T21:03:00.000Z",
        "voteCount": 2,
        "content": "A says: \"In the second region, enable cross-region replication between the original S3 bucket and a new S3 bucket.\" - the part \"In second region\" is tricky as S3 does not require region selection, but the second part says \"enable cross-region replication between the original S3 bucket and a new S3 bucket\" which does not explicitly says on which bucket to configure CRR. \nSo the correct answer is A."
      },
      {
        "date": "2021-10-29T10:33:00.000Z",
        "voteCount": 1,
        "content": "I think it's A"
      },
      {
        "date": "2021-10-28T10:59:00.000Z",
        "voteCount": 1,
        "content": "I think it should be A, but the wording of the question is incorrect.\nNormally the response options are created in a pretty formulaic way: you get a bunch of statements and each response contains some combination of those statements, plus a few unique ones. In this case the statement about S3 CRR appears a few times but is incorrectly worded. I suspect they wanted this to be a valid statement, in which case A is clearly the best answer, since the focus is on RPO and A has an RPO measured in minutes, whereas B, C and D appear to have a 24h RPO."
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 17,
    "url": "https://www.examtopics.com/discussions/amazon/view/47953-exam-aws-devops-engineer-professional-topic-1-question-17/",
    "body": "A company is using AWS CodePipeline to automate its release pipeline. AWS CodeDeploy is being used in the pipeline to deploy an application to Amazon ECS using the blue/green deployment model. The company wants to implement scripts to test the green version of the application before shifting traffic. These scripts will complete in 5 minutes or less. If errors are discovered during these tests, the application must be rolled back.<br>Which strategy will meet these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a stage to the CodePipeline pipeline between the source and deploy stages. Use AWS CodeBuild to create an execution environment and build commands in the buildspec file to invoke test scripts. If errors are found, use the aws deploy stop-deployment command to stop the deployment.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a stage to the CodePipeline pipeline between the source and deploy stages. Use this stage to execute an AWS Lambda function that will run the test scripts. If errors are found, use the aws deploy stop-deployment command to stop the deployment.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a hooks section to the CodeDeploy AppSpec file. Use the AfterAllowTestTraffic lifecycle event to invoke an AWS Lambda function to run the test scripts. If errors are found, exit the Lambda function with an error to trigger rollback.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a hooks section to the CodeDeploy AppSpec file. Use the AfterAllowTraffic lifecycle event to invoke the test scripts. If errors are found, use the aws deploy stop-deployment CLI command to stop the deployment."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 12,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-09-21T01:37:00.000Z",
        "voteCount": 23,
        "content": "C looks correct, reference: https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html#reference-appspec-file-structure-hooks-section-structure-ecs-sample-function"
      },
      {
        "date": "2023-04-03T14:21:00.000Z",
        "voteCount": 1,
        "content": "C is the one"
      },
      {
        "date": "2023-02-04T03:49:00.000Z",
        "voteCount": 1,
        "content": "C and D were excluded because AfterAllowTraffic should be BeforeAllowTraffic.  B is excluded because Lambda function is not needed. CodeBuild has managed test environment."
      },
      {
        "date": "2023-02-04T05:19:00.000Z",
        "voteCount": 3,
        "content": "Sorry, I was wrong.  A does not involve rollback action. In C, AfterAllowTestTraffic is the hook we need. And C is the only option involves rollback action."
      },
      {
        "date": "2022-12-31T22:40:00.000Z",
        "voteCount": 1,
        "content": "C is the correct answer."
      },
      {
        "date": "2022-12-27T09:46:00.000Z",
        "voteCount": 1,
        "content": "C is correct,\nyou need to run the script, the best option is using lambda"
      },
      {
        "date": "2022-11-07T22:03:00.000Z",
        "voteCount": 4,
        "content": "Other answers are just stopping the deployment. Rollback is one of the requirement fulfilled by this answer C"
      },
      {
        "date": "2022-09-28T10:02:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html\n\nAfterAllowTestTraffic \u2013 Use to run tasks after the test listener serves traffic to the replacement task set. The results of a hook function at this point can trigger a rollback."
      },
      {
        "date": "2022-09-18T21:20:00.000Z",
        "voteCount": 3,
        "content": "C\nA, B - u need to deploy first else there is nth to test. And rem CodeBuild is exactly tt, do code building.\nD - AfterAllowTraffic means the Green env is alr \"live\", if there is no \"AfterAllowTestTraffic\" option (C), then D is the correct one.\n\nSince there AfterAllowTestTraffic, its better to chose tt.\nhttps://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html &lt;&lt; take a look at the diagram in this."
      },
      {
        "date": "2022-12-02T15:19:00.000Z",
        "voteCount": 5,
        "content": "do you really need to abbreviate? it makes it way harder to read your explanation. please stop."
      },
      {
        "date": "2022-09-11T01:48:00.000Z",
        "voteCount": 1,
        "content": "AfterAllowTestTraffic \u2013 Use to run tasks after the test listener serves traffic to the replacement task set. The results of a hook function at this point can trigger a rollback.\nhttps://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html"
      },
      {
        "date": "2022-09-08T18:49:00.000Z",
        "voteCount": 1,
        "content": "B - is testing the new deployment, the green env."
      },
      {
        "date": "2022-09-08T17:10:00.000Z",
        "voteCount": 2,
        "content": "Q - \"implement scripts to test the green version of the application before shifting traffic\"\nAns C - \"Use the AfterAllowTestTraffic lifecycle event to invoke an AWS Lambda function\"\n\nWhy is everyone picking the answer that tests the code \"AfterAllowTestTraffic\"?"
      },
      {
        "date": "2022-09-10T07:25:00.000Z",
        "voteCount": 2,
        "content": "Becaus it is \"Test\" traffic , not real traffic"
      },
      {
        "date": "2022-10-15T02:12:00.000Z",
        "voteCount": 1,
        "content": "yeah, It's ECS deployment and the question says \"test the green version of the application before shifting traffic\". In codedeploy ECS deployment has a hook \"AfterallowTestTraffic\" where we can invoke lambda. So the correct answer is C."
      },
      {
        "date": "2021-10-24T22:43:00.000Z",
        "voteCount": 2,
        "content": "C is correct answer"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 18,
    "url": "https://www.examtopics.com/discussions/amazon/view/48436-exam-aws-devops-engineer-professional-topic-1-question-18/",
    "body": "A company requires an RPO of 2 hours and an RTO of 10 minutes for its data and application at all times. An application uses a MySQL database and Amazon<br>EC2 web servers. The development team needs a strategy for failover and disaster recovery.<br>Which combination of deployment strategies will meet these requirements? (Choose two.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon Aurora cluster in one Availability Zone across multiple Regions as the data store. Use Aurora's automatic recovery capabilities in the event of a disaster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon Aurora global database in two Regions as the data store. In the event of a failure, promote the secondary Region as the master for the application.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon Aurora multi-master cluster across multiple Regions as the data store. Use a Network Load Balancer to balance the database traffic in different Regions.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up the application in two Regions and use Amazon Route 53 failover-based routing that points to the Application Load Balancers in both Regions. Use health checks to determine the availability in a given Region. Use Auto Scaling groups in each Region to adjust capacity based on demand.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up the application in two Regions and use a multi-Region Auto Scaling group behind Application Load Balancers to manage the capacity based on demand. In the event of a disaster, adjust the Auto Scaling group's desired instance count to increase baseline capacity in the failover Region."
    ],
    "answer": "BD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BD",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-10-02T12:35:00.000Z",
        "voteCount": 15,
        "content": "I'll go with B,D"
      },
      {
        "date": "2021-10-09T19:26:00.000Z",
        "voteCount": 5,
        "content": "B, D as well here"
      },
      {
        "date": "2023-03-19T16:34:00.000Z",
        "voteCount": 1,
        "content": "B &amp; D is good"
      },
      {
        "date": "2023-02-16T04:20:00.000Z",
        "voteCount": 1,
        "content": "I'll go with B,D"
      },
      {
        "date": "2023-02-04T04:10:00.000Z",
        "voteCount": 1,
        "content": "A was excluded, one AZ across multiple regions does not make senses.\nC was excluded, Network Load Balancer is not able to route traffics across regions.\nE needs much manual work, is not a good practice to failover."
      },
      {
        "date": "2022-12-31T22:44:00.000Z",
        "voteCount": 1,
        "content": "B &amp;D are the right answers"
      },
      {
        "date": "2022-03-02T06:24:00.000Z",
        "voteCount": 3,
        "content": "BD for sure"
      },
      {
        "date": "2022-02-10T11:59:00.000Z",
        "voteCount": 3,
        "content": "BD for me"
      },
      {
        "date": "2021-10-13T09:43:00.000Z",
        "voteCount": 2,
        "content": "Go with B &amp; D"
      },
      {
        "date": "2021-09-23T10:40:00.000Z",
        "voteCount": 2,
        "content": "I will go with B,D"
      },
      {
        "date": "2021-09-22T03:32:00.000Z",
        "voteCount": 2,
        "content": "I will go with B,D"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 19,
    "url": "https://www.examtopics.com/discussions/amazon/view/78641-exam-aws-devops-engineer-professional-topic-1-question-19/",
    "body": "An Amazon EC2 instance is running in a Virtual Private Cloud (VPC) and needs to download an object from a restricted Amazon S3 bucket. When the DevOps engineer tries to download, the object an AccessDenied error is received.<br>What are the possible causes for this error? (Choose two.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe S3 bucket default encryption is enabled",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThere is an error in the S3 bucket policy\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe object has been moved to Amazon Glacier",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThere is an error in the IAM role configuration\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tS3 versioning is enabled"
    ],
    "answer": "BD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BD",
        "count": 3,
        "isMostVoted": true
      },
      {
        "answer": "CD",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-09-02T01:43:00.000Z",
        "voteCount": 15,
        "content": "Ans: B, D\n\nA. The S3 bucket default encryption is enabled\nwill not trigger access denied\nB. There is an error in the S3 bucket policy\nyes\nC. The object has been moved to Amazon Glacier\nThe error should be \"Object is of storage class GLACIER. Unable to perform download operations on GLACIER objects. You must restore the object to be able to perform the operation.\"\nD. There is an error in the IAM role configuration\nyes\nE. S3 versioning is enabled\nwill not trigger access denied"
      },
      {
        "date": "2023-03-16T22:30:00.000Z",
        "voteCount": 1,
        "content": "B. There is an error in the S3 bucket policy: The S3 bucket policy could have been updated, or there might be an error in the existing policy that is preventing access to the object.\n\nD. There is an error in the IAM role configuration: The IAM role attached to the EC2 instance might not have the necessary permissions to access the S3 bucket."
      },
      {
        "date": "2023-02-04T04:29:00.000Z",
        "voteCount": 1,
        "content": "A is excluded, AccessDenied is not caused by enabled encryption.\nB is excluded, S3 bucket policy's error should throw out other errors.\nE is excluded, I am not sure why"
      },
      {
        "date": "2023-02-04T04:30:00.000Z",
        "voteCount": 1,
        "content": "I was wrong about C.  The explaining from devops7 is the one we need to read."
      },
      {
        "date": "2023-02-04T04:33:00.000Z",
        "voteCount": 1,
        "content": "This is a official source about downloading from Glacier https://docs.aws.amazon.com/amazonglacier/latest/dev/downloading-an-archive.html"
      },
      {
        "date": "2022-12-31T22:45:00.000Z",
        "voteCount": 1,
        "content": "B&amp;D is correct"
      },
      {
        "date": "2022-09-08T18:52:00.000Z",
        "voteCount": 1,
        "content": "the other answers still allow downloads to occur."
      },
      {
        "date": "2022-09-02T12:14:00.000Z",
        "voteCount": 1,
        "content": "B and D"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 20,
    "url": "https://www.examtopics.com/discussions/amazon/view/47013-exam-aws-devops-engineer-professional-topic-1-question-20/",
    "body": "A DevOps team manages an API running on-premises that serves as a backend for an Amazon API Gateway endpoint. Customers have been complaining about high response latencies, which the development team has verified using the API Gateway latency metrics in Amazon CloudWatch. To identify the cause, the team needs to collect relevant data without introducing additional latency.<br>Which actions should be taken to accomplish this? (Choose two.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall the CloudWatch agent server side and configure the agent to upload relevant logs to CloudWatch.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable AWS X-Ray tracing in API Gateway, modify the application to capture request segments, and upload those segments to X-Ray during each request.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable AWS X-Ray tracing in API Gateway, modify the application to capture request segments, and use the X-Ray daemon to upload segments to X-Ray.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the on-premises application to send log information back to API Gateway with each request.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the on-premises application to calculate and upload statistical data relevant to the API service requests to CloudWatch metrics."
    ],
    "answer": "AC",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AC",
        "count": 16,
        "isMostVoted": true
      },
      {
        "answer": "CE",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "AB",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-10-27T22:26:00.000Z",
        "voteCount": 24,
        "content": "I'll go with A and C\n\nReferences: \nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/install-CloudWatch-Agent-on-premise.html\nhttps://docs.aws.amazon.com/xray/latest/devguide/xray-api-sendingdata.html"
      },
      {
        "date": "2021-10-31T21:56:00.000Z",
        "voteCount": 4,
        "content": "Agreed. B is sub-optimal because sending every xray segment back to AWS API directly from the application will introduce more latency. Having a local x-ray daemon mitigates that, so C is better."
      },
      {
        "date": "2022-01-22T16:15:00.000Z",
        "voteCount": 4,
        "content": "Installing CloudWatch agents on on-prem servers will provider server metrics. How that will help calculate API latency? Ans A doesn't make sense. Ans E would rather help getting latency information.\nAns C, E"
      },
      {
        "date": "2023-03-20T16:02:00.000Z",
        "voteCount": 2,
        "content": "Having the on-prem instance 'calculate' and send statistical data is more work for the nodes, which makes matters worse. Instead, ship all logs to a log server (CW Logs) and run statistics from there (CW Logs Insights or Metric Filters)"
      },
      {
        "date": "2022-12-11T08:45:00.000Z",
        "voteCount": 9,
        "content": "To identify the cause of the high response latencies, the DevOps team should:\n\nInstall the CloudWatch agent server side and configure the agent to upload relevant logs to CloudWatch. This will allow the team to collect log data without introducing additional latency.\nEnable AWS X-Ray tracing in API Gateway, modify the application to capture request segments, and use the X-Ray daemon to upload segments to X-Ray. This will allow the team to collect tracing data without introducing additional latency.\nTherefore, the correct actions to take are A and C.\n\nOption B is incorrect because uploading request segments to X-Ray during each request would introduce additional latency.\n\nOption D is incorrect because sending log information back to API Gateway with each request would introduce additional latency.\n\nOption E is incorrect because calculating and uploading statistical data relevant to the API service requests to CloudWatch metrics would introduce additional latency."
      },
      {
        "date": "2023-07-01T11:33:00.000Z",
        "voteCount": 1,
        "content": "AC es la bonne r\u00e9ponse"
      },
      {
        "date": "2023-03-16T22:34:00.000Z",
        "voteCount": 2,
        "content": "A. Install the CloudWatch agent server-side and configure the agent to upload relevant logs to CloudWatch is a valid action to collect relevant data. This action will collect logs and other relevant system data from the on-premises API, which can then be analyzed in CloudWatch Logs Insights to identify the cause of the high response latencies.\n\nB. Enable AWS X-Ray tracing in API Gateway, modify the application to capture request segments, and upload those segments to X-Ray during each request is another valid action. This will allow the DevOps team to trace the request as it passes through the system and identify which segments are causing the high response latencies.\n\nTherefore, the correct options are A and B."
      },
      {
        "date": "2023-02-18T00:10:00.000Z",
        "voteCount": 1,
        "content": "A and C for me\nE introduces more latency"
      },
      {
        "date": "2023-02-16T04:22:00.000Z",
        "voteCount": 1,
        "content": "I'll go with A and C"
      },
      {
        "date": "2023-02-15T09:14:00.000Z",
        "voteCount": 1,
        "content": "Enabling X-Ray tracing in the API Gateway console\nYou can use the Amazon API Gateway console to enable active tracing on an API stage.\n\nThese steps assume that you have already deployed the API to a stage.\n\nSign in to the API Gateway console at https://console.aws.amazon.com/apigateway.\n\nIn the APIs pane, choose the API, and then choose Stages.\n\nIn the Stages pane, choose the name of the stage.\n\nIn the Stage Editor pane, choose the Logs/Tracing tab.\n\nTo enable active X-Ray tracing, choose Enable X-Ray Tracing under X-Ray Tracing.\n\nIf desired, choose Set X-Ray Sampling Rules and go to the X-Ray console to configure sampling rules.\n\nOnce you've enabled X-Ray for your API stage, you can use the X-Ray management console to view the traces and service maps.\nNothing shows here that you should use an x-ray daemon everything is collected automatically with proper service role attached on API gateway, not sure C is one of the correct answer."
      },
      {
        "date": "2023-02-04T04:42:00.000Z",
        "voteCount": 1,
        "content": "B, D and E will increase the latency."
      },
      {
        "date": "2023-01-15T17:11:00.000Z",
        "voteCount": 1,
        "content": "The question is about latency and not logging/monitoring. As a result, X-ray is the right answer and not Cloudwatch logs for transaction tracing across API Gateway and on-prem application hosting the API. So the correct answer is C,D. I will go with C and D."
      },
      {
        "date": "2022-12-31T23:16:00.000Z",
        "voteCount": 1,
        "content": "A and C is the correct answer!"
      },
      {
        "date": "2022-11-18T08:45:00.000Z",
        "voteCount": 2,
        "content": "A. After you install the cloudwatch agent, then it will provide the metrics but cannot use them to get the latency information"
      },
      {
        "date": "2022-09-17T09:09:00.000Z",
        "voteCount": 1,
        "content": "A and C for sure"
      },
      {
        "date": "2022-09-11T19:35:00.000Z",
        "voteCount": 1,
        "content": "A &amp; C.  high response latencies could happen when your resource is full"
      },
      {
        "date": "2022-09-08T18:54:00.000Z",
        "voteCount": 1,
        "content": "the only 2 options that dont cause the application to incur more processing."
      },
      {
        "date": "2022-09-05T21:38:00.000Z",
        "voteCount": 1,
        "content": "Ans is : CE"
      },
      {
        "date": "2022-03-04T23:26:00.000Z",
        "voteCount": 1,
        "content": "A for metrics and C for performance data"
      },
      {
        "date": "2022-02-22T03:20:00.000Z",
        "voteCount": 1,
        "content": "It's actually CE\nThere's no such thing as \"CloudWatch agent server side\"\n\nIn comments people mention that E adds latency to application. If you've ever worked with Prometheus them you're aware that is different module of code and it depends meddle with request processing."
      },
      {
        "date": "2023-02-04T04:45:00.000Z",
        "voteCount": 1,
        "content": "There is a such thing as \"CloudWatch agent server side\"\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/install-CloudWatch-Agent-on-premise.html"
      },
      {
        "date": "2023-07-03T07:56:00.000Z",
        "voteCount": 1,
        "content": "that's just cloudwatch agent. server side as a name is not there."
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 21,
    "url": "https://www.examtopics.com/discussions/amazon/view/47585-exam-aws-devops-engineer-professional-topic-1-question-21/",
    "body": "A DevOps engineer wants to find a solution to migrate an application from on premises to AWS. The application is running on Linux and needs to run on specific versions of Apache Tomcat, HAProxy, and Varnish Cache to function properly. The application's operating system-level parameters require tuning. The solution must include a way to automate the deployment of new application versions. The infrastructure should be scalable and faulty servers should be replaced automatically.<br>Which solution should the DevOps engineer use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpload the application as a Docker image that contains all the necessary software to Amazon ECR. Create an Amazon ECS cluster using an AWS Fargate launch type and an Auto Scaling group. Create an AWS CodePipeline pipeline that uses Amazon ECR as a source and Amazon ECS as a deployment provider.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpload the application code to an AWS CodeCommit repository with a saved configuration file to configure and install the software. Create an AWS Elastic Beanstalk web server tier and a load balanced-type environment that uses the Tomcat solution stack. Create an AWS CodePipeline pipeline that uses CodeCommit as a source and Elastic Beanstalk as a deployment provider.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpload the application code to an AWS CodeCommit repository with a set of .ebextensions files to configure and install the software. Create an AWS Elastic Beanstalk worker tier environment that uses the Tomcat solution stack. Create an AWS CodePipeline pipeline that uses CodeCommit as a source and Elastic Beanstalk as a deployment provider.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpload the application code to an AWS CodeCommit repository with an appspec.yml file to configure and install the necessary software. Create an AWS CodeDeploy deployment group associated with an Amazon EC2 Auto Scaling group. Create an AWS CodePipeline pipeline that uses CodeCommit as a source and CodeDeploy as a deployment provider.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 21,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 10,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-10-26T03:44:00.000Z",
        "voteCount": 15,
        "content": "D - there is nothing like Fargate AutoScaling group - this is serverless approach"
      },
      {
        "date": "2021-10-28T17:58:00.000Z",
        "voteCount": 5,
        "content": "It should be A. I think none of the other options really address the tunning issues properly. There is a \"Service Auto Scaling\" on Fargate, but not an \"Auto Scaling Group\", the wording on this question must be wrong. -&gt; https://aws.amazon.com/premiumsupport/knowledge-center/ecs-fargate-service-auto-scaling/"
      },
      {
        "date": "2021-10-10T08:02:00.000Z",
        "voteCount": 12,
        "content": "I'll go with A) \n\nCreating a Docker image of these very customized applications is always a good idea\n\nCreating a docker image with all necessary apps, data e customizations attend the requirements\n\nB, C and D doesn't mention the required customizations of app versions or OS changes. None of them mention an AMI creation"
      },
      {
        "date": "2021-11-07T04:54:00.000Z",
        "voteCount": 3,
        "content": "I upvoted but then checked the other comments. MBJames: \"Fargate does not allow for OS-level access which means OS parameter tuning would not be an option.\" So option A is ruled out. With option D,  EC2 instances are accessible and can be tuned."
      },
      {
        "date": "2021-10-31T18:10:00.000Z",
        "voteCount": 2,
        "content": "How do you address this case with Fargate?  \"The application\u05d2\u20ac\u2122s operating system-level parameters require tuning\""
      },
      {
        "date": "2024-04-07T09:50:00.000Z",
        "voteCount": 1,
        "content": "you can address it with a task definition"
      },
      {
        "date": "2024-08-07T01:40:00.000Z",
        "voteCount": 1,
        "content": "Not always, it depends on the os parameters we'll need to define. Since Fargate's infra is managed by aws, there's very limited freedom when configuring the OS (u can t for example schedule a cronjob in a fargate) so might as well not choose A.\nThen by elimination, D looked like the best option. It had me at Autoscaling group, and the qst wants a scalable fault tolerant solution"
      },
      {
        "date": "2021-11-04T14:29:00.000Z",
        "voteCount": 13,
        "content": "Having Apache Tomcat + Varnish + HAProxy on a single Docker image is a container-level suicide. Hence D is the best option here."
      },
      {
        "date": "2023-03-21T10:58:00.000Z",
        "voteCount": 3,
        "content": "I don't believe it makes sense to put a web server and a cache and a load balancer all on the same instance or image. We're talking separate instances/containers for each function. The Apache would have it's own container, Varnish fronts the server for caching and HAProxy fronts them all for load balancing. Have separate containers and task definitions for each function and scale accordingly. You wouldn't want to scale the load balancer nodes the same way you do the web servers."
      },
      {
        "date": "2024-01-28T04:04:00.000Z",
        "voteCount": 1,
        "content": "can't be D because if you install system dependencies using appspec.yml they won't be present when new instance is added to ASG target group\nB is maybe technically possible to reinstall major dependencies using .ebextensions but it's terrible\nA is great option, but there is a typo i guess with auto scaling group"
      },
      {
        "date": "2023-12-26T20:29:00.000Z",
        "voteCount": 1,
        "content": "Only ECS  , follow all the conditional laid in the question"
      },
      {
        "date": "2023-10-29T18:49:00.000Z",
        "voteCount": 1,
        "content": "I will go with D since Fargate wont give you much more flexibility to configure those OS level params"
      },
      {
        "date": "2023-10-17T06:22:00.000Z",
        "voteCount": 2,
        "content": "It is A, for those questioning how to do parameter tuning for OS, you can do it via the task definition for ECS Fargate, also about Autoscaling you create an Autoscaling group and associate it with your managed Fargate service."
      },
      {
        "date": "2023-07-03T08:16:00.000Z",
        "voteCount": 1,
        "content": "A is the correct answer. As the apps require specific versions fof Tomcat, HAProxy and Varnish Cache, a devops engineer can create docker images based on specific version of their respective images from dockerhub or any other repo then configure environment variables / CMDs that need to be run at the startup of every container. Fargate can be used to launch those containers in a serverless way and any failing container for the above servers  can be replaced automatically."
      },
      {
        "date": "2023-09-25T19:23:00.000Z",
        "voteCount": 1,
        "content": "what about the OS-level tuning ?"
      },
      {
        "date": "2023-03-19T16:48:00.000Z",
        "voteCount": 2,
        "content": "D looks like correct"
      },
      {
        "date": "2023-03-05T12:31:00.000Z",
        "voteCount": 2,
        "content": "A is incorrect since OS level parameters can not be done in ECS Fargate, If it was EC2 ECS then we could change the parameters"
      },
      {
        "date": "2023-02-25T16:19:00.000Z",
        "voteCount": 2,
        "content": "I will go with A, when we have different application ECS is the best solution"
      },
      {
        "date": "2023-02-19T22:30:00.000Z",
        "voteCount": 2,
        "content": "B. Upload the application code to an AWS CodeCommit repository with a saved configuration file to configure and install the software. Create an AWS Elastic Beanstalk web server tier and a load-balanced type environment that uses the Tomcat solution stack. Create an AWS CodePipeline pipeline that uses CodeCommit as a source and Elastic Beanstalk as a deployment provider.\n\nThis solution provides the necessary level of control to configure the application's operating system-level parameters, as well as the ability to use specific versions of Apache Tomcat, HAProxy, and Varnish Cache. The solution also includes automated deployment of new versions through the use of AWS CodePipeline and scalability through the use of AWS Elastic Beanstalk and its ability to automatically replace faulty instances."
      },
      {
        "date": "2023-02-16T05:26:00.000Z",
        "voteCount": 1,
        "content": "I'll go with D"
      },
      {
        "date": "2023-02-04T05:03:00.000Z",
        "voteCount": 1,
        "content": "A is excluded, AWS Fargate launch does not meet requirement, because the application will be migrated from on premises to AWS\nB and C highlight only Tomcat solution stack. I do not understand why."
      },
      {
        "date": "2023-02-04T05:10:00.000Z",
        "voteCount": 1,
        "content": "I changed my answer from D to A after I read through https://docs.aws.amazon.com/AmazonECS/latest/userguide/what-is-fargate.html"
      },
      {
        "date": "2023-01-15T17:17:00.000Z",
        "voteCount": 2,
        "content": "Correct answer is D. Key is automated scaling and ability to replace faulty instances with a new one. Only a combination of CodeDeploy and Autoscaling group for EC2 as the target will satisfy this requirement."
      },
      {
        "date": "2022-12-22T15:10:00.000Z",
        "voteCount": 1,
        "content": "D for sure"
      },
      {
        "date": "2022-11-08T20:44:00.000Z",
        "voteCount": 4,
        "content": "A - Not good to contain all software in one container. \nB -  Not supporting all servers. \nC - Same as above. \nD - Can configure multiple servers including OS level tuning, ASG, Code Deploy. Fulfilling all key requirements."
      },
      {
        "date": "2022-09-11T01:56:00.000Z",
        "voteCount": 7,
        "content": "The application's operating system-level parameters require tuning.\n&gt;EC2\nThe solution must include a way to automate the deployment of new application versions. \n&gt;CodePipeline\nThe infrastructure should be scalable and faulty servers should be replaced automatically.\n&gt;ASG"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 22,
    "url": "https://www.examtopics.com/discussions/amazon/view/47587-exam-aws-devops-engineer-professional-topic-1-question-22/",
    "body": "A company wants to use AWS CloudFormation for infrastructure deployment. The company has strict tagging and resource requirements and wants to limit the deployment to two Regions. Developers will need to deploy multiple versions of the same application.<br>Which solution ensures resources are deployed in accordance with company policy?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate AWS Trusted Advisor checks to find and remediate unapproved CloudFormation StackSets.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a CloudFormation drift detection operation to find and remediate unapproved CloudFormation StackSets.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate CloudFormation StackSets with approved CloudFormation templates.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate AWS Service Catalog products with approved CloudFormation templates.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 21,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 12,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-10-14T01:19:00.000Z",
        "voteCount": 25,
        "content": "The answer is D. https://docs.aws.amazon.com/servicecatalog/latest/adminguide/catalogs_constraints_template-constraints.html"
      },
      {
        "date": "2021-10-24T20:43:00.000Z",
        "voteCount": 3,
        "content": "Agreed, D is the best way to enforce compliance in the scenario."
      },
      {
        "date": "2022-01-08T15:59:00.000Z",
        "voteCount": 5,
        "content": "AWS Service Catalog uses CloudFormation templates and StackSets under the hood to provide a wrapper. Since there are no cost constraints here, ServiceCatalog is a better approach:\nhttps://aws.amazon.com/blogs/mt/how-to-set-up-a-multi-region-multi-account-catalog-of-company-standard-aws-service-catalog-products/"
      },
      {
        "date": "2021-10-01T03:16:00.000Z",
        "voteCount": 12,
        "content": "I'll go with C"
      },
      {
        "date": "2024-05-13T22:44:00.000Z",
        "voteCount": 1,
        "content": "D because compliance must always be declarative."
      },
      {
        "date": "2023-10-02T09:12:00.000Z",
        "voteCount": 1,
        "content": "Answer is C : https://saturncloud.io/blog/how-to-create-a-cloudformation-stack-with-resources-in-multiple-regions/"
      },
      {
        "date": "2023-07-15T13:21:00.000Z",
        "voteCount": 1,
        "content": "The Answer is C: https://docs.aws.amazon.com/servicecatalog/latest/adminguide/catalogs_constraints_template-constraints.html"
      },
      {
        "date": "2023-03-17T21:14:00.000Z",
        "voteCount": 1,
        "content": "Yes its C.\nOption D suggests creating AWS Service Catalog products with approved CloudFormation templates, which is a valid option for deploying resources in accordance with company policy. However, the question specifically mentions using AWS CloudFormation for infrastructure deployment, and using AWS Service Catalog may not be the best fit for this scenario."
      },
      {
        "date": "2023-03-11T09:02:00.000Z",
        "voteCount": 1,
        "content": "I'm going for C baby."
      },
      {
        "date": "2023-03-11T09:05:00.000Z",
        "voteCount": 2,
        "content": "D baby. Fat fingers."
      },
      {
        "date": "2023-03-11T09:05:00.000Z",
        "voteCount": 2,
        "content": "It's D because of need for Tags."
      },
      {
        "date": "2023-02-18T21:17:00.000Z",
        "voteCount": 3,
        "content": "service catalog uses stacksets and can enforce tag and restrict resources\nAWS Customer case with tag enforcement\nhttps://aws.amazon.com/ko/blogs/apn/enforce-centralized-tag-compliance-using-aws-service-catalog-amazon-dynamodb-aws-lambda-and-amazon-cloudwatch-events/\nAnd Youtube video showing how to restrict resources per user with portfolio\nhttps://www.youtube.com/watch?v=LzvhTcqqyog"
      },
      {
        "date": "2023-02-13T06:14:00.000Z",
        "voteCount": 3,
        "content": "D\nhttps://aws.amazon.com/blogs/mt/how-to-set-up-a-multi-region-multi-account-catalog-of-company-standard-aws-service-catalog-products/"
      },
      {
        "date": "2023-02-09T22:19:00.000Z",
        "voteCount": 1,
        "content": "stack sets doesnt restrict users on tagging and other stuff. Service catalog is the perfect usecase here."
      },
      {
        "date": "2023-02-04T05:22:00.000Z",
        "voteCount": 2,
        "content": "A and B are excluded because it's not a good idea to detect. \nD looks weird."
      },
      {
        "date": "2023-01-31T08:35:00.000Z",
        "voteCount": 1,
        "content": "I will go with D"
      },
      {
        "date": "2023-01-16T00:54:00.000Z",
        "voteCount": 5,
        "content": "D. Create AWS Service Catalog products with approved CloudFormation templates.\n\nAWS Service Catalog allows the company to create, manage, and distribute approved products in the form of CloudFormation templates. It allows the company to control the use of AWS resources by imposing constraints on the templates and products available, such as resource tagging, regions, and IAM permissions.\n\nUsing AWS Service Catalog, the company can define products with specific CloudFormation templates and constraints, and allow developers to deploy the appropriate version of the application, while ensuring that the deployment adheres to company policies and resource requirements. Additionally, AWS Service Catalog can be used to restrict the deployment to specific regions, and by creating products, it allows the company to keep track of the versions of the application that are deployed."
      },
      {
        "date": "2023-01-15T17:24:00.000Z",
        "voteCount": 1,
        "content": "The answer is D. The best way to impose template and StackSet constraints is through Service Catalog. https://docs.aws.amazon.com/servicecatalog/latest/adminguide/catalogs_constraints_template-constraints.htmlns"
      },
      {
        "date": "2023-01-09T08:32:00.000Z",
        "voteCount": 2,
        "content": "D because there is no such thing as \u201capproved Cloudformation Templates\u201d"
      },
      {
        "date": "2022-12-27T10:44:00.000Z",
        "voteCount": 2,
        "content": "Ans =  D:\nhttps://docs.aws.amazon.com/servicecatalog/latest/adminguide/constraints-stackset.html"
      },
      {
        "date": "2022-12-22T15:17:00.000Z",
        "voteCount": 1,
        "content": "I'll go with D"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 23,
    "url": "https://www.examtopics.com/discussions/amazon/view/8034-exam-aws-devops-engineer-professional-topic-1-question-23/",
    "body": "A DevOps Engineer must track the health of a stateless RESTful service sitting behind a Classic Load Balancer. The deployment of new application revisions is through a CI/CD pipeline. If the service's latency increases beyond a defined threshold, deployment should be stopped until the service has recovered.<br>Which of the following methods allow for the QUICKEST detection time?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon CloudWatch metrics provided by Elastic Load Balancing to calculate average latency. Alarm and stop deployment when latency increases beyond the defined threshold.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Lambda and Elastic Load Balancing access logs to detect average latency. Alarm and stop deployment when latency increases beyond the defined threshold.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS CodeDeploy's MinimumHealthyHosts setting to define thresholds for rolling back deployments. If these thresholds are breached, roll back the deployment.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Metric Filters to parse application logs in Amazon CloudWatch Logs. Create a filter for latency. Alarm and stop deployment when latency increases beyond the defined threshold."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 10,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-10-03T16:05:00.000Z",
        "voteCount": 22,
        "content": "A:\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-cloudwatch-metrics.html\nhttps://docs.aws.amazon.com/codedeploy/latest/userguide/deployments-stop.html"
      },
      {
        "date": "2021-10-20T20:21:00.000Z",
        "voteCount": 2,
        "content": "Perfect answer with explanation."
      },
      {
        "date": "2022-10-10T01:46:00.000Z",
        "voteCount": 1,
        "content": "but you need the deployment ID to stop a deployment, so you mean you have to get it before with list-deployments command?"
      },
      {
        "date": "2021-10-22T06:51:00.000Z",
        "voteCount": 10,
        "content": "A - correct\nB - this might work but has additonal overhead of a lambda and will depende on how frequently lambda is run. Although minimal but \n    additonal cost of lambda. This won't give QUICKEST detection time.\nC - MinimumHealthyHosts may not be directly correlated with latency. Latency might be more due to network or other issues even if 100% of hosts are healthy.\nD - why do this when a ready made option is available https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-cloudwatch-metrics.html"
      },
      {
        "date": "2023-12-26T20:41:00.000Z",
        "voteCount": 1,
        "content": "A is the right answer , since it is readily available , we don't have to spend time creating the metrics from logs"
      },
      {
        "date": "2023-09-08T06:47:00.000Z",
        "voteCount": 1,
        "content": "Vote for A"
      },
      {
        "date": "2023-06-15T00:24:00.000Z",
        "voteCount": 1,
        "content": "We need QUICKEST detection time, not easiest deployment. Option A can only do per minute. Option D can go for second-level detection."
      },
      {
        "date": "2023-05-10T13:56:00.000Z",
        "voteCount": 2,
        "content": "Which of the following methods allow for the QUICKEST detection time?\nAmazon CloudWatch Metrics: By default, CloudWatch metrics have a minimum resolution of 1 minute. This means that data points for metrics are collected at one-minute intervals. However, you can enable high-resolution custom metrics with a resolution of 1 second.\n\nAmazon CloudWatch Logs: CloudWatch Logs does not have a predefined minimum interval for log data collection. Log events can be sent to CloudWatch Logs in near real-time, allowing for timely analysis and monitoring."
      },
      {
        "date": "2023-03-17T21:18:00.000Z",
        "voteCount": 1,
        "content": "This method allows for the quickest detection time since it relies on CloudWatch metrics provided by Elastic Load Balancing. These metrics are already being collected and aggregated by AWS, so there is no need to set up additional monitoring infrastructure. By creating a CloudWatch alarm on the average latency metric and setting a threshold, the DevOps Engineer can quickly detect when latency increases beyond a defined threshold and stop deployment until the service has recovered."
      },
      {
        "date": "2023-02-25T16:35:00.000Z",
        "voteCount": 1,
        "content": "A- is Correct, it is quick as metrics could be updated every 1 minutes \nC- cannot be the answer https://docs.aws.amazon.com/codedeploy/latest/APIReference/API_MinimumHealthyHosts.html\nwhen we are talking about delay the loadbalancer  could consider that endpoint as healthy and deployment will go through!"
      },
      {
        "date": "2023-02-19T23:03:00.000Z",
        "voteCount": 1,
        "content": "A. Use Amazon CloudWatch metrics provided by Elastic Load Balancing to calculate average latency. Alarm and stop deployment when latency increases beyond the defined threshold is the method that allows for the quickest detection time.\n\nWhen using Amazon CloudWatch metrics provided by Elastic Load Balancing, metrics are collected every minute, which is the most frequent metric collection interval available. This means that the latency metric will be updated frequently, allowing for a quick detection time when latency increases beyond the defined threshold.\n\nIn contrast, the other options may have longer detection times as they involve additional steps such as collecting logs or waiting for a certain number of healthy hosts before rolling back a deployment."
      },
      {
        "date": "2023-02-07T11:19:00.000Z",
        "voteCount": 1,
        "content": "the answer is A"
      },
      {
        "date": "2023-02-04T06:02:00.000Z",
        "voteCount": 1,
        "content": "A is the most straightforward solution.\nB: Lambda and Elastic Load Balancing, is manual solution.\nC and D look weird"
      },
      {
        "date": "2023-01-15T19:44:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is A. When setting up CodeDeploy as a Deployment Provider you can specify the alarm based on a CloudWatch Metric (custom or built-in) and stop the deployment if an alarm threshold is exceeded."
      },
      {
        "date": "2022-11-03T05:08:00.000Z",
        "voteCount": 2,
        "content": "A IS THE CORRECT ONE"
      },
      {
        "date": "2022-10-11T00:11:00.000Z",
        "voteCount": 2,
        "content": "I'll go with A"
      },
      {
        "date": "2022-08-31T21:07:00.000Z",
        "voteCount": 2,
        "content": "A is the way to go here"
      },
      {
        "date": "2022-02-12T14:06:00.000Z",
        "voteCount": 2,
        "content": "A, existing metric would be faster than going over the fuss of creating a new one"
      },
      {
        "date": "2021-11-04T16:26:00.000Z",
        "voteCount": 2,
        "content": "I also vote for A."
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 24,
    "url": "https://www.examtopics.com/discussions/amazon/view/8320-exam-aws-devops-engineer-professional-topic-1-question-24/",
    "body": "An AWS CodePipeline pipeline has implemented a code release process. The pipeline is integrated with AWS CodeDeploy to deploy versions of an application to multiple Amazon EC2 instances for each CodePipeline stage.<br>During a recent deployment, the pipeline failed due to a CodeDeploy issue. The DevOps team wants to improve monitoring and notifications during deployment to decrease resolution times.<br>What should the DevOps Engineer do to create notifications when issues are discovered?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement AWS CloudWatch Logs for CodePipeline and CodeDeploy, create an AWS Config rule to evaluate code deployment issues, and create an Amazon SNS topic to notify stakeholders of deployment issues.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement AWS CloudWatch Events for CodePipeline and CodeDeploy, create an AWS Lambda function to evaluate code deployment issues, and create an Amazon SNS topic to notify stakeholders of deployment issues.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement AWS CloudTrail to record CodePipeline and CodeDeploy API call information, create an AWS Lambda function to evaluate code deployment issues, and create an Amazon SNS topic to notify stakeholders of deployment issues.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement AWS CloudWatch Events for CodePipeline and CodeDeploy, create an Amazon Inspector assessment target to evaluate code deployment issues, and create an Amazon SNS topic to notify stakeholders of deployment issues."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 8,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-09-20T13:08:00.000Z",
        "voteCount": 15,
        "content": "I think the answer is B"
      },
      {
        "date": "2021-09-23T01:09:00.000Z",
        "voteCount": 1,
        "content": "I agree."
      },
      {
        "date": "2021-11-05T00:22:00.000Z",
        "voteCount": 6,
        "content": "I'll go with B"
      },
      {
        "date": "2023-08-01T15:56:00.000Z",
        "voteCount": 1,
        "content": "I think B"
      },
      {
        "date": "2023-03-17T21:21:00.000Z",
        "voteCount": 2,
        "content": "AWS CloudWatch Events can be used to monitor events across different AWS resources, and a CloudWatch Event Rule can be created to trigger an AWS Lambda function when a deployment issue is detected in the pipeline. The Lambda function can then evaluate the issue and send a notification to the appropriate stakeholders through an Amazon SNS topic. This approach allows for real-time notifications and faster resolution times."
      },
      {
        "date": "2023-03-01T09:02:00.000Z",
        "voteCount": 1,
        "content": "B looks good."
      },
      {
        "date": "2023-02-04T06:11:00.000Z",
        "voteCount": 1,
        "content": "A is the most managed approach."
      },
      {
        "date": "2023-02-04T06:15:00.000Z",
        "voteCount": 1,
        "content": "Sorry, I was wrong with A.  AWS Config is to assess configuration, does not help to evaluate code deployment issues."
      },
      {
        "date": "2023-01-15T20:07:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is B. Using CloudWatch event, we can detect CodeDeploy State changes like Failures and then chose Lambda to determine the nature of the Deployment failure and have the root cause sent to SNS which can send an email to the subscribers."
      },
      {
        "date": "2022-12-22T15:28:00.000Z",
        "voteCount": 1,
        "content": "It is B"
      },
      {
        "date": "2022-12-09T00:09:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/codepipeline/latest/userguide/monitoring-cloudtrail-logs.html"
      },
      {
        "date": "2022-09-11T02:08:00.000Z",
        "voteCount": 4,
        "content": "You can use Amazon CloudWatch Events to detect and react to changes in the state of an instance or a deployment (an \"event\") in your CodeDeploy operations. Then, based on rules you create, CloudWatch Events will invoke one or more target actions when a deployment or instance enters the state you specify in a rule. Depending on the type of state change, you might want to send notifications, capture state information, take corrective action, initiate events, or take other actions. You can select the following types of targets when using CloudWatch Events as part of your CodeDeploy operations:\n\nAWS Lambda functions\n\nKinesis streams\n\nAmazon SQS queues\n\nBuilt-in targets (EC2 CreateSnapshot API call, EC2 RebootInstances API call, EC2 StopInstances API call , and EC2 TerminateInstances API call)\n\nAmazon SNS topics\n\nhttps://docs.aws.amazon.com/codedeploy/latest/userguide/monitoring-cloudwatch-events.html"
      },
      {
        "date": "2021-11-06T04:14:00.000Z",
        "voteCount": 2,
        "content": "B it is"
      },
      {
        "date": "2021-10-29T23:56:00.000Z",
        "voteCount": 2,
        "content": "B is the right one."
      },
      {
        "date": "2021-10-25T19:53:00.000Z",
        "voteCount": 1,
        "content": "I go with B"
      },
      {
        "date": "2021-10-22T11:43:00.000Z",
        "voteCount": 1,
        "content": "I  will  stick  with  B."
      },
      {
        "date": "2021-10-17T23:17:00.000Z",
        "voteCount": 1,
        "content": "Answer is ABE"
      },
      {
        "date": "2021-10-18T11:51:00.000Z",
        "voteCount": 1,
        "content": "This is actually for question 99"
      },
      {
        "date": "2021-10-14T18:37:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2021-10-06T19:08:00.000Z",
        "voteCount": 1,
        "content": "B looks right"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 25,
    "url": "https://www.examtopics.com/discussions/amazon/view/47016-exam-aws-devops-engineer-professional-topic-1-question-25/",
    "body": "A development team is using AWS CodeCommit to version control application code and AWS CodePipeline to orchestrate software deployments. The team has decided to use a remote master branch as the trigger for the pipeline to integrate code changes. A developer has pushed code changes to the CodeCommit repository, but noticed that the pipeline had no reaction, even after 10 minutes.<br>Which of the following actions should be taken to troubleshoot this issue?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCheck that an Amazon CloudWatch Events rule has been created for the master branch to trigger the pipeline.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCheck that the CodePipeline service role has permission to access the CodeCommit repository.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCheck that the developer's IAM role has permission to push to the CodeCommit repository.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCheck to see if the pipeline failed to start because of CodeCommit errors in Amazon CloudWatch Logs."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 6,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-10-08T20:37:00.000Z",
        "voteCount": 18,
        "content": "I'll go with A\nWhen you create a pipeline from CodePipeline during the step-by-step it creates a CloudWatch Event rule for a given branch and repo\nlike this:\n{\n  \"source\": [\n    \"aws.codecommit\"\n  ],\n  \"detail-type\": [\n    \"CodeCommit Repository State Change\"\n  ],\n  \"resources\": [\n    \"arn:aws:codecommit:us-east-1:xxxxx:repo-name\"\n  ],\n  \"detail\": {\n    \"event\": [\n      \"referenceCreated\",\n      \"referenceUpdated\"\n    ],\n    \"referenceType\": [\n      \"branch\"\n    ],\n    \"referenceName\": [\n      \"master\"\n    ]\n  }\n}"
      },
      {
        "date": "2023-02-04T06:33:00.000Z",
        "voteCount": 1,
        "content": "You are amazing."
      },
      {
        "date": "2021-10-01T06:33:00.000Z",
        "voteCount": 6,
        "content": "A\nhttps://docs.aws.amazon.com/codepipeline/latest/userguide/pipelines-trigger-source-repo-changes-console.html"
      },
      {
        "date": "2024-01-29T06:04:00.000Z",
        "voteCount": 1,
        "content": "I'll go with B\nit's necessary for codepipeline to have codecommit permissions while it's not necessary to depend on a rule to start a pipeline"
      },
      {
        "date": "2023-05-10T14:29:00.000Z",
        "voteCount": 1,
        "content": "it is not necessary to create an Amazon CloudWatch Events rule specifically for triggering the pipeline in this scenario. The CodePipeline can be set up to directly monitor the CodeCommit repository for changes without the need for an additional CloudWatch Events rule.\nCreating an Amazon CloudWatch Events rule is typically required when you want to trigger an action based on events occurring in AWS services. However, in this case, CodePipeline is already integrated with CodeCommit and can directly monitor the repository for changes without the need for an additional CloudWatch Events rule.\n\nWhen you set up an AWS CodePipeline, the pipeline's execution events and logs are automatically sent to Amazon CloudWatch Logs. CodePipeline integrates with CloudWatch Logs to capture and store the logs for pipeline executions."
      },
      {
        "date": "2023-03-17T21:23:00.000Z",
        "voteCount": 1,
        "content": "Since the pipeline is triggered by changes in the remote master branch, an Amazon CloudWatch Events rule must be created for the branch to trigger the pipeline. Therefore, the first step in troubleshooting the issue should be to check that an Amazon CloudWatch Events rule has been created for the master branch to trigger the pipeline."
      },
      {
        "date": "2023-02-04T06:32:00.000Z",
        "voteCount": 3,
        "content": "C was excluded in the first round. The developer's IAM role's permission should not be related.\nD looks unrelated.\nB is weird."
      },
      {
        "date": "2023-01-15T20:09:00.000Z",
        "voteCount": 1,
        "content": "Answer A is correct."
      },
      {
        "date": "2022-12-28T13:36:00.000Z",
        "voteCount": 1,
        "content": "A:\nhttps://docs.aws.amazon.com/codepipeline/latest/userguide/triggering.html\n\nat the begining I though it was B, but If it were a permission problem you might have had an error pointing to it."
      },
      {
        "date": "2022-12-14T15:29:00.000Z",
        "voteCount": 1,
        "content": "\"the pipeline had no reaction\" =&gt; pipeline is not run, so BCD rule out"
      },
      {
        "date": "2022-11-19T08:48:00.000Z",
        "voteCount": 1,
        "content": "Both A &amp; B are possible to have issue and need to check"
      },
      {
        "date": "2022-09-17T17:54:00.000Z",
        "voteCount": 1,
        "content": "I think B is also correct. As a pipeline trigger, it can be triggered by cloudwatch event, or it can  periodically check the repository for update. If latter, then B is also correct."
      },
      {
        "date": "2022-11-09T18:07:00.000Z",
        "voteCount": 1,
        "content": "When you use the CodePipeline console to create a pipeline, events are enabled by default. In that case, we have to first verify whether the events are created or not. Then the 2nd step would be ensuring the IAM role also has been created with proper permissions."
      },
      {
        "date": "2021-12-27T21:18:00.000Z",
        "voteCount": 2,
        "content": "A is correct \nB &amp; C are wrong as developer is able to push the code that mean he has access.\nD is can be another option but it has not reaction  means even logs are not generated"
      },
      {
        "date": "2021-11-05T00:01:00.000Z",
        "voteCount": 3,
        "content": "The question is asking what could be the possible cause of the problem. The answers are to check the things that is required to make the pipeline work. A CloudWatch Event can help in troubleshoot but it is not required to make it work, nor it can identify what is causing the problem. So I say B is more appropriate."
      },
      {
        "date": "2021-10-25T16:42:00.000Z",
        "voteCount": 2,
        "content": "C\nPermissions required to use the CodeCommit console\nTo allow users to use the CodeCommit console, the administrator must grant them permissions for CodeCommit actions. For example, you could attach the AWSCodeCommitPowerUser managed policy or its equivalent to a user or group.\nhttps://docs.aws.amazon.com/codecommit/latest/userguide/auth-and-access-control-iam-identity-based-access-control.html"
      },
      {
        "date": "2021-10-28T12:21:00.000Z",
        "voteCount": 1,
        "content": "A developer has pushed code\nchanges to the CodeCommit repository ......so this shows that the developer has permission already. I go with A."
      },
      {
        "date": "2021-10-23T07:38:00.000Z",
        "voteCount": 2,
        "content": "Strange Question as no Context to troubleshoot. Sure A, but surely could be any number of other options as well ?"
      },
      {
        "date": "2021-10-02T10:03:00.000Z",
        "voteCount": 3,
        "content": "Ans is A"
      },
      {
        "date": "2021-09-21T01:21:00.000Z",
        "voteCount": 2,
        "content": "ans: A"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 26,
    "url": "https://www.examtopics.com/discussions/amazon/view/47925-exam-aws-devops-engineer-professional-topic-1-question-26/",
    "body": "A DevOps engineer is deploying a new version of a company's application in an AWS CodeDeploy deployment group associated with its Amazon EC2 instances.<br>After some time, the deployment fails. The engineer realizes that all the events associated with the specific deployment ID are in a Skipped status, and code was not deployed in the instances associated with the deployment group.<br>What are valid reasons for this failure? (Choose two.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe networking configuration does not allow the EC2 instances to reach the internet via a NAT gateway or internet gateway, and the CodeDeploy endpoint cannot be reached.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe IAM user who triggered the application deployment does not have permission to interact with the CodeDeploy endpoint.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe target EC2 instances were not properly registered with the CodeDeploy endpoint.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAn instance profile with proper permissions was not attached to the target EC2 instances.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe appspec.yml file was not included in the application revision."
    ],
    "answer": "AD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AD",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "DE",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "AC",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "CD",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-10-22T19:19:00.000Z",
        "voteCount": 14,
        "content": "I'll go with A,D"
      },
      {
        "date": "2021-10-24T21:21:00.000Z",
        "voteCount": 13,
        "content": "Your instance might not be able to reach the CodeDeploy or S3 public endpoint using port 443. Try one of the following:\n\nIf an instance is provisioned in a private subnet, use a NAT gateway instead of an internet gateway in the route table. For more information, see NAT Gateways.\n\nThe instance you're deploying to might not have an IAM instance profile attached, or it might have an IAM instance profile attached that does not have the required permissions."
      },
      {
        "date": "2024-01-07T03:10:00.000Z",
        "voteCount": 2,
        "content": "AD is answer"
      },
      {
        "date": "2023-07-04T02:27:00.000Z",
        "voteCount": 4,
        "content": "The correct answers are A and D and here's why:\nWhen the deployment or all lifecycle events are skipped whether on EC2 or onpremise instances, one of the following errors could occur:\n+\"The overall deployment failed because too many individual instances failed deployment\"\n+\"Too few healthy instances are available for deployment\"\n+\"Some instances in your deployment group are experiencing problems. (Error code: HEALTH_CONSTRAINTS)\"\n\nAnd most of the time, these errors are related to:\n1 - Codedeploy Agents is not installed/running on EC2/On-premise servers\n2 - IAM instance profile missing some permissions\n3 - Network communication (i.e. the agent on EC2/on premise server cannot reach the CodeDeploy endpoint via internet because of a proxy restricting communication etc.)\n4 - Time mismatch between codedeploy and codedeploy agent on EC2/On premise."
      },
      {
        "date": "2023-03-17T21:27:00.000Z",
        "voteCount": 1,
        "content": "Option A is a possible reason for the failure because CodeDeploy uses an endpoint in the CodeDeploy service to receive and manage deployments, and if the instances cannot reach the endpoint, the deployment will fail.\n\nOption C is another possible reason for the failure because CodeDeploy needs to know which instances should receive the deployment, and if they are not properly registered with the deployment group, the deployment will not be executed on them."
      },
      {
        "date": "2023-03-04T09:06:00.000Z",
        "voteCount": 1,
        "content": "A and D are the right answers."
      },
      {
        "date": "2023-02-17T03:44:00.000Z",
        "voteCount": 1,
        "content": "AD is the best answer"
      },
      {
        "date": "2023-02-09T22:03:00.000Z",
        "voteCount": 1,
        "content": "Sabreen_Salama explanation."
      },
      {
        "date": "2023-02-07T11:40:00.000Z",
        "voteCount": 2,
        "content": "the answer is AD as below documentation https://docs.aws.amazon.com/codedeploy/latest/userguide/troubleshooting-deployments.html#troubleshooting-skipped-lifecycle-events"
      },
      {
        "date": "2023-02-04T06:59:00.000Z",
        "voteCount": 1,
        "content": "A B E do not match the \"Skipped\" status"
      },
      {
        "date": "2023-02-04T07:05:00.000Z",
        "voteCount": 1,
        "content": "Sorry, AD are the answers \nhttps://docs.aws.amazon.com/codedeploy/latest/userguide/troubleshooting-deployments.html#troubleshooting-skipped-lifecycle-events"
      },
      {
        "date": "2023-01-24T02:43:00.000Z",
        "voteCount": 2,
        "content": "ADE all seem possible causes. But if appspec is missing, the error will call that out. It won\u2019t just skip events."
      },
      {
        "date": "2023-01-15T20:49:00.000Z",
        "voteCount": 1,
        "content": "Answer is A and D.  There is a slight chance that the answer is D and E as well. But I will go with A and D. E is a possibility because all EC2 lifecycle events are supposed to be skipped when the Deployment ID is in the skipped state. And when the documentation mentions EC2 lifecycle events I am not sure if it means the CodeDeploy lifecycle hook events for EC2. If so then none of those lifecycle hook events can be executed if there is a missing appspec.yml file."
      },
      {
        "date": "2022-12-25T18:28:00.000Z",
        "voteCount": 1,
        "content": "A,D. \nI spent a bunch of time reviewing this \nhttps://docs.aws.amazon.com/codedeploy/latest/userguide/troubleshooting-deployments.html#troubleshooting-skipped-lifecycle-events"
      },
      {
        "date": "2022-12-22T15:42:00.000Z",
        "voteCount": 1,
        "content": "A and D"
      },
      {
        "date": "2022-12-09T00:31:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file.html"
      },
      {
        "date": "2022-11-09T18:14:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/codedeploy/latest/userguide/troubleshooting-deployments.html#troubleshooting-skipped-lifecycle-events"
      },
      {
        "date": "2022-10-25T16:49:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/codedeploy/latest/userguide/troubleshooting-deployments.html#troubleshooting-skipped-lifecycle-events"
      },
      {
        "date": "2022-09-13T18:46:00.000Z",
        "voteCount": 1,
        "content": "ACD\nhttps://docs.aws.amazon.com/codedeploy/latest/userguide/troubleshooting-deployments.html#troubleshooting-skipped-lifecycle-events"
      },
      {
        "date": "2022-09-28T01:56:00.000Z",
        "voteCount": 1,
        "content": "A &amp; D is correct.\nC answer is talking about IAM role - who will execute code deploy/pipeline.\nBut the link being refer. It's talking about Assume role between codedeploy and target endpoint."
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 27,
    "url": "https://www.examtopics.com/discussions/amazon/view/47924-exam-aws-devops-engineer-professional-topic-1-question-27/",
    "body": "A company has an application that is using a MySQL-compatible Amazon Aurora Multi-AZ DB cluster as the database. A cross-Region read replica has been created for disaster recovery purposes. A DevOps engineer wants to automate the promotion of the replica so it becomes the primary database instance in the event of a failure.<br>Which solution will accomplish this?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a latency-based Amazon Route 53 CNAME with health checks so it points to both the primary and replica endpoints. Subscribe an Amazon SNS topic to Amazon RDS failure notifications from AWS CloudTrail and use that topic to trigger an AWS Lambda function that will promote the replica instance as the master.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Aurora custom endpoint to point to the primary database instance. Configure the application to use this endpoint. Configure AWS CloudTrail to run an AWS Lambda function to promote the replica instance and modify the custom endpoint to point to the newly promoted instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Lambda function to modify the application's AWS Cloud Formation template to promote the replica, apply the template to update the stack, and point the application to the newly promoted instance. Create an Amazon CloudWatch alarm to trigger this Lambda function after the failure event occurs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the Aurora endpoint in AWS Systems Manager Parameter Store. Create an Amazon EventBridge (Amazon CloudWatch Events) event that defects the database failure and runs an AWS Lambda function to promote the replica instance and update the endpoint URL stored in AWS Systems Manager Parameter Store. Code the application to reload the endpoint from Parameter Store if a database connection fails.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 7,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 4,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-09-24T11:21:00.000Z",
        "voteCount": 15,
        "content": "ans: D"
      },
      {
        "date": "2021-09-26T10:36:00.000Z",
        "voteCount": 13,
        "content": "The answer is D. EventBridge is needed to detect the database failure. Lambda is needed to promote the replica as it's in another Region (manual promotion, otherwise). Storing and updating the endpoint in Parameter store is important in updating the application. Look at High Availability section of Aurora FAQ:\nhttps://aws.amazon.com/rds/aurora/faqs/"
      },
      {
        "date": "2023-02-20T15:55:00.000Z",
        "voteCount": 1,
        "content": "Option D is the correct solution because it uses AWS Systems Manager Parameter Store to store the Aurora endpoint and Amazon EventBridge (Amazon CloudWatch Events) to detect database failures. It runs an AWS Lambda function to promote the read replica and update the endpoint URL stored in Parameter Store, which can be reloaded by the application if a database connection fails."
      },
      {
        "date": "2023-02-17T04:05:00.000Z",
        "voteCount": 2,
        "content": "Correct Answer is D by method of elimination and either way, parameter store can be used to store configuration. Cloudtrail is not applicable to this question"
      },
      {
        "date": "2023-02-13T14:23:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.Endpoints.html#Aurora.Endpoint.Tutorial"
      },
      {
        "date": "2024-05-02T05:15:00.000Z",
        "voteCount": 1,
        "content": "Configure AWS CloudTrail to run an AWS Lambda function. Are you sure ? I don't think so. D is correct answer"
      },
      {
        "date": "2023-02-18T06:38:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.Endpoints.html#Aurora.Overview.Endpoints.HA"
      },
      {
        "date": "2023-02-18T06:40:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/lambda/latest/dg/with-cloudtrail.html"
      },
      {
        "date": "2023-02-04T07:09:00.000Z",
        "voteCount": 1,
        "content": "A looks most automatical."
      },
      {
        "date": "2023-02-04T07:13:00.000Z",
        "voteCount": 2,
        "content": "I think A is wrong. As \"fartosh 2\" said, CloudTrail does not help to detect failures of database.\nSame reason to exclude B. \nDo you guys know why B is shown as \"Correct Answer\" ?"
      },
      {
        "date": "2023-02-18T06:35:00.000Z",
        "voteCount": 1,
        "content": "CloudTrail can do the job https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/logging-using-cloudtrail.html"
      },
      {
        "date": "2023-05-28T18:02:00.000Z",
        "voteCount": 1,
        "content": "no, it can't detect failure only API calls"
      },
      {
        "date": "2023-01-15T21:01:00.000Z",
        "voteCount": 2,
        "content": "D is the right answer. We need CloudWatch events to detect errors from the Aurora CloudWatch logs and then invoke a Lambda function to promote the Read Replica in another region. The Lambda can run in the primary region and update the Parameter store in the Primary region with the newly promoted cluster address."
      },
      {
        "date": "2022-12-25T18:56:00.000Z",
        "voteCount": 3,
        "content": "This one is hard to understand, but my bust understanding is: A and b are wrong because they use cloud trail, you would want event bridge or cloud watch events or something else. C is wrong because it\u2019s using lambda to update cloud formation template when you really need to be adjusting the infra in real time not waiting on cloud formation. Knowing that D is the only possible choice that makes sense"
      },
      {
        "date": "2022-12-25T09:59:00.000Z",
        "voteCount": 1,
        "content": "I think choice D not right \nREF : https://aws.amazon.com/blogs/architecture/implementing-multi-region-disaster-recovery-using-event-driven-architecture/  -&gt; Parameter Store is hosted in multiple Availability Zones in an AWS Region -&gt;\n\n\nI go with choice A from this link below.\nhttps://aws.amazon.com/blogs/database/cross-region-disaster-recovery-using-amazon-aurora-global-database-for-amazon-aurora-postgresql/"
      },
      {
        "date": "2022-12-25T18:35:00.000Z",
        "voteCount": 2,
        "content": "A talks about RDS, the question is about aurora"
      },
      {
        "date": "2023-02-18T06:34:00.000Z",
        "voteCount": 1,
        "content": "Aurora is a managed RDS"
      },
      {
        "date": "2022-12-22T15:52:00.000Z",
        "voteCount": 1,
        "content": "I'll go with D"
      },
      {
        "date": "2022-12-13T08:14:00.000Z",
        "voteCount": 1,
        "content": "So why B is wrong?"
      },
      {
        "date": "2023-02-13T14:23:00.000Z",
        "voteCount": 1,
        "content": "I think B is correct.  It uses Aurora endpoint neatly\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.Endpoints.html#Aurora.Endpoint.Tutorial"
      },
      {
        "date": "2022-11-19T08:05:00.000Z",
        "voteCount": 2,
        "content": "A: Route 53 is the only one which does not go down in case of region going down.\nD is incorrect because Parameter Store goes down together with region."
      },
      {
        "date": "2023-01-18T12:17:00.000Z",
        "voteCount": 1,
        "content": "But answer A uses CloudTrail which cannot detect failures of the database. Thus, this cannot be correct."
      },
      {
        "date": "2023-02-18T06:35:00.000Z",
        "voteCount": 1,
        "content": "CloudTrail can do the job."
      },
      {
        "date": "2023-02-18T06:35:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/logging-using-cloudtrail.html"
      },
      {
        "date": "2022-09-11T02:39:00.000Z",
        "voteCount": 3,
        "content": "For each of these databases, we store all configuration information in AWS Systems Manager Parameter Store, so the failover process can get all the information it needs to carry out the failover steps.\n\nWe have created a lambda which breaks down the entire failover process into two steps. Step 1 promotes the replica database to be read write, and Step 2 re-establishes resiliency by creating a new replica in the original primary region.\nhttps://developer.gs.com/blog/posts/building-multi-region-resiliency-with-amazon-rds-and-amazon-aurora"
      },
      {
        "date": "2022-04-09T02:39:00.000Z",
        "voteCount": 4,
        "content": "D - always choose decoupling options for endpoints, urls, passwords etc."
      },
      {
        "date": "2022-04-05T07:38:00.000Z",
        "voteCount": 1,
        "content": "D looks right"
      },
      {
        "date": "2022-03-22T06:19:00.000Z",
        "voteCount": 2,
        "content": "Correct Answer: D\nApplicatino can be coded to load endpoint value from AWS Parameter Store. Once the value is changed in the param store, the application can use the new value. Just need a reload.\n\nWrong answer is B, because Cloudtrail is for static content and media stream caching. But not for DB content."
      },
      {
        "date": "2021-12-24T21:36:00.000Z",
        "voteCount": 1,
        "content": "answer is D"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 28,
    "url": "https://www.examtopics.com/discussions/amazon/view/8032-exam-aws-devops-engineer-professional-topic-1-question-28/",
    "body": "An application has microservices spread across different AWS accounts and is integrated with an on-premises legacy system for some of its functionality.<br>Because of the segmented architecture and missing logs, every time the application experiences issues, it is taking too long to gather the logs to identify the issues. A DevOps Engineer must fix the log aggregation process and provide a way to centrally analyze the logs.<br>Which is the MOST efficient and cost-effective solution?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCollect system logs and application logs by using the Amazon CloudWatch Logs agent. Use the Amazon S3 API to export on-premises logs, and store the logs in an S3 bucket in a central account. Build an Amazon EMR cluster to reduce the logs and derive the root cause.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCollect system logs and application logs by using the Amazon CloudWatch Logs agent. Use the Amazon S3 API to import on-premises logs. Store all logs in S3 buckets in individual accounts. Use Amazon Macie to write a query to search for the required specific event-related data point.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCollect system logs and application logs using the Amazon CloudWatch Logs agent. Install the CloudWatch Logs agent on the on-premises servers. Transfer all logs from AWS to the on-premises data center. Use an Amazon Elasticsearch Logstash Kibana stack to analyze logs on premises.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCollect system logs and application logs by using the Amazon CloudWatch Logs agent. Install a CloudWatch Logs agent for on-premises resources. Store all logs in an S3 bucket in a central account. Set up an Amazon S3 trigger and an AWS Lambda function to analyze incoming logs and automatically identify anomalies. Use Amazon Athena to run ad hoc queries on the logs in the central account.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-20T11:32:00.000Z",
        "voteCount": 22,
        "content": "I believe D is the correct answer"
      },
      {
        "date": "2021-10-13T03:11:00.000Z",
        "voteCount": 12,
        "content": "A - when cloud watch agent can push the logs automatically, there is no reason to use s3 api\nB - when cloud watch agent can push the logs automatically, there is no reason to use s3 api. Also Macie is for PII\nC - \"Transfer all logs from AWS to the on-premises data cente\" make it a bad choice. Also ELK is more expensive and will need EC2 provisioning\nD - will work"
      },
      {
        "date": "2023-09-08T08:37:00.000Z",
        "voteCount": 1,
        "content": "D is right"
      },
      {
        "date": "2023-02-04T07:19:00.000Z",
        "voteCount": 1,
        "content": "A and B are excluded because CloudWatch Logs Agent is a good solution. \nBetween C and D, I prefer D, because C transfer logs from AWS to on-premises ..."
      },
      {
        "date": "2023-09-08T08:36:00.000Z",
        "voteCount": 1,
        "content": "Why transferring logs to on-premise resources is a good thing?"
      },
      {
        "date": "2023-01-15T21:03:00.000Z",
        "voteCount": 1,
        "content": "D is the correct answer"
      },
      {
        "date": "2022-08-31T21:15:00.000Z",
        "voteCount": 2,
        "content": "will work"
      },
      {
        "date": "2021-11-07T02:28:00.000Z",
        "voteCount": 1,
        "content": "I believe it's D."
      },
      {
        "date": "2021-11-06T21:44:00.000Z",
        "voteCount": 3,
        "content": "I'll go with D"
      },
      {
        "date": "2021-11-05T22:05:00.000Z",
        "voteCount": 1,
        "content": "D definitely. Agent on premise and collect logs in S3 centralized"
      },
      {
        "date": "2021-10-26T10:29:00.000Z",
        "voteCount": 4,
        "content": "I'll go with D"
      },
      {
        "date": "2021-10-17T22:13:00.000Z",
        "voteCount": 1,
        "content": "C looks fine if the logs transfer to AWS ElasticSearch not the on-premise data center. I'll go with D"
      },
      {
        "date": "2021-10-17T19:32:00.000Z",
        "voteCount": 1,
        "content": "I choose D"
      },
      {
        "date": "2021-10-16T09:25:00.000Z",
        "voteCount": 2,
        "content": "Another important factor is \"Install the CloudWatch Logs agent on the on-premises servers\"\nwithout that we can't get logs from local servers. I choose D."
      },
      {
        "date": "2021-10-08T15:58:00.000Z",
        "voteCount": 1,
        "content": "Why not A ?"
      },
      {
        "date": "2021-10-26T02:22:00.000Z",
        "voteCount": 1,
        "content": "EMR cluster would be too complicated and more cost. Exporting logs from on premise to AWS using the CLI would be also costly and inefficient. You want the Unified CW Logs Agent."
      },
      {
        "date": "2021-10-08T07:01:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2021-10-07T07:44:00.000Z",
        "voteCount": 4,
        "content": "C - logs aggregation \nhttps://medium.com/@sid_sharma/aws-emr-log-aggregation-and-visualization-using-lambda-elasticsearch-and-kibana-5b734fd5f812"
      },
      {
        "date": "2021-10-21T09:04:00.000Z",
        "voteCount": 1,
        "content": "Incorrect, we want the most cost efficient method. This method described in C would accrue a lot of charges to move all the log files to AWS (existing and future logs). D's description just inputs current and future log files to an S3 bucket. Additionally, setting up an ELK stack would incur much more cost than an S3 data events &gt; lambda function and Athena on top of that."
      },
      {
        "date": "2021-10-06T15:20:00.000Z",
        "voteCount": 1,
        "content": "It is D"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 29,
    "url": "https://www.examtopics.com/discussions/amazon/view/78570-exam-aws-devops-engineer-professional-topic-1-question-29/",
    "body": "A company's DevOps engineer is working in a multi-account environment. The company uses AWS Transit Gateway to route all outbound traffic through a network operations account. In the network operations account, all account traffic passes through a firewall appliance for inspection before the traffic goes to an internet gateway.<br>The firewall appliance sends logs to Amazon CloudWatch Logs and includes event severities of CRITICAL, HIGH, MEDIUM, LOW, and INFO. The security team wants to receive an alert if any CRITICAL events occur.<br>What should the DevOps engineer do to meet these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon CloudWatch Synthetics canary to monitor the firewall state. If the firewall reaches a CRITICAL state or logs a CRITICAL event, use a CloudWatch alarm to publish a notification to an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the security team's email address to the topic.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon CloudWatch mettic filter by using a search for CRITICAL events. Publish a custom metric for the finding. Use a CloudWatch alarm based on the custom metric to publish a notification to an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the security team's email address to the topic.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Amazon GuardDuty in the network operations account. Configure GuardDuty to monitor flow logs. Create an Amazon EventBridge (Amazon CloudWatch Events) event rule that is invoked by GuardDuty events that are CRITICAL. Define an Amazon Simple Notification Service (Amazon SNS) topic as a target. Subscribe the security team's email address to the topic.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Firewall Manager to apply consistent policies across all accounts. Create an Amazon EventBridge (Amazon CloudWatch Events) event rule that is invoked by Firewall Manager events that are CRITICAL. Define an Amazon Simple Notification Service (Amazon SNS) topic as a target. Subscribe the security team's email address to the topic."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 20,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 4,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-10-08T05:40:00.000Z",
        "voteCount": 7,
        "content": "If there was no Logs being sent to CW, I would definitly goes with GuardDuty.But GuardDuty is a Threat Detection based on VPCLogs, Cloud Trail, DNS Logs and EventBridge. In this question we need to analyse logs. Thats the point."
      },
      {
        "date": "2023-09-09T07:50:00.000Z",
        "voteCount": 2,
        "content": "Vote for B"
      },
      {
        "date": "2023-07-04T04:19:00.000Z",
        "voteCount": 1,
        "content": "C could also be an answer but it's an overkill because it's not free and too much for a simple log extraction task based on term \"CRITICAL\" that can be achieved with just cloudwatch metric filters"
      },
      {
        "date": "2023-05-21T08:47:00.000Z",
        "voteCount": 1,
        "content": "B\nhttps://www.examtopics.com/discussions/amazon/view/109224-exam-aws-certified-devops-engineer-professional-dop-c02/"
      },
      {
        "date": "2023-02-16T06:40:00.000Z",
        "voteCount": 2,
        "content": "B will alert CRITICAL from other apps, not only the firewall."
      },
      {
        "date": "2023-02-04T07:26:00.000Z",
        "voteCount": 1,
        "content": "B will alert CRITICAL from other apps, not only the firewall. \nC and D are excluded because CloudWatch is sufficient."
      },
      {
        "date": "2023-02-04T07:42:00.000Z",
        "voteCount": 1,
        "content": "I agree with PepsNick.  Amazon GuardDuty is even better."
      },
      {
        "date": "2023-02-04T00:13:00.000Z",
        "voteCount": 1,
        "content": "Answer B\nCompany using 3rd party to send log to cloudwatch....\n\"The firewall appliance sends logs to Amazon CloudWatch Logs and includes event severities of CRITICAL, HIGH, MEDIUM, LOW, and INFO\""
      },
      {
        "date": "2023-01-23T23:34:00.000Z",
        "voteCount": 1,
        "content": "GaurdDuty is the right option to opt for suspicious traffic"
      },
      {
        "date": "2023-01-16T09:31:00.000Z",
        "voteCount": 4,
        "content": "The answer is B as firewall appliances are already sending logs to Cloudwatch logs."
      },
      {
        "date": "2023-01-09T10:37:00.000Z",
        "voteCount": 3,
        "content": "Answer C is in fact correct. This blog has all the information required regarding GuardDuty\nhttps://aws.amazon.com/blogs/security/automatically-block-suspicious-traffic-with-aws-network-firewall-and-amazon-guardduty/"
      },
      {
        "date": "2022-12-28T15:30:00.000Z",
        "voteCount": 2,
        "content": "C is not correct"
      },
      {
        "date": "2022-11-09T23:29:00.000Z",
        "voteCount": 3,
        "content": "They are just asking for receiving notification on critical severity which can be done by Cloudwatch metrics filter and SNS"
      },
      {
        "date": "2022-11-04T01:15:00.000Z",
        "voteCount": 3,
        "content": "I go with B since there is configuration in place that sends logs to CW"
      },
      {
        "date": "2022-08-31T21:16:00.000Z",
        "voteCount": 2,
        "content": "B - as the rest are not searching the logs"
      },
      {
        "date": "2022-08-31T10:36:00.000Z",
        "voteCount": 1,
        "content": "I go with B. Question only asks about receiving an alert in case any CRITICAL alert arises. C could be an option if the company, for example, wants to enhance firewall threat source detection. B keep the solution simple.\nRefer to an example of solution using GuardDuty and Firewall - https://www.juniper.net/documentation/us/en/software/sky-atp/sky-atp/topics/topic-map/sky-atp-guardduty-srx-integration.html"
      },
      {
        "date": "2022-08-30T19:41:00.000Z",
        "voteCount": 3,
        "content": "Since the logs are sent to Cloudwatch via the Firewall Appliance, filtering for the custom metric of CRITCAL from Cloudwatch would be the best response, thus, \"B\"."
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 30,
    "url": "https://www.examtopics.com/discussions/amazon/view/48072-exam-aws-devops-engineer-professional-topic-1-question-30/",
    "body": "A company recently migrated its legacy application from on-premises to AWS. The application is hosted on Amazon EC2 instances behind an Application Load<br>Balancer, which is behind Amazon API Gateway. The company wants to ensure users experience minimal disruptions during any deployment of a new version of the application. The company also wants to ensure it can quickly roll back updates if there is an issue.<br>Which solution will meet these requirements with MINIMAL changes to the application?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIntroduce changes as a separate environment parallel to the existing one. Configure API Gateway to use a canary release deployment to send a small subset of user traffic to the new environment.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIntroduce changes as a separate environment parallel to the existing one. Update the application's DNS alias records to point to the new environment.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIntroduce changes as a separate target group behind the existing Application Load Balancer. Configure API Gateway to route user traffic to the new target group in steps.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIntroduce changes as a separate target group behind the existing Application Load Balancer. Configure API Gateway to route all traffic to the Application Load Balancer, which then sends the traffic to the new target group."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-10-11T21:12:00.000Z",
        "voteCount": 16,
        "content": "I'll got with A\n\nB) required a lot of changes and the DNS can take longer to propagate \nC) API gateway cannot choose the target group, this is done by de ELB \nD) ELB Target groups changes require some work, you can just pointo to a new loadbalancer"
      },
      {
        "date": "2021-10-04T19:34:00.000Z",
        "voteCount": 10,
        "content": "D requires fewer changes than A"
      },
      {
        "date": "2021-11-02T12:20:00.000Z",
        "voteCount": 8,
        "content": "Option D suggests to reroute all traffic at once, which will affect all the users experience. To avoid that, Option A suggests to use canary deployment, which minimizes the blast radius in case of issues and allows to quickly roll back."
      },
      {
        "date": "2023-10-24T05:09:00.000Z",
        "voteCount": 1,
        "content": "LEAST changes is actually C.\n\nB and D switch all traffic at once and are thus out of the question. A is more work than C."
      },
      {
        "date": "2023-02-04T07:50:00.000Z",
        "voteCount": 1,
        "content": "C and D are excluded. We need a second Application Load Balancer. \nB will switch all traffic to the new environment. That disrupts users."
      },
      {
        "date": "2023-01-31T09:52:00.000Z",
        "voteCount": 1,
        "content": "I WILL GO WITH A - CANARY DEPLOYMENT"
      },
      {
        "date": "2023-01-16T09:40:00.000Z",
        "voteCount": 2,
        "content": "Correct Answer is A. API Gateway supports canary deployment on a deployment stage before you direct all traffic to that stage. A parallel environment means we will create a new ALB and a target group that will target a new set of EC2 instances on which the newer version of the app will be deployed. So the canary setting associated to the new version of the API will connect with the new ALB instance which in turn will direct the traffic to the new EC2 instances on which the newer version of the application is deployed."
      },
      {
        "date": "2022-12-22T16:11:00.000Z",
        "voteCount": 1,
        "content": "I'll go with A\nIn case of failure in new code, there will be issues in using option D"
      },
      {
        "date": "2022-12-08T08:55:00.000Z",
        "voteCount": 1,
        "content": "A. Question isn't asking about impacting minimal users, it's asking for minimal disruption. \nD. Simple and elegant with minimal changes to the application."
      },
      {
        "date": "2022-12-06T06:37:00.000Z",
        "voteCount": 1,
        "content": "Why Canary Deployment?\nCanary deployment benefits include zero downtime, easy rollout and quick rollback \u2013 plus the added safety from the gradual rollout process. It also has some drawbacks \u2013 the expense of maintaining multiple server instances, the difficult clone-or-don\u2019t-clone database decision.\nSource: https://www.split.io/glossary/canary-deployment/"
      },
      {
        "date": "2022-12-03T08:14:00.000Z",
        "voteCount": 1,
        "content": "I can see C as correct if you have the ALB configured with routing rules. You can use a http header to route requests to a specific target group. The wording of the question is interesting. The requirement is minimal disruption. D is kind of correct because you could flip to the old target group quickly as long as the old infrastructure hasn't been deprovisioned."
      },
      {
        "date": "2022-11-22T09:13:00.000Z",
        "voteCount": 1,
        "content": "For those that looking at C/D I think those may be cancelled because an ALB can only point to one target group?"
      },
      {
        "date": "2022-11-03T05:23:00.000Z",
        "voteCount": 1,
        "content": "A IS THE RIGHT ANSWER"
      },
      {
        "date": "2022-09-20T17:45:00.000Z",
        "voteCount": 1,
        "content": "Though \"D\" makes more sense in the real world but saying \"minimal disruptions\" requires just sending a portion of traffic to the new environment. So reluctantly picking \"A\""
      },
      {
        "date": "2022-08-31T21:18:00.000Z",
        "voteCount": 6,
        "content": "A - quite simply, questions of this nature w keywords like \"subset of users\", the clear option for ans shld be \"canary\""
      },
      {
        "date": "2022-04-22T10:24:00.000Z",
        "voteCount": 4,
        "content": "Canary deployment for minimal disruptions"
      },
      {
        "date": "2022-03-03T23:48:00.000Z",
        "voteCount": 2,
        "content": "APIG has no target group,  ELB cannt canary, so only Option A is correct"
      },
      {
        "date": "2021-10-24T14:24:00.000Z",
        "voteCount": 1,
        "content": "D, need ASG only\nA, need ALB+ASG"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 31,
    "url": "https://www.examtopics.com/discussions/amazon/view/46365-exam-aws-devops-engineer-professional-topic-1-question-31/",
    "body": "A company recently launched an application that is more popular than expected. The company wants to ensure the application can scale to meet increasing demands and provide reliability using multiple Availability Zones (AZs). The application runs on a fleet of Amazon EC2 instances behind an Application Load<br>Balancer (ALB). A DevOps engineer has created an Auto Scaling group across multiple AZs for the application. Instances launched in the newly added AZs are not receiving any traffic for the application.<br>What is likely causing this issue?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAuto Scaling groups can create new instances in a single AZ only.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe EC2 instances have not been manually associated to the ALB.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe ALB should be replaced with a Network Load Balancer (NLB).",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe new AZ has not been added to the ALB.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 5,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-10-06T04:33:00.000Z",
        "voteCount": 17,
        "content": "D - https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-add-availability-zone.html"
      },
      {
        "date": "2023-05-01T05:54:00.000Z",
        "voteCount": 2,
        "content": "D is more relevant to the given scenario"
      },
      {
        "date": "2023-03-11T10:42:00.000Z",
        "voteCount": 1,
        "content": "its D\nWhen an Auto Scaling group is created across multiple Availability Zones (AZs) for an application, it is important to ensure that the Application Load Balancer (ALB) is configured to route traffic to instances in all AZs. If a new AZ is added to the Auto Scaling group but not added to the ALB, then instances launched in that AZ will not receive any traffic from the ALB."
      },
      {
        "date": "2023-02-04T08:00:00.000Z",
        "voteCount": 1,
        "content": "A and D are obviously incorrect.\nC, why not Elastic Load Balancer....."
      },
      {
        "date": "2023-02-04T08:02:00.000Z",
        "voteCount": 1,
        "content": "I was wrong.  B must be wrong.  We do not add EC2 instances to ALB ...."
      },
      {
        "date": "2023-01-31T10:02:00.000Z",
        "voteCount": 1,
        "content": "I will go with D"
      },
      {
        "date": "2023-01-16T09:46:00.000Z",
        "voteCount": 2,
        "content": "Answer is D. AZ needs to be added to the ALB manually so that ALB nodes are created in that ALB."
      },
      {
        "date": "2022-12-19T15:10:00.000Z",
        "voteCount": 1,
        "content": "D - https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/how-elastic-load-balancing-works.html"
      },
      {
        "date": "2022-12-05T09:08:00.000Z",
        "voteCount": 1,
        "content": "C - https://aws.amazon.com/about-aws/whats-new/2022/11/application-load-balancers-turning-off-cross-zone-load-balancing-per-target-group/"
      },
      {
        "date": "2023-01-18T12:31:00.000Z",
        "voteCount": 1,
        "content": "The link you pasted actually explains that you can TURN OFF cross-zone load balancing. Meaning that by default, ALB routes the traffic across AZs."
      },
      {
        "date": "2022-12-05T09:01:00.000Z",
        "voteCount": 1,
        "content": "C - There is a difference between ALB and NLB in AWS. ALB is a load balancer that routes traffic to multiple targets, such as EC2 instances, in a single Availability Zone. NLB is a load balancer that routes traffic to multiple targets, such as EC2 instances, across multiple Availability Zones."
      },
      {
        "date": "2022-08-31T21:21:00.000Z",
        "voteCount": 3,
        "content": "D - clear cut ans. Except tt why didnt the company's executives do their market research properly that could have avoided such a situation in the firts place :D"
      },
      {
        "date": "2021-10-16T18:19:00.000Z",
        "voteCount": 4,
        "content": "D is the correct answer"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 32,
    "url": "https://www.examtopics.com/discussions/amazon/view/28034-exam-aws-devops-engineer-professional-topic-1-question-32/",
    "body": "A DevOps Engineer manages a web application that runs on Amazon EC2 instances behind an Application Load Balancer (ALB). The instances run in an EC2<br>Auto Scaling group across multiple Availability Zones. The engineer needs to implement a deployment strategy that:<br>\u2711 Launches a second fleet of instances with the same capacity as the original fleet.<br>\u2711 Maintains the original fleet unchanged while the second fleet is launched.<br>\u2711 Transitions traffic to the second fleet when the second fleet is fully deployed.<br>\u2711 Terminates the original fleet automatically 1 hour after transition.<br>Which solution will satisfy these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an AWS CloudFormation template with a retention policy for the ALB set to 1 hour. Update the Amazon Route 53 record to reflect the new ALB.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse two AWS Elastic Beanstalk environments to perform a blue/green deployment from the original environment to the new one. Create an application version lifecycle policy to terminate the original environment in 1 hour.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS CodeDeploy with a deployment group configured with a blue/green deployment configuration. Select the option Terminate the original instances in the deployment group with a waiting period of 1 hour.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Elastic Beanstalk with the configuration set to Immutable. Create an .ebextension using the Resources key that sets the deletion policy of the ALB to 1 hour, and deploy the application."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 6,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-11-02T16:03:00.000Z",
        "voteCount": 13,
        "content": "I'll go with C"
      },
      {
        "date": "2021-10-11T17:38:00.000Z",
        "voteCount": 10,
        "content": "C. \nhttps://docs.aws.amazon.com/codedeploy/latest/APIReference/API_BlueInstanceTerminationOption.html"
      },
      {
        "date": "2021-10-12T17:14:00.000Z",
        "voteCount": 1,
        "content": "I'm wondering why the EB ones are false though, do you know?"
      },
      {
        "date": "2021-10-21T05:09:00.000Z",
        "voteCount": 3,
        "content": "EB application version lifecycle policy is not for EC2 instance. More details can be found here: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/applications-lifecycle.html"
      },
      {
        "date": "2021-10-25T23:17:00.000Z",
        "voteCount": 1,
        "content": "This link eliminates EB options and this one https://docs.aws.amazon.com/codedeploy/latest/APIReference/API_BlueInstanceTerminationOption.html validates option B."
      },
      {
        "date": "2022-12-25T19:26:00.000Z",
        "voteCount": 1,
        "content": "This link shows the answer is indeed C"
      },
      {
        "date": "2022-09-14T12:23:00.000Z",
        "voteCount": 2,
        "content": "Thank you for the link - answer is C"
      },
      {
        "date": "2023-05-12T03:56:00.000Z",
        "voteCount": 1,
        "content": "Is it blue/green deployment configuration present in CodeDeploy deployment configuration?"
      },
      {
        "date": "2023-04-18T13:44:00.000Z",
        "voteCount": 2,
        "content": "Blue/green will also work, it will create the same number of healthy instances in the new group and still keep the old instances for an hour if you specify it."
      },
      {
        "date": "2023-04-04T22:32:00.000Z",
        "voteCount": 1,
        "content": "Immutable \u2013 A slower deployment method, that ensures your new application version is always deployed to new instances, instead of updating existing instances. It also has the additional advantage of a quick and safe rollback in case the deployment fails. With this method, Elastic Beanstalk performs an immutable update to deploy your application. In an immutable update, a second Auto Scaling group is launched in your environment and the new version serves traffic alongside the old version until the new instances pass health checks. https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html"
      },
      {
        "date": "2023-02-04T08:12:00.000Z",
        "voteCount": 1,
        "content": "It must be a blue/green deployment. \nC looks more managed than B"
      },
      {
        "date": "2023-01-31T10:29:00.000Z",
        "voteCount": 1,
        "content": "I will go with C"
      },
      {
        "date": "2023-01-16T11:41:00.000Z",
        "voteCount": 1,
        "content": "Ci s the right answer. Using Deployment configuration of  Blue-Green that enables copying of ASG abd creating a new fleet if EC2 instanced is the right solution. As part of the Deployment configuration you can set the duration in hours after which the old deployment environment will be terminated."
      },
      {
        "date": "2023-01-09T11:14:00.000Z",
        "voteCount": 2,
        "content": "The answer is D. There is a clear ask for an immutable deployment in the question. CodeDeploy in that answer C mentiones does not have an immutable deployment option.\n\nImmutable deployments perform an immutable update to launch a full set of new instances running the new version of the application in a separate Auto Scaling group, alongside the instances running the old version. Immutable deployments can prevent issues caused by partially completed rolling deployments. If the new instances don't pass health checks, Elastic Beanstalk terminates them, leaving the original instances untouched."
      },
      {
        "date": "2023-11-28T10:21:00.000Z",
        "voteCount": 1,
        "content": "D can't be correct as in immutable updates an instance serves traffic alongside all of the instances in the original Auto Scaling group that are running the previous configuration, however requirement says that transition to second fleet only when it is fully deployed."
      },
      {
        "date": "2022-12-05T09:16:00.000Z",
        "voteCount": 1,
        "content": "D - https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environmentmgmt-updates-immutable.html"
      },
      {
        "date": "2022-10-08T05:54:00.000Z",
        "voteCount": 1,
        "content": "Original revision termination settings\n\"The original revision termination settings are configured to wait 1 hour after traffic has been rerouted before terminating the blue task set.\""
      },
      {
        "date": "2022-09-11T02:58:00.000Z",
        "voteCount": 2,
        "content": "Original revision termination settings\nThe original revision termination settings are configured to wait 1 hour after traffic has been rerouted before terminating the blue task set.\nhttps://docs.aws.amazon.com/AmazonECS/latest/developerguide/deployment-type-bluegreen.html"
      },
      {
        "date": "2022-09-06T04:46:00.000Z",
        "voteCount": 2,
        "content": "Ans: C"
      },
      {
        "date": "2021-10-29T09:30:00.000Z",
        "voteCount": 4,
        "content": "Answer is C. B is incorrect because the Elastic Beanstalk Application Version lifecycle just deletes old .config files that dictate how your environment is set up. It does NOT dictate how long until an environment is deleted by Elastic Beanstalk."
      },
      {
        "date": "2021-10-27T10:56:00.000Z",
        "voteCount": 6,
        "content": "Yes, correct is C. The remain answers is all using CloudFormation. The DeletionPolicy of CF is just Retain/Delete/Snapshot. There is no option to keep the resource in the amount of time before deletion.\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html"
      },
      {
        "date": "2021-10-03T21:31:00.000Z",
        "voteCount": 1,
        "content": "Answer B.\nhttps://docs.aws.amazon.com/en-us/codedeploy/latest/userguide/deployment-groups-create-blue-green.html"
      },
      {
        "date": "2021-10-09T03:48:00.000Z",
        "voteCount": 7,
        "content": "I typed wrong. Answer C."
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 33,
    "url": "https://www.examtopics.com/discussions/amazon/view/2843-exam-aws-devops-engineer-professional-topic-1-question-33/",
    "body": "A healthcare services company is concerned about the growing costs of software licensing for an application for monitoring patient wellness. The company wants to create an audit process to ensure that the application is running exclusively on Amazon EC2 Dedicated Hosts. A DevOps Engineer must create a workflow to audit the application to ensure compliance.<br>What steps should the Engineer take to meet this requirement with the LEAST administrative overhead?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Systems Manager Configuration Compliance. Use calls to the put-compliance-items API action to scan and build a database of noncompliant EC2 instances based on their host placement configuration. Use an Amazon DynamoDB table to store these instance IDs for fast access. Generate a report through Systems Manager by calling the list-compliance-summaries API action.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse custom Java code running on an EC2 instance. Set up EC2 Auto Scaling for the instance depending on the number of instances to be checked. Send the list of noncompliant EC2 instance IDs to an Amazon SQS queue. Set up another worker instance to process instance IDs from the SQS queue and write them to Amazon DynamoDB. Use an AWS Lambda function to terminate noncompliant instance IDs obtained from the queue, and send them to an Amazon SNS email topic for distribution.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Config. Identify all EC2 instances to be audited by enabling Config Recording on all Amazon EC2 resources for the region. Create a custom AWS Config rule that triggers an AWS Lambda function by using the \u05d2\u20acconfig-rule-change-triggered\u05d2\u20ac blueprint. Modify the Lambda evaluateCompliance() function to verify host placement to return a NON_COMPLIANT result if the instance is not running on an EC2 Dedicated Host. Use the AWS Config report to address noncompliant instances.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS CloudTrail. Identify all EC2 instances to be audited by analyzing all calls to the EC2 RunCommand API action. Invoke an AWS Lambda function that analyzes the host placement of the instance. Store the EC2 instance ID of noncompliant resources in an Amazon RDS MySQL DB instance. Generate a report by querying the RDS instance and exporting the query results to a CSV text file."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-26T01:22:00.000Z",
        "voteCount": 24,
        "content": "Correct Answer is \"C\" A will only help in ompliance to scan your fleet of managed instances for patch compliance and configuration inconsistencies. but when it comes to software licensing compliance aws config rules nables you to assess compliance with your server-bound software licenses"
      },
      {
        "date": "2021-10-05T21:16:00.000Z",
        "voteCount": 9,
        "content": "C\uff1a\nhttps://aws.amazon.com/about-aws/whats-new/2015/11/use-aws-config-to-track-ec2-instances-on-dedicated-hosts-and-assess-license-compliance/"
      },
      {
        "date": "2021-10-17T23:05:00.000Z",
        "voteCount": 2,
        "content": "This is for tracking what is running on dedicated hosts. It won't help with detecting when your software ISN'T running on your dedicated hosts. For that you need a custom rule. https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config_develop-rules_getting-started.html\n\nAlso, the Lambda blueprint function is actually called \"evaluateChangeNotificationCompliance\". I'm guessing this has just changed over time so it's still the correct answer. https://console.aws.amazon.com/lambda/home?region=us-east-1#/create/function/configure/blueprint?blueprint=config-rule-change-triggered"
      },
      {
        "date": "2023-01-16T11:44:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is C."
      },
      {
        "date": "2022-09-06T04:49:00.000Z",
        "voteCount": 1,
        "content": "Ans: C"
      },
      {
        "date": "2021-11-01T18:52:00.000Z",
        "voteCount": 2,
        "content": "The answer is C why:\nReferences:\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/dedicated-hosts-aws-config.html\n\nhttps://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-compliance-about.html#sysman-compliance-custom\n\nhttps://aws.amazon.com/blogs/aws/now-available-ec2-dedicated-hosts/"
      },
      {
        "date": "2021-11-01T18:23:00.000Z",
        "voteCount": 2,
        "content": "I'll go with C"
      },
      {
        "date": "2021-11-01T07:48:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer: C\n\nReference:\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/dedicated-hosts-aws-config.html\n\nhttps://aws.amazon.com/about-aws/whats-new/2015/11/use-aws-config-to-track-ec2-instances-on-dedicated-hosts-and-assess-license-compliance/"
      },
      {
        "date": "2021-10-28T08:27:00.000Z",
        "voteCount": 7,
        "content": "C is the answer. Remember when compliance is mentioned, think config, config rule, etc. If the compliance has to do with EC2 instance or instance AMI inspection, think inspector"
      },
      {
        "date": "2021-10-25T00:19:00.000Z",
        "voteCount": 2,
        "content": "I'll go with C"
      },
      {
        "date": "2021-10-22T17:34:00.000Z",
        "voteCount": 1,
        "content": "I'll go with C\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/dedicated-hosts-aws-config.html"
      },
      {
        "date": "2021-10-22T08:10:00.000Z",
        "voteCount": 1,
        "content": "C - directly related to https://aws.amazon.com/blogs/aws/now-available-ec2-dedicated-hosts/"
      },
      {
        "date": "2021-10-16T08:26:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2021-10-13T00:10:00.000Z",
        "voteCount": 1,
        "content": "It is C"
      },
      {
        "date": "2021-10-12T14:29:00.000Z",
        "voteCount": 1,
        "content": "C -- AWS Con\ufb01g records the con\ufb01guration details of Dedicated hosts and the instances that you launch on them"
      },
      {
        "date": "2021-10-09T15:42:00.000Z",
        "voteCount": 1,
        "content": "answer is C\nthanks amzngenus for the document"
      },
      {
        "date": "2021-10-02T22:58:00.000Z",
        "voteCount": 3,
        "content": "i will go with c"
      },
      {
        "date": "2021-09-28T12:17:00.000Z",
        "voteCount": 6,
        "content": "Answer is indeed C and mentioned in the aws dedicated hosts web page, check \"License Usage Reporting\" in the link below\nhttps://aws.amazon.com/ec2/dedicated-hosts/"
      },
      {
        "date": "2021-10-17T06:16:00.000Z",
        "voteCount": 1,
        "content": "The page must have been changed - License Usage Reporting is not there."
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 34,
    "url": "https://www.examtopics.com/discussions/amazon/view/48004-exam-aws-devops-engineer-professional-topic-1-question-34/",
    "body": "A company has 100 GB of log data in an Amazon S3 bucket stored in .csv format. SQL developers want to query this data and generate graphs to visualize it.<br>They also need an efficient, automated way to store metadata from the .csv file.<br>Which combination of steps should be taken to meet these requirements with the LEAST amount of effort? (Choose three.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFilter the data through AWS X-Ray to visualize the data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFilter the data through Amazon QuickSight to visualize the data.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tQuery the data with Amazon Athena.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tQuery the data with Amazon Redshift.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Glue as the persistent metadata store.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon S3 as the persistent metadata store."
    ],
    "answer": "BCE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BCE",
        "count": 27,
        "isMostVoted": true
      },
      {
        "answer": "BCF",
        "count": 9,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-09-19T13:46:00.000Z",
        "voteCount": 21,
        "content": "I will go with B,C,E"
      },
      {
        "date": "2021-11-03T07:30:00.000Z",
        "voteCount": 9,
        "content": "Glue is an ETL. S3 would be the service to choose to store metadata:  B, C , F"
      },
      {
        "date": "2021-09-30T05:40:00.000Z",
        "voteCount": 14,
        "content": "I'll go with B, C, F\n\nThat's not use case for Glue"
      },
      {
        "date": "2024-06-22T11:18:00.000Z",
        "voteCount": 1,
        "content": "B, C and F. Glue is not a service to persist data."
      },
      {
        "date": "2024-05-14T01:38:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/glue/latest/dg/components-overview.html#data-catalog-intro"
      },
      {
        "date": "2024-02-13T22:42:00.000Z",
        "voteCount": 2,
        "content": "AWS Glue is not a persistent data store itself. AWS Glue is a fully managed extract, transform, and load (ETL) service that is designed to make it easy for you to prepare and load your data for analysis. It helps in the process of moving and transforming data from various sources to your desired target, such as a data warehouse or data lake."
      },
      {
        "date": "2023-08-01T15:55:00.000Z",
        "voteCount": 1,
        "content": "I would go with BCF: https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingMetadata.html\n\nYou can set object metadata in Amazon S3 at the time you upload the object. Object metadata is a set of name-value pairs and the log files are already in S3"
      },
      {
        "date": "2023-05-08T08:15:00.000Z",
        "voteCount": 1,
        "content": "I was also convinced it was BCF at first, but I think the answer is just worded confusingly. Glue on its own is an ETL tool. But Glue Data Catalog is for storing metadata, so that's probably what the answer is referring to."
      },
      {
        "date": "2023-03-21T08:18:00.000Z",
        "voteCount": 1,
        "content": "B, C, F. Glue Data Catalog stores metadata but not Glue. Glue is the ETL tool."
      },
      {
        "date": "2023-03-03T01:40:00.000Z",
        "voteCount": 1,
        "content": "B, C, E.\nFor those who think F is correct over E, the question is not asking about the metadata storage... its asking about automated way to store metadata... that's what Glue can do easily."
      },
      {
        "date": "2023-02-21T01:38:00.000Z",
        "voteCount": 3,
        "content": "Textbook question\nMetastore -&gt; Always Glue\nAnalyze data from S3 -&gt; Athena + Quicksight (if not real-time)"
      },
      {
        "date": "2023-02-04T08:33:00.000Z",
        "voteCount": 1,
        "content": "X-Ray is for tracing apps. A is excluded\nAmazon Redshift is data warehouse, can be too heavy. D is excluded\nE is better than F."
      },
      {
        "date": "2023-02-04T08:36:00.000Z",
        "voteCount": 1,
        "content": "Sorry, F is better than E, because we are storing metadata."
      },
      {
        "date": "2023-01-16T11:46:00.000Z",
        "voteCount": 2,
        "content": "B,C abd E are the right choices."
      },
      {
        "date": "2022-12-28T15:35:00.000Z",
        "voteCount": 2,
        "content": "Go with BCE"
      },
      {
        "date": "2022-12-28T14:53:00.000Z",
        "voteCount": 2,
        "content": "I'll go with B, C and E"
      },
      {
        "date": "2022-12-26T14:00:00.000Z",
        "voteCount": 2,
        "content": "Glue data catalogue is persistent meta data store per links in other comments"
      },
      {
        "date": "2022-12-17T03:06:00.000Z",
        "voteCount": 3,
        "content": "WS Glue uses the AWS Glue Data Catalog to store metadata about data sources, transforms, and targets.\nREF : https://docs.aws.amazon.com/glue/latest/dg/components-overview.html"
      },
      {
        "date": "2022-12-14T20:00:00.000Z",
        "voteCount": 3,
        "content": "\"The AWS Glue Data Catalog is a fully-managed persistent metadata store...\" Ref: https://docs.aws.amazon.com/whitepapers/latest/best-practices-building-data-lake-for-games/data-cataloging.html"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 35,
    "url": "https://www.examtopics.com/discussions/amazon/view/19604-exam-aws-devops-engineer-professional-topic-1-question-35/",
    "body": "A DevOps Engineer has several legacy applications that all generate different log formats. The Engineer must standardize the formats before writing them to<br>Amazon S3 for querying and analysis.<br>How can this requirement be met at the LOWEST cost?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tHave the application send its logs to an Amazon EMR cluster and normalize the logs before sending them to Amazon S3",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tHave the application send its logs to Amazon QuickSight, then use the Amazon QuickSight SPICE engine to normalize the logs. Do the analysis directly from Amazon QuickSight",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tKeep the logs in Amazon S3 and use Amazon Redshift Spectrum to normalize the logs in place",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Kinesis Agent on each server to upload the logs and have Amazon Kinesis Data Firehose use an AWS Lambda function to normalize the logs before writing them to Amazon S3\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-01-16T11:48:00.000Z",
        "voteCount": 3,
        "content": "Correct answer is D"
      },
      {
        "date": "2022-12-11T09:28:00.000Z",
        "voteCount": 2,
        "content": "The most cost-effective option would be to use Amazon Kinesis Agent on each server to upload the logs and have Amazon Kinesis Data Firehose use an AWS Lambda function to normalize the logs before writing them to Amazon S3. This option allows the DevOps Engineer to use a serverless solution for log normalization, which will reduce costs compared to running an Amazon EMR cluster or using Amazon Redshift Spectrum. Additionally, using Amazon Kinesis Data Firehose and AWS Lambda allows for easy scalability as the volume of logs increases."
      },
      {
        "date": "2021-11-05T04:50:00.000Z",
        "voteCount": 4,
        "content": "I understand why the answer is D but is this solution LOWEST cost?"
      },
      {
        "date": "2023-04-26T05:53:00.000Z",
        "voteCount": 1,
        "content": "yes, compared with EMR and Redshift the cost will be lower ; the B option is not possible"
      },
      {
        "date": "2021-10-30T20:15:00.000Z",
        "voteCount": 3,
        "content": "D it is"
      },
      {
        "date": "2021-10-29T02:12:00.000Z",
        "voteCount": 2,
        "content": "I'll go with D"
      },
      {
        "date": "2021-10-28T00:46:00.000Z",
        "voteCount": 1,
        "content": "Go with D"
      },
      {
        "date": "2021-10-21T22:39:00.000Z",
        "voteCount": 3,
        "content": "Additional  qs from devops exam:\nQ- Need to backup sensitive s3 objects that are stored within an S3 bucket with  private bucket policy using the S3 CROSS region replication. The objects need to be copied to a target bucket in a diff AWS region and account. What should be done for replication? (choose 3)\n\nA. Create a replication IAM role in the source account\nB. Create a replication IAM role in the target account\nC. Add statements to the source bucket policy allowing the replication IAM role to replicate objects\nD. Add statements to the target bucket policy allowing the replication IAM role to replicate objects\nE. Set accesscontroltranslation.owneroverride to true in the replication config and add a statement to the target bucket policy allowing the replication IAM role to override object ownership\nF. Set accesscontroltranslation.owneroverride to destination in the replication config and add a statement to the target bucket policy allowing the replication IAM role to override object ownership"
      },
      {
        "date": "2021-10-22T23:56:00.000Z",
        "voteCount": 5,
        "content": "IMHO - ADF\nhttps://docs.aws.amazon.com/AmazonS3/latest/user-guide/enable-replication.html\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/replication-change-owner.html"
      },
      {
        "date": "2023-03-12T17:17:00.000Z",
        "voteCount": 1,
        "content": "It's ADF Jeff."
      },
      {
        "date": "2021-10-01T01:58:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2021-09-30T18:58:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2021-09-25T15:27:00.000Z",
        "voteCount": 2,
        "content": "Answer is D"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 36,
    "url": "https://www.examtopics.com/discussions/amazon/view/78881-exam-aws-devops-engineer-professional-topic-1-question-36/",
    "body": "A company needs to implement a robust CI/CD pipeline to automate the deployment of an application in AWS. The pipeline must support continuous integration, continuous delivery, and automatic rollback upon deployment failure. The entire CI/CD pipeline must be capable of being re-provisioned in alternate AWS accounts or Regions within minutes. A DevOps engineer has already created an AWS CodeCommit repository to store the source code.<br>Which combination of actions should be taken when building this pipeline to meet these requirements? (Choose three.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an AWS CodePipeline pipeline with a build stage using AWS CodeBuild.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCopy the build artifact from CodeCommit to Amazon S3.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Auto Scaling group of Amazon EC2 instances behind an Application Load Balancer (ALB) and set the ALB as the deployment target in AWS CodePipeline.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Elastic Beanstalk environment as the deployment target in AWS CodePipeline.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement an Amazon SQS queue to decouple the pipeline components.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tProvision all resources using AWS CloudFormation.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "ADF",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ADF",
        "count": 18,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-09-15T01:14:00.000Z",
        "voteCount": 14,
        "content": "A - For \"Integration\"\nD - For \"automatic rollback upon deployment failure\".\nF - For \"The entire CI/CD pipeline must be capable of being re-provisioned in alternate AWS accounts or Regions within minutes\""
      },
      {
        "date": "2023-01-31T11:14:00.000Z",
        "voteCount": 2,
        "content": "I will go ADF, storing artifacts in s3 but why is it required in this scenario"
      },
      {
        "date": "2023-01-16T11:55:00.000Z",
        "voteCount": 3,
        "content": "A- To build artifact\nD- To support automatic deployment rollback\nE- To provision the Codepipeline in minutes in other accounts or regions."
      },
      {
        "date": "2022-11-13T18:56:00.000Z",
        "voteCount": 2,
        "content": "A - for CI\nD - ASG and ALB are not in scope, hence I go with simple deployment. \nF - For provisioning into other AWS Account and region."
      },
      {
        "date": "2022-10-08T06:10:00.000Z",
        "voteCount": 2,
        "content": "ADF, CodeCommit dont have artefacts, CloudFormation is mandatory, ELB(ASG) are not a requirement, Codebuild to bild the project."
      },
      {
        "date": "2022-09-19T06:41:00.000Z",
        "voteCount": 2,
        "content": "ADF.\n\nA - YES. CodeBuild is needed for building the source code\nB - Artifact is built under CodeBuild's container environment, not codecommit\nC - LB ?\nD - YES. EB can be used for rollback\nE - Codepipeline itself can handle different stages\nF - YES. CF is needed from re-provision the same stuff"
      },
      {
        "date": "2022-08-31T21:00:00.000Z",
        "voteCount": 3,
        "content": "ABF - AB self explanatory.\n\nF - https://docs.aws.amazon.com/codepipeline/latest/userguide/tutorials-stackset-deployment.html\n\nu need a way to re-provision the pipelines in another region. CloudFormation is an option."
      },
      {
        "date": "2022-09-02T06:18:00.000Z",
        "voteCount": 3,
        "content": "ADF - to address \"continuous delivery, and automatic rollback upon deployment failure\""
      },
      {
        "date": "2022-09-02T13:48:00.000Z",
        "voteCount": 1,
        "content": "It might be ABF. F because question is not asking about application deployment, but how quickly you can re-provision the pipeline in another region. Cloudformation is an option to re-provision it quickly."
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 37,
    "url": "https://www.examtopics.com/discussions/amazon/view/82871-exam-aws-devops-engineer-professional-topic-1-question-37/",
    "body": "A company is building a solution for storing files containing Personally Identifiable Information (PII) on AWS.<br>Requirements state:<br>\u2711 All data must be encrypted at rest and in transit.<br>\u2711 All data must be replicated in at least two locations that are at least 500 miles (805 kilometers) apart.<br>Which solution meets these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate primary and secondary Amazon S3 buckets in two separate Availability Zones that are at least 500 miles (805 kilometers) apart. Use a bucket policy to enforce access to the buckets only through HTTPS. Use a bucket policy to enforce Amazon S3 SSE-C on all objects uploaded to the bucket. Configure cross- region replication between the two buckets.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate primary and secondary Amazon S3 buckets in two separate AWS Regions that are at least 500 miles (805 kilometers) apart. Use a bucket policy to enforce access to the buckets only through HTTPS. Use a bucket policy to enforce S3-Managed Keys (SSE-S3) on all objects uploaded to the bucket. Configure cross-region replication between the two buckets.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate primary and secondary Amazon S3 buckets in two separate AWS Regions that are at least 500 miles (805 kilometers) apart. Use an IAM role to enforce access to the buckets only through HTTPS. Use a bucket policy to enforce Amazon S3-Managed Keys (SSE-S3) on all objects uploaded to the bucket. Configure cross-region replication between the two buckets.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate primary and secondary Amazon S3 buckets in two separate Availability Zones that are at least 500 miles (805 kilometers) apart. Use a bucket policy to enforce access to the buckets only through HTTPS. Use a bucket policy to enforce AWS KMS encryption on all objects uploaded to the bucket. Configure cross-region replication between the two buckets. Create a KMS Customer Master Key (CMK) in the primary region for encrypting objects."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-02-05T04:05:00.000Z",
        "voteCount": 3,
        "content": "A and D are excluded because we need two regions.\nThe difference between B and C is to use \"IAM role\" or \"bucket policy\" to enforce access only through HTTPS.\nBucket policy is responsible for this type of jobs.  Reference: https://repost.aws/knowledge-center/s3-bucket-policy-for-config-rule"
      },
      {
        "date": "2023-01-16T12:09:00.000Z",
        "voteCount": 1,
        "content": "B us the right answer."
      },
      {
        "date": "2022-12-28T15:18:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2022-12-05T19:29:00.000Z",
        "voteCount": 1,
        "content": "I choose B"
      },
      {
        "date": "2022-10-08T06:13:00.000Z",
        "voteCount": 1,
        "content": "As its in another region and has encryption and replication."
      },
      {
        "date": "2022-09-20T19:23:00.000Z",
        "voteCount": 2,
        "content": "B\n\nhttps://www.examtopics.com/discussions/amazon/view/2753-exam-aws-devops-engineer-professional-topic-1-question-69/"
      },
      {
        "date": "2022-09-20T19:22:00.000Z",
        "voteCount": 2,
        "content": "Cross \"Region\" replication -"
      },
      {
        "date": "2022-09-19T20:41:00.000Z",
        "voteCount": 2,
        "content": "B for sure"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 38,
    "url": "https://www.examtopics.com/discussions/amazon/view/8455-exam-aws-devops-engineer-professional-topic-1-question-38/",
    "body": "A company is using an AWS CodeBuild project to build and package an application. The packages are copied to a shared Amazon S3 bucket before being deployed across multiple AWS accounts.<br>The buildspec.yml file contains the following:<br><img src=\"/assets/media/exam-media/04243/0002400001.png\" class=\"in-exam-image\"><br>The DevOps Engineer has noticed that anybody with an AWS account is able to download the artifacts.<br>What steps should the DevOps Engineer take to stop this?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the post_build to command to use \u05d2\u20ac\"-acl public-read and configure a bucket policy that grants read access to the relevant AWS accounts only.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a default ACL for the S3 bucket that defines the set of authenticated users as the relevant AWS accounts only and grants read-only access.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an S3 bucket policy that grants read access to the relevant AWS accounts and denies read access to the principal \u05d2\u20ac*\u05d2\u20ac",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the post_build command to remove \u05d2\u20ac\"-acl authenticated-read and configure a bucket policy that allows read access to the relevant AWS accounts only.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 6,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-09-24T06:00:00.000Z",
        "voteCount": 22,
        "content": "D is my choose\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/acl-overview.html"
      },
      {
        "date": "2021-10-06T17:47:00.000Z",
        "voteCount": 3,
        "content": "\"S3 ACLs is a legacy access control mechanism that predates IAM\". That is, don't use them! \nhttps://aws.amazon.com/blogs/security/iam-policies-and-bucket-policies-and-acls-oh-my-controlling-access-to-s3-resources/"
      },
      {
        "date": "2021-09-30T15:58:00.000Z",
        "voteCount": 7,
        "content": "i will go with D. C is not correct, if you have deny *, allow AuthenticatedUsers still doesn't allow the access. authenticated-read vs public-read, one is for Owner gets FULL_CONTROL. The AuthenticatedUsers group gets READ access., the other one is Owner gets FULL_CONTROL. The AllUsers group (see Who Is a Grantee?) gets READ access."
      },
      {
        "date": "2023-07-14T13:59:00.000Z",
        "voteCount": 1,
        "content": "D - When setting the flag authenticated-read in the command line, the owner gets FULL_CONTROL. The AuthenticatedUsers group (Anyone with an AWS account) gets READ access.\nReference: https://docs.aws.amazon.com/AmazonS3/latest/userguide/acl-overview.html"
      },
      {
        "date": "2023-02-16T17:12:00.000Z",
        "voteCount": 1,
        "content": "I think it's D, If you don't specify any acl command then acl is not enabled for the bucket which means that the objects accesses are defined only by the bucket policy."
      },
      {
        "date": "2023-02-05T05:36:00.000Z",
        "voteCount": 2,
        "content": "Of course, B and C are excluded. C is self-contradicted, B is not completed.\nD looks like the best, because AWS suggested to stop using ACL on S3 buckets.\nHowever, if we only remove the \"acl authenticated-read\", but not use bucket's ownership to change the default behaviour that uploader of the objects has access, the bucket's owner will not have access to the object and set up policies to the objects."
      },
      {
        "date": "2023-02-05T04:17:00.000Z",
        "voteCount": 1,
        "content": "D looks consistent."
      },
      {
        "date": "2023-02-05T04:38:00.000Z",
        "voteCount": 1,
        "content": "I changed my mind. I pick A."
      },
      {
        "date": "2023-02-28T05:42:00.000Z",
        "voteCount": 9,
        "content": "Why do you do that to every question? Why do you make a random comment and later starts to flip from one option to another... Stop that..."
      },
      {
        "date": "2023-01-16T12:49:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is D. Best practice is to limit the use of ACL and grant access using bucket policy. So in this case it's about allowing access to the bucket to specific AWS accounts and not to every authenticated user or all AWS accounts which are options A and B. I am not sure how Option C would work. It's talking about denying access to the principal. But I am not sure how access can be provided to the account but not to the Role or user in that account."
      },
      {
        "date": "2023-01-10T00:30:00.000Z",
        "voteCount": 1,
        "content": "D is my choice. AWS does not recommend the usage of ACL's. Below is the explanation from https://docs.aws.amazon.com/AmazonS3/latest/userguide/acl-overview.html \n\nA majority of modern use cases in Amazon S3 no longer require the use of ACLs, and we recommend that you disable ACLs except in unusual circumstances where you need to control access for each object individually. With Object Ownership, you can disable ACLs and rely on policies for access control. When you disable ACLs, you can easily maintain a bucket with objects uploaded by different AWS accounts. You, as the bucket owner, own all the objects in the bucket and can manage access to them using policies. For more information, see Controlling ownership of objects and disabling ACLs for your bucket."
      },
      {
        "date": "2022-12-06T07:55:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/AmazonS3/latest/userguide/about-object-ownership.html"
      },
      {
        "date": "2022-12-05T19:26:00.000Z",
        "voteCount": 1,
        "content": "I choose D"
      },
      {
        "date": "2022-11-30T09:54:00.000Z",
        "voteCount": 2,
        "content": "D is my choose\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/acl-overview.html"
      },
      {
        "date": "2022-02-08T14:54:00.000Z",
        "voteCount": 1,
        "content": "I choose D"
      },
      {
        "date": "2021-11-02T16:29:00.000Z",
        "voteCount": 2,
        "content": "Correct Answer: D\nYou can define bucket policy using NotPrincipal to grant permission to specified accounts or users while it explicitly denies access from other users.\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_elements_notprincipal.ht ml"
      },
      {
        "date": "2021-10-30T18:56:00.000Z",
        "voteCount": 1,
        "content": "C is my choose\nhttps://aws.amazon.com/blogs/security/how-to-restrict-amazon-s3-bucket-access-to-a-specific-iam-role/"
      },
      {
        "date": "2021-10-30T06:14:00.000Z",
        "voteCount": 1,
        "content": "I think the answer is A.\nhttps://docs.aws.amazon.com/cli/latest/userguide/cli-services-s3-commands.html\nWhen running s3 sync in the post_build section, granting public-read makes it available for download to anyone who knows the URL."
      },
      {
        "date": "2021-10-30T01:10:00.000Z",
        "voteCount": 1,
        "content": "I go with A. Most restrictive access. \npublic_read definition: Owner gets FULL_CONTROL. The AllUsers group (see Who Is a Grantee?) gets READ access.\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/acl-overview.html#canned-acl\n\nA grantee can be an AWS account or one of the predefined Amazon S3 groups. You grant permission to an AWS account using the email address or the canonical user ID. \nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/acl-overview.html#specifying-grantee"
      },
      {
        "date": "2021-10-28T20:43:00.000Z",
        "voteCount": 1,
        "content": "Answer is C"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 39,
    "url": "https://www.examtopics.com/discussions/amazon/view/78785-exam-aws-devops-engineer-professional-topic-1-question-39/",
    "body": "A DevOps engineer needs to grant several external contractors access to a legacy application that runs on an Amazon Linux Amazon EC2 instance. The application server is available only in a private subnet. The contractors are not authorized for VPN access.<br>What should the DevOps engineer do to grant the contactors access to the application server?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an IAM user and SSH keys for each contractor. Add the public SSH key to the application server's SSH authorized_keys file. Instruct the contractors to install the AWS CLI and AWS Systems Manager Session Manager plugin, update their AWS credentials files with their private keys, and use the aws ssm start-session command to gain access to the target application server instance ID.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAsk each contractor to securely send their SSH public key. Add this public key to the application server's SSH authorized-keys file. Instruct the contractors to use their private key to connect to the application server through SSH.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAsk each contractor to securely send their SSH public key. Use EC2 pairs to import their key. Update the application server's SSH authorized_keys file. Instruct the contractors to use their private key to connect to the application server through SSH.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an IAM user for each contractor with programmatic access. Add each user to an IAM group that has a policy that allows the ssm:StartSession action. Instruct the contractors to install the AWS CLI and AWS Systems Manager Session Manager plugin, update their AWS credentials files with their access keys, and use the aws ssm start-session to gain access to the target application server instance ID.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 11,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-05-01T06:52:00.000Z",
        "voteCount": 1,
        "content": "D is more relevant here."
      },
      {
        "date": "2023-03-11T11:07:00.000Z",
        "voteCount": 1,
        "content": "It's A or D but I go with D. \nOption A requires creating an SSH key pair for each contractor, which can be time-consuming if there are many contractors involved. Additionally, managing SSH keys can be challenging from a security perspective.\n\nOn the other hand, option D provides programmatic access, which is generally more secure than SSH keys. Contractors do not need to manage SSH keys, and access to the instance can be controlled using IAM policies."
      },
      {
        "date": "2023-07-14T14:07:00.000Z",
        "voteCount": 1,
        "content": "the point of connecting through SSM is exactly to avoid creating/managing SSH keys. so It's D"
      },
      {
        "date": "2023-03-02T07:13:00.000Z",
        "voteCount": 1,
        "content": "Using SSM looks more reasonable."
      },
      {
        "date": "2023-02-13T06:54:00.000Z",
        "voteCount": 2,
        "content": "The contractors are external ...."
      },
      {
        "date": "2023-03-03T02:34:00.000Z",
        "voteCount": 1,
        "content": "The question does not mention that contractors are external... it says, they are not allowed to use VPN."
      },
      {
        "date": "2023-08-27T06:37:00.000Z",
        "voteCount": 1,
        "content": "\"to grant several external contractors\"\ni like B answer, but this is private network, so additionally bastion should be used, in that case."
      },
      {
        "date": "2023-02-07T13:34:00.000Z",
        "voteCount": 2,
        "content": "the answer is D"
      },
      {
        "date": "2023-02-05T05:44:00.000Z",
        "voteCount": 1,
        "content": "D is the most AWS-managed option."
      },
      {
        "date": "2023-01-16T12:56:00.000Z",
        "voteCount": 1,
        "content": "D is the right answer. AWS credentials to use AWS CLI to start AWS session manager is the right answer."
      },
      {
        "date": "2022-12-28T15:34:00.000Z",
        "voteCount": 2,
        "content": "D for sure"
      },
      {
        "date": "2022-12-13T20:46:00.000Z",
        "voteCount": 2,
        "content": "All other answers involve a lot of hassles involving addition and removal of public and private keys. In option D you just need to create/delete and add/remove users from group for all future access"
      },
      {
        "date": "2022-12-05T19:22:00.000Z",
        "voteCount": 1,
        "content": "D for me"
      },
      {
        "date": "2022-12-04T04:52:00.000Z",
        "voteCount": 2,
        "content": "https://aws.amazon.com/premiumsupport/knowledge-center/systems-manager-ssh-vpc-resources/"
      },
      {
        "date": "2022-11-13T23:51:00.000Z",
        "voteCount": 2,
        "content": "Use AWS System Manager for easy login"
      },
      {
        "date": "2022-09-27T02:46:00.000Z",
        "voteCount": 3,
        "content": "I would say D if contractors belong to the same company, on the other hand B"
      },
      {
        "date": "2023-02-13T06:54:00.000Z",
        "voteCount": 1,
        "content": "I agree.  B is strict."
      },
      {
        "date": "2022-09-06T05:07:00.000Z",
        "voteCount": 2,
        "content": "Ans: D"
      },
      {
        "date": "2022-09-01T06:21:00.000Z",
        "voteCount": 1,
        "content": "D is the correct answer"
      },
      {
        "date": "2022-08-31T18:52:00.000Z",
        "voteCount": 1,
        "content": "D pls use AWS SSM when possible"
      },
      {
        "date": "2022-08-31T14:38:00.000Z",
        "voteCount": 1,
        "content": "B seems correct. \n\nhttps://aws.amazon.com/premiumsupport/knowledge-center/ec2-ssh-best-practices/\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html"
      },
      {
        "date": "2022-09-02T13:56:00.000Z",
        "voteCount": 1,
        "content": "My bad. D seems correct."
      },
      {
        "date": "2022-09-02T13:57:00.000Z",
        "voteCount": 3,
        "content": "Session Manager allows AWS Identity and Access Management (IAM) users to log in to your instances with encryption and logging capabilities. Systems Manager's traffic goes through the Systems Manager Endpoint, allowing easy and secure access to private instances without opening inbound ports."
      },
      {
        "date": "2022-12-14T01:13:00.000Z",
        "voteCount": 1,
        "content": "if set up only one user, how to trace the user behavior since the are more than one contractor?"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 40,
    "url": "https://www.examtopics.com/discussions/amazon/view/47001-exam-aws-devops-engineer-professional-topic-1-question-40/",
    "body": "A company hosts its staging website using an Amazon EC2 instance backed with Amazon EBS storage. The company wants to recover quickly with minimal data losses in the event of network connectivity issues or power failures on the EC2 instance.<br>Which solution will meet these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd the instance to an EC2 Auto Scaling group with the minimum, maximum, and desired capacity set to 1.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd the instance to an EC2 Auto Scaling group with a lifecycle hook to detach the EBS volume when the EC2 instance shuts down or terminates.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon CloudWatch alarm for the StatusCheckFailed_System metric and select the EC2 action to recover the instance.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon CloudWatch alarm for the StatusCheckFailed_Instance metric and select the EC2 action to reboot the instance."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 10,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-11-03T02:08:00.000Z",
        "voteCount": 7,
        "content": "Answer C"
      },
      {
        "date": "2023-01-10T01:41:00.000Z",
        "voteCount": 5,
        "content": "Answer C. Here is all the information you need. \nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-recover.html"
      },
      {
        "date": "2023-05-23T06:48:00.000Z",
        "voteCount": 1,
        "content": "c-c-c-c-c-c-"
      },
      {
        "date": "2023-02-05T05:56:00.000Z",
        "voteCount": 3,
        "content": "B is the only option satisfying the requirement of \"minimal data losses\"."
      },
      {
        "date": "2023-02-05T06:02:00.000Z",
        "voteCount": 1,
        "content": "Haha, I was in sleep"
      },
      {
        "date": "2023-02-13T06:47:00.000Z",
        "voteCount": 1,
        "content": "The link from PepsNick helps."
      },
      {
        "date": "2023-01-16T14:56:00.000Z",
        "voteCount": 1,
        "content": "Anwer is C"
      },
      {
        "date": "2022-11-25T21:01:00.000Z",
        "voteCount": 2,
        "content": "C\nStatusCheckFailed_System"
      },
      {
        "date": "2022-09-13T07:45:00.000Z",
        "voteCount": 3,
        "content": "The recover action can be used only with StatusCheckFailed_System, not with StatusCheckFailed_Instance.\n-&gt; I will go with C"
      },
      {
        "date": "2022-04-12T05:53:00.000Z",
        "voteCount": 2,
        "content": "C - cw alarm for the statuscheckfailed_system metric + ec2 action to recover the instance"
      },
      {
        "date": "2022-01-31T01:43:00.000Z",
        "voteCount": 1,
        "content": "Answer C"
      },
      {
        "date": "2021-11-05T17:17:00.000Z",
        "voteCount": 4,
        "content": "A is best and easiest answer"
      },
      {
        "date": "2022-01-11T20:31:00.000Z",
        "voteCount": 5,
        "content": "No, the question mentions that \"EC2 instance that is backed up by Amazon EBS storage\", if you use ASG, you need to configure based AMI, which means data loss more, I will choose C."
      },
      {
        "date": "2021-10-28T09:03:00.000Z",
        "voteCount": 1,
        "content": "Answer is C"
      },
      {
        "date": "2021-10-12T08:26:00.000Z",
        "voteCount": 4,
        "content": "I'll go with C"
      },
      {
        "date": "2021-10-11T15:00:00.000Z",
        "voteCount": 1,
        "content": "Ans: C"
      },
      {
        "date": "2021-09-22T22:00:00.000Z",
        "voteCount": 3,
        "content": "C.\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/UsingAlarmActions.html#AddingRecoverActions"
      },
      {
        "date": "2021-09-22T10:14:00.000Z",
        "voteCount": 1,
        "content": "ans: C"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 41,
    "url": "https://www.examtopics.com/discussions/amazon/view/46934-exam-aws-devops-engineer-professional-topic-1-question-41/",
    "body": "A company has built a web service that runs on Amazon EC2 instances behind an Application Load Balancer (ALB). The company has deployed the application in us-east-1. Amazon Route 53 provides an external DNS that routes traffic from example.com to the application, created with appropriate health checks.<br>The company has deployed a second environment for the application in eu-west-1. The company wants traffic to be routed to whichever environment results in the best response time for each user. If there is an outage in one Region, traffic should be directed to the other environment.<br>Which configuration will achieve these requirements?<br>A.<br>\u2711 A subdomain us.example.com with weighted routing: the US ALB with weight 2 and the EU ALB with weight 1.<br>\u2711 Another subdomain eu.example.com with weighted routing: the EU ALB with weight 2 and the US ALB with weight 1.<br>\u2711 Geolocation routing records for example.com: North America aliased to us.example.com and Europe aliased to eu.example.com.<br>B.<br>\u2711 A subdomain us.example.com with latency-based routing: the US ALB as the first target and the EU ALB as the second target.<br>\u2711 Another subdomain eu.example.com with latency-based routing: the EU ALB as the first target and the US ALB as the second target.<br>\u2711 Failover routing records for example.com aliased to us.example.com as the first target and eu.example.com as the second target.<br>C.<br>\u2711 A subdomain us.example.com with failover routing: the US ALB as primary and the EU ALB as secondary.<br>\u2711 Another subdomain eu.example.com with failover routing: the EU ALB as primary and the US ALB as secondary.<br>\u2711 Latency-based routing records for example.com that are aliased to us.example.com and eu.example.com.<br>D.<br>\u2711 A subdomain us.example.com with multivalue answer routing: the US ALB first and the EU ALB second.<br>\u2711 Another subdomain eu.example.com with multivalue answer routing: the EU ALB first and the US ALB second.<br>\u2711 Failover routing records for example.com that are aliased to us.example.com and eu.example.com.<br>",
    "options": [],
    "answer": "C",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2021-10-29T07:37:00.000Z",
        "voteCount": 11,
        "content": "C is correct (reveal answer didnt show anything here)"
      },
      {
        "date": "2024-05-30T22:20:00.000Z",
        "voteCount": 1,
        "content": "C is answer"
      },
      {
        "date": "2023-11-29T05:56:00.000Z",
        "voteCount": 2,
        "content": "B is correct as only latency based routing can provide best response time."
      },
      {
        "date": "2023-10-22T01:17:00.000Z",
        "voteCount": 2,
        "content": "B. \n\nOption A: This option uses weighted routing which does not fulfill the requirement of routing traffic based on best response time.\n\nOption B: This option uses latency-based routing for subdomains which can route traffic based on best response time. It also uses failover routing for the main domain which can route traffic to the other environment in case of an outage. So, this option is correct.\n\nOption C: This option uses failover routing for subdomains which does not fulfill the requirement of routing traffic based on best response time.\n\nOption D: This option uses multivalue answer routing for subdomains which does not fulfill the requirement of routing traffic based on best response time."
      },
      {
        "date": "2023-01-16T15:02:00.000Z",
        "voteCount": 1,
        "content": "C is the right answer. B isn't right as it sets up failover routing on example.com to first route to us.example.com and then to eu.example.com which is not how failover should be configured. Latency should be configured at example.com to route request to us or eu region based on the best response time for each user and this is done in C."
      },
      {
        "date": "2023-01-12T08:53:00.000Z",
        "voteCount": 1,
        "content": "B is correct. remember \"best response time for user\" is required.\nThis configuration will achieve the company's requirements because it uses Route 53's latency-based routing feature to route traffic to the environment with the best response time for each user. It also uses Route 53's failover routing feature to automatically route traffic to the other environment in the event of an outage in one region."
      },
      {
        "date": "2023-02-26T14:44:00.000Z",
        "voteCount": 1,
        "content": "I think C.\n\nLetter C has \"\u2711 Latency-based routing records for example.com that are aliased to us.example.com and eu.example.com.\" which answers the requirement of \"best response time for user\""
      },
      {
        "date": "2022-12-29T14:39:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2022-09-17T15:37:00.000Z",
        "voteCount": 2,
        "content": "C \ninitially was thinking B, but i think it is more common to put failover at DNS level. https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-configuring.html"
      },
      {
        "date": "2022-06-16T15:41:00.000Z",
        "voteCount": 4,
        "content": "Any idea why not b?"
      },
      {
        "date": "2022-06-16T15:27:00.000Z",
        "voteCount": 1,
        "content": "C \nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-configuring.html"
      },
      {
        "date": "2022-02-13T06:37:00.000Z",
        "voteCount": 2,
        "content": "C is right, altho you can just use geoproximity, it has shift over capacity too."
      },
      {
        "date": "2021-10-26T21:21:00.000Z",
        "voteCount": 2,
        "content": "I'll go with C"
      },
      {
        "date": "2021-10-14T22:38:00.000Z",
        "voteCount": 1,
        "content": "Ans: C"
      },
      {
        "date": "2021-10-07T21:44:00.000Z",
        "voteCount": 1,
        "content": "I wll go with C"
      },
      {
        "date": "2021-09-29T05:10:00.000Z",
        "voteCount": 1,
        "content": "ans: A"
      },
      {
        "date": "2021-10-22T16:13:00.000Z",
        "voteCount": 3,
        "content": "A is wrong, cause you need to create the failover for each region domain first... and after you can create a latency based"
      },
      {
        "date": "2021-09-22T15:10:00.000Z",
        "voteCount": 1,
        "content": "C is coorect"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 42,
    "url": "https://www.examtopics.com/discussions/amazon/view/47270-exam-aws-devops-engineer-professional-topic-1-question-42/",
    "body": "A company has multiple development teams sharing one AWS account. The development team's manager wants to be able to automatically stop Amazon EC2 instances and receive notifications if resources are idle and not tagged as production resources.<br>Which solution will meet these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a scheduled Amazon CloudWatch Events rule to filter for Amazon EC2 instance status checks and identify idle EC2 instances. Use the CloudWatch Events rule to target an AWS Lambda function to stop non-production instances and send notifications.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a scheduled Amazon CloudWatch Events rule to filter AWS Systems Manager events and identify idle EC2 instances and resources. Use the CloudWatch Events rule to target an AWS Lambda function to stop non-production instances and send notifications.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a scheduled Amazon CloudWatch Events rule to target a custom AWS Lambda function that runs AWS Trusted Advisor checks. Create a second CloudWatch Events rule to filter events from Trusted Advisor to trigger a Lambda function to stop idle non-production instances and send notifications.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a scheduled Amazon CloudWatch Events rule to target Amazon Inspector events for idle EC2 instances. Use the CloudWatch Events rule to target the AWS Lambda function to stop non-production instances and send notifications."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 6,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-10-24T06:27:00.000Z",
        "voteCount": 24,
        "content": "I'll go with C\n\nReferences:\nhttps://docs.aws.am\nazon.com/awssupport/latest/user/cloudwatch-ta.html\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/events/EventTypes.html#trusted-advisor-event-types\nhttps://aws.amazon.com/premiumsupport/technology/trusted-advisor/"
      },
      {
        "date": "2021-10-27T01:34:00.000Z",
        "voteCount": 2,
        "content": "I will go with C too.\nhttps://gist.github.com/sudharsans/af23ee7e8919947af83ceb269a40d8db\nhttps://docs.aws.amazon.com/awssupport/latest/user/cloudwatch-events-ta.html"
      },
      {
        "date": "2022-12-13T20:58:00.000Z",
        "voteCount": 6,
        "content": "A - No such cloudwatch event which identifies idle EC2 instances\nB - No such AWS System Manager Event that lets you know the idle resources\nC - Correct Answer. Trusted Advisor Checks lets you know idle EC2 instances\nD - Amazon Inspector is for managing vulnerabilities"
      },
      {
        "date": "2023-11-23T08:41:00.000Z",
        "voteCount": 1,
        "content": "C because Trusted Advisor have check Low CPU."
      },
      {
        "date": "2023-02-05T06:18:00.000Z",
        "voteCount": 3,
        "content": "D is eliminated, Amazon Inspector is used to scan vulnerabilities. \nC is eliminated, AWS Trusted Advisor checks accounts.\nB is eliminated, AWS Systems Manager manages resource on AWS and on-premises."
      },
      {
        "date": "2023-02-05T06:23:00.000Z",
        "voteCount": 1,
        "content": "The links offered by JohnnieWalker changed my mind from A to C"
      },
      {
        "date": "2023-01-16T15:08:00.000Z",
        "voteCount": 2,
        "content": "C is the correct answer. Trusted Advisor is used to diagnose issues with your infrastructure including idle EC2 instances."
      },
      {
        "date": "2022-12-29T14:50:00.000Z",
        "voteCount": 2,
        "content": "C for sure"
      },
      {
        "date": "2021-10-23T13:04:00.000Z",
        "voteCount": 2,
        "content": "A - CW event for ec2 state and lambda to stop ec2"
      },
      {
        "date": "2021-10-23T22:08:00.000Z",
        "voteCount": 4,
        "content": "This makes A wrong:\nCloudWatch Events rule to filter for Amazon EC2 &gt;&gt;&gt;INSTANCE STATUS CHECKS AND IDENTIFY IDLE&lt;&lt;&lt;  EC2 instances\n????\n\nThere are two types of status checks: system status checks and instance status checks, AND NONE of them has the \"Idle\" status\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/monitoring-system-instance-status-check.html#types-of-instance-status-checks"
      },
      {
        "date": "2023-01-12T09:04:00.000Z",
        "voteCount": 1,
        "content": "CloudWatch does not have a built-in event that specifically identifies the idle status of EC2 instances, agreed. However, you can create a custom event using CloudWatch Events and a Lambda function to identify idle instances by monitoring for specific conditions that indicate an idle status, such as low CPU/network usage"
      },
      {
        "date": "2023-02-05T06:24:00.000Z",
        "voteCount": 1,
        "content": "I agree.  However, the option C is doing what you suggested, but using Trust Advisor instead of CloudWatch.  A did not say that it will use CloudWatch Event + Lambda function"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 43,
    "url": "https://www.examtopics.com/discussions/amazon/view/28583-exam-aws-devops-engineer-professional-topic-1-question-43/",
    "body": "An n-tier application requires a table in an Amazon RDS MySQL DB instance to be dropped and repopulated at each deployment. This process can take several minutes and the web tier cannot come online until the process is complete. Currently, the web tier is configured in an Amazon EC2 Auto Scaling group, with instances being terminated and replaced at each deployment. The MySQL table is populated by running a SQL query through an AWS CodeBuild job.<br>What should be done to ensure that the web tier does not come online before the database is completely configured?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Aurora as a drop-in replacement for RDS MySQL. Use snapshots to populate the table with the correct data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the launch configuration of the Auto Scaling group to pause user data execution for 600 seconds, allowing the table to be populated.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Step Functions to monitor and maintain the state of data population. Mark the database in service before continuing with the deployment.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an EC2 Auto Scaling lifecycle hook to pause the configuration of the web tier until the table is populated.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-30T14:24:00.000Z",
        "voteCount": 13,
        "content": "D. \nLifecycle hook can trigger lambda to check DB status: https://docs.aws.amazon.com/autoscaling/ec2/userguide/configuring-lifecycle-hook-notifications.html"
      },
      {
        "date": "2023-02-05T06:38:00.000Z",
        "voteCount": 2,
        "content": "A and B are just weird.\nC is eliminated, AWS Step Functions is for workflow of complex business logic."
      },
      {
        "date": "2023-01-16T15:11:00.000Z",
        "voteCount": 1,
        "content": "D is the correct answer."
      },
      {
        "date": "2022-11-25T22:08:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/autoscaling/ec2/userguide/configuring-lifecycle-hook-notifications.html"
      },
      {
        "date": "2021-11-01T09:53:00.000Z",
        "voteCount": 1,
        "content": "repopulating table needs to happen at each deployment, not at each ASG scale-in/out event. with option D, when the ASG scales out, it pauses and triggers the DB table repopulating. is that what we want?"
      },
      {
        "date": "2021-10-14T02:59:00.000Z",
        "voteCount": 1,
        "content": "Only D can be the answer"
      },
      {
        "date": "2021-10-07T18:14:00.000Z",
        "voteCount": 3,
        "content": "I'll go with D"
      },
      {
        "date": "2021-10-07T10:51:00.000Z",
        "voteCount": 1,
        "content": "D is the answer"
      },
      {
        "date": "2021-10-06T00:19:00.000Z",
        "voteCount": 1,
        "content": "Go with D"
      },
      {
        "date": "2021-09-24T23:20:00.000Z",
        "voteCount": 3,
        "content": "Assuming we want to 'guarantee' that database is ready before web tier requires some verification likely through Lambda. \n\nA &amp; B is out. D is applicable but it doesn't quite guarantee. Also, lifecycle hook is not triggering any Lambda for verification. \n\nC is correct. Assuming we are using CodePipeline, after CodeBuild, we invoke Stepfunctions that will periodically check table population through lambda."
      },
      {
        "date": "2021-09-30T08:32:00.000Z",
        "voteCount": 3,
        "content": "C does not ensure the portal isn't coming online. ASG Life cycle hooks are the only way to make it pause, monitor the state of DB and allow it to proceed later. All of this is made through Lambda.\nSo D is the right answer."
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 44,
    "url": "https://www.examtopics.com/discussions/amazon/view/28082-exam-aws-devops-engineer-professional-topic-1-question-44/",
    "body": "A highly regulated company has a policy that DevOps Engineers should not log in to their Amazon EC2 instances except in emergencies. If a DevOps Engineer does log in, the Security team must be notified within 15 minutes of the occurrence.<br>Which solution will meet these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall the Amazon Inspector agent on each EC2 instance. Subscribe to Amazon CloudWatch Events notifications. Trigger an AWS Lambda function to check if a message is about user logins. If it is, send a notification to the Security team using Amazon SNS.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall the Amazon CloudWatch agent on each EC2 instance. Configure the agent to push all logs to Amazon CloudWatch Logs and set up a CloudWatch metric filter that searches for user logins. If a login is found, send a notification to the Security team using Amazon SNS.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up AWS CloudTrail with Amazon CloudWatch Logs. Subscribe CloudWatch Logs to Amazon Kinesis. Attach AWS Lambda to Kinesis to parse and determine if a log contains a user login. If it does, send a notification to the Security team using Amazon SNS.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up a script on each Amazon EC2 instance to push all logs to Amazon S3. Set up an S3 event to trigger an AWS Lambda function, which triggers an Amazon Athena query to run. The Athena query checks for logins and sends the output to the Security team using Amazon SNS."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 10,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 5,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-10-09T03:33:00.000Z",
        "voteCount": 14,
        "content": "https://aws.amazon.com/blogs/security/how-to-monitor-and-visualize-failed-ssh-access-attempts-to-amazon-ec2-linux-instances/ \n\nB"
      },
      {
        "date": "2022-12-29T15:28:00.000Z",
        "voteCount": 1,
        "content": "Thanks for the link"
      },
      {
        "date": "2021-10-19T06:44:00.000Z",
        "voteCount": 1,
        "content": "This is not about failed access."
      },
      {
        "date": "2021-12-26T12:15:00.000Z",
        "voteCount": 3,
        "content": "Bruh. Stop being that person who just read the URL string and not bothered to read or at least skim the article. The technique listed in here could easily be adapted for login attempts."
      },
      {
        "date": "2021-12-26T12:15:00.000Z",
        "voteCount": 2,
        "content": "Excuse me, the technique listed in here could easily be adapted for successful logins*"
      },
      {
        "date": "2021-10-23T15:21:00.000Z",
        "voteCount": 1,
        "content": "Thanks ofr the link"
      },
      {
        "date": "2022-09-11T23:51:00.000Z",
        "voteCount": 2,
        "content": "A CloudWatch Logs agent runs on each EC2 instance. The agents are configured to send SSH logs from the EC2 instance to a log stream identified by an instance ID.\nLog streams are aggregated into a log group. As a result, one log group contains all the logs you want to analyze from one or more instances.\nYou apply metric filters to a log group in order to search for specific keywords. When the metric filter finds specific keywords, the filter counts the occurrences of the keywords in a time-based sliding window. If the occurrence of a keyword exceeds the CloudWatch alarm threshold, an alarm is triggered."
      },
      {
        "date": "2023-10-17T11:35:00.000Z",
        "voteCount": 1,
        "content": "B, Cloud watch agent can collect OS logs included ssh logins"
      },
      {
        "date": "2023-09-10T09:24:00.000Z",
        "voteCount": 1,
        "content": "vote for B"
      },
      {
        "date": "2023-05-01T07:54:00.000Z",
        "voteCount": 2,
        "content": "B is more suitable answer for this scenario."
      },
      {
        "date": "2023-02-13T05:56:00.000Z",
        "voteCount": 1,
        "content": "The SSH example given in other comments is only for Linux. The question asks about EC2, so we have to factor in other types as well. So C seems the better option, as with cloudtrail we are still able to meet the 15 min timeline"
      },
      {
        "date": "2023-02-05T06:47:00.000Z",
        "voteCount": 1,
        "content": "A and B are eliminated, because it is error-prone to install something on each EC2 instance.\nD is more error-prone."
      },
      {
        "date": "2023-02-13T06:25:00.000Z",
        "voteCount": 1,
        "content": "I am in the second round, now I pick B."
      },
      {
        "date": "2023-01-16T15:24:00.000Z",
        "voteCount": 1,
        "content": "Answer is B. Within 15 min is the key. Cloudtrail logs cannot be analyzed within 15 min as it doesn't get to CloudWatch log within 15 min."
      },
      {
        "date": "2022-12-29T15:27:00.000Z",
        "voteCount": 2,
        "content": "B is correct\nA CloudWatch Logs agent runs on each EC2 instance. The agents are configured to send SSH logs from the EC2 instance to a log stream identified by an instance ID.\nhttps://aws.amazon.com/blogs/security/how-to-monitor-and-visualize-failed-ssh-access-attempts-to-amazon-ec2-linux-instances/"
      },
      {
        "date": "2022-12-19T01:51:00.000Z",
        "voteCount": 2,
        "content": "B\nC is incorrect because CloudTrail logs can take up to 15 minutes to record an event."
      },
      {
        "date": "2023-02-23T00:52:00.000Z",
        "voteCount": 1,
        "content": "yes, this is the point"
      },
      {
        "date": "2022-12-13T21:44:00.000Z",
        "voteCount": 4,
        "content": "A - Inspector is for managing vulnerabilities\nB - Correct Answer, Cloudwatch agent can scan through the EC2 instances for SSH login logs\nC - Logging into an AWS EC2 instances is not traced by CloudTrail (Only if we use SSM but nothing is mentioned about it in the question).\nD - Too much work involved to do things"
      },
      {
        "date": "2022-12-11T09:51:00.000Z",
        "voteCount": 1,
        "content": "The correct solution is C. Setting up AWS CloudTrail with Amazon CloudWatch Logs, subscribing CloudWatch Logs to Amazon Kinesis, and attaching an AWS Lambda function to Kinesis to parse the logs and determine if they contain user logins will meet the requirements of the question. This solution will allow the Security team to be notified within 15 minutes of any user logins on the EC2 instances."
      },
      {
        "date": "2022-11-13T07:45:00.000Z",
        "voteCount": 3,
        "content": "B is correct because is simple and fast. C is an over kill and complicated"
      },
      {
        "date": "2022-11-06T08:02:00.000Z",
        "voteCount": 1,
        "content": "C as the explenation says : https://docs.aws.amazon.com/IAM/latest/UserGuide/security-logging-and-monitoring.html"
      },
      {
        "date": "2022-09-06T05:42:00.000Z",
        "voteCount": 3,
        "content": "Ans: C\nLogin requests will be logged only in cloudtrail not cloudwatch"
      },
      {
        "date": "2022-09-09T07:09:00.000Z",
        "voteCount": 3,
        "content": "C should be wrong. Since the log of login to EC2 instance belong OS level. Cloudtrail don't log OS level event."
      },
      {
        "date": "2022-02-13T01:50:00.000Z",
        "voteCount": 4,
        "content": "B is correct, as the question states within 15 minutes. If it says immediately then it would be C. There might be multiple options that can do it, we always need to pick the best one, which includes lowest possible cost as well."
      },
      {
        "date": "2021-11-06T09:01:00.000Z",
        "voteCount": 2,
        "content": "B would be a perfect answer if it mentioned creating a CloudWatch Alarm: having a CloudWatch metric filter along doesn't trigger anything.\n\nI think C is technically possible but involves too much overhead such as building your Lambda function and paying for Kinesis. It's preventing the wheels"
      },
      {
        "date": "2021-10-22T07:33:00.000Z",
        "voteCount": 3,
        "content": "Answer is B\nhttps://medium.com/@matthewleearthur/alerting-on-successful-ec2-ssh-logins-6b97ccfb33eb"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 45,
    "url": "https://www.examtopics.com/discussions/amazon/view/78792-exam-aws-devops-engineer-professional-topic-1-question-45/",
    "body": "A DevOps engineer has automated a web service deployment by using AWS CodePipeline with the following steps:<br>1. An AWS CodeBuild project compiles the deployment artifact and runs unit tests.<br>2. An AWS CodeDeploy deployment group deploys the web service to Amazon EC2 instances in the staging environment.<br>3. A CodeDeploy deployment group deploys the web service to EC2 instances in the production environment.<br>The quality assurance (QA) team requests permission to inspect the build artifact before the deployment to the production environment occurs. The QA team wants to run an internal penetration testing tool to conduct manual tests. The tool will be invoked by a REST API call.<br>Which combination of actions should the DevOps engineer take to fulfill this request? (Choose two.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInsert a manual approval action between the test actions and deployment actions of the pipeline.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the buildspec.yml file for the compilation stage to require manual approval before completion.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the CodeDeploy deployment groups so that they require manual approval to proceed.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the pipeline to directly call the REST API for the penetration testing tool.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the pipeline to invoke a Lambda function that calls the REST API for the penetration testing tool.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "AE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AE",
        "count": 7,
        "isMostVoted": true
      },
      {
        "answer": "BE",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-08-31T15:52:00.000Z",
        "voteCount": 13,
        "content": "A and E - https://www.examtopics.com/discussions/amazon/view/49433-exam-aws-devops-engineer-professional-topic-1-question-190/"
      },
      {
        "date": "2023-10-17T11:39:00.000Z",
        "voteCount": 1,
        "content": "AE\nCode pipeline itself can't send api requests, it should use either codebuild or lambda. In addition, approval step can be set in codepipline and not in code build."
      },
      {
        "date": "2023-05-01T07:58:00.000Z",
        "voteCount": 1,
        "content": "AE are most suitable option here."
      },
      {
        "date": "2023-02-25T22:10:00.000Z",
        "voteCount": 1,
        "content": "Agree with ohcn, AE is correct."
      },
      {
        "date": "2023-02-09T19:42:00.000Z",
        "voteCount": 1,
        "content": "agree with ohcn"
      },
      {
        "date": "2023-02-05T07:29:00.000Z",
        "voteCount": 1,
        "content": "Do not understand why C is in the correct answer.\nBetween A and B. A looks straightforward.  \nHowever, I prefer B, because it is more automatic and integrated."
      },
      {
        "date": "2023-02-25T22:12:00.000Z",
        "voteCount": 1,
        "content": "I am not sure about B, as we cannot add approval to buildspec.yaml\nhttps://www.reddit.com/r/aws/comments/x7e0na/in_the_buildspecyml_for_aws_codebuild_is_it/"
      },
      {
        "date": "2023-01-16T15:26:00.000Z",
        "voteCount": 1,
        "content": "A &amp; E are the right answers"
      },
      {
        "date": "2023-01-10T02:04:00.000Z",
        "voteCount": 2,
        "content": "AE. Here is information about adding a manual approval step which is required. https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals-action-add.html\nYou call a lambda function in parallel to the approval to trigger the API."
      },
      {
        "date": "2022-12-29T15:36:00.000Z",
        "voteCount": 1,
        "content": "A and E"
      },
      {
        "date": "2022-12-22T02:50:00.000Z",
        "voteCount": 1,
        "content": "AE is the correct option"
      },
      {
        "date": "2022-09-20T22:27:00.000Z",
        "voteCount": 2,
        "content": "A and E"
      },
      {
        "date": "2022-09-05T04:11:00.000Z",
        "voteCount": 2,
        "content": "E for sure, not sure about A"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 46,
    "url": "https://www.examtopics.com/discussions/amazon/view/78795-exam-aws-devops-engineer-professional-topic-1-question-46/",
    "body": "A DevOps Engineer manages a large commercial website that runs on Amazon EC2. The website uses Amazon Kinesis Data Streams to collect and process web logs. The DevOps Engineer manages the Kinesis consumer application, which also runs on Amazon EC2.<br>Sudden increases of data cause the Kinesis consumer application to fall behind, and the Kinesis data streams drop records before the records can be processed.<br>The DevOps Engineer must implement a solution to improve stream handling.<br>Which solution meets these requirements with the MOST operational efficiency?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the Kinesis consumer application to store the logs durably in Amazon S3. Use Amazon EMR to process the data directly on Amazon S3 to derive customer insights. Store the results in Amazon S3.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tHorizontally scale the Kinesis consumer application by adding more EC2 instances based on the Amazon CloudWatch GetRecords.IteratorAgeMilliseconds metric. Increase the retention period of the Kinesis Data Streams.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConvert the Kinesis consumer application to run as an AWS Lambda function. Configure the Kinesis Data Streams as the event source for the Lambda function to process the data streams.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the number of shards in the Kinesis Data Streams to increase the overall throughput so that the consumer application processes data faster."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 13,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 13,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-08-31T15:58:00.000Z",
        "voteCount": 6,
        "content": "B - https://www.examtopics.com/discussions/amazon/view/8544-exam-aws-devops-engineer-professional-topic-1-question-129/"
      },
      {
        "date": "2024-10-10T13:04:00.000Z",
        "voteCount": 1,
        "content": "C - Lambda is more operationally efficient and it out scales.\nIt runs in parallel (multi-thread) to process Kinesis records, which would resolve the issue of the consumer falling behind."
      },
      {
        "date": "2024-06-08T07:39:00.000Z",
        "voteCount": 2,
        "content": "Ans is C , \nkey here is \"MOST operational efficiency\" , when compare EC2 vs Lambda , Lambda are more operationally efficient, with EC2 you need to operationally manage it like patching etc etc. though B will also fit , but Operationally efficient would be C\n\nwith regard to application falling behind it can be achieved by lambda ParallelizationFactor , see AWS Documentation \nwhen you set ParallelizationFactor to 2, you can have 200 concurrent Lambda invocations at maximum to process 100 Kinesis data shards (though in practice, you may see a different values for the ConcurrentExecutions metric). This helps scale up the processing throughput when the data volume is volatile and the IteratorAge is high.\n\nhttps://docs.aws.amazon.com/lambda/latest/dg/with-kinesis.html"
      },
      {
        "date": "2024-01-16T00:29:00.000Z",
        "voteCount": 1,
        "content": "more efficient than b"
      },
      {
        "date": "2023-02-26T21:03:00.000Z",
        "voteCount": 2,
        "content": "B is the correct one."
      },
      {
        "date": "2023-02-23T00:58:00.000Z",
        "voteCount": 4,
        "content": "I think the answer is B."
      },
      {
        "date": "2023-02-05T07:38:00.000Z",
        "voteCount": 3,
        "content": "I changed my mind to B.\nThe throughput problem is not from Kinesis Data Streams, but from the EC2 hosting Kinesis Data Streams.\nSo we need to scale up the EC2 instances."
      },
      {
        "date": "2024-06-08T07:42:00.000Z",
        "voteCount": 1,
        "content": "you are right , and lambda ParallelizationFactor can help to solve this problem and Lambda or operationally more efficient than EC2. So answer is C and not B"
      },
      {
        "date": "2023-02-05T07:33:00.000Z",
        "voteCount": 3,
        "content": "We need to use the in-built auto-scaling mechanism in Kinesis Data Streams to solve the problem."
      },
      {
        "date": "2023-02-05T07:38:00.000Z",
        "voteCount": 1,
        "content": "I changed my mind to B. \nThe throughput problem is not from Kinesis Data Streams, but from the EC2 hosting Kinesis Data Streams.\nSo we need to scale up the EC2 instances."
      },
      {
        "date": "2023-01-16T15:42:00.000Z",
        "voteCount": 3,
        "content": "Answer is B. We need to scale the Kinesis consumer application horizontally based on the CloudWatch metric. Answer cannot be C because replacing the Kinesis Consumer app with Lambda does not make the solution operationally efficient. Lambda could also throttle as the data is streamed faster into Kinesis Data Stream. Answer is not D because just my increasing the shards will not result in efficient processing on the consumer side. We also need to increase the no of instances and therefore KCL workers to propose the data from additional shards."
      },
      {
        "date": "2023-01-10T02:07:00.000Z",
        "voteCount": 1,
        "content": "B. They are asking for operational efficiency. Converting the application to lambda may not be possible and is not efficient."
      },
      {
        "date": "2023-01-08T23:34:00.000Z",
        "voteCount": 3,
        "content": "Why not D?\nD. Increase the number of shards in the Kinesis Data Streams to increase the overall throughput so that the consumer application can process the data faster.\n\nIncreasing the number of shards in the Kinesis Data Streams is the most operationally efficient solution because it allows the consumer application to process the data faster by increasing the overall throughput of the data stream. This will allow the consumer application to keep up with sudden increases in data and prevent records from being dropped. It also does not require any changes to the consumer application itself, which can be more operationally efficient than modifying the application or converting it to a Lambda function. Additionally, increasing the number of shards does not require the use of additional resources such as EC2 instances or EMR clusters, which can be more expensive and complex to manage."
      },
      {
        "date": "2023-07-19T15:04:00.000Z",
        "voteCount": 1,
        "content": "the problem is not with the throughput as it can increase. the problem is when this happens, the processing app falls behind which means it's a consumer issue. B is the correct answer in this case"
      },
      {
        "date": "2022-12-31T00:20:00.000Z",
        "voteCount": 4,
        "content": "I go with C. Most efficient (C: because of the sudden spikes) vs fastest approach (B)"
      },
      {
        "date": "2022-12-28T22:28:00.000Z",
        "voteCount": 1,
        "content": "Refer to ohcn's comment."
      },
      {
        "date": "2022-12-22T02:58:00.000Z",
        "voteCount": 1,
        "content": "Go with B"
      },
      {
        "date": "2022-12-10T05:24:00.000Z",
        "voteCount": 2,
        "content": "kinesis stream + lambda is better solution"
      },
      {
        "date": "2022-12-04T05:25:00.000Z",
        "voteCount": 3,
        "content": "1. https://docs.aws.amazon.com/streams/latest/dev/lambda-consumer.html\n2. https://docs.aws.amazon.com/lambda/latest/dg/with-kinesis.html"
      },
      {
        "date": "2022-11-19T14:20:00.000Z",
        "voteCount": 4,
        "content": "I would go with C. Lambda is always more efficient and cheaper than EC2. https://docs.aws.amazon.com/lambda/latest/dg/with-kinesis.html"
      },
      {
        "date": "2022-11-30T19:34:00.000Z",
        "voteCount": 5,
        "content": "Agree. As the question is asking for MOST operational efficiency, then answer is C.\nIf the question is asking for FASTEST method to improve, then answer is B"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 47,
    "url": "https://www.examtopics.com/discussions/amazon/view/52621-exam-aws-devops-engineer-professional-topic-1-question-47/",
    "body": "A company uses AWS Organizations to manage multiple accounts. Information security policies require that all unencrypted Amazon EBS volumes be marked as non-compliant. A DevOps engineer needs to automatically deploy the solution and ensure that this compliance check is always present.<br>With solution will accomplish this?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS CloudFormation template that defines an AWS Inspector rule to check whether EBS encryption is enabled. Save the template to an Amazon S3 bucket that has been shared with all accounts within the company. Update the account creation script pointing to the CloudFormation template in Amazon S3.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Config organizational rule to check whether EBS encryption is enabled and deploy the rule using the AWS CLI. Create and apply an SCP to prohibit stopping and deleting AWS Config across the organization.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an SCP in Organizations. Set the policy to prevent the launch of Amazon EC2 instances without encryption on the EBS volumes using a conditional expression. Apply the SCP to all AWS accounts. Use Amazon Athena to analyze the AWS CloudTrail output, looking for events that deny an ec2:RunInstances action.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy an IAM role to all accounts from a single trusted account. Build a pipeline with AWS CodePipeline with a stage in AWS Lambda to assume the IAM role, and list all EBS volumes in the account. Publish a report to Amazon S3."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 12,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-09-21T08:02:00.000Z",
        "voteCount": 14,
        "content": "The answer is B. We need to use AWS config rules."
      },
      {
        "date": "2022-06-23T00:49:00.000Z",
        "voteCount": 5,
        "content": "https://docs.aws.amazon.com/config/latest/developerguide/ec2-ebs-encryption-by-default.html\nrefer the link"
      },
      {
        "date": "2023-10-17T11:48:00.000Z",
        "voteCount": 1,
        "content": "The answer is be. AWS Config give you the availability to scan your reasorces and find non compliance resources."
      },
      {
        "date": "2023-05-01T10:22:00.000Z",
        "voteCount": 1,
        "content": "B is more suitable for this scenario"
      },
      {
        "date": "2023-03-04T06:44:00.000Z",
        "voteCount": 1,
        "content": "If compliance then config"
      },
      {
        "date": "2023-02-05T07:42:00.000Z",
        "voteCount": 1,
        "content": "C looks most AWS managed"
      },
      {
        "date": "2023-02-28T07:45:00.000Z",
        "voteCount": 1,
        "content": "It is not about preventing the creation... It. Is about checking the compliance status, so you use config.."
      },
      {
        "date": "2023-01-16T15:47:00.000Z",
        "voteCount": 1,
        "content": "Answe is B. When it comes to compliance checks go with AWS Config. Also note that AWS config is used at an organization level."
      },
      {
        "date": "2022-12-17T12:20:00.000Z",
        "voteCount": 2,
        "content": "If the question was to prevent the volumes from being created instead of marking non compliant would it have been C?"
      },
      {
        "date": "2023-01-12T10:00:00.000Z",
        "voteCount": 1,
        "content": "intent should be to \"prevent\"\nC seems a better option"
      },
      {
        "date": "2022-02-08T14:30:00.000Z",
        "voteCount": 2,
        "content": "need Config to detect compliance"
      },
      {
        "date": "2021-12-25T08:41:00.000Z",
        "voteCount": 1,
        "content": "Answer B"
      },
      {
        "date": "2021-12-24T15:03:00.000Z",
        "voteCount": 1,
        "content": "answer is B"
      },
      {
        "date": "2021-12-17T13:51:00.000Z",
        "voteCount": 1,
        "content": "BBBBBBBBBB"
      },
      {
        "date": "2021-10-31T10:55:00.000Z",
        "voteCount": 1,
        "content": "BBBBB    B"
      },
      {
        "date": "2021-10-17T12:36:00.000Z",
        "voteCount": 2,
        "content": "Answer B"
      },
      {
        "date": "2021-10-04T06:30:00.000Z",
        "voteCount": 3,
        "content": "BBBBBBBBBBB"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 48,
    "url": "https://www.examtopics.com/discussions/amazon/view/47930-exam-aws-devops-engineer-professional-topic-1-question-48/",
    "body": "A company develops and maintains a web application using Amazon EC2 instances and an Amazon RDS for SQL Server DB instance in a single Availability<br>Zone. The resources need to run only when new deployments are being tested using AWS CodePipeline. Testing occurs one or more times a week and each test takes 2-3 hours to run. A DevOps engineer wants a solution that does not change the architecture components.<br>Which solution will meet these requirements in the MOST cost-effective manner?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConvert the RDS database to an Amazon Aurora Serverless database. Use an AWS Lambda function to start and stop the EC2 instances before and after tests.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPut the EC2 instances into an Auto Scaling group. Schedule scaling to run at the start of the deployment tests.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReplace the EC2 instances with EC2 Spot Instances and the RDS database with an RDS Reserved Instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSubscribe Amazon CloudWatch Events to CodePipeline to trigger AWS Systems Manager Automation documents that start and stop all EC2 and RDS instances before and after deployment tests.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-11-02T15:21:00.000Z",
        "voteCount": 26,
        "content": "ans: D\nhttps://aws.amazon.com/blogs/mt/systems-manager-automation-documents-manage-instances-cut-costs-off-hours/"
      },
      {
        "date": "2022-09-20T03:50:00.000Z",
        "voteCount": 3,
        "content": "This link perfectly describes the solution."
      },
      {
        "date": "2021-11-03T07:47:00.000Z",
        "voteCount": 6,
        "content": "D \nRevealed B is wrong since breaking requirement to not change architecture components."
      },
      {
        "date": "2023-10-17T11:51:00.000Z",
        "voteCount": 1,
        "content": "Answer is D.\nAurora server less is not cost effective. \nRevered RDS is not cost effective since you pay for it regardless the fact its stopeed or turned on."
      },
      {
        "date": "2023-05-24T16:31:00.000Z",
        "voteCount": 1,
        "content": "d-d-d-d-dd-d-d-d-"
      },
      {
        "date": "2023-05-23T18:54:00.000Z",
        "voteCount": 1,
        "content": "d-d-d-d-dd-d-d-d-"
      },
      {
        "date": "2023-01-16T15:52:00.000Z",
        "voteCount": 1,
        "content": "D is the correct answer. Its AWS best practice to use Cloudwatch event against CodePipeline Deployment actions to trigger AWS System Manager automation documents to start and stop resources such as EC2 and RDS."
      },
      {
        "date": "2023-01-08T23:53:00.000Z",
        "voteCount": 1,
        "content": "Why not A?\nUses an AWS Lambda function to start and stop the EC2 instances before and after tests.\nBy converting the RDS database to an Amazon Aurora Serverless database, you can take advantage of the pay-per-use pricing model, \nwhich charges only for the database capacity that is actually used. This can help to reduce costs compared to using a standard RDS instance.\n\nD would require the use of AWS Systems Manager Automation, which incurs additional charges."
      },
      {
        "date": "2022-12-17T12:25:00.000Z",
        "voteCount": 2,
        "content": "D is good, small caveat is that RDS can only be stopped for 7 days then it\u2019ll automatically start."
      },
      {
        "date": "2022-09-07T13:30:00.000Z",
        "voteCount": 2,
        "content": "D - You can stop RDS instances for up to 7 days. https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_StopInstance.html"
      },
      {
        "date": "2022-02-09T13:40:00.000Z",
        "voteCount": 3,
        "content": "D looks better to me"
      },
      {
        "date": "2021-12-24T05:01:00.000Z",
        "voteCount": 2,
        "content": "A Require a lot of code change\nB They have mentioned about EC2 but nothing about RDS\nC This could be an answer - task require for 2 or 3 hours and reserve the RDS instance to save the cost\nD won't- you cannot stop SQL RDS instance"
      },
      {
        "date": "2022-11-30T19:47:00.000Z",
        "voteCount": 2,
        "content": "RDS can be stopped to save cost: https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_StopInstance.html"
      },
      {
        "date": "2022-01-01T16:42:00.000Z",
        "voteCount": 2,
        "content": "What is the problem if we stop the server but do not terminate. D can be an anwer."
      },
      {
        "date": "2021-12-18T06:41:00.000Z",
        "voteCount": 2,
        "content": "It's D."
      },
      {
        "date": "2021-11-07T00:02:00.000Z",
        "voteCount": 3,
        "content": "It's D. Both EC2, RDS should be launched on-demand for the most cost saving."
      },
      {
        "date": "2021-11-06T18:48:00.000Z",
        "voteCount": 3,
        "content": "note Revealed B is wrong for lowering costs since RDS is presumably then always running."
      },
      {
        "date": "2021-10-28T05:50:00.000Z",
        "voteCount": 1,
        "content": "it does not sounds like the efficient way"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 49,
    "url": "https://www.examtopics.com/discussions/amazon/view/46890-exam-aws-devops-engineer-professional-topic-1-question-49/",
    "body": "The Development team has grown substantially in recent months and so has the number of projects that use separate code repositories. The current process involves configuring AWS CodePipeline manually. There have been service limit alerts regarding the number of Amazon S3 buckets that exist.<br>Which pipeline option will reduce S3 bucket sprawl alerts?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCombine the multiple separate code repositories into a single one, and deploy using an AWS CodePipeline that has logic for each project.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate new pipelines by using the AWS API or AWS CLI, and configure them to use a single S3 bucket with separate prefixes for each project.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new pipeline in a different region for each project to bypass the service limits for S3 buckets in a single region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new pipeline and S3 bucket for each project by using the AWS API or AWS CLI to bypass the service limits for S3 buckets in a single account."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-11-04T02:33:00.000Z",
        "voteCount": 14,
        "content": "B for me"
      },
      {
        "date": "2021-09-29T00:58:00.000Z",
        "voteCount": 9,
        "content": "I'll go with B"
      },
      {
        "date": "2024-05-15T07:19:00.000Z",
        "voteCount": 1,
        "content": "B for me"
      },
      {
        "date": "2023-10-28T11:30:00.000Z",
        "voteCount": 1,
        "content": "Buckets limit is per account."
      },
      {
        "date": "2023-10-17T11:52:00.000Z",
        "voteCount": 1,
        "content": "I will go with B"
      },
      {
        "date": "2023-05-01T10:37:00.000Z",
        "voteCount": 1,
        "content": "B is more precise for this scenario"
      },
      {
        "date": "2023-02-07T00:24:00.000Z",
        "voteCount": 2,
        "content": "A is eliminated, single repository is not a good idea.\nC is eliminated, region is not the point.\nD is neater than B"
      },
      {
        "date": "2023-02-07T00:26:00.000Z",
        "voteCount": 1,
        "content": "Sorry, D is wrong. Buckets limit is on each account, not each project."
      },
      {
        "date": "2023-02-01T09:39:00.000Z",
        "voteCount": 2,
        "content": "I will go with B as well"
      },
      {
        "date": "2023-01-16T15:54:00.000Z",
        "voteCount": 2,
        "content": "B is the right answer"
      },
      {
        "date": "2022-10-17T00:07:00.000Z",
        "voteCount": 3,
        "content": "I'll go with B"
      },
      {
        "date": "2021-09-25T16:58:00.000Z",
        "voteCount": 5,
        "content": "ans is B"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 50,
    "url": "https://www.examtopics.com/discussions/amazon/view/80129-exam-aws-devops-engineer-professional-topic-1-question-50/",
    "body": "A company runs several applications across multiple AWS accounts in an organization in AWS Organizations. Some of the resources are not tagged properly and the company's finance team cannot determine which costs are associated with which applications. A DevOps engineer must remediate this issue and prevent this issue from happening in the future.<br>Which combination of actions should the DevOps engineer take to meet these requirements? (Choose two.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tActivate the user-defined cost allocation tags in each AWS account.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate and attach an SCP that requires a specific tag.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDefine each line of business (LOB) in AWS Budgets. Assign the required tag to each resource.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tScan all accounts with Tag Editor. Assign the required tag to each resource.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the budget report to find untagged resources. Assign the required tag to each resource."
    ],
    "answer": "BD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BD",
        "count": 18,
        "isMostVoted": true
      },
      {
        "answer": "AC",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-09-04T12:52:00.000Z",
        "voteCount": 15,
        "content": "Might be B and D - SCP to prevent resources to be created without mandatory tags and Tag Editor to scan resources are not tagged.\n\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_examples_tagging.html\n\nhttps://aws.amazon.com/about-aws/whats-new/2015/02/19/aws-console-tag-editor-now-supports-not-tagged-and-empty-value-resource-search/"
      },
      {
        "date": "2023-11-24T08:27:00.000Z",
        "voteCount": 1,
        "content": "BD is my answer"
      },
      {
        "date": "2023-05-23T19:05:00.000Z",
        "voteCount": 1,
        "content": "b-d-b-d-b-d-b-d-b-d-b-d-b-d"
      },
      {
        "date": "2023-02-07T01:43:00.000Z",
        "voteCount": 1,
        "content": "D is eliminated"
      },
      {
        "date": "2023-01-16T15:58:00.000Z",
        "voteCount": 1,
        "content": "Answer is B and D."
      },
      {
        "date": "2023-02-04T11:12:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/custom-tags.html"
      },
      {
        "date": "2023-01-08T23:46:00.000Z",
        "voteCount": 3,
        "content": "Why not AB?"
      },
      {
        "date": "2022-12-22T07:44:00.000Z",
        "voteCount": 1,
        "content": "DB for sure"
      },
      {
        "date": "2022-12-22T03:13:00.000Z",
        "voteCount": 1,
        "content": "Answer is BD"
      },
      {
        "date": "2022-12-22T03:11:00.000Z",
        "voteCount": 1,
        "content": "Answer is CD"
      },
      {
        "date": "2022-09-12T22:48:00.000Z",
        "voteCount": 2,
        "content": "I agree with ohcn."
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 51,
    "url": "https://www.examtopics.com/discussions/amazon/view/8075-exam-aws-devops-engineer-professional-topic-1-question-51/",
    "body": "An online company uses Amazon EC2 Auto Scaling extensively to provide an excellent customer experience while minimizing the number of running EC2 instances. The company's self-hosted Puppet environment in the application layer manages the configuration of the instances. The IT manager wants the lowest licensing costs and wants to ensure that whenever the EC2 Auto Scaling group scales down, removed EC2 instances are deregistered from the Puppet master as soon as possible.<br>How can the requirement be met?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAt instance launch time, use EC2 user data to deploy the AWS CodeDeploy agent. Use CodeDeploy to install the Puppet agent. When the Auto Scaling group scales out, run a script to register the newly deployed instances to the Puppet master. When the Auto Scaling group scales in, use the EC2 Auto Scaling EC2_INSTANCE_TERMINATING lifecycle hook to trigger de-registration from the Puppet master.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBake the AWS CodeDeploy agent into the base AMI. When the Auto Scaling group scales out, use CodeDeploy to install the Puppet agent, and execute a script to register the newly deployed instances to the Puppet master. When the Auto Scaling group scales in, use the CodeDeploy ApplicationStop lifecycle hook to run a script to de-register the instance from the Puppet master.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAt instance launch time, use EC2 user data to deploy the AWS CodeDeploy agent. When the Auto Scaling group scales out, use CodeDeploy to install the Puppet agent, and run a script to register the newly deployed instances to the Puppet master. When the Auto Scaling group scales in, use the EC2 user data instance stop script to run a script to de-register the instance from the Puppet master.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBake the AWS Systems Manager agent into the base AMI. When the Auto Scaling group scales out, use the AWS Systems Manager to install the Puppet agent, and run a script to register the newly deployed instances to the Puppet master. When the Auto Scaling group scales in, use the Systems Manager instance stop lifecycle hook to run a script to de-register the instance from the Puppet master."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 9,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 8,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-10-21T11:50:00.000Z",
        "voteCount": 53,
        "content": "This is a tough one. The key to understanding it is that CodeDeploy and ASG's can work together! There's a great blog post about this: https://aws.amazon.com/blogs/devops/under-the-hood-aws-codedeploy-and-auto-scaling-integration/\n\nHere is my analysis:\nA: This is the correct approach and matches the blog post writeup\nB: The CodeDeploy ApplicationStop lifecycle hook relates to upgrading an instance in place. The EC2 lifecycle hook is needed here.\nC: There is no such thing as a \"user data instance stop script\"\nD: SSM does not have lifecycle hooks"
      },
      {
        "date": "2021-09-29T00:43:00.000Z",
        "voteCount": 12,
        "content": "D\uff1a\nhttps://aws.amazon.com/cn/blogs/mt/configure-amazon-ec2-instances-in-an-auto-scaling-group-using-state-manager/"
      },
      {
        "date": "2023-07-21T01:13:00.000Z",
        "voteCount": 1,
        "content": "A is the correct answer. ref. https://docs.aws.amazon.com/autoscaling/ec2/APIReference/API_LifecycleHookSpecification.html"
      },
      {
        "date": "2023-05-08T15:14:00.000Z",
        "voteCount": 1,
        "content": "Agree A"
      },
      {
        "date": "2023-01-16T16:32:00.000Z",
        "voteCount": 1,
        "content": "Answer is A. D is incorrect as there is no such lifecycle hooks on System Manager.  I am just not sure how we could install Puppet using CodeDeploy. Looks like to me it is similar to installing CloudWatch Agent with CodeDeploy using an S3 deployment."
      },
      {
        "date": "2023-01-01T15:33:00.000Z",
        "voteCount": 1,
        "content": "Yes, A is correct"
      },
      {
        "date": "2022-12-22T07:48:00.000Z",
        "voteCount": 1,
        "content": "Agree A"
      },
      {
        "date": "2022-11-13T08:27:00.000Z",
        "voteCount": 2,
        "content": "Why run codedeploy twice?"
      },
      {
        "date": "2022-09-20T23:58:00.000Z",
        "voteCount": 2,
        "content": "it's ASG, and all management is being done through ASG; Other services have no idea about ASG operation ( Scale in/out, ..) by default. The only reasonable response will be A"
      },
      {
        "date": "2022-08-28T08:03:00.000Z",
        "voteCount": 5,
        "content": "A seems to be the right solution. Highest voted D seems suspect - there is no such thing like \"Systems Manager instance stop lifecycle hook\""
      },
      {
        "date": "2022-08-15T11:44:00.000Z",
        "voteCount": 6,
        "content": "The most easy and straight forward solution. https://aws.amazon.com/blogs/infrastructure-and-automation/run-code-before-terminating-an-ec2-auto-scaling-instance/"
      },
      {
        "date": "2021-11-06T07:09:00.000Z",
        "voteCount": 4,
        "content": "I'll go with A"
      },
      {
        "date": "2021-11-04T12:38:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer: A"
      },
      {
        "date": "2021-10-28T17:20:00.000Z",
        "voteCount": 1,
        "content": "A as per below link:\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/lifecycle-hooks.html"
      },
      {
        "date": "2021-10-28T09:57:00.000Z",
        "voteCount": 3,
        "content": "D: explained here\nhttps://aws.amazon.com/blogs/infrastructure-and-automation/run-code-before-terminating-an-ec2-auto-scaling-instance/"
      },
      {
        "date": "2021-10-27T22:46:00.000Z",
        "voteCount": 3,
        "content": "I'll go with A"
      },
      {
        "date": "2021-10-26T00:11:00.000Z",
        "voteCount": 2,
        "content": "Thanks everybody with A. Seem there is no SSM Lifecycle Hook so D is incorrect"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 52,
    "url": "https://www.examtopics.com/discussions/amazon/view/83032-exam-aws-devops-engineer-professional-topic-1-question-52/",
    "body": "A company uses a series of individual Amazon CloudFormation templates to deploy its multi-Region applications. These templates must be deployed in a specific order. The company is making more changes to the templates than previously expected and wants to deploy new templates more efficiently. Additionally, the data engineering team must be notified of all changes to the templates.<br>What should the company do to accomplish these goals?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Lambda function to deploy the CloudFormation templates in the required order. Use stack policies to alert the data engineering team.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tHost the CloudFormation templates in Amazon S3. Use Amazon S3 events to directly trigger CloudFormation updates and Amazon SNS notifications.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement CloudFormation StackSets and use drift detection to trigger update alerts to the data engineering team.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLeverage CloudFormation nested stacks and stack sets for deployments. Use Amazon SNS to notify the data engineering team.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 9,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-09-21T00:14:00.000Z",
        "voteCount": 8,
        "content": "D seems a better choice among others \n\"in a specific order.\" =&gt; Nested Stack guarantee"
      },
      {
        "date": "2022-12-21T05:24:00.000Z",
        "voteCount": 1,
        "content": "Nested Stack doesn't manage the order in which the templates are applied, for hat you use Stack Sets"
      },
      {
        "date": "2023-01-10T14:04:00.000Z",
        "voteCount": 3,
        "content": "I wrote nested stack in CDK. It does deploy the child stacks in order.\nD is the answer because the question is about the (SNS) notification of any changes"
      },
      {
        "date": "2023-03-04T06:52:00.000Z",
        "voteCount": 1,
        "content": "Nested stack for order and sns for notification"
      },
      {
        "date": "2023-02-21T20:13:00.000Z",
        "voteCount": 3,
        "content": "Option C is the best choice to accomplish the company's goals.\n\nWith CloudFormation StackSets, the company can deploy the same set of CloudFormation templates to multiple accounts and regions at once. The StackSets can be used to maintain the ordering of the templates while allowing changes to be deployed more efficiently across multiple accounts and regions.\n\nIn addition, StackSets can use drift detection, which can detect differences between the expected and actual stack configuration in each account and region. If a drift is detected, it can trigger an update of the affected stack, which can then notify the data engineering team of any changes made to the templates.\n\nOption A may be used to deploy the templates in order but may not be efficient for deploying changes. Option B may work for triggering CloudFormation updates and SNS notifications but may not maintain the required order of deployment. Option D may work for notifications but may not handle the required ordering of deployment or efficiently deploy changes."
      },
      {
        "date": "2023-01-16T16:35:00.000Z",
        "voteCount": 2,
        "content": "D is the right answer"
      },
      {
        "date": "2023-01-01T15:40:00.000Z",
        "voteCount": 1,
        "content": "I go with D"
      },
      {
        "date": "2022-12-21T05:35:00.000Z",
        "voteCount": 3,
        "content": "Stack Sets will take care of the multi-region ordered deployment. and I don't see the need for nested stack"
      },
      {
        "date": "2022-12-26T15:27:00.000Z",
        "voteCount": 6,
        "content": "They aren\u2019t looking for drift notifs, they want to notify the team about template changes"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 53,
    "url": "https://www.examtopics.com/discussions/amazon/view/28613-exam-aws-devops-engineer-professional-topic-1-question-53/",
    "body": "A company is running an application on Amazon EC2 instances in an Auto Scaling group. Recently, an issue occurred that prevented EC2 instances from launching successfully, and it took several hours for the Support team to discover the issue. The Support team wants to be notified by email whenever an EC2 instance does not start successfully.<br>Which action will accomplish this?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a health check to the Auto Scaling group to invoke an AWS Lambda function whenever an instance status is impaired.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the Auto Scaling group to send a notification to an Amazon SNS topic whenever a failed instance launch occurs.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon CloudWatch alarm that invokes an AWS Lambda function when a failed AttachInstances Auto Scaling API call is made.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a status check alarm on Amazon EC2 to send a notification to an Amazon SNS topic whenever a status check fail occurs."
    ],
    "answer": "B",
    "answerDescription": "Reference:<br>https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-sns-notifications.html#auto-scaling-sns-notifications",
    "votes": [
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-19T21:35:00.000Z",
        "voteCount": 22,
        "content": "B is correct\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/ASGettingNotifications.html#auto-scaling-sns-notifications"
      },
      {
        "date": "2021-10-02T16:07:00.000Z",
        "voteCount": 2,
        "content": "For me also, B seems to be a suitable option. For D we need to manually create CW events, while we can use inbuilt ASG events feature"
      },
      {
        "date": "2021-10-24T11:01:00.000Z",
        "voteCount": 5,
        "content": "I'll go with B"
      },
      {
        "date": "2024-01-16T01:01:00.000Z",
        "voteCount": 1,
        "content": "good way to automate notifications upon instance start"
      },
      {
        "date": "2023-01-16T16:42:00.000Z",
        "voteCount": 1,
        "content": "B is the right answer. https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-sns-notifications.html#auto-scaling-sns-notifications"
      },
      {
        "date": "2023-01-01T15:47:00.000Z",
        "voteCount": 1,
        "content": "I go with B"
      },
      {
        "date": "2021-10-28T00:38:00.000Z",
        "voteCount": 2,
        "content": "B it is"
      },
      {
        "date": "2021-10-15T05:22:00.000Z",
        "voteCount": 2,
        "content": "B is fine"
      },
      {
        "date": "2021-10-03T11:59:00.000Z",
        "voteCount": 1,
        "content": "B is correct. you can send SNS messages from the ASG console"
      },
      {
        "date": "2021-09-25T13:38:00.000Z",
        "voteCount": 1,
        "content": "C is the correct answer. EC2 cannot send notifications to SNS topics. We need to use cloudwatch events for this.\n\n{\n  \"source\": [\n    \"aws.autoscaling\"\n  ],\n  \"detail-type\": [\n    \"EC2 Instance Launch Unsuccessful\"\n  ]\n}"
      },
      {
        "date": "2021-09-28T23:06:00.000Z",
        "voteCount": 1,
        "content": "Ignore"
      },
      {
        "date": "2022-01-12T14:17:00.000Z",
        "voteCount": 1,
        "content": "EC2 can't but Auto Scaling can. https://docs.aws.amazon.com/autoscaling/ec2/userguide/ASGettingNotifications.html#auto-scaling-sns-notifications"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 54,
    "url": "https://www.examtopics.com/discussions/amazon/view/48068-exam-aws-devops-engineer-professional-topic-1-question-54/",
    "body": "A company uses AWS Storage Gateway in file gateway mode in front of an Amazon S3 bucket that is used by multiple resources. In the morning when business begins, users do not see the objects processed by a third party the previous evening. When a DevOps engineer looks directly at the S3 bucket, the data is there, but it is missing in Storage Gateway.<br>Which solution ensures that all the updated third-party files are available in the morning?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a nightly Amazon EventBridge (Amazon CloudWatch Events) event to trigger an AWS Lambda function to run the RefreshCache command for Storage Gateway.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstruct the third party to put data into the S3 bucket using AWS Transfer for SFTP.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify Storage Gateway to run in volume gateway mode.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse S3 same-Region replication to replicate any changes made directly in the S3 bucket to Storage Gateway."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-10-06T03:42:00.000Z",
        "voteCount": 13,
        "content": "Ans: A \nhttps://docs.aws.amazon.com/storagegateway/latest/APIReference/API_RefreshCache.html"
      },
      {
        "date": "2021-10-26T18:27:00.000Z",
        "voteCount": 6,
        "content": "I'll go with A"
      },
      {
        "date": "2023-08-05T16:16:00.000Z",
        "voteCount": 1,
        "content": "A is the answer: https://repost.aws/knowledge-center/storage-gateway-s3-changes-not-showing"
      },
      {
        "date": "2023-02-16T06:00:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/storagegateway/latest/APIReference/API_RefreshCache.html\n\" It only updates the cached inventory to reflect changes in the inventory of the objects in the S3 bucket. This operation is only supported in the S3 File Gateway types.\""
      },
      {
        "date": "2023-01-16T16:43:00.000Z",
        "voteCount": 1,
        "content": "A is the right answer."
      },
      {
        "date": "2021-09-24T19:36:00.000Z",
        "voteCount": 5,
        "content": "I will go with A"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 55,
    "url": "https://www.examtopics.com/discussions/amazon/view/78822-exam-aws-devops-engineer-professional-topic-1-question-55/",
    "body": "A DevOps engineer sets up two Amazon S3 event notifications for an S3 bucket from the S3 console. Both event notifications will be invoked when an object PUT action occurs. One event notification will invoke an AWS Lambda function if the file suffix is .csv. Another event notification will invoke an Amazon Simple<br>Notification Service (Amazon SNS) topic if the file suffix is .xlsx<br>The DevOps engineer notices that files with the .csv suffix can invoke the Lambda function successfully. However, files with the .xlsx suffix cannot invoke the SNS topic.<br>Which reason explains why the SNS topic is not invoked when .xlsx files are added to the S3 bucket?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOnly one event notification is allowed from the S3 console.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon S3 needs proper permissions to publish an event notification to Amazon SNS.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLambda has precedence over Amazon SNS in handling the event notification.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmazon SNS is not a valid destination for some S3 event notifications, including object PUT."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 3,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-01-16T16:44:00.000Z",
        "voteCount": 3,
        "content": "B is correct."
      },
      {
        "date": "2023-01-01T15:51:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2022-12-10T05:54:00.000Z",
        "voteCount": 1,
        "content": "These are two differnet filters so B"
      },
      {
        "date": "2022-10-17T00:29:00.000Z",
        "voteCount": 1,
        "content": "I'll go with B"
      },
      {
        "date": "2022-10-16T12:56:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/AmazonS3/latest/userguide/notification-how-to-filtering.html shows that you can have two non overlapping filters triggering two distinct actions, which shows A does not apply here.  \nIf B is not taken care of, it certainly will not work, so I will go with B"
      },
      {
        "date": "2022-10-07T20:05:00.000Z",
        "voteCount": 2,
        "content": "B\nhttps://aws.amazon.com/premiumsupport/knowledge-center/unable-validate-destination-s3/"
      },
      {
        "date": "2022-09-28T23:56:00.000Z",
        "voteCount": 1,
        "content": "I will go with A\nRefer: https://docs.aws.amazon.com/AmazonS3/latest/userguide/notification-how-to-filtering.html"
      },
      {
        "date": "2022-09-24T09:11:00.000Z",
        "voteCount": 1,
        "content": "S3 event notification supports SNS, SQS and Lambda. However, it is not possible to have multiple Events defined for the same 'triggers'. \nhttps://stackoverflow.com/questions/55079923/programmatically-add-multiple-event-notifications-to-s3-bucket"
      },
      {
        "date": "2023-02-28T10:01:00.000Z",
        "voteCount": 2,
        "content": "Just tested.. You van trigger lambda using txt and SNS using csv.. So. The answers here is B.. Permission.."
      },
      {
        "date": "2023-02-07T04:48:00.000Z",
        "voteCount": 1,
        "content": "I think .... yeah ....."
      },
      {
        "date": "2022-09-02T01:51:00.000Z",
        "voteCount": 2,
        "content": "B sounds right"
      },
      {
        "date": "2022-08-31T16:57:00.000Z",
        "voteCount": 3,
        "content": "B - https://docs.aws.amazon.com/AmazonS3/latest/userguide/grant-destinations-permissions-to-s3.html"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 56,
    "url": "https://www.examtopics.com/discussions/amazon/view/8121-exam-aws-devops-engineer-professional-topic-1-question-56/",
    "body": "A rapidly growing company wants to scale for Developer demand for AWS development environments. Development environments are created manually in the<br>AWS Management Console. The Networking team uses AWS CloudFormation to manage the networking infrastructure, exporting stack output values for the<br>Amazon VPC and all subnets. The development environments have common standards, such as Application Load Balancers, Amazon EC2 Auto Scaling groups, security groups, and Amazon DynamoDB tables.<br>To keep up with the demand, the DevOps Engineer wants to automate the creation of development environments. Because the infrastructure required to support the application is expected to grow, there must be a way to easily update the deployed infrastructure. CloudFormation will be used to create a template for the development environments.<br>Which approach will meet these requirements and quickly provide consistent AWS environments for Developers?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Fn::ImportValue intrinsic functions in the Resources section of the template to retrieve Virtual Private Cloud (VPC) and subnet values. Use CloudFormation StackSets for the development environments, using the Count input parameter to indicate the number of environments needed. use the UpdateStackSet command to update existing development environments.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse nested stacks to define common infrastructure components. To access the exported values, use TemplateURL to reference the Networking team's template. To retrieve Virtual Private Cloud (VPC) and subnet values, use Fn::ImportValue intrinsic functions in the Parameters section of the master template. Use the CreateChangeSet and ExecuteChangeSet commands to update existing development environments.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse nested stacks to define common infrastructure components. Use Fn::ImportValue intrinsic functions with the resources of the nested stack to retrieve Virtual Private Cloud (VPC) and subnet values. Use the CreateChangeSet and ExecuteChangeSet commands to update existing development environments.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Fn::ImportValue intrinsic functions in the Parameters section of the master template to retrieve Virtual Private Cloud (VPC) and subnet values. Define the development resources in the order they need to be created in the CloudFormation nested stacks. Use the CreateChangeSet and ExecuteChangeSet commands to update existing development environments."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-27T18:59:00.000Z",
        "voteCount": 32,
        "content": "Answer is C\nB and D wrong because of the \"intrinsic functions in the Parameters section\"\nA: what is the use of the count input parameters ? and UpdateChangeSet? you need create and execute"
      },
      {
        "date": "2021-09-30T15:34:00.000Z",
        "voteCount": 5,
        "content": "C is correct.\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference-importvalue.html"
      },
      {
        "date": "2021-10-22T14:28:00.000Z",
        "voteCount": 14,
        "content": "A - Incorrect since StackSets are not needed here. No mention of multi-region/multi-account deployments (although correct usage of intrinsic functions and refs to resource values)\nB - If you reference the template URL, you wouldn't need to use the intrinsic function Fn:import. You would reference the output of the template URL function i.e. TemplateName.Outputs.VariableName\nC - Correct\nD - You do not use Fn:import within the Parameters section. It is solely used within the Resources section to reference stack outputs within nested stacks or other previously deployed stacks.\n\nErgo, answer is C."
      },
      {
        "date": "2023-03-04T06:55:00.000Z",
        "voteCount": 2,
        "content": "C is correct"
      },
      {
        "date": "2023-02-19T15:53:00.000Z",
        "voteCount": 1,
        "content": "The only answer that has no issue is A\n Fn::ImportValue  is only used in cross stack reference\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference-importvalue.html"
      },
      {
        "date": "2023-02-26T22:43:00.000Z",
        "voteCount": 1,
        "content": "Based on  kopper2019 explanation C is correct.\nStackset is also wrong in A"
      },
      {
        "date": "2023-01-16T17:02:00.000Z",
        "voteCount": 3,
        "content": "The correct answer is A. There has been a lot of confusion here because the question is not clear on whether each dev env is created in a different region/ account. However other answers are technically incorrect as per the AWS documentation. B and C are incorrect as fn:ImportValue doesn't work with nested stack and only works with cross-stack references. Answer D is incorrect as fn:ImportValue cannot be used in the Parameter section of a CF template. Therefore through method of elimination the correct answer is A."
      },
      {
        "date": "2023-01-25T00:25:00.000Z",
        "voteCount": 3,
        "content": "Option C doesn\u2019t mention importing the values between nested stacks. It mentions importing values in nested stack from VPC stack."
      },
      {
        "date": "2023-01-25T00:25:00.000Z",
        "voteCount": 1,
        "content": "So C seems bf right."
      },
      {
        "date": "2022-08-28T07:46:00.000Z",
        "voteCount": 4,
        "content": "Answer C seems to be the most suitable one. Why A is marked to be the right one - no idea. \"CloudFormation StackSets\" within the answer - fits to multi-account scenarios - in the question is no word about multi-account"
      },
      {
        "date": "2021-11-07T06:46:00.000Z",
        "voteCount": 1,
        "content": "For cross-stack references, use Fn::ImportValue to import a value from another template. For nested stacks, use Fn::Ref and Fn::GetAtt to reference the value in your current template.\nSo It can' be C"
      },
      {
        "date": "2021-11-03T06:31:00.000Z",
        "voteCount": 1,
        "content": "ANs: C"
      },
      {
        "date": "2021-11-01T12:36:00.000Z",
        "voteCount": 3,
        "content": "A is right with stackset and all others are wrong."
      },
      {
        "date": "2021-10-27T00:57:00.000Z",
        "voteCount": 4,
        "content": "I'll go with C"
      },
      {
        "date": "2021-10-21T05:40:00.000Z",
        "voteCount": 1,
        "content": "C is the correct one. \nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference-importvalue.html\nCF of network exports the VPC, subnet or needed information\nCF of application imports the above information to its stack and UpdateChangeSet/ ExecuteChangeSet"
      },
      {
        "date": "2021-10-19T12:12:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is C."
      },
      {
        "date": "2021-10-07T18:45:00.000Z",
        "voteCount": 1,
        "content": "A is the correct answer.\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-concepts.html#stacksets-concepts-stackset\nC is definitely wrong, and it's why:\nFn::ImportValue is used for cross-stacks reference. i.e. You create a VPC in stack B and export  the VPC so another stacks can reference it using Fn::ImportValue.\nTo pass values from nested stack to parent stack, you should use Fn::GetAttr in  the parent stack"
      },
      {
        "date": "2021-10-21T06:39:00.000Z",
        "voteCount": 1,
        "content": "A might be correct if doesn't have sentence \"using the Count input parameter to indicate the number of environments needed\", there is not needed. What we need is the VPC ID and subnet from Network CF."
      },
      {
        "date": "2021-10-18T14:38:00.000Z",
        "voteCount": 2,
        "content": "Option C says: Use Fn::ImportValue intrinsic functions with the resources of the nested stack to retrieve Virtual Private Cloud (VPC) and subnet values.\nI believe it means that in the resource blocks inside a nested stack you should use Fn::ImportValue function. Which is correct."
      },
      {
        "date": "2021-10-07T13:35:00.000Z",
        "voteCount": 1,
        "content": "C is the right answer"
      },
      {
        "date": "2021-10-06T23:29:00.000Z",
        "voteCount": 1,
        "content": "C is the answer"
      },
      {
        "date": "2021-10-04T20:17:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2021-10-03T18:43:00.000Z",
        "voteCount": 1,
        "content": "voting for C"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 57,
    "url": "https://www.examtopics.com/discussions/amazon/view/47704-exam-aws-devops-engineer-professional-topic-1-question-57/",
    "body": "A DevOps engineer notices that all Amazon EC2 instances running behind an Application Load Balancer in an Auto Scaling group are failing to respond to user requests. The EC2 instances are also failing target group HTTP health checks.<br>Upon inspection, the engineer notices the application process was not running in any EC2 instances. There are a significant number of out of memory messages in the system logs. The engineer needs to improve the resilience of the application to cope with a potential application memory leak. Monitoring and notifications should be enabled to alert when there is an issue.<br>Which combination of actions will meet these requirements? (Choose two.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the Auto Scaling configuration to replace the instances when they fail the load balancer's health checks.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the target group health check HealthCheckIntervalSeconds parameter to reduce the interval between health checks.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the target group health checks from HTTP to TCP to check if the port where the application is listening is reachable.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable the available memory consumption metric within the Amazon CloudWatch dashboard for the entire Auto Scaling group. Create an alarm when the memory utilization is high. Associate an Amazon SNS topic to the alarm to receive notifications when the alarm goes off.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Amazon CloudWatch agent to collect the memory utilization of the EC2 instances in the Auto Scaling group. Create an alarm when the memory utilization is high and associate an Amazon SNS topic to receive a notification.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "AE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AE",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-30T17:34:00.000Z",
        "voteCount": 25,
        "content": "I'll go with A, E\n\nB is wrong because it don't attack the problem\nC is wrong because changing the target group health checks from HTTP to TCP will not help\nD is wrong because of \"notifications when the alarm goes off\"."
      },
      {
        "date": "2023-09-17T00:12:00.000Z",
        "voteCount": 1,
        "content": "alarm goes off means the \"alarm is triggered\"."
      },
      {
        "date": "2023-11-02T22:34:00.000Z",
        "voteCount": 2,
        "content": "The reason that D is incorrect is because there is no \"memory consumption metric\" by default in cloudwatch. You will need to use cloudwatch agent to send system-level metrics to create a custom metric."
      },
      {
        "date": "2021-09-29T17:12:00.000Z",
        "voteCount": 9,
        "content": "I will go with A,E"
      },
      {
        "date": "2023-01-16T17:08:00.000Z",
        "voteCount": 1,
        "content": "A &amp; E is the right answer."
      },
      {
        "date": "2023-01-10T04:01:00.000Z",
        "voteCount": 1,
        "content": "AE: they are looking for a combination of actions. DE is not possible just from that because they are two similar actions"
      },
      {
        "date": "2022-10-07T20:23:00.000Z",
        "voteCount": 2,
        "content": "A and E\nC - Is not possible. Only protocols available for health check are HTTP and HTTPS\nB - Increase the time will not solve"
      },
      {
        "date": "2022-07-12T23:55:00.000Z",
        "voteCount": 3,
        "content": "A company runs a photo processing application that needs to frequently upload and download pictures from Amazon S3 buckets that are located in the same AWS Region.\nA solutions architect has noticed an increased cost in data transfer fees and needs to implement a solution to reduce these costs.\nHow can the solutions architect meet this requirement?\n\nA. Deploy Amazon API Gateway into a public subnet and adjust the route table to route S3 calls through It.\nB. Deploy a NAT gateway into a public subnet and attach an end point policy that allows access to the S3 buckets.\nC. Deploy the application Into a public subnet and allow it to route through an internet gateway to access the S3 Buckets\nD. Deploy an S3 VPC gateway endpoint into the VPC and attach an endpoint policy that allows access to the S3 buckets."
      },
      {
        "date": "2022-07-12T14:47:00.000Z",
        "voteCount": 1,
        "content": "AE is correct."
      },
      {
        "date": "2022-02-10T12:47:00.000Z",
        "voteCount": 4,
        "content": "AE, E is obvious, A is because ASG by default monitor EC2 health check instead of LB, think that's the point of the question."
      },
      {
        "date": "2021-10-15T01:29:00.000Z",
        "voteCount": 1,
        "content": "Ans: A, E"
      },
      {
        "date": "2021-10-05T01:58:00.000Z",
        "voteCount": 1,
        "content": "ans: B, E\n\nThe Requirement is \"Monitoring and notifications\", replacement helps nothing by outofmemory"
      },
      {
        "date": "2021-10-03T21:24:00.000Z",
        "voteCount": 2,
        "content": "left with A,B as only sensible options.\nsinceC doesnt solve problem\nand D &amp; E are invalid as no plain ASG (or EC2I) metrics on memory utilization (need Custom metric but no mention of that)."
      },
      {
        "date": "2021-10-20T17:05:00.000Z",
        "voteCount": 3,
        "content": "I thought, like you, that a custom metric is needed for memory usage, but I came across this: \"By default, AWS gives you visibility into metrics like CPU load logs, network latency, request volume, etc., but not EC2 memory usage. For other metrics like EC2 memory usage, you\u2019ll have to install and configure a CloudWatch agent on the instance...\". For more info, check the \"mem_xxxx\" metrics here: https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/metrics-collected-by-CloudWatch-agent.html"
      },
      {
        "date": "2021-10-24T15:37:00.000Z",
        "voteCount": 1,
        "content": "Cloudwatch agent have memory used metric.\n\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/metrics-collected-by-CloudWatch-agent.html"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 58,
    "url": "https://www.examtopics.com/discussions/amazon/view/46940-exam-aws-devops-engineer-professional-topic-1-question-58/",
    "body": "A company wants to migrate a legacy application to AWS and develop a deployment pipeline that uses AWS services only. A DevOps engineer is migrating all of the application code from a Git repository to AWS CodeCommit while preserving the history of the repository. The DevOps engineer has set all the permissions within CodeCommit, installed the Git client and the AWS CLI on a local computer, and is ready to migrate the repository.<br>Which actions will follow?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate the CodeCommit repository using the AWS CLI. Clone the Git repository directly to CodeCommit using the AWS CLI. Validate that the files were migrated, and publish the CodeCommit repository.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate the CodeCommit repository using the AWS Management Console. Clone both the Git and CodeCommit repositories to the local computer. Copy the files from the Git repository to the CodeCommit repository on the local computer. Commit the CodeCommit repository. Validate that the files were migrated, and share the CodeCommit repository.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate the CodeCommit repository using the AWS Management Console. Use the console to clone the Git repository into the CodeCommit repository. Validate that the files were migrated, and publish the CodeCommit repository.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate the CodeCommit repository using the AWS Management Console or the AWS CLI. Clone the Git repository with a mirror argument to the local computer and push the repository to CodeCommit. Validate that the files were migrated, and share the CodeCommit repository.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-10-04T06:45:00.000Z",
        "voteCount": 18,
        "content": "D\nhttps://docs.aws.amazon.com/codecommit/latest/userguide/how-to-migrate-repository-existing.html"
      },
      {
        "date": "2021-10-09T06:00:00.000Z",
        "voteCount": 9,
        "content": "I'll got with D\n\nReference: https://docs.aws.amazon.com/codecommit/latest/userguide/how-to-migrate-repository-existing.html#how-to-migrate-existing-clone"
      },
      {
        "date": "2021-10-29T08:55:00.000Z",
        "voteCount": 1,
        "content": "The link is very useful to understand. Thanks a lot"
      },
      {
        "date": "2023-02-14T12:49:00.000Z",
        "voteCount": 4,
        "content": "Why is A suggested as \"correct answer\" ?"
      },
      {
        "date": "2023-01-16T19:12:00.000Z",
        "voteCount": 2,
        "content": "Answer D use the --mirror option when cloning git repo"
      },
      {
        "date": "2022-10-23T04:48:00.000Z",
        "voteCount": 2,
        "content": "Answer is \u201dD\u201d \uff01"
      },
      {
        "date": "2022-02-13T07:46:00.000Z",
        "voteCount": 2,
        "content": "D is the correct answer"
      },
      {
        "date": "2021-11-03T11:33:00.000Z",
        "voteCount": 4,
        "content": "D -\n1 - create a repo\n2 - clone the repo (mirror)\n3 - push\n4 - validate"
      },
      {
        "date": "2021-10-26T05:19:00.000Z",
        "voteCount": 2,
        "content": "Ans is D"
      },
      {
        "date": "2021-10-18T05:57:00.000Z",
        "voteCount": 1,
        "content": "Answer: D"
      },
      {
        "date": "2021-10-05T09:01:00.000Z",
        "voteCount": 4,
        "content": "ans: D"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 59,
    "url": "https://www.examtopics.com/discussions/amazon/view/47875-exam-aws-devops-engineer-professional-topic-1-question-59/",
    "body": "A company is using AWS to deploy an application. The development team must automate the deployments. The team has created an AWS CodePipeline pipeline to deploy the application to Amazon EC2 instances using AWS CodeDeploy after it has been built using AWS CodeBuild.<br>The team wants to add automated testing to the pipeline to confirm that the application is healthy before deploying the code to the EC2 instances. The team also requires a manual approval action before the application is deployed, even if the tests are successful. The testing and approval must be accomplished at the lowest costs, using the simplest management solution.<br>Which solution will meet these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a manual approval action after the build action of the pipeline. Use Amazon SNS to inform the team of the stage being triggered. Next, add a test action using CodeBuild to perform the required tests. At the end of the pipeline, add a deploy action to deploy the application to the next stage.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a test action after the CodeBuild build of the pipeline. Configure the action to use CodeBuild to perform the required tests. If these tests are successful, mark the action as successful. Add a manual approval action that uses Amazon SNS to notify the team, and add a deploy action to deploy the application to the next stage.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new pipeline that uses a source action that gets the code from the same repository as the first pipeline. Add a deploy action to deploy the code to a test environment. Use a test action using AWS Lambda to test the deployment. Add a manual approval action by using Amazon SNS to notify the team, and add a deploy action to deploy the application to the next stage.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a test action after the build action. Use a Jenkins server on Amazon EC2 to perform the required tests and mark the action as successful if the tests pass. Create a manual approval action that uses Amazon SQS to notify the team and add a deploy action to deploy the application to the next stage."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-10-06T16:57:00.000Z",
        "voteCount": 8,
        "content": "I will go with B."
      },
      {
        "date": "2021-11-02T16:28:00.000Z",
        "voteCount": 5,
        "content": "The answer is B."
      },
      {
        "date": "2023-01-24T14:10:00.000Z",
        "voteCount": 1,
        "content": "Answer is B\nhttps://docs.aws.amazon.com/codebuild/latest/userguide/how-to-create-pipeline-add-test.html\nhttps://docs.aws.amazon.com/codepipeline/latest/userguide/approvals.html"
      },
      {
        "date": "2023-01-16T19:15:00.000Z",
        "voteCount": 1,
        "content": "Answer is B without a doubt."
      },
      {
        "date": "2021-10-03T23:51:00.000Z",
        "voteCount": 1,
        "content": "I will got with A"
      },
      {
        "date": "2021-10-29T20:36:00.000Z",
        "voteCount": 5,
        "content": "The manual approval must be after Tests and before deploy, not after build"
      },
      {
        "date": "2021-09-28T15:00:00.000Z",
        "voteCount": 2,
        "content": "Reference: https://docs.aws.amazon.com/codebuild/latest/userguide/how-to-create-pipeline-add-test.html"
      },
      {
        "date": "2021-10-27T12:31:00.000Z",
        "voteCount": 5,
        "content": "I'll go with B \n\nbecause its the simplest way with lower costs"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 60,
    "url": "https://www.examtopics.com/discussions/amazon/view/79621-exam-aws-devops-engineer-professional-topic-1-question-60/",
    "body": "A DevOps engineer wants to deploy a serverless web application that is based on AWS Lambda. The deployment must meet the following requirements:<br>\u2711 Provide staging and production environments.<br>\u2711 Restrict developers from accessing the production environment.<br>\u2711 Avoid hardcoding passwords in the Lambda functions.<br>\u2711 Store source code in AWS CodeCommit.<br>\u2711 Use AWS CodePipeline to automate the deployment.<br>What is the MOST operationally efficient solution that meets these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate separate staging and production accounts to segregate deployment targets. Use AWS Key Management Service (AWS KMS) to store environment- specific values. Use CodePipeline to automate deployments with AWS CodeDeploy.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate separate staging and production accounts to segregate deployment targets. Use Lambda environment variables to store environment-specific values. Use CodePipeline to automate deployments with AWS CodeDeploy.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDefine tagging conventions for staging and production environments to segregate deployment targets. Use AWS Key Management Service (AWS KMS) to store environment-specific values. Use CodePipeline to automate deployments with AWS CodeDeploy.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDefine tagging conventions for staging and production environments to segregate deployment targets. Use Lambda environment variables to store environment-specific values. Use CodePipeline to automate deployments with AWS CodeDeploy."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 27,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 14,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-02T07:07:00.000Z",
        "voteCount": 9,
        "content": "You can't use tagging to segregate deployment targets for Lambda. The docu here only says EC2 https://docs.aws.amazon.com/codedeploy/latest/userguide/instances-tagging.html"
      },
      {
        "date": "2022-09-24T15:51:00.000Z",
        "voteCount": 6,
        "content": "Lambda aliases should be used to sperate deplyment stateg. However, with the given options, the best way may be to use different accounts.\nA: Incorrect - KMS can't be used to store environment specific variable. Those are stored as environment variables, but can be encrypted with a KMS key.\nB: Correct\nC and D: Incorrect - Tagging can't be used to segregate deployment targets with Lambda"
      },
      {
        "date": "2023-11-24T09:07:00.000Z",
        "voteCount": 1,
        "content": "My answer is D"
      },
      {
        "date": "2023-08-05T17:39:00.000Z",
        "voteCount": 1,
        "content": "I would go with B as the question asks for MOST operational efficiency: https://aws.amazon.com/blogs/mt/multi-account-strategy-for-small-and-medium-businesses/\n\n- In the Well Architected framework staging and prod should be separated for security and risk management (Limit scope of impact from adverse events)\nhttps://docs.aws.amazon.com/whitepapers/latest/organizing-your-aws-environment/benefits-of-using-multiple-aws-accounts.html"
      },
      {
        "date": "2023-05-28T21:30:00.000Z",
        "voteCount": 1,
        "content": "with option d, it will be very hardto restrict developers to access production environment.\nSo correct answer is b"
      },
      {
        "date": "2023-05-02T22:36:00.000Z",
        "voteCount": 1,
        "content": "B is more suitable here."
      },
      {
        "date": "2023-03-01T00:46:00.000Z",
        "voteCount": 1,
        "content": "KMS is an encryption service, it is not secrets manager.. So you cannot store anything in KMS.. you can either segregate environments using accounts or using tags via policies.. So it would be either B or D.. I think I would go wi B."
      },
      {
        "date": "2023-02-23T15:54:00.000Z",
        "voteCount": 2,
        "content": "A is correct because it suggests to use separate accounts and KMS for secrets"
      },
      {
        "date": "2023-02-21T15:35:00.000Z",
        "voteCount": 1,
        "content": "One of the requirements is this: Restrict developers from accessing the production environment.\nD alone does not restrict users.\nB. meets all requirements."
      },
      {
        "date": "2023-02-07T06:00:00.000Z",
        "voteCount": 1,
        "content": "C and D are eliminated because we need two accounts. Tagging are not strict enough.\nAWS KMS is more efficient than Lambda environment variables.\nI used GCP KMS to manage environment-specific values.\nAWS KMS can also be used to manage environment-specific values. https://medium.com/cloudfordummies/securing-cloud-functions-part-1-using-aws-kms-for-environment-variables-1409597a38ba"
      },
      {
        "date": "2023-01-16T19:19:00.000Z",
        "voteCount": 2,
        "content": "B is the right answer and not D because we want to restrict developer access to the production environment and therefore having a separate account for production makes it more manageble."
      },
      {
        "date": "2023-01-02T02:19:00.000Z",
        "voteCount": 1,
        "content": "B is correct. This is a serverless deployment"
      },
      {
        "date": "2022-12-11T07:15:00.000Z",
        "voteCount": 4,
        "content": "One of the requirements is this: Restrict developers from accessing the production environment.\nD alone does not restrict users.\nB. meets all requirements."
      },
      {
        "date": "2022-11-26T08:30:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/codedeploy/latest/userguide/instances-tagging.html"
      },
      {
        "date": "2023-04-18T17:38:00.000Z",
        "voteCount": 1,
        "content": "only for ec2 and on-prem"
      },
      {
        "date": "2022-11-22T20:11:00.000Z",
        "voteCount": 3,
        "content": "https://aws.amazon.com/blogs/security/guidelines-for-when-to-use-accounts-users-and-groups/\nAlways prefer to use tag to segregate environment rather than using multiple accounts"
      },
      {
        "date": "2022-10-13T05:51:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/IAM/latest/UserGuide/access_tags.html \nOne can use tags and condition statements in policies to restrict access to specific resources."
      },
      {
        "date": "2022-09-09T17:23:00.000Z",
        "voteCount": 4,
        "content": "A,C are wrong. Since KSM can't store environment variable.\nThe difference between B and D is separate environments by account or tagging. Per the requirement - the MOST operationally efficient, I will choice D."
      },
      {
        "date": "2022-09-21T04:03:00.000Z",
        "voteCount": 1,
        "content": "do you have any reference/example showing deployment based on tagging ?!"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 61,
    "url": "https://www.examtopics.com/discussions/amazon/view/52608-exam-aws-devops-engineer-professional-topic-1-question-61/",
    "body": "A company wants to use AWS development tools to replace its current bash deployment scripts. The company currently deploys a LAMP application to a group of<br>Amazon EC2 instances behind an Application Load Balancer (ALB). During the deployments, the company unit tests the committed application, stops and starts services, unregisters and re-registers instances with the load balancer, and updates file permissions. The company wants to maintain the same deployment functionality through the shift to using AWS services.<br>Which solution will meet these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS CodeBuild to test the application. Use bash scripts invoked by AWS CodeDeploy's appspec.yml file to restart services, and deregister and register instances with the ALB. Use the appspec.yml file to update file permissions without a custom script.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS CodePipeline to move the application from the AWS CodeCommit repository to AWS CodeDeploy. Use CodeDeploy's deployment group to test the application, unregister and re-register instances with the ALB, and restart services. Use the appspec.yml file to update the permissions without a custom script.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS CodePipeline to move the application source code from the AWS CodeCommit repository to AWS CodeDeploy. Use CodeDeploy to test the application. Use CodeDeploy's appspec.yml file to restart services and update permissions without a custom script. Use AWS CodeBuild to unregister and re- register instances with the ALB.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS CodePipeline to trigger AWS CodeBuild to test the application. Use bash scripts invoked by AWS CodeDeploy's appspec.yml file to restart services. Unregister and re-register the instances in the AWS CodeDeploy deployment group with the ALB. Update the appspec.yml file to update file permissions without a custom script.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 6,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-09-27T02:49:00.000Z",
        "voteCount": 15,
        "content": "D is the correct answer. CodeBuild to test the appliction."
      },
      {
        "date": "2021-10-15T16:45:00.000Z",
        "voteCount": 1,
        "content": "You can test with codedeploy: https://aws.amazon.com/blogs/devops/how-to-test-and-debug-aws-codedeploy-locally-before-you-ship-your-code/#:~:text=You%20can%20test%20application%20code,local%20server%20or%20EC2%20instance."
      },
      {
        "date": "2022-01-30T14:57:00.000Z",
        "voteCount": 3,
        "content": "You can test application **code packages** (NOT application) on any machine that has the CodeDeploy agent installed before you deploy it through the service. Likewise, to debug locally, you just need to install the CodeDeploy agent on any machine, including your local server or EC2 instance.\nWhere as CodeBuild compiles your source code, runs unit tests, and produces artifacts that are ready to deploy.\nThe correct answer is D"
      },
      {
        "date": "2022-09-27T19:13:00.000Z",
        "voteCount": 5,
        "content": "why not A ?"
      },
      {
        "date": "2024-03-12T15:52:00.000Z",
        "voteCount": 1,
        "content": "Requirement is to replace the current bash script. Answer A and D are out because the solution uses the script. Between B and C, the answer is B"
      },
      {
        "date": "2023-11-27T08:13:00.000Z",
        "voteCount": 1,
        "content": "My answer is D"
      },
      {
        "date": "2023-09-29T21:15:00.000Z",
        "voteCount": 1,
        "content": "you should use codebuild for testing"
      },
      {
        "date": "2023-07-23T00:20:00.000Z",
        "voteCount": 1,
        "content": "which is correct, Correct answer or most votted.. bit confusing"
      },
      {
        "date": "2023-04-12T09:53:00.000Z",
        "voteCount": 1,
        "content": "Since the company wants to use AWS development tools to replace its current bash deployment scripts I don't see any point to choose D."
      },
      {
        "date": "2023-04-12T09:59:00.000Z",
        "voteCount": 1,
        "content": "I think it is better to assume that CodeDeploy agent is installed on EC2 it order to test the code with CodeDeploy than to go against the main requirement for replacement its current bash scripts, so that's why I marked B as an answer."
      },
      {
        "date": "2023-03-09T13:36:00.000Z",
        "voteCount": 1,
        "content": "D is the most appropriate. However, the company wants to replace bash deployment scripts. D suggests using them. hence B serves the company's requirements."
      },
      {
        "date": "2023-02-23T16:02:00.000Z",
        "voteCount": 2,
        "content": "Option B is the most appropriate solution because it uses AWS CodePipeline to move the application from the AWS CodeCommit repository to AWS CodeDeploy, which can test the application, unregister and re-register instances with the ALB, and restart services. The appspec.yml file can be used to update the permissions without a custom script."
      },
      {
        "date": "2023-01-16T19:23:00.000Z",
        "voteCount": 1,
        "content": "D is correct."
      },
      {
        "date": "2022-12-23T05:53:00.000Z",
        "voteCount": 2,
        "content": "d is correct"
      },
      {
        "date": "2022-09-25T04:17:00.000Z",
        "voteCount": 3,
        "content": "Though Codedeploy can be used for unit testing applications, it requires Codedeploy agents installed on the machine (https://aws.amazon.com/blogs/devops/how-to-test-and-debug-aws-codedeploy-locally-before-you-ship-your-code/#:~:text=You%20can%20test%20application%20code,local%20server%20or%20EC2%20instance). However, the correct approach is to use Codebuild for unit testing.\nA: Incorrect: There is no deployment pipeline. Uses only Codebuild\nB and C: Incorrect: Codebuild is used for testing - not Codedeploy\nD: Correct. Has Codepipeline, Codebuild for unit testing and Codedeploy"
      },
      {
        "date": "2022-02-11T07:50:00.000Z",
        "voteCount": 2,
        "content": "D, use CodeBuild for unit tests, CodeDeploy to run scripts"
      },
      {
        "date": "2021-11-04T11:00:00.000Z",
        "voteCount": 3,
        "content": "D is correct. Unit testing should be done with CodeBuild"
      },
      {
        "date": "2021-11-01T14:14:00.000Z",
        "voteCount": 2,
        "content": "D is correct"
      },
      {
        "date": "2021-11-01T01:13:00.000Z",
        "voteCount": 5,
        "content": "B is correct, you can test with codedeploy as long as the agent is installed.."
      },
      {
        "date": "2021-12-26T21:08:00.000Z",
        "voteCount": 3,
        "content": "If the question doesn't say the agent is installed, you assume it is not."
      },
      {
        "date": "2023-02-07T06:19:00.000Z",
        "voteCount": 2,
        "content": "Gee ......"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 62,
    "url": "https://www.examtopics.com/discussions/amazon/view/52620-exam-aws-devops-engineer-professional-topic-1-question-62/",
    "body": "A company gives its employees limited rights to AWS. DevOps engineers have the ability to assume an administrator role. For tracking purposes, the security team wants to receive a near-real-time notification when the administrator role is assumed.<br>How should this be accomplished?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure AWS Config to publish logs to an Amazon S3 bucket. Use Amazon Athena to query the logs and send a notification to the security team when the administrator role is assumed.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Amazon GuardDuty to monitor when the administrator role is assumed and send a notification to the security team.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon EventBridge (Amazon CloudWatch Events) event rule using an AWS Management Console sign-in events event pattern that publishes a message to an Amazon SNS topic if the administrator role is assumed.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon EventBridge (Amazon CloudWatch Events) events rule using an AWS API call that uses an AWS CloudTrail event pattern to trigger an AWS Lambda function that publishes a message to an Amazon SNS topic if the administrator role is assumed.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 6,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-10-07T06:33:00.000Z",
        "voteCount": 19,
        "content": "D should be right answer as it covers all methods of assuming admin role - not just management console"
      },
      {
        "date": "2021-10-09T06:35:00.000Z",
        "voteCount": 1,
        "content": "This is the most convincing answer. Thanks D2"
      },
      {
        "date": "2021-09-30T17:10:00.000Z",
        "voteCount": 13,
        "content": "D is the answer here"
      },
      {
        "date": "2021-10-31T02:28:00.000Z",
        "voteCount": 3,
        "content": "Why is D the answer ? CloudTrail takes up to 15mins whereby question ask for near real time."
      },
      {
        "date": "2022-12-27T07:53:00.000Z",
        "voteCount": 2,
        "content": "The questions says Near Real time and not real time"
      },
      {
        "date": "2023-02-23T16:08:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is option D.\n\nExplanation:\nTo track the assumption of an AWS administrator role in near real-time, you can use Amazon CloudWatch Events and AWS CloudTrail.\n\nAmazon CloudWatch Events allows you to create rules that can match incoming events and take action on them. You can use an AWS CloudTrail event pattern to match the event where the administrator role is assumed.\n\nWhen an event rule matches an incoming event, it triggers an AWS Lambda function. You can configure the Lambda function to publish a message to an Amazon SNS topic that notifies the security team.\n\nTherefore, option D is the correct answer as it provides a near-real-time notification to the security team when the administrator role is assumed by using Amazon EventBridge (CloudWatch Events) events rule, AWS CloudTrail event pattern, AWS Lambda function, and Amazon SNS."
      },
      {
        "date": "2023-02-11T05:44:00.000Z",
        "voteCount": 1,
        "content": "I think it is D"
      },
      {
        "date": "2023-02-07T06:28:00.000Z",
        "voteCount": 1,
        "content": "D must be doable. C is much easier."
      },
      {
        "date": "2023-01-24T14:46:00.000Z",
        "voteCount": 3,
        "content": "D is the asnwer.\n\nExample:\n \"detail\": {\n    \"eventVersion\": \"1.08\",\n    \"userIdentity\": {\n      \"type\": \"AssumedRole\",\n      \"principalId\": \"XYZZYOR:admin\",\n      \"arn\": \"arn:aws:sts::123456789012:role/admin\",\n      \"accountId\": \"123456789012\",\n      \"accessKeyId\": \"XYZZY\",\n      \"sessionContext\": {\n        \"sessionIssuer\": {\n          \"type\": \"Role\",\n          \"principalId\": \"XYZZYOR\",\n          \"arn\": \"arn:aws:iam::123456789012:role/Admin\",\n          \"accountId\": \"123456789012\",\n          \"userName\": \"Admin\"\n        },\n        \"webIdFederationData\": {},\n        \"attributes\": {\n          \"creationDate\": \"2022-02-17T09:41:02Z\",\n          \"mfaAuthenticated\": \"false\"\n        }\n      }\n    },"
      },
      {
        "date": "2023-01-16T19:44:00.000Z",
        "voteCount": 4,
        "content": "Answer is D.  Based on the article below, assuming Role using STS ( IAM switchRole feature) is not considered as one of the AWS Console Sign-in events. Only direct sign-in using root and IAM user along with federated sign-in using AWS SSO are considered to be AWS Console Sign-in events. But once you sign in, any role switching performed to login as an administrator in the master account is not considered a sign-in event. Also as it's possible to Assume role using STS: AssumeRole or AssumeRoleWithSAML API and therefore such events will not be logged in as Console Sign-in events even if my above explanation of Console Sign-in events is not accurate. As a result the correct answer is D."
      },
      {
        "date": "2023-01-16T19:44:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-event-reference-aws-console-sign-in-events.html"
      },
      {
        "date": "2023-01-09T11:32:00.000Z",
        "voteCount": 1,
        "content": "CloudTrail is near real time"
      },
      {
        "date": "2022-12-23T07:36:00.000Z",
        "voteCount": 1,
        "content": "REF : https://aws.amazon.com/blogs/mt/monitor-and-notify-on-aws-account-root-user-activity/"
      },
      {
        "date": "2022-12-10T07:36:00.000Z",
        "voteCount": 1,
        "content": "Cloud trail - 'NEAR' real time is the key word"
      },
      {
        "date": "2022-11-20T16:26:00.000Z",
        "voteCount": 2,
        "content": "C is right answer."
      },
      {
        "date": "2022-11-13T13:19:00.000Z",
        "voteCount": 1,
        "content": "I go for D Since Cloudtrail deals with taking note of who or what accesses any API(in this instance login and assume roles API)"
      },
      {
        "date": "2022-10-28T17:54:00.000Z",
        "voteCount": 1,
        "content": "D\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/cloudtrail-integration.html#cloudtrail-integration_apis"
      },
      {
        "date": "2022-09-25T04:32:00.000Z",
        "voteCount": 3,
        "content": "The requirement is to track whenever the DevOps engineer assumes Admin role (not the console sign-in events as mentioned in answer C).\nCloudTrail logs attempts to sign into the AWS Management Console (https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-event-reference-aws-console-sign-in-events.html). CloudTrail can be configured to send events to CloudWatch Logs, and CloudWatch sends an SNS notification.\nThe requirement is for a NEAR real time - unfortunately CloudTrail typically delivers logs within an average of about 15 minutes of an API call. This time is not guaranteed."
      },
      {
        "date": "2022-08-18T02:09:00.000Z",
        "voteCount": 1,
        "content": "You can use sns topic as an event bridge target. There\u2019s no need to put lambda in between. This solution is easier to implement, cheaper and more straight forward."
      },
      {
        "date": "2022-07-17T01:00:00.000Z",
        "voteCount": 2,
        "content": "The answer is D \nThere is nothing like an AWS Management Console sign-in events event trigger"
      },
      {
        "date": "2022-08-18T02:11:00.000Z",
        "voteCount": 2,
        "content": "Actually there\u2019s https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-event-reference-aws-console-sign-in-events.html"
      },
      {
        "date": "2022-06-23T00:36:00.000Z",
        "voteCount": 2,
        "content": "https://aws.amazon.com/premiumsupport/knowledge-center/root-user-account-eventbridge-rule/\nrefer this link"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 63,
    "url": "https://www.examtopics.com/discussions/amazon/view/28676-exam-aws-devops-engineer-professional-topic-1-question-63/",
    "body": "An ecommerce company uses a large number of Amazon EBS backed Amazon EC2 instances. To decrease manual work across all the instances, a DevOps<br>Engineer is tasked with automating restart actions when EC2 instance retirement events are scheduled.<br>How can this be accomplished?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a scheduled Amazon CloudWatch Events rule to execute an AWS Systems Manager automation document that checks if any EC2 instances are scheduled for retirement once a week. If the instance is scheduled for retirement, the automation document will hibernate the instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable EC2 Auto Recovery on all of the instances. Create an AWS Config rule to limit the recovery to occur during a maintenance window only.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReboot all EC2 instances during an approved maintenance window that is outside of standard business hours. Set up Amazon CloudWatch alarms to send a notification in case any instance is failing EC2 instance status checks.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up an AWS Health Amazon CloudWatch Events rule to execute AWS Systems Manager automation documents that stop and start the EC2 instance when a retirement scheduled event occurs.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-10-03T11:27:00.000Z",
        "voteCount": 13,
        "content": "D is correct (https://aws.amazon.com/premiumsupport/knowledge-center/ec2-instance-retirement/)"
      },
      {
        "date": "2023-02-07T06:40:00.000Z",
        "voteCount": 1,
        "content": "A is eliminated. Once a week may be not enough.\nB is eliminated. Recovery does not match this scenario.\nC is eliminated. Reboot does not match this scenario."
      },
      {
        "date": "2023-01-16T19:48:00.000Z",
        "voteCount": 1,
        "content": "D is the right answer."
      },
      {
        "date": "2022-09-26T07:05:00.000Z",
        "voteCount": 4,
        "content": "Here is the link:\nhttps://aws.amazon.com/blogs/mt/automate-remediation-actions-for-amazon-ec2-notifications-and-beyond-using-ec2-systems-manager-automation-and-aws-health/"
      },
      {
        "date": "2021-10-19T22:57:00.000Z",
        "voteCount": 3,
        "content": "will go with D"
      },
      {
        "date": "2021-10-17T04:25:00.000Z",
        "voteCount": 4,
        "content": "I'll go with D"
      },
      {
        "date": "2021-10-12T15:18:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 64,
    "url": "https://www.examtopics.com/discussions/amazon/view/47010-exam-aws-devops-engineer-professional-topic-1-question-64/",
    "body": "A company that runs many workloads on AWS has an Amazon EBS spend that has increased over time. The DevOps team notices there are many unattached<br>EBS volumes. Although there are workloads where volumes are detached, volumes over 14 days old are stale and no longer needed. A DevOps engineer has been tasked with creating automation that deletes unattached EBS volumes that have been unattached for 14 days.<br>Which solution will accomplish this?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the AWS Config ec2-volume-inuse-check managed rule with a configuration changes trigger type and an Amazon EC2 volume resource target. Create a new Amazon CloudWatch Events rule scheduled to execute an AWS Lambda function in 14 days to delete the specified EBS volume.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon EC2 and Amazon Data Lifecycle Manager to configure a volume lifecycle policy. Set the interval period for unattached EBS volumes to 14 days and set the retention rule to delete. Set the policy target volumes as *.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon CloudWatch Events rule to execute an AWS Lambda function daily. The Lambda function should find unattached EBS volumes and tag them with the current date, and delete unattached volumes that have tags with dates that are more than 14 days old.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Trusted Advisor to detect EBS volumes that have been detached for more than 14 days. Execute an AWS Lambda function that creates a snapshot and then deletes the EBS volume."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 15,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-10-05T17:58:00.000Z",
        "voteCount": 27,
        "content": "I'll go with C\n\nReference: https://aws.amazon.com/blogs/mt/controlling-your-aws-costs-by-deleting-unused-amazon-ebs-volumes/"
      },
      {
        "date": "2021-10-14T05:39:00.000Z",
        "voteCount": 10,
        "content": "There are two correct answers in this question and not much of a reason to pick one over another, very poorly put. I will go with C.\n\tA. AWS Config nope\n\tB. Data Lifecycle Manager nope\n\tC. Lambda does the job and completely remove, no snapshots. The question says \"stale and no longer needed\", so I think that is the way to go.\nTrust Advisor and Lambda works too, but the difference here is that this one creates a snapshot, so should we keep or not? If the question says \"stale and no longer needed\" I don't think we should."
      },
      {
        "date": "2021-10-27T16:07:00.000Z",
        "voteCount": 5,
        "content": "I'd go with D.\nThe way C is described, it would run daily and tag unattached EBS volumes with current date. This means that unattached EBS volumes will keep receiving the current date every day, and the date will never become 14 days old to be deleted. To be correct, C should have said that volumes already having the tag would not be tagged again, but that's not indicated.\nD is a much cleaner way of doing things as it does not relies on these logic faults."
      },
      {
        "date": "2021-10-19T18:15:00.000Z",
        "voteCount": 10,
        "content": "D: https://github.com/aws/Trusted-Advisor-Tools/tree/master/UnderutilzedEBSVolumes\n\nThis is a serverless (Lambda) application that reacts to Trusted Advisor warnings via CloudWatch rules to detect and delete Underutilized EBS volumes - volumes that have been unattached or had low I/O for a number of days. The app will only delete idle, unattached volumes after successfully taking a snapshot. It sends an email with information on how to recover the volume from the snapshot."
      },
      {
        "date": "2021-10-31T22:33:00.000Z",
        "voteCount": 3,
        "content": "https://docs.aws.amazon.com/awssupport/latest/user/trusted-advisor-check-reference.html#underutilized-amazon-ebs-volumes"
      },
      {
        "date": "2023-08-06T21:32:00.000Z",
        "voteCount": 1,
        "content": "I will go with C: Here are tolls that can be used to automate this: https://catalog.workshops.aws/msft-costopt/en-US/storage/ebs/unattached-ebs-volumes\nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/delete-unused-amazon-elastic-block-store-amazon-ebs-volumes-by-using-aws-config-and-aws-systems-manager.html\n\nB is not correct because Amazon Data Lifecycle Manager manages EBS snapshots not volumes and EBS backed AMIs https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/snapshot-lifecycle.html"
      },
      {
        "date": "2023-07-31T10:46:00.000Z",
        "voteCount": 1,
        "content": "A - only checks if volume or related instance is marked for termination/deletion. Nope.\nB. DLM cannot detect unattached volumes. Go actually try &amp; configure it. You filter on tag &amp; say delete volume after 14 days but you cannot specify if its attach or not - just tag filtering.\nD. Does not work.\nC. Only viable option."
      },
      {
        "date": "2023-03-01T13:35:00.000Z",
        "voteCount": 3,
        "content": "Force to choose  C\nA. will not work correctly.\nB. DLM can work only with snapshots\nC. Correct\nD. Trusted advisor cannot check for unattached EBS for 14 days - it can only check for underutilized volumes - those volumes can also be attached and the check is for 7 days.\n\"Yellow: A volume is unattached or had less than 1 IOPS per day for the past 7 days.\""
      },
      {
        "date": "2023-02-07T06:48:00.000Z",
        "voteCount": 1,
        "content": "Data Lifecycle Manager is the most straightforward solution https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/snapshot-lifecycle.html"
      },
      {
        "date": "2023-01-16T20:03:00.000Z",
        "voteCount": 2,
        "content": "Correct answer is C. D is incorrect because Trusted Advisor detects only under provisioned and over provisioned EBS volume meaning utilization. If the EBS volume is already deattached and not utilized by any EC2 instance, then I don't think Trusted Advisor would detect that. I am not very sure about it but that's my guess. C seems logically correct. Some folks didn't select C because they thought Lambda will tag the unattached EBS volume each day ( again and again). But thats not true. Lambda will do so only once per EBS volume and therefore will be able to detect if a specifc volume was unused for 14 days and then delete it."
      },
      {
        "date": "2023-01-10T06:40:00.000Z",
        "voteCount": 1,
        "content": "It should be C not B. Data Lifecycle Manager is for EBS snapshots not the volume. https://aws.amazon.com/blogs/storage/automating-amazon-ebs-snapshot-and-ami-management-using-amazon-dlm/"
      },
      {
        "date": "2023-01-02T15:23:00.000Z",
        "voteCount": 2,
        "content": "I'll go with C"
      },
      {
        "date": "2022-12-10T07:47:00.000Z",
        "voteCount": 2,
        "content": "https://aws.amazon.com/blogs/mt/controlling-your-aws-costs-by-deleting-unused-amazon-ebs-volumes/"
      },
      {
        "date": "2022-11-08T19:41:00.000Z",
        "voteCount": 2,
        "content": "Amazon Data Lifecycle Manager cannot discover unattached disks and AWS Trusted Advisor can only discover currently unattached disks\n\nThis question has a complicated request, so I have no choice but to rely on the Lambda function"
      },
      {
        "date": "2022-11-07T23:45:00.000Z",
        "voteCount": 3,
        "content": "I will go with C\nA. AWS Config  Checks if EBS volumes are attached to EC2 instances. If the volume is 'available state', then you configure CloudWatch Event to invoke a Lambda function. Lambda function will take care of deletion. Note that Config does not only trigger unused EBS volumes for 14 days, but every unused EBS volume(irrespective of the number of days its been unused). Though it can be made work, its not the most efficient solution\nB. Data Lifecycle Manager only looks at EBS snapshot lifecycle. Not the EBS volumes itself\nC. Lambda function triggered periodically and tagging the new volumes and checking if already tagged volumes are &gt;= 14 days old. If it finds any &gt;= 14 days old, Lambda will delete them\n4. Trusted advisor does not check for unused volumes, it checks for under utilised EBS volumes\nSo C is the best answer in this context."
      },
      {
        "date": "2022-09-26T07:22:00.000Z",
        "voteCount": 1,
        "content": "At first I thought Ans D seems the best option - but this answers creates a snapshot. creating a snapshot is not a requirement.\nSo, A seems to be the best option."
      },
      {
        "date": "2022-09-19T03:02:00.000Z",
        "voteCount": 2,
        "content": "Though few of the the answers are poorly worded, D seems to be best of the lot. With A it is not clear how config will identify it is 14 days since volume was unattached, B works with lifecycle of snapshots, not directly with lifecycle of volume, C has a logic flaw if date is updated D. That leaves D(though it is not clear how D will be automated and why it is creating snapshots before delete when not asked in Qs.)"
      },
      {
        "date": "2022-09-19T03:02:00.000Z",
        "voteCount": 2,
        "content": "With C, the lambda will update date tag daily with current date"
      },
      {
        "date": "2022-09-17T18:28:00.000Z",
        "voteCount": 1,
        "content": "honestly there is no correct ans here.\n\nthe closest is C, it would hv been logically doable if the tagging of current date is mentioned AFTER the function/logic that detects &amp; delete 14 days old EBS.\n\nI choose B still. Though tt \"Use Amazon EC2 and Amazon Data Lifecycle Manager to configure a volume lifecycle policy\" line sounds so wrong."
      },
      {
        "date": "2022-09-07T15:30:00.000Z",
        "voteCount": 1,
        "content": "D makes more sense. Trusted Advisor is designed to give you information about underutilized ebs volumes."
      },
      {
        "date": "2022-08-30T18:46:00.000Z",
        "voteCount": 2,
        "content": "D maybe is valid, but i don't see any way to create a automatic way to detect, the C option at least is performing a daily task to validate the volumes"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 65,
    "url": "https://www.examtopics.com/discussions/amazon/view/80257-exam-aws-devops-engineer-professional-topic-1-question-65/",
    "body": "A company has multiple child accounts that are part of an organization in AWS Organizations. The security team needs to review every Amazon EC2 security group and their inbound and outbound rules. The security team wants to programmatically retrieve this information from the child accounts using an AWS Lambda function in the management account of the organization.<br>Which combination of access changes will meet these requirements? (Choose three.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a trust relationship that allows users in the child accounts to assume the management account IAM role.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a trust relationship that allows users in the management account to assume the IAM roles of the child accounts.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an IAM role in each child account that has access to the AmazonEC2ReadOnlyAccess managed policy.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an IAM role in each child account to allow the sts:AssumeRole action against the management account IAM role's ARN.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an IAM role in the management account that allows the sts:AssumeRole action against the child account IAM role's ARN.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an IAM role in the management account that has access to the AmazonEC2ReadOnlyAccess managed policy."
    ],
    "answer": "BCE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BCE",
        "count": 8,
        "isMostVoted": true
      },
      {
        "answer": "BEF",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-03-06T07:46:00.000Z",
        "voteCount": 1,
        "content": "BCE is correct."
      },
      {
        "date": "2023-03-01T13:39:00.000Z",
        "voteCount": 1,
        "content": "BCE - no other correct answers"
      },
      {
        "date": "2023-02-20T14:48:00.000Z",
        "voteCount": 2,
        "content": "BCE is the best answer."
      },
      {
        "date": "2023-02-17T11:57:00.000Z",
        "voteCount": 1,
        "content": "BCE.\nThink of Control Tower, we have the AWSControlTowerExecution role.\nC - This role has to be present in every single child account. It will have the permissions needed to perform the required actions\nB - This role has a trust relationship to the management account users.\nIn the \nE - We have AWSControlTowerAdmin role in the management account that has sts assumerole on the child accounts AWSControlTowerExecution roles"
      },
      {
        "date": "2023-02-07T06:54:00.000Z",
        "voteCount": 1,
        "content": "The other three, ACD, look undoable."
      },
      {
        "date": "2023-01-16T20:11:00.000Z",
        "voteCount": 2,
        "content": "BCE are the correct answers."
      },
      {
        "date": "2022-10-06T15:32:00.000Z",
        "voteCount": 3,
        "content": "watch this AWS Video; well explained =&gt;  https://www.youtube.com/watch?v=20tr9gUY4i0"
      },
      {
        "date": "2022-09-07T15:32:00.000Z",
        "voteCount": 1,
        "content": "BCE make sense."
      },
      {
        "date": "2022-09-06T22:42:00.000Z",
        "voteCount": 1,
        "content": "Ans: BCE"
      },
      {
        "date": "2022-09-05T01:44:00.000Z",
        "voteCount": 2,
        "content": "B,C,EEEEE"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 66,
    "url": "https://www.examtopics.com/discussions/amazon/view/28616-exam-aws-devops-engineer-professional-topic-1-question-66/",
    "body": "An application is deployed on Amazon EC2 instances running in an Auto Scaling group. During the bootstrapping process, the instances register their private IP addresses with a monitoring system. The monitoring system performs health checks frequently by sending ping requests to those IP addresses and sending alerts if an instance becomes non-responsive.<br>The existing deployment strategy replaces the current EC2 instances with new ones. A DevOps Engineer has noticed that the monitoring system is sending false alarms during a deployment, and is tasked with stopping these false alarms.<br>Which solution will meet these requirements without affecting the current deployment method?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDefine an Amazon CloudWatch Events target, an AWS Lambda function, and a lifecycle hook attached to the Auto Scaling group. Configure CloudWatch Events to invoke Amazon SNS to send a message to the Systems Administrator group for remediation.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDefine an AWS Lambda function and a lifecycle hook attached to the Auto Scaling group. Configure the lifecycle hook to invoke the Lambda function, which removes the entry of the private IP from the monitoring system upon instance termination.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDefine an Amazon CloudWatch Events target, an AWS Lambda function, and a lifecycle hook attached to the Auto Scaling group. Configure CloudWatch Events to invoke the Lambda function, which removes the entry of the private IP from the monitoring system upon instance termination.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDefine an AWS Lambda function that will run a script when instance termination occurs in an Auto Scaling group. The script will remove the entry of the private IP from the monitoring system."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-10-23T07:49:00.000Z",
        "voteCount": 11,
        "content": "I'll go with C"
      },
      {
        "date": "2023-01-16T20:17:00.000Z",
        "voteCount": 5,
        "content": "Correct answer is C. ASG needs CloudWatch Events to trigger Lambda to do the needful."
      },
      {
        "date": "2022-12-04T09:31:00.000Z",
        "voteCount": 3,
        "content": "https://docs.aws.amazon.com/autoscaling/ec2/userguide/tutorial-lifecycle-hook-lambda.html"
      },
      {
        "date": "2022-09-21T15:59:00.000Z",
        "voteCount": 1,
        "content": "\"C\"\nASG Lifecycle hook needs CloudWatch event to trigger Lambda"
      },
      {
        "date": "2022-02-21T12:54:00.000Z",
        "voteCount": 2,
        "content": "The question says that there are false alarms only during deployment. So, I don't understand why \"removing the entry of the private IP from the monitoring system upon instance termination\" will solve this problem. Obviously there is a process already in place to remove these IP address - otherwise there will be false alarms all the time, not during the deployment period only.\nAs stated in https://docs.aws.amazon.com/autoscaling/ec2/userguide/configuring-lifecycle-hook-notifications.html, you can use Amazon SNS to set up a notification target (an SNS topic) to receive notifications when a lifecycle action occurs. I am not sure what is the use of CloudWatchEvents and Lambda, when direct SNS notification is possible.\nAll the answers doesn't make sense - but I guess A may be closest."
      },
      {
        "date": "2022-12-27T09:13:00.000Z",
        "voteCount": 1,
        "content": "Option A would not stop the false alarm, you need an architecture that would take an action to stop the false alarm, not just notifying the admin"
      },
      {
        "date": "2021-11-03T18:42:00.000Z",
        "voteCount": 3,
        "content": "B is right for removing instance from monitoring system. C is not correct"
      },
      {
        "date": "2024-09-28T14:40:00.000Z",
        "voteCount": 1,
        "content": "Even I feel B is right , why include Cloudwatch events at all. \nA common pattern is:\nAuto Scaling group has a lifecycle hook\nLifecycle hook triggers an EventBridge rule\nEventBridge rule invokes a Lambda function\nLambda function performs custom logic and calls CompleteLifecycleAction API to signal Auto Scaling"
      },
      {
        "date": "2021-10-27T09:29:00.000Z",
        "voteCount": 2,
        "content": "B is right. Why CW when ASG lifecycle hooks are tailor-made for this? Also, CW events on their own cannot keep the termination in wait before removing the id."
      },
      {
        "date": "2021-10-30T12:45:00.000Z",
        "voteCount": 4,
        "content": "On relook, i see lifecycle hooks still need CW events. So it has to be C."
      },
      {
        "date": "2021-10-25T04:17:00.000Z",
        "voteCount": 1,
        "content": "Not sure, no need for CW but Lambda also  needs  to send notifications to a SNS topic which is not totally clear in the answer"
      },
      {
        "date": "2021-10-20T02:10:00.000Z",
        "voteCount": 2,
        "content": "I would go with B here"
      },
      {
        "date": "2021-10-21T08:36:00.000Z",
        "voteCount": 4,
        "content": "Change to C, You need CW Events or SNS with ASG Target Group to invoke lambda"
      },
      {
        "date": "2021-10-10T00:57:00.000Z",
        "voteCount": 2,
        "content": "B. Lifecycle hooks can invoke Lambda directly by BridgeEvent\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/configuring-lifecycle-hook-notifications.html"
      },
      {
        "date": "2021-10-12T08:30:00.000Z",
        "voteCount": 1,
        "content": "Route notifications to Lambda using EventBridge\nYou can use EventBridge to set up a target to invoke a Lambda function when a lifecycle action occurs."
      },
      {
        "date": "2021-10-17T03:43:00.000Z",
        "voteCount": 2,
        "content": "Sorry my mistake, we need CW Event, now they call it EventBridge"
      },
      {
        "date": "2021-11-03T10:28:00.000Z",
        "voteCount": 4,
        "content": "so correct answer is C"
      },
      {
        "date": "2021-10-07T09:28:00.000Z",
        "voteCount": 1,
        "content": "B. CW Events not needed."
      },
      {
        "date": "2021-10-02T10:20:00.000Z",
        "voteCount": 1,
        "content": "B. \nEven in their own reference there is no need for cloutdwatch event trigger. You just need lifecycle hook and AWS lamda function.\nhttps://aws.amazon.com/blogs/compute/using-aws-lambda-with-auto-scaling-lifecycle-hooks/"
      },
      {
        "date": "2021-10-06T11:05:00.000Z",
        "voteCount": 2,
        "content": "Changing to C. \nYou need SNS topic to invoke lamdba from Lifecycle hooks. Without that you will need Cloudwatch Event."
      },
      {
        "date": "2021-10-12T23:38:00.000Z",
        "voteCount": 2,
        "content": "I see that LifeCycle hooks can invoke \n1. Lambda by EventBridge \n2. SNS\n3. SQS.\nI don't see CW Event involved here"
      },
      {
        "date": "2021-10-16T16:09:00.000Z",
        "voteCount": 1,
        "content": "Sorry my mistake, we need CW Event, now they call it EventBridge"
      },
      {
        "date": "2021-09-22T03:22:00.000Z",
        "voteCount": 1,
        "content": "A : As Devops Engineer is tasked with stopping these false alarms only without affecting current Deployment Model ."
      },
      {
        "date": "2021-09-20T21:35:00.000Z",
        "voteCount": 1,
        "content": "They changed the name of Cloudwatch Events to EventBridge https://docs.aws.amazon.com/autoscaling/ec2/userguide/cloud-watch-events.html"
      },
      {
        "date": "2021-09-20T05:47:00.000Z",
        "voteCount": 4,
        "content": "C is relevant and correct. \nSNS Notification would have been nice as well."
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 67,
    "url": "https://www.examtopics.com/discussions/amazon/view/8471-exam-aws-devops-engineer-professional-topic-1-question-67/",
    "body": "An e-commerce company is running a web application in an AWS Elastic Beanstalk environment. In recent months, the average load of the Amazon EC2 instances has been increased to handle more traffic.<br>The company would like to improve the scalability and resilience of the environment. The Development team has been asked to decouple long-running tasks from the environment if the tasks can be executed asynchronously. Examples of these tasks include confirmation emails when users are registered to the platform, and processing images or videos. Also, some of the periodic tasks that are currently running within the web server should be offloaded.<br>What is the MOST time-efficient and integrated way to achieve this?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon SQS queue and send the tasks that should be decoupled from the Elastic Beanstalk web server environment to the SQS queue. Create a fleet of EC2 instances under an Auto Scaling group. Use an AMI that contains the application to process the asynchronous tasks, configure the application to listen for messages within the SQS queue, and create periodic tasks by placing those into the cron in the operating system. Create an environment variable within the Elastic Beanstalk environment with a value pointing to the SQS queue endpoint.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a second Elastic Beanstalk worker tier environment and deploy the application to process the asynchronous tasks there. Send the tasks that should be decoupled from the original Elastic Beanstalk web server environment to the auto-generated Amazon SQS queue by the Elastic Beanstalk worker environment. Place a cron.yaml file within the root of the application source bundle for the worker environment for periodic tasks. Use environment links to link the web server environment with the worker environment.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a second Elastic Beanstalk web server tier environment and deploy the application to process the asynchronous tasks. Send the tasks that should be decoupled from the original Elastic Beanstalk web server to the auto-generated Amazon SQS queue by the second Elastic Beanstalk web server tier environment. Place a cron.yaml file within the root of the application source bundle for the second web server tier environment with the necessary periodic tasks. Use environment links to link both web server environments.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon SQS queue and send the tasks that should be decoupled from the Elastic Beanstalk web server environment to the SQS queue. Create a fleet of EC2 instances under an Auto Scaling group. Install and configure the application to listen for messages within the SQS queue from UserData and create periodic tasks by placing those into the cron in the operating system. Create an environment variable within the Elastic Beanstalk web server environment with a value pointing to the SQS queue endpoint."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-21T22:01:00.000Z",
        "voteCount": 16,
        "content": "B\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features-managing-env-tiers.html"
      },
      {
        "date": "2023-01-21T07:03:00.000Z",
        "voteCount": 3,
        "content": "Answer is B because it uses the built in capability of ElasticBeanStalk to create an integrated solution in a timely manner."
      },
      {
        "date": "2022-06-24T10:05:00.000Z",
        "voteCount": 1,
        "content": "Vote B"
      },
      {
        "date": "2021-11-05T10:51:00.000Z",
        "voteCount": 4,
        "content": "I'll go with B"
      },
      {
        "date": "2021-11-04T22:02:00.000Z",
        "voteCount": 4,
        "content": "B looks right.\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts-worker.html"
      },
      {
        "date": "2021-10-22T12:22:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2021-10-12T07:31:00.000Z",
        "voteCount": 1,
        "content": "It is B"
      },
      {
        "date": "2021-10-08T13:38:00.000Z",
        "voteCount": 4,
        "content": "B is the right answer ... Link provided by Sunil86rbk has the details"
      },
      {
        "date": "2021-10-08T06:34:00.000Z",
        "voteCount": 2,
        "content": "it should be B, because of worker."
      },
      {
        "date": "2021-10-07T22:06:00.000Z",
        "voteCount": 2,
        "content": "B: decouple long-running tasks and executed asynchronously"
      },
      {
        "date": "2021-09-19T17:36:00.000Z",
        "voteCount": 2,
        "content": "B is right since you need the SQS function which is available in the worker tier. https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts.html"
      },
      {
        "date": "2021-09-19T17:25:00.000Z",
        "voteCount": 2,
        "content": "B looks right for me"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 68,
    "url": "https://www.examtopics.com/discussions/amazon/view/78834-exam-aws-devops-engineer-professional-topic-1-question-68/",
    "body": "A company has an on-premises that is written in Go. A DevOps engineer must move the application to AWS. The company's development team wants to enable blue/green deployments and perform A/B testing.<br>Which solution will meet these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the application on an Amazon EC2 instance and create an AMI of this instance. Use this AMI to create an automatic scaling launch configuration that is used in an Auto Scaling group. Use an Elastic Load Balancer to distribute traffic. When changes are made to the application, a new AMI will be created, which will initiate an EC2 instance refresh.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Lightsail to deploy the application. Store the application in a zipped format in an Amazon S3 bucket. Use this zipped version to deploy new versions of the application to Lightsail. Use Lightsail deployment options to manage the deployment.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS CodeArtifact to store the application code. Use AWS CodeDeploy to deploy the application to a fleet of Amazon EC2 instances. Use Elastic Load Balancing to distribute the traffic to the EC2 instances. When making changes to the application, upload a new version to CodeArtifact and create a new CodeDeploy deployment.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Elastic Beanstalk to host the application. Store a zipped version of the application in Amazon S3, and use that location to deploy new versions of the application using Elastic Beanstalk to manage the deployment options.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 10,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-04T09:57:00.000Z",
        "voteCount": 5,
        "content": "https://d1.awsstatic.com/whitepapers/AWS_Blue_Green_Deployments.pdf"
      },
      {
        "date": "2023-10-18T10:05:00.000Z",
        "voteCount": 1,
        "content": "C. Beanstalk doesn't do A/B testing."
      },
      {
        "date": "2023-02-08T04:39:00.000Z",
        "voteCount": 2,
        "content": "It's not a good idea to use S3 bucket, so B and D are eliminated. \nI would choose C, however, it's only migration, no development work.  A is the most straightforward solution."
      },
      {
        "date": "2023-01-21T07:17:00.000Z",
        "voteCount": 2,
        "content": "Correct answer is D as ElasticBeamstalk support s Bluegreen deployment. Other options are inaccurately described or technically not viable."
      },
      {
        "date": "2022-11-15T04:18:00.000Z",
        "voteCount": 4,
        "content": "Ans is D"
      },
      {
        "date": "2022-10-08T19:26:00.000Z",
        "voteCount": 1,
        "content": "Why the default answer is A ? Where the blue/green ? Where is the test ?\nMy answer is D"
      },
      {
        "date": "2022-09-25T06:34:00.000Z",
        "voteCount": 3,
        "content": "answer is d"
      },
      {
        "date": "2022-09-21T21:50:00.000Z",
        "voteCount": 2,
        "content": "When an application is developed and deployed to an AWS Elastic Beanstalk environment, having two separate, but identical, environments\u2014blue and green\u2014increases availability and reduces risk."
      },
      {
        "date": "2022-09-18T18:57:00.000Z",
        "voteCount": 4,
        "content": "The answer is C. You can deploy via blue/green in CodeDeploy."
      },
      {
        "date": "2023-03-03T20:35:00.000Z",
        "voteCount": 2,
        "content": "I doubt CodeDeploy works with CodeArtifact. You need to use S3/Github - https://docs.aws.amazon.com/codedeploy/latest/userguide/application-revisions-repository-type.html"
      },
      {
        "date": "2022-09-21T21:46:00.000Z",
        "voteCount": 1,
        "content": "what about A/B Test ? no mention about CodeBuild in C"
      },
      {
        "date": "2023-01-12T20:20:00.000Z",
        "voteCount": 2,
        "content": "AWS CodeDeploy also allows you to perform canary deployments, which is a method of releasing new features to a small percentage of users before making them available to the entire user base. This is a variation of A/B testing, where a small percentage of the traffic is directed to the new version of the application, and the rest continues to see the old version."
      },
      {
        "date": "2022-08-31T18:00:00.000Z",
        "voteCount": 2,
        "content": "D - https://www.examtopics.com/discussions/amazon/view/28618-exam-aws-devops-engineer-professional-topic-1-question-184/"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 69,
    "url": "https://www.examtopics.com/discussions/amazon/view/28040-exam-aws-devops-engineer-professional-topic-1-question-69/",
    "body": "An application runs on Amazon EC2 instances behind an Application Load Balancer (ALB). A DevOps Engineer is using AWS CodeDeploy to release a new version. The deployment fails during the AllowTraffic lifecycle event, but a cause for the failure is not indicated in the deployment logs.<br>What would cause this?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe appspec.yml file contains an invalid script to execute in the AllowTraffic lifecycle hook.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe user who initiated the deployment does not have the necessary permissions to interact with the ALB.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe health checks specified for the ALB target group are misconfigured.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe CodeDeploy agent was not installed in the EC2 instances that are part of the ALB target group."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-10-22T03:23:00.000Z",
        "voteCount": 16,
        "content": "A is incorrect because \"The Start, Install, TestTraffic, AllowTraffic, and End events in the deployment cannot be scripted, which is why they appear in gray in this diagram.\"\nhttps://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html\nB.D is incorrect because if the permission or CodeDeploy agent is not installed, the deploy will not go to AllowTraffic hook, it will be fail prior to AllowTraffic."
      },
      {
        "date": "2021-11-01T11:52:00.000Z",
        "voteCount": 4,
        "content": "So C is correct"
      },
      {
        "date": "2021-11-06T13:26:00.000Z",
        "voteCount": 10,
        "content": "I'll go with C"
      },
      {
        "date": "2023-01-25T05:11:00.000Z",
        "voteCount": 1,
        "content": "Health check should be the culprit."
      },
      {
        "date": "2023-01-21T07:23:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is C"
      },
      {
        "date": "2022-09-21T22:31:00.000Z",
        "voteCount": 5,
        "content": "This failure is typically due to incorrectly configured health checks in Elastic Load Balancing for the Classic Load Balancer, Application Load Balancer, or Network Load Balancer used to manage traffic for the deployment group.\nTo resolve the issue, review and correct any errors in the health check configuration for the load balancer.\n\nhttps://docs.aws.amazon.com/codedeploy/latest/userguide/troubleshooting-deployments.html#troubleshooting-deployments-allowtraffic-no-logs"
      },
      {
        "date": "2021-11-14T01:08:00.000Z",
        "voteCount": 2,
        "content": "correct is C --- https://docs.aws.amazon.com/codedeploy/latest/userguide/troubleshooting-deployments.html"
      },
      {
        "date": "2021-11-03T17:03:00.000Z",
        "voteCount": 1,
        "content": "After reading through the explanations and links here, I agree with all. C is the best choice, thanks everyone for the tips!"
      },
      {
        "date": "2021-10-21T22:14:00.000Z",
        "voteCount": 5,
        "content": "A - AllowTraffic is not a user managed hook\nB - permissions managed by CodeDeploy role, not user. And if we on the AllowTraffic step, then we were able to block LB traffic\nC - right\nD - if there was no agent the job would fail on earlier steps"
      },
      {
        "date": "2021-09-27T05:43:00.000Z",
        "voteCount": 5,
        "content": "C \nIn some cases, a blue/green deployment fails during the AllowTraffic lifecycle event, but the deployment logs do not indicate the cause for the failure.\n\nThis failure is typically due to incorrectly configured health checks in Elastic Load Balancing for the Classic Load Balancer, Application Load Balancer, or Network Load Balancer used to manage traffic for the deployment group.\n\nTo resolve the issue, review and correct any errors in the health check configuration for the load balancer.\n\nFor Classic Load Balancers, see Configure Health Checks in the User Guide for Classic Load Balancers and ConfigureHealthCheck in the Elastic Load Balancing API Reference version 2012-06-01.\n\nFor Application Load Balancers, see Health Checks for Your Target Groups in the User Guide for Application Load Balancers.\n\nFor Network Load Balancers, see Health Checks for Your Target Groups in the Network Load Balancer User Guide."
      },
      {
        "date": "2021-10-03T11:03:00.000Z",
        "voteCount": 2,
        "content": "^That sounds right to me. Here's the link where all of that is said:\nhttps://docs.aws.amazon.com/codedeploy/latest/userguide/troubleshooting-deployments.html"
      },
      {
        "date": "2021-09-19T17:22:00.000Z",
        "voteCount": 5,
        "content": "Ans: C\nhttps://docs.aws.amazon.com/pt_br/elasticloadbalancing/latest/application/target-group-health-checks.html"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 70,
    "url": "https://www.examtopics.com/discussions/amazon/view/10086-exam-aws-devops-engineer-professional-topic-1-question-70/",
    "body": "A company has a single developer writing code for an automated deployment pipeline. The developer is storing source code in an Amazon S3 bucket for each project. The company wants to add more developers to the team but is concerned about code conflicts and lost work. The company also wants to build a test environment to deploy newer versions of code for testing and allow developers to automatically deploy to both environments when code is changed in the repository.<br>What is the MOST efficient way to meet these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS CodeCommit repository for each project, use the main branch for production code, and create a testing branch for code deployed to testing. Use feature branches to develop new features and pull requests to merge code to testing and main branches.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate another S3 bucket for each project for testing code, and use an AWS Lambda function to promote code changes between testing and production buckets. Enable versioning on all buckets to prevent code conflicts.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS CodeCommit repository for each project, and use the main branch for production and test code with different deployment pipelines for each environment. Use feature branches to develop new features.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable versioning and branching on each S3 bucket, use the main branch for production code, and create a testing branch for code deployed to testing. Have developers use each branch for developing in each environment."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 12,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-09-30T19:51:00.000Z",
        "voteCount": 15,
        "content": "the correct answer is A"
      },
      {
        "date": "2021-10-11T04:33:00.000Z",
        "voteCount": 6,
        "content": "The answer should be C because of below line in the question: \"allow Developers to automatically deploy to both environments when code is changed in the repository\".\nOption A does not talk about deployment."
      },
      {
        "date": "2021-11-04T12:46:00.000Z",
        "voteCount": 2,
        "content": "I think it is (C) too because it has a pipeline for deployment and this is a requirement. \n(A) doesnt mention deployment, there is no pipelines.\nRegarding WhyIronMan comment, it doesnt mention it will deploy to production without test, in fact it will have 3 pipelines dev, test and prod, so the code will be tested on the test pipeline, and then deployed to prod as they are separate pipelines."
      },
      {
        "date": "2021-10-25T17:10:00.000Z",
        "voteCount": 4,
        "content": "So, you want deploy do production without test? what warranties you that the code is functional and its not broken?\n\nGenius strategy"
      },
      {
        "date": "2021-11-05T18:18:00.000Z",
        "voteCount": 2,
        "content": "I accidentally liked your answer, but I don't. Nobody is saying that you should not test the code deployed to test environment before proceeding to deploy it to production. How will you know that it is the same code that is deployed in test and production if you deploy from different branches? I'm a developer so I should know :) You're not so genius yourself."
      },
      {
        "date": "2023-03-13T15:23:00.000Z",
        "voteCount": 2,
        "content": "using the main branch for both production and test code can lead to issues with tracking changes and version control. It can also make it difficult to deploy code automatically to the correct environment. That's why the answer is A and not C."
      },
      {
        "date": "2023-02-17T07:39:00.000Z",
        "voteCount": 1,
        "content": "B and D are eliminated, will make chaos.\nC is an incomplete version of A.  If we do not use CodeCommit, we should combine different env files to different branches, like test, staging, prod...."
      },
      {
        "date": "2023-01-25T05:12:00.000Z",
        "voteCount": 2,
        "content": "A) Explains the branching strategy"
      },
      {
        "date": "2023-01-21T09:07:00.000Z",
        "voteCount": 6,
        "content": "Correct answer is A. There is nothing that suggests that the code needs to be deployed simultaneously in both environments. It just says automatically to both environments. That doesn't  mean the pull request from a feature branch needs to be created against both test and prod at the same time. What A will enable us to do is have the feature engineer submit a pull request into the test branch and have the admin user of the tear branch review and approve the pull request. This should automatically start the deployment into the test environment. Once the code is full tested by the QA resource in the test environment, they will initiate a pull request into the Prod environment. The prod environment admin user will review and approve the pull request at which point the code will be automatically deployment to the Prod environment. With C on the other hand submitting a pull request from the feature branch to the master branch will result in triggering the deployment via both pipelines at the same time which means the code will be deployed into production before testing it in the test environment successfully."
      },
      {
        "date": "2023-01-03T15:36:00.000Z",
        "voteCount": 2,
        "content": "I'll go with A"
      },
      {
        "date": "2022-12-27T06:20:00.000Z",
        "voteCount": 2,
        "content": "Answer is A ,as per the question you have to eploy new version of code for testing"
      },
      {
        "date": "2022-11-18T09:37:00.000Z",
        "voteCount": 1,
        "content": "C.\nIt cannot be A because you make the same PR to both branches at the same time ONLY when you need a hotfix to prod and test env. In real world you develop in one branch and when you need a release you merge all the dev branch to prod branch (or create a new version from dev branch if you need to support multiple versions).  But you NEVER EVER make PRs to both dev and prod branches during development.\nIf you both branches are always the same - why do you need 2 branches ? If your master and testing branches differ then at some point you will have conflicts. \nOnly C makes sense."
      },
      {
        "date": "2022-09-26T15:31:00.000Z",
        "voteCount": 1,
        "content": "With a different test branch, there is no way to know if the same code is deployed to two different environments."
      },
      {
        "date": "2022-09-10T19:20:00.000Z",
        "voteCount": 3,
        "content": "I choice A \nThe difference between A and C is that C uses master branch for the production and testing environment, A uses different branches for the production and testing environment. The better way is A. \nB,D S3 is not good choice for team working."
      },
      {
        "date": "2022-08-18T13:13:00.000Z",
        "voteCount": 4,
        "content": "Honestly a prefer 3 different branches, A option\n\nEvent with the C option different pipelines, which event is triggering the pipeline, when i push the commit in the test/prod branch bad idea, manual trigger? not too devOps at least, another type of trigger maybe add more complexity."
      },
      {
        "date": "2022-03-04T22:57:00.000Z",
        "voteCount": 2,
        "content": "different codepipelines for the same branch seems normal"
      },
      {
        "date": "2021-11-02T19:52:00.000Z",
        "voteCount": 4,
        "content": "I'll go with A.\n\nfor those choosing C:\nSo, do you want deploy do production without testing first? \nwhat warranties you that the code is functional and its not broken?\n\nYeah, keep deploying untested code to production...its a genius strategy"
      },
      {
        "date": "2021-11-07T05:55:00.000Z",
        "voteCount": 4,
        "content": "You often deploy from the same branch so that you know that it is the same code running in all environments. How will you know this otherwise? That can be really unstable if you dont have a pipeline set up that makes sure that no one can push to the master branch anywhere else than from this \"test branch\". \nBut you ALWAYS test the code in the test / staging environment before proceeding to deploy it to production. This is how its often done (ofc exceptions exists) and it is a very good strategy. I bet y'all answering here have no experience working as developers, I have and I know this."
      },
      {
        "date": "2021-12-26T00:11:00.000Z",
        "voteCount": 3,
        "content": "Agree with C, talking about branch strategy (only master for 2 environments) and mention deployments. \n\nA is not talking about deployment so you are not meeting the requirements... only mention branch strategy (in more detail and different from A)"
      },
      {
        "date": "2021-10-20T23:28:00.000Z",
        "voteCount": 2,
        "content": "Correct Answer: C"
      },
      {
        "date": "2021-10-17T00:45:00.000Z",
        "voteCount": 2,
        "content": "A is correct"
      },
      {
        "date": "2021-10-16T13:07:00.000Z",
        "voteCount": 2,
        "content": "Agreed with A"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 71,
    "url": "https://www.examtopics.com/discussions/amazon/view/80403-exam-aws-devops-engineer-professional-topic-1-question-71/",
    "body": "A development team is building an ecommerce application and is using Amazon Simple Notification Service (Amazon SNS) to send order messages to multiple endpoints. One of the endpoints is an external HTTP endpoint that is not always available. The development team needs to receive a notification if an order message is not delivered to the HTTP endpoint.<br>What should a DevOps engineer do to meet these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon Simple Queue Service (Amazon SQS) queue. On the SNS topic, configure a redrive policy that sends undelivered messages to the SQS queue. Create an Amazon CloudWatch alarm for the new SQS queue to notify the development team when messages are delivered to the queue.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon Simple Queue Service (Amazon SQS) queue. On the HTTP endpoint subscription of the SNS topic, configure a redrive policy that sends undelivered messages to the SQS queue. Create an Amazon CloudWatch alarm for the new SQS queue to notify the development team when messages are delivered to the queue.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOn the SNS topic, configure an HTTPS delivery policy that will retry delivery until the order message is delivered successfully. Configure the backoffFunction parameter in the policy to notify the development team when a message cannot be delivered within the set constraints.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOn the HTTP endpoint subscription of the SNS topic, configure an HTTPS delivery policy that will retry delivery until the order message is delivered successfully. Configure the backoffFunction parameter in the policy to notify the development team when a message cannot be delivered within the set constraints."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-02-08T05:25:00.000Z",
        "voteCount": 3,
        "content": "The link and material offered by Paresh_Jadhav helps."
      },
      {
        "date": "2023-01-21T09:20:00.000Z",
        "voteCount": 1,
        "content": "B us the correct answer. You don't want to lose messages and therefore SQS queue and the retrieve configuration is at the subscription level and not at the topic level."
      },
      {
        "date": "2022-12-27T06:23:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2022-11-27T21:24:00.000Z",
        "voteCount": 4,
        "content": "B make sense\nhttps://docs.aws.amazon.com/sns/latest/dg/sns-message-delivery-retries.html\n\n\"To keep the message after the retries specified in the delivery policy are exhausted, configure your subscription to move undeliverables messages to a dead-letter queue (DLQ).\"\n\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-available-cloudwatch-metrics.html"
      },
      {
        "date": "2022-11-23T21:38:00.000Z",
        "voteCount": 3,
        "content": "Voted B, because backoffFunction is just a definition algorithm of backoff retry, cannot use it to notify the team."
      },
      {
        "date": "2022-09-18T01:06:00.000Z",
        "voteCount": 1,
        "content": "B is true. \nC, D is Https endpoint not http"
      },
      {
        "date": "2022-09-13T23:20:00.000Z",
        "voteCount": 1,
        "content": "You can use Amazon CloudWatch metrics to monitor dead-letter queues associated with your Amazon SNS subscriptions. All Amazon SQS queues emit CloudWatch metrics at one-minute intervals."
      },
      {
        "date": "2022-09-13T04:42:00.000Z",
        "voteCount": 3,
        "content": "The answer is B:\nhttps://docs.aws.amazon.com/sns/latest/dg/sns-dead-letter-queues.html\n\nThe dead letter queue for SNS are SQS."
      },
      {
        "date": "2022-09-05T13:10:00.000Z",
        "voteCount": 1,
        "content": "C makes sense."
      },
      {
        "date": "2022-09-08T07:22:00.000Z",
        "voteCount": 1,
        "content": "In fact might be D because one of the HTTP endpoints is not always available. \n\n\"You should customize your delivery policy according to your HTTP/S server's capacity. You can set the policy as a topic attribute or a subscription attribute. If all HTTP/S subscriptions in your topic target the same HTTP/S server, we recommend that you set the delivery policy as a topic attribute, so that it remains valid for all HTTP/S subscriptions in the topic. Otherwise, you must compose a delivery policy for each HTTP/S subscription in your topic, according the capacity of the HTTP/S server that the policy targets.\"\"\n\nhttps://docs.aws.amazon.com/sns/latest/dg/sns-message-delivery-retries.html"
      },
      {
        "date": "2022-09-18T04:56:00.000Z",
        "voteCount": 1,
        "content": "D is incorrect. the same link clearly mentions, When the delivery policy is exhausted, Amazon SNS stops retrying the delivery and discards the message\u2014unless a dead-letter queue is attached to the subscription. So B is correct."
      },
      {
        "date": "2022-09-05T12:01:00.000Z",
        "voteCount": 1,
        "content": "I vote for B"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 72,
    "url": "https://www.examtopics.com/discussions/amazon/view/28041-exam-aws-devops-engineer-professional-topic-1-question-72/",
    "body": "A company is deploying a container-based application using AWS CodeBuild. The Security team mandates that all containers are scanned for vulnerabilities prior to deployment using a password-protected endpoint. All sensitive information must be stored securely.<br>Which solution should be used to meet these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEncrypt the password using AWS KMS. Store the encrypted password in the buildspec.yml file as an environment variable under the variables mapping. Reference the environment variable to initiate scanning.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImport the password into an AWS CloudHSM key. Reference the CloudHSM key in the buildpec.yml file as an environment variable under the variables mapping. Reference the environment variable to initiate scanning.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the password in the AWS Systems Manager Parameter Store as a secure string. Add the Parameter Store key to the buildspec.yml file as an environment variable under the parameter-store mapping. Reference the environment variable to initiate scanning.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the AWS Encryption SDK to encrypt the password and embed in the buildspec.yml file as a variable under the secrets mapping. Attach a policy to CodeBuild to enable access to the required decryption key."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 3,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-10-02T16:19:00.000Z",
        "voteCount": 12,
        "content": "Ans C\nhttps://docs.aws.amazon.com/pt_br/codebuild/latest/userguide/build-spec-ref.html"
      },
      {
        "date": "2023-02-15T11:19:00.000Z",
        "voteCount": 2,
        "content": "A: \"Store the encrypted password in the buildspec.yml file ...\" , does it make any sense ?\nB: AWS CloudHSM lets you manage and access your keys on FIPS-validated hardware, protected with customer-owned, single-tenant HSM instances that run in your own Virtual Private Cloud (VPC), does not match this context. \nD: unreliable method"
      },
      {
        "date": "2023-01-21T09:22:00.000Z",
        "voteCount": 1,
        "content": "C is the right answer"
      },
      {
        "date": "2022-12-14T20:41:00.000Z",
        "voteCount": 1,
        "content": "It is C"
      },
      {
        "date": "2021-11-01T12:30:00.000Z",
        "voteCount": 1,
        "content": "C it is"
      },
      {
        "date": "2021-10-27T17:18:00.000Z",
        "voteCount": 1,
        "content": "Its completely C"
      },
      {
        "date": "2021-10-15T03:03:00.000Z",
        "voteCount": 3,
        "content": "I'll go with C"
      },
      {
        "date": "2021-10-09T10:46:00.000Z",
        "voteCount": 1,
        "content": "C is fine"
      },
      {
        "date": "2021-10-08T13:29:00.000Z",
        "voteCount": 2,
        "content": "C without a shadow of a doubt"
      },
      {
        "date": "2021-10-07T23:57:00.000Z",
        "voteCount": 1,
        "content": "C. \nStore password in System manager parameter store."
      },
      {
        "date": "2021-10-06T13:27:00.000Z",
        "voteCount": 1,
        "content": "I will go with C"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 73,
    "url": "https://www.examtopics.com/discussions/amazon/view/2748-exam-aws-devops-engineer-professional-topic-1-question-73/",
    "body": "An Engineering team manages a Node.js e-commerce application. The current environment consists of the following components:<br>\u2711 Amazon S3 buckets for storing content<br>\u2711 Amazon EC2 for the front-end web servers<br>\u2711 AWS Lambda for image processing<br>\u2711 Amazon DynamoDB for storing session-related data<br>The team expects a significant increase in traffic to the site. The application should handle the additional load without interruption. The team ran initial tests by adding new servers to the EC2 front-end to handle the larger load, but the instances took up to 20 minutes to become fully configured. The team wants to reduce this configuration time.<br>What changes will the Engineering team need to implement to make the solution the MOST resilient and highly available while meeting the expected increase in demand?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS OpsWorks to automatically configure each new EC2 instance as it is launched. Configure the EC2 instances by using an Auto Scaling group behind an Application Load Balancer across multiple Availability Zones. Implement Amazon DynamoDB Auto Scaling. Use Amazon Route 53 to point the application DNS record to the Application Load Balancer.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy a fleet of EC2 instances, doubling the current capacity, and place them behind an Application Load Balancer. Increase the Amazon DynamoDB read and write capacity units. Add an alias record that contains the Application Load Balancer endpoint to the existing Amazon Route 53 DNS record that points to the application.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Amazon CloudFront and have its origin point to Amazon S3 to host the web application. Implement Amazon DynamoDB Auto Scaling. Use Amazon Route 53 to point the application DNS record to the CloudFront DNS name.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Elastic Beanstalk with a custom AMI including all web components. Deploy the platform by using an Auto Scaling group behind an Application Load Balancer across multiple Availability Zones. Implement Amazon DynamoDB Auto Scaling. Use Amazon Route 53 to point the application DNS record to the Elastic Beanstalk load balancer.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-24T00:54:00.000Z",
        "voteCount": 17,
        "content": "In this case would choose D, because application is executed nodejs | I don't know how S3 would help in this case."
      },
      {
        "date": "2021-09-25T02:06:00.000Z",
        "voteCount": 7,
        "content": "Node.js is a key. Correct answer is D"
      },
      {
        "date": "2021-10-23T15:26:00.000Z",
        "voteCount": 2,
        "content": "Node.js is key because we want to get those Node.js configs into AMI."
      },
      {
        "date": "2023-05-24T17:09:00.000Z",
        "voteCount": 1,
        "content": "In terms of reducing configuration time, option A has the potential to be faster. This is because AWS OpsWorks can automatically configure each new EC2 instance as it is launched. OpsWorks provides a framework for defining and managing applications using Chef or Puppet, allowing for automated configuration and deployment.\n\nOn the other hand, option D involves using AWS Elastic Beanstalk with a custom AMI. While Elastic Beanstalk simplifies the deployment and management of applications, it may still require some initial configuration and setup time for the custom AMI. This could potentially result in longer configuration times compared to using OpsWorks for automated configuration."
      },
      {
        "date": "2023-02-09T07:28:00.000Z",
        "voteCount": 1,
        "content": "D is corretct"
      },
      {
        "date": "2023-01-21T09:27:00.000Z",
        "voteCount": 1,
        "content": "Pre-baked EC2 instances along with node.js points to Elastic BeanStalk."
      },
      {
        "date": "2023-01-21T09:28:00.000Z",
        "voteCount": 1,
        "content": "Answer is D"
      },
      {
        "date": "2021-11-03T20:54:00.000Z",
        "voteCount": 1,
        "content": "Go D -1"
      },
      {
        "date": "2021-11-02T13:48:00.000Z",
        "voteCount": 1,
        "content": "I think the answer is A, D is wrong because THERE IS NOTHING LIKE Elastic Beanstalk load balance"
      },
      {
        "date": "2021-11-02T14:19:00.000Z",
        "voteCount": 4,
        "content": "They refer to a LB created by Beanstalk"
      },
      {
        "date": "2021-11-02T02:13:00.000Z",
        "voteCount": 1,
        "content": "i'll go with D"
      },
      {
        "date": "2021-11-01T22:22:00.000Z",
        "voteCount": 6,
        "content": "I'll go with D)\nCreating AMI is always a good practice when instances took up to 20 minutes to become fully configured..."
      },
      {
        "date": "2021-10-31T18:32:00.000Z",
        "voteCount": 1,
        "content": "D. Use AWS Elastic Beanstalk with a custom AMI including all web components. Deploy the platform by using an Auto Scaling group behind an Application Load Balancer across multiple Availability Zones. Implement Amazon DynamoDB Auto Scaling. Use Amazon Route 53 to point the application DNS record to the Elastic Beanstalk load balancer."
      },
      {
        "date": "2021-10-31T11:26:00.000Z",
        "voteCount": 1,
        "content": "D is the answer."
      },
      {
        "date": "2021-10-25T15:04:00.000Z",
        "voteCount": 1,
        "content": "When your instance is taking time to boot, it means some configurations are going on probably through user data. May be like fetching some application artifacts from internet, and  installing them. This is good but not the best way to configure your instance during booting with user data. The best way is to create a custom IMAGE with preconfigured applications. Then when you lunch the custom image, it automatically come with existing applications, thus reducing time to boot. For instance, AMI from AWS are preconfigured with cloud watch log agent. You don\u2019t need to install it once you lunch AWS ima. But you have to manually when you lunched (using  sudo yum install after ssh into the lunched ec2 or through user data ) when the instance is booting."
      },
      {
        "date": "2021-10-25T07:49:00.000Z",
        "voteCount": 2,
        "content": "Yup, fully baked EC2 is the only option here (D)\nA would not speed things up, if anything, slow things down as chef would have to push that config in to the stock AMI"
      },
      {
        "date": "2021-10-23T03:56:00.000Z",
        "voteCount": 4,
        "content": "Right answer: D - custom AMI is key here it will reduce the provisioning time dramatically - main issue - and also multi-az and ALB are mentioned for resiliency and high-availability. \nWrong:\nA - OpsWorks could do it but \"automatically configure each new EC2 as it is launched\" would keep the slow start issue. \nB - Could be right but does not mention multi-AZ as well is based on manual changes instead of auto scaling. \nC - you need Ec2 due to Node.js server-side"
      },
      {
        "date": "2021-10-22T14:18:00.000Z",
        "voteCount": 3,
        "content": "I'll go with D"
      },
      {
        "date": "2021-10-20T19:47:00.000Z",
        "voteCount": 1,
        "content": "Its Option D"
      },
      {
        "date": "2021-10-18T05:29:00.000Z",
        "voteCount": 1,
        "content": "The answer should be D. Only with custom AMI we can reduce the configuration time. Because custom AMI that means all application/configuration was installed/built in-bulk/in ready and put into an AMI image and we don't need to configure anything after that."
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 74,
    "url": "https://www.examtopics.com/discussions/amazon/view/79093-exam-aws-devops-engineer-professional-topic-1-question-74/",
    "body": "A company's application development team uses Linux-based Amazon EC2 instances as bastion hosts. Inbound SSH access to the bastion hosts is restricted to specific IP addresses, as defined in the associated security groups. The company's security team wants to receive a notification if the security group rules are modified to allow SSH access from any IP address.<br>What should a DevOps engineer do to meet this requirement?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon EventBridge (Amazon CloudWatch Events) rule with a source of aws.cloudtrail and the event name AuthorizeSecurityGroupIngress. Define an Amazon Simple Notification Service (Amazon SNS) topic as the target.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Amazon GuardDuty and check the findings for security group in AWS Security Hub. Configure an Amazon EventBridge (Amazon CloudWatch Events) rule with a custom pattern that matches GuardDuty events with an output of NON_COMPLIANT. Define an Amazon Simple Notification Service (Amazon SNS) topic as the target.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Config rule by using the restricted-ssh managed rule to check whether security groups disallow unrestricted incoming SSH traffic. Configure automatic remediation to publish a message to an Amazon Simple Notification Service (Amazon SNS) topic.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Amazon Inspector. Include the Common Vulnerabilities and Exposures-1.1 rules package to check the security groups that are associated with the bastion hosts. Configure Amazon Inspector to publish a message to an Amazon Simple Notification Service (Amazon SNS) topic."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 17,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 11,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-06-03T09:45:00.000Z",
        "voteCount": 1,
        "content": "Answer is C \ncan use AWS Config rule like `restricted-ssh` to monitor security groups for unrestricted SSH access ensures that any non-compliant changes are detected immediately.\n\nits not A because , though Amazon EventBridge can be used to create a rule that triggers on specific events, the AuthorizeSecurityGroupIngress event does not provide information about the contents of the security group rule that was added, so it cannot be used to detect if SSH access from any IP address was allowed."
      },
      {
        "date": "2024-01-17T01:28:00.000Z",
        "voteCount": 1,
        "content": "Can't be A as any change to any port/ip will trigger it"
      },
      {
        "date": "2024-01-09T09:49:00.000Z",
        "voteCount": 1,
        "content": "Yes A will trigger event for every security group , Correct answer is C the rule will be non-compliant if cidr is 0.0.0.0\nhttps://docs.aws.amazon.com/config/latest/developerguide/restricted-ssh.html"
      },
      {
        "date": "2023-08-10T04:05:00.000Z",
        "voteCount": 1,
        "content": "I vote for A: With option C, the its only recording ipv4 0.0.0.0 what about ipv6 changes\nhttps://docs.aws.amazon.com/whitepapers/latest/ipv6-on-aws/ipv6-security-and-monitoring-considerations.html\n\nOption C only applies to ipv4 so any security group changes to ipv6 CIDR will not be recorded. (The rule is COMPLIANT when IP addresses of the incoming SSH traffic in the security groups are restricted (CIDR other than 0.0.0.0/0). This rule applies only to IPv4)\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/authorizing-access-to-an-instance.html"
      },
      {
        "date": "2023-04-18T19:22:00.000Z",
        "voteCount": 1,
        "content": "I agree with A after reading https://aws.amazon.com/premiumsupport/knowledge-center/monitor-security-group-changes-ec2/ and also interpreting the question as if ANY IP is added, not 0.0.0.0/0 like some suggested."
      },
      {
        "date": "2023-04-13T01:03:00.000Z",
        "voteCount": 1,
        "content": "I think that the core sentence is this - \"Inbound SSH access to the bastion hosts is restricted to specific IP addresses\". That means that we don't want change the current addresses at all or add additional ones. In other words we must to be notified for ANY IP changes, not only from 0.0.0.0/0."
      },
      {
        "date": "2023-03-15T14:33:00.000Z",
        "voteCount": 3,
        "content": "A for the win."
      },
      {
        "date": "2023-02-06T02:42:00.000Z",
        "voteCount": 3,
        "content": "This sentence is key. \"if the security group rules are modified to allow SSH access from any IP address. So I'd choose C. attention to Any IP address.\nC is \"restricted-ssh managed rule\" means that you will verify if your security group dont have setup 0.0.0.0/0 CIDR"
      },
      {
        "date": "2023-02-03T20:18:00.000Z",
        "voteCount": 2,
        "content": "C is the correct answer. A assumes that CloudTrail trail is created to send Cloudtrail events to CloudWatch logs. The link below defending A is using the event source as EC2 and event type as API call using CloudTrail but the option A talks about directly using CloudTrail as the source which is only possible if a trail is created to send events to CloudWatch Logs."
      },
      {
        "date": "2023-01-31T15:15:00.000Z",
        "voteCount": 4,
        "content": "I go with c. \nBetween A and C, I select C because A will send an alert on all the Security group changes"
      },
      {
        "date": "2023-01-29T06:23:00.000Z",
        "voteCount": 3,
        "content": "https://aws.amazon.com/premiumsupport/knowledge-center/monitor-security-group-changes-ec2/"
      },
      {
        "date": "2023-01-25T05:21:00.000Z",
        "voteCount": 3,
        "content": "Restricted SSH rule seems good fit"
      },
      {
        "date": "2023-01-21T09:56:00.000Z",
        "voteCount": 2,
        "content": "Answer is C. If you read the question properly it says change to any IP address which means 0.0.0.0/0 CIDR. So C is the best answer as the config rule will detect changes in the security group from restricted IP CIDR to any IP CIDR."
      },
      {
        "date": "2023-02-02T11:06:00.000Z",
        "voteCount": 2,
        "content": "'Any IP Address' doesn't mean 0.0.0.0/0 CIDR. It simply means ANY IP address. So if (10.10.0.10/32) was allowed ingress, the security team needs to get notified. Not only 0.0.0.0/0. So answer A would satisfy this"
      },
      {
        "date": "2023-01-05T10:49:00.000Z",
        "voteCount": 2,
        "content": "The wording in the answers are confusing. I'll go with C though. The requirement is \"The company's security team wants to receive a notification if the security group rules are modified to allow SSH access from any IP address.\" \nAWS Config is the right service to look for changes to baseline configurations."
      },
      {
        "date": "2023-01-01T18:36:00.000Z",
        "voteCount": 2,
        "content": "C is not the answer because restricted-ssh checks if the incoming SSH traffic for the security groups is accessible. The rule is COMPLIANT when IP addresses of the incoming SSH traffic in the security groups are restricted (CIDR other than 0.0.0.0/0). This rule applies only to IPv4."
      },
      {
        "date": "2022-12-27T06:29:00.000Z",
        "voteCount": 2,
        "content": "C is the best option"
      },
      {
        "date": "2022-12-26T07:56:00.000Z",
        "voteCount": 1,
        "content": "I go with Choice A \n\nREF : https://aws.amazon.com/premiumsupport/knowledge-center/monitor-security-group-changes-ec2/"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 75,
    "url": "https://www.examtopics.com/discussions/amazon/view/79103-exam-aws-devops-engineer-professional-topic-1-question-75/",
    "body": "A company is using AWS Organizations to create separate AWS accounts for each of its departments. The company needs to automate the following tasks:<br>\u2711 Update the Linux AMIs with new patches periodically and generate a golden image<br>\u2711 Install a new version of Chef agents in the golden image, if available<br>\u2711 Provide the newly generated AMIs to the department's accounts<br>Which solution meets these requirements with the LEAST management overhead?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite a script to launch an Amazon EC2 instance from the previous golden image. Apply the patch updates. Install the new version of the Chef agent, generate a new golden image, and then modify the AMI permissions to share only the new image with the department's accounts.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon EC2 Image Builder to create an image pipeline that consists of the base Linux AMI and components to install the Chef agent. Use AWS Resource Access Manager to share EC2 Image Builder images with the department's accounts.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an AWS Systems Manager Automation runbook to update the Linux AMI by using the previous image. Provide the URL for the script that will update the Chef agent. Use AWS Organizations to replace the previous golden image in the department's accounts.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon EC2 Image Builder to create an image pipeline that consists of the base Linux AMI and components to install the Chef agent. Create a parameter in AWS Systems Manager Parameter Store to store the new AMI ID that can be referenced by the department's accounts."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 20,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-28T08:05:00.000Z",
        "voteCount": 6,
        "content": "B looks correct:\nhttps://docs.aws.amazon.com/imagebuilder/latest/userguide/manage-shared-resources.html"
      },
      {
        "date": "2022-09-13T23:46:00.000Z",
        "voteCount": 6,
        "content": "EC2 Image Builder integrates with AWS Resource Access Manager (AWS RAM) to allow you to share certain resources with any AWS account or through AWS Organizations. EC2 Image Builder resources that can be shared are:\n\nComponents\n\nImages\n\nRecipes\n\nhttps://docs.aws.amazon.com/imagebuilder/latest/userguide/manage-shared-resources.html"
      },
      {
        "date": "2023-02-19T08:42:00.000Z",
        "voteCount": 2,
        "content": "A satisfies the demanding of \"LEAST management overhead\"."
      },
      {
        "date": "2023-03-05T12:18:00.000Z",
        "voteCount": 2,
        "content": "How does a custom script satisfy the \"least management overhead\" scenario? This does not make any sense."
      },
      {
        "date": "2023-01-21T19:01:00.000Z",
        "voteCount": 2,
        "content": "Correct answer us B. RAM is used to share Image builder resources as well as images created by the ImageBuilder pipeline."
      },
      {
        "date": "2022-12-14T20:55:00.000Z",
        "voteCount": 5,
        "content": "People struggling with B and D. I would go for B. Though D seems a very good option the answer does not mention anything about how to share the Parameter Store with other accounts."
      },
      {
        "date": "2022-11-20T03:23:00.000Z",
        "voteCount": 3,
        "content": "I would go with D as it does not require anyone to change AMI Id after it has been updated. Having tons of CloudFormation templates, parameter store approach looks to have least overhead"
      },
      {
        "date": "2022-11-23T08:56:00.000Z",
        "voteCount": 2,
        "content": "I like D as well but I'm not sure how the other accounts would access the AMI's if they aren't shared in some way. For that reason I'm leaning toward B for now. But if D mentioned the sharing I think D would be a better overall solution."
      },
      {
        "date": "2022-09-01T11:00:00.000Z",
        "voteCount": 3,
        "content": "B - EC2 Image Builder allows you to create a pipeline which turns into least management overhead. https://docs.aws.amazon.com/imagebuilder/latest/userguide/what-is-image-builder.html\n\"The images you build are created in your account and you can configure them for operating system patches on an ongoing basis.\"\n\nRAM helps you to share accross organizations and accounts. https://docs.aws.amazon.com/imagebuilder/latest/userguide/manage-shared-resources.html"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 76,
    "url": "https://www.examtopics.com/discussions/amazon/view/79160-exam-aws-devops-engineer-professional-topic-1-question-76/",
    "body": "A company has an application that runs on 12 Amazon EC2 instances. The instances run in an Amazon EC2 Auto Scaling group across three Availability Zones.<br>On a typical day each EC2 instance has 30% CPU utilization during business hours and 10% CPU utilization after business hours. The CPU utilization increases suddenly in the first few minutes of business hours each day. Other increases in CPU utilization are gradual. A DevOps engineer needs to optimize costs while maintaining or improving the application's reliability.<br>Which solution meets these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a target tracking scaling policy that is based on the Auto Scaling group's average CPU utilization, and set a target of 75%. Create a scheduled action for the Auto Scaling group to adjust the desired capacity to six instances just before business hours begin.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the Auto Scaling group with two scheduled actions for Amazon EC2 Auto Scaling. Configure one action to start nine EC2 instances at the start of business hours. Configure the other action to stop nine instances at the end of business hours.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange to an AWS Application Auto Scaling group. Configure a target tracking scaling policy that is based on the Auto Scaling group's average CPU utilization, and set a target of 75%. Create a scheduled action for the Auto Scaling group to adjust the minimum number of instances to three instances at the end of business hours and to reset the number to six instances before business hours begin.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange to an AWS Application Auto Scaling group. Configure a target tracking scaling policy that is based on the Auto Scaling group's average CPU utilization, and set a target of 75%. Create a scheduled action to terminate nine instances each evening at the end of business hours."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 15,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-09-22T18:26:00.000Z",
        "voteCount": 11,
        "content": "C &amp; D are wrong. Application Auto Scaling isn't for EC2 : \nhttps://docs.aws.amazon.com/autoscaling/application/userguide/what-is-application-auto-scaling.html\n\nB can not be a good option, as it just starts and terminates EC2 instances regardless of the real CPU utilization. \n\nA is the most appropriate"
      },
      {
        "date": "2022-12-28T01:12:00.000Z",
        "voteCount": 1,
        "content": "Application autoscaling can also be applied to EC2 instances, it only has a broader application, compared to EC2 Autoscaling. But the catch here is we want a solution that address both during working hrs and after working hrs (especially when there is low traffic). Option A only address during working hrs."
      },
      {
        "date": "2022-10-24T10:22:00.000Z",
        "voteCount": 7,
        "content": "Answer is A,\n AWS Application Auto Scaling group isn't provisioned for EC2, so C&amp;D are wrong."
      },
      {
        "date": "2024-05-25T06:04:00.000Z",
        "voteCount": 1,
        "content": "C is Correct"
      },
      {
        "date": "2023-03-17T23:46:00.000Z",
        "voteCount": 1,
        "content": "D is Right."
      },
      {
        "date": "2023-02-11T06:28:00.000Z",
        "voteCount": 2,
        "content": "A - correct\nB - wrong - you can not add or remove X instances. You must explicitly specify either desired min or max.\nC &amp; D - wrong - we are scaling EC2 not other AWS services"
      },
      {
        "date": "2023-01-24T16:49:00.000Z",
        "voteCount": 2,
        "content": "EC2 is not one of the services that can use Application Auto scaling.\nhttps://docs.aws.amazon.com/autoscaling/application/userguide/integrated-services-list.html\n\nso A seems to be right"
      },
      {
        "date": "2023-01-21T19:19:00.000Z",
        "voteCount": 1,
        "content": "Answer is A. Target tracking scaling policy us gong to be more coat effective than just scheduled scaling action. AWS application auto scaling group is used beyond EC2 so  C and D are wrong."
      },
      {
        "date": "2023-01-04T15:40:00.000Z",
        "voteCount": 1,
        "content": "I'll go with A. Application Autoscaling is not for EC2s"
      },
      {
        "date": "2022-12-14T21:57:00.000Z",
        "voteCount": 2,
        "content": "The Answer is actually A. For folks saying C is the answer. Application Autoscaling and EC2 Autoscaling are two different things."
      },
      {
        "date": "2022-10-24T04:59:00.000Z",
        "voteCount": 1,
        "content": "Answer is \u201dC\u201d !!"
      },
      {
        "date": "2022-09-27T19:25:00.000Z",
        "voteCount": 1,
        "content": "I will go with C."
      },
      {
        "date": "2022-09-05T01:56:00.000Z",
        "voteCount": 2,
        "content": "CCCCCCCC"
      },
      {
        "date": "2022-09-01T13:30:00.000Z",
        "voteCount": 1,
        "content": "C - https://www.examtopics.com/discussions/amazon/view/2752-exam-aws-devops-engineer-professional-topic-1-question-71/"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 77,
    "url": "https://www.examtopics.com/discussions/amazon/view/79465-exam-aws-devops-engineer-professional-topic-1-question-77/",
    "body": "A development team is building a full-stack serverless web application. The serverless application will consist of a backend REST API and a front end that is built with a single-page application (SPA) framework.<br>The team wants to use a Git-based workflow to develop and deploy the application. The team has created an AWS CodeCommit repository to store the application code. The team wants to use multiple development branches to test new features. In addition, the team wants to ensure that code changes on the development branches are deployed to the different development environments. Code changes to the main branches must be released automatically to production.<br>The development deployments must be available as a subdomain of the main application website, which is hosted in an Amazon Route 53 public hosted zone.<br>What should a DevOps engineer do to meet these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an application in the AWS Amplify console, and connect the CodeCommit repository. Create a feature branch deployment for each of the environments. Connect the Route 53 domain to the application. Activate the automatic creation of subdomains.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a single AWS CodePipeline pipeline that uses the CodeCommit repository as a source. Configure the pipeline so that it deploys to different environments based on the changed branch. Create an AWS Lambda function that creates a new subdomain based on the source branch name. Invoke the Lambda function in the deployment workflow.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an application in AWS Elastic Beanstalk that uses the CodeCommit repository as a source. Configure Elastic Beanstalk so that it creates a new application environment based on the changed branch. Connect the Route 53 domain to the application. Activate the automatic creation of subdomains.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate multiple AWS CodePipeline pipelines that use the CodeCommit repository as a source. Configure each pipeline so that it deploys to a specific environment based on the configured branch. Configure an AWS CodeDeploy step in the pipeline to deploy the application components and to create the Route 53 public hosted zone."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 12,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-09-27T15:38:00.000Z",
        "voteCount": 6,
        "content": "Amplify leverages Git branches to create new deployments every time a developer connects a new branch in their repository. After connecting your first branch, you can create a new feature branch deployments. Typically the main branch tracks release code and is your production branch. The develop branch is used as an integration branch to test new features. This enables beta testers to test unreleased features on the develop branch deployment, without affecting any of the production end users on the main branch deployment.\nYou can connect a custom domain to an app that you\u2019ve deployed with Amplify Hosting.\nhttps://docs.aws.amazon.com/amplify/latest/userguide/welcome.html"
      },
      {
        "date": "2023-11-24T06:28:00.000Z",
        "voteCount": 1,
        "content": "how can we have multiple dev env's in A? I guess it's B"
      },
      {
        "date": "2023-01-21T20:22:00.000Z",
        "voteCount": 2,
        "content": "Answer is A. Amplify hosting enables all the features requested."
      },
      {
        "date": "2022-12-02T07:21:00.000Z",
        "voteCount": 3,
        "content": "https://www.youtube.com/watch?v=AmJps1bYgs0"
      },
      {
        "date": "2022-09-22T18:56:00.000Z",
        "voteCount": 3,
        "content": "A -&gt; AWS Amplify \n\nhttps://aws.amazon.com/blogs/mobile/complete-guide-to-full-stack-ci-cd-workflows-with-aws-amplify/"
      },
      {
        "date": "2022-09-02T09:29:00.000Z",
        "voteCount": 4,
        "content": "This looks like a case for AWS Amplify. So A.\nhttps://docs.aws.amazon.com/amplify/latest/userguide/welcome.html\nhttps://docs.aws.amazon.com/amplify/latest/userguide/to-set-up-automatic-subdomains-for-a-Route-53-custom-domain.html\nhttps://aws.amazon.com/blogs/mobile/complete-guide-to-full-stack-ci-cd-workflows-with-aws-amplify/"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 78,
    "url": "https://www.examtopics.com/discussions/amazon/view/79173-exam-aws-devops-engineer-professional-topic-1-question-78/",
    "body": "A company is using AWS CodePipeline to deploy an application. According to a new guideline, a member of the company's security team must sign off on any application changes before the changes are deployed into production. The approval must be recorded and retained.<br>Which combination of actions will meet these requirements? (Choose two.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure CodePipeline to write actions to Amazon CloudWatch Logs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure CodePipeline to write actions to an Amazon S3 bucket at the end of each pipeline stage.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS CloudTrail trail to deliver logs to Amazon S3.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a CodePipeline custom action to invoke an AWS Lambda function for approval. Create a policy that gives the security team access to manage CodePipeline custom actions.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a CodePipeline manual approval action before the deployment step. Create a policy that grants the security team access to approve manual approval stages.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "CE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "CE",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-01-21T20:35:00.000Z",
        "voteCount": 2,
        "content": "Answer is C abd E"
      },
      {
        "date": "2022-09-22T19:01:00.000Z",
        "voteCount": 2,
        "content": "C &amp; E \nhttps://stelligent.com/2019/06/11/aws-codepipeline-approval-gate-tracking/"
      },
      {
        "date": "2022-09-05T01:58:00.000Z",
        "voteCount": 2,
        "content": "CCCCCCEEEEEE"
      },
      {
        "date": "2022-09-01T14:11:00.000Z",
        "voteCount": 4,
        "content": "CE - https://www.examtopics.com/discussions/amazon/view/47954-exam-aws-devops-engineer-professional-topic-1-question-254/"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 79,
    "url": "https://www.examtopics.com/discussions/amazon/view/79177-exam-aws-devops-engineer-professional-topic-1-question-79/",
    "body": "A company manages a web application that runs on Amazon EC2 instances behind an Application Load Balancer (ALB). The EC2 instances run in an Auto<br>Scaling group across multiple Availability Zones. The application uses an Amazon RDS for MySQL DB instance to store the data. The company has configured<br>Amazon Route 53 with an alias record that points to the ALB.<br>Anew company guideline requires a geographically isolated disaster recovery (DR) site with an RTO of 4 hours and an RPO of 15 minutes.<br>Which DR strategy will meet these requirements with the LEAST change to the application stack?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLaunch a replica environment of everything except Amazon RDS in a different Availability Zone. Create an RDS read replica in the new Availability Zone, and configure the new stack to point to the local RDS DB instance. Add the new stack to the Route 53 record set by using a health check to configure a failover routing policy.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLaunch a replica environment of everything except Amazon RDS in a different AWS Region. Create an RDS read replica in the new Region, and configure the new stack to point to the local RDS DB instance. Add the new stack to the Route 53 record set by using a health check to configure a latency routing policy.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLaunch a replica environment of everything except Amazon RDS in a different AWS Region. In the event of an outage, copy and restore the latest RDS snapshot from the primary Region to the DR Region. Adjust the Route 53 record set to point to the ALB in the DR Region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLaunch a replica environment of everything except Amazon RDS in a different AWS Region. Create an RDS read replica in the new Region, and configure the new environment to point to the local RDS DB instance. Add the new stack to the Route 53 record set by using a health check to configure a failover routing policy. In the event of an outage, promote the read replica to primary.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-09T12:27:00.000Z",
        "voteCount": 1,
        "content": "If there is a region outage, then  backups may not be even accessible to restore the database, unless cross region backups are configured on the RDS instance. \n\nC - Doesn't state this. RPO and RTO cannot be achieved due to the above scenario.\n\nD - is least change to stack. RPO and RTO are guaranteed even with an AWS Region outage let alone customer specific regional outage."
      },
      {
        "date": "2023-11-28T08:12:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2023-05-03T04:33:00.000Z",
        "voteCount": 1,
        "content": "D is more suitable. Putting the stress on Failover concept for disaster recovery."
      },
      {
        "date": "2023-03-08T06:54:00.000Z",
        "voteCount": 1,
        "content": "D looks good."
      },
      {
        "date": "2023-02-20T17:13:00.000Z",
        "voteCount": 2,
        "content": "D is correct.\nCreating snapshot after outage is too late. RPO is the time between snapshots that DR can tolerate"
      },
      {
        "date": "2023-02-20T17:16:00.000Z",
        "voteCount": 1,
        "content": "Latest snapshot might be older than 15 minutes. Answere doesn't say anything about the last time snapshot has been created. it just say latest snapshot"
      },
      {
        "date": "2023-02-09T07:21:00.000Z",
        "voteCount": 1,
        "content": "Answering C is plain stupid. You can achieve the 15 mins RPO with C. D is correct answer."
      },
      {
        "date": "2023-02-08T07:32:00.000Z",
        "voteCount": 2,
        "content": "A must be wrong because of \"in a different Availability Zone\"\nB is eliminated because of \"latency routing policy\".\nBetween C and D, the correct answer shows C, I do not understand.\nSome people say that \"RTO of 4 hours and RPO of 15 minutes\" does not require \"read replica\".  However, it's not guaranteed that the steps in C can be done in the limited time length."
      },
      {
        "date": "2023-01-21T20:37:00.000Z",
        "voteCount": 1,
        "content": "D is the correct answer"
      },
      {
        "date": "2022-09-13T05:07:00.000Z",
        "voteCount": 2,
        "content": "D is the asnwer!"
      },
      {
        "date": "2022-09-05T01:58:00.000Z",
        "voteCount": 2,
        "content": "DDDDDDDD"
      },
      {
        "date": "2022-09-01T14:26:00.000Z",
        "voteCount": 2,
        "content": "D - https://www.examtopics.com/discussions/amazon/view/2840-exam-aws-devops-engineer-professional-topic-1-question-36/"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 80,
    "url": "https://www.examtopics.com/discussions/amazon/view/79389-exam-aws-devops-engineer-professional-topic-1-question-80/",
    "body": "A DevOps engineer wants to implement an automated response that will occur if AWS Trusted Advisor detects an IAM access key in a public source code repository. The automated response must delete the exposed access key and must notify the security team.<br>Which solution will meet these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Lambda function to delete the 1AM access key. Configure AWS CloudTrail logs to stream to Amazon CloudWatch Logs. Create a CloudWatch Logs metric filter for the AWS_RISK_CREDENTIALS_EXPOSED event with two actions. First, run the Lambda function. Second, use Amazon Simple Notification Service (Amazon SNS) to send a notification to the security team.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Lambda function to delete the IAM access key. Create an AWS Config rule for changes to \"aws.trustedadvisor\" and the \"Exposed Access Keys\" status with two actions. First, run the Lambda function. Second, use Amazon Simple Notification Service (Amazon SNS) to send a notification to the security team.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Lambda function that deletes the IAM access key and then uses Amazon Simple Notification Service (Amazon SNS) to notify the security team. Create an AWS Personal Health Dashboard rule for the AWS_RISK_CREDENTIALS_EXPOSED event. Set the target of the Personal Health Dashboard rule to the ARN of the Lambda function.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Lambda function that deletes the IAM access key. Create an Amazon EventBridge (Amazon CloudWatch Events) rule with an \"aws.trustedadvisor\" event source and the \"Exposed Access Keys\" status. Set the EventBridge (CloudWatch Events) rule to target the Lambda function and an Amazon Simple Notification Service (Amazon SNS) topic that notifies the security team.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 12,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-04-13T06:10:00.000Z",
        "voteCount": 1,
        "content": "For option C we need Step Function to trigger the Lambda function, as can be seen in the provided links bellow. Step Functions are not mentioned in answer C, so the only valid option would be D"
      },
      {
        "date": "2023-03-22T04:26:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/EventTypes.html"
      },
      {
        "date": "2023-03-06T12:37:00.000Z",
        "voteCount": 2,
        "content": "A -  there are no actions for Cloudwatch Logs metric filter\nB - AWS Config is not relevant for this use case as it monitors resources status/compliance\nC - There is no such thing Health dashboard event rule, need to use cloudwatch event rule\nD - Correct"
      },
      {
        "date": "2023-03-02T14:50:00.000Z",
        "voteCount": 2,
        "content": "D: EventBridge Pattern\n\n{\n  \"source\": [\"aws.health\"],\n  \"detail-type\": [\"AWS Health Event\"],\n  \"detail\": {\n    \"service\": [\"RISK\"],\n    \"eventTypeCategory\": [\"issue\"],\n    \"eventTypeCode\": [\"AWS_RISK_CREDENTIALS_EXPOSED\", \"AWS_RISK_CREDENTIALS_COMPROMISED\", \"AWS_RISK_CREDENTIALS_EXPOSURE_SUSPECTED\", \"AWS_RISK_CREDENTIALS_COMPROMISE_SUSPECTED\"]\n  }\n}"
      },
      {
        "date": "2023-02-26T23:54:00.000Z",
        "voteCount": 1,
        "content": "There are two issues with C\nIt should use event bridge which is not mentioned\nIt is aws.health event not the health dashboard event"
      },
      {
        "date": "2023-02-24T19:18:00.000Z",
        "voteCount": 1,
        "content": "I correct myself, it's not C because the event should be coming from aws health events. not health dashboard events"
      },
      {
        "date": "2023-02-24T19:23:00.000Z",
        "voteCount": 1,
        "content": "pls remove this comment, thank you.\nI recorrect myself, the answer is C"
      },
      {
        "date": "2023-02-17T14:33:00.000Z",
        "voteCount": 3,
        "content": "Looks to me to be C.\nhttps://github.com/aws/aws-health-tools/blob/master/automated-actions/AWS_RISK_CREDENTIALS_EXPOSED/README.md"
      },
      {
        "date": "2023-02-17T14:41:00.000Z",
        "voteCount": 1,
        "content": "Definitely C"
      },
      {
        "date": "2023-02-20T17:26:00.000Z",
        "voteCount": 1,
        "content": "Eventbridge is also missing in C"
      },
      {
        "date": "2023-02-21T19:40:00.000Z",
        "voteCount": 1,
        "content": "It\u2019s integrated in the service. You have a tab for it"
      },
      {
        "date": "2023-01-21T21:02:00.000Z",
        "voteCount": 2,
        "content": "D is correct. C  is not because AWS health does not work with Trusted Advisor and the question talks about detecting credentials using AWS trusted advisor."
      },
      {
        "date": "2023-01-05T15:32:00.000Z",
        "voteCount": 1,
        "content": "D for sure\nhttps://github.com/aws/Trusted-Advisor-Tools/blob/master/ExposedAccessKeys/README.md"
      },
      {
        "date": "2022-12-03T10:18:00.000Z",
        "voteCount": 4,
        "content": "I was initially leaning toward C. But the wording says \"Create an AWS Personal Health Dashboard rule\". The rule is technically created in EventBridge. For that reason I'd go with D."
      },
      {
        "date": "2022-11-11T04:35:00.000Z",
        "voteCount": 2,
        "content": "https://aws.amazon.com/blogs/compute/automate-your-it-operations-using-aws-step-functions-and-amazon-cloudwatch-events/"
      },
      {
        "date": "2022-09-27T22:45:00.000Z",
        "voteCount": 4,
        "content": "I will go with C.\nPublic access key &lt;--&gt; AWS Health &lt;--&gt; Event Bridge &lt;--&gt; Lambda Func.\n\nRefer: https://aws.amazon.com/blogs/compute/automate-your-it-operations-using-aws-step-functions-and-amazon-cloudwatch-events/"
      },
      {
        "date": "2022-09-22T19:32:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2022-09-11T07:00:00.000Z",
        "voteCount": 1,
        "content": "C is true\n\nhttps://aws.amazon.com/blogs/compute/automate-your-it-operations-using-aws-step-functions-and-amazon-cloudwatch-events/"
      },
      {
        "date": "2022-09-02T05:45:00.000Z",
        "voteCount": 1,
        "content": "D - Seems to be the right one. But wording around Exposed Access Key status is kinda wrong. Status should be error and check-name should be Exposed Access Key.\nhttps://github.com/aws/Trusted-Advisor-Tools/tree/master/ExposedAccessKeys\n\nC is also doable, but I don't think this is the case.\n\nhttps://www.examtopics.com/discussions/amazon/view/4893-exam-aws-devops-engineer-professional-topic-1-question-64/"
      },
      {
        "date": "2023-02-17T14:42:00.000Z",
        "voteCount": 1,
        "content": "it's C. Literal copy paste\nhttps://github.com/aws/aws-health-tools/blob/master/automated-actions/AWS_RISK_CREDENTIALS_EXPOSED/README.md"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 81,
    "url": "https://www.examtopics.com/discussions/amazon/view/79394-exam-aws-devops-engineer-professional-topic-1-question-81/",
    "body": "A DevOps engineer wants to implement an automated response that will occur if AWS Trusted Advisor detects an IAM access key in a public source code repository. The automated response must delete the exposed access key and must notify the security team.<br>Which solution will meet these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Lambda function to delete the IAM access key. Configure AWS CloudTrail logs to stream to Amazon CloudWatch Logs. Create a CloudWatch Logs metric filter for the AWS_RISK_CREDENTIALS_EXPOSED event with two actions. First, run the Lambda function. Second, use Amazon Simple Notification Service (Amazon SNS) to send a notification to the security team.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Lambda function to delete the IAM access key. Create an AWS Config rule for changes to \"aws.trustedadvisor\" and the \"Exposed Access Keys\" status with two actions. First, run the Lambda function. Second, use Amazon Simple Notification Service (Amazon SNS) to send a notification to the security team.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Lambda function that deletes the IAM access key and then uses Amazon Simple Notification Service (Amazon SNS) to notify the security team. Create an AWS Personal Health Dashboard rule for the AWS_RISK_CREDENTIALS_EXPOSED event. Set the target of the Personal Health Dashboard rule to the ARN of the Lambda function.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Lambda function that deletes the IAM access key. Create an Amazon EventBridge (Amazon CloudWatch Events) rule with an \"aws.trustedadvisor\" event source and the \"Exposed Access Keys\" status. Set the EventBridge (CloudWatch Events) rule to target the Lambda function and an Amazon Simple Notification Service (Amazon SNS) topic that notifies the security team.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 7,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-05-03T04:50:00.000Z",
        "voteCount": 1,
        "content": "D is more suitable answer to the question"
      },
      {
        "date": "2023-03-06T12:39:00.000Z",
        "voteCount": 1,
        "content": "D - See question 80\nWhy not C? There is no such thing health dashboard event rule"
      },
      {
        "date": "2023-01-22T05:47:00.000Z",
        "voteCount": 2,
        "content": "D is the correct answer."
      },
      {
        "date": "2022-12-26T11:43:00.000Z",
        "voteCount": 2,
        "content": "C for sure"
      },
      {
        "date": "2022-12-04T07:47:00.000Z",
        "voteCount": 1,
        "content": "C\nhttps://github.com/aws/Trusted-Advisor-Tools/blob/master/ExposedAccessKeys/README.md"
      },
      {
        "date": "2023-03-04T08:50:00.000Z",
        "voteCount": 1,
        "content": "Isnt't this solution from this link matching option D? Option C is about aws health and not trusted advisor. Correct?"
      },
      {
        "date": "2022-11-27T22:09:00.000Z",
        "voteCount": 1,
        "content": "c\nhttps://docs.aws.amazon.com/awssupport/latest/user/security-checks.html#exposed-access-keys"
      },
      {
        "date": "2022-11-11T04:35:00.000Z",
        "voteCount": 2,
        "content": "https://aws.amazon.com/blogs/compute/automate-your-it-operations-using-aws-step-functions-and-amazon-cloudwatch-events/"
      },
      {
        "date": "2022-09-27T22:58:00.000Z",
        "voteCount": 2,
        "content": "I will go with C.\nPublic access key &lt;--&gt; AWS Health &lt;--&gt; Event Bridge &lt;--&gt; Lambda Func.\n\nRefer: https://aws.amazon.com/blogs/compute/automate-your-it-operations-using-aws-step-functions-and-amazon-cloudwatch-events/"
      },
      {
        "date": "2022-09-25T18:42:00.000Z",
        "voteCount": 4,
        "content": "80/81 share the same answer D."
      },
      {
        "date": "2022-09-22T19:36:00.000Z",
        "voteCount": 1,
        "content": "\"D\" is correct"
      },
      {
        "date": "2022-09-02T05:54:00.000Z",
        "voteCount": 2,
        "content": "80 and 81 are the same question, but they have diff answer."
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 82,
    "url": "https://www.examtopics.com/discussions/amazon/view/80264-exam-aws-devops-engineer-professional-topic-1-question-82/",
    "body": "A company hosts an application in North America. The application uses an Amazon Aurora PostgreSQL DB cluster. A team of analysts in Europe generates real- time reports by using the DB cluster. The analysts must have access to the most up-to-date data. A DevOps engineer discovers that the generation of reports is much slower for users in Europe than for users in North America.<br>What should the DevOps engineer do to resolve this issue?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon DynamoDB table in Europe. Use DynamoDB Accelerator (DAX) to configure replication between the DB cluster and the DynamoDB table. Configure the users' machines to point to the DynamoDB table in Europe.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate cross-Region Aurora Replicas in North America, and activate synchronous replication. Configure the users' machines to point to the Aurora reader endpoint in North America.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Aurora global database. Use the existing DB cluster as the primary cluster, and add a secondary cluster in an AWS Region in Europe. Configure the users' machines to point to the Aurora reader endpoint in Europe.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon DynamoDB global tables in an AWS Region in Europe. Set up continuous replication between the DB cluster and the DynamoDB table by using AWS Database Migration Service (AWS DMS). Configure the users' machines to point to the DynamoDB table in Europe."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-12-10T18:53:00.000Z",
        "voteCount": 6,
        "content": "A &amp; D is incorrect - utilizes DynamoDB which is key value storage, whereas PostgreSQL is relational.\nB is incorrect - \"Aurora PostgreSQL DB clusters don't support Aurora Replicas in different AWS Regions,\" https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraPostgreSQL.Replication.html\nThis leaves C as correct answer"
      },
      {
        "date": "2023-04-19T18:00:00.000Z",
        "voteCount": 1,
        "content": "https://aws.amazon.com/rds/aurora/global-database/"
      },
      {
        "date": "2023-02-08T08:20:00.000Z",
        "voteCount": 2,
        "content": "B will be even better if the users are connected to the Europe region."
      },
      {
        "date": "2023-01-22T05:52:00.000Z",
        "voteCount": 1,
        "content": "C is the right answer"
      },
      {
        "date": "2022-09-25T18:48:00.000Z",
        "voteCount": 2,
        "content": "B. cross-Region Aurora Replicas seems good, but user still points to NA which is wrong.\nThat leaves C as the choice."
      },
      {
        "date": "2023-03-04T08:57:00.000Z",
        "voteCount": 2,
        "content": "B is wrong because of one more reason - cross-Region Aurora Replicas are not supported for postgres. So C is the only valid option."
      },
      {
        "date": "2022-09-05T19:26:00.000Z",
        "voteCount": 1,
        "content": "C: A &amp; D are too much work, B won't help Europe"
      },
      {
        "date": "2022-09-05T13:39:00.000Z",
        "voteCount": 1,
        "content": "C is a good choice"
      },
      {
        "date": "2022-09-05T02:02:00.000Z",
        "voteCount": 2,
        "content": "CCCCCCC"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 83,
    "url": "https://www.examtopics.com/discussions/amazon/view/89500-exam-aws-devops-engineer-professional-topic-1-question-83/",
    "body": "A consulting company was hired to assess security vulnerabilities within a client company's application and propose a plan to remediate all identified issues. The architecture is identified as follows: Amazon S3 storage for content, an Auto Scaling group of Amazon EC2 instances behind an Elastic Load Balancer with attached Amazon EBS storage, and an Amazon RDS MySQL database. There are also several AWS Lambda functions that communicate directly with the RDS database using connection string statements in the code.<br><br>The consultants identified the top security threat as follows: the application is not meeting its requirement to have encryption at rest.<br><br>What solution will address this issue with the LEAST operational overhead and will provide monitoring for potential future violations?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable SSE encryption on the S3 buckets and RDS database. Enable OS-based encryption of data on EBS volumes. Configure Amazon Inspector agents on EC2 instances to report on insecure encryption ciphers. Set up AWS Config rules to periodically check for non-encrypted S3 objects.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the application to encrypt each file prior to storing on Amazon S3. Enable OS-based encryption of data on EBS volumes. Encrypt data on write to RDS. Run cron jobs on each instance to check for unencrypted data and notify via Amazon SNS. Use S3 Events to call an AWS Lambda function and verify if the file is encrypted.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Secure Sockets Layer (SSL) on the load balancer, ensure that AWS Lambda is using SSL to communicate to the RDS database, and enable S3encryption. Configure the application to force SSL for incoming connections and configure RDS to only grant access if the session is encrypted. Configure Amazon Inspector agents on EC2 instances to report on insecure encryption ciphers.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable SSE encryption on the S3 buckets, EBS volumes, and the RDS database. Store RDS credentials in EC2 Parameter Store. Enable a policy on the S3 bucket to deny unencrypted puts. Set up AWS Config rules to periodically check for non-encrypted S3 objects and EBS volumes, and to ensure that RDS storage is encrypted.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-11-30T21:03:00.000Z",
        "voteCount": 5,
        "content": "A - there is no need to use OS based encryption on the EBS volumes. You can just use AWS provided EBS encryption.\nB - No need to configure apps to encrypt when writing to S3. You use encryption on S3.\nC - This is encryption in transit, not encryption on rest.\nD - Correct."
      },
      {
        "date": "2023-05-16T22:04:00.000Z",
        "voteCount": 1,
        "content": "D - what is \"EC2 Parameter Store\"? Is it exists?"
      },
      {
        "date": "2023-12-15T05:13:00.000Z",
        "voteCount": 1,
        "content": "EC2 Paramter store evolved to System manager parameter store."
      },
      {
        "date": "2023-02-19T10:46:00.000Z",
        "voteCount": 3,
        "content": "D - None of the other options mention removing the DB connection string from the code"
      },
      {
        "date": "2023-02-08T08:28:00.000Z",
        "voteCount": 1,
        "content": "A and B are eliminated because of \"OS-based encryption of data on EBS volumes\".\nBetween C and D, monitoring of D is better."
      },
      {
        "date": "2023-01-22T05:58:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is D."
      },
      {
        "date": "2022-12-13T00:16:00.000Z",
        "voteCount": 2,
        "content": "D is the correct answer."
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 84,
    "url": "https://www.examtopics.com/discussions/amazon/view/88913-exam-aws-devops-engineer-professional-topic-1-question-84/",
    "body": "A DevOps engineer is planning to deploy a Ruby-based application to production. The application needs to interact with an Amazon RDS for MySQL database and should have automatic scaling and high availability. The stored data in the database is critical and should persist regardless of the state of the application stack.<br><br>The DevOps engineer needs to set up an automated deployment strategy for the application with automatic rollbacks. The solution also must alert the application team when a deployment fails.<br><br>Which combination of steps will meet these requirements? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the application on AWS Elastic Beanstalk. Deploy an Amazon RDS for MySQL DB instance as part of the Elastic Beanstalk configuration.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the application on AWS Elastic Beanstalk. Deploy a separate Amazon RDS for MySQL DB instance outside of Elastic Beanstalk.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a notification email address that alerts the application team in the AWS Elastic Beanstalk configuration.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an Amazon EventBridge (Amazon CloudWatch Events) rule to monitor AWS Health events. Use an Amazon Simple Notification Service (Amazon SNS) topic as a target to alert the application team.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the immutable deployment method to deploy new application versions.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the rolling deployment method to deploy new application versions."
    ],
    "answer": "BCE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BCE",
        "count": 31,
        "isMostVoted": true
      },
      {
        "answer": "BDE",
        "count": 25,
        "isMostVoted": false
      },
      {
        "answer": "ACF",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "BCF",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-11-30T21:24:00.000Z",
        "voteCount": 22,
        "content": "B and not A because the RDS data is critical. Move RDS out of Beanstalk.\nD and not C, because D constantly monitors the Beanstalk health. See https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/AWSHowTo.eventbridge.html\nE and not F because immutable creates another environment. If rolling deployment fails, you need to redeploy it again. They ask for automated healing. https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html#environments-cfg-rollingdeployments-method"
      },
      {
        "date": "2023-02-17T15:29:00.000Z",
        "voteCount": 2,
        "content": "D uses elastic beanstalk events as source. It does not use Health.\nTherefore C is correct"
      },
      {
        "date": "2023-02-18T12:20:00.000Z",
        "voteCount": 1,
        "content": "In a table of the link:\nEnvironment update failed   ERROR Failed to deploy configuration."
      },
      {
        "date": "2023-02-18T12:21:00.000Z",
        "voteCount": 1,
        "content": "It means C is involved by D"
      },
      {
        "date": "2022-12-03T13:40:00.000Z",
        "voteCount": 15,
        "content": "B:  and not A because the RDS data is critical. Move RDS out of Beanstalk.\nC:  not D, because AWS Health monitor health status of AWS services, not deployments.\nE: This was tricky one. We should pay attention that we need automatic rollback, not rolling update.  Failed rolling update required manual work to deploy new good version.  Failed immutable update will rollback all unhealthy instances automatically, no manual work is needed\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html#environments-cfg-rollingdeployments-method"
      },
      {
        "date": "2022-12-18T19:25:00.000Z",
        "voteCount": 5,
        "content": "Yes, BCE looks correct to me.\nYou can set sns notification in elastic beanstalk console.\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.managing.sns.html"
      },
      {
        "date": "2022-12-18T19:26:00.000Z",
        "voteCount": 1,
        "content": "I mean notification email address, not the sns."
      },
      {
        "date": "2024-07-09T12:49:00.000Z",
        "voteCount": 1,
        "content": "B \nC - https://docs.aws.amazon.com/sns/latest/dg/sns-configure-dead-letter-queue.html . D is incorrect because it says Event bridge and AWS Health integration. \nE - https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html#environments-cfg-rollingdeployments-method"
      },
      {
        "date": "2024-01-17T19:00:00.000Z",
        "voteCount": 1,
        "content": "See USalo"
      },
      {
        "date": "2023-05-29T08:59:00.000Z",
        "voteCount": 1,
        "content": "bce-bce-bce"
      },
      {
        "date": "2023-04-19T18:09:00.000Z",
        "voteCount": 1,
        "content": "Originally thought BDE, but seen this link below so it's BCE, I got confused with CW Events when using codedeploy\n\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.managing.sns.html"
      },
      {
        "date": "2023-03-06T13:03:00.000Z",
        "voteCount": 1,
        "content": "BCE\nWhy not D? \n1) D uses health which is not related.\n2) Beanstalk already creates a SNS topic  just need to update the email address in order to subscribe to it. \nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.managing.sns.html"
      },
      {
        "date": "2023-03-05T12:47:00.000Z",
        "voteCount": 1,
        "content": "B - RDS data is critical\nC - For notification\nE - rollback"
      },
      {
        "date": "2023-03-05T08:31:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/fr_fr/elasticbeanstalk/latest/dg/using-features.managing.sns.html"
      },
      {
        "date": "2023-02-21T13:13:00.000Z",
        "voteCount": 5,
        "content": "Not D, as AWS Health is for service-level faults, not deployment issues."
      },
      {
        "date": "2023-02-21T13:11:00.000Z",
        "voteCount": 1,
        "content": "AWS Health is for monitoring service-level faults, and should not alert anything related to failed deployments"
      },
      {
        "date": "2023-02-14T23:09:00.000Z",
        "voteCount": 3,
        "content": "BCE\n\nB and E seem to be uncontroversial\n\nRegarding C or D, I'll go with C, the reason is that when you generate the beanstalk environment and define in the Notifications section an email, AWS automatically takes care of creating the associated SNS topic and that email as a subscription in pending confirmation status. Once confirmed, when a deployment is done it notifies you of status changes, so if after a deployment or rollback it fails it notifies you of any changes."
      },
      {
        "date": "2023-02-14T23:06:00.000Z",
        "voteCount": 1,
        "content": "BCE\n\nB y E parece que no tienen discusi\u00f3n\n\nCon respecto a C o D, me quedo con C, la raz\u00f3n es que cuando generas el entorno de beanstalk y le defines en la secci\u00f3n Notificaciones un correo electr\u00f3nico, AWS se encarga autom\u00e1ticamente de crear el topic SNS asociado y ese correo como subscripci\u00f3n en estado pending confirmation. Una vez confirmado, cuando se realiza implementaci\u00f3n te notifica de los cambios de estado, as\u00ed que si tras una implementaci\u00f3n o rollback falla te notifica"
      },
      {
        "date": "2023-02-04T07:51:00.000Z",
        "voteCount": 1,
        "content": "B and E are obvious.\nD not C because  \"To configure which notifications are sent under certain circumstances, you also have other options available. You can use Amazon EventBridge to set up event-driven rules , which notify you when Elastic Beanstalk emits events that meet certain criteria. Alternatively, you can configure your environment to publish custom metrics and set up Amazon CloudWatch alarms to notify you when those metrics reach unhealthy thresholds.\" Here requirement is only deployment fails https://docs.aws.amazon.com/zh_cn/elasticbeanstalk/latest/dg/using-features.managing.sns.html#:~:text=To%20configure%20which,reach%20unhealthy%20thresholds."
      },
      {
        "date": "2023-02-06T13:38:00.000Z",
        "voteCount": 2,
        "content": "Why do you need to monitor AWS Health ??? I agree, that you need EventBridge, but why to monitor AWS Health? Even in you link there is no mentioning of AWS Health.\nAWS Health is completely different service that does not show the deployment status of EB"
      },
      {
        "date": "2023-01-22T06:18:00.000Z",
        "voteCount": 4,
        "content": "Correct answer is B,C and E. B because RDS data is critical, C because that's the way to send notifications on failed deployment to your email address using Elastric BeanStalk which uses SNS to send notification to your email address abd E because of the ability to rollback to the previous version after a failed deployment"
      },
      {
        "date": "2023-01-05T17:21:00.000Z",
        "voteCount": 1,
        "content": "I vote for BDE"
      },
      {
        "date": "2022-12-27T20:39:00.000Z",
        "voteCount": 5,
        "content": "A -- RDS is critical and should not be clubbed with AWS Beanstalk ENV.\n\nB -- Correct as RDS should be created separately independent of the BeanStalk ENV.\n\nC -- Correct -- You can send alerts for your ENV via adding email address in config https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.managing.sns.html#configuration-notifications-namespace\n\nD -- Health monitor will give alerts for all the environments and also not related to Deployments\n\nE -- Correct and D -- Incorrect  --&gt; Rolling == No automatic Rollback, Immutable == Automatic Rollback."
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 85,
    "url": "https://www.examtopics.com/discussions/amazon/view/88915-exam-aws-devops-engineer-professional-topic-1-question-85/",
    "body": "An ecommerce company is looking for ways to deploy an application on AWS that satisfies the following requirements:<br><br>\u2022\tHas a simple and automated application deployment process.<br>\u2022\tHas minimal deployment costs while ensuring that at least half of the instances are available to receive end-user requests.<br>\u2022\tIf the application fails, an automated healing mechanism will replace the affected instances.<br><br>Which deployment strategy will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Elastic Beanstalk environment and configure it to use Auto Scaling and an Elastic Load Balancer. Use rolling deployments with a batch size of 50%.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS OpsWorks stack. Configure the application layer to use rolling deployments as a deployment strategy. Add an Elastic Load Balancing layer. Enable auto healing on the application layer.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS CodeDeploy with Auto Scaling and an Elastic Load Balancer. Use the CodeDeployDefault.HalfAtAtime deployment strategy. Enable an Elastic Load Balancing health check to report the status of the application, and set the Auto Scaling health check to ELB.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS CodeDeploy with Auto Scaling and an Elastic Load Balancer. Use a blue/green deployment strategy. Enable an Elastic Load Balancing health check to report the status of the application, and set the Auto Scaling health check to ELB."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 21,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 20,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-11-30T21:30:00.000Z",
        "voteCount": 13,
        "content": "The keyword is \"Has a simple and automated application deployment process.\"\nYou can't find anything simpler than Beanstalk. Answer: A"
      },
      {
        "date": "2024-05-22T01:38:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html#deployments-scenarios\nhow to auto healing ?"
      },
      {
        "date": "2022-12-14T23:07:00.000Z",
        "voteCount": 12,
        "content": "A is missing an important aspect of Automatic Healing. Rolling deployments in Elastic Beanstalk if failed are not rollback automatically. hence the answer is C"
      },
      {
        "date": "2022-12-14T23:07:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html#environments-cfg-rollingdeployments-method"
      },
      {
        "date": "2024-07-09T12:54:00.000Z",
        "voteCount": 1,
        "content": "A  - Incorrect. EB rolling deployments don't auto rollback\nC - CodeDeploy deployment configuration can be set to auto rollback"
      },
      {
        "date": "2024-03-11T20:49:00.000Z",
        "voteCount": 1,
        "content": "Why not B?\nhttps://docs.aws.amazon.com/opsworks/latest/userguide/workinginstances-autohealing.html"
      },
      {
        "date": "2023-11-28T08:25:00.000Z",
        "voteCount": 1,
        "content": "This is C"
      },
      {
        "date": "2023-11-18T07:26:00.000Z",
        "voteCount": 1,
        "content": "1.)AWS CodeDeploy with Auto Scaling and an Elastic Load Balancer provides a simple and automated application deployment process.\n\n2.)The CodeDeployDefault.HalfAtAtime deployment strategy ensures minimal deployment costs while ensuring that at least half of the instances are available to receive end-user requests.\n\n3.)AWS CodeDeploy provides an automated healing mechanism that can replace the affected instances if the application fails."
      },
      {
        "date": "2023-11-10T10:56:00.000Z",
        "voteCount": 1,
        "content": "It is possible to configure Beanstalk ASG health check for using ELB Elastic Load Balancing health check and monitor the application.\nFrom: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environmentconfig-autoscaling-healthchecktype.html\n\"If you want Auto Scaling to replace instances whose application has stopped responding, you can use a configuration file to configure the Auto Scaling group to use Elastic Load Balancing health checks. The following example sets the group to use the load balancer's health checks, in addition to the Amazon EC2 status check, to determine an instance's health\""
      },
      {
        "date": "2023-10-30T11:26:00.000Z",
        "voteCount": 1,
        "content": "It'c C.\n\nC is not correct: In Beanstalk, in case application is not healthy - instance can still stay:\n\"Amazon EC2 status checks only cover an instance's health, not the health of your application, server, or any Docker containers running on the instance. If your application crashes, but the instance that it runs on is still healthy, it may be kicked out of the load balancer, but Auto Scaling won't replace it automatically. \""
      },
      {
        "date": "2023-10-18T10:47:00.000Z",
        "voteCount": 1,
        "content": "Elastic Beanstalk doesn\u2019t have an automated healing mechanism to replace the affected instances. Thus: C."
      },
      {
        "date": "2023-07-02T06:48:00.000Z",
        "voteCount": 1,
        "content": "Considering rolling back i.e healing process .Its C"
      },
      {
        "date": "2023-05-29T09:02:00.000Z",
        "voteCount": 1,
        "content": "It's C not A. \n\nA. Creating an AWS Elastic Beanstalk environment with Auto Scaling and an Elastic Load Balancer and using rolling deployments with a batch size of 50% does not guarantee that at least half of the instances will be available to receive end-user requests. It is possible that during the deployment process, the application could become partially or completely unavailable."
      },
      {
        "date": "2023-05-03T05:35:00.000Z",
        "voteCount": 1,
        "content": "A is more suitable"
      },
      {
        "date": "2023-04-19T18:14:00.000Z",
        "voteCount": 1,
        "content": "Agree with SmileyCloud"
      },
      {
        "date": "2023-03-05T08:39:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html#environments-cfg-rollingdeployments-method"
      },
      {
        "date": "2023-03-04T23:13:00.000Z",
        "voteCount": 1,
        "content": "Beanstalk fulfills all condition"
      },
      {
        "date": "2023-01-22T06:47:00.000Z",
        "voteCount": 4,
        "content": "Correct answer is A."
      },
      {
        "date": "2023-01-17T05:05:00.000Z",
        "voteCount": 3,
        "content": "Should be A, Beanstalk will help to replace failed instance:\n\"If an instance in your environment fails an Amazon EC2 status check, Auto Scaling takes it down and replaces it.\"\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environmentconfig-autoscaling-healthchecktype.html"
      },
      {
        "date": "2023-10-30T11:24:00.000Z",
        "voteCount": 1,
        "content": "It's C:\n\nRegarding A - Beanstalk:\n\"Amazon EC2 status checks only cover an instance's health, not the health of your application, server, or any Docker containers running on the instance. If your application crashes, but the instance that it runs on is still healthy, it may be kicked out of the load balancer, but Auto Scaling won't replace it automatically. \""
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 86,
    "url": "https://www.examtopics.com/discussions/amazon/view/88841-exam-aws-devops-engineer-professional-topic-1-question-86/",
    "body": "A company is implementing AWS CodePipeline to automate its testing process. The company wants to be notified when the execution state fails and used the following custom event pattern in Amazon CloudWatch:<br><br><img src=\"https://img.examtopics.com/aws-devops-engineer-professional/image1.png\"><br><br>Which type of events will match this event pattern?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFailed deploy and build actions across all the pipelines",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAll rejected or failed approval actions across all the pipelines\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAll the events across all pipelines",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tApproval actions across all pipelines"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-03-11T21:03:00.000Z",
        "voteCount": 1,
        "content": "B - State - Failed"
      },
      {
        "date": "2023-03-08T07:14:00.000Z",
        "voteCount": 1,
        "content": "\"state\": \"FAILED\""
      },
      {
        "date": "2023-02-11T10:48:00.000Z",
        "voteCount": 2,
        "content": "the answer is B"
      },
      {
        "date": "2023-02-09T06:06:00.000Z",
        "voteCount": 1,
        "content": "B is right"
      },
      {
        "date": "2023-01-22T06:49:00.000Z",
        "voteCount": 2,
        "content": "B is correct."
      },
      {
        "date": "2022-12-13T00:25:00.000Z",
        "voteCount": 1,
        "content": "B is the correct answer"
      },
      {
        "date": "2022-11-30T21:40:00.000Z",
        "voteCount": 3,
        "content": "B. An example for CodePipeline Action Execution State Change is here. \nhttps://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html"
      },
      {
        "date": "2022-11-26T04:14:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/zh_cn/eventbridge/latest/userguide/eb-event-patterns.html"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 87,
    "url": "https://www.examtopics.com/discussions/amazon/view/88842-exam-aws-devops-engineer-professional-topic-1-question-87/",
    "body": "A company has developed a Node.js web application which provides REST services to store and retrieve time series data. The web application is built by the development team on company laptops, tested locally, and manually deployed to a single on-premises server, which accesses a local MySQL database. The company is starting a trial in two weeks, during which the application will undergo frequent updates based on customer feedback. The following requirements must be met:<br><br>\u2022\tThe team must be able to reliably build, test, and deploy new updates on a daily basis, without downtime or degraded performance.<br>\u2022\tThe application must be able to scale to meet an unpredictable number of concurrent users during the trial.<br><br>Which action will allow the team to quickly meet these objectives?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate two Amazon Lightsail virtual private servers for Node.js; one for test and one for production. Build the Node.js application using existing processes and upload it to the new Lightsail test server using the AWS CLI. Test the application, and if it passes all tests, upload it to the production server. During the trial, monitor the production server usage, and if needed, increase performance by upgrading the instance type.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDevelop an AWS CloudFormation template to create an Application Load Balancer and two Amazon EC2 instances with Amazon EBS (SSD) volumes in an Auto Scaling group with rolling updates enabled. Use AWS CodeBuild to build and test the Node.js application and store it in an Amazon S3 bucket. Use user-data scripts to install the application and the MySQL database on each EC2 instance. Update the stack to deploy new application versions.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure AWS Elastic Beanstalk to automatically build the application using AWS CodeBuild and to deploy it to a test environment that is configured to support auto scaling. Create a second Elastic Beanstalk environment for production. Use Amazon RDS to store data. When new versions of the applications have passed all tests, use Elastic Beanstalk 'swap cname' to promote the test environment to production.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the application to use Amazon DynamoDB instead of a local MySQL database. Use AWS OpsWorks to create a stack for the application with a DynamoDB layer, an Application Load Balancer layer, and an Amazon EC2 instance layer. Use a Chef recipe to build the application and a Chef recipe to deploy the application to the EC2 instance layer. Use custom health checks to run unit tests on each instance with rollback on failure."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-11-30T21:43:00.000Z",
        "voteCount": 8,
        "content": "C. Requires least effort and does everything required."
      },
      {
        "date": "2022-12-28T05:04:00.000Z",
        "voteCount": 1,
        "content": "Elastic Beanstalk, does not build your application, it only deploy. Lightsail will build, test and deploy your application quickly with scalability. check out option A."
      },
      {
        "date": "2022-12-28T05:07:00.000Z",
        "voteCount": 1,
        "content": "AWS lightsail also provides a variety of database options"
      },
      {
        "date": "2022-12-28T05:13:00.000Z",
        "voteCount": 1,
        "content": "I think, the best option is still Beanstalk. C"
      },
      {
        "date": "2023-03-04T09:48:00.000Z",
        "voteCount": 1,
        "content": "This part of option A disqualifies it --&gt; During the trial, monitor the production server usage, and if needed, increase performance by upgrading the instance type. --&gt; Manual scaling is an issue. Hence C is better."
      },
      {
        "date": "2023-04-19T18:31:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/eb-cli-codebuild.html"
      },
      {
        "date": "2023-02-17T15:39:00.000Z",
        "voteCount": 1,
        "content": "How can it be B. How do you configure Beanstalk to automatically build using codebuild?"
      },
      {
        "date": "2023-02-17T15:42:00.000Z",
        "voteCount": 2,
        "content": "I meant how can it be C"
      },
      {
        "date": "2023-01-22T06:54:00.000Z",
        "voteCount": 1,
        "content": "C is the right answer"
      },
      {
        "date": "2023-01-15T23:11:00.000Z",
        "voteCount": 1,
        "content": "https://www.examtopics.com/discussions/amazon/view/8474-exam-aws-devops-engineer-professional-topic-1-question-112/"
      },
      {
        "date": "2022-11-26T04:26:00.000Z",
        "voteCount": 1,
        "content": "without downtime or degraded performance.Elastic Beanstalk can do that"
      },
      {
        "date": "2022-11-26T04:15:00.000Z",
        "voteCount": 1,
        "content": "that is about when aprroval rejecdt"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 88,
    "url": "https://www.examtopics.com/discussions/amazon/view/88843-exam-aws-devops-engineer-professional-topic-1-question-88/",
    "body": "A company is deploying a new application that uses Amazon EC2 instances. The company needs a solution to query application logs and AWS account API activity.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Amazon CloudWatch agent to send logs from the EC2 instances to Amazon CloudWatch Logs. Configure AWS CloudTrail to deliver the API logs to Amazon S3. Use CloudWatch to query both sets of logs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Amazon CloudWatch agent to send logs from the EC2 instances to Amazon CloudWatch Logs. Configure AWS CloudTrail to deliver the API logs to CloudWatch Logs. Use CloudWatch Logs Insights to query both sets of logs.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Amazon CloudWatch agent to send logs from the EC2 instances to Amazon Kinesis. Configure AWS CloudTrail to deliver the API logs to Kinesis. Use Kinesis to load the data into Amazon Redshift. Use Amazon Redshift to query both sets of logs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Amazon CloudWatch agent to send logs from the EC2 instances to Amazon S3. Use AWS CloudTrail to deliver the API logs to Amazon S3. Use Amazon Athena to query both sets of logs in Amazon S3."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 19,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-11-30T21:54:00.000Z",
        "voteCount": 11,
        "content": "A - You can't use CloudWatch to query anything. You have to use CloudWatch Logs Insights, but even then - you can't use it to query S3. \nB - Correct. Very simple to configure and use.\nC - Why would you use this complex solution? RedShift is super expensive.\nD - This is viable too, but you want your logs in CloudWatch not S3."
      },
      {
        "date": "2023-06-19T23:36:00.000Z",
        "voteCount": 1,
        "content": "i am confused... where is it written that i want logs in cloudwatch not in s3"
      },
      {
        "date": "2023-12-14T06:55:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2023-06-19T23:44:00.000Z",
        "voteCount": 1,
        "content": "Option B is correct..\nWe cannot send cloud watch agent logs directly to the S3, we will have to use lambda function to send each logs to S3.. thats why better solution is to use B"
      },
      {
        "date": "2023-05-03T05:41:00.000Z",
        "voteCount": 1,
        "content": "B is more suitable in this case"
      },
      {
        "date": "2023-02-15T03:41:00.000Z",
        "voteCount": 1,
        "content": "B is answer."
      },
      {
        "date": "2023-02-09T06:10:00.000Z",
        "voteCount": 1,
        "content": "very simple."
      },
      {
        "date": "2023-01-27T05:15:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2023-01-22T06:57:00.000Z",
        "voteCount": 1,
        "content": "B is the correct answer. CloudWatch log insights can't query S3."
      },
      {
        "date": "2022-12-13T08:48:00.000Z",
        "voteCount": 2,
        "content": "B for sure."
      },
      {
        "date": "2022-12-04T04:07:00.000Z",
        "voteCount": 2,
        "content": "Why not D?"
      },
      {
        "date": "2022-12-27T18:43:00.000Z",
        "voteCount": 1,
        "content": "The features of cloud watch logs insights. Otherwise yes D also works, but B is \u201cbetter\u201d"
      },
      {
        "date": "2023-02-05T06:57:00.000Z",
        "voteCount": 1,
        "content": "I don't think CloudWatch Agent can \"send\" logs to s3. You can only export them to s3 later,  from log group"
      },
      {
        "date": "2022-11-27T09:25:00.000Z",
        "voteCount": 3,
        "content": "1. https://docs.aws.amazon.com/awscloudtrail/latest/userguide/send-cloudtrail-events-to-cloudwatch-logs.html\n2. https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/AnalyzingLogData.html"
      },
      {
        "date": "2022-11-26T04:27:00.000Z",
        "voteCount": 1,
        "content": "yes,the CloudTrail to deliver the API logs to Amazon S3 is better"
      },
      {
        "date": "2022-11-27T22:51:00.000Z",
        "voteCount": 2,
        "content": "Can you use cloudwatch to query logs in S3, as mentioned Option A?."
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 89,
    "url": "https://www.examtopics.com/discussions/amazon/view/88844-exam-aws-devops-engineer-professional-topic-1-question-89/",
    "body": "A DevOps engineer is tasked with creating a more stable deployment solution for a web application in AWS. Previous deployments have resulted in user-facing bugs, premature user traffic, and inconsistencies between web servers running behind an Application Load Balancer. The current strategy uses AWS CodeCommit to store the code for the application. When developers push to the main branch of the repository, CodeCommit triggers an AWS Lambda deploy function, which invokes an AWS Systems Manager run command to build and deploy the new code to all Amazon EC2 instances.<br><br>Which combination of actions should be taken to implement a more stable deployment solution? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a pipeline in AWS CodePipeline with CodeCommit as a source provider. Create parallel pipeline stages to build and test the application. Pass the build artifact to AWS CodeDeploy.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a pipeline in AWS CodePipeline with CodeCommit as a source provider. Create separate pipeline stages to build and then test the application. Pass the build artifact to AWS CodeDeploy.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate and use an AWS CodeDeploy application and deployment group to deploy code updates to the EC2 fleet. Select the Application Load Balancer for the deployment group.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate individual Lambda functions to run all build, test, and deploy actions using AWS CodeDeploy instead of AWS Systems Manager.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the Lambda function to build a single application package to be shared by all instances. Use AWS CodeDeploy instead of AWS Systems Manager to update the code on the EC2 fleet."
    ],
    "answer": "BC",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BC",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-04-29T02:26:00.000Z",
        "voteCount": 1,
        "content": "Yes, I agree with B and C, however CodeDeploy should also be part of the CodePipeline, and it is not stated in C,"
      },
      {
        "date": "2023-04-19T18:41:00.000Z",
        "voteCount": 1,
        "content": "Agree with obaf1"
      },
      {
        "date": "2023-02-09T02:17:00.000Z",
        "voteCount": 2,
        "content": "Between A and B, parallel pipeline stages does not help for this scenario. \nD and E are eliminated, \"use AWS CodeDeploy instead of AWS\" sounds weird."
      },
      {
        "date": "2023-01-22T07:02:00.000Z",
        "voteCount": 2,
        "content": "Answer is BC"
      },
      {
        "date": "2022-12-28T06:12:00.000Z",
        "voteCount": 2,
        "content": "Ans: BC\nYou can specify Application Load Balancer for deployment group...read the excerpt below (link of the entire page also provided):\n\"For a blue/green deployment, you can specify a Classic Load Balancer, Application Load Balancer, or Network Load Balancer in your deployment group.\"\nhttps://docs.aws.amazon.com/codedeploy/latest/userguide/integrations-aws-elastic-load-balancing.html"
      },
      {
        "date": "2022-12-03T01:20:00.000Z",
        "voteCount": 1,
        "content": "I will go with BD, \nC for selecting the ALB for the deployment group is not supported"
      },
      {
        "date": "2022-11-30T22:00:00.000Z",
        "voteCount": 3,
        "content": "BC makes most sense."
      },
      {
        "date": "2022-11-26T04:32:00.000Z",
        "voteCount": 3,
        "content": "BC is ok"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 90,
    "url": "https://www.examtopics.com/discussions/amazon/view/88845-exam-aws-devops-engineer-professional-topic-1-question-90/",
    "body": "A company wants to set up a continuous delivery pipeline. The company stores application code in a private GitHub repository. The company needs to deploy the application components to Amazon Elastic Container Service (Amazon ECS), Amazon EC2, and AWS Lambda. The pipeline must support manual approval actions.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS CodePipeline with Amazon ECS, Amazon EC2, and Lambda as deploy providers.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS CodePipeline with AWS CodeDeploy as the deploy provider.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS CodePipeline with AWS Elastic Beanstalk as the deploy provider.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS CodeDeploy with GitHub integration to deploy the application."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-02-09T02:40:00.000Z",
        "voteCount": 2,
        "content": "D is not sufficient. \nC is the one for webapp.\nA includes too much manual work."
      },
      {
        "date": "2023-01-22T07:04:00.000Z",
        "voteCount": 1,
        "content": "B meets all requirements"
      },
      {
        "date": "2023-01-19T18:25:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-steps.html\nB meets all requirements, CodeDeploy support EC2, ECS and Lambda"
      },
      {
        "date": "2023-01-13T09:57:00.000Z",
        "voteCount": 2,
        "content": "Option A would meet the requirements of deploying the application components to Amazon ECS, EC2, and Lambda. Option B, using CodePipeline with CodeDeploy as the deploy provider, would allow for manual approval actions to be added to the pipeline, but it would not support deploying to Amazon ECS or Lambda. Option C, using CodePipeline with Elastic Beanstalk as the deploy provider, would not support deploying to Amazon ECS or Lambda. And option D, using CodeDeploy with GitHub integration, would not support deploying to Amazon ECS or Lambda or adding manual approval actions."
      },
      {
        "date": "2023-02-09T02:43:00.000Z",
        "voteCount": 1,
        "content": "CodeDeloy support Lambda, does not support ECS only in Osaka Region.\nhttps://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-steps.html"
      },
      {
        "date": "2022-11-26T04:33:00.000Z",
        "voteCount": 3,
        "content": "AWS CodeDeploy is one of the tools in the AWS Developer Tools series, which can be used to deploy to three computing environments: EC2, Lambda, and ECS."
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 91,
    "url": "https://www.examtopics.com/discussions/amazon/view/89506-exam-aws-devops-engineer-professional-topic-1-question-91/",
    "body": "A DevOps engineer is deploying an AWS Service Catalog portfolio using AWS CodePipeline. The pipeline should create products and templates based on a manifest file in either JSON or YAML, and should enforce security requirements on all AWS Service Catalog products managed through the pipeline.<br><br>Which solution will meet the requirements in an automated fashion?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the AWS Service Catalog deploy action in AWS CodeDeploy to push new versions of products into the AWS Service Catalog with verification steps in the CodeDeploy AppSpec.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the AWS Service Catalog deploy action in AWS CodeBuild to verify and push new versions of products into the AWService Catalog.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an AWS Lambda action in CodePipeline to run a Lambda function to verify and push new versions of products into the AWS Service Catalog.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an AWS Lambda action in AWS CodeBuild to run a Lambda function to verify and push new versions of products into the AWS Service Catalog."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 23,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 11,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-11-30T22:09:00.000Z",
        "voteCount": 10,
        "content": "A is correct. Here is an example. https://docs.aws.amazon.com/codepipeline/latest/userguide/tutorials-S3-servicecatalog.html"
      },
      {
        "date": "2022-12-27T19:16:00.000Z",
        "voteCount": 2,
        "content": "This link explicitly states to skip the build stage, that is why code build is the incorrect answer"
      },
      {
        "date": "2023-02-04T16:10:00.000Z",
        "voteCount": 2,
        "content": "A is incorrect. CodeDeploy is not capable of deploying to AWS Service Catalog.\nhttps://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-groups.html\nhttps://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-steps.html"
      },
      {
        "date": "2023-01-22T17:27:00.000Z",
        "voteCount": 6,
        "content": "AWS service catalog is a deploy action in CodePipeline. It's not an action in CodeDeploy as A) suggests. Therefore the best answer is C. Others options are incorrect as they are not technically viable."
      },
      {
        "date": "2023-09-13T14:19:00.000Z",
        "voteCount": 1,
        "content": "vote for C"
      },
      {
        "date": "2023-04-19T19:09:00.000Z",
        "voteCount": 1,
        "content": "https://aws.amazon.com/blogs/devops/aws-service-catalog-sync-code/"
      },
      {
        "date": "2023-03-15T18:56:00.000Z",
        "voteCount": 1,
        "content": "Option C is the best choice because it allows the DevOps engineer to enforce security requirements on all AWS Service Catalog products managed through the pipeline using an AWS Lambda function."
      },
      {
        "date": "2023-02-21T15:12:00.000Z",
        "voteCount": 4,
        "content": "C - corect - https://aws.amazon.com/blogs/devops/aws-service-catalog-sync-code/"
      },
      {
        "date": "2023-02-09T02:53:00.000Z",
        "voteCount": 1,
        "content": "C and D look weird. Both CodePipeline and CodeBuild are managed products in AWS, it's weird to use Lambda on them.\nA is neater than B"
      },
      {
        "date": "2023-02-09T00:59:00.000Z",
        "voteCount": 3,
        "content": "C - corect - https://aws.amazon.com/blogs/devops/aws-service-catalog-sync-code/"
      },
      {
        "date": "2023-01-31T08:24:00.000Z",
        "voteCount": 2,
        "content": "https://aws.amazon.com/blogs/devops/aws-service-catalog-sync-code/"
      },
      {
        "date": "2023-01-19T18:31:00.000Z",
        "voteCount": 2,
        "content": "Should be C, below example is using CodePipeline and not CodeDeploy.\nhttps://docs.aws.amazon.com/codepipeline/latest/userguide/tutorials-S3-servicecatalog.html"
      },
      {
        "date": "2023-01-16T06:06:00.000Z",
        "voteCount": 4,
        "content": "https://www.examtopics.com/discussions/amazon/view/47927-exam-aws-devops-engineer-professional-topic-1-question-237/"
      },
      {
        "date": "2023-01-09T20:37:00.000Z",
        "voteCount": 1,
        "content": "Option A\nhttps://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file.html"
      },
      {
        "date": "2023-01-08T15:16:00.000Z",
        "voteCount": 4,
        "content": "I'll go with C. Although there is no need for invoking a lambda function for deploying to Service Catalog but it is not possible to use CodeDeploy for deploying to Service Catalog"
      },
      {
        "date": "2023-04-19T19:08:00.000Z",
        "voteCount": 1,
        "content": "Yup, the diagram shows lambda doing the service catalog bits: https://aws.amazon.com/blogs/devops/aws-service-catalog-sync-code/"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 92,
    "url": "https://www.examtopics.com/discussions/amazon/view/88848-exam-aws-devops-engineer-professional-topic-1-question-92/",
    "body": "A company has containerized all of its in-house quality control applications. The company is running Jenkins on Amazon EC2, which requires patching and upgrading. The compliance officer has requested a DevOps engineer begin encrypting build artifacts since they contain company intellectual property.<br><br>What should the DevOps engineer do to accomplish this in the MOST maintainable manner?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAutomate patching and upgrading using AWS Systems Manager on EC2 instances and encrypt Amazon EBS volumes by default.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy Jenkins to an Amazon ECS cluster and copy build artifacts to an Amazon S3 bucket with default encryption enabled.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLeverage AWS CodePipeline with a build action and encrypt the artifacts using AWS Secrets Manager.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS CodeBuild with artifact encryption to replace the Jenkins instance running on Amazon EC2.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 12,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-07-19T06:49:00.000Z",
        "voteCount": 2,
        "content": "Pretty sure B would work, but D is the most sustainable solution."
      },
      {
        "date": "2023-07-12T16:22:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is D.\nAll options can work but the key part of the question is finding the solution that will be the MOST maintainable. AWS CodeBuild provides a fully managed build service which eliminates the need to set up, patch, and maintain your own build servers, which Jenkins on Amazon EC2 requires. AWS CodeBuild is essentially a \"serverless\" build service, where you only pay for the build time you use, and it scales automatically to meet the needs of your builds.\n\nCodeBuild also has built-in support for producing encrypted build artifacts. This would satisfy the compliance officer's request for encrypting build artifacts containing intellectual property. By managing both the build process and artifact encryption within a single service, the solution remains simple and maintainable."
      },
      {
        "date": "2023-06-15T20:04:00.000Z",
        "voteCount": 2,
        "content": "Correct answer is B...\ncontainerized ::: ECS would be the best and most maintainable..\nWe don't have to replace jenkins, company is already using jenkins man.."
      },
      {
        "date": "2023-03-15T19:15:00.000Z",
        "voteCount": 1,
        "content": "B \ndeploying Jenkins to an Amazon ECS cluster and copying build artifacts to an Amazon S3 bucket with default encryption enabled provides a straightforward solution that aligns with the current workflow and requires minimal changes. This option provides a more maintainable solution that satisfies the compliance officer's request for encrypting build artifacts."
      },
      {
        "date": "2023-01-22T17:30:00.000Z",
        "voteCount": 3,
        "content": "D is the correct answer."
      },
      {
        "date": "2023-01-13T10:13:00.000Z",
        "voteCount": 1,
        "content": "B = \" MOST maintainable manner\""
      },
      {
        "date": "2023-02-09T03:13:00.000Z",
        "voteCount": 1,
        "content": "Jenkins gets all of the intellectual properties."
      },
      {
        "date": "2023-03-04T10:26:00.000Z",
        "voteCount": 1,
        "content": "A and B rely on out of box encryption at rest but the requirement is to encrypt the artifact from client side before storing it. D covers this scenario."
      },
      {
        "date": "2022-12-04T21:13:00.000Z",
        "voteCount": 3,
        "content": "https://docs.aws.amazon.com/codebuild/latest/userguide/security-encryption.html"
      },
      {
        "date": "2022-11-30T22:13:00.000Z",
        "voteCount": 1,
        "content": "D is the correct answer."
      },
      {
        "date": "2022-11-26T04:45:00.000Z",
        "voteCount": 3,
        "content": "Build artifact encryption - CodeBuild requires access to an AWS KMS CMK in order to encrypt its build output artifacts. By default, CodeBuild uses an AWS Key Management Service CMK for Amazon S3 in your AWS account. If you do not want to use this CMK, you must create and configure a customer-managed CMK."
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 93,
    "url": "https://www.examtopics.com/discussions/amazon/view/88849-exam-aws-devops-engineer-professional-topic-1-question-93/",
    "body": "A production account has a requirement that any Amazon EC2 instance that has been logged into manually must be terminated within 24 hours. All applications in the production account are using Auto Scaling groups with Amazon CloudWatch Logs agent configured.<br><br>How can this process be automated?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a CloudWatch Logs subscription to an AWS Step Functions application. Configure the function to add a tag to the EC2 instance that produced the login event and mark the instance to be decommissioned. Then create a CloudWatch Events rule to trigger a second AWS Lambda function once a day that will terminate all instances with this tag.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a CloudWatch alarm that will trigger on the login event. Send the notification to an Amazon SNS topic that the operations team is subscribed to, and have them terminate the EC2 instance within 24 hours.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a CloudWatch alarm that will trigger on the login event. Configure the alarm to send to an Amazon SQS queue. Use a group of worker instances to process messages from the queue, which then schedules the Amazon CloudWatch Events rule to trigger.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a CloudWatch Logs subscription in an AWS Lambda function. Configure the function to add a tag to the EC2 instance that produced the login event and mark the instance to be decommissioned. Create a CloudWatch Events rule to trigger a daily Lambda function that terminates all instances with this tag.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 8,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-11-30T22:25:00.000Z",
        "voteCount": 5,
        "content": "A - You can't create a log subscription to AWS Step Function. Only OpenSearch, Kinesis, Kinesis Firehouse and Lambda.\nB - is a manual process. It needs to be automated.\nC - Too complex. Architecture doesn't make any sense.\nD - Correct."
      },
      {
        "date": "2023-01-13T10:20:00.000Z",
        "voteCount": 1,
        "content": "Yes D seems to be a more direct approach but it is possible to create a log subscription to an AWS Step Functions application. CloudWatch Logs can be configured to send log data to a Step Functions state machine, which can then be used to process the log data and perform various actions based on the contents of the log data"
      },
      {
        "date": "2023-03-04T10:36:00.000Z",
        "voteCount": 1,
        "content": "also could not find step functions as an option for cw log subscription filter target - https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/SubscriptionFilters.html"
      },
      {
        "date": "2023-03-04T10:33:00.000Z",
        "voteCount": 1,
        "content": "I could not find CW logs in this list - https://docs.aws.amazon.com/step-functions/latest/dg/concepts-invoke-sfn.html"
      },
      {
        "date": "2023-06-15T20:43:00.000Z",
        "voteCount": 1,
        "content": "whenever you have to terminate instances, prefer using tags.. so option D is correct"
      },
      {
        "date": "2023-05-08T15:45:00.000Z",
        "voteCount": 1,
        "content": "Agree D"
      },
      {
        "date": "2023-02-09T03:42:00.000Z",
        "voteCount": 1,
        "content": "B is the most AWS-managed option"
      },
      {
        "date": "2023-01-22T17:36:00.000Z",
        "voteCount": 1,
        "content": "D is the correct answer. A is an overkill, B is not automated and C idoesnt make any sense as it does not address how EC2 instances will be terminated."
      },
      {
        "date": "2022-12-23T01:58:00.000Z",
        "voteCount": 1,
        "content": "DDDDDDDDDDDDDDD"
      },
      {
        "date": "2022-11-26T04:52:00.000Z",
        "voteCount": 2,
        "content": "Does CloudWatch alarm that will trigger on the login event have such an event?Step function it s use to step like batch exe,just two lambda.no needed"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 94,
    "url": "https://www.examtopics.com/discussions/amazon/view/88850-exam-aws-devops-engineer-professional-topic-1-question-94/",
    "body": "A company's application is running on Amazon EC2 instances in an Auto Scaling group. A DevOps engineer needs to ensure there are at least four application servers running at all times. Whenever an update has to be made to the application, the engineer creates a new AMI with the updated configuration and updates the AWS CloudFormation template with the new AMI ID. After the stack update finishes, the engineer manually terminates the old instances one by one, verifying that the new instance is operational before proceeding. The engineer needs to automate this process.<br><br>Which action will allow for the LEAST number of manual steps moving forward?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the CloudFormation template to include the UpdatePolicy attribute with the AutoScalingRollingUpdate policy.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the CloudFormation template to include the UpdatePolicy attribute with the AutoScalingReplacingUpdate policy.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an Auto Scaling lifecycle hook to verify that the previous instance is operational before allowing the DevOps engineer's selected instance to terminate.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an Auto Scaling lifecycle hook to confirm there are at least four running instances before allowing the DevOps engineer's selected instance to terminate."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 9,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-04-14T02:22:00.000Z",
        "voteCount": 2,
        "content": "https://repost.aws/knowledge-center/auto-scaling-group-rolling-updates\n\nOption B is incorrect because the AutoScalingReplacingUpdate policy terminates all instances in the Auto Scaling group simultaneously and creates new instances, which does not satisfy the requirement of having at least four application servers running at all times."
      },
      {
        "date": "2023-03-15T19:21:00.000Z",
        "voteCount": 2,
        "content": "its A like for Apple."
      },
      {
        "date": "2023-02-27T14:57:00.000Z",
        "voteCount": 2,
        "content": "A is correct\n\nAutoscaling Rolling Update:\n\nAutoscaling Rolling Update is a mechanism that allows for updates to be applied to instances in an autoscaling group in a rolling fashion. This means that a few instances are taken down at a time, updated, and then brought back up before the next set of instances are taken down. This process continues until all instances have been updated. Autoscaling Rolling Update helps to ensure that there is no downtime and that the application remains available throughout the update process.\n\nAutoscaling Replacing Update:\n\nAutoscaling Replacing Update is a mechanism that replaces the entire set of instances in an autoscaling group with new instances. Unlike Autoscaling Rolling Update, this method takes down all the old instances at once and replaces them with new instances. Autoscaling Replacing Update is faster than Autoscaling Rolling Update since it replaces all instances at once, but there is a period of downtime during the update process."
      },
      {
        "date": "2023-02-19T09:36:00.000Z",
        "voteCount": 1,
        "content": "Only B satisfies the scenario: \"After the stack update finishes, the engineer manually terminates the old instances one by one, verifying that the new instance is operational before proceeding. \"\n  https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-updatepolicy.html#cfn-attributes-updatepolicy-replacingupdate"
      },
      {
        "date": "2023-03-30T09:48:00.000Z",
        "voteCount": 6,
        "content": "Please think twice before you comment, so that people won't get confused by all your comments..."
      },
      {
        "date": "2023-02-09T03:50:00.000Z",
        "voteCount": 1,
        "content": "Both A and B cannot guarantee that \"at least four application servers running at all times\".\nC is involved by D."
      },
      {
        "date": "2023-02-09T03:51:00.000Z",
        "voteCount": 1,
        "content": "I changed my mind from D to A because of comment of adozoo"
      },
      {
        "date": "2023-02-09T02:05:00.000Z",
        "voteCount": 2,
        "content": "B - correct (in my opinion) - requirement: LEAST number of manual steps moving forward\nSolution: use AutoScalingReplacingUpdate policy and set\n\"AutoScalingReplacingUpdate\" : {\n     \"WillReplace\" : \"true\"\n}\nThe AutoScalingReplacingUpdate will be terminating instances ONLY after new instances are available and stable. \nA - also correct to some extent but the potential problem is related to roll back - with the rolling update you have to take care of auto scaling group configuration\nNote: If an unexpected scaling action changes the state of the Auto Scaling group during a rolling update, the update can fail. The failure can result from an inconsistent view of the Auto Scaling group by AWS CloudFormation."
      },
      {
        "date": "2023-01-22T18:12:00.000Z",
        "voteCount": 1,
        "content": "A is correct because of the ability to specify MinInstanceInService attributes on the autoscalinggroup update policy."
      },
      {
        "date": "2023-02-02T20:13:00.000Z",
        "voteCount": 1,
        "content": "I think the answer is B not A. With AutoScalingReplacing updatePolicy we specify whether an Auto Scaling group and the instances it contains are replaced during an update. During replacement, CloudFormation retains the old group ( 4 instances that were originally present) until it finishes creating the new one. If the update fails, CloudFormation can roll back to the old Auto Scaling group and delete the new Auto Scaling group.\n\nWhile CloudFormation creates the new group, it doesn't detach or attach any instances. After successfully creating the new Auto Scaling group, CloudFormation deletes the old Auto Scaling group during the cleanup process."
      },
      {
        "date": "2022-12-23T02:05:00.000Z",
        "voteCount": 1,
        "content": "AAAAAAAAAAAAAA"
      },
      {
        "date": "2022-11-26T05:01:00.000Z",
        "voteCount": 4,
        "content": "For rolling updates, you can specify whether AWS CloudFormation updates the instances in the Auto Scaling group in batches, or all instances at once."
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 95,
    "url": "https://www.examtopics.com/discussions/amazon/view/89782-exam-aws-devops-engineer-professional-topic-1-question-95/",
    "body": "A company using AWS CodeCommit for source control wants to automate its continuous integration and continuous delivery pipeline on AWS in its development environment. The company has three requirements:<br><br>1.\tThere must be a legal and a security review of any code change to make sure sensitive information is not leaked through the source code.<br>2.\tEvery change must go through unit testing.<br>3.\tEvery change must go through a suite of functional testing to ensure functionality.<br><br>In addition, the company has the following requirements for automation:<br><br>1.\tCode changes should automatically trigger the CI/CD pipeline.<br>2.\tAny failure in the pipeline should notify devops-admin@xyz.com.<br>3.\tThere must be an approval to stage the assets to Amazon S3 after tests have been performed.<br><br>What should a DevOps Engineer do to meet all of these requirements while following Cl/CD best practices?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCommit to the development branch and trigger AWS CodePipeline from the development branch. Make an individual stage in CodePipeline for security review, unit tests, functional tests, and manual approval. Use Amazon CloudWatch metrics to detect changes in pipeline stages and Amazon SES for emailing devops-admin@xyz.com.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCommit to mainline and trigger AWS CodePipeline from mainline. Make an individual stage in CodePipeline for security review, unit tests, functional tests, and manual approval. Use AWS CloudTrail logs to detect changes in pipeline stages and Amazon SNS for emailing devops-admin@xyz.com.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCommit to the development branch and trigger AWS CodePipeline from the development branch. Make an individual stage in CodePipeline for security review, unit tests, functional tests, and manual approval. Use Amazon CloudWatch Events to detect changes in pipeline stages and Amazon SNS for emailing devops-admin@xyz.com.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCommit to mainline and trigger AWS CodePipeline from mainline. Make an individual stage in CodePipeline for security review, unit tests, functional tests, and manual approval. Use Amazon CloudWatch Events to detect changes in pipeline stages and Amazon SES for emailing devops-admin@xyz.com."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 12,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-12-02T05:05:00.000Z",
        "voteCount": 6,
        "content": "Always Commit to the development branch not on mainline  and trigger AWS CodePipeline from the development branch. \nMake an individual stage in CodePipeline for security review, unit tests, functional tests, and manual approval.\nAmazon CloudWatch Events to detect changes in pipeline \"CodePipeline Stage Execution State Change\" and  Target a Amazon SNS topic."
      },
      {
        "date": "2023-03-21T12:51:00.000Z",
        "voteCount": 1,
        "content": "c-c-c-c-c-c-c--c-c-c-"
      },
      {
        "date": "2023-02-09T04:11:00.000Z",
        "voteCount": 2,
        "content": "B and D: what is \"from mainline\" ?\nA: CloudWatch metrics does not help to detect changes in pipeline stages"
      },
      {
        "date": "2023-01-22T18:24:00.000Z",
        "voteCount": 2,
        "content": "C is correct."
      },
      {
        "date": "2022-12-23T02:08:00.000Z",
        "voteCount": 1,
        "content": "CCCCCCCCCCCCCCCCCCCCCCCC"
      },
      {
        "date": "2022-12-15T04:39:00.000Z",
        "voteCount": 2,
        "content": "A and D - SES instead of SNS\nB - Cloudtrail does not logs for failed pipeline runs\nC - is the correct answer, SNS for email and Cloudwatch Events for failed triggers"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 96,
    "url": "https://www.examtopics.com/discussions/amazon/view/88854-exam-aws-devops-engineer-professional-topic-1-question-96/",
    "body": "A security review has identified that an AWS CodeBuild project is downloading a database population script from an Amazon S3 bucket using an unauthenticated request. The security team does not allow unauthenticated requests to S3 buckets for this project.<br><br>How can this issue be corrected in the MOST secure manner?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd the bucket name to the AllowedBuckets section of the CodeBuild project settings. Update the build spec to use the AWS CLI to download the database population script.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the S3 bucket settings to enable HTTPS basic authentication and specify a token. Update the build spec to use cURL to pass the token and download the database population script.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRemove unauthenticated access from the S3 bucket with a bucket policy. Modify the service role for the CodeBuild project to include Amazon S3 access. Use the AWS CLI to download the database population script.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRemove unauthenticated access from the S3 bucket with a bucket policy. Use the AWS CLI to download the database population script using an IAM access key and a secret access key."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-04-03T08:49:00.000Z",
        "voteCount": 1,
        "content": "Chatgpt said: C. The MOST secure solution to correct this issue would be to remove unauthenticated access from the S3 bucket with a bucket policy and use the AWS CLI to download the database population script using temporary security credentials obtained through an IAM role attached to the CodeBuild project.\n\nOption C partially addresses this by removing unauthenticated access from the S3 bucket and modifying the service role for the CodeBuild project to include Amazon S3 access. However, it does not address the need for secure access to the S3 bucket using temporary security credentials obtained through an IAM role attached to the CodeBuild project.\nTherefore, the correct answer is C with the addition of using temporary security credentials obtained through an IAM role attached to the CodeBuild project to access the S3 bucket."
      },
      {
        "date": "2023-03-21T02:14:00.000Z",
        "voteCount": 1,
        "content": "C Is the correct answer with this assessment you will get free access https://www.netcomlearning.com/en-us/assessment/36703/devops-engineering-aws.html?advid=1356"
      },
      {
        "date": "2023-03-20T18:08:00.000Z",
        "voteCount": 1,
        "content": "C is the way."
      },
      {
        "date": "2023-01-22T19:43:00.000Z",
        "voteCount": 4,
        "content": "C is correct. IAM role is a better practice than using IAM access key and secret access key."
      },
      {
        "date": "2023-01-13T19:55:00.000Z",
        "voteCount": 1,
        "content": "D is the correct answer.Option C is also a secure way to correct the issue.However, using an IAM access key and secret access key in addition to modifying the service role for the CodeBuild project is a more secure way to ensure that the CodeBuild project has the necessary permissions to access the S3 bucket."
      },
      {
        "date": "2022-12-23T02:11:00.000Z",
        "voteCount": 1,
        "content": "CCCCCCCCCCCCCCCCCC"
      },
      {
        "date": "2022-12-13T02:23:00.000Z",
        "voteCount": 1,
        "content": "B is the correct answer"
      },
      {
        "date": "2022-11-30T22:42:00.000Z",
        "voteCount": 4,
        "content": "C is correct. You need a role to access other AWS services.\n\nhttps://docs.aws.amazon.com/codebuild/latest/userguide/setting-up.html#setting-up-service-role"
      },
      {
        "date": "2022-11-26T05:08:00.000Z",
        "voteCount": 2,
        "content": "best practices"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 97,
    "url": "https://www.examtopics.com/discussions/amazon/view/88923-exam-aws-devops-engineer-professional-topic-1-question-97/",
    "body": "A company has multiple development groups working in a single shared AWS account. The senior manager of the groups wants to be alerted via a third-party API call when the creation of resources approaches the service limits for the account.<br><br>Which solution will accomplish this with the LEAST amount of development effort?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon CloudWatch Event rule that runs periodically and targets an AWS Lambda function. Within the Lambda function, evaluate the current state of the AWS environment and compare deployed resource values to resource limits on the account. Notify the senior manager if the account is approaching a service limit.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy an AWS Lambda function that refreshes AWS Trusted Advisor checks, and configure an Amazon CloudWatch Events rule to run the Lambda function periodically. Create another CloudWatch Events rule with an event pattern matching Trusted Advisor events and a target Lambda function. In the target Lambda function, notify the senior manager.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy an AWS Lambda function that refreshes AWS Personal Health Dashboard checks, and configure an Amazon CloudWatch Events rule to run the Lambda function periodically. Create another CloudWatch Events rule with an event pattern matching Personal Health Dashboard events and a target Lambda function. In the target Lambda function, notify the senior manager.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd an AWS Config custom rule that runs periodically, checks the AWS service limit status, and streams notifications to an Amazon SNS topic. Deploy an AWS Lambda function that notifies the senior manager, and subscribe the Lambda function to the SNS topic."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-11-30T22:47:00.000Z",
        "voteCount": 6,
        "content": "B. Any time there is a question about service limits, the answer is Trusted Advisor or Quota Monitor which still uses Trusted Advisor APIs.\nhttps://aws.amazon.com/solutions/implementations/quota-monitor/"
      },
      {
        "date": "2023-04-03T08:47:00.000Z",
        "voteCount": 1,
        "content": "Chatgpt said: B. The solution that will accomplish this with the LEAST amount of development effort is option B.\n\nOption A requires custom code to be developed to evaluate the current state of the AWS environment and compare deployed resource values to resource limits on the account. Option C requires custom code to be developed to refresh AWS Personal Health Dashboard checks, and option D requires custom code to be developed to add an AWS Config custom rule that streams notifications to an Amazon SNS topic.\n\nOption B leverages AWS Trusted Advisor, which provides automated checks for common AWS best practices, including service limits. A Lambda function can be deployed to refresh the Trusted Advisor checks, and a CloudWatch Events rule can be configured to run the Lambda function periodically. Another CloudWatch Events rule with an event pattern matching Trusted Advisor events can be configured to notify the senior manager via a third-party API call. This solution requires the least amount of development effort because it leverages existing AWS services and functionality, and does not require custom code to be developed."
      },
      {
        "date": "2023-10-01T05:40:00.000Z",
        "voteCount": 1,
        "content": "ChatGPT said answer is D"
      },
      {
        "date": "2023-03-20T18:13:00.000Z",
        "voteCount": 1,
        "content": "B-B-B-B"
      },
      {
        "date": "2023-02-09T04:22:00.000Z",
        "voteCount": 2,
        "content": "D is the simplest one"
      },
      {
        "date": "2023-01-22T19:55:00.000Z",
        "voteCount": 1,
        "content": "B is the correct answer."
      },
      {
        "date": "2023-01-20T18:09:00.000Z",
        "voteCount": 1,
        "content": "Ttusted Advisor allow you to check your account's service limits (quotas)\nhttps://docs.aws.amazon.com/awssupport/latest/user/service-limits.html"
      },
      {
        "date": "2023-01-09T15:01:00.000Z",
        "voteCount": 1,
        "content": "B is the answer"
      },
      {
        "date": "2022-12-23T03:18:00.000Z",
        "voteCount": 1,
        "content": "B is the answer, best practice"
      },
      {
        "date": "2022-11-29T21:00:00.000Z",
        "voteCount": 4,
        "content": "1. AWS Trusted Advisor can be used to access all checks in the Service Limits category.\nhttps://docs.aws.amazon.com/awssupport/latest/user/trusted-advisor.html\n2. AWS Quota Monitor would be more opt but is not listed as an option. Quota Monitor leverages Trusted Advisor."
      },
      {
        "date": "2022-11-27T00:34:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/config/latest/developerguide/configlimits.html"
      },
      {
        "date": "2023-02-09T04:24:00.000Z",
        "voteCount": 1,
        "content": "This link shows configuration on limits. We need to detect the limits are hit. \nI though D was correct. \nI changed my mind to B."
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 98,
    "url": "https://www.examtopics.com/discussions/amazon/view/89512-exam-aws-devops-engineer-professional-topic-1-question-98/",
    "body": "A DevOps engineer is architecting a continuous development strategy for a company's software as a service (SaaS) web application running on AWS. For application and security reasons, users subscribing to this application are distributed across multiple Application Load Balancers (ALBs), each of which has a dedicated Auto Scaling group and fleet of Amazon EC2 instances. The application does not require a build stage, and when it is committed to AWS CodeCommit, the application must trigger a simultaneous deployment to all ALBs, Auto Scaling groups, and EC2 fleets.<br><br>Which architecture will meet these requirements with the LEAST amount of configuration?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a single AWS CodePipeline pipeline that deploys the application in parallel using unique AWS CodeDeploy applications and deployment groups created for each ALB-Auto Scaling group pair.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a single AWS CodePipeline pipeline that deploys the application using a single AWS CodeDeploy application and single deployment group.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a single AWS CodePipeline pipeline that deploys the application in parallel using a single AWS CodeDeploy application and unique deployment group for each ALB-Auto Scaling group pair.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS CodePipeline pipeline for each ALB-Auto Scaling group pair that deploys the application using an AWS CodeDeploy application and deployment group created for the same ALB-Auto Scaling group pair."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 6,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-04-03T08:45:00.000Z",
        "voteCount": 1,
        "content": "Chatgpt said: B. \nThe architecture that will meet the requirements with the LEAST amount of configuration is option B, which is to create a single AWS CodePipeline pipeline that deploys the application using a single AWS CodeDeploy application and single deployment group.\n\nThis approach is the simplest and most straightforward way to achieve simultaneous deployment to all ALBs, Auto Scaling groups, and EC2 fleets. The same CodeDeploy application and deployment group can be used across all ALBs and Auto Scaling groups without any additional configuration needed."
      },
      {
        "date": "2023-03-21T12:57:00.000Z",
        "voteCount": 1,
        "content": "c-c-c-c--c-c-c-c-"
      },
      {
        "date": "2023-02-18T05:02:00.000Z",
        "voteCount": 2,
        "content": "A: not correct because of \"using unique AWS CodeDeploy applications\".  There is only one application.\nB: not correct because it's not complete.\nD: not correct. It must be wrong to create CodePipelines for ALB-AutoScaling group respectively."
      },
      {
        "date": "2023-01-22T19:59:00.000Z",
        "voteCount": 1,
        "content": "Answer is C with least amount of configuration"
      },
      {
        "date": "2023-01-16T07:01:00.000Z",
        "voteCount": 1,
        "content": "For me - C"
      },
      {
        "date": "2022-12-27T19:42:00.000Z",
        "voteCount": 1,
        "content": "https://www.examtopics.com/discussions/amazon/view/28608-exam-aws-devops-engineer-professional-topic-1-question-177/"
      },
      {
        "date": "2022-12-24T01:30:00.000Z",
        "voteCount": 1,
        "content": "A is the answer\n\nCreate a single AWS CodePipeline pipeline that deploys the application in parallel using unique AWS CodeDeploy applications and deployment groups created for each ALB-Auto Scaling group pair."
      },
      {
        "date": "2022-11-30T22:58:00.000Z",
        "voteCount": 2,
        "content": "C is correct, but there is a limitation that one deployment group can deploy up to 10 ASGs. The question doesn't say how many ASGs are there, it just says multiple."
      },
      {
        "date": "2022-12-02T05:27:00.000Z",
        "voteCount": 2,
        "content": "why limiation?   it says clearly a different ASG per deployment group.\n\n\"\"unique deployment group for each ALB-Auto Scaling group pair.\"\""
      },
      {
        "date": "2022-12-02T05:27:00.000Z",
        "voteCount": 1,
        "content": "***typo***\nwhy limitation? it says clearly a different ASG per deployment group.\n\"unique deployment group for each ALB-Auto Scaling group pair.\""
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 99,
    "url": "https://www.examtopics.com/discussions/amazon/view/89786-exam-aws-devops-engineer-professional-topic-1-question-99/",
    "body": "A company's primary AWS Region contains the following infrastructure:<br><br>\u2022\tAn Amazon S3 bucket that contains an object package that is used in instance user data to configure an application.<br>\u2022\tAmazon EC2 instances in an Auto Scaling group behind an Application Load Balancer (ALB) with an instance profile that grants s3:Get* access on the S3 bucket.<br><br>The company has the following infrastructure in a backup Region:<br><br>\u2022\tAn S3 bucket with the same configuration as the S3 bucket in the primary AWS Region, but without any objects.<br>\u2022\tEC2 instances in an Auto Scaling group behind an ALB that run with the same configuration as in the primary AWS Region.<br><br>To simulate a disaster recovery scenario, the company turns off all access to Amazon S3 and sets the Auto Scaling group's minimum, maximum, and desired instances to 0 in the primary Region. When the instances in the backup Region scale out, they do not pass Amazon Route 53 health checks.<br><br>Which combination of steps should the company take to resolve this issue? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the Amazon EC2 Auto Scaling service-linked role to allow access to both S3 buckets.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up S3 Cross-Region Replication from the S3 bucket in the primary Region to the S3 bucket in the backup Region.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the instance user data to reference the S3 bucket in the primary Region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the timeout for the target group health check.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the EC2 instance profile to allow s3:list* actions.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the EC2 instance profile to allow read access to both S3 buckets.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "BDF",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BDF",
        "count": 21,
        "isMostVoted": true
      },
      {
        "answer": "BDE",
        "count": 4,
        "isMostVoted": false
      },
      {
        "answer": "BCE",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "BCF",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "ABF",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-11T10:45:00.000Z",
        "voteCount": 17,
        "content": "B - To ensure the secondary S3 contains the required files needed\nD - Provide enough time to download the S3 file and run user data\nF - Since it's a DR scenario and the primary S3 can't be read from we should read from secondary S3. Providing the instance profile read access to both allows it to be used seamlessly before and during the DR scenario"
      },
      {
        "date": "2023-04-03T08:42:00.000Z",
        "voteCount": 1,
        "content": "Chatgpt said: BCF. The three steps that the company should take to resolve this issue are:\n\nB. Set up S3 Cross-Region Replication from the S3 bucket in the primary Region to the S3 bucket in the backup Region. This will ensure that the objects in the primary Region's S3 bucket are automatically replicated to the backup Region's S3 bucket.\n\nC. Update the instance user data to reference the S3 bucket in the primary Region. Since the objects have been replicated to the backup Region, updating the instance user data to reference the S3 bucket in the primary Region will ensure that the instances in the backup Region are configured correctly.\n\nF. Update the EC2 instance profile to allow read access to both S3 buckets. This will ensure that the instances in the backup Region have the necessary permissions to read objects from both the primary and backup Regions' S3 buckets.\nThe other options do not address the issue"
      },
      {
        "date": "2023-03-21T13:06:00.000Z",
        "voteCount": 1,
        "content": "B - Set up S3 Cross-Region Replication from the S3 bucket in the primary Region to the S3 bucket in the backup Region to ensure the backup S3 bucket has the necessary objects needed by the instances to function properly.\n\nD - Increase the timeout for the target group health check to provide enough time for the instances to download the object package from S3 and run the user data.\n\nF - Update the EC2 instance profile to allow read access to both S3 buckets to ensure the EC2 instances have the necessary permissions to access the object package in the S3 bucket."
      },
      {
        "date": "2023-02-11T04:31:00.000Z",
        "voteCount": 2,
        "content": "BDF\nA - wrong - auto scaling will not work with S3, this is EC2 that needs to download the package\nC - wrong - S3 in the primary region will be unavailable\nE - wrong - you don't need to list the content of S3, you need to get from it"
      },
      {
        "date": "2023-02-09T05:09:00.000Z",
        "voteCount": 1,
        "content": "The reason can be 1. The backup S3 bucket does not have objects. 2. The backup EC2 instances read the primary S3 bucket  3. Permission"
      },
      {
        "date": "2023-01-22T20:06:00.000Z",
        "voteCount": 2,
        "content": "BDF for the reasons provided by jaxsbr"
      },
      {
        "date": "2022-12-13T08:57:00.000Z",
        "voteCount": 4,
        "content": "BDF for me."
      },
      {
        "date": "2022-12-11T02:49:00.000Z",
        "voteCount": 3,
        "content": "Set up S3 Cross-Region Replication from the S3 bucket in the primary Region to the S3 bucket in the backup Region. This will ensure that the S3 bucket in the backup Region contains the necessary object package needed to configure the application.\n\n\nUpdate the instance user data to reference the S3 bucket in the primary Region. This will ensure that the instances in the backup Region are configured to use the object package from the S3 bucket in the primary Region.\n\nOption D, increasing the timeout for the target group health check, may also be a helpful step to take, as it may give the instances in the backup Region more time to pass the health check. However, the other options are not necessary for resolving this issue."
      },
      {
        "date": "2022-12-02T06:24:00.000Z",
        "voteCount": 4,
        "content": "options need access to both s3 buckets ruled out on DR region you need access to that region bucket with CCR enabled from primary region bucket.\n\nAs we need 3rd option also to select, increasing timeout on R53 selected."
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 100,
    "url": "https://www.examtopics.com/discussions/amazon/view/89418-exam-aws-devops-engineer-professional-topic-1-question-100/",
    "body": "A company is developing a web application's infrastructure using AWS CloudFormation. The database engineering team maintains the database resources in a CloudFormation template, and the software development team maintains the web application resources in a separate CloudFormation template. As the scope of the application grows, the software development team needs to use resources maintained by the database engineering team. However, both teams have their own review and lifecycle management processes that they want to keep. Both teams also require resource-level change-set reviews. The software development team would like to deploy changes to this template using their CI/CD pipeline.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a stack export from the database CloudFormation template and import those references into the web application CloudFormation template.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a CloudFormation nested stack to make cross-stack resource references and parameters available in both stacks.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a CloudFormation stack set to make cross-stack resource references and parameters available in both stacks.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate input parameters in the web application CloudFormation template and pass resource names and IDs from the database stack."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 25,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 19,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-11T02:53:00.000Z",
        "voteCount": 11,
        "content": "The correct answer is B. Creating a CloudFormation nested stack allows the software development team to make cross-stack resource references and parameters available in both stacks, while still maintaining separate review and lifecycle management processes for each team. In this way, the software development team can use resources maintained by the database engineering team in their CloudFormation template, and can deploy changes to their template using their CI/CD pipeline."
      },
      {
        "date": "2023-01-09T17:14:00.000Z",
        "voteCount": 7,
        "content": "A is correct\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/best-practices.html\n\nWhen you organize your AWS resources based on lifecycle and ownership, you might want to build a stack that uses resources that are in another stack. You can hardcode values or use input parameters to pass resource names and IDs. However, these methods can make templates difficult to reuse or can increase the overhead to get a stack running. Instead, use cross-stack references to export resources from a stack so that other stacks can use them. Stacks can use the exported resources by calling them using the Fn::ImportValue function."
      },
      {
        "date": "2024-01-18T00:25:00.000Z",
        "voteCount": 1,
        "content": "B will satisfy both teams"
      },
      {
        "date": "2023-04-14T02:53:00.000Z",
        "voteCount": 1,
        "content": "When you organize your AWS resources based on lifecycle and ownership, you might want to build a stack that uses resources that are in another stack. You can hardcode values or use input parameters to pass resource names and IDs. However, these methods can make templates difficult to reuse or can increase the overhead to get a stack running. Instead, use cross-stack references to export resources from a stack so that other stacks can use them. Stacks can use the exported resources by calling them using the Fn::ImportValue function."
      },
      {
        "date": "2023-04-03T08:41:00.000Z",
        "voteCount": 1,
        "content": "Chatgpt said: Option B: Create a CloudFormation nested stack to make cross-stack resource references and parameters available in both stacks would be the best solution to meet these requirements.\n\nA CloudFormation nested stack is a stack that can be used as a resource within another stack, which allows for cross-stack resource references and parameters. This will enable the software development team to reference and use resources from the database engineering team's CloudFormation template without having to maintain them in their own template.\n\nUsing a nested stack also allows both teams to maintain their own review and lifecycle management processes, as each team can manage their own stack independently. Additionally, the resource-level change-set reviews can be implemented on both the parent and nested stack, which provides a comprehensive review process."
      },
      {
        "date": "2023-03-21T13:10:00.000Z",
        "voteCount": 1,
        "content": "It's B and here's why:\nA CloudFormation nested stack allows you to create a stack as a set of AWS resources within another stack, enabling you to break down complex stacks into smaller, more manageable stacks. The nested stack can be used to create a set of resources that are managed and maintained by a different team or individual while still maintaining control over the resources by the parent stack. By using nested stacks, both teams can maintain their own review and lifecycle management processes while still enabling cross-stack resource references and parameter sharing"
      },
      {
        "date": "2023-02-22T15:43:00.000Z",
        "voteCount": 3,
        "content": "The answer is not nested stacks as you cannot perform change sets on your own. \nYou need a root stack for that and who manages the root stack?"
      },
      {
        "date": "2023-02-21T20:28:00.000Z",
        "voteCount": 2,
        "content": "B does not make sense to me. If you want the straps to be independent from one another you don\u2019t use nested stacks"
      },
      {
        "date": "2023-02-21T20:35:00.000Z",
        "voteCount": 2,
        "content": "https://blog.shikisoft.com/cloudformation-nested-stacks-vs-cross-stack-references/\n\n\u201cAlternatively, if you need to manage your stacks as separate entities, you should use cross-stack references.\u201d"
      },
      {
        "date": "2023-02-20T16:30:00.000Z",
        "voteCount": 2,
        "content": "B.\nThis will satisfy both teams' requirements."
      },
      {
        "date": "2023-02-19T13:48:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is B\nNested Stacks are a great way to deploy your infrastructure in a modular fashion\""
      },
      {
        "date": "2023-02-19T13:47:00.000Z",
        "voteCount": 1,
        "content": "Nested Stacks are a great way to deploy your infrastructure in a modular fashion\""
      },
      {
        "date": "2023-02-15T09:03:00.000Z",
        "voteCount": 2,
        "content": "I'm not quite sure how this website calculates the proportions of the answers, as in this question there are more users who have voted A than B, at least a tie, and yet it gives more percentage to B...\nIn fact, in my opinion, the correct answer is A.\nBut come on, I'm not saying it's A, it's what I think, I'm saying that I don't know how to get the proportion....."
      },
      {
        "date": "2023-03-30T10:32:00.000Z",
        "voteCount": 1,
        "content": "People select the answer when they comment. If they don't, then their answer is not considered in the \"proportion\"... which is why even many prefer A, they didn't select the answer and B is still shown as the most selected answer."
      },
      {
        "date": "2023-02-14T17:36:00.000Z",
        "voteCount": 2,
        "content": "Option B is the best solution in this case. A nested stack is a stack that is created and managed as part of another stack. By creating a nested stack, both the database engineering team and the software development team can maintain their own CloudFormation templates and associated processes. The database engineering team can deploy their database stack as usual, and the software development team can create a nested stack that references the database stack's resources using cross-stack references."
      },
      {
        "date": "2023-02-09T05:22:00.000Z",
        "voteCount": 3,
        "content": "B and C are eliminated because software development team needs to use database engineering team, not vice verse. \nA is more reasonable than D."
      },
      {
        "date": "2023-02-14T17:38:00.000Z",
        "voteCount": 1,
        "content": "Option A, using stack exports and imports, can allow cross-stack references, but it does not provide a way to maintain separate CloudFormation templates or allow for resource-level change-set reviews.\n\nOption D, passing resource names and IDs as input parameters from the database stack to the web application stack, can work, but it requires manual updates to the web application stack whenever the database stack changes. This solution can be error-prone and time-consuming to maintain.\n\nSo, option B"
      },
      {
        "date": "2023-01-31T14:11:00.000Z",
        "voteCount": 5,
        "content": "DB and web teams want to maintain the stack lifecycle separately ==&gt; cross-stacks..."
      },
      {
        "date": "2023-01-22T20:08:00.000Z",
        "voteCount": 4,
        "content": "A is correct."
      },
      {
        "date": "2023-01-16T07:16:00.000Z",
        "voteCount": 5,
        "content": "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/best-practices.html#cross-stack"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 101,
    "url": "https://www.examtopics.com/discussions/amazon/view/89516-exam-aws-devops-engineer-professional-topic-1-question-101/",
    "body": "A company uses Amazon S3 to store proprietary information. The development team creates buckets for new projects on a daily basis. The security team wants to ensure that all existing and future buckets have encryption, logging, and versioning enabled. Additionally, no buckets should ever be publicly read or write accessible.<br><br>What should a DevOps engineer do to meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable AWS CloudTrail and configure automatic remediation using AWS Lambda.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable AWS Config rules and configure automatic remediation using AWS Systems Manager documents.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable AWS Trusted Advisor and configure automatic remediation using Amazon CloudWatch Events.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable AWS Systems Manager and configure automatic remediation using Systems Manager documents."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 12,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-12-11T02:55:00.000Z",
        "voteCount": 5,
        "content": "B. Enable AWS Config rules and configure automatic remediation using AWS Systems Manager documents.\n\nTo meet the requirements specified in the question, the DevOps engineer should enable AWS Config rules and use AWS Systems Manager documents to automate the process of ensuring that all existing and future Amazon S3 buckets have encryption, logging, and versioning enabled, and that no buckets are publicly readable or writable. AWS Config rules allow the security team to specify rules for how resources should be configured in their AWS environment, and AWS Systems Manager documents can be used to automate the process of remedying any non-compliant resources."
      },
      {
        "date": "2023-03-21T13:13:00.000Z",
        "voteCount": 2,
        "content": "b-b-b-b-b-b-b-\nBy using AWS Config rules, the DevOps engineer can ensure that all existing and future S3 buckets have encryption, logging, and versioning enabled. The DevOps engineer can then use AWS Systems Manager documents to automatically remediate any noncompliant resources, ensuring that all S3 buckets remain secure."
      },
      {
        "date": "2023-03-04T12:31:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/config/latest/developerguide/remediation.html"
      },
      {
        "date": "2023-02-09T05:43:00.000Z",
        "voteCount": 1,
        "content": "A :  CloudTrail+Lambda is  ....\nB :  Looks nice\nC :  ....\nD : ...."
      },
      {
        "date": "2023-01-22T20:16:00.000Z",
        "voteCount": 1,
        "content": "Answer is B"
      },
      {
        "date": "2022-12-23T05:17:00.000Z",
        "voteCount": 1,
        "content": "BBBBBBBBBBBBBB"
      },
      {
        "date": "2022-11-30T23:15:00.000Z",
        "voteCount": 4,
        "content": "B - correct. Anytime there is something regarding compliance and enforcement, your best bet is AWS Config."
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 102,
    "url": "https://www.examtopics.com/discussions/amazon/view/89072-exam-aws-devops-engineer-professional-topic-1-question-102/",
    "body": "A company runs an application on one Amazon EC2 instance. Application metadata is stored in Amazon S3 and must be retrieved if the instance is restarted. The instance must restart or relaunch automatically if the instance becomes unresponsive.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon CloudWatch alarm for the StatusCheckFailed metric. Use the recover action to stop and start the instance. Use an S3 event notification to push the metadata to the instance when the instance is back up and running.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure AWS OpsWorks, and use the auto healing feature to stop and start the instance. Use a lifecycle event in OpsWorks to pull the metadata from Amazon S3 and update it on the instance.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse EC2 Auto Recovery to automatically stop and start the instance in case of a failure. Use an S3 event notification to push the metadata to the instance when the instance is back up and running.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS CloudFormation to create an EC2 instance that includes the UserData property for the EC2 resource. Add a command in UserData to retrieve the application metadata from Amazon S3."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 10,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-05-29T15:32:00.000Z",
        "voteCount": 1,
        "content": "Identical to Question #: 24"
      },
      {
        "date": "2023-03-04T12:33:00.000Z",
        "voteCount": 1,
        "content": "Same reasons as Bulti"
      },
      {
        "date": "2023-02-17T06:55:00.000Z",
        "voteCount": 1,
        "content": "A and C are eliminated, because S3 event notification cannot satisfy the context that the instance will be started when it does not respond. \nD does not work."
      },
      {
        "date": "2023-01-23T17:04:00.000Z",
        "voteCount": 2,
        "content": "B is correct because A and C are technically incorrect and D is incomplete in terms of meeting the requirements."
      },
      {
        "date": "2023-01-16T07:36:00.000Z",
        "voteCount": 1,
        "content": "https://www.examtopics.com/discussions/amazon/view/47002-exam-aws-devops-engineer-professional-topic-1-question-199/"
      },
      {
        "date": "2022-12-15T07:46:00.000Z",
        "voteCount": 4,
        "content": "A and C - S3 event notification are triggered when the objects change in S3.\nD - Only half of solution. No mention of how the instance will be recovered\nB - Is the correct option."
      },
      {
        "date": "2022-12-02T00:26:00.000Z",
        "voteCount": 2,
        "content": "Agree with @smileyCloud"
      },
      {
        "date": "2022-12-01T23:09:00.000Z",
        "voteCount": 2,
        "content": "I'll go with B\n\nA) and C) are wrong because there is no such thing like:\n\"Use a trigger in Amazon S3 to push the metadata to the instance when it is back up and running\"\n\nThere is no information about updating or putting a new metadata file to S3, so you can't create an event if nothing happens to the bucket.\nAlso there is no way to push from s3 to ec2 instance, that's not the case\nD) is incomplete"
      },
      {
        "date": "2022-11-30T23:26:00.000Z",
        "voteCount": 3,
        "content": "While A and C seem straightforward, it doesn't say who's gonna trigger the S3 event notification. This event happens only if there is a change in S3, not EC2. \nLooks strange, but B is the correct answer."
      },
      {
        "date": "2022-11-28T06:12:00.000Z",
        "voteCount": 3,
        "content": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/UsingAlarmActions.html"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 103,
    "url": "https://www.examtopics.com/discussions/amazon/view/89074-exam-aws-devops-engineer-professional-topic-1-question-103/",
    "body": "A devops team uses AWS CloudFormation to build their infrastructure. The security team is concerned about sensitive parameters, such as passwords, being exposed.<br><br>Which combination of steps will enhance the security of AWS CloudFormation? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a secure string with AWS KMS and choose a KMS encryption key. Reference the ARN of the secure string, and give AWS CloudFormation permission to the KMS key for decryption.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate secrets using the AWS Secrets Manager AWS::SecretsManager::Secret resource type. Reference the secret resource return attributes in resources that need a password, such as an Amazon RDS database.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore sensitive static data as secure strings in the AWS Systems Manager Parameter Store. Use dynamic references in the resources that need access to the data.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore sensitive static data in the AWS Systems Manager Parameter Store as strings. Reference the stored value using types of Systems Manager parameters.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS KMS to encrypt the CloudFormation template.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the CloudFormation NoEcho parameter property to mask the parameter value.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "BCF",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BCF",
        "count": 13,
        "isMostVoted": true
      },
      {
        "answer": "BDF",
        "count": 4,
        "isMostVoted": false
      },
      {
        "answer": "BCE",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-03-04T12:44:00.000Z",
        "voteCount": 1,
        "content": "Combination of BCF will secure both secrets and sensitive data"
      },
      {
        "date": "2023-02-15T11:54:00.000Z",
        "voteCount": 1,
        "content": "Between C and D: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/dynamic-references.html"
      },
      {
        "date": "2023-02-15T11:55:00.000Z",
        "voteCount": 1,
        "content": "And this link: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/parameters-section-structure.html#aws-ssm-parameter-types"
      },
      {
        "date": "2023-02-15T12:00:00.000Z",
        "voteCount": 1,
        "content": "Sorry,  BCF is my choice. \nSince we have used C, NoEcho would be sufficient.  \nIt's weird to encrypt the whole template.\nThe first \"Important\" window under the NoEcho hits the point. \nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/parameters-section-structure.html"
      },
      {
        "date": "2023-02-11T05:12:00.000Z",
        "voteCount": 3,
        "content": "A - wrong - I even don't understand what the author wanted to do\nD - wrong - Rather than embedding sensitive information directly in your CloudFormation templates, we recommend you use dynamic parameters in the stack template to reference sensitive information that is stored and managed outside of CloudFormation, such as in the AWS Systems Manager Parameter Store or AWS Secrets Manager.\nE - wrong - having dynamic parameters we don't need encryption"
      },
      {
        "date": "2023-02-09T05:58:00.000Z",
        "voteCount": 1,
        "content": "F: NoEcho is not sufficient"
      },
      {
        "date": "2023-01-23T17:13:00.000Z",
        "voteCount": 2,
        "content": "The main thing to understand here is either Secrets Manager or SSM parameter store should be used to encrypt sensitive information. Usually for passwords or cert keys  use Secrets Manager so that you can rotate the password or cert key  periodically  and for all other static sensitive data that doesn't need to change periodically use SSM parameter store."
      },
      {
        "date": "2022-12-27T21:32:00.000Z",
        "voteCount": 1,
        "content": "BCF -- Correct\nD is wrong as it is simple string and C is secure string"
      },
      {
        "date": "2022-12-04T13:01:00.000Z",
        "voteCount": 3,
        "content": "BCF.\nWhy somebody picks \"D\" instead of \"C\" ?   C = secure strings, D = simple strings. \"C\" should be correct"
      },
      {
        "date": "2022-12-02T01:10:00.000Z",
        "voteCount": 3,
        "content": "Between C &amp; D,  Prefer C due to security concerns, C allows dynamic reference which doesn't store/sow password.\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/dynamic-references.html"
      },
      {
        "date": "2022-12-01T23:12:00.000Z",
        "voteCount": 2,
        "content": "I'll go with B, C, F\n\nA) is not the use case\nD) insecure, you can still see the sensitive value\nE) encrypting the template do not prevent the echo and it is not effective"
      },
      {
        "date": "2022-11-30T23:30:00.000Z",
        "voteCount": 2,
        "content": "BDF makes most sense. KMS is not used for storing secrets. It's either Secrets Manager or Systems Manager."
      },
      {
        "date": "2022-11-28T06:20:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/parameters-section-structure.html"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 104,
    "url": "https://www.examtopics.com/discussions/amazon/view/89748-exam-aws-devops-engineer-professional-topic-1-question-104/",
    "body": "A company has a mission-critical application on AWS that uses automatic scaling. The company wants the deployment lifecycle to meet the following parameters:<br><br>\u2022\tThe application must be deployed one instance at a time to ensure the remaining fleet continues to serve traffic.<br>\u2022\tThe application is CPU intensive and must be closely monitored.<br>\u2022\tThe deployment must automatically roll back if the CPU utilization of the deployment instance exceeds 85%.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS CloudFormation to create an AWS Step Functions state machine and Auto Scaling lifecycle hooks to move to one instance at a time into a wait state. Use AWS Systems Manager automation to deploy the update to each instance and move it back into the Auto Scaling group using the heartbeat timeout.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS CodeDeploy with Amazon EC2 Auto Scaling. Configure an alarm tied to the CPU utilization metric. Use the CodeDeployDefault.OneAtAtime configuration as a deployment strategy. Configure automatic rollbacks within the deployment group to roll back the deployment if the alarm thresholds are breached.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Elastic Beanstalk for load balancing and AWS Auto Scaling. Configure an alarm tied to the CPU utilization metric. Configure rolling deployments with a fixed batch size of one instance. Enable enhanced health to monitor the status of the deployment and roll back based on the alarm previously created.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Systems Manager to perform a blue/green deployment with Amazon EC2 Auto Scaling. Configure an alarm tied to the CPU utilization metric. Deploy updates one at a time. Configure automatic rollbacks within the Auto Scaling group to roll back the deployment if the alarm thresholds are breached."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-03-04T12:47:00.000Z",
        "voteCount": 1,
        "content": "CodeDeploy One Instance at a time with rollback logic based on cloudwatch events"
      },
      {
        "date": "2023-02-17T20:13:00.000Z",
        "voteCount": 1,
        "content": "It's not C as there is no such thing as roll back based on alarms for Elastic Beanstalk"
      },
      {
        "date": "2023-02-09T06:12:00.000Z",
        "voteCount": 2,
        "content": "C is eliminated, Elastic Beanstalk is not the correct product.\nA is eliminated, does not mention the CPU limit.\nD is eliminated, Blue/Green ...."
      },
      {
        "date": "2023-01-23T18:21:00.000Z",
        "voteCount": 1,
        "content": "Answer is B"
      },
      {
        "date": "2022-12-23T05:29:00.000Z",
        "voteCount": 1,
        "content": "this is B answer"
      },
      {
        "date": "2022-12-01T23:15:00.000Z",
        "voteCount": 4,
        "content": "B\nhttps://www.examtopics.com/discussions/amazon/view/28607-exam-aws-devops-engineer-professional-topic-1-question-176/"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 105,
    "url": "https://www.examtopics.com/discussions/amazon/view/89519-exam-aws-devops-engineer-professional-topic-1-question-105/",
    "body": "A company's legacy application uses IAM user credentials to access resources in the company's AWS Organizations organization. A DevOps engineer must ensure that new IAM users cannot be created unless the employee who creates the IAM user is on an exception list.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAttach an Organizations SCP with an explicit deny for all iam:CreateAccessKey actions with a condition that excludes StringEquals for aws:username with a value of the exception list.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAttach an Organizations SCP with an explicit deny for all iam:CreateUser actions with a condition that includes StringNotLike for aws:username with a value of the exception list.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon EventBridge (Amazon CloudWatch Events) rule with a pattern that matches the iam:CreateAccessKey action with an AWS Lambda function target. The function will check the user name and account against an exception list. If the user is not on the exception list, the function will delete the user.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon EventBridge (Amazon CloudWatch Events) rule with a pattern that matches the iam:CreateUser action with an AWS Lambda function target. The function will check the user name and account against an exception list. If the user is not on the exception list, the function will delete the user."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-02-09T06:47:00.000Z",
        "voteCount": 1,
        "content": "C and D are not safe enough.\nA denies CreateAccessKey.\nB matches the scenario."
      },
      {
        "date": "2023-01-23T18:24:00.000Z",
        "voteCount": 1,
        "content": "B is the right answer"
      },
      {
        "date": "2022-12-23T05:31:00.000Z",
        "voteCount": 1,
        "content": "BBBBBBBBBBBB"
      },
      {
        "date": "2022-11-30T23:38:00.000Z",
        "voteCount": 3,
        "content": "B. \nhttps://asecure.cloud/a/scp_deny_iam_user_creation_w_exception/"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 106,
    "url": "https://www.examtopics.com/discussions/amazon/view/89840-exam-aws-devops-engineer-professional-topic-1-question-106/",
    "body": "A company must collect user consent to a privacy agreement. The company deploys an application in six AWS Regions: two Regions in North America, two Regions in Europe, and two Regions in Asia. The application has a user base of 20 million to 30 million users.<br><br>The company needs to read and write data that is related to each user's response. The company also must ensure that the responses are available in all six Regions.<br><br>Which solution will meet these requirements with the LOWEST latency of reads and writes?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement Amazon DocumentDB (with MongoDB compatibility) in each of the six Regions.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement Amazon DynamoDB global tables in each of the six Regions.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement Amazon ElastiCache for Redis replication groups in each of the six Regions.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement Amazon Elasticsearch Service (Amazon ES) in each of the six Regions."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-03-04T13:34:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GlobalTables.html"
      },
      {
        "date": "2023-01-23T18:25:00.000Z",
        "voteCount": 1,
        "content": "Answer is B"
      },
      {
        "date": "2022-12-23T05:32:00.000Z",
        "voteCount": 1,
        "content": "response B"
      },
      {
        "date": "2022-12-11T03:56:00.000Z",
        "voteCount": 4,
        "content": "the best solution for meeting the requirements with the lowest latency of reads and writes would be to implement Amazon DynamoDB global tables in each of the six Regions. DynamoDB global tables automatically replicate data across multiple Regions, allowing for low-latency reads and writes. Additionally, global tables support multi-master writes, which means that each Region can handle read and write requests for all of the users, improving the scalability of the solution."
      },
      {
        "date": "2022-12-03T03:36:00.000Z",
        "voteCount": 1,
        "content": "I will go with B\nhttps://aws.amazon.com/dynamodb/global-tables/"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 107,
    "url": "https://www.examtopics.com/discussions/amazon/view/89520-exam-aws-devops-engineer-professional-topic-1-question-107/",
    "body": "A company is testing a web application that runs on Amazon EC2 instances behind an Application Load Balancer. The instances run in an Auto Scaling group across multiple Availability Zones. The company uses a blue/green deployment process with immutable instances when deploying new software.<br><br>During testing, users are being automatically logged out of the application at random times. Testers also report that, when a new version of the application is deployed, all users are logged out. The development team needs a solution to ensure users remain logged in across scaling events and application deployments.<br><br>What is the MOST efficient way to ensure users remain logged in?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable smart sessions on the load balancer and modify the application to check for an existing session.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable session sharing on the load balancer and modify the application to read from the session store.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore user session information in an Amazon S3 bucket and modify the application to read session information from the bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the application to store user session information in an Amazon ElastiCache cluster.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 7,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-03-04T13:40:00.000Z",
        "voteCount": 1,
        "content": "https://aws.amazon.com/caching/session-management/"
      },
      {
        "date": "2023-02-24T16:24:00.000Z",
        "voteCount": 1,
        "content": "Enabling session sharing on the load balancer allows user session information to be shared across multiple instances in an Auto Scaling group. This ensures that users remain logged in even when instances are added or removed from the group."
      },
      {
        "date": "2023-02-24T16:23:00.000Z",
        "voteCount": 1,
        "content": "Enabling session sharing on the load balancer allows user session information to be shared across multiple instances in an Auto Scaling group. This ensures that users remain logged in even when instances are added or removed from the group."
      },
      {
        "date": "2023-01-23T18:28:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2022-12-23T05:34:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2022-12-11T03:58:00.000Z",
        "voteCount": 3,
        "content": "The most efficient way to ensure users remain logged in is to modify the application to store user session information in an Amazon ElastiCache cluster. Amazon ElastiCache is a fully managed in-memory data store that can be used to store user session information. The in-memory nature of ElastiCache means that it can store and retrieve session data quickly, which is important for maintaining user sessions in a web application. Additionally, ElastiCache clusters can be scaled up or down as needed, making it a good choice for a web application that is running on an Auto Scaling group."
      },
      {
        "date": "2022-11-30T23:44:00.000Z",
        "voteCount": 2,
        "content": "D - correct. \n\nA,B - Smart session and session sharing do not mean anything.\n\nhttps://aws.amazon.com/getting-started/hands-on/building-fast-session-caching-with-amazon-elasticache-for-redis/"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 108,
    "url": "https://www.examtopics.com/discussions/amazon/view/89749-exam-aws-devops-engineer-professional-topic-1-question-108/",
    "body": "A DevOps engineer is troubleshooting deployments to a new application that runs on Amazon EC2 instances behind an Application Load Balancer. The instances run in an EC2 Auto Scaling group across multiple Availability Zones. Instances sometimes come online before they are ready, which is leading to increased error rates among users. The current health check configuration gives instances a 60-second grace period and considers instances healthy after two 200 response codes from /index.php, a page that may respond intermittently during the deployment process. The development team wants instances to come online as soon as possible.<br><br>Which strategy would address this issue?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the instance grace period from 60 seconds to 180 seconds, and the consecutive health check requirement from 2 to 3.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the instance grace period from 60 seconds to 120 seconds, and change the response code requirement from 200 to 204.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the deployment script to create a /health-check.php file when the deployment begins, then modify the health check path to point to that file.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the deployment script to create a /health-check.php file when all tasks are complete, then modify the health check path to point to that file.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-02-09T07:02:00.000Z",
        "voteCount": 2,
        "content": "A and B are eliminated. First of all, it's not a good idea to extend grace period. \nThe problem of this scenario is that the instance \"come online\" before some steps are finished properly.\nBetween C and D, healthcheck shoud be implemented after all tasks are done and the instance is ready."
      },
      {
        "date": "2023-01-23T18:33:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2022-12-02T02:08:00.000Z",
        "voteCount": 4,
        "content": "B wrong-  204 - No Content - Specifies the normal response code for the DELETE operation.\nA wrong -  makes problem worse , delays startup and using index.php doesn't help\nC  wrong-    change to right health check  but not at start of deployment \nD  correct -  change to right health check  after the deployment script finishes"
      },
      {
        "date": "2022-12-01T23:16:00.000Z",
        "voteCount": 3,
        "content": "D for sure"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 109,
    "url": "https://www.examtopics.com/discussions/amazon/view/88859-exam-aws-devops-engineer-professional-topic-1-question-109/",
    "body": "A company has a single-page application that was developed in Angular. A DevOps engineer needs to automate deployments of the application to a website that the company hosts on Amazon S3. The solution must provide version control of the source code and must give developers the ability to perform peer review.<br><br>Which combination of steps will meet these requirements? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS CodeCommit repository to store the source code. Create an AWS CodePipeline pipeline that has a source of the CodeCommit repository.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an S3 bucket to act as the source for developers to upload their source code. Create an AWS CodePipeline pipeline that has the S3 bucket as the source.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the CodePipeline pipeline, configure an AWS CodeBuild phase that compiles the source code and produces build artifacts.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the CodePipeline pipeline, configure an AWS CodeDeploy phase that compiles the source code, produces build artifacts, and then deploys the website.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the CodePipeline pipeline, configure an AWS AppConfig deploy action that deploys the build artifacts to the S3 website bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the CodePipeline pipeline, configure an S3 deploy action that deploys the build artifacts to the S3 website bucket.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "ACF",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ACF",
        "count": 13,
        "isMostVoted": true
      },
      {
        "answer": "ACE",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-03-04T13:46:00.000Z",
        "voteCount": 2,
        "content": "A - CodeCommit to store code\nC - CodeBuild to compile code and build artifact\nF - S3 deploy action to deploy the S3 website - https://docs.aws.amazon.com/codepipeline/latest/userguide/integrations-action-type.html#integrations-deploy-S3"
      },
      {
        "date": "2023-02-09T08:23:00.000Z",
        "voteCount": 1,
        "content": "Between A and B, source of the code should be CodeCommit.\nC and E are the correct following steps."
      },
      {
        "date": "2023-01-23T18:36:00.000Z",
        "voteCount": 1,
        "content": "ACF is correct"
      },
      {
        "date": "2023-01-11T15:00:00.000Z",
        "voteCount": 1,
        "content": "S3 is a target for Codepipeline"
      },
      {
        "date": "2022-12-23T05:48:00.000Z",
        "voteCount": 1,
        "content": "ACF for sure"
      },
      {
        "date": "2022-12-11T04:12:00.000Z",
        "voteCount": 3,
        "content": "A, C, and F are the correct steps to meet the requirements.\n\nTo provide version control and peer review, the company can create an AWS CodeCommit repository to store the source code. Then, they can create an AWS CodePipeline pipeline that has the CodeCommit repository as its source.\n\nIn the CodePipeline pipeline, they can configure an AWS CodeBuild phase to compile the source code and produce build artifacts. This will allow the pipeline to build and package the application code.\n\nFinally, they can configure an S3 deploy action in the CodePipeline pipeline to deploy the build artifacts to the S3 website bucket, which will host the website. This will enable the pipeline to automatically deploy the application to the website hosted on S3."
      },
      {
        "date": "2022-12-02T02:22:00.000Z",
        "voteCount": 3,
        "content": "A - peer review\nC - Build Angular code\nF - deploy to S3 bucket"
      },
      {
        "date": "2022-12-01T23:21:00.000Z",
        "voteCount": 1,
        "content": "I will go with ACD"
      },
      {
        "date": "2022-12-01T23:23:00.000Z",
        "voteCount": 1,
        "content": "Refer: https://docs.aws.amazon.com/codepipeline/latest/userguide/tutorials-s3deploy.html"
      },
      {
        "date": "2022-11-30T23:57:00.000Z",
        "voteCount": 2,
        "content": "A - You need source repo\nC - CodeBuild to compile\nF - CodePipeline to deploy to S3\n\nE - no need to deploy to AppConfig, S3 is also a target."
      },
      {
        "date": "2022-12-01T23:20:00.000Z",
        "voteCount": 1,
        "content": "Look like answer is ACD ?"
      },
      {
        "date": "2022-11-30T05:47:00.000Z",
        "voteCount": 1,
        "content": "For deployment, i would with Amazon S3 deploy action which is option F"
      },
      {
        "date": "2022-11-26T05:57:00.000Z",
        "voteCount": 2,
        "content": "Angular is use NPM? need codebuild..i think"
      },
      {
        "date": "2022-12-01T23:21:00.000Z",
        "voteCount": 1,
        "content": "E do not mention about codebuid?"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 110,
    "url": "https://www.examtopics.com/discussions/amazon/view/89522-exam-aws-devops-engineer-professional-topic-1-question-110/",
    "body": "A DevOps engineer is creating a CI/CD pipeline for an Amazon ECS service. The ECS container instances run behind an Application Load Balancer as the web tier of a three-tier application. An acceptance criterion for a successful deployment is the verification that the web tier can communicate with the database and middleware tiers of the application upon deployment.<br><br>How can this be accomplished in an automated fashion?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a health check endpoint in the web application that tests connectivity to the data and middleware tiers. Use this endpoint as the health check URL for the load balancer.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an approval step for the quality assurance team to validate connectivity. Reject changes in the pipeline if there is an issue with connecting to the dependent tiers.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an Amazon RDS active connection count and an Amazon CloudWatch ELB metric to alarm on a significant change to the number of open connections.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Route 53 health checks to detect issues with the web service and roll back the Cl/CD pipeline if there is an error."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-03-05T05:36:00.000Z",
        "voteCount": 1,
        "content": "A : https://docs.aws.amazon.com/AmazonECS/latest/bestpracticesguide/load-balancer-healthcheck.html"
      },
      {
        "date": "2023-03-04T13:49:00.000Z",
        "voteCount": 1,
        "content": "load balancer health check endpoint is the best"
      },
      {
        "date": "2023-02-18T09:28:00.000Z",
        "voteCount": 1,
        "content": "I am confused, \nWhy is it A, how would you create a health check endpoint in the web application?\nWhy not D 'You can configure a health check that monitors an endpoint that you specify either by IP address or by domain name. At regular intervals that you specify, Route 53 submits automated requests over the internet to your application, server, or other resource to verify that it's reachable, available, and functional. Optionally, you can configure the health check to make requests similar to those that your users make, such as requesting a web page from a specific URL.'\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/health-checks-types.html"
      },
      {
        "date": "2023-02-09T08:34:00.000Z",
        "voteCount": 1,
        "content": "B :  should not be one more team\nC : connection count is not a reliable standard\nD : I do not know, just feel weird"
      },
      {
        "date": "2023-01-23T18:38:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2022-12-23T05:50:00.000Z",
        "voteCount": 2,
        "content": "AAAAAAAAA"
      },
      {
        "date": "2022-12-02T02:27:00.000Z",
        "voteCount": 2,
        "content": "A correct-  CodePipeline -&gt; codeDeploy with health check to middleware/DB if failed automatic rollback.\nB,C wrong\nD  wrong - is for DR scenarios, weighted routing, a health check is must and cannot rollback"
      },
      {
        "date": "2022-11-30T23:59:00.000Z",
        "voteCount": 3,
        "content": "A - correct. You need some sort of a health check."
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 111,
    "url": "https://www.examtopics.com/discussions/amazon/view/89752-exam-aws-devops-engineer-professional-topic-1-question-111/",
    "body": "A development team manages website deployments using AWS CodeDeploy blue/green deployments. The application is running on Amazon EC2 instances behind an Application Load Balancer in an Auto Scaling group.<br><br>When deploying a new revision, the team notices the deployment eventually fails, but it takes a long time to fail. After further inspection, the team discovers the AllowTraffic lifecycle event ran for an hour and eventually failed without providing any other information. The team wants to ensure failure notices are delivered more quickly while maintaining application availability even upon failure.<br><br>Which combination of actions should be taken to meet these requirements? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the deployment configuration to CodeDeployDefault.AllAtOnce to speed up the deployment process by deploying to all of the instances at the same time.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a CodeDeploy trigger for the deployment failure event and make the deployment fail as soon as a single health check failure is detected.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReduce the HealthCheckIntervalSeconds and UnhealthyThresholdCount values within the target group health checks to decrease the amount of time it takes for the application to be considered unhealthy.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the appspec.yml file to run a script on the AllowTraffic hook to perform lighter health checks on the application instead of making CodeDeploy wait for the target group health checks to pass.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the appspec.yml file to run a script on the BeforeAllowTraffic hook to perform health checks on the application and fail the deployment if the health checks performed by the script are not successful.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "BE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BE",
        "count": 16,
        "isMostVoted": true
      },
      {
        "answer": "CE",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "BD",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-02T03:19:00.000Z",
        "voteCount": 10,
        "content": "A - disaster \nC - wrong - already reached AllowTraffic lifecycle event that means it is not taking time in ValidateService event, problem is in health check itself.\nB/E - correct\nBetween D/E, why E? AllowTraffic - u cannot run scripts, it belongs to codedeploy\n\nBeforeAllowTraffic \u2013 You can use this deployment lifecycle event to run tasks on instances before they are registered with a load balancer.\n\nAllowTraffic \u2013 During this deployment lifecycle event, internet traffic is allowed to access instances after a deployment. This event is reserved for the CodeDeploy agent and cannot be used to run scripts."
      },
      {
        "date": "2024-09-29T21:29:00.000Z",
        "voteCount": 1,
        "content": "CE seems to be correct"
      },
      {
        "date": "2023-04-09T18:18:00.000Z",
        "voteCount": 2,
        "content": "ChatGPT said CE\nC. Reduce the HealthCheckIntervalSeconds and UnhealthyThresholdCount values within the target group health checks to decrease the amount of time it takes for the application to be considered unhealthy. This will make the target group health checks more sensitive to application health changes, which will result in quicker detection of application issues."
      },
      {
        "date": "2023-03-04T14:37:00.000Z",
        "voteCount": 2,
        "content": "I was confused between B and C but after reading more about BeforeAllowTraffic hook, its clear now.\n\nFor ec2 which is relevant for our question --&gt; You can use this deployment lifecycle event to run tasks on instances before they are registered with a load balancer. This means script mentioned in option E is run on each ec2. Hence option B is suitable one where we need to fail deployment as soon as check fails on any 1 ec2.\n\nIf we had ECS then option C would have applied because \nBeforeAllowTraffic - Used to run tasks after the second target group is associated with the replacement task set, but before traffic is shifted to the replacement task set. The results of a hook function at this lifecycle event can trigger a rollback.\n\nrefer https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html#reference-appspec-file-structure-hooks-list"
      },
      {
        "date": "2023-02-22T23:27:00.000Z",
        "voteCount": 4,
        "content": "Piccaso makes me confused...really"
      },
      {
        "date": "2023-02-09T08:46:00.000Z",
        "voteCount": 1,
        "content": "A is not good for an error has been located in a specific step.\nC, reduce time interval and threshold count can have biased error\nE, BeforeAllowTraffic does not match the scenario."
      },
      {
        "date": "2023-02-17T20:32:00.000Z",
        "voteCount": 1,
        "content": "You cannot edit or create scripts for the AllowTraffic hook"
      },
      {
        "date": "2023-01-23T18:48:00.000Z",
        "voteCount": 3,
        "content": "B and E is correct. B because of the use  of CodeDeploy trigger- You can create a trigger that publishes an Amazon Simple Notification Service (Amazon SNS) topic for a AWS CodeDeploy deployment or instance event. Then, when that event occurs, all subscribers to the associated topic receive notifications through the endpoint specified in the topic, such as an SMS message or email message. Amazon SNS offers multiple methods for subscribing to topics. E over D because you need to execute a script before you allow traffic and moreover we cannot intercept and write any custom script in the AfterAllowTraffic event since its reserved for CodeDeploy."
      },
      {
        "date": "2023-01-16T10:38:00.000Z",
        "voteCount": 1,
        "content": "https://www.examtopics.com/discussions/amazon/view/53194-exam-aws-devops-engineer-professional-topic-1-question-534/"
      },
      {
        "date": "2022-12-23T05:56:00.000Z",
        "voteCount": 1,
        "content": "BE !!!!!"
      },
      {
        "date": "2022-12-11T04:19:00.000Z",
        "voteCount": 3,
        "content": "To quickly deliver failure notices while maintaining application availability, the team should:\n\nUse the appspec.yml file to run a script on the BeforeAllowTraffic hook to perform health checks on the application and fail the deployment if the health checks performed by the script are not successful. (E)\nReduce the HealthCheckIntervalSeconds and UnhealthyThresholdCount values within the target group health checks to decrease the amount of time it takes for the application to be considered unhealthy. (C)\nBy performing health checks on the BeforeAllowTraffic hook, the team can quickly determine if the deployment is successful or not, and fail the deployment if the health checks performed by the script are not successful. This will allow the team to receive failure notices more quickly while maintaining application availability."
      },
      {
        "date": "2023-02-09T08:50:00.000Z",
        "voteCount": 1,
        "content": "Thanks for the E stuff."
      },
      {
        "date": "2022-12-01T23:24:00.000Z",
        "voteCount": 1,
        "content": "BE for sure"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 112,
    "url": "https://www.examtopics.com/discussions/amazon/view/88918-exam-aws-devops-engineer-professional-topic-1-question-112/",
    "body": "A development team manually builds an artifact locally and then places it in an Amazon S3 bucket. The application has a local cache that must be cleared when a deployment occurs. The team executes a command to do this, downloads the artifact from Amazon S3, and unzips the artifact to complete the deployment.<br><br>A DevOps team wants to migrate to a CI/CD process and build in checks to stop and roll back the deployment when a failure occurs. This requires the team to track the progression of the deployment.<br><br>Which combination of actions will accomplish this? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAllow developers to check the code into a code repository. Using Amazon CloudWatch Events, on every pull into master, trigger an AWS Lambda function to build the artifact and store it in Amazon S3.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a custom script to clear the cache. Specify the script in the Beforelnstall lifecycle hook in the AppSpec file.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate user data for each Amazon EC2 instance that contains the clear cache script. Once deployed, test the application. If it is not successful, deploy it again.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up AWS CodePipeline to deploy the application. Allow developers to check the code into a code repository as a source for the pipeline.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS CodeBuild to build the artifact and place it in Amazon S3. Use AWS CodeDeploy to deploy the artifact to Amazon EC2 instances.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Systems Manager to fetch the artifact from Amazon S3 and deploy it to all the instances."
    ],
    "answer": "BDE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BDE",
        "count": 15,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-03-04T14:44:00.000Z",
        "voteCount": 3,
        "content": "D - need codepipeline to orchestrate all stages. Code will be fetched from codecommit\nE - CodeBuild to build the artifact and store in S3. CodeDeploy to deploy the app\nB - clear cache before installing new revision using codedeploy hook"
      },
      {
        "date": "2023-02-11T12:56:00.000Z",
        "voteCount": 2,
        "content": "the answer is BDE"
      },
      {
        "date": "2023-02-09T08:58:00.000Z",
        "voteCount": 2,
        "content": "Do not understand why \"correct answer\" choose A over B"
      },
      {
        "date": "2023-01-23T18:52:00.000Z",
        "voteCount": 2,
        "content": "BDE are correct."
      },
      {
        "date": "2022-12-23T06:00:00.000Z",
        "voteCount": 1,
        "content": "BDE is correct"
      },
      {
        "date": "2022-12-02T03:32:00.000Z",
        "voteCount": 2,
        "content": "BDE - correct"
      },
      {
        "date": "2022-12-01T00:02:00.000Z",
        "voteCount": 3,
        "content": "BDE - correct."
      },
      {
        "date": "2022-11-28T22:55:00.000Z",
        "voteCount": 2,
        "content": "DEB makes sense.."
      },
      {
        "date": "2022-11-27T00:11:00.000Z",
        "voteCount": 3,
        "content": "AppSpec  can clear cache by bash script"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 113,
    "url": "https://www.examtopics.com/discussions/amazon/view/88920-exam-aws-devops-engineer-professional-topic-1-question-113/",
    "body": "A company is implementing a well-architected design for its globally accessible API stack. The design needs to ensure both high reliability and fast response times for users located in North America and Europe.<br><br>The API stack contains the following three tiers:<br><br>\u2022\tAmazon API Gateway<br>\u2022\tAWS Lambda<br>\u2022\tAmazon DynamoDB<br><br>Which solution will meet the requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Amazon Route 53 to point to API Gateway APIs in North America and Europe using health checks. Configure the APIs to forward requests to a Lambda function in that Region. Configure the Lambda functions to retrieve and update the data in a DynamoDB table in the same Region as the Lambda function.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Amazon Route 53 to point to API Gateway APIs in North America and Europe using latency-based routing and health checks. Configure the APIs to forward requests to a Lambda function in that Region. Configure the Lambda functions to retrieve and update the data in a DynamoDB global table.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Amazon Route 53 to point to API Gateway in North America, create a disaster recovery API in Europe, and configure both APIs to forward requests to the Lambda functions in that Region. Retrieve the data from a DynamoDB global table. Deploy a Lambda function to check the North America API health every 5 minutes. In the event of a failure, update Route 53 to point to the disaster recovery API.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Amazon Route 53 to point to API Gateway API in North America using latency-based routing. Configure the API to forward requests to the Lambda function in the Region nearest to the user. Configure the Lambda function to retrieve and updathe data in a DynamoDB table."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-03-04T14:48:00.000Z",
        "voteCount": 1,
        "content": "multi region, active-active, latency based routing"
      },
      {
        "date": "2023-02-09T13:39:00.000Z",
        "voteCount": 3,
        "content": "Global DynamoDB table is a must"
      },
      {
        "date": "2023-01-23T18:56:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2022-12-23T06:02:00.000Z",
        "voteCount": 2,
        "content": "B - correct"
      },
      {
        "date": "2022-12-01T00:04:00.000Z",
        "voteCount": 2,
        "content": "B - correct. Anytime there is a multi-region app that is not a DR, look for latency based Rt 53."
      },
      {
        "date": "2022-11-27T00:15:00.000Z",
        "voteCount": 1,
        "content": "latency-based is nessesary, Lambda function api region abotion ,not edge"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 114,
    "url": "https://www.examtopics.com/discussions/amazon/view/88919-exam-aws-devops-engineer-professional-topic-1-question-114/",
    "body": "A company manages an application that stores logs in Amazon CloudWatch Logs. The company wants to archive the logs in Amazon S3. Logs are rarely accessed after 90 days and must be retained for 10 years.<br><br>Which combination of steps should a DevOps engineer take to meet these requirements? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a CloudWatch Logs subscription filter to use AWS Glue to transfer all logs to an S3 bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a CloudWatch Logs subscription filter to use Amazon Kinesis Data Firehose to stream all logs to an S3 bucket.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a CloudWatch Logs subscription filter to stream all logs to an S3 bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the S3 bucket lifecycle policy to transition logs to S3 Glacier after 90 days and to expire logs after 3,650 days.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the S3 bucket lifecycle policy to transition logs to Reduced Redundancy after 90 days and to expire logs after 3,650 days."
    ],
    "answer": "BD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BD",
        "count": 6,
        "isMostVoted": true
      },
      {
        "answer": "CD",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-03-04T14:50:00.000Z",
        "voteCount": 2,
        "content": "B is right as you can use a subscription filter with Kinesis, Lambda, or Kinesis Data Firehose.\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/SubscriptionFilters.html\n\nD is right as glacier is perfect for a rarely used object"
      },
      {
        "date": "2023-02-09T13:49:00.000Z",
        "voteCount": 1,
        "content": "Glue and Kinesis are overkill -&gt; A and B are eliminated.\nReduced Redundancy is not related."
      },
      {
        "date": "2023-02-18T09:43:00.000Z",
        "voteCount": 3,
        "content": "You cannot use a subscription filter on S3 directly."
      },
      {
        "date": "2023-02-19T15:01:00.000Z",
        "voteCount": 1,
        "content": "Got it. Thanks."
      },
      {
        "date": "2023-01-23T18:58:00.000Z",
        "voteCount": 1,
        "content": "BD is correct."
      },
      {
        "date": "2022-12-23T06:04:00.000Z",
        "voteCount": 1,
        "content": "BD - correct"
      },
      {
        "date": "2022-12-01T23:33:00.000Z",
        "voteCount": 2,
        "content": "BD for sure,\n\nYou can use a subscription filter with Kinesis, Lambda, or Kinesis Data Firehose.\nRefer: https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/SubscriptionFilters.html"
      },
      {
        "date": "2023-02-16T07:52:00.000Z",
        "voteCount": 1,
        "content": "And Amazon Opensearch too\nBD for sure!"
      },
      {
        "date": "2023-02-16T07:53:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CWL_OpenSearch_Stream.html"
      },
      {
        "date": "2022-12-01T00:07:00.000Z",
        "voteCount": 2,
        "content": "BD - correct. You can't subscribe CW logs to S3."
      },
      {
        "date": "2022-11-27T00:13:00.000Z",
        "voteCount": 1,
        "content": "Kinesis Data Firehose get logs and send to s3,bucket lifecycle policy change to  S3 Glacier"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 115,
    "url": "https://www.examtopics.com/discussions/amazon/view/88922-exam-aws-devops-engineer-professional-topic-1-question-115/",
    "body": "A DevOps engineer is building a continuous deployment pipeline for a serverless application that uses AWS Lambda functions. The company wants to reduce the customer impact of an unsuccessful deployment. The company also wants to monitor for issues.<br><br>Which deploy stage configuration will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an AWS Serverless Application Model (AWS SAM) template to define the serverless application. Use AWS CodeDeploy to deploy the Lambda functions with the Canary10Percent15Minutes Deployment Preference Type. Use Amazon CloudWatch alarms to monitor the health of the functions.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS CloudFormation to publish a new stack update, and include Amazon CloudWatch alarms on all resources. Set up an AWS CodePipeline approval action for a developer to verify and approve the AWS CloudFormation change set.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS CloudFormation to publish a new version on every stack update, and include Amazon CloudWatch alarms on all resources. Use the RoutingConfig property of the AWS:: Lambda:: Alias resource to update the traffic routing during the stack update.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS CodeBuild to add sample event payloads for testing to the Lambda functions. Publish a new version of the functions, and include Amazon CloudWatch alarms. Update the production alias to point to the new version. Configure rollbacks to occur when an alarm is in the ALARM state."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 12,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-01T00:11:00.000Z",
        "voteCount": 8,
        "content": "A - correct. See these links. \n\nhttps://docs.aws.amazon.com/serverless-application-model/latest/developerguide/automating-updates-to-serverless-apps.html\n\nYou can even do some testing. \n\nhttps://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-sam-cli-using-generate-event.html"
      },
      {
        "date": "2023-03-31T06:30:00.000Z",
        "voteCount": 1,
        "content": "Setting CW alarm on all resources in this scenario does not make sense as stated in B and C, Codebuild cannot be used to rollback deployment, we need CodeDeploy (D), so the only viable option is A"
      },
      {
        "date": "2023-03-28T18:35:00.000Z",
        "voteCount": 1,
        "content": "A is correct. \n\nThis option meets the requirements because it uses AWS CodeDeploy with the Canary deployment strategy, which deploys the new version of the Lambda function to a small percentage of users and monitors the function for errors or performance issues before gradually rolling out the update to the remaining users."
      },
      {
        "date": "2023-03-04T14:53:00.000Z",
        "voteCount": 1,
        "content": "SAM with CodeDeploy and canary strategy to minimize impact by shifting traffic gradually"
      },
      {
        "date": "2023-02-09T14:11:00.000Z",
        "voteCount": 1,
        "content": "AWS SAM is a must"
      },
      {
        "date": "2023-02-07T13:42:00.000Z",
        "voteCount": 1,
        "content": "A - correct - canary assures reduced customer impact, SAM uses CodeDeploy\nhttps://docs.aws.amazon.com/serverless-application-model/latest/developerguide/automating-updates-to-serverless-apps.html\nB - wrong - veryfing CF will not assure that Lambda will work correctly\nC - wrong - will not reduce customer impact - may lead to all Lambdas unavailable\nD - wrong - will not reduce impact since it may lead to rollback (unavailability)"
      },
      {
        "date": "2023-01-23T19:55:00.000Z",
        "voteCount": 1,
        "content": "A seems right but not sure if CodeDeploy deploys  SAM application. It can deploy Lambda direclty satisfying all the requirements. However it is still the best answer."
      },
      {
        "date": "2022-12-23T06:08:00.000Z",
        "voteCount": 1,
        "content": "A is the best"
      },
      {
        "date": "2022-12-17T10:17:00.000Z",
        "voteCount": 2,
        "content": "C is the most appropriate configuration for meeting the requirements.\nIn Option C, AWS CloudFormation is used to publish a new version of the stack with every update.\nThis allows the company to roll back to a previous version if there are issues with the new version.\nAmazon CloudWatch alarms are included on all resources to monitor the health of the application and alert the company if there are any issues.\nThe RoutingConfig property of the AWS::Lambda::Alias resource is used to update the traffic routing during the stack update, which allows the company to gradually roll out the new version and reduce the impact on customers."
      },
      {
        "date": "2022-11-27T00:21:00.000Z",
        "voteCount": 1,
        "content": "B is crroct ,AWS SAM just way to deploy"
      },
      {
        "date": "2023-01-08T19:06:00.000Z",
        "voteCount": 1,
        "content": "Wrong\nhttps://aws.amazon.com/blogs/compute/introducing-aws-sam-pipelines-automatically-generate-deployment-pipelines-for-serverless-applications/"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 116,
    "url": "https://www.examtopics.com/discussions/amazon/view/88921-exam-aws-devops-engineer-professional-topic-1-question-116/",
    "body": "A large enterprise is deploying a web application on AWS. The application runs on Amazon EC2 instances behind an Application Load Balancer. The instances run in an Auto Scaling group across multiple Availability Zones. The application stores data in an Amazon RDS Oracle DB instance and Amazon DynamoDB. There are separate environments for development, testing, and production.<br><br>What is the MOST secure and flexible way to obtain password credentials during deployment?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRetrieve an access key from an AWS Systems Manager SecureString parameter to access AWS services. Retrieve the database credentials from a Systems Manager SecureString parameter.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLaunch the EC2 instances with an EC2 IAM role to access AWS services. Retrieve the database credentials from AWS Secrets Manager.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRetrieve an access key from an AWS Systems Manager plaintext parameter to access AWS services. Retrieve the database credentials from a Systems Manager SecureString parameter.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLaunch the EC2 instances with an EC2 IAM role to access AWS services. Store the database passwords in an encrypted config file with the application artifacts."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 8,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-03-04T18:19:00.000Z",
        "voteCount": 1,
        "content": "Store secrets in secrets manager and access that using an IAM role"
      },
      {
        "date": "2023-02-09T14:16:00.000Z",
        "voteCount": 1,
        "content": "B and D are eliminated. \nBetween A and C, maybe C ...."
      },
      {
        "date": "2024-05-17T08:53:00.000Z",
        "voteCount": 1,
        "content": "stupid"
      },
      {
        "date": "2023-01-23T19:59:00.000Z",
        "voteCount": 1,
        "content": "B is correct way to handle DB credentials inside an application."
      },
      {
        "date": "2022-12-23T06:11:00.000Z",
        "voteCount": 1,
        "content": "B - correct"
      },
      {
        "date": "2022-12-11T04:31:00.000Z",
        "voteCount": 1,
        "content": "Option B is the most secure and flexible way to obtain password credentials during deployment because it uses an IAM role to grant permissions to the EC2 instances to access AWS services. The database credentials can be securely stored in AWS Secrets Manager and accessed by the EC2 instances when needed. Using an IAM role eliminates the need to manage access keys, which can be a security risk if they are not properly protected. In addition, using AWS Secrets Manager to store the database credentials allows for easy rotation and management of the credentials."
      },
      {
        "date": "2022-12-01T00:14:00.000Z",
        "voteCount": 3,
        "content": "B - Correct. \nYou want a role to access AWS service so you don't have to worry about access keys and passwords."
      },
      {
        "date": "2022-11-27T00:18:00.000Z",
        "voteCount": 2,
        "content": "Role and Secrets or system manager paramater store."
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 117,
    "url": "https://www.examtopics.com/discussions/amazon/view/93046-exam-aws-devops-engineer-professional-topic-1-question-117/",
    "body": "A company wants to use a grid system for proprietary enterprise in-memory data store on top of AWS. The system can run in multiple server nodes in any Linux-based distribution. The system must be able to reconfigure the entire cluster every time a node is added or removed. When adding or removing nodes, an /etc/cluster/nodes.config file must be updated listing the IP addresses of the current node member of that cluster.<br><br>The company wants to automate the task of adding new nodes to a cluster.<br><br>What can a DevOps engineer do to meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS OpsWorks Stacks to layer the server nodes of that cluster. Create a Chief recipe that populates the content of the /etc/cluster/nodes.config file and restarts the service by using the current members of the layers. Assign that recipe to the Configure lifecycle event.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPut the file nodes.config in version control. Create an AWS CodeDeploy deployment configuration and deployment group based on an Amazon EC2 tag value for the cluster nodes. When adding a new node to the cluster, update the file with all tagged instances, and make a commit in version control. Deploy the new file and restart the services.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon S3 bucket and upload a version of the /etc/cluster/nodes.config file. Create a crontab script that will poll for that S3 file and download it frequently. Use a process manager, such as Monit or systemd, to restart the cluster services when it detects that the new file was modified. When adding a node to the cluster, edit the file\u2019s most recent members. Upload the new file to the S3 bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a user data script that lists all members of the current security group of the cluster and automatically updates the /etc/cluster/nodes.config file whenever a new instance is added to the cluster."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-12-27T22:06:00.000Z",
        "voteCount": 5,
        "content": "OPSWork's Configure Lifecycle event runs on all the nodes."
      },
      {
        "date": "2023-03-28T18:44:00.000Z",
        "voteCount": 1,
        "content": "It's B."
      },
      {
        "date": "2023-02-10T04:48:00.000Z",
        "voteCount": 1,
        "content": "A looks more automatic."
      },
      {
        "date": "2023-01-23T20:06:00.000Z",
        "voteCount": 4,
        "content": "A is correct. OpsWork is the right DevOps tool. Not B because using CodeDeploy here to reconfigure the cluster when new nodes are added doesn't make sense. It's not a deployment function. It is an Infrastructure provisioning function."
      },
      {
        "date": "2022-12-28T17:23:00.000Z",
        "voteCount": 4,
        "content": "question discussed at below link with A as possible answer\nhttps://www.examtopics.com/discussions/amazon/view/3418-exam-aws-devops-engineer-professional-topic-1-question-4/"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 118,
    "url": "https://www.examtopics.com/discussions/amazon/view/92576-exam-aws-devops-engineer-professional-topic-1-question-118/",
    "body": "A DevOps engineer is researching the least expensive way to implement an image batch processing cluster on AWS. The application cannot run in Docker containers and must run on Amazon EC2. The batch job stores checkpoint data on an NFS and can tolerate interruptions. Configuring the cluster software from a generic EC2 Linux image takes 30 minutes.<br><br>What is the MOST cost-effective solution?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon EFS for checkpoint data. To complete the job, use an EC2 Auto Scaling group and an On-Demand pricing model to provision EC2 instances temporarily.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse GlusterFS on EC2 instances for checkpoint data. To run the batch job, configure EC2 instances manually. When the job completes, shut down the instances manually.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon EFS for checkpoint data. Use EC2 Fleet to launch EC2 Spot Instances, and utilize user data to configure the EC2 Linux instance on startup.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon EFS for checkpoint data. Use EC2 Fleet to launch EC2 Spot Instances. Create a custom AMI for the cluster and use the latest AMI when creating instances.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 3,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-09-29T23:35:00.000Z",
        "voteCount": 1,
        "content": "D. While this could work, creating and maintaining custom AMIs adds unnecessary complexity and potential costs when user data can achieve the same result more efficiently.\nThe chosen solution (C) provides the best balance of cost-effectiveness, scalability, and ease of management:\nIt uses the cheapest compute option (Spot Instances) that meets the requirements.\nIt leverages a fully managed, scalable file system (EFS) for checkpoint data.\nIt automates instance configuration using user data, reducing management overhead.\nIt can easily scale up or down based on demand using EC2 Fleet."
      },
      {
        "date": "2024-09-29T23:37:00.000Z",
        "voteCount": 1,
        "content": "My bad it has to be D for the 30 min load up time"
      },
      {
        "date": "2023-03-28T18:53:00.000Z",
        "voteCount": 1,
        "content": "C is cheaper than D"
      },
      {
        "date": "2023-02-10T06:00:00.000Z",
        "voteCount": 2,
        "content": "A is almost un-doable.\nB is using GlusterFS. AWS-native products are better.\nBetween C and D.  D is better because customised AMI brings convenience."
      },
      {
        "date": "2023-01-23T20:10:00.000Z",
        "voteCount": 1,
        "content": "D is correct."
      },
      {
        "date": "2022-12-28T17:24:00.000Z",
        "voteCount": 1,
        "content": "D sounds reasonable"
      },
      {
        "date": "2022-12-24T23:08:00.000Z",
        "voteCount": 2,
        "content": "D is the correct ans"
      },
      {
        "date": "2022-12-23T06:15:00.000Z",
        "voteCount": 1,
        "content": "D - correct"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 119,
    "url": "https://www.examtopics.com/discussions/amazon/view/92516-exam-aws-devops-engineer-professional-topic-1-question-119/",
    "body": "A mobile application running on eight Amazon EC2 instances is relying on a third-party API endpoint. The third-party service has a high failure rate because of limited capacity which is expected to be resolved in a few weeks.<br><br>In the meantime, the mobile application developers have added a retry mechanism and are logging failed API requests. A DevOps engineer must automate the monitoring of application logs and count the specific error messages, if there are more than 10 errors within a 1-minute window the system must issue an alert.<br><br>How can the requirements be met with MINIMAL management overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall the Amazon CloudWatch agent on all instances to push the application logs to CloudWatch Logs. Use metric filters to count the error messages every minute, and initiate a CloudWatch alarm if the count exceeds 10 errors.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall the Amazon CloudWatch agent on all instances to push the access logs to CloudWatch Logs. Create an Amazon EventBridge (Amazon CloudWatch Events) rule to count the error messages every minute, and initiate a CloudWatch alarm if the count exceeds 10 errors.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall the Amazon CloudWatch agent on all instances to push the application logs to CloudWatch Logs. Use a metric filter to generate a custom CloudWatch metric that records the number of failures and initiates a CloudWatch alarm if the custom metric reaches 10 errors in a 1-minute period.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy a custom script on all instances to check application logs regularly in a cron job. Count the number of error messages every minute, and push a data point to a custom CloudWatch metric. Initiate a CloudWatch alarm if the custom metric reaches 10 errors in a 1-minute period."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 15,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 9,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-27T22:18:00.000Z",
        "voteCount": 8,
        "content": "C -- We need to use CloudWatch metric filter to generate count and Cloudwatch alarm for checking number of counts in a minute.\nA -- is wrong as you can only use metric filters to get count per occurrence in logs and not count in a particular time frame."
      },
      {
        "date": "2023-05-06T02:45:00.000Z",
        "voteCount": 1,
        "content": "A is more promising in this scenario."
      },
      {
        "date": "2024-05-05T06:07:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringLogData.html\ncheck this link how to create metric filters to count the error messages every minute. \nC is correct answer"
      },
      {
        "date": "2023-05-01T06:45:00.000Z",
        "voteCount": 1,
        "content": "The solution that meets the requirements with MINIMAL management overhead is option A. Here's why:\n\nOption A suggests installing the Amazon CloudWatch agent on all instances to push the application logs to CloudWatch Logs. Then, using metric filters to count the error messages every minute, and initiating a CloudWatch alarm if the count exceeds 10 errors. This approach is the most straightforward and requires the least amount of custom coding or scripts. It uses Amazon CloudWatch and its built-in capabilities for monitoring and alerting on logs."
      },
      {
        "date": "2023-03-28T18:59:00.000Z",
        "voteCount": 1,
        "content": "It's C, and here's why...\n\nCloudWatch agent -  easy to install and configure, and requires minimal management overhead.\nMetric filter to count the specific error messages - a lightweight and efficient way to monitor logs and generate metrics.\nGenerates a custom CloudWatch metric - can be used to track the specific error messages over time and trigger alarms if the count exceeds a certain threshold.\nAlso meets the requirement of issuing an alert if there are more than 10 errors within a 1-minute window."
      },
      {
        "date": "2023-03-09T13:29:00.000Z",
        "voteCount": 2,
        "content": "For me C is the correct one.\nWhy not A?  how can a metric filter trigger a cloudwatch alarm? you must use the custom metric created by it in order to trigger the alarm - this cannot be done directly based on a filter."
      },
      {
        "date": "2023-03-30T12:21:00.000Z",
        "voteCount": 1,
        "content": "The metric filter is there to count the number of error messages per minute, and THEN we set up CloudWatch alarm with the metric. Maybe I'm wrong, but your rationale might be actually invalid."
      },
      {
        "date": "2023-02-28T08:21:00.000Z",
        "voteCount": 3,
        "content": "A vs C --&gt; A\n\"if there are more than 10 errors within a 1-minute\" \nC is checking 10 per minute"
      },
      {
        "date": "2023-03-30T12:23:00.000Z",
        "voteCount": 1,
        "content": "Ohhhh you're right...!"
      },
      {
        "date": "2023-02-18T10:03:00.000Z",
        "voteCount": 2,
        "content": "Definitely A.\nYou can use metric filter to count the error, create an alarm for it within a 1 minute period that can check if threshold for the metric (count in this case) is above 10.\nJust go and test it"
      },
      {
        "date": "2023-02-18T10:06:00.000Z",
        "voteCount": 1,
        "content": "Most definitely not C, how can you generate a custom metric from a metric filter?!"
      },
      {
        "date": "2023-02-10T06:54:00.000Z",
        "voteCount": 1,
        "content": "D is eliminated in the first round because of \"cron job\".\nB is pushing only \"access logs\"\nA looks all errors, not a specific type of errors."
      },
      {
        "date": "2023-02-08T04:06:00.000Z",
        "voteCount": 1,
        "content": "B &amp; D - wrong\nA - wrong - metric filter is only providing custom metrics, it is not doing any calculations (\"Use metric filters to count the error messages every minute\")\nC - correct - metrics filter provides a new custom metric used by CW alarm to trigger an action"
      },
      {
        "date": "2023-01-23T20:30:00.000Z",
        "voteCount": 2,
        "content": "A is the right answer. You can generate a custom metric from a metric filter. You can generate only a standard metric from the CloudWatch metric filter. Custom metric can be generated only via CLI."
      },
      {
        "date": "2023-01-19T06:58:00.000Z",
        "voteCount": 2,
        "content": "C is answer, you could test this solution from aws console. When you create metrics filter it will create new metric for you"
      },
      {
        "date": "2023-01-18T05:02:00.000Z",
        "voteCount": 1,
        "content": "correct answer C"
      },
      {
        "date": "2023-01-18T04:55:00.000Z",
        "voteCount": 1,
        "content": "there is no need to create custom metric"
      },
      {
        "date": "2023-01-04T01:25:00.000Z",
        "voteCount": 1,
        "content": "Ans:A\nC is wrong because to create a \"custom CloudWatch metric\", you need to write your own script or use an application monitoring script. See AWS docs below:\nhttps://docs.aws.amazon.com/managedservices/latest/userguide/custom-cloudwatch-events.html"
      },
      {
        "date": "2023-01-02T05:47:00.000Z",
        "voteCount": 1,
        "content": "Going with C. C states use of custom metric filter."
      },
      {
        "date": "2023-01-01T15:49:00.000Z",
        "voteCount": 1,
        "content": "Ans: A\nFor those choosing C, please, read AWS documentation about \"custom CloudWatch metrics\". You don't use metric filter to generate \"custom CloudWatch metric\", instead you use the AWS CLI or an API (see excerpt below):\nFull link: https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/publishingMetrics.html \n\"Publishing custom metrics\n-------------------------------------------\nYou can publish your own metrics to CloudWatch using the AWS CLI or an API. You can view statistical graphs of your published metrics with the AWS Management Console.\""
      },
      {
        "date": "2022-12-29T09:14:00.000Z",
        "voteCount": 1,
        "content": "Ans is A\nUser Saggy4 is wrong in stating that \"A is wrong as you can only use metric filters to get count per occurrence in logs and not count in a particular time frame.\"\nScroll to 2:08 of the tutorial below:\nhttps://www.youtube.com/watch?v=I_VjSvSSoF4&amp;ab_channel=SREMasterClass\n(You can specify the time period within which the error count needs to occur by creating an alarm on top of your metric. Please, see the video above eg 10 errors within a minute)"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 120,
    "url": "https://www.examtopics.com/discussions/amazon/view/92577-exam-aws-devops-engineer-professional-topic-1-question-120/",
    "body": "A company has 20 service teams. Each service team is responsible for its own microservice. Each service team uses a separate AWS account for its microservice and a VPC with the 192.168.0.0/22 CIDR block. The company manages the AWS accounts with AWS Organizations.<br><br>Each service team hosts its microservice on multiple Amazon EC2 instances behind an Application Load Balancer. The microservices communicate with each other across the public Internet. The company's security team has issued a new guideline that all communication between microservices must use HTTPS over private network connections and cannot traverse the public Internet.<br><br>A DevOps engineer must implement a solution that fulfills these obligations and minimizes the number of changes for each service team.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new AWS account in AWS Organizations. Create a VPC in this account and use AWS Resource Access Manager to share the private subnets of this VPC with the organization. Instruct the service teams to launch a new Network Load Balancer (NLB) and EC2 instances that use the shared private subnets. Use the NLB DNS names for communication between microservices.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Network Load Balancer (NLB) in each of the microservice VPCs. Use AWS PrivateLink to create VPC endpoints in each AWS account for the NLBs. Create subscriptions to each VPC endpoint in each of the other AWS accounts. Use the VPC endpoint DNS names for communication between microservices.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Network Load Balancer (NLB) in each of the microservice VPCs. Create VPC peering connections between each of the microservice VPCs. Update the route tables for each VPC to use the peering links. Use the NLB DNS names for communication between microservices.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new AWS account in AWS Organizations. Create a transit gateway in this account. and use AWS Resource Access Manager to share the transit gateway with the organization. In each of the microservice VPCs, create a transit gateway attachment to the shared transit gateway. Update the route tables of each VPC to use the transit gateway. Create a Network Load Balancer (NLB) in each of the microservice VPCs. Use the NLB DNS names for communication between microservices."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 8,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-01-23T20:52:00.000Z",
        "voteCount": 4,
        "content": "https://aws.amazon.com/blogs/networking-and-content-delivery/connecting-networks-with-overlapping-ip-ranges/  Private link is the best option because Transit Gateway doesn't support overlapping CIDR ranges."
      },
      {
        "date": "2023-01-12T15:39:00.000Z",
        "voteCount": 3,
        "content": "I'll go with B. TGW cannot be used for vpc with overlapping ips. The same for vpc peering"
      },
      {
        "date": "2023-01-06T04:52:00.000Z",
        "voteCount": 1,
        "content": "B. the link https://aws.amazon.com/blogs/networking-and-content-delivery/connecting-networks-with-overlapping-ip-ranges/   describes NAT + Private Links and Transit Gateways. In the article it is mentioned that TGW can be used when CIDR ranges don't overlap. So the possible solution is \"B\""
      },
      {
        "date": "2023-02-08T04:31:00.000Z",
        "voteCount": 1,
        "content": "You can use TG with overlapping CIDRS but you also have to implement private nat. Read carefully."
      },
      {
        "date": "2024-05-05T05:45:00.000Z",
        "voteCount": 1,
        "content": "no you can not use TGW with overlapping CIDRS. The link say subnets not overlapping"
      },
      {
        "date": "2023-01-04T01:14:00.000Z",
        "voteCount": 2,
        "content": "Transit Gateway fits better ; Connect Amazon VPCs, AWS accounts, and on-premises networks to a single gateway"
      },
      {
        "date": "2023-01-06T04:49:00.000Z",
        "voteCount": 4,
        "content": "If I am not mistaken Transit Gateway cannot have VPCs with overlapping IP addresses. So \"D\" is incorrect.\n\"B\" will definitely work"
      },
      {
        "date": "2022-12-23T06:22:00.000Z",
        "voteCount": 4,
        "content": "B - private link fits well in this situation"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 121,
    "url": "https://www.examtopics.com/discussions/amazon/view/92580-exam-aws-devops-engineer-professional-topic-1-question-121/",
    "body": "A company is adopting AWS CodeDeploy to automate its application deployments for a Java-Apache Tomcat application with an Apache webserver The development team started with a proof of concept, created a deployment group for a developer environment, and performed functional tests within the application After completion, the team will create additional deployment groups for staging and production.<br><br>The current log level is configured within the Apache settings, but the team wants to change this configuration dynamically when the deployment occurs, so that they can set different log level configurations depending on the deployment group without having a different application revision for each group.<br><br>How can these requirements be met with the LEAST management overhead and without requiring different script versions for each deployment group?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTag the Amazon EC2 instances depending on the deployment group. Then place a script into the application revision that calls the metadata service and the EC2 API to identify which deployment group the instance is part of. Use this information to configure the log level settings. Reference the script as part of the Afterinstall lifecycle hook in the appspec.yml file.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a script that uses the CodeDeploy environment variable DEPLOYMENT_GROUP_NAME to identify which deployment group the instance is part of. Use this information to configure the log level settings. Reference this script as part of the Beforelnstall lifecycle hook in the appspec.yml file.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a CodeDeploy custom environment variable for each environment Then place a script into the application revision that checks this environment variable to identify which deployment group the instance is part of. Use this information to configure the log level settings. Reference this script as part of the ValidateService lifecycle hook in the appspec.yml file.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a script that uses the CodeDeploy environment variable DEPLOYMENT_GROUP_ID to identify which deployment group the instance is part of to configure the log level settings. Reference this script as part of the Install lifecycle hook in the appspec.yml file."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-12-28T06:50:00.000Z",
        "voteCount": 5,
        "content": "B is the correct answer here is the link to the docs explaining just that https://aws.amazon.com/blogs/devops/using-codedeploy-environment-variables/"
      },
      {
        "date": "2023-03-04T19:10:00.000Z",
        "voteCount": 1,
        "content": "Check the 2nd example in this doc - https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html"
      },
      {
        "date": "2023-02-15T02:20:00.000Z",
        "voteCount": 1,
        "content": "Why is C suggested as \"correct answer\" ?"
      },
      {
        "date": "2023-01-24T17:00:00.000Z",
        "voteCount": 3,
        "content": "https://aws.amazon.com/blogs/devops/using-codedeploy-environment-variables/"
      },
      {
        "date": "2022-12-26T08:53:00.000Z",
        "voteCount": 1,
        "content": "B the correct one"
      },
      {
        "date": "2022-12-23T06:58:00.000Z",
        "voteCount": 2,
        "content": "B - Correct"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 122,
    "url": "https://www.examtopics.com/discussions/amazon/view/92581-exam-aws-devops-engineer-professional-topic-1-question-122/",
    "body": "A company is running a custom-built application that processes records. All the components run on Amazon EC2 instances that run in an Auto Scaling group. Each record's processing is a multistep sequential action that is compute-intensive. Each step is always completed in 5 minutes or less.<br><br>A limitation of the current system is that if any steps fail, the application has to reprocess the record from the beginning. The company wants to update the architecture so that the application must reprocess only the failed steps.<br><br>What is the MOST operationally efficient solution that meets these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a web application to write records to Amazon S3. Use S3 Event Notifications to publish to an Amazon Simple Notification Service (Amazon SNS) topic. Use an EC2 instance to poll Amazon SNS and start processing. Save intermediate results to Amazon S3 to pass on to the next step.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPerform the processing steps by using logic in the application. Convert the application code to run in a container. Use AWS Fargate to manage the container instances. Configure the container to invoke itself to pass the state from one step to the next.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a web application to pass records to an Amazon Kinesis data stream. Decouple the processing by using the Kinesis data stream and AWS Lambda functions.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a web application to pass records to AWS Step Functions. Decouple the processing into Step Functions tasks and AWS Lambda functions.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-03-31T12:54:00.000Z",
        "voteCount": 1,
        "content": "its D baby."
      },
      {
        "date": "2023-02-10T07:36:00.000Z",
        "voteCount": 1,
        "content": "A, haha ...\nB, I think I tried this method, headache ....\nC, haha"
      },
      {
        "date": "2023-01-24T17:03:00.000Z",
        "voteCount": 1,
        "content": "D is the right answer"
      },
      {
        "date": "2022-12-23T07:01:00.000Z",
        "voteCount": 4,
        "content": "D DDDDDDD"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 123,
    "url": "https://www.examtopics.com/discussions/amazon/view/92419-exam-aws-devops-engineer-professional-topic-1-question-123/",
    "body": "A DevOps engineer is designing a multi-Region disaster recovery strategy for an application. The application requires an RPO of 1 hour and requires an RTO of 4 hours. The application is deployed with an AWS CloudFormation template that creates an Application Load Balancer (ALB), Amazon EC2 instances in an Auto Scaling group and an Amazon RDS Multi-AZ DB instance with 20 GB of allocated storage. The AMI of the application instance does not contain data and has been copied to the destination Region.<br><br>Which combination of actions will meet the recovery objectives at the LOWEST cost? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLaunch an RDS DB instance in the failover Region. Use AWS Database Migration Service (AWS DMS) to configure ongoing replication from the source database.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpon failover, update the AWS CloudFormation stack in the failover Region to increase the desired number of instances in the Auto Scaling group. When the stack update is complete, change the DNS records to point to the failover Region's ALB.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpon failover, launch the AWS CloudFormation template in the failover Region with the DB snapshot ID as an input parameter. When the stack creation is complete, change the DNS records to point to the failover Region's ALB.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Backup to take a snapshot of the DB instance every hour and to copy the snapshot to the failover Region.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon EventBridge (Amazon CloudWatch Events) event that invokes an AWS Lambda function to copy the RDS automated snapshot to the failover Region."
    ],
    "answer": "CD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "CD",
        "count": 8,
        "isMostVoted": true
      },
      {
        "answer": "BD",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-28T07:16:00.000Z",
        "voteCount": 5,
        "content": "Answer is C and D.\nB is wrong because from the question there is just AMI in the disaster region, there is no Autoscaling group for which we can increase the count."
      },
      {
        "date": "2023-10-20T07:24:00.000Z",
        "voteCount": 1,
        "content": "It is often cheaper to set up and autoreplicate a DB and leave it running than to create it failure-time. Also, copying snapshots interregionally every hour is not cheap. And restoring them can take more than 4 hours.\n\nTherefore: AB."
      },
      {
        "date": "2023-03-28T20:02:00.000Z",
        "voteCount": 1,
        "content": "C,D baby."
      },
      {
        "date": "2023-03-04T19:18:00.000Z",
        "voteCount": 1,
        "content": "CD combination is the only one that fulfills RPO of 1 hour at lowest cost."
      },
      {
        "date": "2023-02-28T08:54:00.000Z",
        "voteCount": 2,
        "content": "D vs E\nWe need an RPO of 1 hour, we need backups younger or equal to 1 hour!\n\n\"E. Create an Amazon EventBridge (Amazon CloudWatch Events) event that invokes an AWS Lambda function to copy the RDS automated snapshot to the failover Region.\"\n-&gt; The CW Event should be a scheduled one. In addition, by default, this automatic snapshot occurs only once each day during a 30-minute backup window. We cannot guarantee the RPO.\n\nTherefore D is the correct one."
      },
      {
        "date": "2023-02-10T08:02:00.000Z",
        "voteCount": 2,
        "content": "A, does not match the requirement of \"LOWEST cost\".\nC and E ..... do not make sense."
      },
      {
        "date": "2023-02-16T05:50:00.000Z",
        "voteCount": 1,
        "content": "Sorry for the above ....\nB is not correct, because it does not recover the dataset.\nI changed my mind from \"BD\" to \"CD\"."
      },
      {
        "date": "2023-01-24T17:14:00.000Z",
        "voteCount": 1,
        "content": "C and D is correct. The same CloudFormation template should run upon failover to provision the same resources as in the primary with the exception that the DB storage is restored from the AWS backup taken every hour using the DB Snapshot ID."
      },
      {
        "date": "2022-12-22T01:21:00.000Z",
        "voteCount": 4,
        "content": "Answer: C,D\nB is wrong because it's an Auto Scaling Group, therefore there's no need to manually increase number of instances in the failover region."
      },
      {
        "date": "2022-12-22T00:31:00.000Z",
        "voteCount": 3,
        "content": "Answer: C,D\nB is wring because it's an Auto Scaling Group and there's no need to manually increase number of instances in the failover region"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 124,
    "url": "https://www.examtopics.com/discussions/amazon/view/93073-exam-aws-devops-engineer-professional-topic-1-question-124/",
    "body": "A company runs a three-tier web application in its production environment, which is built on a single AWS CloudFormation template made up of Amazon EC2 instances behind an ELB Application Load Balancer. The instances run in an EC2 Auto Scaling group across multiple Availability Zones. Data is stored in an Amazon RDS Multi-AZ DB instance with read replicas. Amazon Route 53 manages the application's public DNS record.<br><br>A DevOps engineer must create a workflow to mitigate a failed software deployment by rolling back changes in the production environment when a software cutover occurs for new application software.<br><br>What steps should the engineer perform to meet these requirements with the LEAST amount of downtime?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse CloudFormation to deploy an additional staging environment and configure the Route 53 DNS with weighted records. During cutover change the Route 53 A record weights to achieve an even traffic distribution between the two environments. Validate the traffic in the new environment and immediately terminate the old environment if tests are successful.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a single AWS Elastic Beanstalk environment to deploy the staging and production environments. Update the environment by uploading the ZIP file with the new application code. Swap the Elastic Beanstalk environment CNAME. Validate the traffic in the new environment and immediately terminate the old environment if tests are successful.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a single AWS Elastic Beanstalk environment and an AWS OpsWorks environment to deploy the staging and production environments. Update the environment by uploading the ZIP file with the new application code into the Elastic Beanstalk environment deployed with the OpsWorks stack. Validate the traffic in the new environment and immediately terminate the old environment if tests are successful.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS CloudFormation to deploy an additional staging environment, and configure the Route 53 DNS with weighted records. During cutover, increase the weight distribution to have more traffic directed to the new staging environment as workloads are successfully validated. Keep the old production environment in place until the new staging environment handles all traffic.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-02-10T08:15:00.000Z",
        "voteCount": 2,
        "content": "\"Immediately terminate\" is not a good practice found in A B C"
      },
      {
        "date": "2023-02-04T12:08:00.000Z",
        "voteCount": 3,
        "content": "Requirement: with the least amount of downtime\nB - wrong - this is not how you do it with the beanstalk - you also don't have to change cname in this case (single environment) - when you perform this type of update the app can be unavailable for some time\nD - correct"
      },
      {
        "date": "2023-01-24T18:13:00.000Z",
        "voteCount": 3,
        "content": "D is correct because as mentioned in the question there is  already a CloudFormation template to provision the resources  so why switch to EB. But I have to admit that this question is tricky. The answer could be B as well."
      },
      {
        "date": "2023-02-01T18:56:00.000Z",
        "voteCount": 1,
        "content": "B is correct as it clearly indicates how 2 environments will be provisioned and application deployed using EB. D on the other hand doesn't mention anything about app deployment, only about creating the environment using CloudFormation. Cloudformation cannot deploy an application."
      },
      {
        "date": "2023-01-22T23:37:00.000Z",
        "voteCount": 2,
        "content": "Option A - use A record, which is used with IP addresses\nOption C - OpsWorks - not needed\nOption B - with EB you use CNAME to swap between TWO environments\nOption D - fits well"
      },
      {
        "date": "2023-01-16T12:22:00.000Z",
        "voteCount": 2,
        "content": "https://www.examtopics.com/discussions/amazon/view/8099-exam-aws-devops-engineer-professional-topic-1-question-94/"
      },
      {
        "date": "2022-12-29T11:41:00.000Z",
        "voteCount": 2,
        "content": "B is the correct answer"
      },
      {
        "date": "2023-02-19T09:42:00.000Z",
        "voteCount": 1,
        "content": "Why not Z ?"
      },
      {
        "date": "2022-12-28T07:25:00.000Z",
        "voteCount": 4,
        "content": "D is correct\nElastic Beanstalk will be a much of a hassle as we will need to move the existing system in EC2 hence B and C are wrong\nBetween A and D, A tries deletes the older env immediately and does not change the R53 entries"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 125,
    "url": "https://www.examtopics.com/discussions/amazon/view/92414-exam-aws-devops-engineer-professional-topic-1-question-125/",
    "body": "An IT team has built an AWS CloudFormation template so others in the company can quickly and reliably deploy and terminate an application. The template creates an Amazon EC2 instance with a user data script to install the application and an Amazon S3 bucket that the application uses to serve static webpages while it is running.<br><br>All resources should be removed when the CloudFormation stack is deleted. However, the team observes that CloudFormation reports an error during stack deletion, and the S3 bucket created by the stack is not deleted.<br><br>How can the team resolve the error in the MOST efficient manner to ensure that all resources are deleted without errors?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a DeletionPolicy attribute to the S3 bucket resource, with the value Delete forcing the bucket to be removed when the stack is deleted.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a custom resource with an AWS Lambda function with the DependsOn attribute specifying the S3 bucket, and an IAM role. Write the Lambda function to delete all objects from the bucket when RequestType is Delete.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIdentify the resource that was not deleted. From the S3 console, empty the S3 bucket and then delete it.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReplace the EC2 and S3 bucket resources with a single AWS OpsWorks Stacks resource. Define a custom recipe for the stack to create and delete the EC2 instance and the S3 bucket."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 3,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-03-27T19:12:00.000Z",
        "voteCount": 2,
        "content": "I'm going for A:\nOption A uses the built-in CloudFormation functionality of DeletionPolicy to ensure that the S3 bucket is removed when the CloudFormation stack is deleted. This is a simple and well-documented feature that is widely used in CloudFormation templates."
      },
      {
        "date": "2023-10-04T09:17:00.000Z",
        "voteCount": 1,
        "content": "Correct IS BBBBBB:\nFor Amazon S3 buckets, you must delete all objects in the bucket for deletion to succeed.\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html"
      },
      {
        "date": "2023-01-24T18:15:00.000Z",
        "voteCount": 1,
        "content": "B is the right answer."
      },
      {
        "date": "2022-12-28T07:27:00.000Z",
        "voteCount": 2,
        "content": "The Correct Option is B"
      },
      {
        "date": "2022-12-23T07:16:00.000Z",
        "voteCount": 2,
        "content": "BBBBBBBBB"
      },
      {
        "date": "2022-12-21T23:40:00.000Z",
        "voteCount": 1,
        "content": "It's B. You can create a lambda function to clean up your bucket and invoke your lambda from your CloudFormation stack using a CustomResource."
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 126,
    "url": "https://www.examtopics.com/discussions/amazon/view/92417-exam-aws-devops-engineer-professional-topic-1-question-126/",
    "body": "A DevOps engineer must create a Linux AMI in an automated fashion. The newly created AMI identification must be stored in a location where other build pipelines can access the new identification programmatically.<br><br>What is the MOST cost-effective way to do this?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBuild a pipeline in AWS CodePipeline to download and save the latest operating system Open Virtualization Format (OVF) image to an Amazon S3 bucket. Customize the image by using the guestfish utility. Use the virtual machine (VM) import command to convert the OVF to an AMI. Store the AMI identification output as an AWS Systems Manager Parameter Store parameter.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Systems Manager Automation runbook with values instructing how the image should be created. Build a pipeline in AWS CodePipeline to execute the runbook to create the AMI. Store the AMI identification output as a Systems Manager Parameter Store parameter.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBuild a pipeline in AWS CodePipeline to take a snapshot of an Amazon EC2 instance running the latest version of the application. Start a new EC2 instance from the snapshot and update the running instance by using an AWS Lambda function. Take a snapshot of the updated instance and convert it to an AMI. Store the AMI identification output in an Amazon DynamoDB table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLaunch an Amazon EC2 instance and install Packer. Configure a Packer build with values defining how the image should be created. Build a Jenkins pipeline to invoke the Packer build to create an AMI. Store the AMI identification output in an Amazon DynamoDB table."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-03-04T19:28:00.000Z",
        "voteCount": 1,
        "content": "C and D are not efficient at all. B looks better than A."
      },
      {
        "date": "2023-02-10T08:32:00.000Z",
        "voteCount": 1,
        "content": "B is free ?"
      },
      {
        "date": "2023-01-24T18:19:00.000Z",
        "voteCount": 1,
        "content": "B is the right answer."
      },
      {
        "date": "2022-12-23T10:23:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/systems-manager/latest/userguide/automation-documents.html"
      },
      {
        "date": "2022-12-23T08:27:00.000Z",
        "voteCount": 2,
        "content": "Answer is B"
      },
      {
        "date": "2022-12-21T23:51:00.000Z",
        "voteCount": 2,
        "content": "Answer is B"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 127,
    "url": "https://www.examtopics.com/discussions/amazon/view/92995-exam-aws-devops-engineer-professional-topic-1-question-127/",
    "body": "An application running on a set of Amazon EC2 instances in an Auto Scaling group requires a configuration file to operate. The instances are created and maintained with AWS CloudFormation. A DevOps engineer wants the instances to have the latest configuration file when launched, and wants changes to the configuration file to be reflected on all the instances with a minimal delay when the CloudFormation template is updated. Company policy requires that application configuration files be maintained along with AWS infrastructure configuration files in source control.<br><br>Which solution will accomplish this?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the CloudFormation template, add an AWS Config rule. Place the configuration file content in the rule's InputParameters property, and set the Scope property to the EC2 Auto Scaling group. Add an AWS Systems Manager Resource Data Sync resource to the template to poll for updates to the configuration.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the CloudFormation template, add an EC2 launch template resource. Place the configuration file content in the launch template. Configure the cfn-init script to run when the instance is launched, and configure the cfn-hup script to poll for updates to the configuration.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the CloudFormation template add an EC2 launch template resource. Place the configuration file content in the launch template. Add an AWS Systems Manager Resource Data Sync resource to the template to poll for updates to the configuration.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the CloudFormation template, add CloudFormation init metadata. Place the configuration file content in the metadata. Configure the cfn-init script to run when the instance is launched, and configure the cfn-hup script to poll for updates to the configuration.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 6,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-10-20T07:38:00.000Z",
        "voteCount": 1,
        "content": "AWS Systems Manager Resource Data Sync is used to synchronize inventory data from multiple AWS accounts and multiple AWS Regions into a single Amazon S3 bucket. It is not used for updating configuration files on EC2 instances.\n\nThus, B."
      },
      {
        "date": "2023-04-15T05:32:00.000Z",
        "voteCount": 1,
        "content": "Both options B and D are valid solutions for the scenario described, but if company policy requires that application configuration files be maintained along with AWS infrastructure configuration files in source control, then option B would be the better solution as it allows the configuration file to be included in the launch template, which is part of the CloudFormation template, and thus, can be versioned and maintained along with the infrastructure configuration files in source control."
      },
      {
        "date": "2023-02-10T08:39:00.000Z",
        "voteCount": 1,
        "content": "A looks fast."
      },
      {
        "date": "2023-01-24T18:45:00.000Z",
        "voteCount": 3,
        "content": "D is the right answer. Need a MetaData with Cloudformation:init and a cfn-init command in userdata to retrieve the content (script) that needs to be executed on the EC2 instance. cfn-signal and wait can be used to have EC2 signal Cloudformation that the instance has been successfully configured."
      },
      {
        "date": "2022-12-28T09:03:00.000Z",
        "voteCount": 3,
        "content": "D seems to be the correct answer more details here\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/deploying.applications.html"
      },
      {
        "date": "2022-12-27T09:02:00.000Z",
        "voteCount": 1,
        "content": "Answer is D"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 128,
    "url": "https://www.examtopics.com/discussions/amazon/view/92168-exam-aws-devops-engineer-professional-topic-1-question-128/",
    "body": "A DevOps engineer is building a multistage pipeline with AWS CodePipeline to build, verify, stage, test, and deploy an application. A manual approval stage is required between the test stage and the deploy stage. The development team uses a custom chat tool with webhook support that requires near-real-time notifications.<br><br>How should the DevOps engineer configure status updates for pipeline activity and approval requests to post to the chat tool?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon CloudWatch Logs subscription that filters on CodePipeline Pipeline Execution State Change. Publish subscription events to an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the chat webhook URL to the SNS topic, and complete the subscription validation.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Lambda function that is invoked by AWS CloudTrail events. When a CodePipeline Pipeline Execution State Change event is detected, send the event details to the chat webhook URL.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon EventBridge (Amazon CloudWatch Events) rule that filters on CodePipeline Pipeline Execution State Change. Publish the events to an Amazon Simple Notification Service (Amazon SNS) topic. Create an AWS Lambda function that sends event details to the chat webhook URL. Subscribe the function to the SNS topic.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the pipeline code to send the event details to the chat webhook URL at the end of each stage. Parameterize the URL so that each pipeline can send to a different URL based on the pipeline environment."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 7,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-05-06T04:13:00.000Z",
        "voteCount": 1,
        "content": "C is more suitable"
      },
      {
        "date": "2023-02-28T09:20:00.000Z",
        "voteCount": 3,
        "content": "C. Create an Amazon EventBridge (Amazon CloudWatch Events) rule that filters on CodePipeline Pipeline Execution State Change. Publish the events to an Amazon Simple Notification Service (Amazon SNS) topic. Create an AWS Lambda function that sends event details to the chat webhook URL. Subscribe the function to the SNS topic.\n\nhttps://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html\n\nhttps://aws.amazon.com/es/premiumsupport/knowledge-center/lambda-subscribe-sns-topic-same-account/"
      },
      {
        "date": "2023-02-10T08:48:00.000Z",
        "voteCount": 2,
        "content": "D looks nearest to the \"near-real-time\" requirement."
      },
      {
        "date": "2023-01-24T18:47:00.000Z",
        "voteCount": 2,
        "content": "C is the answer"
      },
      {
        "date": "2023-01-04T00:17:00.000Z",
        "voteCount": 1,
        "content": "C is the answer"
      },
      {
        "date": "2022-12-28T17:56:00.000Z",
        "voteCount": 1,
        "content": "C is the answer"
      },
      {
        "date": "2022-12-23T07:30:00.000Z",
        "voteCount": 2,
        "content": "CCCCCCCC"
      },
      {
        "date": "2022-12-22T00:02:00.000Z",
        "voteCount": 4,
        "content": "It is C."
      },
      {
        "date": "2022-12-20T03:59:00.000Z",
        "voteCount": 4,
        "content": "C\nBest practise"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 129,
    "url": "https://www.examtopics.com/discussions/amazon/view/92583-exam-aws-devops-engineer-professional-topic-1-question-129/",
    "body": "A security team is concerned that a developer can unintentionally attach an Elastic IP address to an Amazon EC2 instance in production. No developer should be allowed to attach an Elastic IP address to an instance. The security team must be notified if any production server has an Elastic IP address at any time.<br><br>How can this task be automated?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Athena to query AWS CloudTrail logs to check for any associate-address attempts. Create an AWS Lambda function to disassociate the Elastic IP address from the instance, and alert the security team.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAttach an IAM policy to the developers' IAM group to deny associate-address permissions. Create a custom AWS Config rule to check whether an Elastic IP address is associated with any instance tagged as production, and alert the security team.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure that all IAM groups associated with developers do not have associate-address permissions. Create a scheduled AWS Lambda function to check whether an Elastic IP address is associated with any instance tagged as production, and alert the security team if an instance has an Elastic IP address associated with it.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Config rule to check that all production instances have EC2 IAM roles that include deny associate-address permissions. Verify whether there is an Elastic IP address associated with any instance, and alert the security team if an instance has an Elastic IP address associated with it."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-02-10T08:55:00.000Z",
        "voteCount": 2,
        "content": "Check with Lambda function does not prevent the accidents safely.\nD looks at only EC2 with a specifc IAM roles"
      },
      {
        "date": "2023-01-24T18:53:00.000Z",
        "voteCount": 3,
        "content": "B is correct. Config rule to check if EIP is associated with the EC2 instance and have have a IAM policy on the IAM group to ensure that no developer belonging to that IAM group is allowed to attach EIP."
      },
      {
        "date": "2022-12-28T09:11:00.000Z",
        "voteCount": 4,
        "content": "A - This only checks and remidiates, does not restrict the developers to attach EIP\nB - Correct\nC - This is not optimal\nD - EC2 IAM roles are not responsible for the EIP attachment the roles associated to developers are"
      },
      {
        "date": "2022-12-23T07:46:00.000Z",
        "voteCount": 1,
        "content": "B - correct"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 130,
    "url": "https://www.examtopics.com/discussions/amazon/view/92584-exam-aws-devops-engineer-professional-topic-1-question-130/",
    "body": "A company uses AWS Control Tower to manage its multi-account AWS environment. The company has historically created AWS accounts by using AWS Control Tower through the AWS Management Console. The company wants to implement an automated solution that will create new AWS accounts by using AWS Control Tower Account Factory.<br><br>A DevOps engineer is testing a new approach in which employees will upload a csv file into an Amazon S3 bucket. The .csv file will contain the information that is necessary to create a new AWS account. An AWS Lambda function will process event notifications from Amazon S3 when new files are created in the S3 bucket. The Lambda function will create the AWS account by using the AWS Service Catalog APIs.<br><br>The DevOps engineer needs to implement a solution to publish a notification to an Amazon Simple Notification Service (Amazon SNS) topic when the account creation process ends successfully.<br><br>What should the DevOps engineer do to automate the SNS notification?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure AWS Control Tower to publish to the SNS topic when the automatic drift detection feature identifies that a new account has been added to the service.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the AWS Control Tower Account Factory product in AWS Service Catalog to publish to the SNS topic when a new account product is launched with the service.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an Amazon EventBridge (Amazon CloudWatch Events) rule that reacts to the AWS Service Catalog ProvisionProduct event and publishes to the SNS topic.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an Amazon EventBridge (Amazon CloudWatch Events) rule that reacts to the AWS Control Tower CreateManagedAccount event and publishes to the SNS topic.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-02-17T08:14:00.000Z",
        "voteCount": 1,
        "content": "I thought B but it is clearly D\n\nhttps://aws.amazon.com/es/blogs/mt/using-lifecycle-events-to-track-aws-control-tower-actions-and-trigger-automated-workflows/"
      },
      {
        "date": "2023-02-06T05:02:00.000Z",
        "voteCount": 1,
        "content": "D - CreateManagedAccount: The log records whether AWS Control Tower successfully completed every action to create and provision a new account using account factory."
      },
      {
        "date": "2023-01-24T18:56:00.000Z",
        "voteCount": 2,
        "content": "D is correct."
      },
      {
        "date": "2022-12-28T17:59:00.000Z",
        "voteCount": 3,
        "content": "D is correct\nhttps://docs.aws.amazon.com/controltower/latest/userguide/lifecycle-events.html#create-managed-account"
      },
      {
        "date": "2022-12-23T07:48:00.000Z",
        "voteCount": 1,
        "content": "D - correct"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 131,
    "url": "https://www.examtopics.com/discussions/amazon/view/92585-exam-aws-devops-engineer-professional-topic-1-question-131/",
    "body": "A company has microservices running in AWS Lambda that read data from Amazon DynamoDB. The Lambda code is manually deployed by developers after successful testing. The company now needs the tests and deployments be automated and run in the cloud. Additionally, traffic to the new versions of each microservice should be incrementally shifted over time after deployment.<br><br>What solution meets all the requirements, ensuring the MOST developer velocity?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS CodePipeline configuration and set up a post-commit hook to trigger the pipeline after tests have passed. Use AWS CodeDeploy and create a Canary deployment configuration that specifies the percentage of traffic and interval.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS CodeBuild configuration that triggers when the test code is pushed. Use AWS CloudFormation to trigger an AWS CodePipeline configuration that deploys the new Lambda versions and specifies the traffic shift percentage and interval.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS CodePipeline configuration and set up the source code step to trigger when code is pushed. Set up the build step to use AWS CodeBuild to run the tests. Set up an AWS CodeDeploy configuration to deploy, then select the CodeDeployDefault.LambdaLinea10PercentEvery3Minutes option.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the AWS CLI to set up a post-commit hook that uploads the code to an Amazon S3 bucket after tests have passed. Set up an S3 event trigger that runs a Lambda function that deploys the new version. Use an interval in the Lambda function to deploy the code over time at the required percentage."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-02-10T12:36:00.000Z",
        "voteCount": 1,
        "content": "B is not correct, we do not need Cloud Formation \nD is too manual.\nA looks good, but does not make tests to be automated."
      },
      {
        "date": "2023-01-24T18:58:00.000Z",
        "voteCount": 2,
        "content": "C is correct."
      },
      {
        "date": "2023-01-04T00:09:00.000Z",
        "voteCount": 3,
        "content": "https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-configurations.html#deployment-configuration-lambda"
      },
      {
        "date": "2022-12-23T07:51:00.000Z",
        "voteCount": 4,
        "content": "C - correct"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 132,
    "url": "https://www.examtopics.com/discussions/amazon/view/93084-exam-aws-devops-engineer-professional-topic-1-question-132/",
    "body": "A development team wants to use AWS CloudFormation stacks to deploy an application. However, the developer IAM role does not have the required permissions to provision the resources that are specified in the AWS CloudFormation template. A DevOps engineer needs to implement a solution that allows the developers to deploy the stacks. The solution must follow the principle of least privilege.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an IAM policy that allows the developers to provision the required resources. Attach the policy to the developer IAM role.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an IAM policy that allows full access to AWS CloudFormation. Attach the policy to the developer IAM role.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS CloudFormation service role that has the required permissions. Grant the developer IAM role a cloudforrnation:* action. Use the new service role during stack deployments.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS CloudFormation service role that has the required permissions. Grant the developer IAM role the iam:PassRole permission. Use the new service role during stack deployments.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-04-06T18:14:00.000Z",
        "voteCount": 1,
        "content": "D-D-D-D-D-D-D-"
      },
      {
        "date": "2023-02-10T12:48:00.000Z",
        "voteCount": 1,
        "content": "A, does not follow the least privilege principle because the developers will have access to provision the resources without using CloudFormation.\nB, full access gives too much accesses.\nC, cloudformation:*.action gives too much accesses."
      },
      {
        "date": "2023-01-24T19:13:00.000Z",
        "voteCount": 2,
        "content": "D is correct. As an IAM user you need to have the IAM policy that allow you to pass the service role you are assigning to CloudFormation to execute the template to create/ update stack."
      },
      {
        "date": "2022-12-28T09:24:00.000Z",
        "voteCount": 4,
        "content": "D is the correct option\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-iam-servicerole.html"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 133,
    "url": "https://www.examtopics.com/discussions/amazon/view/93126-exam-aws-devops-engineer-professional-topic-1-question-133/",
    "body": "A developer is maintaining a fleet of 50 Amazon EC2 Linux servers. The servers are part of an Amazon EC2 Auto Scaling group, and also use Elastic Load Balancing for load balancing.<br><br>Occasionally, some application servers are being terminated after failing ELB HTTP health checks. The developer would like to perform a root cause analysis on the issue, but before being able to access application logs, the server is terminated.<br><br>How can log collection be automated?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Auto Scaling lifecycle hooks to put instances in a Pending:Wait state. Create an Amazon CloudWatch alarm for EC2 Instance Terminate Successful and trigger an AWS Lambda function that invokes an SSM Run Command script to collect logs, push them to Amazon S3, and complete the lifecycle action once logs are collected.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Auto Scaling lifecycle hooks to put instances in a Terminating:Wait state. Create an AWS Config rule for EC2 instance-terminate Lifecycle Action and trigger a step function that invokes a script to collect logs, push them to Amazon S3, and complete the lifecycle action once logs are collected.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Auto Scaling lifecycle hooks to put instances in a Terminating:Wait state. Create an Amazon CloudWatch subscription filter for EC2 Instance Terminate Successful and trigger a CloudWatch agent that invokes a script to collect logs, push them to Amazon S3, and complete the lifecycle action once logs are collected.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Auto Scaling lifecycle hooks to put instances in a Terminating:Wait state. Create an Amazon EventBridge rule for EC2 Instance-terminate Lifecycle Action and trigger an AWS Lambda function that invokes an SSM Run Command script to collect logs, push them to Amazon S3, and complete the lifecycle action once logs are collected.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 11,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-28T23:08:00.000Z",
        "voteCount": 9,
        "content": "A -- Pending Wait state is when the instance is created\nB -- AWS Config has no such rule\nC -- You cannot trigger a Cloudwatch agent\nD -- Correct"
      },
      {
        "date": "2023-02-10T13:14:00.000Z",
        "voteCount": 1,
        "content": "A must be eliminated because of \"Pending:Wait State\"\nC looks best, stick to CloudWatch"
      },
      {
        "date": "2023-02-17T08:35:00.000Z",
        "voteCount": 3,
        "content": "Cant be C\nCW subscription filters only Kinesis, Kinesis Firehorse, Lambda and Opensearch\nCorrect answer is D"
      },
      {
        "date": "2023-02-17T14:29:00.000Z",
        "voteCount": 1,
        "content": "Agree.  I am walking through the questions one more time. C must be wrong."
      },
      {
        "date": "2023-05-05T23:25:00.000Z",
        "voteCount": 1,
        "content": "Please don't confuse others."
      },
      {
        "date": "2023-01-24T19:17:00.000Z",
        "voteCount": 1,
        "content": "D is correct."
      },
      {
        "date": "2023-01-16T13:05:00.000Z",
        "voteCount": 1,
        "content": "https://www.examtopics.com/discussions/amazon/view/8042-exam-aws-devops-engineer-professional-topic-1-question-55/"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 134,
    "url": "https://www.examtopics.com/discussions/amazon/view/92560-exam-aws-devops-engineer-professional-topic-1-question-134/",
    "body": "A company is building a web and mobile application that uses a serverless architecture powered by AWS Lambda and Amazon API Gateway. The company wants to fully automate the backend Lambda deployment based on code that is pushed to the appropriate environment branch in an AWS CodeCommit repository.<br><br>The deployment must have the following:<br>\u2022\tSeparate environment pipelines for testing and production<br>\u2022\tAutomatic deployment that occurs for test environments only<br><br>Which steps should be taken to meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a new AWS CodePipeline service. Create a CodeCommit repository for each environment. Set up CodePipeline to retrieve the source code from the appropriate repository. Set up the deployment step to deploy the Lambda functions with AWS CloudFormation.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate two AWS CodePipeline configurations for test and production environments. Configure the production pipeline to have a manual approval step. Create a CodeCommit repository for each environment. Set up each CodePipeline to retrieve the source code from the appropriate repository. Set up the deployment step to deploy the Lambda functions with AWS CloudFormation.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate two AWS CodePipeline configurations for test and production environments. Configure the production pipeline to have a manual approval step. Create one CodeCommit repository with a branch for each environment. Set up each CodePipeline to retrieve the source code from the appropriate branch in the repository. Set up the deployment step to deploy the Lambda functions with AWS CloudFormation.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS CodeBuild configuration for test and production environments. Configure the production pipeline to have a manual approval step. Create one CodeCommit repository with a branch for each environment. Push the Lambda function code to an Amazon S3 bucket. Set up the deployment step to deploy the Lambda functions from the S3 bucket."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 3,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-12-23T03:54:00.000Z",
        "voteCount": 5,
        "content": "The answer is C. Two codepipelines two branches and manual approval step for production and automated deployment for test."
      },
      {
        "date": "2023-02-14T13:14:00.000Z",
        "voteCount": 2,
        "content": "D uses manual deployment, eliminated. \nA and B create two repositories, cannot be correct."
      },
      {
        "date": "2023-01-24T19:21:00.000Z",
        "voteCount": 1,
        "content": "C is correct."
      },
      {
        "date": "2023-01-16T15:18:00.000Z",
        "voteCount": 1,
        "content": "I'll go with C"
      },
      {
        "date": "2022-12-23T08:03:00.000Z",
        "voteCount": 2,
        "content": "c correct"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 135,
    "url": "https://www.examtopics.com/discussions/amazon/view/92563-exam-aws-devops-engineer-professional-topic-1-question-135/",
    "body": "A company has a guideline that every Amazon EC2 instance must be launched from an AMI that the company's security team produces. Every month, the security team sends an email message with the latest approved AMIs to all the development teams.<br><br>The development teams use AWS CloudFormation to deploy their applications. When developers launch a new service, they have to search their email for the latest AMIs that the security department sent. A DevOps engineer wants to automate the process that the security team uses to provide the AMI IDs to the development teams.<br><br>What is the MOST scalable solution that meets these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDirect the security team to use CloudFormation to create new versions of the AMIs and to list the AMI ARNs in an encrypted Amazon S3 object as part of the stack's Outputs section. Instruct the developers to use a cross-stack reference to load the encrypted S3 object and obtain the most recent AMI ARNs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDirect the security team to use a CloudFormation stack to create an AWS CodePipeline pipeline that builds new AMIs and places the latest AMI ARNs in an encrypted Amazon S3 object as part of the pipeline output. Instruct the developers to use a cross-stack reference within their own CloudFormation template to obtain the S3 object location and the most recent AMI ARNs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDirect the security team to use Amazon EC2 Image Builder to create new AMIs and to place the AMI ARNs as parameters in AWS Systems Manager Parameter Store. Instruct the developers to specify a parameter of type SSM in their CloudFormation stack to obtain the most recent AMI ARNs from Parameter Store.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDirect the security team to use Amazon EC2 Image Builder to create new AMIs and to create an Amazon Simple Notification Service (Amazon SNS) topic so that every development team can receive notifications. When the development teams receive a notification, instruct them to write an AWS Lambda function that will update their CloudFormation stack with the most recent AMI ARNs."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 8,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-23T04:27:00.000Z",
        "voteCount": 9,
        "content": "In the question \"When developer launches a new service\" A parameter store can be used to dynamically get new AMI ID's \nAnswer is C"
      },
      {
        "date": "2022-12-23T08:06:00.000Z",
        "voteCount": 6,
        "content": "c is the answer"
      },
      {
        "date": "2023-02-11T06:35:00.000Z",
        "voteCount": 1,
        "content": "C and D are better than A and B, because EC2 Image Builder is the correct tool to build AMI.\nD is more automated than C."
      },
      {
        "date": "2023-02-11T06:40:00.000Z",
        "voteCount": 1,
        "content": "Since the IAM is created by security team, dynamic reference should not be used. There is another reason to eliminate C."
      },
      {
        "date": "2023-02-07T06:10:00.000Z",
        "voteCount": 2,
        "content": "C - correct - https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/dynamic-references.html"
      },
      {
        "date": "2023-01-25T10:32:00.000Z",
        "voteCount": 1,
        "content": "C is the correct answer with this one"
      },
      {
        "date": "2023-01-24T19:24:00.000Z",
        "voteCount": 1,
        "content": "C is the right answer."
      },
      {
        "date": "2022-12-28T09:33:00.000Z",
        "voteCount": 5,
        "content": "D is wrong as it has too many manual steps and hassles\nC is correct"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 136,
    "url": "https://www.examtopics.com/discussions/amazon/view/92564-exam-aws-devops-engineer-professional-topic-1-question-136/",
    "body": "A DevOps engineer is setting up a container-based architecture. The engineer has decided to use AWS CloudFormation to automatically provision an Amazon ECS cluster and an Amazon EC2 Auto Scaling group to launch the EC2 container instances. After successfully creating the CloudFormation stack, the engineer noticed that, even though the ECS cluster and the EC2 instances were created successfully and the stack finished the creation, the EC2 instances were associating with a different cluster.<br><br>How should the DevOps engineer update the CloudFormation template to resolve this issue?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReference the EC2 instances in the AWS::ECS::Cluster resource and reference the ECS cluster in the AWS::ECS::Service resource.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReference the ECS cluster in the AWS::AutoScaling::LaunchConfiguration resource of the UserData property.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReference the ECS cluster in the AWS::EC2::Instance resource of the UserData property.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReference the ECS cluster in the AWS::CloudFormation::CustomResource resource to trigger an AWS Lambda function that registers the EC2 instances with the appropriate ECS cluster."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 10,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-04-07T18:11:00.000Z",
        "voteCount": 1,
        "content": "aaaaaaand it's D for me."
      },
      {
        "date": "2023-04-19T07:13:00.000Z",
        "voteCount": 1,
        "content": "change of mind - it's B."
      },
      {
        "date": "2023-03-04T20:22:00.000Z",
        "voteCount": 2,
        "content": "Its B. Refer this sample template https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/quickref-ecs.html"
      },
      {
        "date": "2023-02-28T10:14:00.000Z",
        "voteCount": 2,
        "content": "I don't use CloudFormation (I use Terraform instead) but it seems to be B:\nhttps://gist.github.com/TheDeveloper/242ebf1bfb11b8b6f9a9b0f454897ac7#file-aws-cluster-stack-yml-L84"
      },
      {
        "date": "2023-02-18T01:49:00.000Z",
        "voteCount": 1,
        "content": "Marque inicialmente D, pero en mi opini\u00f3n es B\n\nhttps://dev.to/awscommunity-asean/new-ecs-instance-registration-with-ecs-cluster-using-cmd-and-console-19f5"
      },
      {
        "date": "2023-02-17T19:07:00.000Z",
        "voteCount": 1,
        "content": "The reason for this is that when using AWS CloudFormation to create an Amazon ECS cluster and an EC2 Auto Scaling group to launch EC2 container instances, the EC2 instances must be registered with the ECS cluster to be able to participate in the cluster. This registration process can be achieved through an AWS Lambda function that is triggered by a CloudFormation Custom Resource.\n\nTherefore, by using a Custom Resource that triggers a Lambda function to register the EC2 instances with the appropriate ECS cluster, the DevOps engineer can ensure that the EC2 instances associate with the intended ECS cluster. Option A, B, and C are not the correct options since none of them address the issue of associating the EC2 instances with the correct ECS cluster."
      },
      {
        "date": "2023-02-10T14:37:00.000Z",
        "voteCount": 2,
        "content": "The correct answer is B.\n\nReferencing the ECS cluster in the AWS::AutoScaling::LaunchConfiguration resource of the UserData property allows you to associate the EC2 instances with the desired ECS cluster.\n\nOption D, which references the ECS cluster in the AWS::CloudFormation::CustomResource resource, can also be a valid solution, especially if you need to perform more complex operations beyond simply associating the EC2 instances with the ECS cluster. However, it requires additional steps, such as defining the AWS Lambda function and updating the IAM role permissions."
      },
      {
        "date": "2023-02-04T04:18:00.000Z",
        "voteCount": 2,
        "content": "B - correct - https://docs.aws.amazon.com/AmazonECS/latest/developerguide/launch_container_instance.html"
      },
      {
        "date": "2023-01-24T19:53:00.000Z",
        "voteCount": 2,
        "content": "I think the answer is D. I didn't see a way to update either LaunchConfiguration or ECS cluster section of the ECS template using EC2 with the new EC2 instance IDs. The only way is to automate that registration via a Lambda function."
      },
      {
        "date": "2023-01-16T05:12:00.000Z",
        "voteCount": 4,
        "content": "https://www.examtopics.com/discussions/amazon/view/28605-exam-aws-devops-engineer-professional-topic-1-question-173/"
      },
      {
        "date": "2023-01-13T23:04:00.000Z",
        "voteCount": 1,
        "content": "C is sufficient"
      },
      {
        "date": "2022-12-23T08:09:00.000Z",
        "voteCount": 3,
        "content": "BBBBBBBB"
      },
      {
        "date": "2022-12-23T04:37:00.000Z",
        "voteCount": 3,
        "content": "B is the answer"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 137,
    "url": "https://www.examtopics.com/discussions/amazon/view/93116-exam-aws-devops-engineer-professional-topic-1-question-137/",
    "body": "A company wants to use AWS Systems Manager documents to bootstrap physical laptops for developers. The bootstrap code is stored in GitHub. A DevOps engineer has already created a Systems Manager activation, installed the Systems Manager agent with the registration code, and installed an activation ID on all the laptops.<br><br>Which set of steps should be taken next?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the Systems Manager document to use the AWS-RunShellScript command to copy the files from GitHub to Amazon S3, then use the aws-downloadContent plugin with a sourceType of S3.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the Systems Manager document to use the aws-configurePackage plugin with an install action and point to the Git repository.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the Systems Manager document to use the aws-downloadContent plugin with a sourceType of GitHub and sourcelnfo with the repository details.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the Systems Manager document to use the aws:softwarelnventory plugin and run the script from the Git repository."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-04-07T18:15:00.000Z",
        "voteCount": 1,
        "content": "C looks sharp."
      },
      {
        "date": "2023-03-29T06:28:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/systems-manager/latest/userguide/composite-docs.html"
      },
      {
        "date": "2023-02-06T01:15:00.000Z",
        "voteCount": 1,
        "content": "A is the right answer"
      },
      {
        "date": "2023-02-04T04:45:00.000Z",
        "voteCount": 1,
        "content": "C - i would choose this one although it is called aws:downloadcontent (not -)\nD - wrong - it only gathers data and it is called aws:gathersoftwareinventory"
      },
      {
        "date": "2023-01-25T19:54:00.000Z",
        "voteCount": 1,
        "content": "Answer is C."
      },
      {
        "date": "2022-12-28T23:21:00.000Z",
        "voteCount": 2,
        "content": "C is correct as long as the GitHub is not of Enterprise version"
      },
      {
        "date": "2022-12-28T18:10:00.000Z",
        "voteCount": 2,
        "content": "This is discussed here as well with C as answer\nhttps://www.examtopics.com/discussions/amazon/view/47928-exam-aws-devops-engineer-professional-topic-1-question-238/"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 138,
    "url": "https://www.examtopics.com/discussions/amazon/view/92587-exam-aws-devops-engineer-professional-topic-1-question-138/",
    "body": "A company that uses electronic health records is running a fleet of Amazon EC2 instances with an Amazon Linux operating system. As part of patient privacy requirements, the company must ensure continuous compliance for patches for operating system and applications running on the EC2 instances.<br><br>How can the deployments of the operating system and application patches be automated using a default and custom repository?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Systems Manager to create a new patch baseline including the custom repository. Run the AWS-RunPatchBaseline document using the run command to verify and install patches.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Direct Connect to integrate the corporate repository and deploy the patches using Amazon CloudWatch scheduled events, then use the CloudWatch dashboard to create reports.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse yum-config-manager to add the custom repository under /etc/yum.repos.d and run yum-config-manager-enable to activate the repository.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Systems Manager to create a new patch baseline including the corporate repository. Run the AWS-AmazonLinuxDefaultPatchBaseline document using the run command to verify and install patches."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-12-25T10:19:00.000Z",
        "voteCount": 5,
        "content": "A looks correct.\nYou can use the document AWS-RunPatchBaseline to apply patches for both operating systems and applications. Taken from: https://docs.aws.amazon.com/systems-manager/latest/userguide/patch-manager-about-aws-runpatchbaseline.html"
      },
      {
        "date": "2023-04-30T12:28:00.000Z",
        "voteCount": 1,
        "content": "between A and D A is correct; document PatchBaseline for AmazonLinux does not exist , actually it is included in AWS-runPacthBaseline"
      },
      {
        "date": "2024-05-06T05:01:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/systems-manager/latest/userguide/patch-manager-predefined-and-custom-patch-baselines.html\nD wrong because you can't custom them"
      },
      {
        "date": "2023-04-07T18:23:00.000Z",
        "voteCount": 1,
        "content": "I say A."
      },
      {
        "date": "2023-02-11T13:23:00.000Z",
        "voteCount": 1,
        "content": "Because of the context \"As part of patient privacy ...\" This is a case of \"industry regulatory compliance\" match the \"Example 3\" of the document https://docs.aws.amazon.com/systems-manager/latest/userguide/patch-manager-how-it-works-alt-source-repository.html"
      },
      {
        "date": "2023-01-26T19:42:00.000Z",
        "voteCount": 2,
        "content": "Answer is A. The AWS_RunPatchBaseline command executes the default patch ( could be a custom patch that overrides the AWS default patch as the new default) for all OS and application layers."
      },
      {
        "date": "2023-01-22T06:51:00.000Z",
        "voteCount": 2,
        "content": "D is correct\nhttps://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-patch-baselines.html"
      },
      {
        "date": "2023-01-16T14:17:00.000Z",
        "voteCount": 1,
        "content": "https://www.examtopics.com/discussions/amazon/view/8525-exam-aws-devops-engineer-professional-topic-1-question-120/"
      },
      {
        "date": "2022-12-28T18:13:00.000Z",
        "voteCount": 2,
        "content": "A is the correct answer\nhttps://docs.aws.amazon.com/systems-manager/latest/userguide/patch-manager-about-aws-runpatchbaseline.html"
      },
      {
        "date": "2022-12-28T09:48:00.000Z",
        "voteCount": 3,
        "content": "A is the correct answer"
      },
      {
        "date": "2022-12-23T08:13:00.000Z",
        "voteCount": 1,
        "content": "DDDDDDDD"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 139,
    "url": "https://www.examtopics.com/discussions/amazon/view/93027-exam-aws-devops-engineer-professional-topic-1-question-139/",
    "body": "A web application has been deployed using an AWS Elastic Beanstalk application. The application developers are concerned that they are seeing high latency in two different areas of the application:<br><br>\u2022\tHTTP client requests to a third-party API<br>\u2022\tMySQL client library queries to an Amazon RDS database<br><br>A DevOps engineer must gather trace data to diagnose the issues.<br><br>Which steps will gather the trace information with the LEAST amount of changes and performance impacts to the application?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd additional logging to the application code. Use the Amazon CloudWatch agent to stream the application logs into Amazon OpenSearch Service. Query the log data in OpenSearch Service.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstrument the application to use the AWS X-Ray SDK. Post trace data to an Amazon OpenSearch Service cluster. Query the trace data for calls to the HTTP client and the MySQL client.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOn the AWS Elastic Beanstalk management page for the application, enable the AWS X-Ray daemon. View the trace data in the X-Ray console.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstrument the application using the AWS X-Ray SDK. On the AWS Elastic Beanstalk management page for the application, enable the X-Ray daemon. View the trace data in the X-Ray console.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 8,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-27T16:43:00.000Z",
        "voteCount": 10,
        "content": "It's D. https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environment-configuration-debugging.html"
      },
      {
        "date": "2022-12-28T09:51:00.000Z",
        "voteCount": 7,
        "content": "It is D. https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environment-configuration-debugging.html"
      },
      {
        "date": "2023-10-23T00:39:00.000Z",
        "voteCount": 2,
        "content": "C. All other options involve changes to the application."
      },
      {
        "date": "2024-05-06T05:29:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environment-configuration-debugging.html#:~:text=You%20can%20enable%20the%20X%2DRay%20daemon%20on%20a%20running%20environment%20in%20the%20Elastic%20Beanstalk%20console.\nAgree with C least amount of change and preformance"
      },
      {
        "date": "2023-10-20T08:10:00.000Z",
        "voteCount": 1,
        "content": "C. Least changes to the app, remember? \"Instrument the application\" is something to avoid."
      },
      {
        "date": "2023-02-11T13:36:00.000Z",
        "voteCount": 1,
        "content": "Between C and D, I prefer D. \nAWS X-Ray SDK is required for debugging https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environment-configuration-debugging.html\nIn this context, we only need to investigate latency, no debugging work."
      },
      {
        "date": "2023-01-26T19:49:00.000Z",
        "voteCount": 1,
        "content": "Answer is D. Both X-Ray Deamon and X-Ray SDK is needed."
      },
      {
        "date": "2023-01-26T05:45:00.000Z",
        "voteCount": 1,
        "content": "Question asks for least amount of changes. In the Beanstalk console it is one click to turn on X-Ray Daemon."
      },
      {
        "date": "2023-01-26T05:49:00.000Z",
        "voteCount": 1,
        "content": "Looking over the docs shared here, instrumentation in the code is needed. D, is better."
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 140,
    "url": "https://www.examtopics.com/discussions/amazon/view/92828-exam-aws-devops-engineer-professional-topic-1-question-140/",
    "body": "A company requires its internal business teams to launch resources through pre-approved AWS CloudFormation templates only. The security team requires automated monitoring when resources drift from their expected state.<br><br>Which strategy should be used to meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAllow users to deploy CloudFormation stacks using a CloudFormation service role only. Use CloudFormation drift detection to detect when resources have drifted from their expected state.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAllow users to deploy CloudFormation stacks using a CloudFormation service role only. Use AWS Config rules to detect when resources have drifted from their expected state.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAllow users to deploy CloudFormation stacks using AWS Service Catalog only. Enforce the use of a launch constraint. Use AWS Config rules to detect when resources have drifted from their expected state.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAllow users to deploy CloudFormation stacks using AWS Service Catalog only. Enforce the use of a template constraint. Use Amazon EventBridge notifications to detect when resources have drifted from their expected state."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-12-25T21:23:00.000Z",
        "voteCount": 7,
        "content": "The correct strategy to meet these requirements is C. Allow users to deploy CloudFormation stacks using AWS Service Catalog only. Enforce the use of a launch constraint. Use AWS Config rules to detect when resources have drifted from their expected state.\n\nAWS Service Catalog allows you to create and manage catalogs of IT services that are approved for use on AWS. You can use Service Catalog to create a centralized catalog of CloudFormation templates that have been pre-approved for use within your organization. By enforcing the use of a launch constraint, you can ensure that users can only launch resources through the approved CloudFormation templates."
      },
      {
        "date": "2022-12-25T21:24:00.000Z",
        "voteCount": 3,
        "content": "AWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. You can use Config rules to automatically check the configuration of your resources against predetermined conditions, and receive notifications when resource configurations drift from their expected state. By using Config rules to detect drift, you can ensure that your resources remain in their expected state.\n\nOption A is incorrect because it involves using CloudFormation drift detection rather than Config rules to detect drift. Option D is incorrect because it involves using Amazon EventBridge notifications rather than Config rules to detect drift. Option B is incorrect because it doesn't involve enforcing the use of a launch constraint in AWS Service Catalog."
      },
      {
        "date": "2023-04-30T13:03:00.000Z",
        "voteCount": 1,
        "content": "Yes C is correct, however you would need to create a custom rule tor detecting a 'drift'; https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config_develop-rules_cfn-guard.html\nJust to say that it is not straight-forward to detect the drift."
      },
      {
        "date": "2023-02-11T13:50:00.000Z",
        "voteCount": 2,
        "content": "I went for A because of this link https://aws.amazon.com/blogs/aws/new-cloudformation-drift-detection/\nHowever, we need to use Service Catalog because of the template is pre-approved. \nBetween C and D EventBridge cannot detect."
      },
      {
        "date": "2023-01-26T20:30:00.000Z",
        "voteCount": 1,
        "content": "Answer is C as we are supposed to use only approved Cloudformation templates and AWS Config is to be used for Drift detection. D is incorrect because EventBridge Notfication cannot detect drift, it is EventBridge rules that can look for certain events to detect drift."
      },
      {
        "date": "2023-01-16T14:22:00.000Z",
        "voteCount": 1,
        "content": "https://www.examtopics.com/discussions/amazon/view/48163-exam-aws-devops-engineer-professional-topic-1-question-239/"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 141,
    "url": "https://www.examtopics.com/discussions/amazon/view/92779-exam-aws-devops-engineer-professional-topic-1-question-141/",
    "body": "A space exploration company receives telemetry data from multiple satellites. Small packets of data are received through Amazon API Gateway and are placed directly into an Amazon Simple Queue Service (Amazon SOS) standard queue. A custom application is subscribed to the queue and transforms the data into a standard format.<br><br>Because of inconsistencies in the data that the satellites produce, the application is occasionally unable to transform the data. In these cases, the messages remain in the SQS queue. A DevOps engineer must develop a solution that retains the failed messages and makes them available to scientists for review and future processing.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure AWS Lambda to poll the SOS queue and invoke a Lambda function to check whether the queue messages are valid. If validation fails, send a copy of the data that is not valid to an Amazon S3 bucket so that the scientists can review and correct the data. When the data is corrected, amend the message in the SOS queue by using a replay Lambda function with the corrected data",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConvert the SQS standard queue to an SQS FIFO queue. Configure AWS Lambda to poll the SQS queue every 10 minutes by using an Amazon EventBridge schedule. Invoke the Lambda function to identify any messages with a SentTimestamp value that is older than 5 minutes, push the data to the same location as the application's output location, and remove the messages from the queue.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an SOS dead-letter queue. Modify the existing queue by including a redrive policy that sets the Maximum Receives setting to 1 and sets the dead-letter queue ARN to the ARN of the newly created queue. Instruct the scientists to use the dead-letter queue to review the data that is not valid. Reprocess this data at a later time.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure API Gateway to send messages to different SOS virtual queues that are named for each of the satellites. Update the application to use a new virtual queue for any data that it cannot transform, and send the message to the new virtual queue. Instruct the scientists to use the virtual queue to review the data that is not valid. Reprocess this data at a later time."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 9,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-02-19T11:17:00.000Z",
        "voteCount": 1,
        "content": "I don't understand C\nA redrive policy does not have a Maximum Receives setting.\nOnly the Dead-letter queue option has it."
      },
      {
        "date": "2023-02-19T11:19:00.000Z",
        "voteCount": 1,
        "content": "Go on test it on the console if you don't believe me"
      },
      {
        "date": "2023-02-18T04:31:00.000Z",
        "voteCount": 1,
        "content": "Umm...I have gone through the question again.....and I think A is not as that may generate a correct message, but there is no solution for the incorrect one already received in the SQS, obviate my argument to go with A; the most logical answer is C."
      },
      {
        "date": "2023-02-18T04:26:00.000Z",
        "voteCount": 1,
        "content": "I will disagree with the majority of responses. The doubt is between A or C, C seems the most correct, but if you read at no time says that the messages will be corrected, only that they will be \"reviewed\" by scientists, if only reviewed there is no correction and reprocess them again send them back to the dead queue ... a loop.... I vote for the A, which also coincides with the answer offered by the platform"
      },
      {
        "date": "2023-02-11T14:06:00.000Z",
        "voteCount": 2,
        "content": "B and D do not make sense, are eliminated. \nBetween A and C, I prefer A, because it's not a problem of data transporting. The untransformed telemetry data will be processed by SOS as normal. That's not a scenario to use DLQ."
      },
      {
        "date": "2023-01-26T20:36:00.000Z",
        "voteCount": 1,
        "content": "C is the right answer."
      },
      {
        "date": "2023-01-15T13:59:00.000Z",
        "voteCount": 2,
        "content": "https://www.examtopics.com/discussions/amazon/view/92779-exam-aws-devops-engineer-professional-topic-1-question-141/"
      },
      {
        "date": "2022-12-26T08:19:00.000Z",
        "voteCount": 3,
        "content": "C for sure"
      },
      {
        "date": "2022-12-25T21:19:00.000Z",
        "voteCount": 3,
        "content": "The correct solution to meet these requirements is C. Create an SQS dead-letter queue. Modify the existing queue by including a redrive policy that sets the Maximum Receives setting to 1 and sets the dead-letter queue ARN to the ARN of the newly created queue. Instruct the scientists to use the dead-letter queue to review the data that is not valid. Reprocess this data at a later time."
      },
      {
        "date": "2022-12-25T21:20:00.000Z",
        "voteCount": 3,
        "content": "A dead-letter queue is a queue that other Amazon SQS queues can target for messages that can't be processed. A message is sent to a dead-letter queue when the maximum number of receives of the message from the source queue has been reached without the message being deleted. By configuring the existing queue with a redrive policy and setting the Maximum Receives setting to 1, you can ensure that any messages that the application is unable to transform will be sent to the dead-letter queue after one failed attempt. The scientists can then review the data in the dead-letter queue and reprocess it at a later time."
      },
      {
        "date": "2022-12-25T21:21:00.000Z",
        "voteCount": 2,
        "content": "Option A is incorrect because it doesn't involve creating a dead-letter queue. Option B is incorrect because it involves deleting messages from the queue after a certain time has passed, rather than retaining them for later review and reprocessing. Option D is incorrect because it involves sending messages to different virtual queues based on the satellite that produced the data, rather than retaining them in a dead-letter queue for later review and reprocessing."
      },
      {
        "date": "2023-04-07T18:37:00.000Z",
        "voteCount": 1,
        "content": "Thanks HTTPS - great answer."
      },
      {
        "date": "2022-12-25T10:29:00.000Z",
        "voteCount": 1,
        "content": "Going with C on this. This seems like a good use case for a DLQ. The redrive policy with Maximum Receives of 1 will put a failed message into DLQ for investigation and retry after the Lambda code is updated to handle the new data."
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 142,
    "url": "https://www.examtopics.com/discussions/amazon/view/95469-exam-aws-devops-engineer-professional-topic-1-question-142/",
    "body": "A company runs applications in AWS accounts that are in an organization in AWS Organizations. The applications use Amazon EC2 instances and Amazon S3.<br><br>The company wants to detect potentially compromised EC2 instances, suspicious network activity, and unusual API activity in its existing AWS accounts and in any AWS accounts that the company creates in the future. When the company detects one of these events, the company wants to use an existing Amazon Simple Notification Service (Amazon SNS) topic to send a notification to its operational support team for investigation and remediation.<br><br>Which solution will meet these requirements in accordance with AWS best practices?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the organization's management account, configure an AWS account as the Amazon GuardDuty administrator account. In the GuardDuty administrator account, add the company's existing AWS accounts to GuardDuty as members. In the GuardDuty administrator account, create an Amazon EventBridge (Amazon CloudWatch Events) rule with an event pattern to match GuardDuty events and to forward matching events to the SNS topic.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the organization's management account, configure Amazon GuardDuty to add newly created AWS accounts by invitation and to send invitations to the existing AWS accounts. Create an AWS CloudFormation stack set that accepts the GuardDuty invitation and creates an Amazon EventBridge (Amazon CloudWatch Events) rule. Configure the rule with an event pattern to match GuardDuty events and to forward matching events to the SNS topic. Configure the CloudFormation stack set to deploy into all AWS accounts in the organization.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the organization's management account, create an AWS CloudTrail organization trail. Activate the organization trail in all AWS accounts in the organization. Create an SCP that enables VPC Flow Logs in each account in the organization Configure AWS Security Hub for the organization. Create an Amazon EventBridge (Amazon CloudWatch Events) rule with an event pattern to match Security Hub events and to forward matching events to the SNS topic.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the organization's management account, configure an AWS account as the AWS CloudTrail administrator account. In the CloudTrail administrator account, create a CloudTrail organization trail. Add the company's existing AWS accounts to the organization trail. Create an SCP that enables VPC Flow Logs in each account in the organization. Configure AWS Security Hub for the organization. Create an Amazon EventBridge (Amazon CloudWatch Events) rule with an event pattern to match Security Hub events and to forward matching events to the SNS topic."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 8,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 5,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-06-17T06:30:00.000Z",
        "voteCount": 1,
        "content": "\"Create an AWS CloudFormation stack set that accepts the GuardDuty invitation\" Really? Its A"
      },
      {
        "date": "2023-10-20T08:18:00.000Z",
        "voteCount": 2,
        "content": "A. Correct. The admin account handles event for all accounts.\nB. Stack sets? Not necessary.\nC. Doesn't involve GuardDuty.\nD. SCPs can't enable Flow logs."
      },
      {
        "date": "2023-06-20T02:07:00.000Z",
        "voteCount": 1,
        "content": "Option B is the best solution because it meets all of the requirements stated in the question and is in accordance with AWS best practices. It allows the company to detect potentially compromised EC2 instances, suspicious network activity, and unusual API activity in its existing AWS accounts and in any AWS accounts that the company creates in the future using Amazon GuardDuty. It also provides a solution for automatically adding future AWS accounts to GuardDuty by configuring GuardDuty to add newly created AWS accounts by invitation and to send invitations to the existing AWS accounts.\n\nOption A is not the best solution because it does not provide a solution for automatically adding future AWS accounts to GuardDuty. It only involves configuring an AWS account as the GuardDuty administrator account and adding the company\u2019s existing AWS accounts to GuardDuty as members."
      },
      {
        "date": "2023-06-15T16:21:00.000Z",
        "voteCount": 1,
        "content": "has to be C \"to detect potentially compromised EC2 instances, suspicious network activity, and unusual API activity\""
      },
      {
        "date": "2023-04-01T23:22:00.000Z",
        "voteCount": 2,
        "content": "According ChatGPT, option A deals with both existing and future AWS accounts. The solution involves configuring an AWS account as the Amazon GuardDuty administrator account in the organization's management account, and adding existing AWS accounts to GuardDuty as members. For future AWS accounts, the solution leverages GuardDuty's ability to automatically add new accounts that are created within the organization by enabling \"Add new accounts by invitation\" feature. This ensures that any new AWS accounts that are created in the future will automatically be added to the GuardDuty member accounts list and monitored for suspicious activity."
      },
      {
        "date": "2023-02-25T23:24:00.000Z",
        "voteCount": 4,
        "content": "repost: Cannot be A, it does not deal with future accounts at all"
      },
      {
        "date": "2023-02-24T07:37:00.000Z",
        "voteCount": 3,
        "content": "Option A is incorrect because while it correctly identifies Amazon GuardDuty as a solution to detect threats, it does not provide a way to deploy the necessary resources to all AWS accounts in the organization automatically."
      },
      {
        "date": "2023-02-28T23:39:00.000Z",
        "voteCount": 1,
        "content": "Sorry, I will go with A after having second look"
      },
      {
        "date": "2023-02-19T11:36:00.000Z",
        "voteCount": 1,
        "content": "Cannot be A, it does not deal with future accounts at all"
      },
      {
        "date": "2023-02-06T05:26:00.000Z",
        "voteCount": 1,
        "content": "B is the answer"
      },
      {
        "date": "2023-01-26T20:46:00.000Z",
        "voteCount": 1,
        "content": "Answer is A. You don't need to accept a GuardDuty invitation if the member GuardDuty accounts are within the same AWS Org."
      },
      {
        "date": "2023-01-17T15:04:00.000Z",
        "voteCount": 2,
        "content": "I'll go with A\nhttps://docs.aws.amazon.com/guardduty/latest/ug/guardduty_organizations.html"
      },
      {
        "date": "2023-01-17T11:48:00.000Z",
        "voteCount": 1,
        "content": "A sure"
      },
      {
        "date": "2023-01-15T10:49:00.000Z",
        "voteCount": 1,
        "content": "A seems correct"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 143,
    "url": "https://www.examtopics.com/discussions/amazon/view/95163-exam-aws-devops-engineer-professional-topic-1-question-143/",
    "body": "A retail company wants to use AWS Elastic Beanstalk to host its online sales website running on Java. Since this will be the production website the CTO has the following requirements for the deployment strategy:<br>\u2022\tZero downtime. While the deployment is ongoing the current Amazon EC2 instances in service should remain in service. No deployment or any other action should be performed on the EC2 instances because they serve production traffic.<br>\u2022\tA new fleet of instances should be provisioned for deploying the new application version.<br>\u2022\tOnce the new application version is deployed successfully in the new fleet of instances, the new instances should be placed in service and the old ones should be removed.<br>\u2022\tThe rollback should be as easy as possible. If the new fleet of instances fails to deploy the new application version, they should be terminated and the current instances should continue serving traffic as normal.<br>\u2022\tThe resources within the environment (EC2 Auto Scaling group, Elastic Load Balancing, Elastic Beanstalk DNS CNAME) should remain the same and no DNS change should be made.<br><br>Which deployment strategy will meet the requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse rolling deployments with a fixed amount of one instance at a time and set the healthy threshold to OK.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse rolling deployments with additional batch with a fixed amount of one instance at a time and set the healthy threshold to OK.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLaunch a new environment and deploy the new application version there, then perform a CNAME swap between environments.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse immutable environment updates to meet all the necessary requirements.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 7,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-01-14T01:39:00.000Z",
        "voteCount": 6,
        "content": "no rolling. So, eliminate A &amp; B\nno C (CNAME swap) because Elastic beanstalk DNS CNAME should remain the same\nD: immutable beanstalk meets all requirements"
      },
      {
        "date": "2023-12-26T12:03:00.000Z",
        "voteCount": 1,
        "content": "D, immutable deployment - new instances, fast rollback and no DNS change"
      },
      {
        "date": "2023-03-04T21:32:00.000Z",
        "voteCount": 1,
        "content": "D - Immutable - https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environmentmgmt-updates-immutable.html"
      },
      {
        "date": "2023-02-23T22:03:00.000Z",
        "voteCount": 1,
        "content": "Although D is the best answer but according to the following constraint \nThe resources within the environment (EC2 Auto Scaling group, Elastic Load Balancing, Elastic Beanstalk DNS CNAME) should remain the same and no DNS change should be made.\nIn Immutable deployment a new ASG is created"
      },
      {
        "date": "2023-03-04T21:31:00.000Z",
        "voteCount": 1,
        "content": "Its temporary. Ultimately, new instances are associated with original ASG - https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environmentmgmt-updates-immutable.html"
      },
      {
        "date": "2023-02-19T11:43:00.000Z",
        "voteCount": 1,
        "content": "Would be either C (blue/green) or D (immutable). \nSince it's mentioned that the DNS CNAME shoud remain the same, the answer is D"
      },
      {
        "date": "2023-01-26T21:02:00.000Z",
        "voteCount": 2,
        "content": "Answer is D.  Given the constraints C is not the right choice. Only D will meet the needs."
      },
      {
        "date": "2023-01-24T09:31:00.000Z",
        "voteCount": 1,
        "content": "With all these requirements in aggregate there is no \"right\" solution."
      },
      {
        "date": "2023-01-15T13:50:00.000Z",
        "voteCount": 1,
        "content": "https://www.examtopics.com/discussions/amazon/view/8523-exam-aws-devops-engineer-professional-topic-1-question-118/"
      },
      {
        "date": "2023-01-15T10:49:00.000Z",
        "voteCount": 2,
        "content": "D seems correct"
      },
      {
        "date": "2023-01-15T08:09:00.000Z",
        "voteCount": 2,
        "content": "immutable  updates"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 144,
    "url": "https://www.examtopics.com/discussions/amazon/view/95468-exam-aws-devops-engineer-professional-topic-1-question-144/",
    "body": "A company has AWS accounts that are members of the same organization in AWS Organizations. According to the company's security policy, IAM customer managed policies must be scoped to specific actions and must not include wildcard actions on wildcard resources.<br><br>If an IAM customer managed policy is created or modified in any of the company's AWS accounts to grant wildcard actions on resources that also specify wildcards, the policy must be detached from any IAM user, role, or group that the policy is attached to Individual AWS account administrators must not be able to prevent the removal of the policies.<br><br>Which combination of steps will meet these requirements? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure automatic remediation to run the AWSConfigRemediation-DetachIAMPolicy AWS Systems Manager Automation runbook.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure automatic remediation to invoke a custom AWS Lambda function to detach the IAM policy from the affected resources.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure automatic remediation to use AWS Systems Manager Run Command to detach the IAM policy from the affected resources.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTurn on AWS Config by using an AWS CloudFormation stack set that is created in a central account. Configure automatic deployment for the stack set, and specify the organization as the target. Configure the iam-policy-no-statements-with-full-access AWS Config managed rule in the central account.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTurn on AWS Config for the organization. Create a new AWS account. Configure the account as a delegated administrator account for AWS Config. Configure the iam-policy-no-statements-with-full-access AWS Config managed rule in the delegated administrator account."
    ],
    "answer": "AD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AD",
        "count": 4,
        "isMostVoted": true
      },
      {
        "answer": "AE",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "BD",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-10-20T08:24:00.000Z",
        "voteCount": 1,
        "content": "AD (though E is also correct)"
      },
      {
        "date": "2023-10-23T01:06:00.000Z",
        "voteCount": 2,
        "content": "\"Individual AWS account administrators must not be able to prevent the removal of the policies\" \u2013 only E satisfies this requirement. Therefore, AE."
      },
      {
        "date": "2023-04-01T23:45:00.000Z",
        "voteCount": 1,
        "content": "Option A suggests using the AWSConfigRemediation-DetachIAMPolicy AWS Systems Manager Automation runbook to detach the IAM policy from the affected resources. However, this runbook is not designed to detect or remediate IAM policies with wildcard actions on wildcard resources. Instead, this runbook is used to remediate IAM policies that allow all actions for specified services, actions, or resources.\n\nTherefore, this option is not appropriate for the scenario described in the question, as it does not meet the requirement to detect and remove IAM policies with wildcard actions on wildcard resources, that's why for me the correct answer is B and D"
      },
      {
        "date": "2023-04-02T00:16:00.000Z",
        "voteCount": 1,
        "content": "no, A and D has to be the correct answer, my bad sorry"
      },
      {
        "date": "2023-02-19T11:59:00.000Z",
        "voteCount": 1,
        "content": "https://aws.amazon.com/blogs/mt/using-delegated-admin-for-aws-config-operations-and-aggregation/\nI'll go with D. Only way to turn on AWS Config for the other accounts is either with a stackset or from the accounts directly.\n\n\nCannot be E as there is no such thing as turn on AWS Config for the organization.\nThe rule since it's a managed rule cannot be deployed to the organization directly. \nOnly a custom rule can. That's why we need D"
      },
      {
        "date": "2023-02-10T15:26:00.000Z",
        "voteCount": 3,
        "content": "The correct options are A and D. Option A is to use AWSConfigRemediation-DetachIAMPolicy, an AWS Systems Manager Automation runbook, to automatically detach the IAM policy from the affected resources. Option D is to turn on AWS Config in the central account using an AWS CloudFormation stack set and configure the iam-policy-no-statements-with-full-access AWS Config managed rule. This allows you to monitor IAM policies across all member accounts in the organization and detect when policies are in violation of the company's security policy, ensuring that they are automatically remediated."
      },
      {
        "date": "2023-01-27T19:08:00.000Z",
        "voteCount": 2,
        "content": "A&amp;D, System Manager Automation to remediate and use of StackSets to apply the config rules across all accounts and regions."
      },
      {
        "date": "2023-01-25T16:19:00.000Z",
        "voteCount": 2,
        "content": "Was reading through the Config FAQs today. Config is deployed centrally in a AWS Organization with a StackSet."
      },
      {
        "date": "2023-01-17T17:00:00.000Z",
        "voteCount": 2,
        "content": "I'll go with A and D"
      },
      {
        "date": "2023-01-17T17:01:00.000Z",
        "voteCount": 2,
        "content": "https://aws.amazon.com/blogs/mt/managing-aws-organizations-accounts-using-aws-config-and-aws-cloudformation-stacksets/"
      },
      {
        "date": "2023-01-17T11:53:00.000Z",
        "voteCount": 3,
        "content": "AE for me"
      },
      {
        "date": "2023-01-15T10:48:00.000Z",
        "voteCount": 2,
        "content": "A, E seems corrrect"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 145,
    "url": "https://www.examtopics.com/discussions/amazon/view/95097-exam-aws-devops-engineer-professional-topic-1-question-145/",
    "body": "An application team has three environments for their application: development, pre-production, and production. The team recently adopted AWS CodePipeline. However, the team has had several deployments of misconfigured or nonfunctional development code into the production environment, resulting in user disruption and downtime. The DevOps engineer must review the pipeline and add steps to identify problems with the application before it is deployed.<br><br>What should the engineer do to identify functional issues during the deployment process? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Inspector to add a test action to the pipeline. Use the Amazon Inspector Runtime Behavior Analysis Inspector rules package to check that the deployed code complies with company security standards before deploying it to production.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUsing AWS CodeBuild to add a test action to the pipeline to replicate common user activities and ensure that the results are as expected before progressing to production deployment.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS CodeDeploy action in the pipeline with a deployment configuration that automatically deploys the application code to a limited number of instances. The action then pauses the deployment so that the QA team can review the application functionality. When the review is complete, CodeDeploy resumes and deploys the application to the remaining production Amazon EC2 instances.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAfter the deployment process is complete, run a testing activity on an Amazon EC2 instance in a different region that accesses the application to simulate user behavior. If unexpected results occur the testing activity sends a warning to an Amazon SNS topic. Subscribe to the topic to get updates.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd an AWS CodeDeploy action in the pipeline to deploy the latest version of the development code to pre-production Add a manual approval action in the pipeline so that the QA team can test and confirm the expected functionality. After the manual approval action, add a second CodeDeploy action that deploys the approved code to the production environment.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "BE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BE",
        "count": 8,
        "isMostVoted": true
      },
      {
        "answer": "BC",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "AC",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-01-13T14:27:00.000Z",
        "voteCount": 6,
        "content": "B E for me"
      },
      {
        "date": "2023-05-06T23:53:00.000Z",
        "voteCount": 1,
        "content": "B and E makes more sense, according to what is given in the question"
      },
      {
        "date": "2023-04-02T04:50:00.000Z",
        "voteCount": 1,
        "content": "Deploying to pre-production before deploying to production is a good practice, but for the purpose of identifying functional issues during the deployment process, options B and C are the best choices.\n\nOption B allows you to test the application before it is deployed to pre-production or production, ensuring that the code changes have not introduced any functional issues that can cause user disruption or downtime.\n\nOption C allows you to deploy the application code to a limited number of instances in the production environment and pause the deployment so that the QA team can review the application functionality. Once the review is complete, CodeDeploy resumes and deploys the application to the remaining production instances. This approach allows you to catch any functional issues during the deployment process, preventing them from causing user disruption or downtime."
      },
      {
        "date": "2023-04-02T04:53:00.000Z",
        "voteCount": 1,
        "content": "In option C, the functional testing occurs during the deployment process, before the application is deployed to the entire production environment. This approach allows for the quick identification and resolution of any issues, reducing the risk of user disruption or downtime.\n\nIn option E, the functional testing occurs after the application has been deployed to the pre-production environment and manual approval has been granted. Although the QA team can still identify and resolve any issues before approving the deployment to the production environment, this approach does not provide the same level of risk reduction as option C because the application has already been deployed to a live environment"
      },
      {
        "date": "2023-04-02T04:56:00.000Z",
        "voteCount": 1,
        "content": "In option B, the testing occurs within AWS CodeBuild as part of the pipeline, and it involves replicating common user activities and ensuring that the results are as expected before progressing to production deployment. This type of testing is typically referred to as \"unit testing\" and focuses on verifying the individual components of the application.\n\nIn option C, the testing occurs during the deployment process using AWS CodeDeploy, and it involves functional testing of the entire application on a limited number of instances in the production environment. This type of testing is typically referred to as \"integration testing\" and focuses on verifying that the components of the application work together correctly in a live environment."
      },
      {
        "date": "2023-02-18T05:44:00.000Z",
        "voteCount": 1,
        "content": "B and E"
      },
      {
        "date": "2023-02-12T05:08:00.000Z",
        "voteCount": 1,
        "content": "C is better than E. In E, \"a second CodeDeploy\" action is not a good practice. \nSince \"the team has had several deployments of misconfigured or nonfunctional development code into the production environment ....\", we need to detect existing problems.  A is better than B."
      },
      {
        "date": "2023-02-18T05:43:00.000Z",
        "voteCount": 1,
        "content": "\"Instances\".... where in the statement does it talk about instances? \n  why should we assume that they are involved?\n\nI vote for A and E"
      },
      {
        "date": "2023-02-10T15:30:00.000Z",
        "voteCount": 1,
        "content": "Yes, B and E are the correct answers. Option B involves using AWS CodeBuild to add a test action to the pipeline and replicate common user activities to ensure that the results are as expected before progressing to production deployment. Option E involves adding an AWS CodeDeploy action in the pipeline to deploy the latest version of the development code to pre-production, a manual approval action in the pipeline so that the QA team can test and confirm the expected functionality, and a second CodeDeploy action that deploys the approved code to the production environment. These two options help to ensure that functional issues are identified and addressed before the application is deployed to the production environment, reducing the risk of user disruption and downtime."
      },
      {
        "date": "2023-01-27T19:10:00.000Z",
        "voteCount": 1,
        "content": "B and E are correct"
      },
      {
        "date": "2023-01-17T17:04:00.000Z",
        "voteCount": 1,
        "content": "B E are correct"
      },
      {
        "date": "2023-01-15T10:56:00.000Z",
        "voteCount": 1,
        "content": "B, E are seems correct"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 146,
    "url": "https://www.examtopics.com/discussions/amazon/view/95164-exam-aws-devops-engineer-professional-topic-1-question-146/",
    "body": "A company is using AWS CodeDeploy to automate software deployment. The deployment must meet these requirements:<br><br>\u2022\tA number of instances must be available to serve traffic during the deployment. Traffic must be balanced across those instances, and the instances must automatically heal in the event of failure.<br>\u2022\tA new fleet of instances must be launched for deploying a new revision automatically, with no manual provisioning.<br>\u2022\tTraffic must be rerouted to the new environment to half of the new instances at a time. The deployment should succeed if traffic is rerouted to at least half of the instances; otherwise, it should fail.<br>\u2022\tBefore routing traffic to the new fleet of instances, the temporary files generated during the deployment process must be deleted.<br>\u2022\tAt the end of a successful deployment, the original instances in the deployment group must be deleted immediately to reduce costs.<br><br>How can a DevOps engineer meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an Application Load Balancer and an in-place deployment. Associate the Auto Scaling group with the deployment group. Use the Automatically copy Auto Scaling group option, and use CodeDeployDefault.OneAtAtime as the deployment configuration. Instruct AWS CodeDeploy to terminate the original instances in the deployment group, and use the AllowTraffic hook within appspec.yml to delete the temporary files.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an Application Load Balancer and a blue/green deployment. Associate the Auto Scaling group and Application Load Balancer target group with the deployment group. Use the Automatically copy Auto scaling group option, create a custom deployment configuration with minimum healthy hosts defined as 50%, and assign the configuration to the deployment group. Instruct AWS CodeDeploy to terminate the original instances in the deployment group, and use the BeforeBlockTraffic hook within appspec.yml to delete the temporary files.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an Application Load Balancer and a blue/green deployment. Associate the Auto Scaling group and the Application Load Balancer target group with the deployment group. Use the Automatically copy Auto Scaling group option, and use CodeDeployDefault.HalfAtAtime as the deployment configuration. Instruct AWS CodeDeploy to terminate the original instances in the deployment group, and use the BeforeAllowTraffic hook within appspec.yml to delete the temporary files.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an Application Load Balancer and an in-place deployment. Associate the Auto Scaling group and Application Load Balancer target group with the deployment group. Use the Automatically copy Auto Scaling group option, and use CodeDeployDefault AllatOnce as a deployment configuration. Instruct AWS CodeDeploy to terminate the original instances in the deployment group, and use the BlockTraffic hook within appspec.yml to delete the temporary files."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-10-20T08:32:00.000Z",
        "voteCount": 1,
        "content": "B, as BeforeBlockTraffic happens before traffic is shifted, and BeforeAllowTraffic afterwards."
      },
      {
        "date": "2023-10-13T04:41:00.000Z",
        "voteCount": 1,
        "content": "B, not C. \n\nB. This option uses blue/green deployment, which meets the requirement for automatic launching of a new fleet of instances. It also uses a custom deployment configuration with minimum healthy hosts defined as 50%, which meets the requirement for rerouting traffic to half of the new instances at a time. The BeforeBlockTraffic hook will delete temporary files before routing traffic, and the original instances will be terminated post deployment, meeting all the requirements.\n\nC. This option uses blue/green deployment and CodeDeployDefault.HalfAtAtime. However, the BeforeAllowTraffic hook is used to delete temporary files. The requirement states that temporary files must be deleted before routing traffic. In a blue/green deployment, the BeforeAllowTraffic hook is invoked after the traffic has been rerouted. Therefore, this option does not meet the requirement."
      },
      {
        "date": "2024-05-06T07:03:00.000Z",
        "voteCount": 1,
        "content": "Before routing traffic to the new fleet of instances, the temporary files generated during the deployment process must be deleted =&gt; BeforeAllowTraffic"
      },
      {
        "date": "2024-05-06T07:03:00.000Z",
        "voteCount": 1,
        "content": "BeforeBlockTraffic using when delete original"
      },
      {
        "date": "2023-05-01T00:49:00.000Z",
        "voteCount": 1,
        "content": "C -&gt; correct; A and D eliminated because of 'in-place' deployment; B eliminated because of 'before block traffic'"
      },
      {
        "date": "2023-02-12T05:17:00.000Z",
        "voteCount": 1,
        "content": "A and D are eliminated because \"in-place\" deployment does not match the scenario.\nB is a manual version of C, should be avoided."
      },
      {
        "date": "2023-02-15T02:27:00.000Z",
        "voteCount": 1,
        "content": "\"B is a manual version of C\" is not a correct description."
      },
      {
        "date": "2023-01-27T19:31:00.000Z",
        "voteCount": 3,
        "content": "C is correct. CodeDeployDefault.HalfAtAtime and BeforeAllowTraffic which is associated with the green instance where the cleanup is required just before the traffic is directed to these instances."
      },
      {
        "date": "2023-01-15T13:28:00.000Z",
        "voteCount": 1,
        "content": "https://www.examtopics.com/discussions/amazon/view/5505-exam-aws-devops-engineer-professional-topic-1-question-70/"
      },
      {
        "date": "2023-01-15T11:10:00.000Z",
        "voteCount": 1,
        "content": "C seems correct."
      },
      {
        "date": "2023-01-14T01:43:00.000Z",
        "voteCount": 2,
        "content": "C because of HaltAtATime"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 147,
    "url": "https://www.examtopics.com/discussions/amazon/view/95098-exam-aws-devops-engineer-professional-topic-1-question-147/",
    "body": "The security team depends on AWS CloudTrail to detect sensitive security issues in the company's AWS account The DevOps engineer needs a solution to auto-remediate CloudTrail being turned off in an AWS account.<br><br>What solution ensures the LEAST amount of downtime for the CloudTrail log deliveries?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon EventBridge (Amazon CloudWatch Events) rule for the CloudTrail StopLogging event. Create an AWS Lambda function that uses the AWS SDK to call StartLogging on the ARN of the resource in which StopLogging was called. Add the Lambda function ARN as a target to the EventBridge (CloudWatch Events) rule.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the AWS-managed CloudTrail-enabled AWS Config rule, set with a periodic interval of 1 hour. Create an Amazon EventBridge (Amazon CloudWatch Events) rule for AWS Config rules compliance change. Create an AWS Lambda function that uses the AWS SDK to call StartLogging on the ARN of the resource in which StopLogging was called. Add the Lambda function ARN as a target to the EventBridge (CloudWatch Events) rule.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon EventBridge (Amazon CloudWatch Events) rule for a scheduled event every 5 minutes. Create an AWS Lambda function that uses the AWS SDK to call StartLogging on a CloudTrail trail in the AWS account. Add the Lambda function ARN as a target to the EventBridge (CloudWatch Events) rule.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLaunch a t2.nano instance with a script running every 5 minutes that uses the AWS SDK to query CloudTrail in the current account. If the CloudTrail trail is disabled, have the script re-enable the trail."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-02-12T05:33:00.000Z",
        "voteCount": 3,
        "content": "Except A, all have time interval."
      },
      {
        "date": "2023-01-27T19:54:00.000Z",
        "voteCount": 1,
        "content": "Answer is A- B although is a detective mechansim, not sure why CloudWatch logs is used to look for Config events. Also periodic trigger type of 1 hour is not as good as an average delay of 15 minutes in receiving cloudTrail logs into CloudWatch. Also C and D are not detective mechansims."
      },
      {
        "date": "2023-01-15T13:27:00.000Z",
        "voteCount": 1,
        "content": "https://www.examtopics.com/discussions/amazon/view/6898-exam-aws-devops-engineer-professional-topic-1-question-65/"
      },
      {
        "date": "2023-01-15T11:33:00.000Z",
        "voteCount": 1,
        "content": "B seems correct\nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/automatically-re-enable-aws-cloudtrail-by-using-a-custom-remediation-rule-in-aws-config.html"
      },
      {
        "date": "2023-01-15T11:42:00.000Z",
        "voteCount": 1,
        "content": "Chaning from B to A as This will ensure least downtime. B may create about an hour downtime and that can not be the correct answer."
      },
      {
        "date": "2023-01-13T14:28:00.000Z",
        "voteCount": 4,
        "content": "A for me"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 148,
    "url": "https://www.examtopics.com/discussions/amazon/view/95099-exam-aws-devops-engineer-professional-topic-1-question-148/",
    "body": "A DevOps team supports many accounts across an organization in AWS Organizations. The DevOps team has decided to use AWS Coring across the organization to implement centralized automatic remediation of Amazon S3 buckets that have public ACLs. Individual accounts must not be able to modify the remediation strategy.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Config conformance pack that contains a rule that checks for S3 buckets that have public ACLs. Configure the conformance pack to use an AWS Systems Manager Automation runbook to block public access to the S3 buckets. Deploy the conformance pack across the organization.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure AWS Config rules that detect S3 buckets that have public ACLs. Configure a remediation action that uses AWS Lambda to block public access to the S3 buckets. Use AWS CloudFormation StackSets to deploy the rules across the organization.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure AWS Config rules that detect S3 buckets that have public ACLs. Configure a remediation action that uses an AWS Systems Manager Automation runbook to block public access to the S3 buckets. Use AWS CloudFormation StackSets to deploy the rules across the organization.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Config conformance pack that contains a rule that checks for 53 buckets that have public ACLs. Configure the conformance pack to use an AWS Lambda function to block public access to the S3 buckets. Deploy the conformance pack across the organization."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 8,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-10-31T04:16:00.000Z",
        "voteCount": 1,
        "content": "A\n\n\"These conformance packs and their underlying config rules and remediations actions are not modifiable by your organization\u2019s member accounts. Only master accounts can create, update, and delete organization conformance packs.\""
      },
      {
        "date": "2023-03-04T13:39:00.000Z",
        "voteCount": 1,
        "content": "Option B https://aws.amazon.com/blogs/security/how-to-use-aws-config-to-monitor-for-and-respond-to-amazon-s3-buckets-allowing-public-access/"
      },
      {
        "date": "2023-02-19T12:28:00.000Z",
        "voteCount": 1,
        "content": "It's not D because you can't configure a conformance pack to use an AWS Lambda function.\nYou can do it with systems manager automation which is not referenced here.\nTherefore it's A"
      },
      {
        "date": "2023-02-12T05:53:00.000Z",
        "voteCount": 1,
        "content": "https://aws.amazon.com/blogs/mt/best-practices-for-aws-config-conformance-packs/"
      },
      {
        "date": "2023-02-17T08:00:00.000Z",
        "voteCount": 1,
        "content": "Sorry, D is wrong. Lambda function does not help here. I go A."
      },
      {
        "date": "2023-02-04T11:29:00.000Z",
        "voteCount": 3,
        "content": "A - correct - conformance pack bundles rules with remediation - you can not change it - conformance pack can use AWS-DisableS3BucketPublicReadWrite SM runbook"
      },
      {
        "date": "2023-01-27T20:12:00.000Z",
        "voteCount": 3,
        "content": "A is the right answer based on the blog below. A is correct and not D because remediation action on a config rule is possible only using System Manager Automation runbook. You can trigger Lambda only if you use CloudWatchEvents to detect Config Rule Compliance Change event."
      },
      {
        "date": "2023-02-17T08:02:00.000Z",
        "voteCount": 1,
        "content": "But \"CloudWatchEvents to detect Config Rule\" does not match this scenario crossing many accounts in an organisation.\nSo A must be the answer."
      },
      {
        "date": "2023-01-15T11:55:00.000Z",
        "voteCount": 2,
        "content": "A seems correct"
      },
      {
        "date": "2023-01-15T08:34:00.000Z",
        "voteCount": 2,
        "content": "why not D ?"
      },
      {
        "date": "2023-01-14T17:28:00.000Z",
        "voteCount": 3,
        "content": "Option A\nhttps://aws.amazon.com/blogs/mt/deploying-conformance-packs-across-an-organization-with-automatic-remediation/"
      },
      {
        "date": "2023-01-13T14:30:00.000Z",
        "voteCount": 4,
        "content": "A for me. Conformance packs are more suitable for organization"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 149,
    "url": "https://www.examtopics.com/discussions/amazon/view/95094-exam-aws-devops-engineer-professional-topic-1-question-149/",
    "body": "A DevOps engineer is working on a data archival project that requires the migration of on-premises data to an Amazon S3 bucket. The DevOps engineer develops a script that incrementally archives on-premises data that is older than 1 month to Amazon S3. Data that is transferred to Amazon S3 is deleted from the on-premises location. The script uses the S3 PutObject operation.<br><br>During a code review, the DevOps engineer notices that the script does not verify whether the data was successfully copied to Amazon S3. The DevOps engineer must update the script to ensure that data is not corrupted during transmission. The script must use MD5 checksums to verify data integrity before the on-premises data is deleted.<br><br>Which solutions for the script will meet these requirements? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCheck the returned response for the Versionld. Compare the returned VersionId against the MD5 checksum.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInclude the MD5 checksum within the Content-MD5 parameter. Check the operation call's return status to find out if an error was returned.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInclude the checksum digest within the tagging parameter as a URL query parameter.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCheck the returned response for the ETag. Compare the returned ETag against the MD5 checksum.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInclude the checksum digest within the Metadata parameter as a name-value pair. After upload, use the S3 HeadObject operation to retrieve metadata from the object."
    ],
    "answer": "BD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BD",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-02-28T11:15:00.000Z",
        "voteCount": 3,
        "content": "B for checking that it was sent without any data loss.\nD for checking that the data stored was the expected one without any issues."
      },
      {
        "date": "2023-02-10T16:37:00.000Z",
        "voteCount": 3,
        "content": "Option B is to include the MD5 checksum within the Content-MD5 parameter and check the operation call's return status to find out if an error was returned. Option D is to check the returned response for the ETag and compare the returned ETag against the MD5 checksum. This way, the script can ensure the data was successfully transferred to Amazon S3 and is not corrupted during transmission."
      },
      {
        "date": "2023-01-27T20:24:00.000Z",
        "voteCount": 1,
        "content": "B,D is correct. MD5 checksum and ETag to verify the checksum passed."
      },
      {
        "date": "2023-01-13T14:25:00.000Z",
        "voteCount": 3,
        "content": "B D for me.https://docs.aws.amazon.com/AmazonS3/latest/userguide/checking-object-integrity.html"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 150,
    "url": "https://www.examtopics.com/discussions/amazon/view/95101-exam-aws-devops-engineer-professional-topic-1-question-150/",
    "body": "A company uses AWS CodeCommit for source code control. Developers apply their changes to various feature branches and create pull requests to move those changes to the main branch when the changes are ready for production.<br><br>The developers should not be able to push changes directly to the main branch. The company applied the AWSCodeCommitPowerUser managed policy to the developers' IAM role, and now these developers can push changes to the main branch directly on every repository in the AWS account.<br><br>What should the company do to restrict the developers' ability to push changes to the main branch directly?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an additional policy to include a Deny rule for the GitPush and PutFile actions. Include a restriction for the specific repositories in the policy statement with a condition that references the main branch.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRemove the IAM policy, and add an AWSCodeCommitReadOnly managed policy. Add an Allow rule for the GitPush and PutFile actions for the specific repositories in the policy statement with a condition that references the main branch.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the IAM policy. Include a Deny rule for the GitPush and PutFile actions for the specific repositories in the policy statement with a condition that references the main branch.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an additional policy to include an Allow rule for the GitPush and PutFile actions. Include a restriction for the specific repositories in the policy statement with a condition that references the feature branches."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 18,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-02-17T19:30:00.000Z",
        "voteCount": 6,
        "content": "By default, the AWSCodeCommitPowerUser managed policy allows users to push changes to any branch in any repository in the AWS account. To restrict the developers' ability to push changes to the main branch directly, an additional policy is needed that explicitly denies these actions for the main branch.\n\nThe Deny rule should be included in a policy statement that targets the specific repositories and includes a condition that references the main branch. The policy statement should look something like this: \n{\n    \"Effect\": \"Deny\",\n    \"Action\": [\n        \"codecommit:GitPush\",\n        \"codecommit:PutFile\"\n    ],\n    \"Resource\": \"arn:aws:codecommit:&lt;region&gt;:&lt;account-id&gt;:&lt;repository-name&gt;\",\n    \"Condition\": {\n        \"StringEqualsIfExists\": {\n            \"codecommit:References\": [\n                \"refs/heads/main\"\n            ]\n        }\n    }"
      },
      {
        "date": "2023-02-18T09:41:00.000Z",
        "voteCount": 5,
        "content": "The correct answer is A\n\nOption C is ruled out at the outset, as it is a Managed Policy (managed by AWS) that cannot be changed."
      },
      {
        "date": "2023-02-18T06:10:00.000Z",
        "voteCount": 3,
        "content": "https://docs.aws.amazon.com/codecommit/latest/userguide/how-to-conditional-branch.html\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_manage-edit.html"
      },
      {
        "date": "2023-02-13T18:35:00.000Z",
        "voteCount": 2,
        "content": "C is incorrect. You can't modify managed policy"
      },
      {
        "date": "2023-02-13T16:00:00.000Z",
        "voteCount": 2,
        "content": "Add an additional policy"
      },
      {
        "date": "2023-02-12T06:23:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/codecommit/latest/userguide/how-to-conditional-branch.html"
      },
      {
        "date": "2023-02-11T15:13:00.000Z",
        "voteCount": 2,
        "content": "answer is A"
      },
      {
        "date": "2023-02-10T16:41:00.000Z",
        "voteCount": 2,
        "content": "A for me. It describes the correct approach to restrict the developers' ability to push changes directly to the main branch in AWS CodeCommit. The company can create an additional policy that includes a Deny rule for the GitPush and PutFile actions for the specific repositories and with a condition that references the main branch. This will restrict the developers from pushing changes directly to the main branch."
      },
      {
        "date": "2023-01-28T05:31:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/codecommit/latest/userguide/how-to-conditional-branch.html - Other developers can still pull from the branch, make their own branches, and create pull requests, but they cannot push or merge changes to that branch. You can configure this access by creating a conditional policy that uses a context key for one or more branches in IAM.\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_manage-edit.html - AWS managed policies cannot be edited"
      },
      {
        "date": "2023-01-27T20:31:00.000Z",
        "voteCount": 2,
        "content": "A is the right answer. We need to add a new policy that denies action to push to the repository 's main branch. C is incorrect because you cannot modify a managed IAM policy."
      },
      {
        "date": "2023-01-18T17:25:00.000Z",
        "voteCount": 3,
        "content": "A for me. Managed policy can not be modified"
      },
      {
        "date": "2023-01-19T07:27:00.000Z",
        "voteCount": 2,
        "content": "You are right. A is answer."
      },
      {
        "date": "2023-01-13T14:34:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/codecommit/latest/userguide/how-to-conditional-branch.html"
      },
      {
        "date": "2023-01-13T14:32:00.000Z",
        "voteCount": 3,
        "content": "C for me"
      },
      {
        "date": "2023-01-22T14:33:00.000Z",
        "voteCount": 3,
        "content": "You cannot modify MANAGED aws policy. You can only add additional one that forbids. So \"A\" is correct."
      },
      {
        "date": "2023-01-13T14:34:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/codecommit/latest/userguide/how-to-conditional-branch.html"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 151,
    "url": "https://www.examtopics.com/discussions/amazon/view/95102-exam-aws-devops-engineer-professional-topic-1-question-151/",
    "body": "A company uses a single AWS account to test applications on Amazon EC2 instances. The company has turned on AWS Config in the AWS account and has activated the restricted-ssh AWS Config managed rule.<br><br>The company needs an automated monitoring solution that will provide a customized notification in real time if any security group in the account is not compliant with the restricted-ssh rule. The customized notification must contain the name and ID of the noncompliant security group.<br><br>A DevOps engineer creates an Amazon Simple Notification Service (Amazon SNS) topic in the account and subscribes the appropriate personnel to the topic.<br><br>What should the DevOps engineer do next to meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon EventBridge (Amazon CloudWatch Events) rule that matches an AWS Config evaluation result of NON_COMPLIANT for the restricted-ssh rule. Configure an input transformer for the EventBridge (CloudWatch Events) rule. Configure the EventBridge (CloudWatch Events) rule to publish a notification to the SNS topic.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure AWS Config to send all evaluation results for the restricted-ssh rule to the SNS topic. Configure a filter policy on the SNS topic to send only notifications that contain the text of NON_COMPLIANT in the notification to subscribers.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon EventBridge (Amazon CloudWatch Events) rule that matches an AWS Config evaluation result of NON_COMPLIANT for the restricted-ssh rule. Configure the EventBridge (CloudWatch Events) rule to invoke AWS Systems Manager Run Command on the SNS topic to customize a notification and to publish the notification to the SNS topic.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon EventBridge (Amazon CloudWatch Events) rule that matches all AWS Config evaluation results of NON_COMPLIANT. Configure an input transformer for the restricted-ssh rule. Configure the EventBridge (CloudWatch Events) rule to publish a notification to the SNS topic."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 10,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-01-13T14:47:00.000Z",
        "voteCount": 7,
        "content": "A for me.https://aws.amazon.com/ru/premiumsupport/knowledge-center/config-resource-non-compliant/"
      },
      {
        "date": "2023-02-12T16:14:00.000Z",
        "voteCount": 4,
        "content": "The correct answer is A. Create an Amazon EventBridge (Amazon CloudWatch Events) rule that matches an AWS Config evaluation result of NON_COMPLIANT for the restricted-ssh rule. Configure an input transformer for the EventBridge (CloudWatch Events) rule. Configure the EventBridge (CloudWatch Events) rule to publish a notification to the SNS topic.\n\nThis approach uses Amazon EventBridge (previously known as Amazon CloudWatch Events) to filter AWS Config evaluation results based on the restricted-ssh rule and its compliance status (NON_COMPLIANT). An input transformer can be used to customize the information contained in the notification, such as the name and ID of the noncompliant security group. The EventBridge (CloudWatch Events) rule can then be configured to publish a notification to the SNS topic, which will notify the appropriate personnel in real-time."
      },
      {
        "date": "2023-02-12T06:55:00.000Z",
        "voteCount": 3,
        "content": "The difference between A and D is:\nA: matches an AWS Config evaluation result of NON_COMPLIANT for the restricted-ssh rule. \nD: matches all AWS Config evaluation results of NON_COMPLIANT --&gt; Configure an input transformer for the restricted-ssh rule\n\nWe do need to match all results first, then configure a specific input transformer."
      },
      {
        "date": "2023-01-28T07:01:00.000Z",
        "voteCount": 1,
        "content": "A is correct as this is the most efficient way to deliver customized NON_COMPLIANT evaluation results to the subscriber. B is another roundabout way of doing the same but the way it describes is not enough. It will require more complex filtering logic on not just NON_COMPLIANT text but also other attributes of the message since all config change events occurring across all resources are sent to the SNS Topic and filtering only the ones for a specific config rule involves much more than just searching for NON_COMPLIANT text.   https://stackoverflow.com/questions/64146609/how-to-configure-aws-config-to-send-compliance-change-notification-to-sns-topic"
      },
      {
        "date": "2023-01-21T14:49:00.000Z",
        "voteCount": 1,
        "content": "Short description\nUse an EventBridge rule with a custom event pattern and an input transformer to match an AWS Config evaluation rule output as NON_COMPLIANT. Then, route the response to an Amazon Simple Notification Service (Amazon SNS) topic."
      },
      {
        "date": "2023-01-15T15:38:00.000Z",
        "voteCount": 1,
        "content": "I go with A"
      },
      {
        "date": "2023-01-15T12:31:00.000Z",
        "voteCount": 2,
        "content": "i agree"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 152,
    "url": "https://www.examtopics.com/discussions/amazon/view/95103-exam-aws-devops-engineer-professional-topic-1-question-152/",
    "body": "A company has an organization in AWS Organizations. The company has configured AWS Single Sign-On (AWS SSO) to centrally manage access to the AWS accounts in the organization. A DevOps engineer needs to ensure that all users sign in by using multi-factor authentication (MFA). Users must be allowed to manage their own MFA devices. Users also must be prompted for MFA every time they sign in.<br><br>What should the DevOps engineer do to meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn AWS SSO, configure always-on MFBlock user sign-in when a user does not yet have a registered MFA device.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn AWS SSO, configure always-on MFA. Require a user to register an MFA device at sign-in when the user does not yet have a registered MFA device.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn AWS SSO, configure context-aware MFA. Update the trust policy of all permission sets to include the aws:MultiFactorAuthPresent condition on the sts:AssumeRole action.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn AWS SSO, configure context-aware MFA. Block user sign-in when a user does not yet have a registered MFA device."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-01-13T14:50:00.000Z",
        "voteCount": 5,
        "content": "B for me. https://docs.aws.amazon.com/singlesignon/latest/userguide/mfa-enable-how-to.html"
      },
      {
        "date": "2023-02-12T09:10:00.000Z",
        "voteCount": 2,
        "content": "Between \"always-on\" and \"context-aware\", we need to use \"always-on\" because of \"must be prompted for MFA every time they sign in\".  C and D are eliminated \nhttps://docs.aws.amazon.com/singlesignon/latest/userguide/mfa-enable-how-to.html\nBetween A and B, B is better https://docs.aws.amazon.com/singlesignon/latest/userguide/how-to-configure-mfa-device-enforcement.html"
      },
      {
        "date": "2023-01-28T07:35:00.000Z",
        "voteCount": 1,
        "content": "B is the right answer"
      },
      {
        "date": "2023-01-25T03:31:00.000Z",
        "voteCount": 2,
        "content": "B: On the Configure multi-factor authentication page, under If a user does not yet have a registered MFA device choose one of the following choices:\nRequire them to register an MFA device at sign in"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 153,
    "url": "https://www.examtopics.com/discussions/amazon/view/95165-exam-aws-devops-engineer-professional-topic-1-question-153/",
    "body": "A DevOps engineer needs to apply a core set of security controls to an existing set of AWS accounts. The accounts are in an organization in AWS Organizations. Individual teams will administer individual accounts by using the AdministratorAccess AWS managed policy. For all accounts, AWS CloudTrail and AWS Config must be turned on in all available AWS Regions. Individual account administrators must not be able to edit or delete any of the baseline resources. However, individual account administrators must be able to edit or delete their own CloudTrail trails and AWS Config rules.<br><br>Which solution will meet these requirements in the MOST operationally efficient way?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS CloudFormation template that defines the standard account resources. Deploy the template to all accounts from the organization's management account by using CloudFormation StackSets. Set the stack policy to deny Update Delete actions.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable AWS Control Tower. Enroll the existing accounts in AWS Control Tower. Grant the individual account administrators access to CloudTrail and AWS Config.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDesignate an AWS Config management account. Create AWS Config recorders in all accounts by using AWS CloudFormation StackSets. Deploy AWS Config rules to the organization by using the AWS Config management account. Create a CloudTrail organization trail in the organization's management account. Deny modification or deletion of the AWS Config recorders by using an SCP.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS CloudFormation template that defines the standard account resources. Deploy the template to all accounts from the organization's management account by using CloudFormation StackSets. Create an SCP that prevents updates or deletions to CloudTrail resources or AWS Config resources unless the principal is an administrator of the organization's management account.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 9,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-11-29T00:41:00.000Z",
        "voteCount": 1,
        "content": "Between C and D. C missing prevent cloudtrail resource deletion. D is efficient and do not prevent individual account from modify trails and config rules but prevent deletion of their resources."
      },
      {
        "date": "2024-05-06T18:19:00.000Z",
        "voteCount": 2,
        "content": "but SCP not support principal =&gt; D not correct"
      },
      {
        "date": "2023-10-20T10:25:00.000Z",
        "voteCount": 1,
        "content": "B is the most _operationally_efficient."
      },
      {
        "date": "2023-07-29T14:35:00.000Z",
        "voteCount": 3,
        "content": "So much discussion. Let's break this down. D=Create CFN SSets. SCP prevent CT + Config unless admin in mgmt acct. Does not meet req of local admin being able to edit their own CT/Config settings.\nC=Config delegated = protected Orgz Config. CT via mgmt acct = protected CT config. Both CT &amp; Config protected. Deny Config recorders - fine.\nC is the only viable answer to meet all the reqs. Forget the most efficient way - answers only give you one way anyway. Question misdirect is option D."
      },
      {
        "date": "2023-07-29T14:35:00.000Z",
        "voteCount": 2,
        "content": "Voted D by accident - should be C - sry"
      },
      {
        "date": "2023-04-25T14:54:00.000Z",
        "voteCount": 1,
        "content": "most Operationally Efficient is D."
      },
      {
        "date": "2023-03-04T05:02:00.000Z",
        "voteCount": 1,
        "content": "SCP Prevents users from disabling AWS Config or changing its rules so it should be C"
      },
      {
        "date": "2023-01-28T07:59:00.000Z",
        "voteCount": 2,
        "content": "The answer is D. It's more efficient to use SCP than stack policies to deny permissions to update or delete resources that are provisioned using CloudFormation StackSets. A is also possible but create more overhead and is not a best practice."
      },
      {
        "date": "2023-01-21T14:55:00.000Z",
        "voteCount": 1,
        "content": "I chose D because only D mentioned the individual admin account"
      },
      {
        "date": "2023-01-19T15:38:00.000Z",
        "voteCount": 2,
        "content": "C for me"
      },
      {
        "date": "2023-01-21T15:19:00.000Z",
        "voteCount": 2,
        "content": "Stacksets enables aws config in all accounts\nOrganizational Trail add trail to all acounts and can not be deleted by accounts\nSCP prevents deleting aws config"
      },
      {
        "date": "2023-01-25T19:51:00.000Z",
        "voteCount": 2,
        "content": "SCP doesn't apply to management account"
      },
      {
        "date": "2023-02-23T23:53:00.000Z",
        "voteCount": 1,
        "content": "Create an SCP that prevents updates or deletions to CloudTrail resources or AWS Config resources unless the principal is an administrator of the organization's management account\nIs not correct because SCP doesn't apply to management account. You don't need to bring it in SCP"
      },
      {
        "date": "2023-01-19T14:37:00.000Z",
        "voteCount": 1,
        "content": "I think it is D for me"
      },
      {
        "date": "2023-01-19T14:35:00.000Z",
        "voteCount": 1,
        "content": "is it not A? Because you attach rule restriction of deny or delete"
      },
      {
        "date": "2023-01-19T07:31:00.000Z",
        "voteCount": 2,
        "content": "D for me"
      },
      {
        "date": "2023-01-16T10:39:00.000Z",
        "voteCount": 1,
        "content": "So is it B? For me B too"
      },
      {
        "date": "2023-01-15T01:36:00.000Z",
        "voteCount": 1,
        "content": "Also, C is missing the deny \"edit or delete their own CloudTrail trails\""
      },
      {
        "date": "2023-01-14T01:57:00.000Z",
        "voteCount": 1,
        "content": "B sounds wrong. I don't think Control Tower can \"Grant the individual account administrators access to CloudTrail and AWS Config\"\nC is too complicated and not operationally efficient way\nSo, between A &amp; D\nI pick D because it sounds like a complete solution and is operationally efficient."
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 154,
    "url": "https://www.examtopics.com/discussions/amazon/view/95751-exam-aws-devops-engineer-professional-topic-1-question-154/",
    "body": "An ecommerce company has chosen AWS to host its new platform. The company's DevOps team has started building an AWS Control Tower landing zone. The DevOps team has set the identity store within AWS Single Sign-On (AWS SSO) to external identity provider (IdP) and has configured SAML 2 0.<br><br>The DevOps team wants a robust permission model that applies the principle of least privilege. The model must allow the team to build and manage only the team's own resources.<br><br>Which combination of steps will meet these requirements? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate IAM policies that include the required permissions. Include the aws PrincipalTag condition key.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate permission sets. Attach an inline policy that includes the required permissions and uses the aws:PrincipalTag condition key to scope the permissions.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a group in the IdP. Place users in the group. Assign the group to accounts and the permission sets in AWS SSO.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a group in the IdP. Place users in the group. Assign the group to OUs and IAM policies.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable attributes for access control in AWS SSO. Apply tags to users. Map the tags as key-value pairs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable attributes for access control in AWS SSO. Map attributes from the IdP as key-value pairs.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "BCF",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BCF",
        "count": 3,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-04-25T14:59:00.000Z",
        "voteCount": 1,
        "content": "BCF is best."
      },
      {
        "date": "2023-01-28T08:19:00.000Z",
        "voteCount": 3,
        "content": "B, C and F seems correct. Using the principalTag in the Permission Set inline policy a logged in user belonging to a specific AD group in the IDP can be permitted access to perform operations on certain resources if their group matches the group used in the PrincipleTag. Basically you are narrowing the scope of privileges assigned via Permission policies conditionally based on whether the logged in user belongs to a specific AD Group in IDP. The mapping of the AD group to the request attributes can be done using SSO attributes where we can pass other attributes like the SAML token as well."
      },
      {
        "date": "2023-01-31T18:27:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/singlesignon/latest/userguide/abac.html"
      },
      {
        "date": "2023-01-19T16:56:00.000Z",
        "voteCount": 1,
        "content": "B C F for me"
      },
      {
        "date": "2023-01-17T15:39:00.000Z",
        "voteCount": 2,
        "content": "B C F for me"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 155,
    "url": "https://www.examtopics.com/discussions/amazon/view/95456-exam-aws-devops-engineer-professional-topic-1-question-155/",
    "body": "A DevOps engineer is using AWS CodePipeline and AWS CodeBuild to create a CI/CD pipeline for a serverless application that is based on the AWS Serverless Application Model (AWS SAM). The source, build and test steps have been completed. The DevOps engineer has also created two pipeline deployment stages that use AWS CloudFormation as the action provider. One stage uses the \"Create or replace a change set\" action mode. The other stage uses the \"Execute a change set\" action mode.<br><br>The DevOps engineer needs to pass some parameters to a CloudFormation stack during the deployment without changing the code and pipeline structure.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the \u2013parameter-overrides option in the sam deploy command when the CodeBuild stage is invoked.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd all parameters in AWS Systems Manager Parameter Store. Use dynamic references to specify template values in Parameter Store.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the deployment stage where the \"Create or replace a change set\" action mode resides, apply the JSON object in the ParameterOverrides property.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the deployment stage where the \"Execute a change set\" action mode resides, apply the JSON object in the ParameterOverrides property."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 6,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 6,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-11-25T04:20:00.000Z",
        "voteCount": 1,
        "content": "So is the answer A or C?"
      },
      {
        "date": "2023-10-20T10:33:00.000Z",
        "voteCount": 3,
        "content": "--parameter-overrides are only used with`sam deploy`. Thus, A is out of the question.\n\nB would change the pipeline structure.\n\nC: correct.\n\nD: incorrect since the parameters would be applied too late."
      },
      {
        "date": "2023-06-20T22:57:00.000Z",
        "voteCount": 1,
        "content": "You can use the ParameterOverrides property in both the Create or replace a change set and Execute a change set action modes. However, the Execute a change set action mode allows you to pass parameters to a CloudFormation stack that has already been created. This can be useful if you want to update the parameters of an existing CloudFormation stack without having to recreate the stack."
      },
      {
        "date": "2023-03-13T22:23:00.000Z",
        "voteCount": 2,
        "content": "A - wrong sam deploy in code build ?\nD - wrong exectue has already prepared parameters"
      },
      {
        "date": "2023-03-01T11:58:00.000Z",
        "voteCount": 2,
        "content": "\"To specify properties, you can use the CodePipeline console, or you can create a JSON object to use for the AWS CLI, CodePipeline API, or AWS CloudFormation templates.\"\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/continuous-delivery-codepipeline-action-reference.html#w2ab1c21c13b9"
      },
      {
        "date": "2023-03-01T11:59:00.000Z",
        "voteCount": 1,
        "content": "I meant C"
      },
      {
        "date": "2023-02-17T19:41:00.000Z",
        "voteCount": 3,
        "content": "sam deploy --stack-name mystack --template-file template.yaml --parameter-overrides Environment=dev Region=us-west-2"
      },
      {
        "date": "2023-01-30T15:38:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stackinstances-override.html"
      },
      {
        "date": "2023-01-28T08:36:00.000Z",
        "voteCount": 4,
        "content": "The answer is C. Cloudformation is used as a deployment provider and therefore Parameter override should happen in the Deploy action. Since this is a new CodePipeline, you will need to perform Parameter override in \"Create or replace change set\" and not in \"Execute Change Set\"."
      },
      {
        "date": "2023-01-25T05:33:00.000Z",
        "voteCount": 1,
        "content": "A: --parameter-overrides\nA string that contains AWS CloudFormation parameter overrides encoded as key-value pairs. Use the same format as the AWS Command Line Interface (AWS CLI). For example, ParameterKey=ParameterValue InstanceType=t1.micro."
      },
      {
        "date": "2023-01-19T17:22:00.000Z",
        "voteCount": 1,
        "content": "Create stage creates the changeset and execute changeset gets the change from the creating stage"
      },
      {
        "date": "2023-01-19T17:37:00.000Z",
        "voteCount": 2,
        "content": "Sorry I meant C"
      },
      {
        "date": "2023-01-15T08:47:00.000Z",
        "voteCount": 1,
        "content": "--parameter-overrides\tA string that contains AWS CloudFormation parameter overrides encoded as key-value pairs. Use the same format as the AWS Command Line Interface (AWS CLI)."
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 156,
    "url": "https://www.examtopics.com/discussions/amazon/view/95450-exam-aws-devops-engineer-professional-topic-1-question-156/",
    "body": "A development team uses AWS CodeCommit, AWS CodePipeline, and AWS CodeBuild to develop and deploy an application. Changes to the code are submitted by pull requests. The development team reviews and merges the pull requests, and then the pipeline builds and tests the application.<br><br>Over time, the number of pull requests has increased. The pipeline is frequently blocked because of failing tests. To prevent this blockage, the development team wants to run the unit and integration tests on each pull request before it is merged.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a CodeBuild project to run the unit and integration tests. Create a CodeCommit approval rule template. Configure the template to require the successful invocation of the CodeBuild project. Attach the approval rule to the project's CodeCommit repository.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon EventBridge (Amazon CloudWatch Events) rule to match pullRequestCreated events from CodeCommit. Create a CodeBuild project to run the unit and integration tests. Configure the CodeBuild project as a target of the EventBridge (CloudWatch Events) rule that includes a custom event payload with the CodeCommit repository and branch information from the event.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon EventBridge (Amazon CloudWatch Events) rule to match pullRequestCreated events from CodeCommit. Modify the existing CodePipeline pipeline to not run the deploy steps if the build is started from a pull request. Configure the EventBridge (CloudWatch Events) rule to run the pipeline with a custom payload that contains the CodeCommit repository and branch information from the event.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a CodeBuild project to run the unit and integration tests. Create a CodeCommit notification rule that matches when a pull request is created or updated. Configure the notification rule to invoke the CodeBuild project."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 6,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "\u0411",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-03-02T12:07:00.000Z",
        "voteCount": 2,
        "content": "CodeCommit generates events in CloudWatch, CloudWatch triggers the CodeBuild\n\nhttps://aws.amazon.com/es/blogs/devops/complete-ci-cd-with-aws-codecommit-aws-codebuild-aws-codedeploy-and-aws-codepipeline/"
      },
      {
        "date": "2023-02-12T10:15:00.000Z",
        "voteCount": 1,
        "content": "We can use CodeBuild directly. https://docs.aws.amazon.com/codebuild/latest/userguide/how-to-create-pipeline.html\nA is eliminated because failed tests will stop the pipeline. We do not need \"approval.\""
      },
      {
        "date": "2024-05-06T19:18:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/codecommit/latest/userguide/notification-rule-create.html notification rule not direct run code build"
      },
      {
        "date": "2023-01-28T08:38:00.000Z",
        "voteCount": 1,
        "content": "B is the right answer."
      },
      {
        "date": "2023-01-19T21:25:00.000Z",
        "voteCount": 1,
        "content": "B for me"
      },
      {
        "date": "2023-01-17T12:31:00.000Z",
        "voteCount": 2,
        "content": "B sure"
      },
      {
        "date": "2023-01-15T08:31:00.000Z",
        "voteCount": 1,
        "content": "B or \u0421"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 157,
    "url": "https://www.examtopics.com/discussions/amazon/view/95446-exam-aws-devops-engineer-professional-topic-1-question-157/",
    "body": "A DevOps engineer is using AWS CodeDeploy across a fleet of Amazon EC2 Instances in an EC2 Auto Scaling group. The associated CodeDeploy deployment group, which is integrated with EC2 Auto Scaling, is configured to perform in-place deployments with CodeDeplcyDefault.OneAtATime. During an ongoing new deployment, the engineer discovers that although the overall deployment finished successfully, two out of five instances have the previous application revision deployed. The other three instances have the newest application revision.<br><br>What is likely causing this issue?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe two affected instances failed to fetch the new deployment.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA failed AfterInstall lifecycle event hook caused the CodeDeploy agent to roll back to the previous version on the affected instances.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe CodeDeploy agent was not installed in two affected instances.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEC2 Auto Scaling launched two new instances while the new deployment had not yet finished, causing the previous version to be deployed on the affected instances.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 13,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-01-25T05:51:00.000Z",
        "voteCount": 6,
        "content": "D: Scale-out events during a deployment\nIf an Amazon EC2 Auto Scaling scale-out event occurs while a deployment is underway, the new instances will be updated with the application revision that was most recently deployed, not the application revision that is currently being deployed. If the deployment succeeds, the old instances and the newly scaled-out instances will be hosting different application revisions. To bring those instances up to date, CodeDeploy automatically starts a follow-on deployment (immediately after the first) to update any outdated instances. If you'd like to change this default behavior so that outdated EC2 instances are left at the older revision, see Automatic updates to outdated instances."
      },
      {
        "date": "2023-05-07T02:57:00.000Z",
        "voteCount": 1,
        "content": "D is more suitable answer."
      },
      {
        "date": "2023-02-26T00:49:00.000Z",
        "voteCount": 1,
        "content": "D for sure"
      },
      {
        "date": "2023-02-18T10:59:00.000Z",
        "voteCount": 1,
        "content": "I voted for C too"
      },
      {
        "date": "2023-02-12T12:22:00.000Z",
        "voteCount": 2,
        "content": "I picked D, and was introduced to read this link https://docs.aws.amazon.com/codedeploy/latest/userguide/integrations-aws-auto-scaling.html\nAfter reading, I changed my mind from D to C, because it says that \"CodeDeployDefault.OneAtTime\" will automatically start and follow-up deployment to bring the instances up to date. \nSince the problem can be solved automatically, C is the only choice."
      },
      {
        "date": "2023-01-28T08:40:00.000Z",
        "voteCount": 1,
        "content": "D is the right answer."
      },
      {
        "date": "2023-01-18T22:34:00.000Z",
        "voteCount": 1,
        "content": "D for sure. It is a very common SysOps exam question"
      },
      {
        "date": "2023-01-17T12:36:00.000Z",
        "voteCount": 2,
        "content": "D sure"
      },
      {
        "date": "2023-01-15T08:19:00.000Z",
        "voteCount": 2,
        "content": "https://www.examtopics.com/discussions/amazon/view/8098-exam-aws-devops-engineer-professional-topic-1-question-93/"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 158,
    "url": "https://www.examtopics.com/discussions/amazon/view/96103-exam-aws-devops-engineer-professional-topic-1-question-158/",
    "body": "A company uses AWS CodePipeline pipelines to automate releases of its application. A typical pipeline consists of three stages: build, test, and deployment. The company has been using a separate AWS CodeBuild project to run scripts for each stage. However, the company now wants to use AWS CodeDeploy to handle the deployment stage of the pipelines.<br><br>The company has packaged the application as an RPM package and must deploy the application to a fleet of Amazon EC2 instances. The EC2 instances are in an EC2 Auto Scaling group and are launched from a common AMI.<br><br>Which combination of steps should a DevOps engineer perform to meet these requirements? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new version of the common AMI with the CodeDeploy agent installed. Update the IAM role of the EC2 instances to allow access to CodeDeploy.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new version of the common AMI with the CodeDeploy agent installed. Create an AppSpec file that contains application deployment scripts and grants access to CodeDeploy.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an application in CodeDeploy. Configure an in-place deployment type. Specify the Auto Scaling group as the deployment target. Add a step to the CodePipeline pipeline to use EC2 Image Builder to create a new AMI. Configure CodeDeploy to deploy the newly created AMI.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an application in CodeDeploy. Configure an in-place deployment type. Specify the Auto Scaling group as the deployment target. Update the CodePipeline pipeline to use the CodeDeploy action to deploy the application.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an application in CodeDeploy. Configure an in-place deployment type. Specify the EC2 instances that are launched from the common AMI as the deployment target. Update the CodePipeline pipeline to use the CodeDeploy action to deploy the application."
    ],
    "answer": "AD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AD",
        "count": 8,
        "isMostVoted": true
      },
      {
        "answer": "AE",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-03-05T09:05:00.000Z",
        "voteCount": 2,
        "content": "AD is right one as ASG is involved here. See https://docs.aws.amazon.com/codedeploy/latest/userguide/integrations-aws-auto-scaling.html"
      },
      {
        "date": "2023-02-12T12:48:00.000Z",
        "voteCount": 2,
        "content": "Maybe E is better than D?"
      },
      {
        "date": "2024-05-07T06:51:00.000Z",
        "voteCount": 1,
        "content": "The company has packaged the application as an RPM package and must deploy the application to a fleet of Amazon EC2 instances. The EC2 instances are in an EC2 Auto Scaling group and are launched from a common AMI.\nwhy E ??????????"
      },
      {
        "date": "2023-01-28T09:07:00.000Z",
        "voteCount": 2,
        "content": "A and D are correct.  B is incorrect because we cannot create an IAM role within the script. Its too late, the code is already running on the EC2 instance by now and it requires the Instance role to grant necessary permission to EC2 to access the artifacts in S3 for deploying them on EC2 instances."
      },
      {
        "date": "2023-01-26T05:53:00.000Z",
        "voteCount": 1,
        "content": "A and D for me"
      },
      {
        "date": "2023-01-19T22:35:00.000Z",
        "voteCount": 3,
        "content": "A and D for me"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 159,
    "url": "https://www.examtopics.com/discussions/amazon/view/95441-exam-aws-devops-engineer-professional-topic-1-question-159/",
    "body": "A company is using an organization in AWS Organizations to manage multiple AWS accounts. The company's development team wants to use AWS Lambda functions to meet resiliency requirements and is rewriting all applications to work with Lambda functions that are deployed in a VPC. The development team is using Amazon Elastic File System (Amazon EFS) as shared storage in Account A in the organization.<br><br>The company wants to continue to use Amazon EFS with Lambda. Company policy requires all serverless projects to be deployed in Account B.<br><br>A DevOps engineer needs to reconfigure an existing EFS file system to allow Lambda functions to access the data through an existing EFS access point.<br><br>Which combination of steps should the DevOps engineer take to meet these requirements? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the EFS file system policy to provide Account B with access to mount and write to the EFS file system in Account A.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate SCPs to set permission guardrails with fine-grained control for Amazon EFS.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new EFS file system in Account B. Use AWS Database Migration Service (AWS DMS) to keep data from Account A and Account B synchronized.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the Lambda execution roles with permission to access the VPC and the EFS file system. E. Create a VPC peering connection to connect Account A to Account B.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the Lambda functions in Account B to assume an existing IAM role in Account A.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "ADF",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ADF",
        "count": 6,
        "isMostVoted": true
      },
      {
        "answer": "BF",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "AD",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-01-28T09:57:00.000Z",
        "voteCount": 8,
        "content": "AEF are the right answers. \n1. Need to update the file system policy on EFS to allow mounting the file system into  Account B. \n## File System Policy\n$ cat file-system-policy.json\n{\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"elasticfilesystem:ClientMount\",\n                \"elasticfilesystem:ClientWrite\"\n            ],\n            \"Principal\": {\n                \"AWS\": \"arn:aws:iam::&lt;aws-account-id-A&gt;:root\" # Replace with AWS account ID of EKS cluster\n            }\n        }                                                                                                 \n    ]\n}\n2. Need VPC peering between Account A and Account B as the pre-requisite\n3. Need to assume cross-account IAM role to describe the mounts so that a specific mount can be chosen."
      },
      {
        "date": "2024-05-06T19:52:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/lambda/latest/dg/lambda-intro-execution-role.html\nwhy you need assume role ???"
      },
      {
        "date": "2024-07-10T12:15:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/lambda/latest/dg/configuration-filesystem-cross-account.html#:~:text=For%20your%20Lambda%20function%20to,Elastic%20File%20System%20User%20Guide.\n\nA \nD \nE"
      },
      {
        "date": "2024-03-28T20:41:00.000Z",
        "voteCount": 1,
        "content": "Why is there need for peering? We have one VPC, no mention of any addition, so is out, it's ADF"
      },
      {
        "date": "2023-10-23T03:54:00.000Z",
        "voteCount": 2,
        "content": "ADE. Peering is needed. F is unnecessary."
      },
      {
        "date": "2023-07-28T07:37:00.000Z",
        "voteCount": 2,
        "content": "At the time of writing this comment, there is no option E. Only A,B,C,D,F - E is missing. Sweet."
      },
      {
        "date": "2023-04-25T15:30:00.000Z",
        "voteCount": 1,
        "content": "AEF is best"
      },
      {
        "date": "2023-03-05T09:40:00.000Z",
        "voteCount": 2,
        "content": "AEF for me based on explanation here - https://aws.amazon.com/ru/blogs/storage/mount-amazon-efs-file-systems-cross-account-from-amazon-eks/"
      },
      {
        "date": "2023-02-13T00:49:00.000Z",
        "voteCount": 2,
        "content": "E is lacking in the \"Chosen Answer\"\nE and F are obviously correct. \nI prefer B to A because of the least privilege principle."
      },
      {
        "date": "2023-02-03T11:49:00.000Z",
        "voteCount": 1,
        "content": "A E F\nShould be E instead of D: \nA Lambda function in one account can mount a file system in a different account. For this scenario, you configure VPC peering between the function VPC and the file system VPC.\nhttps://docs.aws.amazon.com/lambda/latest/dg/services-efs.html"
      },
      {
        "date": "2023-01-30T16:28:00.000Z",
        "voteCount": 1,
        "content": "Vote for AEF\nhttps://aws.amazon.com/premiumsupport/knowledge-center/access-efs-across-accounts/"
      },
      {
        "date": "2023-01-28T09:57:00.000Z",
        "voteCount": 2,
        "content": "https://aws.amazon.com/blogs/storage/mount-amazon-efs-file-systems-cross-account-from-amazon-eks/"
      },
      {
        "date": "2023-01-19T22:47:00.000Z",
        "voteCount": 2,
        "content": "A D F for me"
      },
      {
        "date": "2023-01-19T23:01:00.000Z",
        "voteCount": 3,
        "content": "ADE\nVPC peering is needed"
      },
      {
        "date": "2023-01-17T12:39:00.000Z",
        "voteCount": 2,
        "content": "A D F  for me"
      },
      {
        "date": "2023-01-17T12:41:00.000Z",
        "voteCount": 2,
        "content": "https://aws.amazon.com/ru/blogs/storage/mount-amazon-efs-file-systems-cross-account-from-amazon-eks/"
      },
      {
        "date": "2023-03-05T09:38:00.000Z",
        "voteCount": 1,
        "content": "This link mentioned VPC peering requirement as well as need to assume role. AEF?"
      },
      {
        "date": "2023-01-15T08:05:00.000Z",
        "voteCount": 2,
        "content": "i think ADF"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 160,
    "url": "https://www.examtopics.com/discussions/amazon/view/95439-exam-aws-devops-engineer-professional-topic-1-question-160/",
    "body": "A company has an organization in AWS Organizations. The organization includes workload accounts that contain enterprise applications. The company centrally manages users from an operations account. No users can be created in the workload accounts. The company recently added an operations team and must provide the operations team members with administrator access to each workload account.<br><br>Which combination of actions will provide this access? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a SysAdmin role in the operations account. Attach the AdministratorAccess policy to the role. Modify the trust relationship to allow the sts:AssumeRole action from the workload accounts.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a SysAdmin role in each workload account. Attach the AdministratorAccess policy to the role. Modify the trust relationship to allow the sts:AssumeRole action from the operations account.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon Cognito identity pool in the operations account. Attach the SysAdmin role as an authenticated role.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the operations account, create an IAM user for each operations team member.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the operations account, create an IAM user group that is named SysAdmins. Add an IAM policy that allows the sts:AssumeRole action for the SysAdmin role in each workload account. Add all operations team members to the group.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon Cognito user pool in the operations account. Create an Amazon Cognito user for each operations team member."
    ],
    "answer": "BDE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BDE",
        "count": 10,
        "isMostVoted": true
      },
      {
        "answer": "BCE",
        "count": 6,
        "isMostVoted": false
      },
      {
        "answer": "BEF",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-02-03T12:27:00.000Z",
        "voteCount": 9,
        "content": "B D E\nA - wrong -&gt; Modify the trust relationship to allow the sts:AssumeRole action from --&gt;the workload accounts&lt;-- \nB - correct -&gt; You need to have roles with proper permissions in each workload account that can be used from the operations account\nC - wrong -&gt; why do you want to use Cognito - it is not mentioned as used or required\nD - correct -&gt; each member must have an account\nE - correct -&gt; each member should be part of the group that can assume the workload role in each workload account\nF - wrong -&gt; why do you want to use Cognito - it is not mentioned as used or required"
      },
      {
        "date": "2023-01-21T15:18:00.000Z",
        "voteCount": 5,
        "content": "I chose BCE. \n1. Create an assume role for operation team to access workload team, \n2. then add operation team members to a group, attach this assume role"
      },
      {
        "date": "2023-04-25T15:34:00.000Z",
        "voteCount": 1,
        "content": "BDE for me."
      },
      {
        "date": "2023-03-05T09:29:00.000Z",
        "voteCount": 1,
        "content": "BDE looks good. Good summary by DerekKey"
      },
      {
        "date": "2023-02-24T03:15:00.000Z",
        "voteCount": 1,
        "content": "B. Create a SysAdmin role in each workload account, attach the AdministratorAccess policy to the role, and modify the trust relationship to allow the sts:AssumeRole action from the operations account. This will enable the SysAdmin role in the workload accounts to be assumed by the operations team members in the operations account.\n\nD. In the operations account, create an IAM user for each operations team member.\n\nE. In the operations account, create an IAM user group that is named SysAdmins. Add an IAM policy that allows the sts:AssumeRole action for the SysAdmin role in each workload account. Add all operations team members to the group. This will grant the operations team members the necessary permissions to assume the SysAdmin role in each workload account.\n\nOption A, C, and F are not necessary because creating an Amazon Cognito identity pool or Amazon Cognito user pool, or modifying the trust relationship of the SysAdmin role to allow the sts:AssumeRole action from the workload accounts, is not required to provide the necessary access to the operations team members."
      },
      {
        "date": "2023-02-13T01:16:00.000Z",
        "voteCount": 1,
        "content": "The suggested answer is \"BCF\".  I think it's a type error there. C and F are in a pair, only one of them should be picked.\nAbviously, B is the correct one from the AB pair. F is the one from CF pair. \nFrom DE pair, E is the correct one, because we need to use sts:AssumeRole."
      },
      {
        "date": "2023-01-28T10:08:00.000Z",
        "voteCount": 1,
        "content": "I think the answer is B, E, F. \nB and E are correct for sure. Now between C and E, I am not sure what C really means. Does it mean create IAM user for each operations team member and add them to the IAM Group? I dont' think so because the question reads \"all users are centrally managed in the operations account\". It indicates that AWS SSO is configured and Cognito User Pool is setup as the Identity Provider. Once the user logs in using AWS SSO and authenticates against the Cognito user pool, they can be mapped to the IAM group that has required permissions to assume an SysAsdmin role containing the AdmnistratorAccess permissions in the workload accounts."
      },
      {
        "date": "2023-01-28T10:09:00.000Z",
        "voteCount": 1,
        "content": "I meant between C and F*"
      },
      {
        "date": "2023-01-19T23:16:00.000Z",
        "voteCount": 4,
        "content": "BDE for me"
      },
      {
        "date": "2023-01-15T07:34:00.000Z",
        "voteCount": 4,
        "content": "BDE https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html"
      },
      {
        "date": "2023-01-17T23:57:00.000Z",
        "voteCount": 1,
        "content": "I am unsure of the answer yet, but I am 90% sure that it is not BDE because there are 3 pairs.\nAB, DE, CF. The answer should be one from each pair. So, DE together is wrong."
      },
      {
        "date": "2023-01-19T07:39:00.000Z",
        "voteCount": 1,
        "content": "Why bde is wrong. We used such approach before migrating to sso"
      },
      {
        "date": "2023-01-19T15:44:00.000Z",
        "voteCount": 1,
        "content": "As I said before, \"I am unsure.\" However, based on all previous AWS question patterns; for example, questions 9, 12, 13 &amp; etc. The answers are 3 pairs and pick one from each pair. You could be right and BDE is the answer but I have not seen this kind of AWS answer pattern before."
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 161,
    "url": "https://www.examtopics.com/discussions/amazon/view/95434-exam-aws-devops-engineer-professional-topic-1-question-161/",
    "body": "A company's DevOps engineer manages an organization in AWS Organizations. The organization includes many accounts. The company needs all AWS CloudFormation stacks in production accounts to have termination protection enabled. Non-production accounts do not need termination protection.<br><br>The company has designated a centralized account for AWS Config aggregation and has configured all accounts to support the use of CloudFormation and AWS Config. The company also has grouped all production accounts into an OU.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Config rule to detect stacks that do not have termination protection enabled. Add a remediation action to the rule to enable termination protection. Deploy the rule across the organization by using the PutOrganizationConfigRule API operation.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a CloudFormation template that deploys an AWS Config rule to detect stacks that do not have termination protection enabled. Add a remediation action to the rule to enable termination protection. Deploy the template to the OU of the production accounts by using CloudFormation StackSets.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an SCP that denies cloudformation:DeleteStack actions. Apply the SCP to the OU of the production accounts by using CloudFormation StackSets.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a CloudFormation stack policy that denies Update:Delete actions. Apply the policy to the OU of the production accounts by using CloudFormation StackSets."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 11,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 7,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-03-24T07:24:00.000Z",
        "voteCount": 5,
        "content": "A - is wrong if you put this across the organization it will also impact non-prod accounts\nC - might work but it denies on organization level and not enabling termination protection - also how can you apply the SCP by using cloudformation stackssets? you apply it via organizations\nD- how do you apply a cloudformation a new stack policy to other stacks using stacksset? you cant, you need to update current ones.\nB- the only valid answer as a custom config rule can be created and deployed using stacksset to an OU."
      },
      {
        "date": "2023-03-14T19:18:00.000Z",
        "voteCount": 1,
        "content": "Create a custom config rule to detect the stacks that don't enable the termination protection. In remediation actions to make the changes (enable it)."
      },
      {
        "date": "2023-02-21T22:35:00.000Z",
        "voteCount": 1,
        "content": "C is the answer"
      },
      {
        "date": "2023-02-21T22:32:00.000Z",
        "voteCount": 1,
        "content": "C is the right answer. B is incorrect because there for no Config rule StackSets that can detect Termination Protection."
      },
      {
        "date": "2023-02-19T14:22:00.000Z",
        "voteCount": 2,
        "content": "A is wrong as this will detect stacks from the whole organization and apply remediation actions to all accounts, not just production accounts\nB is correct\nC is wrong as we want to enable terminate protection, we don't want to prevent the deletion of the stacks for good.\nD does not make sense"
      },
      {
        "date": "2023-02-19T14:25:00.000Z",
        "voteCount": 1,
        "content": "To the people saying B is incorrect as there is no rule for termination protection, I guess it just means it must be a lambda rule and not aws managed."
      },
      {
        "date": "2023-02-19T06:56:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-protect-stacks.html"
      },
      {
        "date": "2023-02-03T13:56:00.000Z",
        "voteCount": 3,
        "content": "C &amp; D - wrong - company wants to --&gt;enable&lt;-- termination protection\nB - wrong - there are only two AWS Config rules for cloud formation: cloudformation-stack-drift-detection-check and cloudformation-stack-notification-check\nA - my choice - you have to create a rule (with Guard) and then deploy it - PutOrganizationConfigRule can be done with aws cli - it allows you to exclude accounts that you don't want to target (--excluded-accounts)"
      },
      {
        "date": "2023-02-03T14:00:00.000Z",
        "voteCount": 1,
        "content": "There is a difference between SCP policy and termination protection. Termination protection will disregard the delete request even if you have permission to do it. You have to disable protection to be able to delete CF."
      },
      {
        "date": "2023-01-28T10:26:00.000Z",
        "voteCount": 3,
        "content": "C is the right answer. B is incorrect because there for no Config rule StackSets that can detect Termination Protection."
      },
      {
        "date": "2023-01-30T05:35:00.000Z",
        "voteCount": 3,
        "content": "A could have been a potential choice considering we can create custom Config rules using Lambda even if AWS config rule is not available for CF termiation protection. However it states calling the PutOrganizationConfigRule API across the entire organization but we don't want Termination protection in the Non-Prod accounts. So I will still go with C."
      },
      {
        "date": "2023-01-25T19:30:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-protect-stacks.html"
      },
      {
        "date": "2023-01-26T15:09:00.000Z",
        "voteCount": 1,
        "content": "aws config rule?"
      },
      {
        "date": "2023-01-20T00:26:00.000Z",
        "voteCount": 2,
        "content": "It seems there are no config rules for cloudformation terminate protection\nThen C is the best"
      },
      {
        "date": "2023-01-19T23:44:00.000Z",
        "voteCount": 1,
        "content": "I'll go with B"
      },
      {
        "date": "2023-01-20T00:26:00.000Z",
        "voteCount": 1,
        "content": "It seems there are no config rules for cloudformation terminate protection\nThen C is the best"
      },
      {
        "date": "2023-01-15T07:12:00.000Z",
        "voteCount": 2,
        "content": "i vote C"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 162,
    "url": "https://www.examtopics.com/discussions/amazon/view/95430-exam-aws-devops-engineer-professional-topic-1-question-162/",
    "body": "A company has deployed an application in a production VPC in a single AWS account. The application is popular and is experiencing heavy usage. The company's security team wants to add additional security, such as AWS WAF, to the application deployment. However, the application's product manager is concerned about cost and does not want to approve the change unless the security team can prove that additional security is necessary.<br><br>The security team believes that some of the application's demand might come from users that have IP addresses that are on a deny list. The security team provides the deny list to a DevOps engineer. If any of the IP addresses on the deny list access the application, the security team wants to receive automated notification in near real time so that the security team can document that the application needs additional security. The DevOps engineer creates a VPC flow log for the production VPC.<br><br>Which set of additional steps should the DevOps engineer take to meet these requirements MOST cost-effectively?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a log group in Amazon CloudWatch Logs. Configure the VPC flow log to capture accepted traffic and to send the data to the log group. Create an Amazon CloudWatch metric filter for IP addresses on the deny list. Create a CloudWatch alarm with the metric filter as input. Set the period to 5 minutes and the datapoints to alarm to 1. Use an Amazon Simple Notification Service (Amazon SNS) topic to send alarm notices to the security team.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon S3 bucket for log files. Configure the VPC flow log to capture all traffic and to send the data to the S3 bucket. Configure Amazon Athena to return all log files in the S3 bucket for IP addresses on the deny list. Configure Amazon QuickSight to accept data from Athena and to publish the data as a dashboard that the security team can access. Create a threshold alert of 1 for successful access. Configure the alert to automatically notify the security team as frequently as possible when the alert threshold is met.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon S3 bucket for log files. Configure the VPC flow log to capture accepted traffic and to send the data to the S3 bucket. Configure an Amazon OpenSearch Service duster and domain for the log files. Create an AWS Lambda function to retrieve the logs from the S3 bucket, format the logs, and load the logs into the OpenSearch Service cluster. Schedule the Lambda function to run every 5 minutes. Configure an alert and condition in OpenSearch Service to send alerts to the security team through an Amazon Simple Notification Service (Amazon SNS) topic when access from the IP addresses on the deny list is detected.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a log group in Amazon CloudWatch Logs. Create an Amazon S3 bucket to hold query results. Configure the VPC flow log to capture all traffic and to send the data to the log group. Deploy an Amazon Athena CloudWatch connector in AWS Lambda. Connect the connector to the log group. Configure Athena to periodically query for all accepted traffic from the IP addresses on the deny list and to store the results in the S3 bucket. Configure an S3 event notification to automatically notify the security team through an Amazon Simple Notification Service (Amazon SNS) topic when new objects are added to the S3 bucket."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 12,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-10-20T10:56:00.000Z",
        "voteCount": 1,
        "content": "A. All other alternatives are not cost-efficient, which is the most important factor here."
      },
      {
        "date": "2023-08-23T09:12:00.000Z",
        "voteCount": 1,
        "content": "As much as keeping costs low is a priority, near real time notifications is also important. \n\nB seems to get the balance. A, C and D talk about \"5 minutes\" or periodic checks (far from real time)"
      },
      {
        "date": "2023-03-24T07:52:00.000Z",
        "voteCount": 2,
        "content": "B  - wrong -  no need to capture all logs only incoming also no need to use quicksight\nC - wrong - no need to use openseach cluster - very expensive \nD - no need to capture all traffic + expensive why to use lambda to scan log groups if we can use it on a bucket?\nA - the only correct answer incoming traffic will be will be captured to a log group, metric filter will be set and an alarm will be triggered based on it + SNS."
      },
      {
        "date": "2023-03-05T09:55:00.000Z",
        "voteCount": 1,
        "content": "A - Simple, near real time (5 mins) and cheapest of all 4 options"
      },
      {
        "date": "2023-02-13T01:39:00.000Z",
        "voteCount": 1,
        "content": "A and C are eliminated, because \"5 minutes\" are not near real time.\nBetween B and D, B is cheaper, because D configures Anthena to periodically query."
      },
      {
        "date": "2023-01-28T10:32:00.000Z",
        "voteCount": 3,
        "content": "A is the right answer."
      },
      {
        "date": "2023-01-21T15:23:00.000Z",
        "voteCount": 3,
        "content": "most cost-effectively, so I choose A"
      },
      {
        "date": "2023-01-21T02:46:00.000Z",
        "voteCount": 2,
        "content": "A for me"
      },
      {
        "date": "2023-01-17T12:53:00.000Z",
        "voteCount": 2,
        "content": "A sure"
      },
      {
        "date": "2023-01-15T06:52:00.000Z",
        "voteCount": 2,
        "content": "i think A"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 163,
    "url": "https://www.examtopics.com/discussions/amazon/view/95426-exam-aws-devops-engineer-professional-topic-1-question-163/",
    "body": "A company has developed a serverless web application that is hosted on AWS. The application consists of Amazon S3. Amazon API Gateway, several AWS Lambda functions, and an Amazon RDS for MySQL database. The company is using AWS CodeCommit to store the source code. The source code is a combination of AWS Serverless Application Model (AWS SAM) templates and Python code.<br><br>A security audit and penetration test reveal that user names and passwords for authentication to the database are hardcoded within CodeCommit repositories. A DevOps engineer must implement a solution to automatically detect and prevent hardcoded secrets.<br><br>What is the MOST secure solution that meets these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Amazon CodeGuru Profiler. Decorate the handler function with @with_lambda_profiler(). Manually review the recommendation report. Write the secret to AWS Systems Manager Parameter Store as a secure string. Update the SAM templates and the Python code to pull the secret from Parameter Store.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAssociate the CodeCommit repository with Amazon CodeGuru Reviewer. Manually check the code review for any recommendations. Choose the option to protect the secret. Update the SAM templates and the Python code to pull the secret from AWS Secrets Manager.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Amazon CodeGuru Profiler. Decorate the handler function with @with lambda profiler(). Manually review the recommendation report. Choose the option to protect the secret. Update the SAM templates and the Python code to pull the secret from AWS Secrets Manager.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAssociate the CodeCommit repository with Amazon CodeGuru Reviewer. Manually check the code review for any recommendations. Write the secret to AWS Systems Manager Parameter Store as a string. Update the SAM templates and the Python code to pull the secret from Parameter Store."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-05-07T03:52:00.000Z",
        "voteCount": 1,
        "content": "B is more suitable"
      },
      {
        "date": "2023-02-13T01:49:00.000Z",
        "voteCount": 1,
        "content": "B, obviously, https://docs.aws.amazon.com/codecommit/latest/userguide/how-to-amazon-codeguru-reviewer.html\nNo idea why C is suggested as \"correct answer\"."
      },
      {
        "date": "2023-01-28T10:42:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2023-01-21T15:28:00.000Z",
        "voteCount": 1,
        "content": "B for me"
      },
      {
        "date": "2023-01-15T06:38:00.000Z",
        "voteCount": 2,
        "content": "B- https://aws.amazon.com/ru/blogs/aws/codeguru-reviewer-secrets-detector-identify-hardcoded-secrets/"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 164,
    "url": "https://www.examtopics.com/discussions/amazon/view/95748-exam-aws-devops-engineer-professional-topic-1-question-164/",
    "body": "A company has an application that monitors user activity on the company's website and mobile apps. The application uses Amazon ElastiCache for Redis as a write-through cache and uses an Amazon RDS for PostgreSQL database for longer storage. When the application receives a request to record a user's action, the application writes to the Redis cluster and the database at the same time. Internal recommendation applications consume the data to produce content recommendations for each user.<br><br>During peak periods, the recommendation applications cannot generate recommendations for users because of stale and missing data. The Redis cache is configured with cluster mode turned off, and the database is configured with a single read replica.<br><br>The company wants to ensure that the recommendation applications can generate content recommendations during peak periods. A DevOps engineer already has created a new ElastiCache for Redis cluster with cluster mode enabled.<br><br>What should the DevOps engineer do next to meet the company's requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a target tracking auto scaling policy for the Redis cluster's ElastiCachePrimaryEngineCPUUtilization metric. Configure the auto scaling policy to increase and decrease shards to the Redis cluster. Update the recommendation applications to use the clusters configuration endpoint to access Redis.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a target tracking auto scaling policy for the Redis cluster's ElastiCachePrimaryEngineCPUUtilization metric. Configure the auto scaling policy to increase and decrease shards to the Redis cluster. Update the recommendation applications to use the cluster's read replica endpoint to access Redis.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a scheduled auto scaling policy for the Redis cluster's ElastiCachePrimaryEngineCPUUtilization metric. Configure the auto scaling policy to add read replicas to the Redis cluster. Update the recommendation applications to use the clusters configuration endpoint to access Redis.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a scheduled auto scaling policy for the Redis cluster's ElastiCachePrimaryEngineCPUUtilization metric. Configure the auto scaling policy to add read replicas to the Redis cluster. Update the recommendation applications to use the database's read replica endpoint instead of Redis."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 13,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-01-17T15:08:00.000Z",
        "voteCount": 5,
        "content": "I think between A and B. Choose B because no need to write.https://aws.amazon.com/ru/about-aws/whats-new/2019/06/amazon-elasticache-launches-reader-endpoint-for-redis/"
      },
      {
        "date": "2024-07-10T12:38:00.000Z",
        "voteCount": 1,
        "content": "A - ElastiCache cluster with configuration endpoint ensures writes go to the primary and reads  go to the replica shards. \n\nThe cache needs to be written to as well, and read too"
      },
      {
        "date": "2023-10-20T11:02:00.000Z",
        "voteCount": 1,
        "content": "A. \n\nRedis clusters do not support read replicas, which are mentioned in all other options."
      },
      {
        "date": "2023-03-05T13:32:00.000Z",
        "voteCount": 1,
        "content": "what is this crap? robot answers are different from the user poll. this sucks. what is the correct answer damn it!!"
      },
      {
        "date": "2023-03-05T10:14:00.000Z",
        "voteCount": 2,
        "content": "Its A based on this - https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/Endpoints.html\nRedis (cluster mode enabled) clusters, use the cluster's Configuration Endpoint for all operations that support cluster mode enabled commands."
      },
      {
        "date": "2023-02-19T13:13:00.000Z",
        "voteCount": 1,
        "content": "No idea why D is suggested by this web. \nAre the peak periods come regularly every day ?"
      },
      {
        "date": "2023-02-18T11:21:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/Endpoints.html"
      },
      {
        "date": "2023-02-13T02:14:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/Endpoints.html"
      },
      {
        "date": "2023-02-04T03:47:00.000Z",
        "voteCount": 3,
        "content": "A &amp; B - wrong - cluster mode is turned-off - it means that there is only one shard (one primary and up to 5 secondary nodes), it can not be changed - you can not increase/decrease shards\nC - wrong - configuration endpoint is for cluster enabled Redis\nD - correct - the recommendation app should use read replica endpoint"
      },
      {
        "date": "2023-02-06T07:52:00.000Z",
        "voteCount": 1,
        "content": "regarding cluster mode, ready attentively:\nA DevOps engineer already has created a new ElastiCache for Redis cluster with cluster mode enabled."
      },
      {
        "date": "2023-01-29T05:07:00.000Z",
        "voteCount": 3,
        "content": "I think the answer is B from link: https://aws.amazon.com/ru/about-aws/whats-new/2019/06/amazon-elasticache-launches-reader-endpoint-for-redis/. \"You can now use a single reader endpoint to connect to your Redis read replicas\""
      },
      {
        "date": "2023-01-30T05:28:00.000Z",
        "voteCount": 4,
        "content": "In the same link:\n\"Reader endpoints work with ElastiCache for Redis clusters with cluster-mode disabled.\" But it the question it was clearly mentioned that new Redis cluster was created with cluster mode ENABLED.\nAlso please read another URL:\nhttps://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/Endpoints.html\n\nRedis (cluster mode disabled) clusters, use the Primary Endpoint for all write operations. Use the Reader Endpoint to evenly split incoming connections to the endpoint between all read replicas. \n\nRedis (cluster mode enabled) clusters, use the cluster's Configuration Endpoint for all operations that support cluster mode enabled commands. \n\nThe answer should be \"A\""
      },
      {
        "date": "2023-12-01T10:32:00.000Z",
        "voteCount": 1,
        "content": "Cluster mode is turned off thus disabled, as a result the Read Replica Endpoint is correct.\n\nStandalone Node \no\tOne endpoint for read and write operations.\nCluster Mode Disabled Cluster \no\tPrimary Endpoint \u2013 for all write operations. \no\tReader Endpoint \u2013 Split read operations across all read replicas evenly.\no\tNode Endpoint \u2013 for read operations.\nCluster Mode Enabled Cluster \no\tConfiguration Endpoint \u2013for all read/write operations that support Cluster Mode Enabled commands. (way to connect to cluster)\no\tNode Endpoint \u2013for read operations."
      },
      {
        "date": "2023-01-28T11:12:00.000Z",
        "voteCount": 3,
        "content": "A is the right answer. use Cluster configuration endpoint and when target tracking scaling policy is used with a predefined metric of ElastiCachePrimaryEngineCPUUtilization it scale out the shards and not the replicas."
      },
      {
        "date": "2023-01-22T01:21:00.000Z",
        "voteCount": 3,
        "content": "A for me\ncluster mode is enabled.\nhttps://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/Endpoints.html\nanswere B is good for cluster mode disabled"
      },
      {
        "date": "2023-01-29T14:25:00.000Z",
        "voteCount": 1,
        "content": "By the way cluster mode disabled only uses one shard"
      },
      {
        "date": "2023-01-21T15:31:00.000Z",
        "voteCount": 4,
        "content": "read replicas endpoint for traffic"
      },
      {
        "date": "2023-01-19T01:21:00.000Z",
        "voteCount": 2,
        "content": "I agree the answer is either A or B but I don't see \"cluster's read replica\" in the question. So, I select A"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 165,
    "url": "https://www.examtopics.com/discussions/amazon/view/95414-exam-aws-devops-engineer-professional-topic-1-question-165/",
    "body": "A company stores purchase history in an Amazon DynamoDB table. The company needs other workloads that run on AWS to react to data changes in the table.<br><br>The company has enabled a DynamoDB stream on the table. Three existing AWS Lambda functions have an event source mapping configured for the DynamoDB stream. The company's application developers plan to add other applications that will need to react to changes in the table. A DevOps engineer must design an architecture that will give the additional consumers this functionality.<br><br>Which solution will meet these requirements in the MOST operationally efficient way?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon EventBridge event bus. Create a new Lambda function that uses the existing DynamoDB stream as an event source. Configure the new Lambda function to post those events to the event bus. Update the original Lambda functions to react to events in the event bus. As other applications need the events, configure the applications to use the event bus as an event source.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon Simple Queue Service (Amazon SOS) queue. Create a new Lambda function that uses the existing DynamoDB stream as an event source. Configure the new Lambda function to post those events to the SOS queue. Update the original Lambda functions to react to entries in the SOS queue. As other applications need the events, configure the applications to use the SOS queue as an event source.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon Kinesis data stream. Create a new Lambda function that uses the existing DynamoDB stream as an event source. Configure the new Lambda function to post those events to the Kinesis data stream. Update the original Lambda functions to subscribe to records in the Kinesis data stream. As other applications need the events, configure the applications to use the Kinesis data stream as an event source.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the DynamoDB table to use on-demand capacity mode. Increase the memory of the Lambda functions. Configure the Lambda functions to use provisioned concurrency."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 11,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-01-15T06:12:00.000Z",
        "voteCount": 6,
        "content": "agree C\nhttps://aws.amazon.com/ru/blogs/database/dynamodb-streams-use-cases-and-design-patterns/"
      },
      {
        "date": "2024-07-10T12:49:00.000Z",
        "voteCount": 1,
        "content": "A - if order of delivery is not important. EventBus can archive events as well for replay\nC - Incorrect, You might as well enable Kinesis Streams On the Table instead. Sounds like an unnecessary engineering effort and twice the cost to have two streams - DDB Change Stream + Kinesis Data Stream"
      },
      {
        "date": "2023-10-20T11:04:00.000Z",
        "voteCount": 1,
        "content": "A.\n\nKinesis is not an option here."
      },
      {
        "date": "2023-08-18T13:03:00.000Z",
        "voteCount": 1,
        "content": "https://aws.amazon.com/ru/blogs/database/dynamodb-streams-use-cases-and-design-patterns/"
      },
      {
        "date": "2023-02-04T04:02:00.000Z",
        "voteCount": 1,
        "content": "B - wrong - SQS has one tiem message delivery, once read by a customer it will not be available anymore"
      },
      {
        "date": "2023-02-16T03:47:00.000Z",
        "voteCount": 1,
        "content": "Wrong. The messages are still available after reading them (they do have a visibility timeout, though). But in order to disappear from the queue, they need to be deleted.\n\nAltogether, SQS is not the answer to this question."
      },
      {
        "date": "2023-01-28T11:38:00.000Z",
        "voteCount": 1,
        "content": "C is a better option than A because it's more operationality efficient. Option B is wrong because EventBrige Bus doesn't have a persistent store whereas KDS does. So there won't be a loss of data."
      },
      {
        "date": "2023-01-22T01:57:00.000Z",
        "voteCount": 1,
        "content": "You don't need lambda to send data from streams to kinesis stream. It can be done automatically\nI go with B"
      },
      {
        "date": "2023-01-22T02:09:00.000Z",
        "voteCount": 1,
        "content": "Only drawback with B is if an aplication process the event data then it will be deleted from the Queue"
      },
      {
        "date": "2023-02-24T22:03:00.000Z",
        "voteCount": 2,
        "content": "because of this SQS is wrong in this scenario\nC is the correct answere"
      },
      {
        "date": "2023-01-17T13:07:00.000Z",
        "voteCount": 2,
        "content": "C for me"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 166,
    "url": "https://www.examtopics.com/discussions/amazon/view/95413-exam-aws-devops-engineer-professional-topic-1-question-166/",
    "body": "A media company has several thousand Amazon EC2 instances in an AWS account. The company is using Slack and a shared email inbox for team communications and important updates. A DevOps engineer needs to send all AWS-scheduled EC2 maintenance notifications to the Slack channel and the shared inbox. The solution must include the instances' Name and Owner tags.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIntegrate AWS Trusted Advisor with AWS Config. Configure a custom AWS Config rule to invoke an AWS Lambda function to publish notifications to an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe a Slack channel endpoint and the shared inbox to the topic.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon EventBridge to monitor for AWS Health events. Configure the maintenance events to target an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe an AWS Lambda function to the SNS topic to send notifications to the Slack channel and the shared inbox.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Lambda function that sends EC2 maintenance notifications to the Slack channel and the shared inbox. Monitor EC2 health events by using Amazon CloudWatch metrics. Configure a CloudWatch alarm that invokes the Lambda function when a maintenance notification is received.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure AWS Support integration with AWS CloudTrail. Create a CloudTrail lookup event to invoke an AWS Lambda function to pass EC2 maintenance notifications to Amazon Simple Notification Service (Amazon SNS). Configure Amazon SNS to target the Slack channel and the shared inbox."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-01-15T05:48:00.000Z",
        "voteCount": 6,
        "content": "B\nhttps://docs.aws.amazon.com/health/latest/ug/cloudwatch-events-health.html"
      },
      {
        "date": "2023-02-13T02:37:00.000Z",
        "voteCount": 1,
        "content": "I think everyone is agree with B."
      },
      {
        "date": "2023-01-22T15:12:00.000Z",
        "voteCount": 1,
        "content": "B for me"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 167,
    "url": "https://www.examtopics.com/discussions/amazon/view/95412-exam-aws-devops-engineer-professional-topic-1-question-167/",
    "body": "A company is using AWS Database Migration Service (AWS DMS) to replicate data from a source database in a data center to a target Amazon Aurora PostgreSQL database. The company has created a DMS replication task with change data capture (CDC).<br><br>The replication instance sometimes gets interrupted and affects critical functionality. The company must improve the replication instance's resiliency and receive notifications about interruptions.<br><br>Which solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCopy data from the source database to Amazon S3 by using AWS DataSync. Configure AWS Lambda functions to copy the data to the target database. Configure Amazon CloudWatch alarms to monitor the Lambda functions for errors and throttles. Use an Amazon Simple Notification Service (Amazon SNS) topic for email notification.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate Amazon CloudWatch alarms to monitor DMS replication task metrics and host metrics. Use an Amazon Simple Notification Service (Amazon SNS) topic for email notification and to invoke an AWS Lambda function to configure a standby DMS replication instance in a different AWS Region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate Amazon CloudWatch alarms to monitor DMS replication task metrics and host metrics. Use an Amazon Simple Notification Service (Amazon SNS) topic for email notification. After receiving the notification, configure a new DMS replication task in the same AWS Region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the DMS replication instance by tuming on Multi-AZ support. Create Amazon CloudWatch alarms to monitor DMS replication task metrics and host metrics. Use an Amazon Simple Notification Service (Amazon SNS) topic for email notification.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 8,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-03-05T10:27:00.000Z",
        "voteCount": 1,
        "content": "You can modify dms instance to enable multi az - https://awscli.amazonaws.com/v2/documentation/api/latest/reference/dms/modify-replication-instance.html"
      },
      {
        "date": "2023-02-13T02:49:00.000Z",
        "voteCount": 2,
        "content": "\"With Availability Zones, you can design and operate applications and databases that automatically fail over between zones without interruption. Availability Zones are more highly available, fault tolerant, and scalable than traditional single or multiple data center infrastructures.\"\nhttps://docs.aws.amazon.com/mgn/latest/ug/disaster-recovery-resiliency.html"
      },
      {
        "date": "2023-01-28T11:41:00.000Z",
        "voteCount": 3,
        "content": "D is correct. The keyword is \"to make the DMS replication instance resilient\". This should ring Multi-AZ."
      },
      {
        "date": "2023-01-25T01:26:00.000Z",
        "voteCount": 2,
        "content": "C - doesn't make sense - you don't have to create new task you can restart exisrting task if you have storage\nD - If you use AWS DMS for ongoing replication purposes, choosing a Multi-AZ instance can improve your availability should a storage issue occur. When using a single AZ or Multi-AZ replication instance during a FULL LOAD and a failover or host replacement occurs, the full load task is expected to fail. You can restart the task from the point of failure for the remaining tables that didn't complete, or are in an error state."
      },
      {
        "date": "2023-01-22T15:26:00.000Z",
        "voteCount": 3,
        "content": "I go with D. Question is asking about least operational overhead. By the way multi-AZ allows continuous data replication by enabling redundant replication instances\nhttps://aws.amazon.com/dms/features/"
      },
      {
        "date": "2023-01-15T05:44:00.000Z",
        "voteCount": 2,
        "content": "i vote c"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 168,
    "url": "https://www.examtopics.com/discussions/amazon/view/95375-exam-aws-devops-engineer-professional-topic-1-question-168/",
    "body": "A company has deployed an application on AWS Elastic Beanstalk by using an all-at-once deployment method. The deployment failed recently because of an application misconfiguration and resulted in significant downtime.<br><br>To prevent such downtime in the future, a DevOps engineer needs to revise the deployment method while maintaining the application performance. The DevOps engineer must ensure that application versions are consistently configured across all instances without creating new environments.<br><br>Which deployment solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSwitch to a rolling deployment strategy for future application updates.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSwitch to a rolling deployment with additional batch strategy for future application updates.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSwitch to an immutable deployment strategy for future application updates.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSwitch to a blue/green deployment strategy for future application updates."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-10-25T04:31:00.000Z",
        "voteCount": 2,
        "content": "C is correct. Beanstalk, when using the immutable deployment method, puts new instances in a new scaling group \u2013 which isn't an environment - and then transfers them to the original one when tests have passed."
      },
      {
        "date": "2023-07-26T10:08:00.000Z",
        "voteCount": 2,
        "content": "Deployment method revise , consider application versions are consistently configured across all instances without creating new environments so B is correct\nNot A, it is all-at-once method which may create same problem\nNot C &amp; D as without creating new environments"
      },
      {
        "date": "2023-02-20T06:48:00.000Z",
        "voteCount": 2,
        "content": "A: Rolling deployments occur whenever you deploy your application and can typically be performed without replacing instances in your environment. Elastic Beanstalk takes each batch out of service, deploys the new application version, and then places it back in service."
      },
      {
        "date": "2023-02-20T06:48:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rollingupdates.html"
      },
      {
        "date": "2023-02-18T12:35:00.000Z",
        "voteCount": 2,
        "content": "\"The DevOps engineer must ensure that application versions are consistently configured across all instances without creating new environments.\"\nI think this means that ....."
      },
      {
        "date": "2023-02-18T12:36:00.000Z",
        "voteCount": 2,
        "content": "Only blue/green can ensure that the working instances are using a same application version."
      },
      {
        "date": "2023-02-13T02:58:00.000Z",
        "voteCount": 3,
        "content": "Default behaviour of \"all-at-once\" will bring outdated instance to the newest version. \nNo idea why D is suggested as \"correct answer\". Blue/Green creates new environment."
      },
      {
        "date": "2023-01-30T21:08:00.000Z",
        "voteCount": 2,
        "content": "Immutable deployment is the best option to meet the requirements of maintaining the performance and consistent versions and you don't need to create new environments for the immutable deployment."
      },
      {
        "date": "2023-01-28T11:45:00.000Z",
        "voteCount": 1,
        "content": "C is correct."
      },
      {
        "date": "2023-01-25T01:08:00.000Z",
        "voteCount": 4,
        "content": "My choice\nImmutable - will add additionall auto scaling for new instances still keeping old instances serving requests - roll back is simle, new auto scaling gets decomissioned\nA &amp; B - requires manual redeploy = app performance &amp; downtime + will have a mix of old and new versions\nD - requires new environment"
      },
      {
        "date": "2023-01-23T07:05:00.000Z",
        "voteCount": 4,
        "content": "This is one of the most poorly worded question. Agreed, what is meant by environment in this. I would have to choose C here given the \"versions are consistently configured across all instances\" requirement."
      },
      {
        "date": "2023-01-22T15:54:00.000Z",
        "voteCount": 2,
        "content": "What are new environments? beanstalk environments or new instances?\nHow to maintain the application performance with rolling deployment?\nIf environment means instance, then A is correct otherwise C is the best answer to maintain performance"
      },
      {
        "date": "2023-01-17T13:12:00.000Z",
        "voteCount": 1,
        "content": "A for me"
      },
      {
        "date": "2023-01-17T03:58:00.000Z",
        "voteCount": 1,
        "content": "Only A do not create new environment."
      },
      {
        "date": "2023-01-15T05:26:00.000Z",
        "voteCount": 2,
        "content": "B-  without creating new environments"
      },
      {
        "date": "2023-01-16T01:38:00.000Z",
        "voteCount": 1,
        "content": "But B is \"with additional batch.\" It is creating new environments"
      },
      {
        "date": "2023-01-15T00:10:00.000Z",
        "voteCount": 2,
        "content": "A is the only option without creating new environments"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 169,
    "url": "https://www.examtopics.com/discussions/amazon/view/95050-exam-aws-devops-engineer-professional-topic-1-question-169/",
    "body": "A company requires that its internally facing web application be highly available. The architecture is made up of one Amazon EC2 web server instance and one NAT instance that provides outbound internet access for updates and accessing public data.<br><br>Which combination of architecture adjustments should the company implement to achieve high availability? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd the NAT instance to an EC2 Auto Scaling group that spans multiple Availability Zones. Update the route tables.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate additional EC2 instances spanning multiple Availability Zones. Add an Application Load Balancer to split the load between them.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an Application Load Balancer in front of the EC2 instance. Configure Amazon Cloud Watch alarms to recover the EC2 instance upon host failure.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReplace the NAT instance with a NAT gateway in each Availability Zone. Update the route tables.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReplace the NAT instance with a NAT gateway that spans multiple Availability Zones. Update the route tables."
    ],
    "answer": "BD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BD",
        "count": 11,
        "isMostVoted": true
      },
      {
        "answer": "AB",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-01-13T07:06:00.000Z",
        "voteCount": 6,
        "content": "BD\nhttps://www.examtopics.com/discussions/amazon/view/47652-exam-aws-devops-engineer-professional-topic-1-question-215/"
      },
      {
        "date": "2023-02-19T12:55:00.000Z",
        "voteCount": 1,
        "content": "I changed my mind to AB. Application Load Balancer + Nat Gateway, looks weird."
      },
      {
        "date": "2023-02-13T03:10:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html"
      },
      {
        "date": "2023-01-28T11:48:00.000Z",
        "voteCount": 2,
        "content": "B and D due to the need to select a combination of choices."
      },
      {
        "date": "2023-01-22T16:59:00.000Z",
        "voteCount": 1,
        "content": "B and D"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 170,
    "url": "https://www.examtopics.com/discussions/amazon/view/95410-exam-aws-devops-engineer-professional-topic-1-question-170/",
    "body": "A company is running an application on Amazon EC2 instances. A DevOps engineer needs to aggregate the application logs to a central system for the company's application team to search. A critical error message periodically appears in the log files. The DevOps engineer needs to notify the application team by email when these error messages occur.<br><br>Which solution will meet these requirements in the MOST operationally efficient manner?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the unified Amazon CloudWatch agent on the EC2 instances to publish the application logs files to a CloudWatch log group. Configure a metric filter on the CloudWatch log group to detect the critical errors and to create a custom metric. Create an Amazon Simple Notification Service (Amazon SNS) topic. Configure a CloudWatch alarm to use the custom metric to notify the SNS topic. Subscribe the application team's email address to the SNS topic.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall the Amazon Kinesis agent on the EC2 instances. Configure the Kinesis agent with the location of the log files. Stream the logs to a Kinesis Data Firehose delivery stream with an Amazon CloudWatch metrics stream as a destination. Configure an AWS Lambda function to detect the error message and to create a custom metric. Associate the Lambda function with the stream. Create an Amazon Simple Notification Service (Amazon SNS) topic. Configure a CloudWatch alarm to use the custom metric to notify the SNS topic. Subscribe the application team's email address to the SNS topic.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall the AWS X-Ray daemon on the EC2 instances. Instrument the application with the AWS Distro for OpenTelemetry (ADOT). Configure the ADOT collector with the location of the custom log files and the name of an Amazon CloudWatch log group. Use the CloudWatch embedded metric format to generate a custom metric that is based on the error message. Create an Amazon Simple Notification Service (Amazon SNS) topic. Configure a CloudWatch alarm to use the custom metric to notify the SNS topic. Subscribe the application team's email address to the SNS topic.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the unified Amazon CloudWatch agent on the EC2 instances to publish the application logs files to a CloudWatch log group. Create an Amazon OpenSearch Service domain. Subscribe the CloudWatch log group to the OpenSearch Service domain. Create an Amazon Simple Notification Service (Amazon SNS) topic. Configure an OpenSearch Service alert monitor to notify the SNS topic. Subscribe the application team's email address to the SNS topic."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 13,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-07-10T13:17:00.000Z",
        "voteCount": 1,
        "content": "Most operationally efficient - A\nCentralized/Search logs - CloudWatch Log Insights can do this for much cheaper price and without needing additional services"
      },
      {
        "date": "2023-03-04T14:02:00.000Z",
        "voteCount": 2,
        "content": "The Key here is, the engineer needs to aggregate the application logs to a central system for the company's application team to search.  Opensearch is the best option for the centralized log search"
      },
      {
        "date": "2023-02-24T22:29:00.000Z",
        "voteCount": 1,
        "content": "A seems to be the best but why saying create custom metric?  metric filter doesn't create custom metric"
      },
      {
        "date": "2023-02-13T03:17:00.000Z",
        "voteCount": 1,
        "content": "A is good. \nD is more operationally efficient \nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CWL_OpenSearch_Stream.html"
      },
      {
        "date": "2023-02-17T14:55:00.000Z",
        "voteCount": 2,
        "content": "\"MOST operationally efficient\",  I think A is more operationally efficient than D. \nOn A, we only need to operate CWL and SNS\nOn D, we have to operate CWL, SNS and OpenSearch"
      },
      {
        "date": "2023-03-02T15:04:00.000Z",
        "voteCount": 1,
        "content": "Both A and D are valid for me. For A you need to provide all the developers access to AWS for checking CloudWatch, and searching in CloudWatch is worst than in OpenSearch, maybe it is less efficient ..."
      },
      {
        "date": "2023-01-28T13:21:00.000Z",
        "voteCount": 4,
        "content": "A is correct."
      },
      {
        "date": "2023-01-21T15:45:00.000Z",
        "voteCount": 4,
        "content": "A for me"
      },
      {
        "date": "2023-01-17T14:28:00.000Z",
        "voteCount": 4,
        "content": "A for me"
      },
      {
        "date": "2023-01-15T05:12:00.000Z",
        "voteCount": 3,
        "content": "I vote A"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 171,
    "url": "https://www.examtopics.com/discussions/amazon/view/95409-exam-aws-devops-engineer-professional-topic-1-question-171/",
    "body": "A company has deployed a new Amazon API Gateway API that retrieves the cost of items for the company's online store. An AWS Lambda function supports the API and retrieves the data from an Amazon DynamoDB table. The API's latency increases during times of peak usage each day. However, the latency of the DynamoDB table reads is constant throughout the day.<br><br>A DevOps engineer configures DynamoDB Accelerator (DAX) for the DynamoDB table, and the API latency decreases throughout the day. The DevOps engineer then configures Lambda provisioned concurrency with a limit of two concurrent invocations. This change reduces the latency during normal usage. However, the company is still experiencing higher latency during times of peak usage than during times of normal usage.<br><br>Which set of additional steps should the DevOps engineer take to produce the LARGEST decrease in API latency?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the read capacity of the DynamoDB table. Use AWS Application Auto Scaling to manage provisioned concurrency for the Lambda function.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable caching in API Gateway. Stop using provisioned concurrency for the Lambda function.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDelete the DAX cluster for the DynamoDB table. Use AWS Application Auto Scaling to manage provisioned concurrency for the Lambda function.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable caching in API Gateway. Use AWS Application Auto Scaling to manage provisioned concurrency for the Lambda function\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-02-13T03:47:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/lambda/latest/dg/provisioned-concurrency.html"
      },
      {
        "date": "2023-01-28T13:49:00.000Z",
        "voteCount": 1,
        "content": "D is correct. Need to use both autoscaling of provisioned concurrency and API Gteway caching as just having provisioned concurrency of 2 and DAX did not help during peak load."
      },
      {
        "date": "2023-01-23T15:41:00.000Z",
        "voteCount": 1,
        "content": "D for me"
      },
      {
        "date": "2023-01-21T15:51:00.000Z",
        "voteCount": 1,
        "content": "D for me as well"
      },
      {
        "date": "2023-01-15T05:06:00.000Z",
        "voteCount": 1,
        "content": "\u0412 \nhttps://docs.aws.amazon.com/lambda/latest/dg/provisioned-concurrency.html"
      },
      {
        "date": "2023-01-15T05:07:00.000Z",
        "voteCount": 2,
        "content": "D ))) (Russian keyboard  \u0412=D)"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 172,
    "url": "https://www.examtopics.com/discussions/amazon/view/95408-exam-aws-devops-engineer-professional-topic-1-question-172/",
    "body": "A DevOps engineer has implemented a Cl/CD pipeline to deploy an AWS CloudFormation template that provisions a web application. The web application consists of an Application Load Balancer (ALB), a target group, a launch template that uses an Amazon Linux 2 AMI, an Auto Scaling group of Amazon EC2 instances, a security group, and an Amazon RDS for MySOL database. The launch template includes user data that specifies a script to install and start the application.<br><br>The initial deployment of the application was successful. The DevOps engineer made changes to update the version of the application with the user data. The CI/CD pipeline has deployed a new version of the template. However, the health checks on the ALB are now failing. The health checks have marked all targets as unhealthy.<br><br>During investigation, the DevOps engineer notices that the CloudFormation stack has a status of UPDATE_COMPLETE. However, when the DevOps engineer connects to one of the EC2 instances and checks /var/log/messages, the DevOps engineer notices that the Apache web server failed to start successfully because of a configuration error.<br><br>How can the DevOps engineer ensure that the CloudFormation deployment will fail if the user data fails to successfully finish running?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the cfn-signal helper script to signal success or failure to CloudFormation. Use the WaitOnResourceSignals update policy within the CloudFormation template. Set an appropriate timeout for the update policy.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon CloudWatch alarm for the UnhealthyHostCount metric. Include an appropriate alarm threshold for the target group. Create an Amazon Simple Notification Service (Amazon SNS) topic as the target to signal success or failure to CloudFormation.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a lifecycle hook on the Auto Scaling group by using the AWS::AutoScaling::LifecycleHook resource. Create an Amazon Simple Notification Service (Amazon SNS) topic as the target to signal success or failure to CloudFormation. Set an appropriate timeout on the lifecycle hook.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Amazon CloudWatch agent to stream the cloud-init logs. Create a subscription filter that includes an AWS Lambda function with an appropriate invocation timeout. Configure the Lambda function to use the SignalResource API operation to signal success or failure to CloudFormation."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 3,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-01-28T13:53:00.000Z",
        "voteCount": 1,
        "content": "A is correct."
      },
      {
        "date": "2023-01-23T15:56:00.000Z",
        "voteCount": 1,
        "content": "A for me"
      },
      {
        "date": "2023-01-23T13:06:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-signal.html"
      },
      {
        "date": "2023-01-15T04:56:00.000Z",
        "voteCount": 1,
        "content": "I agree A https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-updatepolicy.html"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 173,
    "url": "https://www.examtopics.com/discussions/amazon/view/95406-exam-aws-devops-engineer-professional-topic-1-question-173/",
    "body": "A company has a data ingestion application that runs across multiple AWS accounts. The accounts are in an organization in AWS Organizations. The company needs to monitor the application and consolidate access to the application. Currently, the company is running the application on Amazon EC2 instances from several Auto Scaling groups. The EC2 instances have no access to the internet because the data is sensitive. Engineers have deployed the necessary VPC endpoints. The EC2 instances run a custom AMI that is built specifically for the application.<br><br>To maintain and troubleshoot the application, system administrators need the ability to log in to the EC2 instances. This access must be automated and controlled centrally. The company's security team must receive a notification whenever the instances are accessed.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon EventBridge (Amazon CloudWatch Events) rule to send notifications to the security team whenever a user logs in to an EC2 instance. Use EC2 Instance Connect to log in to the instances. Deploy Auto Scaling groups by using AWS CloudFormation. Use the cfn-init helper script to deploy appropriate VPC routes for external access. Rebuild the custom AMI so that the custom AMI includes AWS Systems Manager Agent.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy a NAT gateway and a bastion host that has internet access. Create a security group that allows incoming traffic on all the EC2 instances from the bastion host. Install AWS Systems Manager Agent on all the EC2 instances. Use Auto Scaling group lifecycle hooks for monitoring and auditing access. Use Systems Manager Session Manager to log in to the instances. Send logs to a log group in Amazon CloudWatch Logs. Export data to Amazon 83 for auditing. Send notifications to the security team by using S3 event notifications.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse EC2 Image Builder to rebuild the custom AMI. Include the most recent version of AWS Systems Manager Agent in the image. Configure the Auto Scaling group to attach the AmazonSSMManagedlnstanceCore role to all the EC2 instances. Use Systems Manager Session Manager to log in to the instances. Enable logging of session details to Amazon S3. Create an S3 notification for new file uploads to send a message to the security team through an Amazon Simple Notification Service (Amazon SNS) topic.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Systems Manager Automation to build Systems Manager Agent into the custom AMI. Configure AWS Config to attach an SCP to the root organization account to allow the EC2 instances to connect to Systems Manager. Use Systems Manager Session Manager to log in to the instances. Enable logging of session details to Amazon S3. Create an S3 notification for new file uploads to send a message to the security team through an Amazon Simple Notification Service (Amazon SNS) topic."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 8,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-01-15T04:48:00.000Z",
        "voteCount": 5,
        "content": "I agree"
      },
      {
        "date": "2023-11-01T02:25:00.000Z",
        "voteCount": 2,
        "content": "D is incorrect. You can't use Config to configure SCP."
      },
      {
        "date": "2024-05-08T02:06:00.000Z",
        "voteCount": 1,
        "content": "correct answer is C https://docs.aws.amazon.com/aws-managed-policy/latest/reference/AmazonSSMManagedInstanceCore.html"
      },
      {
        "date": "2023-04-25T16:24:00.000Z",
        "voteCount": 1,
        "content": "I think it's C guys."
      },
      {
        "date": "2023-04-18T00:11:00.000Z",
        "voteCount": 2,
        "content": "D is incorrect because AWS Config cannot configure resources"
      },
      {
        "date": "2023-03-02T03:33:00.000Z",
        "voteCount": 1,
        "content": "C seems correct"
      },
      {
        "date": "2023-02-20T12:47:00.000Z",
        "voteCount": 3,
        "content": "Selected Answer: D\nI agree"
      },
      {
        "date": "2023-02-19T04:03:00.000Z",
        "voteCount": 3,
        "content": "The role does not exist, it is a policy as has been said; moreover, the statement talks about organisations and the answer C does not reflect any relationship .... while in D it does, that and the platform indicates D.... so I will choose D (I do not assure that it is not C, but I mark D)."
      },
      {
        "date": "2023-02-13T04:17:00.000Z",
        "voteCount": 3,
        "content": "Agree with DerekKey, AmazonSSMManagedlnstanceCore is an IAM policy, not a role.\nAWS Systems Manager Automation is a tool we need."
      },
      {
        "date": "2024-05-08T02:08:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/aws-managed-policy/latest/reference/AmazonSSMManagedInstanceCore.html\npls before comment check document"
      },
      {
        "date": "2023-01-28T14:06:00.000Z",
        "voteCount": 2,
        "content": "Answer is C. D is incorrect because AWS Config cannot configure resources but only can define rules that the resources should comply with. Therefore the answer is C. Even if AmazonSSMManagedlnstanceCore is a managed policy and not an IAM  role I will go with C because this policy is to be attached to an IAM role for EC2 to access System Manager."
      },
      {
        "date": "2023-01-23T17:17:00.000Z",
        "voteCount": 3,
        "content": "C for me\nD seems wrong: AWS Config to attach an SCP to the root organization is wrong you don't need aws config to apply SCP to root or ou's\nSCP allow, doesn't provide the necessary permissions for EC2 to access SSM. SCP is designed to limit accounts to access resources otherwise by default root enables accessing for all accounts"
      },
      {
        "date": "2023-01-23T07:52:00.000Z",
        "voteCount": 2,
        "content": "D seems misleading. Can't find documentation of using AWS Config to attach an SCP.\n\"Configure AWS Config to attach an SCP to the root organization account\". Wouldn't you do this in AWS Organizations?"
      },
      {
        "date": "2023-01-23T04:02:00.000Z",
        "voteCount": 3,
        "content": "AmazonSSMManagedlnstanceCore is the IAM policy, not a role - C is wrong"
      },
      {
        "date": "2023-01-17T14:37:00.000Z",
        "voteCount": 2,
        "content": "C for me"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 174,
    "url": "https://www.examtopics.com/discussions/amazon/view/95405-exam-aws-devops-engineer-professional-topic-1-question-174/",
    "body": "During the next CodePipeline run, the pipeline exits with a FAILED state during the build stage. The DevOps engineer verifies that the correct Systems Manager parameter path is in place for the environment variable values that were changed. The DevOps engineer also validates that the environment variable type is Parameter.<br><br>Why did the pipeline fail?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe CodePipeline IAM service role does not have the required IAM permissions to use Parameter Store.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe CodePipeline IAM service role does not have the required IAM permissions to use the aws/ssm KMS key.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe CodeBuild IAM service role does not have the required IAM permissions to use Parameter Store.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe CodeBuild IAM service role does not have the required IAM permissions to use the aws/ssm KMS key."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-05-07T04:38:00.000Z",
        "voteCount": 1,
        "content": "C is more suitable."
      },
      {
        "date": "2023-05-03T10:26:00.000Z",
        "voteCount": 1,
        "content": "A. If Codepipeline iam service role's does not have the required iam services to use parameter store it would fail in all stages of the codepipeline services.\nB. it's not right because the decripter is SSMM\nC.  it looks more plausible, because codebuild service needs permissions to access to parameter store."
      },
      {
        "date": "2023-04-25T16:26:00.000Z",
        "voteCount": 1,
        "content": "c-c-c-c-c-c-c-c-c"
      },
      {
        "date": "2023-01-28T20:35:00.000Z",
        "voteCount": 3,
        "content": "Answer is C. It has to be permission of the IAM Service Role of CodeBuild. It cannot be AWS/SSM Key because System Manager is supposed to decrypt secured string on behalf of the caller of System Manager to retrieve the value from the Parameter Store."
      },
      {
        "date": "2023-01-17T04:47:00.000Z",
        "voteCount": 4,
        "content": "CodePipeline uses CodeBuild to run the build stage, so the CodeBuild IAM service role is responsible for interacting with the services and resources specified in the build stage. If the role does not have the required IAM permissions to use Parameter Store, the pipeline will fail during the build stage when the CodeBuild service attempts to access the parameter values specified in the environment variables.\nIt's a good practice to check the IAM permissions for the roles that CodePipeline, CodeBuild and CodeDeploy uses and make sure they have the correct permissions to access the services and resources that are being used in the pipeline."
      },
      {
        "date": "2023-01-15T04:33:00.000Z",
        "voteCount": 2,
        "content": "i vote C"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 175,
    "url": "https://www.examtopics.com/discussions/amazon/view/95038-exam-aws-devops-engineer-professional-topic-1-question-175/",
    "body": "A company has multiple AWS accounts. The company uses AWS Single Sign-On (AWS SSO) that is integrated with AWS Toolkit for Microsoft Azure DevOps. The attributes for access control feature is enabled in AWS SSO.<br><br>The attribute mapping list contains two entries. The department key is mapped to ${path:enterprise.department}. The costCenter key is mapped to ${path:enterprise.costCenter}.<br><br>All existing Amazon EC2 instances have a department tag that corresponds to three company departments (d1, d2, d3). A DevOps engineer must create policies based on the matching attributes. The policies must minimize administrative effort and must grant each Azure AD user access to only the EC2 instances that are tagged with the user's respective department name.<br><br>Which condition key should the DevOps engineer include in the custom permissions policies to meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<img src=\"https://img.examtopics.com/aws-devops-engineer-professional/image2.png\"><br>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<img src=\"https://img.examtopics.com/aws-devops-engineer-professional/image3.png\"><br>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<img src=\"https://img.examtopics.com/aws-devops-engineer-professional/image4.png\"><br>\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<img src=\"https://img.examtopics.com/aws-devops-engineer-professional/image5.png\">"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-05-07T04:40:00.000Z",
        "voteCount": 1,
        "content": "C is correct answer"
      },
      {
        "date": "2023-01-28T20:49:00.000Z",
        "voteCount": 1,
        "content": "C is correct."
      },
      {
        "date": "2023-01-13T05:58:00.000Z",
        "voteCount": 2,
        "content": "i think"
      },
      {
        "date": "2023-01-14T19:11:00.000Z",
        "voteCount": 2,
        "content": "Agree with Option C as per this link\nhttps://aws.amazon.com/blogs/security/simplify-granting-access-to-your-aws-resources-by-using-tags-on-aws-iam-users-and-roles/"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 176,
    "url": "https://www.examtopics.com/discussions/amazon/view/95692-exam-aws-devops-engineer-professional-topic-1-question-176/",
    "body": "A company deploys updates to its Amazon API Gateway API several times a week by using an AWS CodePipeline pipeline. As part of the update process, the company exports the JavaScript SDK for the API from the API Gateway console and uploads the SDK to an Amazon S3 bucket.<br><br>The company has configured an Amazon CloudFront distribution that uses the S3 bucket as an origin. Web clients then download the SDK by using the CloudFront distribution's endpoint. A DevOps engineer needs to implement a solution to make the new SDK available automatically during new API deployments.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a CodePipeline action immediately after the deployment stage of the API. Configure the action to invoke an AWS Lambda function. Configure the Lambda function to download the SDK from API Gateway, upload the SDK to the S3 bucket, and create a CloudFront invalidation for the SDK path.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a CodePipeline action immediately after the deployment stage of the API. Configure the action to use the CodePipeline integration with API Gateway to export the SDK to Amazon S3. Create another action that uses the CodePipeline integration with Amazon S3 to invalidate the cache for the SDK path.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon EventBridge (Amazon CloudWatch Events) rule that reacts to UpdateStage events from aws.apigateway. Configure the rule to invoke an AWS Lambda function to download the SDK from API Gateway, upload the SDK to the S3 bucket, and call the CloudFront API to create an invalidation for the SDK path.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon EventBridge (Amazon CloudWatch Events) rule that reacts to CreateDeployment events from aws.apigateway. Configure the rule to invoke an AWS Lambda function to download the SDK from API Gateway, upload the SDK to the S3 bucket, and call the S3 API to invalidate the cache for the SDK path."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 10,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-05-07T04:47:00.000Z",
        "voteCount": 1,
        "content": "A is more suitable in this scenario."
      },
      {
        "date": "2023-04-25T16:34:00.000Z",
        "voteCount": 1,
        "content": "a-a-a-a-a-a-a-a-a"
      },
      {
        "date": "2023-02-13T04:28:00.000Z",
        "voteCount": 1,
        "content": "CodePipeline is better than Lambda function."
      },
      {
        "date": "2023-01-29T07:21:00.000Z",
        "voteCount": 4,
        "content": "Although A will meet the requirements, C will as well. So why would I select A over C? Only because all required steps are carried as part of the CodePipeline workflow. With C, the Pipeline execution completes without knowing whether or not the Lambda function invoked via CloudWatch event rule is going to successfully complete or not."
      },
      {
        "date": "2023-01-24T15:17:00.000Z",
        "voteCount": 2,
        "content": "I go with A"
      },
      {
        "date": "2023-01-17T04:52:00.000Z",
        "voteCount": 3,
        "content": "This solution would allow the company to automate the process of updating the SDK and making it available to web clients.\nBy adding a CodePipeline action immediately after the deployment stage of the API, the Lambda function will be invoked automatically each time the API is updated.\nThe Lambda function should be able to download the new SDK from API Gateway, upload it to the S3 bucket and also create a CloudFront invalidation for the SDK path so that the latest version of the SDK is available for the web clients.\nThis is the most straight forward solution and it will meet the requirements."
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 177,
    "url": "https://www.examtopics.com/discussions/amazon/view/95358-exam-aws-devops-engineer-professional-topic-1-question-177/",
    "body": "A company has a single AWS account that runs hundreds of Amazon EC2 instances in a single AWS Region. New EC2 instances are launched and terminated each hour in the account. The account also includes existing EC2 instances that have been running for longer than a week.<br><br>The company's security policy requires all running EC2 instances to use an EC2 instance profile. If an EC2 instance does not have an instance profile attached, the EC2 instance must use a default instance profile that has no IAM permissions assigned.<br><br>A DevOps engineer reviews the account and discovers EC2 instances that are running without an instance profile. During the review, the DevOps engineer also observes that new EC2 instances are being launched without an instance profile.<br><br>Which solution will ensure that an instance profile is attached to all existing and future EC2 instances in the Region?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an Amazon EventBridge (Amazon CloudWatch Events) rule that reacts to EC2 RunInstances API calls. Configure the rule to invoke an AWS Lambda function to attach the default instance profile to the EC2 instances.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the ec2-instance-profile-attached AWS Config managed rule with a trigger type of configuration changes. Configure an automatic remediation action that invokes an AWS Systems Manager Automation runbook to attach the default instance profile to the EC2 instances.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an Amazon EventBridge (Amazon CloudWatch Events) rule that reacts to EC2 Startlnstances API calls. Configure the rule to invoke an AWS Systems Manager Automation runbook to attach the default instance profile to the EC2 instances.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the iam-role-managed-policy-check AWS Config managed rule with a trigger type of configuration changes. Configure an automatic remediation action that invokes an AWS Lambda function to attach the default instance profile to the EC2 instances."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-08-18T14:09:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/config/latest/developerguide/ec2-instance-profile-attached.html"
      },
      {
        "date": "2023-06-21T01:49:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2023-05-07T05:08:00.000Z",
        "voteCount": 1,
        "content": "B is more suitable then D. I will go with B."
      },
      {
        "date": "2023-01-29T07:43:00.000Z",
        "voteCount": 1,
        "content": "B is correct."
      },
      {
        "date": "2023-01-17T05:07:00.000Z",
        "voteCount": 2,
        "content": "The most secure solution is B. Associate the CodeCommit repository with Amazon CodeGuru Reviewer. Manually check the code review for any recommendations. Choose the option to protect the secret. Update the SAM templates and the Python code to pull the secret from AWS Secrets Manager."
      },
      {
        "date": "2023-01-14T19:05:00.000Z",
        "voteCount": 3,
        "content": "I think should be Option B as per this linkhttps://docs.aws.amazon.com/config/latest/developerguide/ec2-instance-profile-attached.html"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 178,
    "url": "https://www.examtopics.com/discussions/amazon/view/95029-exam-aws-devops-engineer-professional-topic-1-question-178/",
    "body": "A company has a single AWS account where active development occurs. The company's security team has implemented Amazon GuardDuty, AWS Config, and AWS CloudTrail within the account. The security team wants to receive notifications in near real time for only high-severity findings from GuardDuty. The security team uses an Amazon Simple Notification Service (Amazon SNS) topic for notifications from other security tools in the account.<br><br>How can a DevOps engineer meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an Amazon EventBridge (Amazon CloudWatch Events) rule that detects GuardDuty findings. Use an input transformer to detect high-severity event patterns. Configure the rule to publish a message to the SNS topic.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an Amazon EventBridge (Amazon CloudWatch Events) rule that detects noncompliance with the guardduty-non-archived-findings AWS Config managed rule for high-severity GuardDuty findings. Configure the EventBridge (CloudWatch Events) rule to publish a message to the SNS topic.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an Amazon EventBridge (Amazon CloudWatch Events) rule with an event pattern that matches GuardDuty ListFindings API calls with a high severity level. Configure the rule to publish a message to the SNS topic.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an Amazon EventBridge (Amazon CloudWatch Events) rule with an event pattern that matches GuardOuty findings that have a high severity level within the event. Configure the rule to publish a message to the SNS topic.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-05-07T05:10:00.000Z",
        "voteCount": 1,
        "content": "D is straightforward answer to the scenario."
      },
      {
        "date": "2023-04-25T16:46:00.000Z",
        "voteCount": 1,
        "content": "d-d-d-d-d-d"
      },
      {
        "date": "2023-01-29T08:00:00.000Z",
        "voteCount": 1,
        "content": "D is correct."
      },
      {
        "date": "2023-01-17T05:09:00.000Z",
        "voteCount": 3,
        "content": "This solution will meet the requirements because it uses EventBridge to match only high-severity GuardDuty findings, and it publishes a message to the SNS topic for near real-time notifications."
      },
      {
        "date": "2023-01-13T05:10:00.000Z",
        "voteCount": 1,
        "content": "D\nhttps://docs.aws.amazon.com/guardduty/latest/ug/guardduty_findings_cloudwatch.html"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 179,
    "url": "https://www.examtopics.com/discussions/amazon/view/100642-exam-aws-devops-engineer-professional-topic-1-question-179/",
    "body": "A company has a VPC that consists of a public subnet and a private subnet. The company has an application that runs on Amazon EC2 instances that are in the private subnet. An Application Load Balancer is in the public subnet and distributes traffic to the EC2 instances.<br><br>The company has enabled Amazon GuardDuty for the account. The company\u2019s DevOps team has a list of external IP ranges that is updated each day. The list is stored in an Amazon S3 bucket in the account. A DevOps engineer needs to configure GuardDuty to create a GuardDuty finding when traffic to the application originates from an IP range in the external IP range list.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon EventBridge rule that runs daily and invokes an AWS Lambda function. Configure the Lambda function to retrieve the most recent list of external IP ranges from the S3 bucket. For each IP range in the list, configure the Lambda function to create a GuardDuty finding filter on the publicIp filter attribute.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a threat list in GuardDuty. Set the source as the list of external IP ranges in the S3 bucket. Create an Amazon EventBridge rule that runs daily and invokes an AWS Lambda function. Configure the Lambda function to refresh the threat list in GuardDuty to match the list of external IP ranges in the S3 bucket.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a trusted IP list in GuardDuty. Set the source as the list of external IP ranges in the S3 bucket. Create an Amazon EventBridge rule that runs daily and invokes an AWS Lambda function. Configure the Lambda function to refresh the trusted IP list in GuardDuty to match the list of external IP ranges in the S3 bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon EventBridge rule that runs daily and invokes an AWS Lambda function. Configure the Lambda function to retrieve the most recent list of external IP ranges from the S3 bucket. For each IP range in the list, configure the Lambda function to create a GuardDuty finding filter on the localIp filter attribute."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 4,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-07-31T12:22:00.000Z",
        "voteCount": 1,
        "content": "Can only be B.\nC= does not generate a finding for trusted IPs. They might be logged but by default no finding."
      },
      {
        "date": "2023-04-25T16:50:00.000Z",
        "voteCount": 1,
        "content": "b-b-b-bb-b-b-b-b-b-b-b"
      },
      {
        "date": "2023-02-25T00:31:00.000Z",
        "voteCount": 3,
        "content": "It is definitely the B\n\nhttps://docs.aws.amazon.com/guardduty/latest/ug/guardduty_upload-lists.html"
      },
      {
        "date": "2023-02-26T05:24:00.000Z",
        "voteCount": 1,
        "content": "Agree. From the link: \"For Location, specify the location of the list - this is the S3 bucket where you store your trusted IP list or threat list and the file that contains your list\""
      },
      {
        "date": "2023-02-24T11:45:00.000Z",
        "voteCount": 3,
        "content": "B is the right answer because configuring a threat list will automatically create a finding\nnot A because creates only filter not a finding\nnot C because we want to block the list not trust it\nnot D because creates only filter not finding"
      },
      {
        "date": "2023-02-24T05:30:00.000Z",
        "voteCount": 1,
        "content": "C is the right answer https://docs.aws.amazon.com/guardduty/latest/ug/guardduty_upload-lists.html"
      },
      {
        "date": "2023-02-24T02:56:00.000Z",
        "voteCount": 1,
        "content": "Correct answere is A\nhttps://docs.aws.amazon.com/guardduty/latest/ug/guardduty_filter-findings.html"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 180,
    "url": "https://www.examtopics.com/discussions/amazon/view/100498-exam-aws-devops-engineer-professional-topic-1-question-180/",
    "body": "A company is using an Amazon API Gateway API and an AWS Lambda function to host a microservice. The microservice accesses pricing data in an Amazon DynamoDB table for the company\u2019s online store.<br><br>Interest in the online store has increased. As a result, latency issues and throttling on the DynamoDB table are occurring when a specific query runs. Some internal services access the DynamoDB table directly. No caching is enabled for the current solution.<br><br>A DevOps engineer notices that repeat requests to the API are taking the same amount of time as unique requests. The DevOps engineer must reduce the latency for the repeat requests to the API and must reduce the throttling on the DynamoDB table.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable caching for API Gateway stages. Use DynamoDB Accelerator (DAX) for the DynamoDB table.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable caching tor API Gateway stages. Use Amazon ElastiCache for Memcached caching for the DynamoDB table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse provisioned concurrency for the Lambda function. Use DynamoDB Accelerator (DAX) for the DynamoDB table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse provisioned concurrency for the Lambda function. Increase the RCUs for the DynamoDB table."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-05-07T05:19:00.000Z",
        "voteCount": 1,
        "content": "A is more suitable as combination of API Gateway Caching and DAX is reliable for throttling and repeat request on DB."
      },
      {
        "date": "2023-02-26T09:09:00.000Z",
        "voteCount": 1,
        "content": "DAX is recomended against memcached"
      },
      {
        "date": "2023-02-24T12:09:00.000Z",
        "voteCount": 3,
        "content": "API GATEWAY CACHE/DAX combo good for caching/repeat requests and db throttling issue"
      },
      {
        "date": "2023-02-24T02:36:00.000Z",
        "voteCount": 3,
        "content": "dynamodb DAX and api gateway caching"
      },
      {
        "date": "2023-02-23T07:13:00.000Z",
        "voteCount": 3,
        "content": "Answer is A . Enable caching for API Gateway stages. Use DynamoDB Accelerator (DAX) for the DynamoDB table. Amazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for Amazon DynamoDB that delivers up to a 10 times performance improvement\u2014from milliseconds to microseconds\u2014even at millions of requests per second"
      },
      {
        "date": "2023-02-23T07:11:00.000Z",
        "voteCount": 2,
        "content": "Answer is A . Enable caching for API Gateway stages. Use DynamoDB Accelerator (DAX) for the DynamoDB table. Amazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for Amazon DynamoDB that delivers up to a 10 times performance improvement\u2014from milliseconds to microseconds\u2014even at millions of requests per second"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 181,
    "url": "https://www.examtopics.com/discussions/amazon/view/100637-exam-aws-devops-engineer-professional-topic-1-question-181/",
    "body": "A production account has a requirement that any Amazon EC2 instance that has been logged in to manually must be terminated within 24 hours. All applications in the production account are using Auto Scaling groups with the Amazon CloudWatch Logs agent configured.<br><br>How can this process be automated?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a CloudWatch Logs subscription to an AWS Step Functions application. Configure an AWS Lambda function to add a tag to the EC2 instance that produced the login event and mark the instance to be decommissioned. Create an Amazon EventBridge rule to invoke a second Lambda function once a day that will terminate all instances with this tag.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon CloudWatch alarm that will be invoked by the login event. Send the notification to an Amazon Simple Notification Service (Amazon SNS) topic that the operations team is subscribed to, and have them terminate the EC2 instance within 24 hours.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon CloudWatch alarm that will be invoked by the login event. Configure the alarm to send to an Amazon Simple Queue Service (Amazon SQS) queue. Use a group of worker instances to process messages from the queue, which then schedules an Amazon EvantBridge rule to be invoked.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a CloudWatch Logs subscription in an AWS Lambda function. Configure the function to add a tag to the EC2 instance that produced the login event and mark the instance to be decommissioned. Create an Amazon EventBridge rule to invoke a daily Lambda function that terminates all instances with this tag.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-02-24T12:20:00.000Z",
        "voteCount": 3,
        "content": "voting D\nA - also, does not say how the lambda knows the event to mark the ec2 instance \nB -  manual, not automated\nC - too complex\nD - will do the job"
      },
      {
        "date": "2023-02-24T02:38:00.000Z",
        "voteCount": 2,
        "content": "I'll go with D\nCloudWatch Logs subscription with step functions is not possible"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 182,
    "url": "https://www.examtopics.com/discussions/amazon/view/100639-exam-aws-devops-engineer-professional-topic-1-question-182/",
    "body": "A company is migrating Docker repositories to Amazon Elastic Container Registry (Amazon ECR) in an existing AWS account. A DevOps engineer needs to automate the management of images that are uploaded to the repositories. The solution must limit the number of image versions. As a first step, the DevOps engineer creates a private repository in Amazon ECR for each repository that the company will migrate.<br><br>What should the DevOps engineer do next to meet the requirements in the MOST operationally efficient manner?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Lambda function to scan the images in each repository for the number of versions present. Configure the Lambda function to delete older versions of images if the number of images is greater than the desired number of images. Schedule the Lambda function to run automatically at regular intervals,",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a repository policy that assesses the number of images and deletes older versions if the number of images is greater than the desired number of images. Apply the repository policy to each private repository.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Step Functions state machine Express Workflow to scan the images in each repository for the number of versions present. Configure the Express Workflow to delete older versions of images if the number of images is greater than the desired number of images. Configure the state machine to run every time an image is pushed to a repository.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPush an image into each private repository. In each private repository, create a lifecycle policy preview to delete older versions of images if the number of images is greater than the desired number of images. Test the lifecycle policy and validate the impact. Apply the lifecycle policy to manage the images.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-08-19T01:40:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/AmazonECR/latest/userguide/LifecyclePolicies.html"
      },
      {
        "date": "2023-04-25T16:53:00.000Z",
        "voteCount": 1,
        "content": "d-d-d-d-d-d-d-d"
      },
      {
        "date": "2023-02-24T12:31:00.000Z",
        "voteCount": 2,
        "content": "D\nECR lifecycle policies are meant for this"
      },
      {
        "date": "2023-02-24T06:28:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/AmazonECR/latest/userguide/LifecyclePolicies.html"
      },
      {
        "date": "2023-02-24T02:39:00.000Z",
        "voteCount": 1,
        "content": "D is the correct answere\nhttps://docs.aws.amazon.com/AmazonECR/latest/userguide/lifecycle_policy_examples.html"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 183,
    "url": "https://www.examtopics.com/discussions/amazon/view/100640-exam-aws-devops-engineer-professional-topic-1-question-183/",
    "body": "A DevOps engineer has created an AWS CloudFormation template. The template includes the following snippet:<br><br><img src=\"https://img.examtopics.com/aws-devops-engineer-professional/image6.png\"><br><br>When the template is launched, CloudFormation performs a rollback and reports the following error message: Received 0 SUCCESS signal(s) cut of 1.<br><br>Which combination of steps should the DevOps engineer take to resolve this error? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the UserData attribute to use the cfn-signal helper script.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the AutoScalingGroup resource with a DependsOn LaunchConfig.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the LaunchConfig resource type to AWS::EC2::LaunchTemplate.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the CreationPolicy ResourceSignal Timeout.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRemove the CreationPolicy attribute. Create new WaitHandle and WaitCondition resources."
    ],
    "answer": "AD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AD",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-02-24T12:51:00.000Z",
        "voteCount": 5,
        "content": "By increasing the Timeout, AWS CloudFormation will wait longer for the ResourceSignal, giving the resource more time to become available and signaling that it is ready. This can help prevent the stack creation from failing due to a premature timeout.\nPT5M -&gt; 5 minutes  \nalso add, cfn-signal with exit code etc. in userdata\n/opt/aws/bin/cfn-signal -e $? --stack &lt;stack name&gt; --resource &lt;resource name&gt;"
      },
      {
        "date": "2023-03-05T12:49:00.000Z",
        "voteCount": 1,
        "content": "AD sounds right"
      },
      {
        "date": "2023-02-24T06:42:00.000Z",
        "voteCount": 2,
        "content": "https://aws.amazon.com/premiumsupport/knowledge-center/cloudformation-failed-signal/?nc1=h_ls"
      },
      {
        "date": "2023-02-24T02:44:00.000Z",
        "voteCount": 1,
        "content": "https://aws.amazon.com/blogs/devops/use-a-creationpolicy-to-wait-for-on-instance-configurations/"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 184,
    "url": "https://www.examtopics.com/discussions/amazon/view/100500-exam-aws-devops-engineer-professional-topic-1-question-184/",
    "body": "A company hosts a multi-tenant application on Amazon EC2 instances behind an Application Load Balancer. The instances run Windows Server and are in an Auto Scaling group. The application uses a license file on the instances that can be updated on the instances without customer disruption. When a new customer purchases access to the application, the company's licensing team adds a new license key to a file in an Amazon S3 bucket. After the license file is updated, the operations team manually updates the EC2 instances.<br><br>A DevOps engineer needs to automate the EC2 instance file update process. The automated process must decrease the time for EC2 instances to get the updated license file and must notify the operations team about success or failure of the update process.<br><br>The DevOps engineer creates a resource group in AWS Resource Groups. The resource group uses a tag that the Auto Sealing group applies to the EC2 instances.<br><br>What should the DevOps engineer do next to meet the requirements MOST cost-effectively?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an S3 event notification to invoke an AWS Lambda function when the license file is updated in the S3 bucket. Configure the Lambda function to invoke AWS Systems Manager Run Command to run the AWS-RunRemoteScript document to download the updated license file. Specify the command from Lambda to run on the application's resource group with 50% concurrency. Configure Amazon Simple Email Service (Amazon SES) notifications for event notifications of SUCCESS and FAILED to send email notifications to the operations team.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an S3 event notification to invoke an AWS Lambda function when the license file is updated in the S3 bucket. Configure the Lambda function to invoke AWS Systems Manager Run Command to run the AWS-RunPowerShellScript document to download the updated license file. Specify the command from Lambda to run on the application's resource group with 50% concurrency. Configure an Amazon Simple Notification Service (Amazon SNS) topic to send event notifications of SUCCESS and FAILED. Subscribe the email addresses of the operations team members to the SNS topic.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon EventBridge scheduled rule that runs each hour to invoke an AWS Lambda function. Configure the Lambda function to invoke AWS Systems Manager Run Command to run the AWS-RunPowerShellScript document to download the updated license file. Specify the command from Lambda to run on the application's resource group with 50% concurrency. Configure an Amazon Simple Notification Service (Amazon SNS) topic to send event notifications of SUCCESS and FAILED. Subscribe the email addresses of the operations team members to the SNS topic.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon EventBridge scheduled rule that runs each hour to invoke an AWS Lambda function. Configure the Lambda function to invoke AWS Systems Manager Run Command to run the AWS-RunRemoteScript document to download the updated license file. Specify the command from Lambda to run on the application's resource group with 50% concurrency. Configure Amazon Simple Email Service (Amazon SES) notifications for event notifications of SUCCESS and FAILED to send email notifications to the operations team."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-02-26T05:46:00.000Z",
        "voteCount": 3,
        "content": "RunPowerShellScript exists (I couldn't image it). In addition, SNS is better than SES, it does not mention that the notification has to be sent by e-mail."
      },
      {
        "date": "2023-02-24T13:06:00.000Z",
        "voteCount": 3,
        "content": "not A,D - runremotescript is for linux\nnot C - too slow\nB - quick"
      },
      {
        "date": "2023-02-24T03:02:00.000Z",
        "voteCount": 1,
        "content": "B is good"
      },
      {
        "date": "2023-02-23T07:21:00.000Z",
        "voteCount": 2,
        "content": "windows=powershell"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 185,
    "url": "https://www.examtopics.com/discussions/amazon/view/100643-exam-aws-devops-engineer-professional-topic-1-question-185/",
    "body": "A company uses AWS Organizations to manage its AWS accounts. A DevOps engineer wants to deploy a new AWS Lambda function to all accounts in the organization by using AWS CloudFormation StackSets. The DevOps engineer uses a delegated administrator account to deploy the stack sets to the member accounts. The stack operation keeps failing, and the stack instance status is OUTDATED.<br><br>Which actions should the DevOps engineer take to remediate this error? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure that the AWS Region is the same for the stack sets and the target resources.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure that the delegated administrator account has a trust relationship with the target account.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure that the resources in the stacks do not have termination protection enabled by default.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure that the CloudFormation template is creating unique global resources.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the stack sets from the management account and not from the delegated administrator account."
    ],
    "answer": "BD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BD",
        "count": 9,
        "isMostVoted": true
      },
      {
        "answer": "AB",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "BE",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-05-26T08:09:00.000Z",
        "voteCount": 1,
        "content": "If we assume that B is correct, then it makes sense that you need to deploy from the management account (in this setup) for the trust relationship to apply"
      },
      {
        "date": "2024-05-08T05:52:00.000Z",
        "voteCount": 1,
        "content": "pls read link: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-troubleshooting.html"
      },
      {
        "date": "2023-03-04T20:55:00.000Z",
        "voteCount": 1,
        "content": "BD\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-troubleshooting.html"
      },
      {
        "date": "2023-02-25T01:40:00.000Z",
        "voteCount": 1,
        "content": "AB\n\nThe only resource to deploy is a lambda, it is not a global resource, D is discarded, C has no impact, E the account has to be the delegate."
      },
      {
        "date": "2023-02-24T19:19:00.000Z",
        "voteCount": 4,
        "content": "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-troubleshooting.html"
      },
      {
        "date": "2023-02-24T13:24:00.000Z",
        "voteCount": 3,
        "content": "A - StackSets cannot create or update stack instances in regions that do not exist in the stack set.\nB-  the delegated administrator account must be authorized to perform actions in the target account.\nNOT C - enabling termination protection on the resources in the stack should not affect the stack instance status.\nNOT D - creating unique global resources should not affect the stack instance status; if you create a non-unique global resource, it would result in stack creation failure not instance status of outdated."
      },
      {
        "date": "2023-02-24T03:03:00.000Z",
        "voteCount": 4,
        "content": "B and D are the best answeres"
      },
      {
        "date": "2023-02-24T03:04:00.000Z",
        "voteCount": 1,
        "content": "Global resources like s3 bucket name must be insured that are given unique name regardless of region or account"
      },
      {
        "date": "2023-02-25T18:49:00.000Z",
        "voteCount": 5,
        "content": "A is wrong. You can have your stack sets defined in a different region but deploy stacks in other regions"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 186,
    "url": "https://www.examtopics.com/discussions/amazon/view/100698-exam-aws-devops-engineer-professional-topic-1-question-186/",
    "body": "A large company has acquired a small company. The large company has an organization in AWS Organizations. The large company needs to integrate the small company\u2019s single AWS account into the organization with minimal impact to the applications that are deployed in the small company's account.<br><br>The large company has deployed AWS Control Tower in its organization and wants to enroll the small company\u2019s account in AWS Control Tower. The large company\u2019s AWS Control Tower configuration includes a security OU, a sandbox OU, and a new destination OU that is set up for the small company's migration. Each company is using AWS Config as part of its account management strategy.<br><br>Which combination of steps should a DevOps engineer take lo meet these requirements? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a landing zone in the security OU of the large company's AWS Control Tower landing zone. Provide the account's email address, the account owners first and last name, and the name of the landing zone created in the security OU to complete the AWS Control Tower Account Factory enrollment request.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate and apply SCPs in the destination OU to restrict the types of resources that can be created in the small company\u2019s account. Assess the impact of the applied SCPs on the small company's account. Delete existing SCPs in the small company\u2019s account.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Config conformance pack that contains the policies that are currently applied to the large company's account. Use AWS Config to assess the impact that enrollment in AWS Control Tower will have on the small company's account. Delete the configuration recorder and delivery channels from the AWS Config settings of the small company's account.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnroll the OU of the small company's account in the large company\u2019s AWS Control Tower environment. Specify the destination OU in the large company's AWS Control Tower landing zone as the receiving OU in the request.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWSControlTowerExecution role in the small company's account. Provide the account's email address, the account owner's first and last name, and the destination OU to complete the AWS Control Tower Account Factory enrollment request.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "BE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BE",
        "count": 9,
        "isMostVoted": true
      },
      {
        "answer": "CE",
        "count": 4,
        "isMostVoted": false
      },
      {
        "answer": "AB",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-10-23T11:02:00.000Z",
        "voteCount": 1,
        "content": "DE. Alternatives ABC are incorrect."
      },
      {
        "date": "2023-09-06T18:08:00.000Z",
        "voteCount": 2,
        "content": "B: Create and apply SCPs in the destination OU to restrict the types of resources that can be created in the small company's account. Assess the impact of the applied SCPs on the small company's account. Delete existing SCPs in the small company\u2019s account."
      },
      {
        "date": "2024-05-25T21:02:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/controltower/latest/userguide/enroll-account.html#what-happens-during-account-enrollment"
      },
      {
        "date": "2023-06-21T05:56:00.000Z",
        "voteCount": 1,
        "content": "EC are correct answers"
      },
      {
        "date": "2023-04-27T18:49:00.000Z",
        "voteCount": 1,
        "content": "it's B and it's also E"
      },
      {
        "date": "2023-04-18T06:30:00.000Z",
        "voteCount": 3,
        "content": "https://docs.aws.amazon.com/controltower/latest/userguide/enroll-account.html\nMoreover the question asks for  minimal impact to the applications that are deployed in the small company's account. Option B is eventually next step"
      },
      {
        "date": "2023-02-26T11:32:00.000Z",
        "voteCount": 3,
        "content": "Agree, BE\nE -  per saeidp aws link, \"you must add this role to each account before you enroll it.\""
      },
      {
        "date": "2023-02-25T19:43:00.000Z",
        "voteCount": 4,
        "content": "Trusted access is necessary. AWSControlTowerExecution role conducts activities required to manage the small account.\nPlace the account into the OU and Apply all the SCPs that are applied in the current OU"
      },
      {
        "date": "2023-02-25T20:00:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/controltower/latest/userguide/enroll-account.html"
      },
      {
        "date": "2023-02-24T13:44:00.000Z",
        "voteCount": 2,
        "content": "A -  company needs to enroll first\nB -  small fish needs to comply with big fish \nC - not required for enrollment\nD - there is no OU to enroll\nE - This role is used for automated operations in AWS Control Tower, such as creating new accounts and setting up guardrails."
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 187,
    "url": "https://www.examtopics.com/discussions/amazon/view/100645-exam-aws-devops-engineer-professional-topic-1-question-187/",
    "body": "A software-as-a-service (SaaS) company is using AWS Elastic Beanstalk to deploy its primary .NET application. The Elastic Beanstalk environment is configured to use Amazon EC2 Auto Scaling and Elastic Load Balancing (ELB) for its underlying Amazon EC2 instances.<br><br>The company is experiencing incidents in which EC2 instances are marked unhealthy and are terminated by Auto Scaling groups after a failed ELB health check. The company's DevOps team must build a solution that will notify the operations team whenever an Auto Scaling group terminates EC2 instances for any existing client environments.<br><br>What should the DevOps team do to meet this requirement?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the email addresses of all operations team members to the SNS topic. Apply a notification configuration for the autoscaling:EC2_INSTANCE_LAUNCH notification type to all the existing Auto Scaling groups.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon Simple Queue Service (Amazon SQS) queue. Add an AWS Lambda function trigger to the SQS queue. Apply a notification configuration for the autoscaling:EC2_INSTANCE_LAUNCH notification type to all the existing Auto Scaling groups.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the email addresses of all operations team members to the SNS topic. Apply a notification configuration for the autoscaling:EC2_INSTANCE_TERMINATE notification type to all the existing Auto Scaling groups.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon Simple Queue Service (Amazon SQS) queue. Add an AWS Lambda function trigger to the SQS queue. Apply a notification configuration for the autoscaling:EC2_INSTANCE_TERMINATE notification type to all the existing Auto Scaling groups."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-04-27T18:51:00.000Z",
        "voteCount": 1,
        "content": "c-c-c-c-c-c-c-c-c-c"
      },
      {
        "date": "2023-02-24T13:58:00.000Z",
        "voteCount": 3,
        "content": "not A - why launch, need terminate notice\nnot B -  same as above\nC -  does the job\nnot D - convoluted"
      },
      {
        "date": "2023-02-24T03:06:00.000Z",
        "voteCount": 3,
        "content": "C probably\nEC2_INSTANCE_TERMINATE plus SNS"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 188,
    "url": "https://www.examtopics.com/discussions/amazon/view/101271-exam-aws-devops-engineer-professional-topic-1-question-188/",
    "body": "A company is hosting a web application in an AWS Region. For disaster recovery purposes, a second region is being used as a standby. Disaster recovery requirements state that session data must be replicated between regions in near-real time and 1% of requests should route to the secondary region to continuously verify system functionality. Additionally, if there is a disruption in service in the main region, traffic should be automatically routed to the secondary region, and the secondary region must be able to scale up to handle all traffic.<br><br>How should a DevOps engineer meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn both regions, deploy the application on AWS Elastic Beanstalk and use Amazon DynamoDB global tables for session data. Use an Amazon Route 53 weighted routing policy with health checks to distribute the traffic across the regions.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn both regions, launch the application in Auto Scaling groups and use DynamoDB for session data. Use a Route 53 failover routing policy with health checks to distribute the traffic across the regions.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn both regions, deploy the application in AWS Lambda, exposed by Amazon API Gateway, and use Amazon RDS PostgreSQL with cross-region replication for session data. Deploy the web application with client-side logic to call the API Gateway directly.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn both regions, launch the application in Auto Scaling groups and use DynamoDB global tables for session data. Enable an Amazon CloudFront weighted distribution across regions. Point the Amazon Route 53 DNS record at the CloudFront distribution."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 11,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 4,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-04-27T18:54:00.000Z",
        "voteCount": 1,
        "content": "a-a-a-a-a-a-a-a-a-a"
      },
      {
        "date": "2023-03-05T13:23:00.000Z",
        "voteCount": 1,
        "content": "A is the most optimal."
      },
      {
        "date": "2023-03-02T08:09:00.000Z",
        "voteCount": 4,
        "content": "revising my pick, A seems right\nnot B because it is either/or failover policy i.e. no 1 percent traffic\nnot C as this is a web app\nnot D, as this is just two regions and cloudfront may not be needed"
      },
      {
        "date": "2023-03-01T20:56:00.000Z",
        "voteCount": 3,
        "content": "only choice with route53 weighted routing policy that is needed to route 1% traffic."
      },
      {
        "date": "2023-03-01T19:28:00.000Z",
        "voteCount": 3,
        "content": "A is right"
      },
      {
        "date": "2023-03-01T18:58:00.000Z",
        "voteCount": 3,
        "content": "A is better than the others. Dynamodb global must be used"
      },
      {
        "date": "2023-03-01T09:33:00.000Z",
        "voteCount": 3,
        "content": "In both regions, launch the application in Auto Scaling groups and use DynamoDB for session data. Use a Route 53 failover routing policy with health checks to distribute the traffic across the regions.\n\nThis solution meets all the requirements mentioned in the question. The use of Auto Scaling groups and DynamoDB allows the application to scale up and down in response to changes in traffic, and DynamoDB's cross-region replication provides near-real-time replication of session data. The use of a Route 53 failover routing policy ensures that traffic is routed to the secondary region in case of a disruption in service in the main region. The 1% traffic routing requirement can be achieved by configuring Route 53's traffic flow policies to direct a portion of traffic to the secondary region."
      },
      {
        "date": "2023-03-01T08:09:00.000Z",
        "voteCount": 4,
        "content": "Option D is the correct answer because it uses DynamoDB global tables for near-real-time replication of session data, CloudFront for a weighted distribution of traffic across regions, and Route 53 for DNS resolution with health checks to automatically route traffic to the secondary region in case of a disruption. This option also meets the requirement to continuously verify system functionality by routing 1% of requests to the secondary region."
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 189,
    "url": "https://www.examtopics.com/discussions/amazon/view/101272-exam-aws-devops-engineer-professional-topic-1-question-189/",
    "body": "A company is implementing an Amazon Elastic Container Service (Amazon ECS) cluster to run its workload. The company architecture will run multiple ECS services on the cluster. The architecture includes an Application Load Balancer on the front end and uses multiple target groups to route traffic.<br><br>A DevOps engineer must collect application and access logs. The DevOps engineer then needs to send the logs to an Amazon S3 bucket for near-real-time analysis.<br><br>Which combination of steps must the DevOps engineer take to meet these requirements? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDownload the Amazon CloudWatch Logs container instance from AWS. Configure this instance as a task. Update the application service definitions to include the logging task",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall the Amazon CloudWatch Logs agent on the ECS instances. Change the logging driver in the ECS task definition to awslogs.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon EventBridge to schedule an AWS Lambda function that will run every 60 seconds and will run the Amazon CloudWatch Logs create-export-task command. Then point the output to the logging S3 bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tActivate access logging on the ALB. Then point the ALB directly to the logging S3 bucket.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tActivate access logging on the target groups that the ECS services use. Then send the logs directly to the logging S3 bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon Kinesis Data Firehose delivery stream that has a destination of the logging S3 bucket. Then create an Amazon CloudWatch Logs subscription filter for Kinesis Data Firehose.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "BDF",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BDF",
        "count": 9,
        "isMostVoted": true
      },
      {
        "answer": "BEF",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-03-05T13:31:00.000Z",
        "voteCount": 3,
        "content": "B - Use cloud watch logs to capture application and access logs from containers\nD - ALB access logs directly go to s3\nF - Firehose will stream cloud watch logs to S3"
      },
      {
        "date": "2023-03-05T11:20:00.000Z",
        "voteCount": 1,
        "content": "C could work but F is more efficient and closer to real-time"
      },
      {
        "date": "2023-03-05T05:53:00.000Z",
        "voteCount": 1,
        "content": "No access logging but have dmesg, messages, docker, ecs-init logs through awslogs driver"
      },
      {
        "date": "2023-03-02T05:58:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-access-logs.html"
      },
      {
        "date": "2023-03-02T04:32:00.000Z",
        "voteCount": 1,
        "content": "https://stackoverflow.com/questions/55487541/enabling-access-logs-for-application-load-balancer"
      },
      {
        "date": "2023-03-02T04:31:00.000Z",
        "voteCount": 1,
        "content": "BDF\n\nE : there is no such thing as server logging access for target group and there is for ALB."
      },
      {
        "date": "2023-03-01T19:07:00.000Z",
        "voteCount": 3,
        "content": "It seems B D F"
      },
      {
        "date": "2023-03-01T09:35:00.000Z",
        "voteCount": 1,
        "content": "I will go with B,E and F"
      },
      {
        "date": "2023-03-01T08:18:00.000Z",
        "voteCount": 1,
        "content": "BEF\nA - not necessary when using cloudwatch logs agent\nB - correct, logs will be captured\nC - not efficient\nD - does not capture ECS logs, only ALB\nE - correct, captures access logs\nF - correct, firehose to s3 is good."
      },
      {
        "date": "2023-03-05T13:30:00.000Z",
        "voteCount": 1,
        "content": "Itd BDF. ECS access logs are captured by changing the logging driver in the ECS task definition to awslogs - https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-logging-monitoring.html"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 190,
    "url": "https://www.examtopics.com/discussions/amazon/view/101273-exam-aws-devops-engineer-professional-topic-1-question-190/",
    "body": "A company has developed a static website hosted on an Amazon S3 bucket. The website is deployed using AWS CloudFormation. The Cloud Formation template defines an S3 bucket and a custom resource that copies content into the bucket from a source location.<br><br>The company has decided that it needs to move the website to a new location, so the existing CloudFormation stack must be deleted and re-created. However, CloudFormation reports that the stack could not be deleted cleanly.<br><br>What is the MOST likely cause and how can the DevOps engineer mitigate this problem for this and future versions of the website?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeletion has failed because the S3 bucket has an active website configuration. Modify the CloudFormation template to remove the WebsiteConfiguration property from the S3 bucket resource",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeletion has failed because the S3 bucket is not empty. Modify the custom resource's AWS Lambda function code to recursively empty the bucket when RequestType is Delete.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeletion has failed because the custom resource does not define a deletion policy. Add a DeletionPolicy property to the custom resource definition with a value of RemoveOnDeletion.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeletion has failed because the S3 bucket is not empty. Modify the S3 bucket resource in the CloudFormation template to add a DeletionPolicy property with a value of Empty."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-03-05T13:37:00.000Z",
        "voteCount": 1,
        "content": "Option B as we bucket needs to be empty before it can be deleted"
      },
      {
        "date": "2023-03-05T11:27:00.000Z",
        "voteCount": 1,
        "content": "NOT D\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html\n\"\"\"\nDelete\nCloudFormation deletes the resource and all its content if applicable during stack deletion. You can add this deletion policy to any resource type. By default, if you don't specify a DeletionPolicy, CloudFormation deletes your resources. However, be aware of the following considerations:\n\nFor AWS::RDS::DBCluster resources, the default policy is Snapshot.\n\nFor AWS::RDS::DBInstance resources that don't specify the DBClusterIdentifier property, the default policy is Snapshot.\n\nFor Amazon S3 buckets, you must delete all objects in the bucket for deletion to succeed.\n\"\"\""
      },
      {
        "date": "2023-03-01T19:32:00.000Z",
        "voteCount": 1,
        "content": "Agree with B"
      },
      {
        "date": "2023-03-01T19:11:00.000Z",
        "voteCount": 1,
        "content": "B is correct. Custom resource"
      },
      {
        "date": "2023-03-01T10:10:00.000Z",
        "voteCount": 1,
        "content": "B is the right answer"
      },
      {
        "date": "2023-03-01T09:38:00.000Z",
        "voteCount": 2,
        "content": "I will go with B"
      },
      {
        "date": "2023-03-01T08:27:00.000Z",
        "voteCount": 1,
        "content": "A - not necessary \nB - not a recommended approach\nC - not applicable\nD - correct"
      },
      {
        "date": "2023-03-05T13:36:00.000Z",
        "voteCount": 2,
        "content": "Option B - For Amazon S3 buckets, you must delete all objects in the bucket for deletion to succeed - https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 191,
    "url": "https://www.examtopics.com/discussions/amazon/view/101263-exam-aws-devops-engineer-professional-topic-1-question-191/",
    "body": "A company is reviewing its IAM policies. One policy written by the DevOps engineer has been flagged as too permissive. The policy is used by an AWS Lambda function that issues a stop command to Amazon EC2 instances tagged with Environment: NonProduction over the weekend. The current policy is:<br><br><img src=\"https://img.examtopics.com/aws-devops-engineer-professional/image7.png\"><br><br>What changes should the engineer make to achieve a policy of least permission? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd the following conditional expression:<br><img src=\"https://img.examtopics.com/aws-devops-engineer-professional/image8.png\"><br>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange \u201cResource\u201d: \u201c*\u201d to \u201cResource\u201d: \u201carn:aws:ec2:*:*:instance/*\u201d<br>\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd the following conditional expression:<br><img src=\"https://img.examtopics.com/aws-devops-engineer-professional/image9.png\"><br>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd the following conditional expression:<br><img src=\"https://img.examtopics.com/aws-devops-engineer-professional/image10.png\">\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange \u201cAction\u201d: \u201cec2:*\u201d to \u201cAction\u201d: \u201cec2:StopInstances\u201d\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd the following conditional expression:<br><img src=\"https://img.examtopics.com/aws-devops-engineer-professional/image11.png\">"
    ],
    "answer": "BDE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BDE",
        "count": 19,
        "isMostVoted": true
      },
      {
        "answer": "DEF",
        "count": 4,
        "isMostVoted": false
      },
      {
        "answer": "BEF",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "BDF",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-05-10T15:53:00.000Z",
        "voteCount": 2,
        "content": "BEF for me."
      },
      {
        "date": "2023-05-07T10:28:00.000Z",
        "voteCount": 2,
        "content": "BDE is correct answer."
      },
      {
        "date": "2023-03-05T13:50:00.000Z",
        "voteCount": 4,
        "content": "Option F is made up :-)\nB - restrict actions to resources of ec2 type\nD - limit actions to ec2s with tag NonProduction\nE - only allow stop instance action"
      },
      {
        "date": "2023-03-05T11:44:00.000Z",
        "voteCount": 3,
        "content": "B. In EC2, select the resource type (instance)\narn:partition:service:region:account-id:resource-type/resource-id\nD. The known tag value\nE. Do not use wildcard action"
      },
      {
        "date": "2023-03-04T23:31:00.000Z",
        "voteCount": 3,
        "content": "Based on JDB333 comment F cannot be right\nIn this case only B D E are left"
      },
      {
        "date": "2023-03-02T15:03:00.000Z",
        "voteCount": 1,
        "content": "I initially selected B E F wrongly. It was a typo"
      },
      {
        "date": "2023-03-02T08:37:00.000Z",
        "voteCount": 4,
        "content": "aws:datetime:friday doesn't exist so F cannot be right."
      },
      {
        "date": "2023-03-03T04:05:00.000Z",
        "voteCount": 1,
        "content": "You are right, I correct -&gt; BDE"
      },
      {
        "date": "2023-03-02T06:12:00.000Z",
        "voteCount": 3,
        "content": "Going with BDE"
      },
      {
        "date": "2023-03-02T03:05:00.000Z",
        "voteCount": 1,
        "content": "DEF\n\nA --&gt; Trust relationships (not policy)\nB and C make no sense"
      },
      {
        "date": "2023-03-03T04:05:00.000Z",
        "voteCount": 2,
        "content": "Correction -&gt; BDE"
      },
      {
        "date": "2023-03-01T21:58:00.000Z",
        "voteCount": 1,
        "content": "DEF\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_examples_ec2-start-stop-tags.html"
      },
      {
        "date": "2023-03-01T21:11:00.000Z",
        "voteCount": 1,
        "content": "DEF looks good to me."
      },
      {
        "date": "2023-03-01T19:52:00.000Z",
        "voteCount": 1,
        "content": "I select DEF"
      },
      {
        "date": "2023-03-01T19:20:00.000Z",
        "voteCount": 1,
        "content": "I go with B E F"
      },
      {
        "date": "2023-03-02T15:02:00.000Z",
        "voteCount": 1,
        "content": "sorry D E F. Typo in my answere"
      },
      {
        "date": "2023-03-01T12:28:00.000Z",
        "voteCount": 2,
        "content": "ABC does not meet requirement"
      },
      {
        "date": "2023-03-01T06:57:00.000Z",
        "voteCount": 1,
        "content": "BDF is my answer"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 192,
    "url": "https://www.examtopics.com/discussions/amazon/view/101275-exam-aws-devops-engineer-professional-topic-1-question-192/",
    "body": "A company updated the AWS CloudFormation template for a critical business application. The stack update process failed due to an error in the updated template, and AWS CloudFormation automatically began the stack rollback process. Later, a DevOps engineer discovered that the application was still unavailable and that the stack was in the UPDATE_ROLLBACK_FAILED state.<br><br>Which combination of actions should the DevOps engineer perform so that the stack rollback can complete successfully? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAttach the AWSCIoudFormationFullAccess IAM policy to the AWS CloudFormation role.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAutomatically recover the stack resources by using AWS CloudFormation drift detection.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIssue a ContinueUpdateRollback command from the AWS CloudFormation console or the AWS CLI.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tManually adjust the resources to match the expectations of the stack.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the existing AWS CloudFormation stack by using the original template."
    ],
    "answer": "CD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "CD",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-05-01T13:00:00.000Z",
        "voteCount": 1,
        "content": "Actually the drift detection is perfect when there is change in resources. Yes I see that the recommendation from AWS is to manually adjust the resources, so in that case C and D would be the answer . Actually rarely an manual is in the correct answer, here it seems it is :)"
      },
      {
        "date": "2023-03-05T13:57:00.000Z",
        "voteCount": 2,
        "content": "CD - https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/troubleshooting.html#troubleshooting-errors-update-rollback-failed"
      },
      {
        "date": "2023-03-01T19:56:00.000Z",
        "voteCount": 2,
        "content": "agree with CD"
      },
      {
        "date": "2023-03-01T19:23:00.000Z",
        "voteCount": 3,
        "content": "I go with C and D"
      },
      {
        "date": "2023-03-01T10:09:00.000Z",
        "voteCount": 1,
        "content": "CD\n\nhttps://aws.amazon.com/es/premiumsupport/knowledge-center/cloudformation-update-rollback-failed/"
      },
      {
        "date": "2023-03-01T08:32:00.000Z",
        "voteCount": 2,
        "content": "A - not necessary\nB - not a suitable approach\nC - correct\nD - correct\nE - does not address the issue"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 193,
    "url": "https://www.examtopics.com/discussions/amazon/view/101265-exam-aws-devops-engineer-professional-topic-1-question-193/",
    "body": "A DevOps engineer is creating an AWS CloudFormation template to deploy a web service. The web service will run on Amazon EC2 instances in a private subnet behind an Application Load Balancer (ALB). The DevOps engineer must ensure that the service can accept requests from clients that have IPv6 addresses.<br><br>What should the DevOps engineer do with the CloudFormation template so that IPv6 clients can access the web service?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd an IPv6 CIDR block to the VPC and the private subnet for the EC2 instances. Create route table entries for the IPv6 network, use EC2 instance types that support IPv6, and assign IPv6 addresses to each EC2 instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAssign each EC2 instance an IPv6 Elastic IP address. Create a target group and add the EC2 instances as targets. Create a listener on port 443 of the ALB, and associate the target group with the ALB.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReplace the ALB with a Network Load Balancer (NLB). Add an IPv6 CIDR block to the VPC and subnets for the NLB, and assign the NLB an IPv6 Elastic IP address.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd an IPv6 CIDR block to the VPC and subnets for the ALB. Create a listener on port 443, and specify the dualstack IP address type on the ALB. Create a target group and add the EC2 instances as targets. Associate the target group with the ALB.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-05-10T15:56:00.000Z",
        "voteCount": 1,
        "content": "Option D is correct - it involves adding an IPv6 CIDR block to the VPC and subnets for the ALB and specifying the dualstack IP address type on the ALB listener. This allows the ALB to listen on both IPv4 and IPv6 addresses, and forward requests to the EC2 instances that are added as targets to the target group associated with the ALB."
      },
      {
        "date": "2023-03-05T11:52:00.000Z",
        "voteCount": 1,
        "content": "IPv6 traffic will be established between ALB and clients. We do not need to take care of EC2 instances."
      },
      {
        "date": "2023-03-01T20:02:00.000Z",
        "voteCount": 2,
        "content": "D is right"
      },
      {
        "date": "2023-03-01T19:29:00.000Z",
        "voteCount": 2,
        "content": "D is correct"
      },
      {
        "date": "2023-03-01T13:02:00.000Z",
        "voteCount": 1,
        "content": "D\n\nDualstack IP"
      },
      {
        "date": "2023-03-01T08:37:00.000Z",
        "voteCount": 2,
        "content": "A - unnecessary\nB - unnecessary as ALB is doing the routing\nC - unnecessary\nD - meets requirement"
      },
      {
        "date": "2023-03-01T07:01:00.000Z",
        "voteCount": 2,
        "content": "D is my answer"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 194,
    "url": "https://www.examtopics.com/discussions/amazon/view/101276-exam-aws-devops-engineer-professional-topic-1-question-194/",
    "body": "A company has migrated its container-based applications to Amazon EKS and want to establish automated email notifications. The notifications sent to each email address are for specific activities related to EKS components. The solution will include Amazon SNS topics and an AWS Lambda function to evaluate incoming log events and publish messages to the correct SNS topic.<br><br>Which logging solution will support these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Amazon CloudWatch Logs to log the EKS components. Create a CloudWatch subscription filter for each component with Lambda as the subscription feed destination.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Amazon CloudWatch Logs to log the EKS components. Create CloudWatch Logs Insights queries linked to Amazon EventBridge events that invoke Lambda.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Amazon S3 logging for the EKS components. Configure an Amazon CloudWatch subscription filter for each component with Lambda as the subscription feed destination.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Amazon S3 logging for the EKS components. Configure S3 PUT Object event notifications with AWS Lambda as the destination."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-05-10T15:59:00.000Z",
        "voteCount": 2,
        "content": "It's A ok?"
      },
      {
        "date": "2023-03-02T15:12:00.000Z",
        "voteCount": 2,
        "content": "A is the correct answer"
      },
      {
        "date": "2023-03-01T20:10:00.000Z",
        "voteCount": 2,
        "content": "A is right"
      },
      {
        "date": "2023-03-01T08:43:00.000Z",
        "voteCount": 3,
        "content": "A - meets requirement\nB - inefficient\nC - lambda subscription unnecessary in this case\nD - may not meet the requirement"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 195,
    "url": "https://www.examtopics.com/discussions/amazon/view/101283-exam-aws-devops-engineer-professional-topic-1-question-195/",
    "body": "A company has a web application that users access over the internet. The web application runs on Amazon EC2 instances behind an Application Load Balancer (ALB). The EC2 instances are in an Auto Scaling group. The ALB is associated with a security group that allows traffic from the internet. The web application has a local cache on each EC2 instance.<br><br>During a recent security incident requests overloaded the web application and caused an outage for the company's customers. In response to the incident, the company added Amazon CloudFront in front of the web application. All customers now access the web application through CloudFront.<br><br>A DevOps engineer must implement a solution that routes all requests through CloudFront. The solution also must give the company the ability to block requests based on the content of the requests, such as header or body information.<br><br>Which combination of steps should the DevOps engineer take to meet these requirements? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS WAF web ACL. Associate the web ACL with the CloudFront distribution. Create rules for each type of traffic that the company wants to block.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate new ALB listener rules on the existing listeners. Configure the new rules to allow or reject incoming traffic based on whether the host header matches the CloudFront fully qualified domain name (FQDN).\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS PrivateLink endpoint service for the ALB Configure the endpoint service to allow requests from CloudFront. Update the web application origin in CloudFront to use the newly created endpoint service's DNS name.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a CloudFront origin access identity (OAI) for the web application. Update the web application origin in CloudFront to use the OAI Update the ALB rules to check for the OAI and return an HTTP 403 error if the OAI header is not present.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Firewall Manager security policy. Attach the security policy to the CloudFront distribution. Use the security policy to attach AWS WAF rule groups for each type of traffic that the company wants to block."
    ],
    "answer": "AB",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AB",
        "count": 10,
        "isMostVoted": true
      },
      {
        "answer": "AE",
        "count": 5,
        "isMostVoted": false
      },
      {
        "answer": "AD",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-11-01T04:55:00.000Z",
        "voteCount": 2,
        "content": "A and B.\nhttps://aws.amazon.com/about-aws/whats-new/2016/01/aws-waf-now-inspects-http-request-body-and-adds-size-constraint-condition/"
      },
      {
        "date": "2024-05-18T18:22:00.000Z",
        "voteCount": 1,
        "content": "Agree with AB. E more complex with use case"
      },
      {
        "date": "2023-10-23T11:19:00.000Z",
        "voteCount": 1,
        "content": "AE. The other ones are incorrect."
      },
      {
        "date": "2023-05-10T16:05:00.000Z",
        "voteCount": 1,
        "content": "A, B for me baby."
      },
      {
        "date": "2023-05-01T05:02:00.000Z",
        "voteCount": 1,
        "content": "ChatGPT: The two steps that the DevOps engineer should take to meet the requirements are:\nA. Create an AWS WAF web ACL. Associate the web ACL with the CloudFront distribution. Create rules for each type of traffic that the company wants to block.\n\nE. Create an AWS Firewall Manager security policy. Attach the security policy to the CloudFront distribution. Use the security policy to attach AWS WAF rule groups for each type of traffic that the company wants to block."
      },
      {
        "date": "2023-03-08T22:50:00.000Z",
        "voteCount": 1,
        "content": "Only AE:\n\nlook at the question: t. The solution also must give the company the ability to block requests based on the content of the requests, such as header or body information.\n\nWe need to block not only by header but also based on body (on demand). To do that we need security policy in firewall."
      },
      {
        "date": "2023-11-01T04:55:00.000Z",
        "voteCount": 1,
        "content": "This could be achieved also with WAF:\nhttps://aws.amazon.com/about-aws/whats-new/2016/01/aws-waf-now-inspects-http-request-body-and-adds-size-constraint-condition/"
      },
      {
        "date": "2023-03-05T14:09:00.000Z",
        "voteCount": 1,
        "content": "Option A is correct because it suggests creating an AWS WAF web ACL and associating it with the CloudFront distribution. AWS WAF is a web application firewall that helps protect web applications from common web exploits that could affect application availability, compromise security, or consume excessive resources. By creating a web ACL and associating it with CloudFront, the DevOps engineer can block requests based on the content of the requests, such as header or body information.\n\nOption D is correct because it suggests creating a CloudFront origin access identity (OAI) for the web application and updating the web application origin in CloudFront to use the OAI. By doing this, the ALB rules can be updated to check for the OAI and return an HTTP 403 error if the OAI header is not present, which ensures that all requests are routed through CloudFront."
      },
      {
        "date": "2023-04-28T07:32:00.000Z",
        "voteCount": 1,
        "content": "D is not correct, because OAI only can be used with aws S3. https://docs.aws.amazon.com/es_es/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html#private-content-restricting-access-to-s3-oai\nSo I'm with A and E, because both says about WAF and Security policy.\nhttps://docs.aws.amazon.com/es_es/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html#private-content-restricting-access-to-s3-oai"
      },
      {
        "date": "2023-03-02T08:46:00.000Z",
        "voteCount": 4,
        "content": "agree, revising my pick to AB.     ALB can do the content based routing better here."
      },
      {
        "date": "2023-03-01T19:35:00.000Z",
        "voteCount": 4,
        "content": "A and B for me"
      },
      {
        "date": "2023-03-01T09:35:00.000Z",
        "voteCount": 4,
        "content": "Both options A and E provide ways to block requests based on the content of the requests"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 196,
    "url": "https://www.examtopics.com/discussions/amazon/view/101284-exam-aws-devops-engineer-professional-topic-1-question-196/",
    "body": "A company needs to scan code changes for security issues before deployment and must prevent noncompliant code from being deployed. The company uses an AWS CodePipeline pipeline that starts when code changes occur. The code changes occur many times each day.<br><br>The company's security team supports a third-party application for code scans and has provided command-line integration steps to submit code scans. The code scan step requires a user name and password.<br><br>Which solution will meet these requirements in the MOST secure way?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new AWS CodeBuild project. Configure the user name and password in an environment variable. Use the user name and password to run the command-line integration steps. Update the CodePipeline pipeline to include a new scan stage. In the new scan stage, include a test action that uses the newly created CodeBuild project.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new AWS CodeBuild project. Store the user name and password as a secret in AWS Secrets Manager Read the secret from Secrets Manager. Use the user name and password to run the command-line integration steps. Update the CodePipeline pipeline to include a new scan stage. In the new scan stage, include a test action that uses the newly created CodeBuild project.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new AWS CodeBuild project. Store the user name and password as a string in AWS Systems Manager Parameter Store. Read the string from Parameter Store. Use the user name and password to run the command-line integration steps. Update the CodePipeline pipeline to include a new scan stage. In the new scan stage, include a test action that uses the newly created CodeBuild project.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpload the user name and password in an encrypted JSON file to an Amazon S3 bucket that has a specific policy to allow only administrators to read the file. Create a new AWS CodeBuild project. Use the user name and password from the file in Amazon S3 to run the command-line integration steps. Update the CodePipeline pipeline to include a new scan stage. In the new scan stage, include a test action that uses the newly created CodeBuild project."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-03-01T09:39:00.000Z",
        "voteCount": 5,
        "content": "B is most secure way"
      },
      {
        "date": "2023-05-10T16:08:00.000Z",
        "voteCount": 1,
        "content": "It's B. \nif you need to store secrets like API keys, database credentials, and certificates, Secrets Manager is the best choice. On the other hand, if you need to store configuration data like application settings and parameters, Parameter Store is the better option."
      },
      {
        "date": "2023-05-07T10:42:00.000Z",
        "voteCount": 2,
        "content": "B is more secure"
      },
      {
        "date": "2023-05-01T13:51:00.000Z",
        "voteCount": 1,
        "content": "Parameter Store in theory can do the same with 'secure'  string. Here he secret manager looks better..."
      },
      {
        "date": "2023-03-02T03:11:00.000Z",
        "voteCount": 3,
        "content": "B is correct (Secret Manager is key)"
      },
      {
        "date": "2023-03-01T20:18:00.000Z",
        "voteCount": 2,
        "content": "agree with B"
      },
      {
        "date": "2023-03-01T19:39:00.000Z",
        "voteCount": 3,
        "content": "B is correct \"Secret manager\""
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 197,
    "url": "https://www.examtopics.com/discussions/amazon/view/101285-exam-aws-devops-engineer-professional-topic-1-question-197/",
    "body": "A company uses AWS CloudFormation to manage an application that runs on Amazon EC2 Instances. The instances are in an Amazon EC2 Auto Scaling group. The company wants to treat its infrastructure as immutable.<br><br>A DevOps engineer must implement a solution to replace two EC2 instances at a time whenever operating system configuration updates are needed or when new Amazon Machine. Images (AMIs) are needed. A minimum of four EC2 instances must be running whenever an update is in progress.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the CloudFormation template to include an UpdatePolicy attribute for the Auto Scaling group. Include the AutoScalingRollingUpdate policy with MinInstancesInService of 4 and MaxBatchSize of 2. Whenever a software update is needed, alter either or both of the ImageId and UserData of the AWS::EC2::LaunchTemplate and update the stack.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the Auto Scaling group\u2019s minimum capacity to 4. Create an AWS CodeDeploy deployment group that has an in-place deployment type. Select Amazon EC2 Auto Scaling group for the environment configuration. Whenever a new revision is available, create a new CodeDeploy deployment that has a deployment configuration of CodeDeployDefault HalfAtATime.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the Auto Scaling group's minimum capacity to 4. Create an AWS CodeDeploy deployment group that has a blue/green deployment type. Select Amazon EC2 Auto Scaling group for the environment configuration. Whenever a new revision is available, create a new CodeDeploy deployment that has a deployment configuration of CodeDeployDefault HalfAtATime.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the CloudFormation template to include a StackPolicy. Designate an AutoScalingReplacingUpdate policy to control the update. Specify MinInstancesInService of 4 and MaxBatchSize of 2. Whenever a software update is needed, alter either or both of the ImageId and UserData of the AWS::EC2::LaunchTemplate and update the stack."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-05-10T16:12:00.000Z",
        "voteCount": 1,
        "content": "A you say?"
      },
      {
        "date": "2023-05-01T22:24:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-updatepolicy.html"
      },
      {
        "date": "2023-05-01T22:24:00.000Z",
        "voteCount": 1,
        "content": "yes, it has to be A"
      },
      {
        "date": "2023-03-01T19:59:00.000Z",
        "voteCount": 3,
        "content": "A is the best"
      },
      {
        "date": "2023-03-01T09:45:00.000Z",
        "voteCount": 4,
        "content": "Option B is incorrect because it suggests using AWS CodeDeploy to perform an in-place deployment type, which is not appropriate for replacing EC2 instances in batches.\n\nOption C is incorrect because it suggests using AWS CodeDeploy to perform a blue/green deployment type, which is not appropriate for replacing EC2 instances in batches.\n\nOption D is incorrect because it suggests using a StackPolicy to control the update, which is not appropriate for this use case."
      },
      {
        "date": "2023-03-01T09:44:00.000Z",
        "voteCount": 4,
        "content": "Option A is the correct solution. It involves modifying the CloudFormation template to include an UpdatePolicy attribute for the Auto Scaling group with the AutoScalingRollingUpdate policy. This policy allows for rolling updates that replace instances in batches of a specified size, which in this case is two instances at a time. The MinInstancesInService attribute ensures that a minimum of four instances are running whenever an update is in progress. To update the software, either the ImageId or UserData of the AWS::EC2::LaunchTemplate is altered and the stack is updated."
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 198,
    "url": "https://www.examtopics.com/discussions/amazon/view/101287-exam-aws-devops-engineer-professional-topic-1-question-198/",
    "body": "A video-sharing company stores its videos in Amazon S3. The company has observed a sudden increase in video access requests, but the company does not know which videos are most popular. The company needs to identify the general access pattern for the video files. This pattern includes the number of users who access a certain file on a given day, as well as the number of pull requests for certain files.<br><br>How can the company meet these requirements with the LEAST amount of effort?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tActivate S3 server access logging. Import the access logs into an Amazon Aurora database. Use an Aurora SQL query to analyze the access patterns.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tActivate S3 server access logging. Use Amazon Athena to create an external table with the log files. Use Athena to create a SQL query to analyze the access patterns.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInvoke an AWS Lambda function for every S3 object access event. Configure the Lambda function to write the file access information, such as user, S3 bucket, and file key, to an Amazon Aurora database. Use an Aurora SQL query to analyze the access patterns.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRecord an Amazon CloudWatch Logs log message for every S3 object access event. Configure a CloudWatch Logs log stream to write the file access information such as user. S3 bucket, and file key, to an Amazon Kinesis Data Analytics for SQL application. Perform a sliding window analysis."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-08-19T05:20:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-s3-access-logs-to-identify-requests.html"
      },
      {
        "date": "2023-05-10T16:13:00.000Z",
        "voteCount": 1,
        "content": "B for me."
      },
      {
        "date": "2023-03-01T20:01:00.000Z",
        "voteCount": 3,
        "content": "B is correct"
      },
      {
        "date": "2023-03-01T09:50:00.000Z",
        "voteCount": 4,
        "content": "Option A involves importing S3 server access logs into an Amazon Aurora database, which requires more effort than using Amazon Athena. Additionally, Aurora is a more complex and expensive option than Athena for this use case.\n\nOption C involves invoking an AWS Lambda function for every S3 object access event and storing the file access information in an Amazon Aurora database. This option requires more setup and ongoing management than using Amazon Athena, which is a simpler and more cost-effective solution.\n\nOption D involves recording a CloudWatch Logs log message for every S3 object access event and sending the log stream to an Amazon Kinesis Data Analytics for SQL application. This option requires additional setup and configuration, which makes it less straightforward and more time-consuming than using Amazon Athena. Additionally, using Kinesis Data Analytics for SQL may be overkill for this use case, as it is primarily designed for real-time streaming data analysis, which is not a requirement for the current scenario."
      },
      {
        "date": "2023-03-01T09:48:00.000Z",
        "voteCount": 4,
        "content": "Activating S3 server access logging and using Amazon Athena to create an external table with the log files is the easiest and most cost-effective way to analyze access patterns. This option requires minimal setup and allows for quick analysis of the access patterns with SQL queries. Additionally, Amazon Athena scales automatically to match the query load, so there is no need for additional infrastructure provisioning or management."
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 199,
    "url": "https://www.examtopics.com/discussions/amazon/view/101304-exam-aws-devops-engineer-professional-topic-1-question-199/",
    "body": "A DevOps engineer is working on a project that is hosted on Amazon Linux and has failed a security review. The DevOps manager has been asked to review the company buildspec.yaml file for an AWS CodeBuild project and provide recommendations. The buildspec.yaml file is configured as follows:<br><br><img src=\"https://img.examtopics.com/aws-devops-engineer-professional/image12.png\"><br><br>What changes should be recommended to comply with AWS security best practices? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a post-build command to remove the temporary files from the container before termination to ensure they cannot be seen by other CodeBuild users.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the CodeBuild project role with the necessary permissions and then remove the AWS credentials from the environment variable.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the DB_PASSWORD as a SecureString value in AWS Systems Manager Parameter Store and then remove the DB_PASSWORD from the environment variables.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMove the environment variables to the \u2018db-deploy-bucket\u2019 Amazon S3 bucket add a prebuild stage to download, then export the variables.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Systems Manager run command versus scp and ssh commands directly to the instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tScramble the environment variables using XOR followed by Base64, add a section to install, and then run XOR and Base64 to the build phase."
    ],
    "answer": "ABC",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ABC",
        "count": 11,
        "isMostVoted": true
      },
      {
        "answer": "BCE",
        "count": 7,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-03-01T12:38:00.000Z",
        "voteCount": 8,
        "content": "Option A: Remove temporary files post-build.\nOption B: Remove AWS credentials from environment.\nOption C: Store DB_PASSWORD in Parameter Store.\nOption D: Move environment variables to S3. ok, ABC options above will meet requirement.\nOption E: Using SSM run command is good for securing instances not directly related to this process.\nOption F: Scramble environment variables is adding complexity."
      },
      {
        "date": "2023-11-01T05:03:00.000Z",
        "voteCount": 1,
        "content": "A is wrong. In no artifact are provided to the code-build run, all the files are automatically removed at the end of the build."
      },
      {
        "date": "2023-05-10T16:16:00.000Z",
        "voteCount": 2,
        "content": "ABC seems correct here."
      },
      {
        "date": "2023-05-07T10:52:00.000Z",
        "voteCount": 3,
        "content": "BCE are the most suitable options for the given scenario."
      },
      {
        "date": "2023-03-08T22:19:00.000Z",
        "voteCount": 4,
        "content": "BCE:\n\nA is wrong. No need to tidy up after CodeBuild invocation. Post build has other purpose:\n\nphases/post_build\nOptional sequence. Represents the commands, if any, that CodeBuild runs after the build. For example, you might use Maven to package the build artifacts into a JAR or WAR file, or you might push a Docker image into Amazon ECR. Then you might send a build notification through Amazon SNS.\n\nphases/post_build/commands\nRequired if post_build is specified. Contains a sequence of scalars, where each scalar represents a single command that CodeBuild runs after the build. CodeBuild runs each command, one at a time, in the order listed, from beginning to end."
      },
      {
        "date": "2023-03-05T12:21:00.000Z",
        "voteCount": 3,
        "content": "NOT A. When a container is terminated, if no volume is mounted, everything will be deleted.\nB. Role with permissions, no need to have an AWS User with Access Key/secret access key.\nC. A way to inject secrets as env vars\nE. System Manager is designed for that!\n\nIn the build command, we are getting from S3 a file called my.cnf and SSH keys (instance.keys) and we are running some commands for having SSH conectividy from the EC2 instance to the instance with the IP 10.25.15.23 for copying there the my.cnf file and restrating the MySQL server ..."
      },
      {
        "date": "2023-03-02T03:21:00.000Z",
        "voteCount": 3,
        "content": "ABC is correct"
      },
      {
        "date": "2023-03-01T20:05:00.000Z",
        "voteCount": 3,
        "content": "A B C are the correct ones"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 200,
    "url": "https://www.examtopics.com/discussions/amazon/view/101288-exam-aws-devops-engineer-professional-topic-1-question-200/",
    "body": "A DevOps engineer at a company is supporting an AWS environment in which all users use AWS IAM identity Center (AWS Single Sign-On). The company wants to immediately disable credentials of any new IAM user and wants the security team to receive a notification.<br><br>Which combination of steps should the DevOps engineer take to meet these requirements? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon EventBridge rule that reacts to an IAM CreateUser API call in AWS CloudTrail.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon EventBridge rule that reacts to an IAM GetLoginProfile API call in AWS CloudTrail.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Lambda function that is a target of the EventBridge rule. Configure the Lambda function to disable any access keys and delete the login profiles that are associated with the IAM user.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Lambda function that is a target of the EventBridge rule. Configure the Lambda function to delete the login profiles that are associated with the IAM user.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon Simple Notification Service (Amazon SNS) topic that is a target of the EventBridge rule. Subscribe the security team\u2019s group email address to the topic.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon Simple Queue Service (Amazon SQS) queue that is a target of the Lambda function. Subscribe the security team's group email address to the queue."
    ],
    "answer": "ACE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ACE",
        "count": 14,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-03-01T09:53:00.000Z",
        "voteCount": 6,
        "content": "ACE meets requirement"
      },
      {
        "date": "2023-05-10T16:21:00.000Z",
        "voteCount": 1,
        "content": "ACE it."
      },
      {
        "date": "2023-05-07T10:55:00.000Z",
        "voteCount": 1,
        "content": "ACE are more suitable for this scenario."
      },
      {
        "date": "2023-03-01T20:10:00.000Z",
        "voteCount": 3,
        "content": "A C E are the correct ones"
      },
      {
        "date": "2023-03-01T10:08:00.000Z",
        "voteCount": 4,
        "content": "I will go with A,C&amp;E"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 201,
    "url": "https://www.examtopics.com/discussions/amazon/view/101289-exam-aws-devops-engineer-professional-topic-1-question-201/",
    "body": "An ecommerce company is receiving reports that its order history page is experiencing delays in reflecting the processing status of orders. The order processing system consists of an AWS Lambda function that uses reserved concurrency. The Lambda function processes order messages from an Amazon Simple Queue Service (Amazon SQS) queue and inserts processed orders into an Amazon DynamoDB table. The DynamoDB table has auto scaling enabled for read and write capacity.<br><br>Which actions should a DevOps engineer take to resolve this delay? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCheck the ApproximateAgeOfOldestMessage metric for the SQS queue. Increase the Lambda function concurrency limit.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCheck the ApproximateAgeOfOldestMessage metric for the SQS queue. Configure a redrive policy on the SQS queue.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCheck the NumberOfMessagesSent metric for the SQS queue. Increase the SQS queue visibility timeout.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCheck the WriteThrottleEvents metric for the DynamoDB table. Increase the maximum write capacity units (WCUs) for the table's scaling policy.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCheck the Throttles metric for the Lambda function. Increase the Lambda function timeout."
    ],
    "answer": "AD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AD",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-05-10T16:26:00.000Z",
        "voteCount": 1,
        "content": "A and D for me."
      },
      {
        "date": "2023-03-01T20:15:00.000Z",
        "voteCount": 2,
        "content": "A and D are the correct answers."
      },
      {
        "date": "2023-03-01T20:17:00.000Z",
        "voteCount": 3,
        "content": "maximum write capacity and read capacity are adjustable in dynabodb auto scaling"
      },
      {
        "date": "2023-03-01T11:58:00.000Z",
        "voteCount": 2,
        "content": "Ad is answer"
      },
      {
        "date": "2023-03-01T09:58:00.000Z",
        "voteCount": 3,
        "content": "AD will help resolve the issue"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 202,
    "url": "https://www.examtopics.com/discussions/amazon/view/101293-exam-aws-devops-engineer-professional-topic-1-question-202/",
    "body": "A company has an application that runs on current-generation Amazon EC2 instances in a VPC. The EC2 instances run Amazon Linux and are launched in an Amazon EC2 Auto Scaling group. The application retrieves data from an Amazon S3 bucket, processes the data, and uploads the processed data to a different S3 bucket.<br><br>Recently, the application's performance worsened. A manual investigation identified that outbound network bandwidth utilization was too high for the type of EC2 instance. The company updated the EC2 instances to a larger EC2 instance size.<br><br>The company's DevOps team needs to receive notification from an Amazon CloudWatch alarm if the application attempts to use more outbound network bandwidth than is available to the EC2 instances.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure EC2 detailed monitoring for the EC2 instances. Create an AWS Lambda function to create a CloudWatch alarm for the bw_out_allowance_exceeded CloudWatch metric for each EC2 instance Configure the alarm to notify the DevOps team.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the unified CloudWatch agent on the EC2 instances to export the bw_out_allowance_exceeded metric to CloudWatch metrics. Create a CloudWatch composite alarm to monitor all bw_out_allowance_exceeded metrics. Configure the alarm to notify the DevOps team.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure VPC flow logging to Amazon CloudWatch Logs for the EC2 instances. Create a CloudWatch Logs metric filter to match events in which bandwidth allowance is exceeded. Create a CloudWatch composite alarm to monitor all bw_out_allowance_exceeded metrics. Configure the alarm to notify the DevOps team.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the unified CloudWatch agent on the EC2 instances to export the bw_out_allowance_exceeded metric to CloudWatch metrics. Create an AWS Lambda function to create a CloudWatch alarm for the bw_out_allowance_exceeded CloudWatch metric for each EC2 instance. Configure the alarm to notify the DevOps team."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 6,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-10-09T08:09:00.000Z",
        "voteCount": 2,
        "content": "That's is D from Official Question Set on AWS Skill Builder."
      },
      {
        "date": "2023-08-25T23:24:00.000Z",
        "voteCount": 1,
        "content": "That's is D from Official Question Set on AWS Skill Builder.\n\nYou can use the unified CloudWatch agent to publish network performance metrics to CloudWatch. The metrics are for each EC2 instance and include a dimension that contains the instance ID. Therefore, you must implement some automation by deploying a Lambda function to create new CloudWatch alarms. The Elastic Network Adapter (ENA) is turned on by default on current-generation EC2 instances that run Amazon Linux. The ENA is required for the CloudWatch agent to export the metrics.\n\nFor more information about the CloudWatch agent and network performance metrics, see Collect Network Performance Metrics.\n\nFor more information about how to test for enhanced networking, see Test Whether Enhanced Networking Is Enabled.\n\nFor more information about the ENA driver, see Metrics for the ENA Driver."
      },
      {
        "date": "2023-08-26T08:55:00.000Z",
        "voteCount": 1,
        "content": "Link1:https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch-Agent-network-performance.html\n\nLink2:https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/enhanced-networking-ena.html#test-enhanced-networking-ena\n\nLink3: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/enhanced-networking-ena.html#test-enhanced-networking-ena"
      },
      {
        "date": "2023-08-26T08:56:00.000Z",
        "voteCount": 1,
        "content": "Link1:https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch-Agent-network-performance.html\nLink2:https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/enhanced-networking-ena.html#test-enhanced-networking-ena\nLink3:https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/monitoring-network-performance-ena.html#network-performance-metrics"
      },
      {
        "date": "2023-05-10T16:43:00.000Z",
        "voteCount": 1,
        "content": "B for me."
      },
      {
        "date": "2023-05-01T04:05:00.000Z",
        "voteCount": 1,
        "content": "ChatGPT: B"
      },
      {
        "date": "2023-04-27T09:46:00.000Z",
        "voteCount": 1,
        "content": "B - Here is the solution: https://repost.aws/es/knowledge-center/ec2-instance-exceeding-network-limits"
      },
      {
        "date": "2023-03-01T20:24:00.000Z",
        "voteCount": 2,
        "content": "Yes, B is the answer\nhttps://aws.amazon.com/blogs/networking-and-content-delivery/amazon-ec2-instance-level-network-performance-metrics-uncover-new-insights/"
      },
      {
        "date": "2023-03-01T11:14:00.000Z",
        "voteCount": 3,
        "content": "voting for B"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 203,
    "url": "https://www.examtopics.com/discussions/amazon/view/101295-exam-aws-devops-engineer-professional-topic-1-question-203/",
    "body": "A company uses Application Load Balancers (ALBs) as part of its application architecture. The company has ALBs in AWS accounts that are part of an organization in AWS Organizations. The company has configured AWS Config in all AWS accounts in the organization.<br><br>The company needs to apply an AWS WAF web ACL with a common set of rules to all ALBs, including any ALBs that are created in the future. Administrators of each AWS account must be able to define their own AWS WAF rules that are in addition to the common rules that the company\u2019s security team provides for all the accounts.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure AWS Firewall Manager for the organization. In the Firewall Manager administrator account, create an AWS WAF policy. Turn on automatic remediation and define the web ACL. Configure the policy scope to apply to all ALBs in the organization.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Resource Access Manager (AWS RAM) from the organization's management account to enable resource sharing in the organization. Create the web ACL. Configure a resource share of the web ACL for the organization. Associate the shared web ACL with all the ALBs in the organization.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up the ALB_WAF_ENABLED AWS Config managed rule with automatic remediation. Configure the rule to create the web ACL and to attach the web ACL to all ALBs in an AWS account. Create an AWS Config conformance pack that contains the rule. Deploy the conformance pack to all AWS accounts in the organization.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure AWS Firewall Manager for the organization. In the Firewall Manager administrator account, create an AWS WAF policy that defines the web ACL. Set up the ALB_WAF_ENABLED AWS Config managed rule with automatic remediation. Configure the rule to attach the web ACL to all ALBs in an AWS account. Deploy the rule to all AWS accounts in the organization."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 6,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-05-10T18:02:00.000Z",
        "voteCount": 1,
        "content": "It's B. Good old RAM to the rescue."
      },
      {
        "date": "2023-05-01T04:03:00.000Z",
        "voteCount": 2,
        "content": "ChatGPT: Option D is the correct solution.\nThis solution involves using AWS Firewall Manager to centrally manage the AWS WAF policy and web ACL for all ALBs in the organization. In the Firewall Manager administrator account, a web ACL is created and defined in an AWS WAF policy. The policy is then deployed to all AWS accounts in the organization.\n\nNext, the ALB_WAF_ENABLED AWS Config managed rule is set up in each AWS account with automatic remediation. This rule will attach the web ACL defined in the AWS WAF policy to all ALBs in the account.\n\nThis approach allows each AWS account to have its own AWS WAF rules in addition to the common set of rules provided by the company\u2019s security team. The central management of the AWS WAF policy and web ACL ensures consistency across all ALBs in the organization, including any ALBs that are created in the future."
      },
      {
        "date": "2023-03-02T10:14:00.000Z",
        "voteCount": 2,
        "content": "revising to A, since WAF is not shareable per saeidp link below."
      },
      {
        "date": "2023-03-02T05:03:00.000Z",
        "voteCount": 1,
        "content": "I like A based on the link saeidp posted"
      },
      {
        "date": "2023-03-01T20:52:00.000Z",
        "voteCount": 3,
        "content": "A for me\nAWS config is only used for detecting the new resources\nhttps://aws.amazon.com/blogs/security/using-aws-firewall-manager-and-waf-to-protect-your-web-applications-with-master-rules-and-application-specific-rules/"
      },
      {
        "date": "2023-03-01T20:52:00.000Z",
        "voteCount": 3,
        "content": "By the way WAF acl are not part of shareables from RAM\nhttps://docs.aws.amazon.com/ram/latest/userguide/shareable.html"
      },
      {
        "date": "2023-03-01T11:22:00.000Z",
        "voteCount": 1,
        "content": "Option A is incorrect because AWS Firewall Manager is not necessary to achieve the goal. Option B is the better solution because it uses AWS RAM to share the web ACL across all accounts. Option C is incorrect because it applies only to AWS Config and does not provide a solution for deploying the web ACL. Option D is incorrect because it also involves AWS Firewall Manager, which is not necessary to achieve the goal."
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 204,
    "url": "https://www.examtopics.com/discussions/amazon/view/101297-exam-aws-devops-engineer-professional-topic-1-question-204/",
    "body": "A company publishes application logs to an Amazon CloudWatch Logs log group in the us-east-1 Region. The company needs to export the logs from us-east-1 to the us-west-2 Region on a weekly basis. The logs must be encrypted in both Regions.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon S3 bucket in us-west-2. Configure server-side encryption with Amazon S3 managed encryption keys (SSE-S3) for the S3 bucket. Create and schedule an AWS Lambda function to run weekly to export the CloudWatch logs from the last week to the S3 bucket in us-west-2.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon S3 bucket in us-west-2. Configure server-side encryption with AWS KMS keys (SSE-KMS) for the S3 bucket. Create and schedule an AWS Lambda function to run weekly to export the CloudWatch logs from the last week to the S3 bucket in us-west-2.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon S3 bucket in us-east-1. Create an S3 bucket in us-west-2. Configure server-side encryption with Amazon S3 managed encryption keys (SSE-S3) and turn on versioning for both S3 buckets. Create and schedule an AWS Lambda function to run weekly to export the CloudWatch logs from the last week to the S3 bucket in us-east-1. Configure a replication rule on the S3 bucket in us-east-1 to replicate the logs to the S3 bucket in us-west-2.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon S3 bucket in us-east-1. Create an S3 bucket in us-west-2. Configure server-side encryption with AWS KMS keys (SSE-KMS) and turn on versioning for both S3 buckets. Create and schedule an AWS Lambda function to run weekly to export the CloudWatch logs from the last week to the S3 bucket in us-east-1. Configure a replication rule on the S3 bucket in us-east-1 to replicate the logs to the S3 bucket in us-west-2.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 6,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 4,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-10-23T11:44:00.000Z",
        "voteCount": 3,
        "content": "A and B are out of the question, as both source and destination buckets must be encrypted.\nC and D differ only in whether KMS is used or not, and KMS has added operational overhead. Standard encryption is just fine. Thus, C for me."
      },
      {
        "date": "2023-07-22T09:15:00.000Z",
        "voteCount": 1,
        "content": "Not sure why so many people choosing wrong answers.\nC+D wrong = S3 replication give you no control for meet weekly backup requirement.\nA or B. A is right. If the IAM principal (Lambda) is in the same account, then it can use the aws/s3. S3 as of 2023 by default enables encryption on buckets. Simplest solution with the minimum of words. B would be the typical answer but it actually involves a few more vital steps, none of which is mentioned. A is answer."
      },
      {
        "date": "2023-05-01T04:00:00.000Z",
        "voteCount": 1,
        "content": "ChatGPT choose D: This solution involves creating two S3 buckets, one in us-east-1 and another in us-west-2, with server-side encryption enabled using AWS KMS keys (SSE-KMS). Both S3 buckets are also configured with versioning to keep track of changes in the logs."
      },
      {
        "date": "2023-03-05T12:54:00.000Z",
        "voteCount": 2,
        "content": "CloudWatch is already encrypted:\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/encrypt-log-data-kms.html\n\nMissing steps in B: KMS key has to be created, and the config in the permissions for publishing and reading for the bucket, etc. \nLeave AWS to do the hard part."
      },
      {
        "date": "2023-03-03T21:32:00.000Z",
        "voteCount": 2,
        "content": "Bucket need to be created.The S3 bucket must reside in the same Region as the log data to export. CloudWatch Logs doesn't support exporting data to S3 buckets in a different Region.\n\nKMS is best option for encryption\n\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/S3ExportTasksConsole.html"
      },
      {
        "date": "2023-03-03T03:42:00.000Z",
        "voteCount": 2,
        "content": "I stay in B\n\nCloudwatch logs encrypted by default, so there is no need for a bucket in the us-east-1 region, so between a and b, b is more secure."
      },
      {
        "date": "2023-03-03T03:43:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/data-protection.html"
      },
      {
        "date": "2023-03-03T20:52:00.000Z",
        "voteCount": 1,
        "content": "I think I will have to agree with B after seeing this link.  If the logs are already encrypted, there is no need to move them to an S3 bucket in us-east-1 first."
      },
      {
        "date": "2023-03-02T14:04:00.000Z",
        "voteCount": 3,
        "content": "For me its D\nThere is no requirement to save cost or management. so for me using AWS KMS-managed keys (SSE-KMS) is a better option than using SSE-S3"
      },
      {
        "date": "2023-03-02T10:35:00.000Z",
        "voteCount": 3,
        "content": "https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/S3Export.html\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/S3ExportTasks.html\nThis mentions that The S3 bucket must reside in the same Region as the log data to export. CloudWatch Logs doesn't support exporting data to S3 buckets in a different Region.\nSo B is wrong. D is correct."
      },
      {
        "date": "2023-03-02T10:22:00.000Z",
        "voteCount": 2,
        "content": "agree, both regions should be encrypted, missed that part.  KMS is better than SSE-S3 as KMS gives control but more setup.  SSE-S3 is less setup work and automatic.  This question does not ask about setup overhead etc, so going with C rather than D."
      },
      {
        "date": "2023-03-02T05:16:00.000Z",
        "voteCount": 2,
        "content": "I think it is C or D based on the requirements of logs being encrypted in both regions."
      },
      {
        "date": "2023-03-01T21:04:00.000Z",
        "voteCount": 1,
        "content": "I go with C\nYou must have the logs encrypted in both side"
      },
      {
        "date": "2023-03-01T11:29:00.000Z",
        "voteCount": 2,
        "content": "Option A also works but uses SSE-S3 instead of Option B AWS KMS keys which offers more better, granular control. Option C and D are not as efficient as they involve replicating logs from the S3 bucket in us-east-1 to the S3 bucket in us-west-2, which increases costs and complexity."
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 205,
    "url": "https://www.examtopics.com/discussions/amazon/view/101301-exam-aws-devops-engineer-professional-topic-1-question-205/",
    "body": "A company runs an application on Amazon EC2 instances. The company uses a series of AWS CloudFormation stacks to define the application resources. A developer performs updates by building and testing the application on a laptop and then uploading the build output and CloudFormation stack templates to Amazon S3. The developer\u2019s peers review the changes before the developer performs the CloudFormation stack update and installs a new version of the application onto the EC2 instances.<br><br>The deployment process is prone to errors and is time-consuming when the developer updates each EC2 instance with the new application. The company wants to automate as much of the application deployment process as possible while retaining a final manual approval step before the modification of the application or resources.<br><br>The company already has moved the source code for the application and the CloudFormation templates to AWS CodeCommit. The company also has created an AWS CodeBuild project to build and test the application.<br><br>Which combination of steps will meet the company's requirements? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an application group and a deployment group in AWS CodeDeploy. Install the CodeDeploy agent on the EC2 instances.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an application revision and a deployment group in AWS CodeDeploy. Create an environment in CodeDeploy. Register the EC2 instances to the CodeDeploy environment.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS CodePipeline to invoke the CodeBuild job, run the CloudFormation update, and pause for a manual approval step. After approval, start the AWS CodeDeploy deployment.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS CodePipeline to invoke the CodeBuild job, create CloudFormation change sets for each of the application stacks, and pause for a manual approval step. After approval, run the CloudFormation change sets and start the AWS CodeDeploy deployment.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS CodePipeline to invoke the CodeBuild job, create CloudFormation change sets for each of the application stacks, and pause for a manual approval step. After approval, start the AWS CodeDeploy deployment."
    ],
    "answer": "AD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AD",
        "count": 7,
        "isMostVoted": true
      },
      {
        "answer": "BD",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-05-07T21:15:00.000Z",
        "voteCount": 1,
        "content": "AD is more suitable for the required achievement."
      },
      {
        "date": "2023-04-18T13:12:00.000Z",
        "voteCount": 1,
        "content": "A- https://docs.aws.amazon.com/codedeploy/latest/userguide/codedeploy-agent.html\nD - This option correctly utilizes AWS CodePipeline to invoke the CodeBuild job and create CloudFormation change sets. It adds a manual approval step before executing the change sets and starting the AWS CodeDeploy deployment. This ensures that the deployment process is automated while retaining the final manual approval step."
      },
      {
        "date": "2023-04-14T10:56:00.000Z",
        "voteCount": 1,
        "content": "B - cannot be a correct since it must have a codedeploy agent before registering\nC - cannot be a correct since according to the question both CF stack changes and code deployment must be reviewed. this answer suggest to commit CF changes before.\nE - Cannot be correct since it skips the part for applying CF change sets.\nTherefore only AD can be correct - for A not sure why need Application group and not a regular application."
      },
      {
        "date": "2023-03-05T15:56:00.000Z",
        "voteCount": 1,
        "content": "Confused between D and E. Isn't \"run the CloudFormation change sets\" same as executing change sets which is same as starting deoployment?"
      },
      {
        "date": "2023-03-05T14:51:00.000Z",
        "voteCount": 1,
        "content": "the discussion part is the most useless part of the portal as consistently there are folks who are dividing the split between answers in a ratio that you end up in a jeopardy deciding what the actual answer is! horrible value add by the discussion session."
      },
      {
        "date": "2023-03-06T06:37:00.000Z",
        "voteCount": 3,
        "content": "FMPOV Moderators should remove answers without pieces of evidence"
      },
      {
        "date": "2023-05-10T18:13:00.000Z",
        "voteCount": 1,
        "content": "And yet here you are."
      },
      {
        "date": "2023-03-02T07:37:00.000Z",
        "voteCount": 2,
        "content": "Think it is AD"
      },
      {
        "date": "2023-03-01T21:22:00.000Z",
        "voteCount": 2,
        "content": "Maybe A and D"
      },
      {
        "date": "2023-03-01T21:18:00.000Z",
        "voteCount": 2,
        "content": "It is a strange question"
      },
      {
        "date": "2023-03-01T11:44:00.000Z",
        "voteCount": 1,
        "content": "A. Incorrect - CodeDeploy is not needed and installing the agent on each EC2 instance is not an efficient approach.\nB. Correct - Creates an application revision and a deployment group in AWS CodeDeploy, creates an environment in CodeDeploy, registers the EC2 instances to the CodeDeploy environment.\nC. Incorrect - This approach does not use AWS CodeDeploy, which is necessary for the final manual approval step.\nD. Correct - Creates a CloudFormation change set for each application stack, pauses for a manual approval step, and runs the CloudFormation change sets and starts the AWS CodeDeploy deployment.\nE. Incorrect - Similar to D, but starts the AWS CodeDeploy deployment before running the CloudFormation change sets, which is not necessary."
      },
      {
        "date": "2023-04-18T13:11:00.000Z",
        "voteCount": 1,
        "content": "Of course CodeDeploy agent is needed for EC2. It is not needed if you deploy app on  Amazon ECS or AWS Lambda:\nhttps://docs.aws.amazon.com/codedeploy/latest/userguide/codedeploy-agent.html"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 206,
    "url": "https://www.examtopics.com/discussions/amazon/view/101299-exam-aws-devops-engineer-professional-topic-1-question-206/",
    "body": "A DevOps engineer is implementing governance controls for a company that requires its infrastructure to be housed within the United States. The engineer must restrict which AWS Regions can be used, and ensure an alert is sent as soon as possible if any activity outside the governance policy takes place. The controls should be automatically enabled on any new Region outside the United States (US).<br><br>Which combination of actions will meet these requirements? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Organizations SCP that denies access to all non-global services in non-US Regions. Attach the policy to the root of the organization.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure AWS CloudTrail to send logs to Amazon CloudWatch Logs and enable it for all Regions. Use a CloudWatch Logs metric filter to send an alert on any service activity in non-US Regions.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an AWS Lambda function that checks for AWS service activity and deploy it to all Regions. Write an Amazon EventBridge rule that runs the Lambda function every hour, sending an alert if activity is found in a non-US Region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an AWS Lambda function to query Amazon Inspector to look for service activity in non-US Regions and send alerts if any activity is found.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite an SCP using the aws:RequestedRegion condition key limiting access to US Regions. Apply the policy to all users, groups and roles."
    ],
    "answer": "AB",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AB",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-05-07T21:16:00.000Z",
        "voteCount": 1,
        "content": "AB makes more sense here"
      },
      {
        "date": "2023-03-08T00:29:00.000Z",
        "voteCount": 1,
        "content": "Its A/B - inspector checks Vulnerabilities / deploying lambda in every regions is useless / restricting all users as well"
      },
      {
        "date": "2023-03-02T05:29:00.000Z",
        "voteCount": 2,
        "content": "I agree with AB"
      },
      {
        "date": "2023-03-01T21:26:00.000Z",
        "voteCount": 3,
        "content": "A and B for me"
      },
      {
        "date": "2023-03-01T11:56:00.000Z",
        "voteCount": 3,
        "content": "Option A: Restricts non-US regions.\nOption B: Sends alerts for non-US activity.\nOption C: Doesn't restrict region usage.\nOption D: Doesn't restrict region usage.\nOption E: Only restricts US regions."
      },
      {
        "date": "2023-03-01T11:35:00.000Z",
        "voteCount": 2,
        "content": "Ab is answer"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 207,
    "url": "https://www.examtopics.com/discussions/amazon/view/101300-exam-aws-devops-engineer-professional-topic-1-question-207/",
    "body": "A company grants external users access to its AWS account by creating an IAM user for each external user. A DevOps engineer must implement a solution to revoke access from IAM users that have not accessed the account in 90 days.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTurn on AWS Config in the AWS account. Deploy the lam-user-unused-credentials-check AWS Config managed rule Configure the rule to run periodically Configure AWS. Config automatic remediation to run the AWSConfigRemediation-RevokeUnusedlAMUserCredentials AWS Systems Manager Automation runbook.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Identity and Access Management Access Analyzer to create an analyzer in the AWS account. Create an Amazon EventBridge rule to match IAM Access Analyzer events for IAM users that were last accessed more than 90 days ago. Configure the rule to run the AWSConfigRemediation-DetachlAMPolicy AWS Systems Manager Automation runbook to detach any policies that are attached to the IAM user.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable AWS Trusted Advisor in the AWS account. Use the AWS Developer Support plan to access the AWS Support API. Configure an Amazon EventBridge scheduled rule to use the Support API\u2019s Trusted Advisor IAM Access Key Rotation check to discover IAM credentials that have not been accessed for more than 90 days. Configure another EventBridge rule to use the Trusted Advisor Check Item Refresh Status event type and to run the AWSConfigRemediation-RevokeUnusedlAMUserCredentials AWS Systems Manager Automation runbook.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable AWS Security Hub in the AWS account. Configure a Security Hub rule that determines when an IAM user was last accessed. Configure an Amazon EventBridge rule to match the Security Hub rule and to run the AWSConfigRemediation-RevokeUnusedlAMUserCredentials AWS Systems Manager Automation runbook."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 5,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-03-01T12:03:00.000Z",
        "voteCount": 5,
        "content": "Option A:  correct, Revokes unused IAM credentials.\nOption B:  does not revoke, Detaches policies for inactive IAM users.\nOption C: does not revoke, Uses Trusted Advisor to discover inactive IAM credentials.\nOption D: does not revoke, Determines last IAM access."
      },
      {
        "date": "2023-03-08T00:21:00.000Z",
        "voteCount": 1,
        "content": "The answer is A."
      },
      {
        "date": "2023-03-02T02:39:00.000Z",
        "voteCount": 2,
        "content": "I vote for A"
      },
      {
        "date": "2023-03-01T21:31:00.000Z",
        "voteCount": 2,
        "content": "A seems correct\nhttps://docs.aws.amazon.com/config/latest/developerguide/iam-user-unused-credentials-check.html"
      },
      {
        "date": "2023-03-01T11:38:00.000Z",
        "voteCount": 1,
        "content": "B is answer"
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 208,
    "url": "https://www.examtopics.com/discussions/amazon/view/101303-exam-aws-devops-engineer-professional-topic-1-question-208/",
    "body": "A company has provided an externally hosted third-party vendor product with access to the company's AWS account. The vendor product performs various AWS actions in the AWS account and requires various IAM permissions. The company granted the access by creating an IAM user, associating IAM policies and inserting the IAM user credentials into the vendor product.<br><br>A security review reveals that the vendor\u2019s access is overly permissive. The company wants to apply the principle of least privilege and wants to continue giving the vendor permissions to perform only the actions that the vendor has performed in the last 6 months.<br><br>Which solution will meet these requirements with the LEAST effort?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Identity and Access Management Access Analyzer to generate a new IAM policy based on the IAM user\u2019s AWS CloudTrail history. Replace the IAM user policy with the newly generated policy.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Identity and Access Management Access Analyzer to generate a new IAM policy based on the IAM user\u2019s AWS CloudTrail history. Attach the newly generated policy as a permissions boundary to the IAM user.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Identity and Access Management Access Analyzer to discover the last accessed information for the IAM user and to create a new IAM policy that allows only the services and actions that the last accessed review identified. Replace the IAM user policy with the newly generated policy.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Identity and Access Management Access Analyzer to discover the last accessed information for the IAM user and to create a new IAM policy that allows only the services and actions that the last accessed review identified. Attach the newly generated policy as a permissions boundary to the IAM user."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-03-08T00:20:00.000Z",
        "voteCount": 2,
        "content": "All of them are more or less OK - but the answer is B - the easiest solution (A is also OK but if existing policy is used by other users/systems - can bring more work)"
      },
      {
        "date": "2023-03-05T14:25:00.000Z",
        "voteCount": 1,
        "content": "Both A and B could work, but B is the least effort, it limits the current permissions and does not replace them with a new policy with a narrower scope."
      },
      {
        "date": "2023-03-02T02:49:00.000Z",
        "voteCount": 1,
        "content": "B is correct\n\nI had marked A, because it is between A or B; but because I have not taken into account that the history of cloudtrail activities must be limited precisely because the permissions are too many at first; therefore it is necessary to limit, and as CloudFloater has indicated, the correct answer is B."
      },
      {
        "date": "2023-03-01T21:35:00.000Z",
        "voteCount": 1,
        "content": "B makes more sense than others"
      },
      {
        "date": "2023-03-01T21:35:00.000Z",
        "voteCount": 1,
        "content": "B makes more sense than others"
      },
      {
        "date": "2023-03-01T12:08:00.000Z",
        "voteCount": 4,
        "content": "Option A: Policy based on CloudTrail history.\nOption B: Policy boundary based on CloudTrail.\nOption C: Policy based on last access.\nOption D: Policy boundary based on last access."
      }
    ],
    "examNameCode": "aws-devops-engineer-professional",
    "topicNumber": "1"
  }
]